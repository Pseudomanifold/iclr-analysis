{"title": "The authors propose a generative model of networks via embeddings with the addition of a prior distribution over networks which facilitates learning more semantic embeddings. They use this model successfully in a variety of tasks.", "review": "The authors propose a generative model of networks by learning embeddings and pairing the embeddings with a prior distribution over networks. The idea is that the prior distribution may explain structure that the embeddings would not have to capture.\n\nThe motivation for doing this is that this structure is typically hard to model for network embeddings.\nThe authors propose a clean -if improper- prior on networks and proceed to perform maximum likelihood inference on it.\nThe experiments show that the approach works fine for link porediction and can be used for visualization.\n\nTwo points: \na) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.\nb) The prior seems to be picked according to properties of the observed data and expressed in a product of constraints. This seems clunky, I would have been more impressed with a prior structure that ties in closer with the embeddings and requires less hand-engineering.\n\nA key point of interest is the following: very exciting recent work (GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models by You et al ICML2018) has proposed neural generative models of networks with a high degree of fidelity and much less hand-picked features.  The work here tries to not learn a lot of these structures but impose them. Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?\nThe drawback in this model here is that ultimately networks are embedded, but not really generated during test time. A more predictive generative model that makes less hard assumptions on graph data would be interesting.\n\nUpdate After rebuttal:\nGiven the authors' rebuttal to all reviews, I am upgrading my score to a 6. I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}