{"title": "The problem studied in the paper is interesting. However, there are various mathematical and theoretical problems with the paper, some of which are mentioned below. In addition, the claims and novelty of the paper fall short in the provided methods and results.", "review": "\nPage 2: What are p_i, i=1,2,...,n, their set T and \\mathcal{P}?\n\nWhat is | | used to compute distortion between a and b?\n\nPlease fix the definition of the Riemannian manifold, such that M is not just any manifold, but should be a smooth manifold or a particular differentiable manifold. Please update your definition more precisely, by checking page 328 in J.M. Lee, Introduction to Smooth Manifolds, 2012, or Page 38 in do Cormo, Riemannian Geometry, 1992.\n\nPlease define \\mathcal{P} in equation (1).\n\nDefine K used in the definition of the hyperboloid more precisely.\n\nPlease provide proofs of these statements for product of manifolds with nonnegative and nonpositive curvatures: \u201cIn particular, the squared distance in the product decomposes via (1). In other words, dP is simply the l2 norm of the component distances dMi.\u201d\n\nPlease explain what you mean by \u201cwithout the need for optimization\u201d in \u201cThese distances provide simple and interpretable embedding spaces using P, enabling us to introduce combinatorial constructions that allow for embeddings without the need for optimization.\u201d In addition, how can you compute geodesic etc. if you use l1 distance for the embedded space?\n\nBy equation (2), the paper focuses on embedding graphs, which is indeed the main goal of the paper. Therefore, first, the novelty and claims of the paper should be revised for graph embedding. Second, three particular spaces are considered in this work, which are the sphere, hyperbolic manifold, and Euclidean space. Therefore, you cannot simply state your novelty for a general class of product spaces. Thus, the title, novelty, claims and other parts of the paper should be revised and updated according to the particular input and output spaces of embeddings considered in the paper. \n\nPlease explain how you compute the metric tensor  g_P and apply the Riemannian correction (multiply by the inverse of the metric tensor g_P) to determine the Riemannian gradient in the Algorithm 1, more precisely. \n\nStep (9) of the Algorithm 1 is either wrong, or you compute v_i without projecting the Riemannian gradient. Please check your theoretical/experimental results and code according to this step.\n\nWhat is h_i used in the Algorithm 1? Can we suppose that it is the ith component of h?\n\nIn step (6) and step (8), do you project individual components of the Riemannian gradient to the product manifold? Since their dimensions are different, how do you perform these projections, since definitions of the projections given on Page 5 cannot be applied? Please check your theoretical/experimental results and code accordingly.\n\nPlease define exp_{x^(t)_i}(vi) and Exp(U) more precisely. I suppose that they denote exponential maps.\n\nHow do you initialize x^(0) randomly?\n\nThe notation is pretty confusing and ambiguous. First, does x belong to an embedded Riemannian manifold P or a point on the graph, which will be embedded? According to equation (2), they are on the graph and they will be embedded. According to Algorithm 1, x^0 belongs to P, which is a Riemannian manifold as defined before. So, if x^(0) belongs to P, then L is already defined from P to R (in input of the Algorithm 1). Thereby, gradient \\nabla L(x) is already a Riemannian gradient, not the Euclidean gradient, while you claim that \\nabla L(x) is the Euclidean gradient in the text.\n\nOverall, Algorithm 1 just performs a projection of Riemannian or Euclidean gradient  \\nabla L(x) onto a point v_i for each ith individual manifold. Then, each v_i is projected back to a point on an individual component of the product manifold by an exponential map. \n\nWhat do you mean by \u201csectional curvature, which is a function of a point p and two directions x; y from p\u201d? Are x and y not points on a manifold?\n \nYou define \\xi_G(m;b,c) for curvature estimation for a graph G. However, the goal was to map G to a Riemannian manifold. Then, do you also consider that G is itself a Riemannian manifold, or a submanifold?\n\nWhat is P in the statement \u201cthe components of\nthe points in P\u201d in Lemma 2?\n\nWhat is \\epsilon in Lemma 2?\n\nHow do you optimize positive w_i, i=1,2,...,n?\n\nWhat is the \u201cgradient descent\u201d refered to in Lemma 2?\n\nPlease provide computational complexity and running time of the methods.\n\nPlease define \\mathbb{I}_r.\n\nAt the third line of the first equation of the proof of Lemma 1, there is no x_2. Is this equation correct?\n\nIf at least of two of x1, y1, x2 and y2 are linearly dependents, then how does the result of Lemma 1 change?\n\nStatements and results given in Lemma 1 are confusing. According to the result, e.g. for K=1, curvature of product manifold of sphere S and Euclidean space E is 1, and that of E and hyperbolic  H is 0. Then, could you please explain this result for the product of S, E and H, that is, explain the statement \u201cThe last case (one negative, one positive space) follows along the same lines.\u201d? If the curvature of the product manifold is non-negative, then does it mean that the curvature of H is ignored in the computations?\n\nWhat is \\gamma more precisely? Is it a distribution or density function? If it is, then what does (\\gamma+1)/2 denote?\n\nThe statements related to use of Algorithm 1 and SGD to optimize equation (2) are confusing. Please explain how you employed them together in detail.\n\nCould you please clarify estimation of K_1 and K_2, if they are unknown. More precisely, the following statements are not clear;\n\n- \u201cFurthermore, without knowing K1, K2 a priori, an estimate for these curvatures can be found by matching the distribution of sectional curvature from Algorithm 2 to the empirical curvature computed from Algorithm 3. In particular, Algorithm 2 can be used to generate distributions, and K1, K2 can then be found by matching moments.\u201d Please explain how in more detail? What is matching moments?\n\n- \u201cwe find the distribution via sampling (Algorithm 3) in the calculations for Table 3, before being fed into Algorithm 2 to estimate Ki\u201d How do you estimation K_1 and K_2 using Algorithm 3?\n\n- Please define, \u201crandom (V)\u201d, \u201crandom neighbor m\u201d and \u201c\\delta_K/s\u201d used in Algorithm 3 more precisely.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}