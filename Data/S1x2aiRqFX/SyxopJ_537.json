{"title": "Too many approximations in formulation, few experiments and discussion", "review": "This paper proposed a differentiable metric for text generation tasks inspired by BLEU and a random training method by the Gumbel-softmax trick to utilize the proposed metric. Experiments showed that the proposed method improves BLEU compared with simple cross entropy training and policy gradient training.\n\nPros:\n* The new metric provides a direct perspective on how good the conjunction of a generated sentence is, which has not been provided other metric historically used on language generation tasks, such as cross-entropy. \n\nCons:\n* Too many approximations that blur the relationship between the original metric (BLEU) and the derived metric.\n* Not enough experiments and poor discussion. Authors should consume more space in the paper for experiments.\n\nThe formulation of the metric consists of many approximations and it looks no longer BLEU, although the new metric shares the same motivation: \"introducing accuracy of n-gram conjunction\" to evaluate outputs. Selecting BLEU as the starting point of this study seems not a reasonable idea. Most approximations look gratuitously introduced to force to modify BLEU to the final metric, but choosing an appropriate motivation first may conduct more straightforward metric for this purpose.\n\nIn experiments on machine translation, its setting looks problematic. The corpus size is relatively smaller than other standard tasks (e.g., WMT) but the size of the network layers is large. This may result in an over-fitting of the model easily, as shown in the results of cross-entropy training in Figure 3. Authors mentioned that this tendency is caused by the \"misalignment between cross entropy and BLEU,\" however they should first remove other trivial reasons before referring an additional hypothesis.\nIn addition, the paper proposed a training method based on Gumbel softmax and annealing which affect the training stability through additional hyperparameters and annealing settings. Since the paper provided only one training case of the proposed method, we couldn't discuss if the result can be generalized or just a lucky.\n\nIf the lengths of source and target are assumed as same, the BP factor becomes always 1. Why the final metric (Eq. 17) maintains this factor?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}