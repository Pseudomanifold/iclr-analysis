{"title": "Interesting idea. Not sure the significance of its experimental results", "review": "This paper proposes the concept of decision state, which is the state where decision is made \u201cmore\u201d dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups.\n\nIn general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting.\n\nI have a couple questions about the experimental part though, mostly about the baselines.\n1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? \n2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?\n3.  What is the reasoning of picking \u201cCount-base baseline\u201d for Figure 4, rather than the method of curiosity-based exploration?\n4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. \n5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks.\n6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.\n\nOther comments:\n1. Equation (3) should be \\le.\n2. Why would Equation (5) hold? \n3. Right before section 2.2, incomplete sentence.\n\n\nDisclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results.\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\"\u00a0arXiv preprint arXiv:1801.01290\u00a0(2018).\n[2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. \"Addressing Function Approximation Error in Actor-Critic Methods.\"\u00a0arXiv preprint arXiv:1802.09477\u00a0(2018).", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}