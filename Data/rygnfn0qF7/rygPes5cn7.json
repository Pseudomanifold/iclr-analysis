{"title": "Good incremental work showing the value of pretraining", "review": "Summary: \nThis paper proposes to extend the pretraining used for word representations in QA (e.g., ELMO) in the following sense: Instead of just predicting next/previous words in a sentence/paragraph, performing a hierarchical prediction over the whole document, by having a local LSTM and a global LSTM as presented in Fig. 1 + the idea of masked language model. Authors show meaningful improvements in 3 tasks that require document level understanding: extractive summarization, document segmentation, and answer passage retrieval for doc level QA. \n\nPros:\n- Good presentation and clear explanations.\n- Meaningful improvements in various tasks requiring document level understanding.\n\nCons:\n- Novelty is mainly incremental\n\nMinor comment: \n- Use a bigger picture for Fig. 1\n- In page 1, Introduction, paragraph 2, line 10, \"due the long-distance ...\" ==> \"due to the long-distance ...\"\n\n**********\nI would like to thank authors for their feedback. After reading their feedback I still believe that novelty is incremental and would like to keep my score. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}