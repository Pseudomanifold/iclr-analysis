{"title": "Some points must clarified", "review": "I think the paper could be published, however I have some concerns.\n\nMayor comments:\n\n- My main concern is that I do not understand why do not directly apply the  KL divergence with respect to p(x) and q(x) instead of considering p(x)\\times q(x') and  p(x')\\times q(x). More specifically, I have understood that your approach is motivated by Theorem 1 (nice result, by the way) but I am not sure it is better than just applying the KL divergence with respect to p(x) and q(x), directly.\n\n- The state-of-the-art discussion for MCMC schemes in the introduction must be completed at least including the Multiple Try Metropolis algorithms, \n\nJ. S. Liu, F. Liang, W. H. Wong, The multiple-try method and local optimization in metropolis sampling, Journal of the American Statistical Association 95 (449) (2000) 121\u2013134.\n\nL. Martino, \"A Review of Multiple Try MCMC algorithms for Signal Processing\", Digital Signal Processing, Volume 75, Pages: 134-152, 2018. \n\nThe sentence about adaptive MCMC's should be also completed.\n\nMinor comments:\n\n- Why do you say that \"MCMC is non-parametric\" in the introduction? in which sense? MCMC methods are sampling algorithms. Please, clarify.\n\n- In my opinion, Eq. (5)-(6)-(8)-(9)-(11)-(12)-(13) are not proper mathematically written  (maybe the same \"wrong\" way of written that, is repeated  in other parts of the text).\n\n- The results in the Figures in the simulations should be averaged more. Specially, Figure 3.\n\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}