{"title": "Empirical evaluation lacking to test the validity of the proposed objective", "review": "The paper proposes to learn transition kernels for MCMC by optimizing the acceptance rate (or its lower bound) of Metropolis-Hastings.\n\nMy main reason for worry is the use of independent proposals for this particular learning objective. While I can buy the argument in the Appendix on how this avoids the collapse problem of Markov proposals, I think the rate of convergence of the chain would greatly reduce in practice because of this assumption. \n\nUnfortunately, the empirical evaluation in this work is lacking to formally confirm or reject my hypothesis. In particular, it is absolutely crucial to compare the performance of this method with Song et al., 2017 (which does not make this assumption) using standard metrics such as Effective Sample Size. Another recent work [1] optimizes for the expected square jump distance and should also have been compared against.\n\n[1]: Levy, Daniel, Matthew D Hoffman, and Jascha Sohl-Dickstein. 2017. \u201cGeneralizing Hamiltonian Monte Carlo with Neural Networks.\u201d\u00a0ArXiv Preprint ArXiv:1711.09268.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}