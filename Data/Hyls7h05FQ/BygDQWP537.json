{"title": "This paper points out an important evaluation perspective, but the model architecture is incremental (limited novelty). ", "review": "This paper proposes GASI to disambiguate different sense identities and learn sense representations given contextual information. \nThe main idea is to use scaled Gumbel softmax as the sense selection method instead of soft or hard attention, which is the novelty and contribution of this paper.\nIn addition, the authors proposed a new evaluation task, contextual word sense selection, which can be used to quantitatively evaluate the semantic meaningfulness of sense embeddings.\nThe proposed model achieves comparable performance on traditional word/sense intrinsic evaluation and word intrusion test as previous models, while it outperforms baselines on the proposed contextual word sense selection task.\n\nWhile the scaled Gumbel softmax is the claimed novelty, it is more like an extension of the original MUSE model (Lee and Chen, 2017), which proposed the sense selection and representation learning modules for learning sense-level embeddings.\nThe only difference between the proposed one and Lee and Chen (2017) is Gumbel softmax instead of reinforcement learning between sense selection and representation learning modules.\nTherefore, the idea from the proposed model is similar to Li and Jurafsky (2015), because the sense selection is not one-hot but a distribution.\nThe novelty of this paper is limited because the model is relatively incremental.\n\nFrom my perspective, the more influential contribution is that this paper points out the importance of evaluating sense selection capability, which is ignored by most prior work.\nTherefore, I expect to see more detailed evaluation on the selection module of the model. \nAlso, because the task of this paper is multi-sense embeddings, the traditional word similarity (without contexts) task seems unnecessary. \nMoreover, there is no error analysis about the result on the proposed contextual word sense selection task, which may shed more light on the strength and weakness of the model. \nFinally, I suggest the authors remove the word-level similarity task and try the recently released Word in Context (WiC) dataset, which is a binary classification task that determines whether the meaning of a word is different given two contexts.\nIt would be better to see that GASI performs well on this task given its better sense selection module.\n\nOverall, the contribution is somewhat incremental and the evaluation/discussion should focus more on the sense selection module. \nConsidering the issues mentioned above, I will expect better quality for an ICLR paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}