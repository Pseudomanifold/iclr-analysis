{"title": "Novel work breaking ground on abstractive unsupervised multi-document summarization", "review": "# Positive aspects of this submission\n\n- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.\n\n- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.\n\n- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.\n\n# Criticism\n\n- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.\n\n- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.\n\n- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}