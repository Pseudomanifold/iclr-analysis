{"title": "An interesting paper", "review": "This is an interesting paper, trying to find the adversarial cases in reinforcement learning agents. The paper discusses several different settings to investigate how generalizable the worst-case environment is across different models and conjectured that it comes from the bias in training the agents. Overall the paper is well-written and the experiments seem convincing. I have two questions regarding the presented result.\n\n1. The search algorithm depicted in section 2 is only able to find a local optimum in the environment space. How robust is the result given different initializations?\n\n2. It is briefly discussed in the paper that the failure in certain mazes might come from the structural bias in the training and the \u201ccomplex\u201d mazes are under-represented in the training dataset. It is hence natural to ask, if the procedure described in this paper can be incorporated to enhance the performance by some simple heuristics like re-weighting the training samples. I think some discussion on this would be beneficial for verifying the conjecture made here.\n\n3. The authors compared the \u201chardness\u201d of the mazes based on the number of walls in the maze. But it is arguably a good metric as the authors also mentioned visibility and other factors in measuring the complexity of the task. I would like to see more exploration in different factors that accounts for the complexity and maybe compare different agents to see if they are sensitive in the same set of factors. \n\nTo summarize, I like the idea of the paper and I think the result can be illuminating and worth some more follow-up work to understand the RL training in general.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}