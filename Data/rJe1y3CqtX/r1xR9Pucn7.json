{"title": "Efficient robust optimization in simulation through careful selection of diverse set of perturbations", "review": "This paper addresses the problem of finding a policy that will perform well in a real environment when training in a simulator that may have errors.  It takes the now standard approach of trying to find a policy that performs well in an ensemble of simulated environments that are perturbations of the basic simulator.   The question is:  how can we construct an ensemble that represents the uncertainty about the real world well while being small enough for computational efficiency.  The idea is to construct a diverse set of samples that represents the whole space of important variations in the simulation;  the particular novelty here is to ensure that the sample set attains coverage over the *behaviors* of the simulator rather than the parameters of the simulation.  This problem is made difficult by the fact that there is no finite space of samples to choose from and the fact that we don't have a natural distance metric on the simulator behavior.\n\nThe main positive contributions of the paper are:\n- The view of the problem of selecting from an infinite set as one of streaming sub-modular optimization.  This is a nice idea that is new to me and seemed appropriate for the problem.\n- The idea that we want diversity in behavior, and then the technical approach of defining a kernel on simulator parameter sets that depends on the trajectories that those parameters induce.\n\nI do have a set of questions and concerns:\n- Might it not be better (more robust) to use not just trajectories from the current policy, but from other policies as well, to compute the kernel on parameter sets?  \n- How do you get the length-scale parameters for the kernel?\n- The confidence intervals in table 1 are too big to really justify firm conclusions;  it would be better to run the algorithms several more times, until the intervals pull apart.\n- You say: \"For ease of implementation  and since, in higher dimensional system, the variance of the policy gradients becomes  a significant factor, we train the  robot  on both the environment summaries and the N_s random rollouts.\"   This seems like it might be an important point that should be addressed earlier.  And, why does this ease implementation?\n- I didn't understand:  \"Diverse summaries are more consistent than pure random sampling.\"  What do you mean by consistent here?\n- The metrics used in the empirical comparisons don't seem exactly right to me.  The goal of this work is to learn a policy that is robust in some sense (so  that,  e.g., it will do sim-2-real well).   We  really want  it to  work well in all  possible cases, not just in expectation or according to the sampling distribution you create, (I guess---since the paper said  the minimax criterion was desirable but difficult to work with).  So, then, it seems like  the best performance criterion would be to sample a whole lot of  domain parameters and report performance  on the worst  (rather than reporting performance on a distribution that's like the one you  trained it on or on an easy random one).\n\nOverall, my view is that the idea is good, but somewhat small, and it hasn't really yet been proven to make a big difference.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}