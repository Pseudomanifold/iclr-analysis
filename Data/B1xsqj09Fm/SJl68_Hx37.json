{"title": "Great progress achievement in the field of image generation", "review": "This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes. \nThe key components of the approach are :\n- increasing the batch size by a factor 8\n- augmenting the width of the networks by 50% \nThese first two elements result in an Inception score (IS) boost from 52 to 93.  \n- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.\nThe core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.\nVariations of this threshold lead to variations in FD and IS, as shown in insightful experiments. The comments that more data helps (internal dataset experiments) is also informative. \nVery nice to have included negative results and detailed parameter sweeps.\n\nThis is a very nice work with impressive results, a great progress achievement in the field of image generation. \nVery well written.\n\nSuggestions/questions: \n- it would be nice to also propose unconditioned experiments. \nIt would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve. \n- I understand that no data augmentation was used during training?    \n- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?\n- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.\n- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.\n- It would be nice to display more Nearest neighbors for the dog image.\n- It would be nice to add a figure of random generations.\n- make the bib uniform: remove unnecessary doi - url - cvpr page numbers\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}