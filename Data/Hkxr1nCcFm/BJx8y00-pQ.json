{"title": "Valuable results that don't quite support the conclusions", "review": "The authors describe their application of a ConvLSTM network architecture to a number of 2-D environments that have been used in the RL community to evaluate agents' capabilities for planning.\n\nExtensive experiments show an increase in performance and generalisation compared to a number of other network architectures, including some from recent works which are designed to include an inductive bias towards implicit planning. Their model can also benefit from a deliberation phase that uses extra computation at the beginning of an episode.\n\nThe authors take this as evidence that comparatively unstructured architectures learn effective planning algorithms.\n\nOverall, I find the paper to make a useful empirical contribution, presenting a performant architecture and results that can help guide how the community thinks about this type of benchmark. However, I believe that the framing of the results and discussion of the nature of planning should be more careful. Beyond a more careful discussion, the claims could be supported by explicit comparison to a \"true\" planning algorithm that makes use of a learned model.\n\nIn more depth:\n(A) The empirical contributions.\nThe proposed architecture is described fairly clearly, and less critical elements are appropriately identified.\nComparisons to a number of planning-inspired architectures are quite comprehensive.\nExperiments testing generalisation add to the body of evidence that overparameterised deep neural models can generalise well even with limited data.\nI worry that the architecture may be overfit to the highly structured 2-D environments used. However, these environments are valuable testbeds whose structure is also exploited by some of the planning-inspired approaches.\n\n(B) Conclusions & nature of planning\nThe authors take a behaviourist approach, identifying three properties of an \"effective planning algorithm\". I am not totally convinced that these are comprehensive, nor that the experiments demonstrate their clear fulfillment. This is difficult to assess because the criteria are extremely subjective.\n\n(1) generalisation to \"radically different situations\". Sokoban clearly has a large state space, but it is unclear that held-out levels, or those with 7 rather than 4 boxes, are \"radically\" different to the training tasks.\n(2) Learning from \"small amounts of data\". What this means is clearly subjective and highly problem-dependent.\n(3) Making use of additional computation at runtime. The use of an additional deliberation phase at the beginning of the episode shows some limited scalability with computation. However, it does not permit later on-line planning to benefit from additional computation, which is a core feature of most standard planning algorithms.\n\nCritically, the current experiments show that the large version of the proposed architecture performs better on these 3 metrics than some other architectures, but do *not* compare to anything we could unanimously agree performs \"true planning\".\nTo support the paper's claims, and to reduce the subjectivity of these metrics, it would be extremely useful to see comparisons to a \"true\" planning algorithm using an explicit environment model.\nHow many unique levels and environment interactions are required to learn a model of Sokoban? How well does that model generalise to new levels or numbers of boxes? What is the performance of e.g. MCTS using this model using different amounts of computation?\nThese questions could be considered out-of-scope for this particular submission, and certainly require important decisions to formulate appropriate comparisons to the model-free approach. But without at least some attempt at their answers it is hard to assess how well this model-free approach matches the behaviour of \"true planning\". The authors' implementation of I2A and an unspecified \"powerful tree search algorithm\" make me optimistic that these model-based experiments may even be feasible!\nAs it stands, I believe some of the claims are insufficiently supported and the overall presentation of the results overreaches. \n\nI believe the authors could address many of these concerns and that the core contributions of the paper are valuable.\nAs an addendum, the Discussion section is clearer and more well supported than the framing in the Introduction.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}