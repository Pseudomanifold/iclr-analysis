{"title": "Unclear about the extent of the contribution", "review": "The paper proposes a method for converting a non-differentiable machine learning pipeline into a stochastic, differentiable, pipeline that can be trained end-to-end with gradient descent approaches.\n\n* Clarity: The language in the paper is very clear and easy to follow. The paper is lacking in clarity only when discussing some results/concepts from previous work (see detailed comments below).\n* Quality: Overall the paper is in good shape, aside from some concerns which I will describe further.\n* Originality: The originality is not very clear because it seems that a lot of ideas are borrowed from Schulman et al. (2015) (i.e. the concept of stochastic computation graph and how to compute the gradient) and from Rao et, al (2018) (i.e. sampling bounding boxes in some stages of the pipeline). To be fair to the authors, I am not very familiar with the two papers mentioned above, which makes this hard to judge. However, I think this paper could have explained more clearly which part exactly is a novelty of this paper, and where it separates from the rest.\n* Significance: The concept of converting a non-differentiable pipeline to a differentiable version is indeed very useful and widely applicable, but the experimental section did not convince me that this particular method indeed works: the results show a very small improvement (0.7-2%) on a single system (Faster R-CNN), that has already been pretrained (so not clear if this method can learn from scratch).\n\nPros:\n1)\tOverall the paper is well written.\n2)\tThe algorithm shown in Figure 4 nicely summarizes the whole algorithm.\n3)\tI particularly liked the part of Section 3 where it is shown the equivalence between the optimal parameters for the non-differentiable pipeline and the optimal parameters for the differentiable version.\n4)\tFigure 5 with detailed results is useful.\n\nCons:\n5)\tThe way the paper is written, it is not clear where the contribution of this paper separates from existing work, mainly Schulman et al. (2015). I believe the idea of going around non-differentiability via minimizing a surrogate loss (i.e. your equation (2) introduced by Schulman et al. (2015)) is already known. I\u2019m not sure exactly where this work diverges from that.\n6)\tThe contribution of this paper is posed as a general framework for turning an arbitrary non-differentiable pipeline into a similar differentiable and stochastic version. However, the experimental section does not convince me that: \n    a)\tit is general \u2013 because it is applied only on the Faster R-CNN problem. \n    b)\tthat it can learn from scratch \u2013 it is only applied after the base method has been pre-trained. There are no experiments where you train a network from scratch with this new differentiable pipeline. If the reason is that ResNets are hard to train from scratch, then you can always try your pipeline on a smaller problem, even a synthetic dataset, just to prove that it works.\n     c)\tthat the improvement is significant from the baseline method \u2013 the results section show only a 1-2% increase in mAP, and only for the smaller networks (on larger ResNet models the gain is less than 1%, and the standard deviation is getting larger). \n\nDetailed comments:\n7)\tYou only cite the work of Schulman et al. (2015) at the beginning of section 2.1. While moving to section 2.2, I initially got the wrong impression that this us your contribution. Please state clearly where this comes from.\n8)\tIt is not explained well why the new gradient can be estimated as in equation (2). I spent quite some time trying to figure out where that comes from (particularly the log part), only to realize that the explanation is probably in the original work (at the time when I thought this is your contribution). Please point the readers to it. \n\nFinal remarks: \nOverall this paper introduces some interesting ideas. My main concerns were: (1) the originality, and (2) the results are not convincing. Perhaps concern (1) can be easily clarified by the authors, but for concern (2) it might be useful to show new results (training from scratch, other architectures to prove generality), as well as give arguments as to why the 1-2% gain in mAP is significant. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}