{"title": "potentially useful heuristic for GANs with vague maths", "review": "GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications. Unfortunately, GANs often show unstable behaviour during the training phase. The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.\n\nWhile the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:\n1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\\theta_{old}}, ...\n2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.\n3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.\n4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.\n\nWhile the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.\n\n---\n\nAfter paper revisions: \n\nThank you for the updates. The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}