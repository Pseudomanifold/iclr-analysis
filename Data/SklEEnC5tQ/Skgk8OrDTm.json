{"title": "Sound method and good results", "review": "Summary:\nThis paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. The paper motivates the proposed method as follows:\n-       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution\n-       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity*\n-       Then, the paper proves that this condition can be satisfied by ensuring that generator\u2019s objective (L) is concave\n-       Since it\u2019s difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective\nExperiments confirm the validity the proposed approach. Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets\n \nStrengths:\n-   \tThe proposed method is very interesting and is based on sound theory\n-   \tConnection to optimal transport theory is also interesting\n-   \tIn practice, the method is very simple to implement and seems to produce good results\n \nWeaknesses:\n-       Readability of the paper can be generally improved. I had to go over the paper many times to get the idea.\n-       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines).\n \nQuestions/Comments:\n-       Equation (7) has typos (uses theta_old instead of theta in some places)\n-       Section 4.1 (effect of monoticity) is a bit confusing. My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there. Can you clarify what you do there and in general this experiment a bit more?\n-       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I\u2019m wondering if you can compare it on low dimensional settings (e.g. as in Fig 2). It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1.\n\n \nOverall recommendation:\nThe paper is based on sound theory and provides very interesting perspective. The method seems to work in practice on a variety of experimental setting. Therefore, I recommend accepting it.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}