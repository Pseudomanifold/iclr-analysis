{"title": "good paper, with minor issues", "review": "This paper uses convex relaxation to verify a larger class of specifications\nfor neural network's properties. Many previous papers use convex relaxations on\nthe ReLU activation function and solve a relaxed convex problem to give\nverification bounds.  However, most papers consider the verification\nspecification simply as an affine transformation of neural network's output.\nThis paper extends the verification specifications to a larger family of\nfunctions that can be efficiently relaxed.\n\nThe author demonstrates three use cases for non-linear specifications,\nincluding verifying specifications involving label semantics, physic laws and\ndown-stream tasks, and show some experiments that the proposed verification\nmethod can find non-vacuous bound for these problems. Additionally, this paper\nshows some interesting experiments on the value of verification - a more\nverifiable model seems to provide more interpretable results.\n\nOverall, the proposed method seems to be a straightforward extension to\nexisting works like [2]. However the demonstrated applications of non-linear\nspecifications are indeed interesting, and the proposed method works well on \nthese tasks.\n\nI have some minor questions regarding this paper:\n\n1) For some non-linear specifications, we can convert these non-linear elements\ninto activation functions, and build an equivalent network for verification\nsuch that the final verification specification becomes linear. For example, for\nverifying the quadratic specification in physics we can add a \"quadratic\nactivation function\" to the network and deal with it using techniques in [1] or\n[2].  The authors should distinguish the proposed technique with these existing\ntechniques. My understanding is that the proposed method is more general, but\nthe authors should better discussing more on the differences in this paper.\n\n2) The authors should report the details on how they solve the relaxed convex\nproblem, and report verification time. Are there any tricks used to improve\nsolving time? What is the largest scale of network that the algorithm can\nhandle within a reasonable time?\n\n3) The detailed network architecture (Model A, Model B) is not shown. How many\nlayers and neurons are there in these networks? This is important to show the\nscalability of the proposed method.\n\n4) For the Mujoco experiment, I am not sure how to interpret the delta values\nin Figure 1. For CIFAR I know it is the delta of pixel values but it is not\nclear about the delta in Mujoco model. What is the normal range of predicted\nnumbers in this model?  How does the delta compare to it? Is the delta very\nsmall or trivial?\n\n5) Is it possible to show how loose the convex relaxation is for a small toy\nexample? For example, the specification involving quadratic function is a\ngood candidate.\n\nThere are some small glitches in equations:\n\n* In (4), k is undefined\n* In (20), I am not sure if it is equivalent to the four inequalities after (22).\nThere are 4 inequalities after (22) but only 3 in (20).\n\n\nMany papers uses convex relaxations for neural network verification. However\nvery few of them can deal with general non-linear units in neural networks.\nReLU activation is usually the only non-linear element than we can handle in\nmost neural network verification works. Currently the only works that can\nhandle other general non-linear elements are [1][2]. This paper uses more\ngeneral convex relaxations than these previous approaches, and it can handle\nnon-separable non-linear specifications. This is a unique contribution to this\nfield. I recommend accepting this paper as long as the minor issues mentioned\nabove can be fixed.\n\n[1] \"Efficient Neural Network Robustness Certification with General Activation\nFunctions\" by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel.\nNIPS 2018\n\n[2] \"A dual approach to scalable verification of deep networks.\" by\nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and\nPushmeet Kohli. UAI 2018.\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}