{"title": "large-scale spectral decomposition - high practical value", "review": "Spectral Inference Networks, Unifying Deep and Spectral Learning\n\nThis paper presents a framework to learn eigenfunctions via a stochastic process. They are exploited in an unsupervised setting to learn representation of video data. Computing eigenfunctions can be computationally challenging in large-scale context. This paper proposes to tackle this challenge b y approximating then using a two-phase stochastic optimization process. The fundamental motivation is to merge approaches from spectral decomposition via stochastic approximation and learning an implicit representation. This is achievement with a clever use of masked gradients, Cholesky decomposition and explicit orthogonalization of resulting eigenvectors. A bilevel optimization process finds local minima as approximate eigenfunction, mimicking Borkar\u201997. Results are shown to correctly recover known 2d- schrodinger eigenfunctions and interpretable latent representation a video dataset, with a practical promising results using the arcade learning environment.\n\nPositive\n+ Computation of eigenfunctions on very large settings, without relying on Nystrom approximation\n+ Unifying spectral decomposition within a neural net framework\n\nSpecific comments\n- Accuracy issue - Shape of eigenfunctions are said to be correctly recovered, but no words indicates their accuracy. If eigenfunction values are wrong, this may be critical to the generalization of the method.\n- Clarity could be improved in the neural network implementation, what is exactly done and why, when building the network\n- Algorithm requires computing the jacobian of the covariance, which can be large and computationally expensive - how to scale it to large settings?\n- Fundamentally, a local minimum is reached - any future work on tackling a global solution?  Perhaps by exploring varying learning rates?\n- Practically, eigenfunction have an ambiguity to rotation - how is this enforced and checked during validation? (e.g., rotating eigenfunctions in Fig 1c)\n- Eigenfunction of transition matrix should, if not mistaken, be smooth, whereas Fig 2a shows granularity in the eigenfunctions values (noisy red-blue maps) - Is this regularization issue, and can this be explicitly correctly?\n- Perhaps a word on computational time/complexity?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}