{"title": "A simple but not very convincing clipping method for activation quantization in deep networks", "review": "This paper empirically finds that the distribution of activations in quantized networks follow  Gaussian or Laplacian distribution, and proposes to determine the optimal clipping factor by minimizing the quantization error based on the distribution assumption.\n\nThe pros of the work are its simplicity, the proposed clipping and quantization does not need additional re-training. However, while the key of this paper is to determine a good clipping factor, the authors use uniform density function to represent the middle part of both Gaussian and Laplacian distributions where the majority of data points lie in, but exact computation for the tails of the distributions at both ends. Thus the computation of quantization error is not quite convincing. Moreover, the authors do not compare with the other recent works that also clip the activations, thus it is hard to validate the efficacy of the proposed method.\n\nFor the experiments, the authors mention that a look-up table can be pre-computed for fast retrieval of clipping factors given the mean and sigma of a distribution.  However, the mean and sigma are continuous numbers, how is the look-up table made?  Moreover, how is the mean and std estimated for each weight tensor and what is  the complexity?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}