{"title": "Errors and Contributions not significant ", "review": "The paper describes a clipping method to improve the performance of one particular type of quantization method that is naive clipping to closest \"bins\". The contribution of the paper is the (possibly incorrect) derivation of the clipping value that causes the least quantization error IF assumptions can be made about the distribution of the parameters (in a non-bayesian sense). Thus, the significance is low due to both reasons.\n\nOne conceptual issue is the assumed relationship between quantization error and classification accuracy. The literature has shown that high quantization error does not necessarily mean low classification accuracy when using non-uniform quantization. The proposed clipping does not account for classification accuracy (on training set), but I understand the motivation being that the training set is not available. \n\n1. There seems to be an error in derivation of Eq (3), the first term should be $(x-sgn(x).\\alpha) = x+\\alpha$ for $x$ negative. Please comment on this.\n\n2. When solving the integrals, the authors simply pull the solution \"out of the hat\" and show that the derivative is the integrand. This is a very opaque presentation that we cannot see how you solved the integral. What is C in $\\psi(x)$?\n\n3. The assumptions on the parameters are only valid for the particular model/dataset/precision. The assumption does not generalize arbitrarily. For example, models with quantized weights have bi-modal distributions. How would you clip the  activations after e.g. a ReLu? This is without going in to the weaknesses of the K-S test. \n\n4. Experiments do not show any comparison to the large body of prior work in this area. \n\n5. Page 4, para below (3), what is \"common additive orthogonal noise\"? You should explain or give intuition instead of simply referring to a different paper.\n\n6. In the uniform case, one would think f(x)=1/<range of the interval>=2\\alpha. Why is it 1/\\Delta?\n\n6. Section 4, range should be [-\\alpha, \\alpha] instead of [\\alpha, -\\alpha]? Since \\alpha is positive.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}