{"title": "review", "review": "This paper derives a formula for finding the minimum and maximum clipping values for uniform quantization which minimize the square error resulting from quantization, for either a Laplace or Gaussian distribution over pre-quantized value. This seems like too small a contribution to warrant a paper. I wasn't convinced that appropriate baselines were used in experiments. There were a number of statements that I believed to be technically slightly incorrect. There were also some small language problems (though these didn't hinder understanding).\n\nmore specific comments:\n\nabstract:\n\"derive exact expressions\" -- these expressions aren't exact. they turn out to be based on a piecewise zeroth order Taylor approximation to the density.\n\nmain paper:\n\"allow fit bigger networks into\" -> \"allow bigger network to fit into\"\n\"that we are need\" -> \"that need\"\n\"introduces an additional\" -> \"introduces additional\"\nclippig -> clipping\n\nit's not clear a-priori that information loss is the property to minimize that maximizes performance of the quantized network.\n\n\"distributions of tensors\" -> \"distribution of tensor elements\"\nthis comment also applies in a number of other places, where the writing refers to the marginal distribution of values taken on by entries in a tensor as the distribution over the tensor. note that a distribution over tensors is a joint distribution over all entries in a tensor. e.g. it would capture things like eigenvalues, entry-entry covariance, rather than just marginal statistics.\n\n\"than they could have by working individually\" -> \"than could have been achieved by each individually\"\n\nWhy the focus on small activation bit depth? I would imagine weight bit-depth was more important than activation bit depth. Especially since you're using ?32-bit? precision in the weight/activations multiplications, so activations are computed at a high bit depth anyways.\n\nTable 1: Give absolute accuracies too! Improvement relative to what baseline?\n\nsec 2:\nsufficeint -> sufficient\n\\citep often used when it should instead be \\citet.\n\"As contrast\" -> \"In contrast\"\n\nsection 3:\nuniformity -> uniformly\n\nI don't believe the notion of p-value is being used correctly here w.r.t. the Kolmogorov-Smirnov test.\n\nFigure 1: The mean square error should never go to 0. This suggests something is wrong. If it's just a scaling issue, consider a semilogy plot.\n\nFigure 2: I'm unclear what baseline (no clipping) refers to in terms of clipping values. For uniform quantization there needs to be some min and max value.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}