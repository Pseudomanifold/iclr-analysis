{"title": "nicely presented ideas, lacking discussion around guarantees (or not)", "review": "Being familiar but not an expert in either game theory or adversarial training, my review will focus on the overall soundness of the proposed method\n\nSummary:\n\nThe authors propose to tackle the problem of adversarial training.\nDeep networks are know to be susceptible to adversarial attacks.\nAdversarial training is concerned with the training of networks that both achieve good performance for the original task while being robust to adversarial attacks.\n\nThey propose to focus on universal adversarial perturbations, as opposed to per-sample perturbations. The latter is a subclass of the former. \nIt doesn\u2019t strike as the most natural scenario: I can\u2019t really think of a practical image classification scenario where one would want to perturb a whole dataset of image with a single perturbation. That said, this focus leads to simpler algorithms (complexity and storage wise) which are worth exploring.\n\nThe authors first present the min-max problem of adversarial training at hand where a classifier f mimizes a loss L for a dataset D, while the conman maximizes the loss over perturbation of the dataset \\epsilon.\nThey then introduce an algorithm to solve it inspired by fictitious play:\nA sequence of classifiers and perturbed datasets are created iteratively by the two players (classifier, conman) and each player uses the complete history of its opponent to make its next move.\n\nThe objective solved by each player  is :\nconman: fool all past classifiers with a single new perturbation\nclassifier: be robust to all past perturbations so far.\n\nAlthough it makes intuitive sense, it is unclear from the manuscript whether this formulation provides any convergence guarantees. It would be great to know whether the connection to fictitious play is purely inspirational or if any of the theoretical guarantees from game theory apply here.\n\nThe conman\u2019s objective to fool all past classifiers is the bottleneck (in terms of storage) and an approximation is proposed: the mean loss over past classifiers is replaced by the loss under a single \u2018average\u2019 classifier trained on all past dataset, with the intuition that this average classifier summarizes all past classifiers\n\nA particular algorithm for perturbation learning is described and the proposed algorithm is compared against two baselines: a pre-existing adversarial training algorithm, an non-adversarial algorithm\n\nThe metrics chosen are accuracy and adversarial accuracy.\nOn standard classification tasks, adversarial algorithms perform slightly less well on the original task (accuracy) but are robust to perturbation as expected,\n\nIt would be interesting to know if these good performances extend to per-sample perturbations: Do a network trained on universal perturbations perform well against per sample perturbation? \n\n\nRemarks:\nsgn missing in the adversarial patch update (and who is alpha?)\nintroduce terminology: white box black box\n", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}