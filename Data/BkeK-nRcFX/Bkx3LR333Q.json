{"title": "Solid contribution", "review": "This paper proposes a metric to measure the \"nonlinearity\" of neural network, and presents evidence that the value of this metric at initialization time is predictive of generalization performance.\n\nApart from a few problems I think this paper is well written and thorough. The contribution is solid, although not earth shattering given previous work on such metrics.  There seems to be a basic error in some of the early math, although I don't think this will qualitatively affect the results in any significant way.\n\n\n-----------------\nDetailed comments by section:\n------------------\n\nSection 3:\n\nIt seems like a 1/sqrt(d) factor is missing from these Q_i(S_x x(i)) and Q_j(S_x f(x,j)) formulas.  As far as I can tell this doesn't affect Def 1 because you seemed to use the correct formula there. \n\nHowever, the rewritten version with the traces doesn't seem to be correct. There should be a d_in factor in the denominator (inside the square root). This error seems unrelated to the other one.  Assuming I'm correct and that this is an error, does this affect your results in the various figures?  And what is the actual final definition of NLC that you used?\n\nIn general, it's annoying for the reader to verify that all of these forms are equivalent.  And it's fiddly enough with the sqrt(d) terms constantly disappearing and reappearing in the numerator and denominators that even you made multiple errors (as far as I can tell).  I would suggest making this section more rigorous and writing out everything carefully. And you probably don't need to rewrite it in so many equivalent forms with different notation unless they are useful somehow. \n\nThe use of the Q and S symbols feels superfluous and counterproductive. Standard notation with expectations and squares wouldn't take much more space and would be a lot clearer.\n\n\nSection 4:\n\n\"we plot the relative diameter of the linearly approximable regions of the network as defined in section 3\": but you don't seem to define \"relative diameter\" there. As far as I can tell it's only defined in Appendix E, and this is only mentioned in the caption of figure 1.  It's impossible to interpret this result without knowing precisely what \"relative diameter\" is.  If you can't afford to describe this in the main paper you should at least mention that it's a different (more expensive) way of estimating the same thing that the NLC estimates.\n\nIn Figure 2, are the higher test errors due to the optimizer failing to lower the training error, or due to a greater generalization gap?  I guess the Figure 3 results suggest the latter possibility, which is surprising to me. \n\n\nWhat does it mean to have a \"very biased output\".  What does that inequality mean intuitively?  Should there be an absolute value on the RHS?  It would be much easier to parse it if it were written in plain notation without these S and Q symbols.\n\n\nSection 6:\n\n\"metric also an\" -> \"metric also has an\"\n\nCan you generate a failure case for \"correlation information\" that doesn't involve Batch Norm layers?  I don't think the authors of those works meant for their results to deal with that.\n\nNote that there are actually a lot of papers going back to the 90s that discussed and proved representational benefits of depth in neural networks.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}