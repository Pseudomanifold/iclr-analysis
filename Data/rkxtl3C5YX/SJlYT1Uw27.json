{"title": "heavy on notations, limited impact applicability / experimental results", "review": "The paper proposes a formal framework to claim that Alpha Zero might converges to a Nash equilibrium. The main theoretical result is that the reward difference between a pair of policy and the Nash policy is bounded by the expected KL of these policy on a state distribution sampled from the Nash policies. \n\nThe paper is quite heavy on notations and relatively light on experimental results. The main theoretical results is a bit remote from the case Alpha Zero is applied to. Indeed the bound is in 1/(1-/gamma) while Alpha Zero works with gamma = 1. Also \n\nCasting a one player environment as a two player game in which nature plays the role of the second player makes the paper very heavy on notations.\n\nIn the experimental sections, the only comparison with RL types algorithm is with SARSA, it would be interesting to know how other RL algorithms, perhaps model free, would compare to this, i.e. is Alpha Zero actually necessary to solve this tasks?\n\n\n--- \np 1\n\n' it uses the current policy network g_theta' : policy and value network.\n\np 2 / appendix\nNo need to provide pseudo code for alpha zero the original paper already describes that?\n\np2 (2). It seems a bit surprising to me that the state density rho does not depend upon pi but only on pi star? \n\np4:\nNot sure why you need to introduce R(pi), isnt it just V_pi (s_0) ? Also usually the letter R is used for the return i.e. the sum of discounted reward without the expectation, so this notation is a bit confusing?\n\np5:\nparagraph2: I don't quite see the point of this.\n\np8:\n\"~es, because at most on packet can get serviced from any input or output port.~\" typo ?\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}