{"title": "More experimental evaluations can improve the paper", "review": "Summary\nThis paper proposed to estimate the conditional average treatment effect (CATE) in counterfactual inference by modifying an existing approach, called X-learner, in the following ways. First, both the treatment response function and the control response function were modeled as neural networks, denoted as f_theta, rather than random forest. The imputed treatment effect can be computed. Second, the mapping from the imputed treatment effect to the CATE were also modeled as neural networks, denoted as f_tau. Therefore, f_theta and f_tau can be jointly optimized through backpropagation. Experimental results on a simulated dataset showed that the co-training of f_tau and f_theta contributed most to the improvement of Y-learner compared to the X-learner. Experimental results on six simulated datasets, the VOTE dataset, and MNIST dataset showed the proposed Y-learner can outperform its competing alternatives, including T-leaner, S-learner, R-learner, X-learner, on a subset of all datasets.\n\nComments\nThis work aimed to estimate the conditional average treatment effect. The related work section contains relevant work from the statistics and economics community. Did the author try to implement and compare against some strong baselines commonly used in the machine learning community, such as the Bayesian additive regression trees (BART, Hill 2011), the counterfactual regression with integral probability metrics (Shalit et al. ICML 2017), counterfactual multiple-task Gaussian process (Alaa et al. NIPS 2017)?\n\nWhy didn't the MSE of X-learner (Figure3: Simulation 3) and T-learner (Figure 3: Simulation 1, Figure 5) monotonically decrease w.r.t. the number of samples?\n\nWere the confidence intervals shown in Figure 2,3 generated by simulating those synthetic datasets multiple times? Could the authors explain why the confidence interval of the Y-learner is very small when the number of samples used is quite small?\n\nWhat were the network architectures and detailed hyper-parameter settings used for f_theta and f_tau? Were the results sensitive to the choice of those parameters? For the competing baselines (T-leaner, S-learner, R-learner, X-learner), which model were used as the function approximator, neural network or random forest?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}