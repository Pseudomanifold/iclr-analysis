{"title": "A paper addressing an interesting problem, but lacks clarity and hard to understand, tech novelty is unknown", "review": "This paper proposes a deep GNN network for graph classification problems using their adaptive graph pooling layer. It turns the graph down-sampling problem into a column sampling problem. The approach is applied to several benchmark datasets and achieves good results.\n\nWeakness\n\n1.\tThis paper is poorly written and hard to follow. There are lots of typos even in the abstract. It should be at least proofread by an English-proficient person before submitted. For example, in the last paragraph before Section 3. \u201cIn Ying et al. \u2026\u2026. In Ying et al.\u201d\n2.\tIn paragraph 1 of Section 3, there should be 9 pixels around the center pixel including itself in regular 3x3 convolution layers.\n3.\tThe definition of W in Eq(2) is vague. Is this W shared across all nodes? If so, what\u2019s the difference between this and regular GNN layers except for replacing summation with a max function?\n4.\tThe network proposed in this paper is just a simple CNN. GNN can adopt such kind of architectures as well. And I didn\u2019t get numbers of first block in Figure 1. The input d is 64?\n5.\tThe algorithm described in Algorithm 1 is hard to follow. There are some latex tools for coding the algorithms.\n6.\tThe authors claim that improvements on several datasets are strong. But I think the improvement is not that big. For some datasets, the network without pooling layers even performs better at one dataset. The authors didn\u2019t provide enough analysis on these parts.\n\nStrength:\n1.\tThe idea used in this paper for graph nodes sampling is interesting. But it needs more experimental studies to support this idea. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}