{"title": "The authors present a novel bound for the generalization error of 1-layer neural networks with multiple outputs and ReLU activations. ", "review": "It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. This might explain why in some cases overparametrized models have better generalization properties.\n\nThis paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better. First, the concepts of \\textit{capacity} and \\textit{impact} of a hidden unit are introduced. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.\n\n## Strengths\n- The paper is theoretically sound, the statement of the theorems\n    are clear and the authors seem knowledgeable when bounding the\n    generalization error via Rademacher complexity estimation.\n\n- The paper is readable and the notation is consistent throughout.\n\n- The experimental section is well described, provides enough empirical\n    evidence for the claims made, and the plots are readable and well\n    presented, although they are best viewed on a screen.\n\n- The appendix provides proofs for the theoretical claims in the\n    paper. However, I cannot certify that they are correct.\n\n- The problem studied is not new, but to my knowledge the\n    presented bounds are novel and the concepts of capacity and\n    impact are new. Theorem 3 improves substantially over\n    previous results.\n\n- The ideas presented in the paper might be useful for other researchers\n    that could build upon them, and attempt to extend and generalize\n    the results to different network architectures.\n\n- The authors acknowledge that there might be other reasons\n    that could also explain the better generalization properties in the\n    over-parameterized regime, and tone down their claims accordingly.\n\n## Weaknesses\n\\begin{itemize}\n- The abstract reads \"Our capacity bound correlates with the behavior\n    of test error with increasing network sizes ...\", it should\n    be pointed out that the actual bound increases with increasing\n    network size (because of a sqrt(h/m) term), and that such claim\n    holds only in practice.\n\n- In page 8 (discussion following Theorem 3) the claim\n    \"... all the previous capacity lower bounds for spectral\n        norm bounded classes of neural networks (...) correspond to\n        the Lipschitz constant of the network. Our lower bound strictly\n    improves over this ...\", is not clear. Perhaps a more concise\n    presentation of the argument is needed. In particular it is not clear\n    how a lower bound for the Rademacher complexity of F_W translates into a\n    lower bound for the rademacher complexity of l_\\gamma F_W. This makes the claim of tightness of Theorem 1 not clear. Also this makes\n    the initial claim about the tightness of Theorem 2 not clear.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}