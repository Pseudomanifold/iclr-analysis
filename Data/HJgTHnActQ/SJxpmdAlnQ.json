{"title": "Comparison and experiment setting are not explained well", "review": "This paper proposes an unpaired image-to-image translation method which applies the co-segmentation network and adaptive instance normalization techniques to enable the manipulation on the local regions.\n\nPros:\n* This paper proposes to jointly learn the local mask to make the translation focus on the foreground instead of the whole image.\n* The local mask-based highway adaptive instance normalization apply the style information to the local region correctly.\n\nCons:\n* There seems a conflict in the introduction (page 1): the authors clarify that \u201cprevious methods [1,2,3] have a drawback of ....\u201d and then clarify that \u201c[1,2,3] have taken a user-selected exemplar image as additional input ...\u201d. \n* As the main experiments are about facial attributes translation, I strongly recommend to the author to compare their work with StarGAN [4]. \n* It is mentioned in the introduction (page 2) that \u201cThis approach has something in common with those recent approaches that have attempted to leverage an attention mask in image translation\u201d. However, the differences between the proposed method with these prior works are not compared or mentioned. Some of these works also applied the mask technique or adaptive instance normalization to the image-to-image translation problem. I wonder the advantages of the proposed method compared to these works.\n* The experiment setting is not clear enough. If I understand correctly, the face images are divided into two groups based on their attributes (e.g. smile vs no smile). If so, what role does the exemplar image play here? Since the attribute information has been modeled by the network parameters, will different exemplar image lead to different translation outputs? \n* The github link for code should not provide any author information.\n\n[1] Multimodal Unsupervised Image-to-Image Translation\n[2] Diverse Image-to-Image Translation via Disentangled Representations\n[3] Exemplar Guided Unsupervised Image-to-Image Translation\n[4] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation\n\nOverall, I think the proposed method is well-designed but the comparison and experiment setting are not explained well. My initial rating is weakly reject.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}