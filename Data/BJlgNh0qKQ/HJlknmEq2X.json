{"title": "An interesting application of VAEs", "review": "[Summary]\nThis paper proposes to do semi-supervised learning , via a generative model, of an arc-factored dependency parser by using  amortized variational inference.  The parse tree is the latent variable, the parser is the encoder that maps a sentence to a distribution over parse-trees, and the decoder is a generative model that maps a parse tree to a distribution over sentences.  \n\n[Pros]\nSemi-supervised learning for dependency parsing is both important and difficult and this paper presents a novel approach using variational auto-encoders. And the semi-supervised learning method in this paper gives a small but non-zero improvement over a reasonably strong baseline. \n\n[Cons]\n1. My main concern with this paper currently are the \"explanations\" provided in the paper which are quite hand-wavy. E.g. the authors state that using a KL term in semi-supervised learning is exactly opposite to the \"low density separation assumption\".  And therefore they set the KL term to be zero. One has to wonder that why is the \"low density separation assumption\" so critical for dependency parsing only? VAEs have been used with a prior for semi-supervised learning before, why didn't this assumption affect those models ? \n\nA better explanation will have been that since the authors first trained the parser in a supervised fashion, therefore their inference network already represents a \"good\" distribution over parses, even though this distribution is specified only upto sampling but not in a mathematically closed form. Finally, setting the KL divergence between the posterior of the inference network and the prior to be zero is the same as dynamically specifying the prior to be the same as the inference network's distribution.  \n\n2. A number of important details are missing in the submitted version of the paper which the authors addressed in their reply to my public comment.\n\n3. The current paper does not contain any comparison to self-training which is a natural baseline for this work. The authors replied to my comment saying that self-training requires a number of heuristics but it's not clear to me how much more difficult can these heuristics be than the tuning required for training their VAE.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}