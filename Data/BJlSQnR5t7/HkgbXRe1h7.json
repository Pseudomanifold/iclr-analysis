{"title": "Nystroem meets deep learning - review", "review": "DEEPSTR\u00d6M NETWORKS\n\nQuality: good\nOriginality: original\nSignificance: relevant for ICLR\nPros: - interesting idea -- see detailed comments\nCons: a number of open issues in the presentation -- see detailed comments\n\nThe paper is based on some concepts to link non-linear data representations\nby means of non-linear kernel mappings, with the deep learning approach.\nThe presented concept is compared to former methods going into this direction\nand random fourier feature approaches. Additionally an adaptive scheme is\nsuggested where the internally used landmark matrix (which forms a kind\nof mahalanobis like weighting matrix to the data) is adapted. \n\nI think the main idea of the paper is interesting but the presentation\nlacks a number of details in the description/parametrization and the\nexperimental design. \n\n- 'Indeed, since the Nystr\u00f6m approximation uses an explicit feature map from the data kernel matrix, it is not restricted to a specific kernel function and not limited only to\n   RBF kernels, as in Fastfood approximation.' -- well yes an no, Nystroem works only effectively if the intrinsic dimensionality of the data is reasonable small - otherwise\n   you need a lot of disjunct landmarks. Additionally the parameterization - how many landmarks and which one is not so easy - in this sense random fourier features are easier to\n   handle. Further the random fourier features are not restricted to RBF kernel and not even to shift-invariant ones - there are a few proposals \n- The 'feature map' obtained by the Nystroem approximation is only helpful as long as the number of landmarks is reasonable small - otherwise an out of sample extension (mapping\n  new test points) in the spanned feature space may not be very reliable\n- please correct the spell errors like 'using ful training sets' (full); 'In (24), wo architectures' -- two?\n- random subsampling of the landmarks can lead to rather high approximation errors if the number of landmarks is small - there are some proposal to address this using leverage score sampling and other\n  see e.g. work of K. Zhang, Suykens, C. Musco\n- 'it is restricted to the Gaussian kernel' - well this is not true - see comments above (but I agree that there may not have been done much work to show this in practice, \n  and it is probably not possible to be used for - all - kinds of psd kernels)\n- 'Singular Value Decomposition (SVD) of K 11 . In the case where the size of the subsample L, m,is large, the computational complexity of the SVD is O(m 3 ).'\n  -- well - yes and no, with Nystroem you would expect that N is much ! larger than m - than the m^3 may still be somewhat ok. To learn the weightings on the fly in your\n  adaptive approach will address the complexity issue but may in parts degenerate performance - now it is a question what the user is more interested on\n- You mainly ignore the point how the landmarks have to be selected - random is (often) not a very reasonable choice - see respective work around this\n  In your datasets the problem may not show up because you have images only which have very strong characteristics and are intrinsically very low dimensional.\n  --> I think you have to add another section here - evaluating either different sampling schemes or just use a more state of the art one like leverage score sampling \n  (getting I think O(N^2) complexity); you should also address how you find a good guess for the number of landmarks\n- I suggest to use also additional non-image data. Basically one motivation in your approach is to go beyond the classical deep learning data representation \n  - this may not be so interesting for image recognition problems\n- I may suggest to spend a small subsection to a complexity analysis (including parameter optimization) of the various methods\n- Figure 2 - what do you mean by number of parameters? - there are a few strong fluctuations (fully connected, addative deep linear) - why\n- please increase the level of details of the accuracy axis in Figure 2 - so far it basically has 10% steps\n- if I have not missed it you did not say how the parameter \\sigma (or \\gamma) in the rbf kernel has been determined (same for other parametric kernels)\n- multiple kernels - well how did you combine the kernels (there are many options) \n- there are number of typos in the refs (see e.g. ref 14); some other references are very incomplete e.g. ref 15\n- plots should be (prefered) close to the respective text (the appendix is a bit strange here)\n  \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}