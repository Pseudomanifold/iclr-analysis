{"title": "Interesting idea, does not seem to work consistently, limited theoretical explanation", "review": "\nThis work proposes an optimization method called All Learning Rate\nAt Once (Alrao) for hyper-parameter tuning in neural networks.\nInstead of using a fixed learning rate, Alrao assigns the learning\nrate for each neuron by randomly sampling from a log-uniform\ndistribution while training neural networks. The neurons with\nproper learning rate will be well trained, which makes the whole\nnetwork eventually converge. The proposed method achieves\nperformance close to the SGD with well-tuned learning rate on the\nexperiments of image classification and text prediction.\n\n\n#Pros:\n\n-- The use of randomly sampled learning rate for deep learning\nmodels is novel and easy to implement. It can become a good\napproximation of using SGD with the optimal learning rate.\n\n-- The paper is well-written and easy to follow. The proposed\nmethod is illustrated in a clear way.\n\n-- The experiments are solid, and the performance on three\ndifferent architectures are shown for comparison. According to the\nexperiments, the proposed method is not sensitive to the\nhyper-parameter \\eta_{min} and \\eta_{max}.\n\n#Cons:\n\n-- The authors have not given any theoretical convergence analysis\non the proposed method.\n\n-- Out of all four experiments, the proposed method only\noutperforms Adam once, which does not look like strong support.\n\n-- Alrao achieves good performance with SGD, but not with Adam.\nAlso, there are no experimental results on Alrao with other\noptimization methods.\n\n#Detailed comments:\n\n(1) I understand that Alrao will be more efficient compared to\napplying SGD with different learning rate, but will it be more\nefficient compared to Adam? No clear clarification or experimental\nresults have been shown in the paper.\n\n(2) The units with proper learning rate could learn well and\nconstruct good subnetworks. I am wondering if the units with \"bad\"\n(too small or too large) learning rate might give a bad influence\non the convergence or performance of the whole network.\n\n(3) The experimental setting is not clear, such as, how the input\nnormalized, how data augmentation is used in the training phase,\nand what are the depth, width and other settings for all three\narchitectures.\n\n(4) The explanation on the influence of using random learning rate\nin the final layer is not clear to me.\n\n(5) Several small comments regarding writing:\n    (a) Is the final classifier layer denoted as $C_{\\theta^c}$ or  $C_{\\theta^{cl}}$ in the third paragraph of \"Definitions and notations\"?\n    (b) In algorithm 1, what is the stop criteria for the do while? The \"Convergence ?\" in the while condition is confusing.\n    (c) Is the learning curve in Figure 2 from one run or is it the average of all runs? Are the results consistent for each run? How about the learning curves for VGG19 and LSTM, do they have similar learning curves with the two architectures in Figure 2?\n    (d) For Figure 3, it will be easier to compare the performance on the training and test set, if the color bars for the two figures share the same range.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}