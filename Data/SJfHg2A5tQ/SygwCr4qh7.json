{"title": "Impressive results for binarized neural networks by combining existing ideas", "review": "The authors of this paper aim to reduce the constraints required by neural networks so they can be evaluated on lower-power devices. Their approach is to quantize weights, i.e. rounding weights and hidden units so they can be evaluated using bit operations. There are many challenges in this approach, namely that one cannot back-propagate through discrete weights or discrete sign functions. The authors introduce an approximation of the sign function, which they call the SignSwish, and they back-propagate through this, quantizing the weights during the forward pass. Further, they introduce a regularization term to encourage weights to be around learned scales. They evaluate on CIFAR-10 and Imagenet, surpassing most other quantization methods. \n\nThe paper is pretty clear throughout. The authors do a good job of motivating the problem and placing their approach in the context of previous work. I found Figures 1 and 2 helpful for understanding previous work and the SignSwish activation function, respectively. However, I did not get much out of Figures 3 or 4. I thought Figure 3 was unnecessary (it shows the difference between l1 and l2 regularization), and I thought the psuedo-code in Algorithm 1 was a lot clearer than Figure 4 for showing the scaling factors. Algorithm 1 helped with the clarity of the approach, although it left me with a question: In section 3.3, the authors say that they train by \"replacing the sign binarization with the SS_\\beta activation\" and that they can back-propagate through it. However, in the psuedo-code it seems like they indeed use the sign-function in the forward-pass, replacing it with the signswish in the backward pass. Which is it?\n\nThe original aspects of their approach are in introducing a new continuous approximation to the sign function and introducing learnable scales for l1 and l2 regularization. The new activation function, the SignSwish, is based off the Swish-activation from Ramachandran et al. (2018). They modify it by centering it and taking the derivative. I'm not sure I understand the intuition behind using the derivative of the Swish as the new activation. It's also unclear how much of BNN+'s success is due to the modification of the Swish function over using the original Swish activation. For this reason I would've liked to see results with just fitting the Swish. In terms of their regularization, they point out that their L2 regularization term is a generalization of the one introduced in Tang et al. (2017). The authors parameterize the regularization term by a scale that is similar to one introduced by Rastegari et al. (2016). As far as I can tell, these are the main novel contributions of the authors' approach. \n\nThis paper's main selling point isn't originality -- rather, it's that their combination of tweaks lead to state-of-the-art results. Their methods come very close to AlexNet and VGG in terms of top-1 and top-5 CIFAR10 accuracy (with the BNN+ VGG even eclipsing the full-precision VGG top-1 accuracy). When applied to ImageNet, BNN+ outperforms most of the other methods by a good margin, although there is still a lot of room between the BNN+ and full-precision accuracies. The fact that some of the architectures did not converge is a bit concerning. It's an important detail if a training method is unstable, so I would've liked to see more discussion of this instability. The authors don't compare their method to the Bi-Real Net from Liu et al. (2018) since it introduces a shortcut connection to the architecture, although the Bi-Real net is SOTA for Resnet-18 on Imagenet. Did you try implementing the shortcut connection in your architecture? \n\nSome more minor points:\n- The bolding on Table 2 is misleading. It makes it seem like BNN+ has the best top-5 accuracy for Resnet-18, although XNOR-net is in fact superior. \n- It's unclear to me why the zeros of the derivative of sign swish being at +/- 2.4beta means that when beta is larger, we get a closer approximation to the sign function. The derivative of the sign function is zero almost everywhere, so what's the connection?\n- Is the initialization of alpha a nice trick, or is it necessary for stable optimization? Experiments on the importance of alpha initialization would've been nice. \n\nPROS:\n- Results. The top-1 and top-5 accuracies for CIFAR10 and Imagenet are SOTA for binarized neural networks.\n- Importance of problem. Reducing the size of neural networks is an important direction of research in terms of machine learning applications. There is still a lot to be explored.\n- Clarity: The paper is generally clear throughout.\n\nCONS:\n-Originality. The contributions are an activation function that's a modification of the swish activation, along with parameterized l1 and l2 regularization. \n-Explanation. The authors don't provide much intuition for why the new activation function is superior to the swish (even including the swish in Figure 2 could improve this). Moreover, they mention that training is unstable without explaining more. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}