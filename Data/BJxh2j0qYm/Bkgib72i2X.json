{"title": "Review comments on \u201cDynamic Channel Pruning: Feature Boosting and Suppression\u201d", "review": "Summary: \n\nThis paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. \n\nMy detailed comments are as follows.\n\n\nStrengths:\n\n1. The motivation for this paper is reasonable and very important. \n\n2. The authors proposed a new method for dynamic channel pruning.\n\nWeaknesses:\n\n1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming?\n\n2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read.\n\n3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue.\n\n4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments.\n\n5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet?\n\n6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments.\n\n7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018)\n[1] Channel pruning for accelerating very deep neural networks. CVPR 2017.\n[2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017.\n[3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}