{"title": "Nice idea, paper could be improved", "review": "Paper summary:\n\nGenerating from autoregressive image models such as PixelCNN is slow, because each pixel must be generated sequentially. The paper proposes a method for speeding up generation from PixelCNN. The idea is to train two additional models together with PixelCNN: a fast generator and a confidence model. The fast generator predicts the next few pixels in the image (e.g. the next row) in parallel. The confidence model scores each predicted pixel with how likely it is to have been generated by the original PixelCNN. Only pixels with a low confidence score are generated by the PixelCNN, whereas the rest are taken from the fast generator.\n\nTechnical quality:\n\nThe proposed method for speeding up generation from PixelCNN is a nice idea. The proposed implementation is sensible, and the experimental evaluation is reasonable.\n\nIt's important to note that the fast generator proposed in the paper is a rather weak generative model, as it's strictly more limited than the original PixelCNN. If I understand correctly, the \"prior pixels\" z_{i..j} are a deterministic function of the previous pixels x_{<i}, and the predicted pixels x_{i..j} are conditionally independent given x_{<i} under the fast generator. This is a modelling limitation that will only be accurate locally in smooth images (such as faces). Saying that \"q will approach p if z_{i..j} approaches x_{i..j}\" (bottom of page 3 and appendix B) doesn't make sense to me; this cannot possibly happen, as z_{i..j} is a deterministic function of x_{<i} whereas x_{i..j} is a random variable given x_{<i}.\n\nIn my opinion, the experimental evaluation should report the log likelihood of samples generated from the Skim-PixelCNN under the original PixelCNN, for various values of epsilon. Such an experiment would quantify the discrepancy between Skim-PixelCNN and the original PixelCNN. A graph showing the trade-off between this discrepancy and generation speed-up would be an important result to report.\n\nThe paper says that the method has been evaluated on CIFAR-10, but the reported evaluation on CIFAR-10 is much less thorough than the one on CelebA. I think the paper should report the evaluation on both datasets equally thoroughly. Right now, the claim in the abstract that the evaluation has been done on \"diverse image datasets\" cannot be substantiated. My expectation is that the method works better on CelebA than CIFAR-10, because face images are fairly smooth.\n\nOriginality:\n\nAs far as I know, the idea and the proposed implementation are original and interesting contributions.\n\nI think the discussion of related work is limited. In my opinion, the related work section should also discuss flow-based models such as Real NVP, Glow, Masked Autoregressive Flow and Inverse Autoregressive Flow, some of which are fast to generate from. More importantly, the related work section should discuss other ways of speeding up generation from autoregressive models, such as those used in Parallel WaveNet.\n\nClarity:\n\nThe paper is well structured and organized into sections, and the main idea is clearly stated from the beginning. However, in terms of language, grammar, expression and choice of vocabulary the paper is below publication standard and needs to be improved.\n\nSignificance:\n\nI believe that the problem the paper tackles is an important one. Autoregressive models perform very well in terms of log-likelihood, but are slow to sample from which limits their applicability. On the other hand, models that are fast to sample from often don't provide exact log-likelihoods. In practice we often need both, and finding ways in which we can speed-up generation from autoregressive models is a way of achieving this.\n\nHaving said that, it's important to note that Skim-PixelCNN still samples autoregressively (e.g. one row at a time for epsilon = 1) and therefore doesn't completely solve the problem. The question is, when should one prefer Skim-PixelCNN versus e.g. distilling PixelCNN to a fast flow-based model (such as done with Parallel WaveNet)? I think the paper should include such a discussion.\n\nReview summary:\n\nPros:\n+ Nice idea, sensible implementation.\n+ Well structured, main idea clearly stated from start.\n+ Evaluation is reasonable.\n\nCons:\n- Poorly written in terms of language.\n- Evaluation incomplete, should include results on CIFAR-10 and quantify discrepancy with original PixelCNN vs generation speed-up.\n- Related work incomplete: should discuss at least flow-based models and Parallel WaveNet.\n\nMinor points:\n- Some citations are incorrect. For example, video generation cites WaveNet (first paragraph of introduction), and GANs cite adversarial examples (fist paragraph of related work).\n- Fist paragraph of introduction: the characterization of a GAN as a \"point estimator\" doesn't make sense to me.\n- \"j > i for all i, j in [1, n^2]\" doesn't make sense. Just writing \"j >i\" is enough here.\n- Beginning of section 3.3 says that the fast generator must be corrected because \"it's autoregressive and hence accumulates error\". This is not the reason; as I explained above, the reason is that the fast generator is a weak model. Accumulation of error due to autoregressive sampling affects the fast generator no more than it affects the original PixelCNN.\n- Figure 4(c), vertical axis says 10^$%.\n- In conclusions: I don't think that Skim-PixelCNN has achieved \"SOTA performance\" in any sense. It's not the fastest model to generate from (GANs, VAEs and flows are), and it's not better at modelling than the original PixelCNN.\n- Algorithm 1: indices v^(i) and k^(i) are used inconsistently. Also, removing ^(i) as it's unnecessary will improve clarity.\n- Algorithm 2, line 16: shouldn't g_V also take d[<=i] as input?", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}