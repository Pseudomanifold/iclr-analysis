{"title": "Pixel-CNN with option to generate multiple pixels in parallel; interesting idea, lacking (approximate) likelihoods, insufficient experimental validation", "review": "Summary: \n\nThe authors propose a generative image model based on Pixel-CNNs, they introduce components that allow the model to generate multiple pixels in parallel, instead of purely sequentially. \nThe model switches automatically between fully sequential mode and partially parallel mode based on a module that predicts the confidence in the parallel predicted pixel values.\nExperimental validation of the model is performed using the CelebA and CIFAR10 datasets. The authors also show qualitative examples of conditional generation for a model trained on ImageNet. \nThe proposed model achieves a speed up of about 5 to 10 times, wrt a baseline pixelCNN++ model. \n\nA \"fast generator network\", f, is used to predict a set of B pixels following pixel i in parallel at once, conditioned on the pixels 1 up to i generated so far.\nThis is done by using a conventional pixel-CNN to predict the B pixels in the set following i, conditioned in this case on the output of the fast generator rather than the sampled values of the preceding pixels in the set. \n\nOne of the main drawbacks of the proposed method is that the authors are not able to provide negative log-likelihood (NLL) scores on held-out data for non-trivial settings of the threshold parameter epsilon. As is, the authors only provide NLL for epsilon equal to 0 or 1, in which cases the model reverts to a plain pixelCNN, or to a skim-only version of the proposed model. Therefore, there is no quantitative evaluation in terms of NLL of the more interesting cases where epsilon is between 0 and 1. \nThe model lacks therefore one of the attractive properties of pixelCNNs: the tractable likelihood computation. \nAs a result, the model is trained in three parts the optimize the performance of (i) the per-pixel pixelCNN model, (ii) the skim-ahead prediction model, and (iii) the gating module that predicts which of the two generators should be used.\nIt is not completely clear how the targets for the gating module are defined, see below.\n\nThe paper misses an important reference to Reed et al., 2017. Which proposes another fast pixel-CNN model, by imposing partial pixel independence structure. Discussion and experimental comparison to this model should be added to the paper.  \nReed et al. Parallel Multiscale Autoregressive Density Estimation, ICML'17\n\nOther important references include models that combine a VAE model with a (light) pixelCNN decoder, to benefit from a fast overall generation of the main image content, and detailed AR pixelCNN model to obtain sharp detail. There is no qualitative or quantitative comparison to such models, nor discussion in the related work section. \nChen et al.  Variational Lossy Autoencoder, ICLR'17\nGuljarani et al.  PixelVAE: A Latent Variable Model for Natural Images, ICLR'17\n\nMore generally, the paper lacks any direct quantitative and qualitative comparison to the literature, except for pixelCNN++.\n\nWhat do the authors precisely mean with the statement \"..., our model achieves the state of the art performance on the generation speed of the AR models\". \nShould such a statement not involve both speed and quantitative performance, since a fast algorithm is not hard to design, a fast one with non-trivial performance however is more interesting. Given the presented experimental data, there seems to be no support for this claim.\n\nSpecific comments:\n\n- The \"approximate\" distribution q defined in section 3.2 is introduced without explaining what is approximated and why. \nThe presentation could be improved by clearly motivating this approximation and its role in the model at this point.   \nIt becomes clear upon a second reading, but at first this section is a little confusing, as it alludes to approximate distributions as used in variational methods, using similar notation (q and phi) as used for amortized inference networks.\n\n- Equation 5 provides a loss to train W and theta, but it was not clear to me why it is based on samples from the pixelCNN, which themselves depend on theta, rather than on ground-truth pixels. The dependence of these samples on theta seems to be ignored. Please clarify this point.\n\n- What is f_k precisely defined in equation 7? How does it differ, if at all, from the conditional distribution defined in equation 3?\n\n- How is the hyper parameter B, which controls the size of the skim-ahead model, set in the experiments? \nWhat happens for different settings of the buffer size B?\n\n- The results in figure 3 show a clear horizontal striping pattern in the confidence maps. \nWhere does this come from, which part of the model is reflected by this striping?\n\n- The scale of figure 4c is unclear: what do you mean with \"\u00d710$%\" ?!\n\n- The results in table 3 show a comparison to pixel-CNN++. For the proposed model, in the no-skimming setting: what is the difference with pixel-CNN++ in this case? Shouldn't it be exactly the same, and therefore no wonder that you get a similar NLL?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}