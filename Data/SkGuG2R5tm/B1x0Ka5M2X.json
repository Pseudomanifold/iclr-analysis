{"title": "Well motivated novel idea; excellent results", "review": "Pros\n----\n\n[Originality]\nThe authors propose a novel idea of learning representations that improves the performance of the subsequent fixed discretization method.\n\n[Clarity]\nThe authors clearly motivate their solution and explain the different ideas and enhancements introduced. The manuscript is fairly easy to follow. The different terms in the optimization problem are clearly explained and their individual behaviour are presented for the better understanding.\n\n[Significance]\nThe empirical results for the proposed scheme are compared against various baselines under various scenarios and the results demonstrate the significant utility of the proposed scheme.\n\nLimitations\n-----------\n\n[Clarity]\nThe training times for the catalyzer is never discussed in this manuscript (even relative to the training times of the considered baselines). Moreover, it is not clear if the inference time of the catalyzer is included in the results such as Table 1. Even if, PQ and the catalyzer+lattice might have comparable search recalls, it would be good to understand the relative search times to get similar accuracy especially since the inference time for the catalyzer (which is part of the search time) can be fairly significant.\n\n[Clarity/Significance]\nOne important point not discussed in this manuscript is the choice of the structure (architechture) of the catalyzer. Is the catalyzer architecture dependent on the data?\n  - If yes, how to find an appropriate architecture?\n  - If no, what is it about the proposed architecture that makes it sufficient for all data sets?\nIn my opinion, this is extremely important since this drives the applicability of the proposed scheme beyond the presented examples.\n\n[Minor question]\n- Is the parameter r in the rank loss same as the norm r in the lattice quantizer? This is a bit confusing.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}