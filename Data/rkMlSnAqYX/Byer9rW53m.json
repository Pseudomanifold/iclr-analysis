{"title": "Cool idea on how to mitigate biases in NLI data, but unconvincing results", "review": "In this paper, the authors attempt to mitigate annotation artifacts in natural language inference data. The authors propose an adversarial setup to discourage the model from overfitting to examples that can be solved by only considering the hypothesis. They consider two variants of the adversarial setup: 1) an independent adversarial hypothesis-only classifier trained jointly with an NLI model (\u201cdouble classifier\u201d) and 2) a single classifier that is trained adversarially with randomized premises (\u201csingle classifier\u201d). \n\nWhile the idea of this paper is appealing and well-motivated. The results are not super strong, but they are relatively consistent (at least for the double classifier method). I also liked the extensive analysis. My main concern is that I am not entirely convinced that the experimental results support the main claim made by the authors---that the proposed adversarial approach is a viable solution to mitigating annotation artifacts. \n\nIf I understand correctly, the authors make two explicit assumptions: (a) that the biases in SNLI are somewhat responsible to its success. As a result, eliminating them also leads to performance degradation both in SNLI and on other datasets with similar biases (page 6: \u201cThis is expected, as we saw relatively small gains with the adversarial models, and can be explained by SNLI and MNLI having similar biases\u201d). (b) that these biases are preventing the model from generalizing to other datasets that do not contain biases (or alternatively---contain other biases).\nAs a result, eliminating bias is desired, as it will improve generalizability, at the expense of lower in-domain results, that are somewhat inflated to begin with.\n\nThis is a nice and appealing story (though it could have been written more coherently). What I am concerned is whether the experimental gains observed actually result from mitigating biases. I can think of two alternative (though related) explanations for these gains: \nWere the pretrained SNLI baseline hyperparameters tuned on each corresponding dataset as well, or was the same model used in all cases? If the latter, this could partially explain the observed gains using their method: although the model has not seen any development example, selecting the best model based on the dev results gives it an unfair advantage over a model tuned on the original SNLI dataset.\nThe adversarial models suggested by the authors can be seen as a type of regularization. Did the authors try more traditional regularization methods such as L1/L2/dropout on the original SNLI model? Using high regularization values could have a similar effect (reduced performance on SNLI, increased performance on other datasets). Assuming you tune the regularization values on the target dataset (as you do with your methods), this is expected to lead to similar phenomena. I suspect that most gains observed in Table 1 are small enough in most cases to be obtained by such methods.\n\n\n \n\n\nOther points:\n\nOne experiment that I would like to see is whether biases are actually eliminated from SNLI. The authors could test the original SNLI model and the adversarial ones on a dataset containing the NULL string as premise (with the original hypothesis). Although the models are not trained on such data, based on the hypothesis-only results, I would expect the original model to do fairly well on it. If the authors\u2019 claim is correct, we should see significantly lower performance by the adversarial ones.\nThe results on testing the adversarial models on the SNLI test set (Table 4), as well as Figure 4 should be in the main text rather than the appendix.\n\nMinor: \n\u2014 Page 6: \u201cThe fact that the two architectures agree to a large extent on which datasets benefit from adversarial training is a validation of our basic approach.\u201d: this claim is unconvincing: the double classifier improved on 9/12 cases (75%). Selecting 5 of them at random, it is likely that 4 of them will show improvement with double.\n\n-- Typos and such:\n- Last paragraph on page 1: \"...biases.In this way...\" (missing white space)\n- Scitail is inconsistently spelled throughout the paper\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}