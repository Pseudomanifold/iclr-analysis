{"title": "A great attempt but not clear experiment and unsatisfactory impact yet", "review": "This paper proposes two adversarial learning frameworks to make a neural entailment more robust against the annotation artifacts or biases. It is clearly written and easy to understand. Especially, I am very impressed by the survey on previous works which covers a lot of recent studies on biases in NLI dataset and robust NLI models. And, this leads to a clear motivation of the work and a well-defined problem setting. \nHowever, weakness of this paper is the lack of detailed analysis in the experiment and unclear impact of it. Here are more specific reasons for that. \n\n# unclear evidence for the generalization of the model \nAuthors mention a possibility of generalization of the proposed techniques to other tasks, but without any empirical evidence. Would you please conduct a simple experiment on synthetic data which are artificially generated by a certain degree of bias you set. Then, as the bias degree increases, the proposed method needs to show linear improvements against the bias. If this is true, I would believe the generalization capability of the proposed model. Otherwise, given the ambiguous observation from the absolute accuracy improvements between the baseline and the proposed model, I can\u2019t believe it so. \n\n# lack of explanation for the relevance to active/transfer learning\nRemoving the bias in the training data seems to be relevant to active learning setting which finds informative points from the training data in general ML. Have you found any theoretical relevance of your adversarial training model with active learning? Otherwise, at least authors need to refer some recent studies. How is the adversarial training of two objectives analogous to the sampling mechanisms in active learning?\n\n# lack of further analysis of between bias and performance\nYou propose an architecture that can remove the bias of data. However, I can\u2019t find any following analysis between the degree of biases and the improvements in performance. For example, at least authors need to show how much bias each of the target NLI datasets have. What happened if the dataset has no bias? What parts of the proposed model can handle/control this? Such a lack of details in the analysis makes me difficult to believe the robustness of the model. For instance, in the last paragraph of Section 6, the authors didn\u2019t provide a clear effect of hyperparameters yet. \n\n# transfer setting is not an answer to the hypothesis\nThe most tricky part to understand is the Experiment section. The accuracy difference between the baseline and the proposed adversarial model doesn\u2019t give the answer for the question of the authors, whether the proposed architecture actually removes the (hypothesis) bias in NLI dataset. Do authors perform the same experiment with the training and testing data from the same type of dataset for each? If then, how much improvement does the model achieve? How does your transfer setting -- training only with the SNLI dataset and testing on other datasets -- provide an evidence of your model that actually removes the data bias in NLI dataset and appropriately deals with them in the testing set? I don\u2019t get quite convinced how transfer setting could be a practical solution for this problem. What is your clear conclusion here? By removing the bias in the dataset, do you actually make a better performing model than the baselines or only show that the two proposed architectures could possibly remove a few biases without a clear logic behind it? In order to better answer the original hypothesis you made, in my opinion, you need a clear dataset that does not contain any biases or some of the biases with a certain degree that you can control, and then show the correlation between the degree and the improvement (i.e., accuracy or whatever). \n\n# little or ambiguous impact of the result\nThe accuracy difference in Table 1 seems to be very minor and difficult to understand it due to lack of additional information about (1) how much bias each target dataset has, (2) a statistical test, and (3) real examples where the adversarial model can only answer. The absolute accuracy scores don\u2019t include much information to provide any scientific observation of your original hypothesis. Especially, please provide a few examples where the hypothesis biases are actually resolved by the given architecture.\n\n# some questions on the model design\nThe double/single classifier designs only penalize the hidden representation of the f_H to remove the hypothesis bias. However, as the authors mentioned, the key part of the NLI task is appropriately finding the proper mapping function (g_NLI) between two sentences. How do you guarantee that the adversarial losses (i.e. L_Adv, L_RanAdv) actually remove the hypothesis bias while preserving the original meaning representation and so not hurting the performance in g_NLI?\n\nInferSent (Conneau et al., 2017) is not the state-of-the-art (SOTA) model for the tasks. Are the accuracy difference in Table 1 similar to other SOTA baselines? I know this might be an unnecessary question because authors like to show a proof-of-concept system of the proposed architecture instead of proposing another SOTA model. But, I am just curious about how the adversarial loss could be independent of other types of complicated classifier function (i.e. g_NLI). \n\nThe proposed architecture is not actually making the original g_NLI more robust, but it (probably) hurts the performance of it, even though it resolves the hypothesis bias in some sense. So, this is not a robust model but a trade-off design at this moment. Would you provide some experimental results that show improvements on the same training/testing data for each type of dataset? \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}