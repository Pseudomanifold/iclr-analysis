{"title": "Laudable goal, but paper not good enough", "review": "Beyond Games: Bringing Exploration to Robots in Real-world\n===========================================================\n\nThis paper tackles the laudable goal of making an algorithm for efficient exploration in \"real-world\" RL.\nTo do this, they augment the \"curiosity\" algorithm of Pathak et al with a differentiable approximation to the reward prediction model.\nThey motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE.\n\n\nThere are several things to like about this paper:\n\n- The problem of making \"real-world\" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning.\n\n- The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines.\n\n- The authors clearly make an effort to survey a wide variety of recent papers in the field\n\n\n\nHowever, there are several important places where this paper falls down:\n\n- In a paper that posits a new, groundbreaking, real-world application of \"exploration\" there is remarkably little discussion of the key issues of \"efficient exploration\". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration.\n  + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds?\n  + Of course, this is not a paper designed for \"tabular MDPs\", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches?\n\n- There is very little *science* in this paper, beyond the experiments pitting \"improved algorithm\" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond \"sample-efficient exploration formulation\") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled.\n\n- A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me:\n  + \"... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ...\"\n  + \"Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient\"\nI would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not!\n\n- Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper.\n\n\nOverall, it is clear that this is an interesting area to do work in.\nThe goal of making a practical algorithm for real-world exploration tasks is exciting.\nHowever, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}