{"title": "The paper is not very novel and does not have strong results", "review": "Review for GENERALIZED LABEL PROPAGATION METHODS FOR SEMI-SUPERVISED LEARNING\nSummary:\nThe paper proposes an extension to label propagation where they replace the label matrix with a feature matrix in the label propagation objective and they replace the Laplacian with functions of the Laplacian. These new features that are the solution to this new objective are then used in supervised classifiers.\nNovelty/Significance:\nIt is not very clear what the novelty of this paper is. The paper proposes a new algorithm for semi-supervised learning, which is claimed to have better performance than current algorithms. Their proposed method is more on the lines of smoothing features using both labeled and unlabeled data so that they can train supervised learners on few labeled points. It is not exactly semi-supervised because there is no synergy between the process that uses all the data and learning the classifier.\nThe main concern about the novelty is that the proposed method seems like a slight variation on label propagation in order to get nicer features and then just using whatever classifier. Overall the amount of content in the paper feels lacking and there seems to be large amounts of review, repetitiveness, and unnecessary details.\nQuestions/Clarity:\nWhat is the intuition behind why GLP which is a \u201cmulti-stage\u201d process, works better than jointly modeling graph and feature information? Normally multi-stage methods work worse than joint models because they essentially model independently or in a greedy fashion (1 step before the next).\nIn section 4.1, the paper explains why normalization, 2 layers, and re-normalization are all important due to their effects on the eigenvalues. Is there intuition on why it is better for the eigenvalues to have the certain shaped explain in that section?\nPart of the introduction is repeated in the related works section. This section should be moved to the front as it is wasting space being repeated at the end.\nThe datasets seem to all be ones where more classical learning techniques are known to do worse than neural networks. It seems like the majority of the improvement in accuracy is due to the use of the neural network, not the feature smoothing/learning part. An obvious example of this is that the SVM with GLP features in Table 5 are worse/marginally better than ManifReg, which does semi-supervised SVMs.\nIn the experiments GCN does worse than GLP, but in the paper it is shown that GCN is a special case of GLP with the ReLU function removed from Eq. 9. Why does removing this function make it worse? Is it not the ReLU part, but that GLP uses the Laplacian and not one of the other filters? If this is the case, why could the Laplacian not be replaced with one of the other filters, which are essentially functions of the Laplacian.\nThe filters in section 5 should be summarized in a chart or something and the details left in the appendix. The filters are not new, and the details take unnecessary space in the main part of the paper. \nThe references do not have a consistent format.\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}