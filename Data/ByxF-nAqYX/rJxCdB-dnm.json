{"title": "Locally Linear Unsupervised Feature Selection", "review": "In this paper, the authors presented Locally Linear Unsupervised Feature Selection (LLUFS), where a dimensionality reduction is first performed to extract data patterns, which are used to evaluate compliance of features to the patterns, applying the idea of Locally Linear Embedding.\n\n1. This work basically assumes that the dataset is (well) clustered. This might be true for most real world dataset, but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. For example, if most data points are concentrated on a particular area not being well clustered, how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.\n\n2. For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD? Theoretical and experimental comparison should be interesting and useful.\n\n3. This paper is well-written, clearly explaining the idea mathematically. It is also good to mention limitation and future direction of this work. It is also good to cover a corner case (XOR problem) in details.\n\n4. Minor comments:\n - Bold face is recommended for vectors and matrices. For instance, 1 = [1, 1, ..., 1]^T, where we usually denote the left-hand 1 in bold-face.\n - It seems x_j is missing in Johnson-Lindenstrauss Lemma formula. As it is, \\sum_j W_{i,j} is subject to be 1, so the formula does not make sense.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}