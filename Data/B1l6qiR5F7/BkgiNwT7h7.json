{"title": "Solid contribution", "review": "The paper proposes a new RNN unit: ON-LSTM. The idea is to explicitly integrates the latent tree structure into recurrent models. Experiments are conducted to evaluate performances on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. Good results on unsupervised parsing show that the model learns something close to human judgments of the sentence parses.\n\nThe paper is clearly written, and the experiments seem planned well.\nThe language modeling results are not state-of-the-art, but the unsupervised parsing results of layer 2 are quite impressive. The analyses are reasonable.\n\nOverall, the paper seems worthy of being accepted.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}