{"title": "Model-based RL with action sequences, need more convincing regarding applicability", "review": "This paper proposes learning a transition model that takes an action sequence as an input (instead of a single action), and performing model-based planning by using the cross-entropy method.\n\nOne obvious concern with this is that this produces a sequence of open-loop plans, rather than a closed-loop policies, with all the inherent limitations. I could see this working well in practice in problems where anticipating how future decisions will react to state changes is not that important, however the authors should discuss the trade-offs more.\n\nA larger concern for me revolves around learning the transition model. Taking the action sequence as an input (which is one of the main novelties in the paper) is likely to require a lot of data, and maybe this is fine on relatively simple Mujoco tasks but I see it as a potential issue when trying to expand this to more realistic problems.\n\nFinally, I suggest that the authors change the title to something more descriptive of the paper\u2019s contents, as there is no analysis of asymptotic performance in the paper (as I would have thought from the title). I also recommend that they look to see if there is any model-based work in the semi-MDP literature, which could be relevant here.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}