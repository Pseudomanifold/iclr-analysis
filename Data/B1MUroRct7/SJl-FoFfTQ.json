{"title": "Rebadging of Incremental Generalized Eigenvalue Decomposition.", "review": "This paper studies sufficient dimension reduction problem, and proposes an incremental sliced inverse regression algorithm. Numerical experiments are provided to demonstrate the effectiveness of the proposed algorithms.\n\nThe sliced inverse regression here is nothing but generalized eigenvalue decomposition:\n\nAx=lambda Bx.\n\nNote that Multiclass Fisher Linear Discriminant Analysis, Canonical Correlation Analysis, Nonlinear Manifold Embedding and many subspace learning methods can also be formulated as generalized eigenvalue decomposition. All these methods need to compute covariance-like matrices in the additive form, which makes incremental update very convenient.\n\nThe incremental generalized eigenvalue decomposition has been extensively studied for over decades, especially between 1995 and 2005 in the face recognition community. I am just listing a few here:\n\nYe et al., IDR/QR: An Incremental Dimension Reduction Algorithm via QR Decomposition, 2005\n\nLaw and Jain, Incremental Nonlinear Dimensionality Reduction by Manifold Learning, 2006\n\nYan et al. Towards incremental and large scale face recognition, 2011\n\nGhassabeh et al. A New Incremental Face Recognition System, 2007\n\nSong et al. A Novel Supervised Dimensionality Reduction Algorithm for Online Image Recognition, 2006.\n\nWang et al. Incremental two-dimensional linear discriminant analysis with applications to face recognition, 2010.\n\nSalman et al. Efficient update of the covariance matrix inverse in iterated linear discriminant analysis, 2010\n\nPark and Park, A comparison of generalized linear discriminant analysis algorithms, 2008\n\nWang, INCREMENTAL AND REGULARIZED LINEAR DISCRIMINANT ANALYSIS, 2012\n\nThese algorithms become less popular/known now, because (1) they are not scalable and efficient for large p, and (2) these classical dimensionality reduction methods perform poorly in many tasks, compared with the state of art results.\n\nThis paper only cites a few papers on incremental LDA,  but does not even mention that both LDA and SIR are essentially solving similar optimization problems. Moreover, it does not compare the results with any of the above references, either.\n\nThis paper even claims applying the Sherman\u2013Morrison formula as the contribution. However, such an  update has been used in Salman et al. 2010, Park and Park 2008, Wang 2012.\n\nIn summary, this paper is far below the bar of ICLR.\n\nMinor: There are numerous typos in this paper. The authors even misspell \"Morrison\" in the Sherman\u2013Morrison formula as \"Morison\".", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}