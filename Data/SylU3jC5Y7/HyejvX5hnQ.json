{"title": "Confusion about inference", "review": "The authors propose a dropout method that uses the beta-Bernoulli process to learn the sparsity rate for each node. \n\nThe model itself make sense to me, though I don't have an understanding of why learning a node-specific sparsity rate should improve dropout -- i.e., what is there to learn? From what I understand about dropout, it's a stochastic method that has the same marginal as the original model, but because of the randomness induced it avoids bad local optimal solutions. Thus it's a learning trick, not a modeling technique. This treats dropout as something to directly model.\n\nMy confusion is mainly about inference. While there are many approximations introduced to make it work, if the sparsity z is something to be learned then why is it only being sampled from the beta prior in (15)? There is a likelihood term that incorporates z as well and it seems like this should be included as well to be strictly correct from a modeling standpoint. I didn't see any explanation in the discussion.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}