{"title": "Graph-regularized NNs", "review": "This paper proposes the interesting addition of a graph-based regularisers, in NNs architectures, for improving their robustness to different perturbations or noise. The regularisation enforces smoothness on a graph built on the different features at different layers of the NN system. The proposed ideas are quite interesting, and integrates nicely into NN architectures. \n\nA few paths for improvements:\n\n- the 'optimal' choice of the power of the Laplacian, in 3.5, is eluded\n- the figures are not presented ideally, nor in a very readable form - for example, their are 90-degree rotated compared to classical presentations, and the plots are hardly readable\n- the might exist a tradeoff between robustness, and performance (accuracy), that seem to be explaining the proposed results (see Fawzi - Machine Learning 2018, for example)\n- in 4.2, what is a mean case of adversarial noise? Also, it would be good to see the effect of the regularizer of both the 'original' network, and on the network trained with data augmentation. It is not clear which one is considered here, but it would be interesting to study both, actually. \n- the second paragraph of the conclusion (transfer of perturbations) opens interesting perspective, but the problem might not be as trivial as the authors seem to hint in the text. \n\nOverall, very interesting and nice work, which might be better positioned (especially in terms of experiments) wrt to other recent methods that propose to improve robustness in NNs.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}