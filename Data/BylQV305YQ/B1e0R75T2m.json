{"title": "The paper addresses asynchronous optimization with a focus on staleness effect. A strong hypothesis is made on the path followed by the optimization walk and concerns should be raised with the hyperparameters in the empirical validation.", "review": "The papers addresses the important issue with asynchronous SGD: stale gradients.\n\nConvergence is proven under an assumption on the path followed by the optimization walk. Namely, gradient are assumed to be all pointing to the close directions along the walk. My major concern is that this is a strong (if not completely wrong) hypothesis in the practical case of deep learning, with high dimensional models and totally non-convex loss functions (see e.g. \nChoromanska et al. 2014).\n\nThe paper illustrates empirically the convergence claims, but only under fixed hyper-parameters, which completely illustrates the recent concerns about the reproducibility crisis in ML.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}