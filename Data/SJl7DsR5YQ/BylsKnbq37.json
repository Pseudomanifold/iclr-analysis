{"title": "Interesting topic, but poorly communicated and lacking novelty", "review": "Despite many high profile successes in research, DeepRL is still not widely used in applications. One reason for this is that current methods typically assume the agent can learn by exploring many states and actions, however, in many real world tasks, such as driving used here, poor actions can be dangerous. Therefore, methods that can provides the flexible benefits of RL while avoiding this are of significant interest, one promising general ideas pursued for this has been to use human demonstrations.\n\nA number of approaches to Inverse RL have been studied, but many make the assumption that the demonstrations are all examples of optimal behavior. This can be challenging if, for example, some examples contain suboptimal behavior, and it also means that the agent does not get to observe non-optimal trajectories and how to correct for them; the resulting policy often performs poorly due to the distributional shift between the demonstration trajectories and the trajectories induced by the learned policy.\n\nThis work attempts to correct for these problems by labeling the demonstration actions between $[-1, 1]$ indicating how good or bad the demonstration actions are. This introduces a challenge for learning, since good actions can be copied, but a bad action is more ambiguous: it does not necessarily imply the action are far away from the bad action is a good action.\n\nOne view of this work is that they introducing 3 losses for behavior cloning with weighted labels: A weighted (based on the label) L2 loss, an exponential loss and directly fitting the loss and searching over a discrete set of actions to find the highest estimate weighting. Note the current equation for $Loss_{FNET}$ doesn't make sense because it simply minimizing the output of the network, from the text it should be something like $(f - \\hat{\\theta})^2$?\n\nThe text discusses why rescaling the negative examples may be beneficial, but as far as I can tell, figure 4 you only consider $\\alpha=\\{0, 1\\}$? Based on the text, why weren't intermediate values of $\\alpha$ considered?\n\nIt could benefit from copy-editing, checking the equations and in some cases describing concepts more concisely using clear mathematical notation instead of wordy descriptions that are difficult to follow.\n\n``Thus the assumption\nthat our training data is independent and identically distributed (i.i.d.) from the agent\u2019s encountered\ndistribution goes out the window'' This is a misleading statement regarding the challenge of distributional shift in off-policy RL. The challenge is that state distribution between the behavior policy and the learned policy may be quite different, not just not iid. Even in on-policy RL the state visitation is certainly not usually iid.\n\n``In the off-policy policy gradient RL framework, this issue is typically circumvented by changing the\nobjective function from an expectation of the learned policy\u2019s value function over the learned policy\nstate-visitation distribution to an expectation of the learned policy\u2019s value function over the behavior\n(exploratory) state-visitation distribution (Degris et al., 2012). In the RL framework, this could be\ndealt with by an approximation off-policy stochastic policy gradient that scales the stochastic policy\ngradient by an importance sampling ratio (Silver et al., 2014) (Degris et al., 2012). ''. The importance sampling in Degris is not to correct for the objective being under the behavior state policy and DPG (Silver et al., 2014) specifically does not require importance sampling so it shouldn't be referenced here. This paragraph seems to be conflating two issues: the distributional shift between the behavior state distribution and the policy state distribution that can make off-policy learning unstable, and importance sampling to estimate outcome likelihoods using behavior experience.\n\nThis work is on a very important topic. However, in its current form it is not well-communicated. Additionally, the best performing method is not novel (as the author's state $\\alpha=1$, the best performing setting, is essentially the same as COACH but with scalar labels). For these reasons reason, I think this work may be of limited interested.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}