{"title": "This paper addresses the problem of applying reinforcement learning in cases where exploration is too dangerous. The authors presented an algorithm that collects driver data and solicits human feedback during operations, hence the name \"Backseat driver.\" They demonstrate benefit in collecting both negative examples (examples of bad driving) and positive examples.", "review": "This paper is clearly written and identifies an important point that exploration is dangerous in the autonomous driving domain. My key objection to this paper is that, even though the method is intended to deal with problems where exploration is dangerous and therefore should not be done, the method relies on negative examples, which are presumably dangerous. If simulations are used to generate negative examples and those are used, then the benefit of the presented method over standard reinforcement learning goes away.\n\nI have several questions/comments/suggestions about the paper:\n\n1. Can one perhaps present only mildly bad examples (e.g., mild swerving) to reinforcement learning in a way where the algorithm can understand that significant swerving, like what is shown in figure 2, is even worse?\n2. The backseat driver feedback described seems to granular. I think that, to be realistic, the algorithm should allow for feedback that is less precise (e.g., turn further, turn the other way), without requiring information on proportions.\n3. Please add an architecture diagram.\n4. In figure 4, what is the difference between the first and fourth items? They have exactly the same description in the legend.\n5. The experiments are not convincing. They lead one to conclude that negative examples are beneficial, which is good, but not surprising. Because negative examples are generated, a comparison with regular reinforcement learning should be done.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}