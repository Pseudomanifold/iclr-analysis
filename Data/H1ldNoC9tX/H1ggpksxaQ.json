{"title": "Paper correct and carefully written but results rather straightforward", "review": "The authors \frst present standard binary (positive negative or PN) classi\fca-\ntion, followed by positive unlabeled (PU) classi\fcation, that they motivate with\nexamples, such as one-class remote sensing classi\fcation. The new setting that\nthey introduce and study is called positive unlabeled biaised negative (PUbN\nclassi\fcation) and adds a biaised negative sample to PU learning. They give\nmotivating examples and compare this setting to the existing literature. A con-\nvincing case is made regarding the di\u000berence between the PUbN problem and\nthe known problems of semi-supervised learning and dataset shift.\nThey start by recalling the notations and nature of standard binary classi\f-\ncation, PU classi\fcation and the nnPU (non-negative PU) strategy, as in the\nprevious PU learning papers. Then, they present the semi-supervised setting\nunder the name PNU learning, which simply studies the minimization of a con-\nvex combination of the PN risk and the PU risk. As in PU learning, a correction\nexists to avoid considering the estimate of the negative risk to be negative, re-\nferred to as nnPNU.\nFinally, the authors introduce PUbN learning as the problem in which we\nonly have access to negatives that follow the law p(x\\mid y = -1; s = +1), where s\nis a latent variable that formalizes the bias.\nAs in PU learning, the authors derive an unbiased estimator of the risk that\ninvolves only distributions for which data is available. However, they need\nto reweight the P and bN distribution by the unknown posterior probability\n\u001bsigma(x) = p(s = +1\\mid x) of s. Considering s as the label, the problem of learning\na probabilistic classi\fer separating the elements for which s = +1 and s = \udbc0\udc001\ncan be seen as a PU learning problem, which gives an estimator ^\u001b of sigma,\nand makes the method practical.\nThey derive estimation error bounds, that depend on the mean squared\ndi\u000bfference between \u001bsigma and sigma^\u001b and a term of order n^-1/2 where n is the cardinal\nof the smallest sample. They considered the function ^\u001b as a \fxed function in\ntheir bounds, which implies that the bounds are only true if some of the data is\nkept for the estimation of sigma^\u001b. Finally, they present a variant of their algorithm\nfor PU learning, named PUbNnN where unlabeled instances are not all given\nthe same weight, but weighted according to sigma hat\u001b. The experiments use neural networks with stochastic optimization, on the classic datasets MNIST, CIFAR-10 and 20 Newsgroup. They report better per-\nformance using their technique on all datasets. The authors documented their\nexperiences thoroughly in the appendix. However, I did not \ffind information\nabout the nature of the estimator of the posterior probability sigma^\u001b, which is im-\nportant for reproducibility. Furthermore, in appendix B, choosing sigma^\u001b = 0 will\nminimize the criterion . Finally, they proceed to justify the dominance of\nthe variant of their method over usual nnPU learning.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}