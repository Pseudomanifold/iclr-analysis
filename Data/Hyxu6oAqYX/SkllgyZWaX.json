{"title": "Need more theoretical guarantee", "review": "This paper addresses Type III label noise correction problem in which the labeling noise depends on the features. They assume that we can obtain a small amount of cleanly labeled data, and use an energy-based semi-supervised learning approach to bootstrap the relabeling process. \n\nPros:\n- Problem is well-motivated with a reasonably good overview of this research area. \n- Paper is generally well-written with enough details to follow and good experimental result discussion.\n\nCons:\n- The energy-based approach based on contrastive divergence is pretty straightforwardly defined, but it will make the paper much stronger if the authors can have more analysis on this approach and/or provide theoretical guarantee on generalization. \n- It is not obvious to me how to extend the proposed approach to multi-class problems. \n- It will be beneficial to test the approach on more real-world problems on top of the toy-data-alike binary classification problems. \n\nMinor clarification questions: \n- What amount of cleanly labeled data is sufficiently required for the proposed approach to work? The authors have some pre-selected percentage in experiments but it is non-trivial to establish that for different applications.\n- Related to the previous comment, how much clean data were used in AE (known) columns in all experiments? \n- In Fig 2 between the two subgraphs, why is the left one showing positive thetas while the right one showing negative thetas?\n- Were the hyperparameters a & b chosen from cross-validation or from std of E terms in all experiment results? \n- In Table 1, for Breast Cancer dataset, how can AE (known) be better than the upper-bound LR-C? \n- It would be good to vary the noise parameters and show how robust the proposed approach is in dealing with different levels of noise. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}