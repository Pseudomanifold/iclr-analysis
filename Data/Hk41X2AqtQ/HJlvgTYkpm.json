{"title": "Nice experiments, but limited novelty", "review": "This paper proposed a hierarchical generative model for generating long text. The authors use a hierarchical LSTM decoder to first generate sentence-level representations; then based on the representation of each sentence, a word-level LSTM decoder is utilized to generate a sequence of words in this sentence. In addition, they use multiple layers of latent variables to address the posterior collapse issue.\nThe paper studies an important problem and the authors performed extensive experiments.\n\n\nMy major concern is about the novelty of this paper.\nHierarchical LSTM for generating long txt has been widely studied. For example, in the following works:\nLi, Jiwei, Minh-Thang Luong, and Dan Jurafsky. \"A hierarchical neural autoencoder for paragraphs and documents.\" arXiv preprint arXiv:1506.01057 (2015).\nHierarchical LSTM for Sign Language Translation, AAAI, 2018.\n\nPlacing hierarchical latent variables in VAE  is also investigated before.\nFor example, in \nZhao, Shengjia, Jiaming Song, and Stefano Ermon. \"Infovae: Information maximizing variational autoencoders.\" arXiv preprint arXiv:1706.02262 (2017).  With some adaption from image domain to text domain\nSerban, Iulian Vlad, et al. \"A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues.\" AAAI. 2017.\n\nThe author combines this two ideas together, which is incremental in terms of novelty.\n\n\nIn the writing of section 3.2, the authors should clearly cite the previous works on hierarchical LSTM and acknowledge that this is not the contributions of this paper. Under the current writing, for unfamiliarized readers, it sounds like this is proposed by the authors of this paper, which is not the case.\n\nThe notations of this paper is confusing, which hinders its readbility.\nFor example, in equation 5, the distribution is parameterized by theta.\nIn equation 6, p(x|z) is also parametrized by theta.\n\nIn the experiments, I'd like to see a comparison with the following works.\nI suggest the authors to compare with the following works.\n\nFan, Angela, Mike Lewis, and Yann Dauphin. \"Hierarchical Neural Story Generation.\" ACL (2018).\n\nGhosh, S., Vinyals, O., Strope, B., Roy, S., Dean, T., & Heck, L. (2016). Contextual LSTM: A Step towards Hierarchical Language Modeling.\n\nZhao, Shengjia, Jiaming Song, and Stefano Ermon. \"Infovae: Information maximizing variational autoencoders.\" arXiv preprint arXiv:1706.02262 (2017).  With some adaption from image domain to text domain", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}