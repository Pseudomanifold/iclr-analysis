{"title": "Making CNNs translation equivariant again, potentially important line of work with multiple loose ends", "review": "\nSummary\n\nFrom a theoretical point of view, one might be tempted to believe that deep CNNs are translation equivariant and their predictions are translation invariant. In practice, this is not necessarily true. The authors propose to augment standard deep CNNs with low-pass filters to reduce this problem. The results seem promising for an older VGG architecture.\n\nQuality\n\nThe paper is very verbose, the figures and captions are tedious to read, the mathematical notation seems strange as well, making the writing more concise is highly encouraged. The main ideas are easy to follow and the choice of experiments seems fine. \n\nSignificance\n\nThis is the first empirical work trying to fix the issue of non-translation equivariance in convolutional neural networks. The conclusions of this work are potentially relevant for a wide audience of CNN practitioners.\n\nMain Concerns\n\nTo show that all claims of the paper do indeed hold, the authors should attack their augmented network with the translation attack of [1]. As robustness to this type of transformations is one of the main goals, it should be tested if it was achieved. The attack can be found in some open source frameworks [2] and should be easy to apply.\n\nWall-clock times need to be reported for the various blurring kernels and compared to the baselines.\n\nExtend results to a cutting-edge architecture, e.g. DenseNets or Wide ResNets. If this result is not provided the significance of the work is not clear.\n\nDespite being more expensive, do dilations fix the issue of missing translation equivariance provably and not just approximately like the low-pass filtering approach proposed here? This should be discussed and a comparison in terms of wall-clock time would be great as well.\n\nMinor\n\n- Strange notation e.g. in equation 1. Why not write: x+\\delta x in the argument of the function instead of \"Shift\". The current notation seems unnecessarily informal.\n- Figure 4: show scale and color bar.\n\n[1] Engstrom et al., \"A rotation and a translation suffice: Fooling cnns with simple transformations.\"\n[2] https://foolbox.readthedocs.io/en/latest/modules/attacks/decision.html#foolbox.attacks.SpatialAttack", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}