{"title": "Highly thought provoking!", "review": "==== Summary ====\n\nIt is widely known that large neural networks can typically be compressed into smaller networks that perform as well as the original network while directly training small networks can be complicated. This paper proposes a conjecture to explain this phenomenon that the authors call \u201cThe Lottery Ticket Hypothesis\u201d:  large networks that can be trained successfully contain at initialization time small sub-networks \u2014 which are defined by both connectivity and the initial weights that the authors call \u201cwinning tickets\u201d \u2014 that if trained separately for similar number of iterations could reach the same performance as the large network. The paper follows by proposing a method to find these winning tickets by pruning methods, which are typically used for compressing networks, and then proceed to test this hypothesis on several architectures and tasks. The paper also conjectures that the reason large networks are more straightforward to train is that when randomly initialized large networks have more combinations for subnetworks which makes have a winning ticket more likely.\n\n==== Detailed Review ====\n\nI have found the hypothesis that the paper puts forth to be very appealing, as it articulates the essence of many ideas that have been floating around for quite a while.  For example, the notion that having a large network makes it more probable for some of the initialized weights to be in the \u201cright\u201d direction for the beginning of the training, as mentioned in [1] that was cited in this submission. Given our lack of understanding of the optimization and generalization properties of neural networks, as well as how these two interact, then any insight into this process, like this paper suggests, could have a significant impact on both theory and practice. To that effect, I generally found the experiments in support of the hypothesis to be pretty convincing, or at the very least that there is some truth to it. Most importantly, the hypothesis and experiments presented in this paper gave me a new perspective on both the generalization and optimization problem, which as a theoretician gave me new ideas on how to approach analyzing them rigorously \u2014 and that is why I strongly vote for the acceptance of this paper.\n\nThough I have very much enjoyed reading this submission, which for the most part is very well written, it does have some issues:\n\n1. Though this is an empirical paper about an observed phenomenon, it should contain a bit more background and discussion on the theoretical implications of its subject. For example, see [2] which is also an empirical work about a theoretical hypothesis, but still includes the right theoretical context that helps the reader judge the meaning of their results. The same should be done here. For instance, there is a growing interest in the link between compression and generalization that is relevant to this work [3,4], and the effect of winning ticket leading to better generalization could be explained via other works which link structure to inductive bias [5,6].\n2. The lottery ticket hypothesis is described in the paper as being both about optimization (faster \u201cconvergence\u201d) and about generalization (better \u201cgeneralization accuracy\u201d). However, there is a slight issue with how these terms are treated in the paper. First, \u201cconvergence\u201d is defined as the point at which the test accuracy reaches to a minimum and before it begins to rise again, but it does not mean (and most likely not) that it is the point at which the optimization algorithm converged to its minimum \u2014 it is better to write that early stopping regularization was used in this case. Second, the convergence point is chosen according to the test set which is bad methodology, because the test set cannot be used for choosing the final model (only the training and validation sets). Third, the training accuracies are not reported in the paper, and without them, it is difficult to judge if a given model fails to generalize is simply fails to converge to 100% accuracy on the training set. As a minor note, \u201cgeneralization accuracy\u201d as a term is not that common and might be a bit confusing, so it is better to write \u201ctest accuracy\u201d.\n\nTo conclude, even though I urge the authors to address the above issues, which could significantly improve its quality and clarity, I think that this article thought-provoking and highly deserving of being accepted to ICLR.\n\n[1] Bengio et al. Convex neural networks. NIPS 2006.\n[2] Zhang et al. Understanding deep learning requires rethinking generalization. ICLR 2017.\n[3] Arora et al. Stronger generalization bounds for deep nets via a compression approach. ICML 2018.\n[4] Zhou et al. Compressibility and Generalization in Large-Scale Deep Learning. Arxiv preprint 2018.\n[5] Cohen et al. Inductive Bias of Deep Convolutional Networks through Pooling Geometry. ICLR 2017.\n[6] Levine et al. Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design. ICLR 2018. \n\n==== Updated Review Following Rebuttal ====\n\nThe authors have addressed all of the concerns that I have mentioned above, and so I have updated my score accordingly. The additional background on related works, as well as the additional experiments in response to the other reviews will help readers appreciate the observations that are raised by the authors. The new revision is a very strong submission, and I highly recommend accepting it to ICLR. ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}