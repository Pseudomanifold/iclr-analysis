{"title": "Review", "review": "In this paper, the authors propose a SOSELETO (source selection for target optimization) framework to transfer learning and training with noisy labels. The intuition is some source instances are more informative than the others. Specifically, source instances are weighted and the weights are learned in a bilevel optimization scheme. Experimental studies on both training with noisy label problems and transfer learning problems demonstrate the effectiveness of the proposed SOSELETO.\n\nOverall, this paper is well-written, and easy to follow. The intuition is clear and reasonable, although it is not new. Regarding the technical section, I have the following comments:\n(1)\tThe paper assumes that the source and target domains share the same feature representation parameters \\theta. This is a widely used assumption in the existing works. However, these works usually have a specific part to align two domains to support the assumption, e.g. adversarial loss or MMD. In objective of SOSELETO, I do not see such a domain alignment part. I am wondering whether the assumption is still valid in this case. From the experimental study, I find SOSELETO achieves very good results in transfer learning problems. I am wondering whether the performance would be further improved if a domain alignment objective is added in the weighted source loss.\n(2)\tEach source has a weight, and thus there are n^s \\alpha. As mini-batch is used in the training, I am wondering whether batches are overlapping or not. If overlapping, how to decide the final \\alpha_i for x^s_i as you may obtain several \\alpha_i in batches. \n(3)\tAnother point is abouth \\lambda_p. In the contents, you omit the last term Q \\alpha_m \\lambda_p in eq.(4) as you use the fact that it is very small. I am not convincing on this omission as \\lambda_p is also a weight for the entire derivative. Moreover, if \\lambda_p is very small, the convergence would be very slow. In the experimental studies, you use different \\lambda_p for different problems. Then, what\u2019s the rule of setting \\lambda_p given a new problem?\n\nRegarding the experimental results, the experimental settings for the section 4.2 are not very clear to me. You may need to clearly state the train and test set (e.g. data size) for each method.  \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}