{"title": "Interesting idea, weak experiments", "review": "This paper proposes a new method to generate parallel sentences combining NMT and GANs. While I think that the paper contains many interesting ideas, it lacks a good motivation and, more importantly, I find the experimental design (as well as some results) to be very weak. Please find my comments below:\n\n- The paper is not properly motivated. Why would one want to generate parallel sentences? I do not mean that the problem is uninteresting, I am just saying that the motivation is not there. My initial expectation (before I read the paper) was that the NMT part would benefit (or at least be influenced by) the text generation part, but that is not the case, as the NMT model is trained offline before the generator.\n\n- You do not compare your results to any other system or baseline in your experiments. A simple baseline would be to generate sentences monolingually, and translate them using machine translation (either supervised or unsupervised). I think that some baseline like this is necessary.\n\n- The translation BLEU scores are very weak. 8 BLEU points in fr-en Europarl seems way too low to take these results seriously.\n\n- Given that the (unsupervised) machine translation part is completely independent from the text generation unit, its evaluation seems of little relevance, as there is nothing new on it.\n\n- The training set seems very small for natural language generation (200k sentences for Europarl and 12-29k for Multi30k), so I am not sure about how meaningful the reported results are.\n\n- I do not understand what the \"generation BLEU\" is, but evaluating a text generator with BLEU does not seem to make much sense, as there is no reference to compare to.\n\n- I am not sure if I understand how perplexity is used in your evaluation. Do you train a separate language model and use it to measure the perplexity of your generated text? If so, this seems unusual and problematic to me. Do you have any reference of anybody else doing this?", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}