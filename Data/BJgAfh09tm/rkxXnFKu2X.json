{"title": "Experiments not convincing; not well-motivated", "review": "[Summary]\nThis paper proposed an adversarial latent space based architecture capable of generating parallel sentences in two languages concurrently and translating bidirectionally. More specifically, there are two units in the framework: (1) A translation unit, which consists of two NMT models, that can translate sentences from one domain to another. The two NMT models are trained in a similar way to (Lample et al 2018a). The outputs from the two encoders of NMT models are regularized by a GAN. (2) A text generation unit, that can map a random noise into a code $c$. $c$ can be decoded into two different languages with the decoders of the NMT models. Experiments on on Europarl and Multi30k datasets are carried out to verify the proposed algorithm.\n\n[Details]\n1.\tWhat is the motivation of this work? I am not fully convinced that building \u201can adversarial latent space based architecture capable of generating parallel sentences in two languages concurrently and translating bidirectionally\u201d is a well-motivated topic, because in this system, (1) Do the NMT models have better performance due to the joint training? I found that leaving out the GAN loss can lead to better performance. (See Table 1) (2) Why do I need such a generator? Can it generate sentences with specific semantic? Hope the authors can give stronger motivations for this work.\n2.\tFor NMT experiments, why don\u2019t you work on WMT En->Fr (36M data after filtration) or En->De (4.5M data) tasks, which are more popular. It is strange to see that: (1) In Section 4.1, 2nd paragraph, \u201cwe removed sentences longer than 20 words\u201d. That is, you only work on extremely short sentences, which is not a common practice. You should also use the official test sets, i.e., newstest series to evaluate your models. (2) For supervised setting, why don\u2019t you use Transformer [ref1], a much strong baseline in NMT? Even if for using LSTM, a deep model like GNMT [ref2]? For unsupervised settings, why not follow (Lample et al 2018), which is the state-of-the-art unsupervised NMT model? (3) In Table 1, the rows with \u201cNoAdv\u201d have higher BLEU scores. Then, what is the role of the GAN in your NMT system?\n3.\tI am very confused by the evaluation of text generation. (1) Can you give a detailed explanation how the sentences are generated? (2) After generating a pair of sentences, how to choose the references for \u201cGeneration BLEU\u201d? (3) Even if for translation BLEU, given the input is a synthesis sentence instead of a natural sentence, how can Google Translation give a correct \u201creference sentence\u201d? \n\n[Typos]\n(1) Section 4.3, first line: results=> results \n(2) reference (Lample et al 2018a) and (Lample et al 2018b) are the same one. Please correct them.\n\n[ref1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n[ref2] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}