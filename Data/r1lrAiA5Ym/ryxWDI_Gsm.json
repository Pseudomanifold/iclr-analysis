{"title": "Interesting extension of differentiable plasticity with evaluation that falls too short ", "review": "The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule. In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces. Experiments are performaed on 2 simple toy datasets and a simple language modeling task. A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this. Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.\n\nOverall I like the motivation, provided background information and simplicity of the approach. Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation. However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short. Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training. Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.\n\n\nStrengths:\n- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step\n- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation\n- maze navigation shows incremental benefits over non-modulated plasticity\n- thorough experimentation\n- clipping-trick is a neat observation \n\n\nWeaknesses:\n- evaluation: only on toy tasks (which includes PTB), no real world tasks\n- very incremental improvements on PTB over a very simple baseline (far from SotA)\n- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures\n- no qualitative analysis on how modulation is actually use by the systems. E.g., when is modulation strong and when is it not used \n\n\nComments:\n- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements. Furthermore PTB is not a \"challenging\" LM benchmark.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}