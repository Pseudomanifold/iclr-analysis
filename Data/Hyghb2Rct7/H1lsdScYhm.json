{"title": "Straightforward extension of learning-from-demonstration approaches to exploit recurrent neural network", "review": "The paper puts forward the idea of using a recurrent neural network in algorithms for learning from demonstration in order to take into account sequential information. The authors test it in the inverse reinforcement learning setting and the behavioral cloning setting on different control problems.\n\nI feel the basic idea is really straightforward. Although some promising results are obtained in the experimental setting, I believe the contribution may not be sufficient for a publication at ICLR. Moreover, there are some issues in the writing, e.g., \n\n- classically, as far as I know, RL is not considered to be a metaheuristic, although I understand that someone could make the case for it.\n\n- although there\u2019s not really a consensus on terminology, I think using imitation learning to define the whole class of problems encompassing IRL and behavioral cloning is not the best. Generally, imitation learning is equated to behavioral cloning. I think a better term for this general class is learning from demonstration. For instance, there are some IRL approaches that don\u2019t try to mimic a demonstrated policy, but aim at learning an even better policy.\n\n- the issue described in the paper about the missing sequential information is due to the fact the authors consider POMDPs and not MDPs. This should be made clearer. I think the authors should also cite the following paper:\n\n@article{ChoiKim11,\n\tAuthor = {Jaedeug Choi and Kee-Eung Kim},\n\tJournal = {JMLR},\n\tPages = {691--730},\n\tTitle = {Inverse Reinforcement Learning in Partially Observable Environments},\n\tVolume = {12},\n\tYear = {2011}}\n\n- the related work has to be reworked. Kuderer et al. (2013) is not about urban route planning, but deals with learning driving style; Mnih et al. (2015) is not about training multi-agent systems, but introduces DQN; Silver et al. (2016) is about go, not chess. Are TRPO or PPO really off-policy or asynchronous?\n\n- the last section of Sec.3.4 sounds strange. It\u2019s not MC that assumes that the impact of an action decays with time. The discount factor comes from the choice of the total discounted reward criterion.\n\nOther comments:\n\n- in abstract: BL -> BC\n- notations issues in (2-5)\n- l.6-7, Algo 1: t = T_m?\n- The text should be checked for typos.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}