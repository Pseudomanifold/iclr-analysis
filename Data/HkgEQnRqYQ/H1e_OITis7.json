{"title": "Is it the RotatE scoring function or the adversarial sampling?", "review": "# Summary\nThis paper presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base, whereas previous methods were only able to support a subset. The method achieves state of the art on FB15k-237, WN18RR and Countries benchmark knowledge bases. I think this will be interesting to the ICLR community. I particularly enjoyed the analysis of existing methods regarding the expressiveness of relational patterns mentioned above.\n\n# Strengths\n- Improvements over prior neural link prediction methods\n- Clearly written paper\n- Interesting analysis of existing neural link prediction methods\n\n# Weaknesses\n- As the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, I believe a more careful ablation study should have been carried out. There is an ablation study showing the impact of the negative sampling on the baseline TransE, as well as another ablation in the appendix demonstrating the impact of negative sampling on TransE and the proposed method, RotatE, for the FB15k-237. However, from Table 10 in the appendix, one can see that the two competing methods, TransE and RotatE, in fact, perform fairly similarly once both use adversarial sampling it still remains unclear whether the gains observed in table 4 and 5 are due to adversarial sampling or a better scoring function. Particularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach. Ideally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results (as it has been done for Countries in Table 6).\n\n# Minor Comments\n- Eq 5: Already introduce gamma (the fixed margin) here.\n- While I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention as some of them can also model symmetry, anti-symmetry, inversion and composition patterns of relations as well (though they might be less scalable and therefore of less practical relevance), e.g. the following come to mind:\n  - Lao et al. (2011). Random walk inference and learning in a large scale knowledge base.\n  - Neelakantan et al. (2015). Compositional vector space models for knowledge base completion.\n  - Das et al. (2016). Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks. \n  - Rocktaschel and Riedel (2017). End-to-end Differentiable Proving.\n  - Yang et al. (2017). Differentiable Learning of Logical Rules for Knowledge Base Completion.\n- Table 6: How many repeats were used for estimating the standard deviation?\n\n\nUpdate: I thank the authors for their response and additional experiments. I am increasing my score to 7.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}