{"title": "Highly Specialized Analysis for a Toy Problem", "review": "Summary of the paper:\nThis paper studies using a three-layer convolutional neural network for the XOR detection problem. The first layer consists of 2k 2 dimensional filters, the second layer is ReLU + max pooling and the third layer are k 1s and k (-1)s. This paper assumes the input data is generated from {-1,+1}^{2d} and a margin loss is used for training. \nThe main result is Theorem 4.1, which shows to achieve the same generalization error, defined as the difference between training and test error, the over-parameterized neural network needs significantly fewer samples than the non-over-parameterized one. \nTheorem 5.2 and 5.3 further shows randomly initialized gradient descent can find a global minimum (I assume is 0?) for both small and large networks. \n\n\nMajor Comments:\n1.  While this paper demonstrates some advantages of using over-parameterized neural networks, I have several concerns.\nThis is a very toy example, XORD problem with boolean cube input and non-overlapping filters. Furthermore, the entire analysis is highly tailored to this toy problem and it is very hard to see how it can be generalized to more practical settings like real-valued input. \n2. The statement of Theorem 4.1 is not clear. The probabilities p_+ and p_- are induced by the distribution D. However, the statement is given p_+ and p_-, there exists one D satisfies certain properties. \n3. In Theorem 5.1 and 5.2, the success probability decreases as the number of samples (m) increases. \n\n\nMinor Comments:\n1. The statement of Theorem 4.1 itself does not show the advantage of over-parameterization because optimization is not discussed. I suggest also adding discussion on the optimization to Sec.4 as well.\n2. Page 5, last paragraph: (p1p-1)^m -> (p_+p_-)^m.\n3. There are many typos in the references, e.g. cnn -> CNN, relu -> ReLU, xor -> XOR.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}