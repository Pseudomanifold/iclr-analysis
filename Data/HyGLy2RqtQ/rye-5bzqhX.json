{"title": "Fixed labeling function?", "review": "The paper tries to offer an explanation about why over-parametrization can be helpful in neural networks; in particular, why over-parametrization can help having better generalization errors when we train the network with SGD and the activation functions are RELU.\n\nThe authors consider a particular setting where the labeling function is fixed (i.e., a certain XOR function). The SGD however does not use this information, and it is shown that SGD may converge to better global minimums when the network is over-parametrized. \n\nThe considered CNN is a basic one: only the weights of one layer is trained (others are fixed), and the only non-linearities are max-pooling and RELU (one can remove these two max-based operators with one appropriately defined max operator).\n\nThe simplicity of the CNN makes it unclear how much of the observed phenomenon is relevant to CNNs: Can the analysis made simpler by considering (appropriately-defined) linear classifiers instead of CNNs? Is there something inherently special about CNNs?\n\nMy main concern is, however, the combination of these two assumptions:\n+ Labeling function is fixed \n+ The distribution of data is of a certain form (i.e., Theorem 4.1 reads like: for every parameter p+ and p- there \"exists\" a distribution such that ...)\n \nIsn't this too restrictive? For any two reasonable learning algorithms, there often exists a particular scenario (i.e., labeling function and distribution) that the first one could do better than the other.\n\nOn a minor note, the lower bound is proved for a certain range of parameters (similar to the upper bound). How do we know that these ranges are not specifically chosen so that they are \"good\" for the over-parametrized one and \"bad\" for the other? \n\n--\nI updated my score after reading other reviews and the authors' response.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}