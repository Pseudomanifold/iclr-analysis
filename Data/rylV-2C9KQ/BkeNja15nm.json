{"title": "A very interesting paper with good analysis and decent experiments..", "review": "Brief summary:\n\nThis paper presents a deep decoder model which given a target natural image and a random noise tensor learns to decode the noise tensor into the target image by a series of 1x1 convolutions, RELUs, layer wise normalizations and upsampling. The parameter of the convolution are fitted to each target image, where the source noise tensor is fixed. The method is shown to serve as a good model for natural image for a variety of image processing tasks such as denoising and compression.\n\nPros:\n* an interesting model which is quite intriguing in its simplicity.\n* good results and good analysis of the model\n* mostly clear writing and presentation (few typos etc. nothing too serious).\n\nCons and comments:\n* The author say explicitly that this is not a convolutional model because of the use of 1x1 convolutions. I disagree and I actually think this is important for two reasons. First, though these are 1x1 convolutions, because of the up-sampling operation and the layer wise normalizations the influence of each operation goes beyond the 1x1 support. Furthermore, and more importantly is the weight sharing scheme induced by this - using convolutions is a very natural choice for natural images (no pun intended) due to the translation invariant statistics of natural images. I doubt this would have worked so well hadn't it been modeled this way (not to mention this allows a small number of parameters).\n\n* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice. Natural images are only approximately piece-wise smooth after all.\n\n* The use of the name \"batch-norm\" for the layer wise normalization is both wrong and misleading. This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it's implemented with the same function) as there is no \"batch\".\n\n* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation. Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.\n\n* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell)\n\n* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor? I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context).", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}