{"title": "Overall a nice paper", "review": "The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc. The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP. Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can\u2019t fit random noise (and thus maybe better suited for denoising).\n\nThe paper is clearly written, and the proposed architecture has too cool properties: it\u2019s compact enough to be used for image compression; and it doesn\u2019t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).\n\nI have two main concerns about this paper.\nFirst, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture. Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with \u201cThe network is not learned and itself incorporates all assumptions on the data.\u201d).\n\nMy second concern is about the theoretical contribution. On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising. On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters. Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)). This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.\nAlso, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.\n\nSince the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.\n\nSome less important points:\n\nFig 4 is very confusing.\nFirst, it doesn\u2019t label the X axis.\nSecond, the caption mentions that early stopping is beneficial for the proposed method, but I can\u2019t see it from the figure.\nThird, I don\u2019t get what is plotted on different subplots. The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models? Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it\u2019s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.\nAlso, in this quote \u201cIn Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + \u03b7 (i.e., FORMULA ...\u201d the formula doesn\u2019t correspond to the text.\nAnd finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.\n\nI don\u2019t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?\n\nThe authors claim that the model is not convolutional. But first, it\u2019s not obvious why this would be a good thing (or a bad thing for that matter). Second, it\u2019s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.\n\n> The deep decoder is a deep image model G: R N \u2192 R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).\nI think it should be vice versa, N >> n\n\nThe following footnote\n> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512\u00d7512\u00d73, and choose k = 64 and k = 128 for the respective compression ratios.\nUses unintroduced (at that point) notation and is very confusing.\n\nIt would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).\n\nI\u2019m also wondering, is it harder to optimize the proposed architecture compared to DIP? The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}