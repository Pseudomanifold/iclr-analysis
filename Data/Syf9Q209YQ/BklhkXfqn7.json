{"title": "A simple but interesting idea, but the results are not very significant and some baselines are missing.", "review": "The paper tackles the problem of semi-supervised classification using GAN-based models. They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference. The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations. The idea is to use GAN to learn the manifold. The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs. They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \\delta to z directly (||f(z) - f(g(z+\\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small. The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \\bar r(z) and then adding a tunable magnitude of \\bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\\epsilon \\bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.\n\nPros:\n- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.\n- The idea is simple and easy to implement based on a standard GAN.\n- The authors conduct various experiments to show the interaction of the regularization and the generator.\n\nCons:\n- For semi-supervised classification, the paper did not report the best results in other baselines. E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels). The performance of the proposed method is worse than the previous work but they claimed \"state-of-the-art\" results. The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2]. The experimental results are not very convincing because many importance baselines are neglected.\n- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018). \n\nI'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3]. It would be better to compare with them.\n\nReferences:\n[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018\n[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018\n[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}