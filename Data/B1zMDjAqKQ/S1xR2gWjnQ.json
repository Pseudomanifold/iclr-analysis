{"title": "a model that mimics expectation learning i.e. learning multisensory representations by training to predict the other modalities from a given modality", "review": "Overview and contributions: The authors introduce a model that mimics expectation learning (i.e. learning multisensory representations by training to predict the other modalities from a given modality, for example, image to audio, audio to image). The proposed model is based on an autoencoder structure with a recurrent self-organizing network for\nmultisensory binding of latent representations. The authors perform experiments to show the reconstruction of image and audio signals given the other, as well as discriminative results on audio and image classification.\n\nStrengths:\n1. The paper is well motivated by the point of view of human learning. I really liked the abstract and introduction!\n2. I liked the recurrent self-organizing network presentation and usage.\n\nWeaknesses:\n1. While the paper is well motivated, I believe that the presence of multisensory expectation learning depends heavily on the type of multisensory data. For some modalities such as audio, it is clear what the mapping to language is (audio-to-language transcribing). For language-to-audio, there are multiple audio translations depending on the different tone of voice used by each person. Image-to-audio translation is also a one-to-many mapping. So I have concerns about how the model would work in these cases depending on the data used.\n2. I don't believe that the proposed model achieves state-of-the-art results: from Table 1, image classification performance is outperformed by Inception V3, and for both modalities, I'm not sure why the authors did not compare with more recent baselines. The best audio baseline is from 2016...\n3. It seems that this approach needs paired multisensory data for training, which limits the amount of training data as compared to unisensory models. Also, what if some sensors are noisy or missing? Is this model robust to such cases?\n\nQuestions to authors: \n1. Refer to weakness points.\n2. Can you comment on when you think multisensory expectation learning would work, and when it wouldn't? What types of data do we need, and from which modalities/sensors?\n\nPresentation improvements, typos, edits, style, missing references:\n1. Page 2: Our hybrid approach allowed -> Our hybrid approach allows\n1. Page 7: audiotry recognition -> auditory recognition\n2. Page 7: while improved the visual stimuli in 3% -> and improving ... by 3%\n3. Multiple other typos and awkward phrasing, I would suggest the authors spend more time proof-reading their paper before submission.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}