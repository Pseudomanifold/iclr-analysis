{"title": "Review", "review": "This work proposed using temporal logic formulas to augment RL learning via the composition of previously learned skills. This work was very difficult to follow, so it is somewhat unclear what were the main contributions (since much of this seems to be covered by other works as referenced within the paper and as related to similar unreferenced works below). Moreover, regarding the experiments, many things were unclear (some of the issues are outlined below). While the overall idea of using logic in this way to help with skill composition is interesting and exciting, I believe several things must be addressed with this work. This includes: situating this work more clearly against existing similar works which use logic in this way, clearly defining the novel contributions of this work as compared to those and others, overall making the methodology more clear and specific (including experimental methodology), and comparing/contrasting against (or at least discussing differences with) methods with similar motivations (e.g., HRL multi-task learning, meta-learning) to emphasize the need/importance of this work \u2014 I am aware that at least 1 HRL work is mentioned, but this work is not really contrasted against it to help situate it.\n\nQuestions/Concerns about Experiments:\n\n+ Does Figure 5 show the averaged return over 5 runs, sum of discounted rewards averaged over 5 episodes per update step, or 5 episodes, each from a separate run averaged together? It is a bit unclear especially because the main text and the figure caption slightly differ. Also, average discounted return is somewhat different than average return,  suggest updating the label to be clear also with the discount factor used.\n+ What were the standard deviations for this across experiments? Even with averaging it seems that these runs are very high variance, would be good to understand what variance bounds to expect if using this method.\n+ Why were average discounted returns reported in Figure 5 and not in Table 1?\n+  What were the standard deviations on success rate and training time? Also what about sample complexity? \n+ To my understanding the benefit here is reusability of learned skills via the automata methods described here. It would have made sense to compare against other HRL or multi-task learning methods in addition to just SQL or learning from scratch. For example how would MAML compare to this?\n+ It is also unclear whether the presented results in Table 1 and Figure 5 are on the real robot or in simulation. The main text says, \u201cAll of our training is performed in simulation and the policy is able to transfer to the real robot without further fine-tuning.\u201d So does this mean that Figure 5 is simulated results and Table 1 is on the real robot?\n\n\n\nCitations that should likely be made:\n\n+ Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. \"Reinforcement Learning for LTLf/LDLf Goals.\"\u00a0arXiv preprint arXiv:1807.06333\u00a0(2018).\u00a0\n+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Decision-making with non-markovian rewards: From LTL to automata-based reward shaping.\"\u00a0 In\u00a0Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017. \n+ Camacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping.\" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017.\u00a0\n\n\nTypos/Suggested grammar edits:\n\n\u201cSkills learned through (deep) reinforcement learning often generalizes poorly across tasks and re-training is necessary when presented with a new task.\u201d \u2014> Often generalize poorly\n\n\u201cWe present a framework that combines techniques in formal methods with reinforcement learning (RL) that allows for convenient specification of complex temporal dependent tasks with logical expressions and construction of new skills from existing ones with no additional exploration.\u201d \u2014> Sentence kind of difficult to parse and is a run-on\n\n\u201cPolicies learned using reinforcement learning aim to maximize the given reward function and is often difficult to transfer to other problem domains.\u201d \u2014> ..and are often..\n\n\u201cby authors of (Todorov, 2009) and (Da Silva et al., 2009)\u201d \u2014> by Todorov (2009) and Da Silva et al. (2009) Also several other places where you can use \\citet instead of \\cite", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}