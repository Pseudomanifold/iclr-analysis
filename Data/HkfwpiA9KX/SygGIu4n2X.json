{"title": "Interesting topic but little technique contribution", "review": "This paper mainly focuses on combining RL tasks with linear temporal logic formulas and proposed a method that helps to construct policy from learned subtasks. This method provides a structured solution for reusing learned skills (with scTLTL formulas), and can also help when new skills need to be involved in original tasks. The topic of the composition of skills is interesting. However, the joining of LTL and RL has been developed previously. The main contribution of this work is limited to the application of the previous techniques.\n\nThe proposed approach also has some limitations. \nWill this method work on composing scTLTL formula with temporal operators other than disjunction and conjunction?\nCan this approach deal with continuous state space and actions? This paper describes a discretization way, which, however, can introduce inaccuracies. \nThe design of the skills is by hand, which restricts badly its usability.\nThe experiments results show that the composition method does better than soft Q-learning on composing learned policies, but how it performed compared to earlier hierarchical reinforcement learning algorithms? \n  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}