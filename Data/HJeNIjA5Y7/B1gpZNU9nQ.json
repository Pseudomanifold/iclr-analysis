{"title": "A semi-supervised learning paper, lacking thorough experimentation", "review": "This paper proposes a new metric called the image score that compares the similarity of activation between a given image with a pool of groundtruth images. The paper finds it useful for semi-supervised learning with self-teaching, where the network picks the most confident sample and use the network prediction as the label. It finds that the proposed method is better than 1) not using the unlabeled data and 2) using softmax as an indicator for model prediction certainty.\n\nMotivation: The introduction begins by motivating the interpretability story of deep learning, but I don\u2019t see gaining any more interpretability by reading the rest of the paper. The paper proposes to improve interpretability by assigning a score to each individual example, but then the obtained scores are not properly analyzed in the paper, and only final classification accuracy is evaluated. What are the training samples that makes the model make certain decision at test time? How to measure the correlation between the usefulness of training samples and the proposed image score? These questions left unanswered in the paper. Figure 1 helps a little bit, but then the top row is not necessarily the bad images, but maybe hard examples that needs extra attention to learn. Therefore, I think the end results presented in the experiments do not align with the motivation. Rather than shooting for interpretability, this is just another semi-supervised learning paper.\n\nModels: The major issue of this paper is the model formulation that is not well motivated. The intuition of how the authors come up with the equation for computing the image score is not well explained. Hence the formulation seems very ad-hoc, and it is unclear why this is the selected method.\n\nExperiments: As a semi-supervised learning paper, a common setting for CIFAR-10 is to use 4k labeled images. Here, the method uses 30k, which is 7.5x the size of the usual setting. It also does not compare to prior semi-supervised learning work (e.g. one of the recent one is: https://arxiv.org/abs/1711.00258). The only two baselines discussed here are weak. Also the improvement from the baselines by using the proposed method is not very significant.\n\nComparison: Figure 2-4 shows some positive correlation between the accuracy and score, which is fair, but it doesn\u2019t compare to any baselines--the only one we have is softmax baseline and it is not shown in the figure.\n\nIn conclusion, I couldn\u2019t see how the paper improves interpretability as claimed in the introduction. The proposed method seems ad-hoc, without any justification. Being considered as a semi-supervised learning paper, it lack significant amount of comparison to prior work and adopting a common semi-supervised benchmark. Due to the above reasons, I recommend reject.\n\n---\nMinor points:\n\u201c...almost all of the existed works investigate only the models and ignore the relationship between models and samples\u201d. This is over-exaggerated. I believe most of the visualization techniques are dependent on the actual input samples. It is true to say about \u201ctraining samples\u201d not \u201csamples\u201d in general.\n\n\u201call correctly classified images should have similar chain of activation, while incorrectly classified images should have very different activations both within themselves and with correctly classified images\u201d. This claim seems not backed up. How do you know it is the case for \u201call\u201d correctly classified images? What defines similar/different?", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}