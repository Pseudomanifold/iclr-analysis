{"title": "The results are competitive, but not too much innovation", "review": "The paper is easy to read. The authors did a job in describing the problem, concepts, and the proposed multi-objective optimization method. The computational results are on par with NASNet-A mobile. \n\nIt is good to know that we can use standard multi-objective method for neural architecture search. The implementation seems to be straightforward. The paper mainly uses existing ideas, but with some incremental improvements. It lacks novelty.  \n\nThe time reduction of this method on ImageNet comes from transfer learning by training on CIFAR-10 first. As the paper admits this is not going to generalizing well. How good the method is if just using a single dateset? For CIFAR-10, is this method comparable with ENAS(https://arxiv.org/pdf/1802.03268.pdf)?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}