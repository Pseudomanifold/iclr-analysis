{"title": "This paper focuses on lossy compression techniques and federated dropout strategies to control the update burden that\u2019s needed to coordinate nodes in a federated learning setting. ", "review": "The paper is well written and addresses an interesting problem. Overall, I do find the federated dropout idea quite interesting. As for the lossy compression part, I am a bit skeptical on its application for this problem. In general, I believe that the manuscript could greatly benefit from answering the questions that I am raising below. It would certainly help me better appreciate the contributions of this work. \n\nThe lossy aspect of the compression inevitably introduces performance downgrades. However, compression/communication systems are designed to make sure that the information dropped is not important for the task at hand (e.g., high frequencies that are not perceived by our eyes in the spatial domain are typically dropped when compressing images through zig zag scanning after transformation). Randomly dropping coefficients as suggested in this paper seems odd to me (the subsampling technique that is used). Can you justify this approach? The manuscript does hint that this approach provides lukewarm results. Could there be a better approach that focuses on parts of the model that deemed \u201cless\u201d important if a notion of coefficient importance can be derived? \n\nCan you emphasize more the benefits of compression and federated drop out, versus training a low capacity model with less parameters? The introduction refers to the low capacity approach as a naive model. Could this be compared experimentally? This would help better appreciate the benefits of the federated dropout strategies that are proposed here. In the experiments, could you explain why increases in q (quantization steps) seems to lead to limited or marginal accuracy improvements? \n\nFor the results shown in Figure 4, did you also use any form of subsampling and quantization? Also, do you have a justification for why with some amounts of dropout, the accuracy may improve but at a slower pace (pretty much the punch line of these experiments)? It is an interesting finding but it is counter intuitive and requires explanations in my view. \n\nOn the communication cost experiments, can you explain precisely how did you compute these reduction factors? Did you tolerate some form of accuracy degradation? Also, did you consider the fact that more \"rounds\" are needed to get to a target accuracy level? Is there a cost associated with these additional rounds and was that cost taken into consideration? Adding clarity on this would certainly help. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}