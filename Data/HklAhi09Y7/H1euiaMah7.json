{"title": "Interesting idea but not novel enough", "review": "Overall:\nThis paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form. The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a \u201cscratchpad\u201d memory to keep track of what has been generated so far and to guide future generation. \n\nQuality and Clarity:\n-- The paper is well-written and easy to read. \n-- Consider using a standard fonts for the equations. \n\n\nOriginality :\nThe idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful. \nCompared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write \u201cdecoding information\u201d to the \u201cencoder\u201d output. The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively \u201cwrites\u201d a message (\u201ccoverage\u201d) to the encoder's hidden states.\nIn the original coverage paper (Tu et.al, 2016), they also proposed a \u201cneural network based coverage model\u201d where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder. However, the modification is slightly marginal but seems quite effective. It is better to explain the major difference and the motivation of updating the hidden states.\n\n-------------------\nComments:\n-- In Equation (13), is there an activation function between W1 and W2?\n-- Based on Table 1, why did not evaluate the proposed model with beam-search?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}