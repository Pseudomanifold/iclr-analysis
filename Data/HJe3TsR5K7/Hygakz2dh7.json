{"title": "a straightforward extension to WAE", "review": "The whole model can be simplified by this: using auto-encoders for X and Y's reconstruction, then use Triple GAN loss to match the joint distribution of (X, Y).  However, the deterministic model with GAN loss looks problematic to me.\n\nquestions:\n\n1. Although the authors showed strong evidence in their experiment part, they still failed to compare models with Bicycle-GAN, i.e., how Bicycle GAN performs on these two dataset?\n\n2. missing some comparison: why use simplified Triple-GAN loss (i.e. without two regularization terms)  instead of Triangle-GAN, which is addressed to be better? I think the authors need to discuss about this. Also, the authors need to use MMD and other methods mentioned in the original WAE paper.\n\n3. In table 1, without triple-GAN loss, the whole model is deterministic, but the authors can still show the FID score for the generalization ability, which is better than all other cycle-GAN based models, why is that possible? Is this equivalent to claim that auto-encoder has the ability to generate realistic images just by sampling z? \n(If I understand the experiment correctly, the author's synthesized images is generated by $y_hat = G_2(E_1(X))$, no sampling z required)\n\n4. Can the authors show the generalization ability of JWAE? For example, with input X, we can have different correct corresponding Ys, just like Bicycle-GAN did.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}