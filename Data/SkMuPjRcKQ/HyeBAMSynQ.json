{"title": "valid technical contribution. a little on the incremental side", "review": "The main contribution of the paper are methods for propagating approximate uncertainty in neural networks through max and argmax layers. The proposed methods are explained well. The paper is clearly written. The methods are validated in small scale experiments and seem to work well.\n\nThe proposed approach is not much more accurate than Monte Carlo dropout, but is more computationally efficient. The standard way of efficiently predicting at test time with a dropout-trained network is to simply scale the weights. Could the authors try calibration on networks of this type and compare against the proposed method with calibration? (i.e. scale the predicted logits of the standard test-time network to be on the same scale as the logits under your approach)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}