{"title": "While the experiment of fault tolerance is interesting, the obtained implications are somewhat trivial.", "review": "This paper considers the trade-off between the prediction accuracy of deep neural networks (DNNs) and sensitivity to adversarial examples.\nReviewing the (Gaussian) channel capacity and rate-distortion theory, i.e., the information bottleneck, the authors discuss their implications on the generalization performance of DNNs. The experiments demonstrate the SNR of gradients, information plane, the generalization gap, and fault tolerance against adversarial examples.\n\nWhile the interpretations of DNN learning by the information theoretic concepts are interesting, most of them are already known results, and hence provide little novel theoretical knowledge.\n\nThe discussions in Section 3 are superficial. It is not clear how they are related to the main arguments of this paper.\n\nWhile the experiment of fault tolerance is interesting, the implications obtained from experiments are somewhat trivial.\n\nminor comments:\np.2, l.15: h, w, and c are undefined.   \nSection 2: Rate-distortion theory is usually explained by the sphere covering argument instead of sphere packing. \nSection 4.3.1: It is not explained what zero and one-shot transfer learning is.\n\nPros:\nThe experiment of fault tolerance is interesting. \nCons:\nTheoretical parts are basic results of information theory.\nThe implications of experiments are somewhat trivial. \n\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}