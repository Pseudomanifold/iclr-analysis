{"title": "Interesting idea but implications, significance, theoretical analysis and experiments need improvements.", "review": "\nQuality and clarity: good.\n\nOriginality and significance: This paper studies the stochasticity\nof the norms and directions of the mini-batch gradients, to\nunderstand SGD dynamics. The contributions of this paper can be\nsummarized as: a) This paper defines gradient norm stochasticity as\nthe ratio of the variance of the stochastic norm to the expectation\nof the stochastic norm. It theoretically and empirically shows that\nthis value is reduced as the batch size increases b) This paper\nempirically finds that the distribution of angles between\nmini-batch gradient and a given uniformly sampled unit vector\nconverges to an asymptotic distribution with mean 90 degrees, which\nimplies a uniform distribution of the mini-batch gradients. c)\t\nThis paper uses von Mises-Fisher Distribution to approximate the\ndistribution of the mini-batch gradients. By theoretically and\nempirically observing that the estimated parameter \\hat \\kappa\ndecreases during training, they claim that the directional\nuniformity of mini-batch gradients increases over SGD training.\n\nThe idea of measuring the uniformity of mini-batch gradients\nthrough VMF distribution seems interesting. But it is unclear how\nthe study of this stochasticity dynamics of SGD can be related to\nthe convergence behavior of SGD for non-convex problems and/or the\ngeneralization performance of SGD.\n\nThere are additional concerns/questions regarding both theoretical\npart and empirical part:\n\n[1] Section3.3: Assumption that p_i(w_0^0) =p_i(w_1^0) = p_i is not\nreasonable when theoretically comparing \\hat \\kappa(w_1^0) and \\hat\n\\kappa(w_0^0). The concentration parameter \\hat \\kappa(w) should be\nestimated by the sum of the normalized mini-batch gradients \"\\hat\ng_i(w)/||\\hat g_i(w)||\" . Instead of using mini-batch gradient,\nthis paper uses the sum of \"p_i-w\" by assuming that \"p_i(w_0^0) -w\"\nis parallel to \"\\hat g_i(w)\", which is ok. However, when comparing\n\\hat \\kappa(w_0^0) and \\hat \\kappa(w_1^0), we say \\hat\n\\kappa(w_0^0) = h(\\sum p_i(w_0^0) - w_0^0) ) and \\hat \\kappa(w_1^0)\n= h(\\sum p_i(w_1^0) - w_1^0) ). It is not reasonable to use the\nsame p_i for p_i(w_0^0) and p_i(w_1^0) because p_i(w_0^0) -w_1^0 is\ndefinitely not parallel to \\hat g_i(w_1^0).\n\n[2] Section 3.3: Assumption \\hat g_i(w_t^{i-1}) \\hat g_i(w_t^0) is\nnot convincing. With this assumption, the paper writes w_1^0 =\nw_0^0 - \\eta\\sum_i \\hat g_i(w_0^{i-1}) = w_0^0 - \\eta\\sum_i \\hat\ng_i(w_0^0) = w_0^0 - \\eta \\sum_i p_i-w_0^0. These equalities are\nnot persuasive. Because, \\sum_i \\hat g_i(w_0^0) is the full\ngradient g(w_0^0) at w_0^0. In other words, these equalities imply\nthat from w_0^0 to w_1^0 (one epoch), SGD is doing a full gradient\ndescent: w_1^0 = w_0^0 -\\eta g(w_0^0), which is not the case in\nreality.\n\n[3] Experiment: Batch size should be consistent with the given\nassumption in the theoretical part. In theoretical part, \\hat\n\\kappa(w_1^0) < \\hat \\kappa(w_0^0) is based on the assumption that\n|\\hat g_i(wt^{i-1}| \\tat for all i, with *large mini-batch size*.\nBut in the experiment, they prove \\hat \\kappa(w_1^0) < \\hat\n\\kappa(w_0^0) by using small-batch size which is 64. The authors\nshould either provide experiments with large batch size or try to\navoid the assumption of large batch size in theoretical part.\n\n[4] The CNN experiment; It is better to add a discussion why the\n\\kappa increases in the early phase of training.\n\n[5] The experiment results show, by the end of training, all models\nFNN, DENN and CNN have very large value of \\kappa which is around\n10^4. This value implies that the mini-batch gradients distribution\nis pretty concentrated, and it is contradictory to the statement in\nthe introduction which is \"SGD converges or terminates when either\nthe norm of the minibatch gradient vanishes to zeros, or when the\nangles of the mini-batch gradients are uniformly distributed and\ntheir non-zero norms are close to each other''. It is also\ncontradictory to the experiment in 3.2 which implies the mini-batch\ngradient are uniformly distributed after training.\n\n[6] The notations in this paper can be improved, some notations are\nusing \"i\" for batch index, some notations are using \"i\" for one\ndata sample. Some notations in Section 3.3 and 3.1 can be moved to\nSection 2 Preliminaries. It will be clearer to define all the\nnotations in one place.\n\nTypos: -Section 3.1: first paragraph, E\\hat g(w) -> E[\\hat g(w)]; -\nParagraph before Lemma2: \\hat \\kappa increases -> \\hat \\kappa\ndecreases; - Paragraph after Theorem2: double the directions in \"If\nSGD iterations indeed drive the directions the directions of\nminibatch gradients to be uniform\".", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}