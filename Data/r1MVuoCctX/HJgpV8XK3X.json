{"title": "Problems of clarity , novelty and significance", "review": "The paper proposes an architecture for language modeling composed of a bigger and smaller LSTM. The authors propose to add skip connections directly from the input layer to the layers of the \"major\" lstm. The skip-connections are computed using the small LSTM. The authors claim to reach better performance with smaller number of parameters in two language modeling tasks (PTB and WT2).\n\nI had a hard time reading and understanding the paper. The clarity and style of the writing should be improved to better stress the significance of the underlying motivation. Increasing the efficiency of learning algorithms in terms of the number of parameters is clearly an important endeavour. However, the novelty of the architecture (skip-connections are well-known in the context of language modeling, e.g. , Melis et al. (2017) for a recent account) as well as the reduction in the number of overall parameters are limited enough to make the marginal gains observed in the reported results insufficient for publication.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}