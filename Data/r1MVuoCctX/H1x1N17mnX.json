{"title": "More efficient spending of the parameters in LSTM language models", "review": "The paper suggests more efficient spending of the parameters in LSTM language models: instead of increasing the hidden layer size of the LSTM, the authors suggest building a small assistant LSTM and its output is concatenated with the output of the main LSTM.\n\nThe approach reminds me the SCRN Model of Mikolov et al. (2015). I don't claim it is the same, as you have an independent LSTM, but there are some similarities: in both cases an assisting small layer is added, and in both cases one needs to choose an appropriate size of this assisting layer. It would be interesting to see if your Minor LSTM plays the same role as the context state in SCRN, e.g. does the hidden state in Minor LSTM change slower than that of the Major LSTM? If yes, then we could say that the Minor LSTM focuses more on what is being talked about (context).\n\nExperimental evaluation of the idea seems adequate. However, my feeling is that the recent optimization and regularization techniques (such as those used in AWD-LSTM) may hide the effect of your idea. If possible, could you please evaluate your approach under less recent regularization, e.g. variational dropout with tied weights as in Inan et al. (2016).\n\nMinor comment on \"more parameters means better model performance\" in the abstract: If by model performance we understand performance on held-out data, then I would say that \"more parameters means more flexible model\".\n\nNote after reading other reviews: I agree with Reviewer1 and Reviewer2 that the use of skip-connections (Melis et al., 2017; Zilly et al., 2016) is one of the main reasons why the model achieves its performance (Table 5). Without skip-connections the proposed Major-Minor LSTM is practically on par with the AWD-LSTM-MoS of Yang et al. (2017) on PTB and only 0.57 perplexity units better on WT-2.\n\nReferences\n- Inan, H., Khosravi, K. and Socher, R., 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462.\n- Melis, G., Dyer, C. and Blunsom, P., 2017. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589.\n- Mikolov, T., Joulin, A., Chopra, S., Mathieu, M. and Ranzato, M.A., 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.\n- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv preprint arXiv:1711.03953.\n- Zilly, J.G., Srivastava, R.K., Koutn\u00edk, J. and Schmidhuber, J., 2016. Recurrent highway networks. arXiv preprint arXiv:1607.03474.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}