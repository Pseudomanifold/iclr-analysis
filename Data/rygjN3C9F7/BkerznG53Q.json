{"title": "Good Writing, Comparisons Needed. ", "review": "This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones. This problem has a natural variational form that can be easily implemented from VIB. Experiments show good performance comparing to VIB. \n\nThis paper is well-written and easy to read. The idea using KL divergence creating a deficiency channel to learn data representation is very natural. It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9). \n\nMy biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem. However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function. For example, how does the method compare with (variants of) Variational Autoencoder? A discussion on this or some empirical evaluations would be nice. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}