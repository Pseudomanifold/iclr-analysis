{"title": "Interesting examples, somewhat weaker evaluation", "review": "The authors present a very interesting work on predicting future motion of a self-driving vehicle given image inputs that represent its surrounding and history. The authors use RNNs for this task, and data augmentation to make their model more robust. They also present a number of very interesting videos showcasing the performance.\n- Several key aspects of the work are not well explained. E.g., what are pixel sizes, time resolution, where is the vehicle positioned within the image? All this is missing.\n- Traffic lights are represented as \"a sequence of grayscale images\", how exactly, one for each state? Or some other way.\n- How were videos generated, how were various channels collapsed?\n- Dashed arrows not explained in Fig 2.\n- \"a small regression tower\", this needs to be elaborated. As well as other mentions of \"towers\".\n- In (3), is the sum over all pixels missing?\n- Section 4.1.2 is not clear, this needs to be expanded. It is not well explained how exactly these losses are computed and used.\n- For past-motion dropout, then you simply give blank input?\n- Figure 6 is referenced in the regular text although it is located in the appendix.\n- Orienting vertical axis with delta of +-25deg (as explained in Section 6.2) is not observed in the given videos, seems that there is no delta there. Is that done only during training?\n- What is the exact difference between open- and closed-loop experiments? Given that a number of other key aspects are missing, I am not sure I fully understand a difference here as well.\n- One of major issues in the evaluation section is that other baselines are missing (especially in the context of Fig 5). Even the more obvious ones would help a lot with understanding the performances, such as vehicle continuing to do what it was doing, or baseline predicting the route). This is a major flaw of the paper.\n- Some recent related work missing, see [1], [2], and related work.\n[1] Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net, Luo, Wenjie, Bin Yang, and Raquel Urtasun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[2] Short-Term Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, Djuric, N., Radosavljevic, V., Cui, H., Nguyen, T., Chou, F.-C., Lin, T.-H., Schneider, J., arXiv preprint:1808.05819, 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}