{"title": "Pseudosaccades", "review": "The paper proposes a data augmentation technique where the input image is sub-sampled by randomly sampling rows and columns without replacement, which the authors call \u2018pseudosaccades\u2019. Rather than multiple classifiers, the authors ensemble using multiple \u2018pseudosaccades\u2019 as input, with the same network.\n\nComments:\nI think that the proposed augmentation is a neat trick. However, the inner-workings of the method are poorly presented (or not well understood). For eg. In section 3.5, while discussing the effects of the method on individual classes, the authors mention \u2018different architectures do tend to be affected by the pseudosaccades differently\u2019 and provide no further insights.\n\nThere are no experiments that compare this method with other standard data augmentation techniques. For instance, one could use a similar ensembling technique for transformations like shear, translation, rotation, etc. by randomly sampling their corresponding parameters. I would be interested in experimental results that compare the proposed ensemble with ensembles constructed using these common techniques.\n\nSince there is no reason for this technique to be used in isolation (I found no such motivation in the paper), it would be insightful to have experimental results where this technique is combined with the aforementioned standard augmentation techniques. Will this method\u2019s impact on the accuracy change with these other augmentations? (Ablation studies would be useful).\n\nThis is a a form of regularization and can be thought of reverse structured dropout. Also have the authors compared this with Cutout [1, 2]? Similar experiments and comparisons would be insightful.\n\n[1] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n[2] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017.\n\nIn summary:\nThe performance improvements are incremental. The paper lacks sufficient technical contribution. Further, it does not provide comparisons with standard techniques and similar augmentation methods to demonstrate the usefulness of the method.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}