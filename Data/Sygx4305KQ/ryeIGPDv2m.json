{"title": "Good Paper, Accept", "review": "In this paper, the authors introduce a new second-order algorithm for training deep networks. The method, named CurveBall, is motivated as an inexpensive alternative to Newton-CG. At its core, the method augments the update role for SGD+M with a Hessian-vector product that can be done efficiently (Algorithm 1). While a few new hyperparameters are introduced, the authors propose ways by which they can be calibrated automatically (Equation 16) and also prove convergence for quadratic functions (Theorem A.1) and guaranteed descent (Theorem A.2). The authors also present numerical results showing improved training on common benchmarks. I enjoyed reading the paper and found the motivation and results to be convincing. I especially appreciate that the authors performed experiments on ImageNet instead of just CIFAR-10, and the differentiation modes are explained well. As such, I recommend the paper for acceptance. \n\n\nI suggest ways in which the paper can be further improved below:\n\n- In essence, the closest algorithm to CurveBall is LiSSA proposed by Agarwal et al. They use a series expansion for approximating the inverse whereas your work uses one iteration of CG. If you limit LiSSA to only one expansion, the update rule that you would get would be similar to that of CurveBall (but not exactly the same). I feel that a careful comparison to LiSSA is necessary in the paper, highlighting the algorithmic and theoretical differences. I don't see the need for any additional experiments, however.\n- For books, such as Nocedal & Wright, please provide page numbers for each citation since the information quoted is across hundreds of pages. \n- It's a bit non-standard to see vectors being denoted by capital letters, e.g. J(w) \\in R^p on Page 2. I think it's better you don't change it now, however, since that might introduce inadvertent typos. \n- It would be good if you could expand on the details concerning the automatic determination of the hyperparameters (Equation 16). It was a bit unclear to me where those equations came from. \n- Could you plot the evolution of \\beta, \\rho and \\lambda for a couple of your experiments? I am curious whether our intuition about the values aligns with what happens in reality. In Newton-CG or Levenberg-Marquardt-esque algorithms, with standard local strong convexity assumptions, the amount of damping necessary near the solution usually falls to 0. Further, in the SGD+M paper of Sutskever et al., they talked about how it was necessary to zero out the momentum at the end. It would be fascinating if such insights (or contradictory ones) were discovered by Equation 16 and the damping mechanism automatically. \n- I'm somewhat concerned about the damping for \\lambda using \\gamma. There has been quite a lot of work recently in the area of Stochastic Line Searches which underscores the issues involving computation with noisy estimates of function values. I wonder if the randomness inherent in the computation of f(w) can throw off your estimates enough to cause convergence issues. Can you comment on this?\n- It was a bit odd to see BFGS implemented with a cubic line search. The beneficial properties of BFGS, such as superlinear convergence and self-correction, usually work out only if you're using the Armijo-Wolfe (Strong/Weak) line search. Can you re-do those experiments with this line search? It is unexpected that BFGS would take O(100) iterations to converge on a two dimensional problem. \n- In the same experiment, did you also try (true) Newton's method? Maybe we some form of damping? Given that you're proposing an approximate Newton's method, it would be a good upper baseline to have this experiment. \n- I enjoyed reading your experimental section on random architectures, I think it is quite illuminating. \n- Please consider rephrasing some phrases in the paper such as \"soon the latter\" (Page 1), \"which is known to improve optimisation\", (Page 7), \"non-deep problems\" (Page 9). ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}