{"title": "Well-motivated idea", "review": "Authors propose choosing direction by using a single step of gradient descent \"towards Newton step\" from an original estimate, and then taking this direction instead of original gradient. This direction is reused as a starting estimate for the next iteration of the algorithm. This can be efficiently implemented since it only relies on Hessian-vector products which are accessible in all major frameworks.\n\nBased on the fact that this is an easy to implement idea, clearly described, and that it seems to benefit some tasks using standard architectures, I would recommend this paper for acceptance.\n\nComments:\n- introducing \\rho parameter and solving for optimal \\rho, \\beta complicates things. I'm assuming \\rho was needed for practical reasons, this should be explained better in the paper. (ie, what if we leave rho at 1)\n- For  ImageNet results, they show 82% accuracy after 20 epochs on full ImageNet using VGG. Is this top5 or top1 error? I'm assuming top5 since top1 would be new world record for the number of epochs needed. For top5, it seems SGD has stopped optimizing at 60% top5. Since all the current records on ImageNet are achieved with SGD (which beats Adam), this suggests that the SGD implementation is badly tuned\n- I appreciate that CIFAR experiments were made using standard architectures, ie using networks with batch-norm which clearly benefits SGD", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}