{"title": "Interesting paper with important connection between GANs' loss and robust estimation", "review": "The authors considered Huber contamination model.\nThey use f-divergence and its variational lower bound to get a criterion for probability distribution function estimation.\nThey showed that  under different functions f in f-divergence they can get different criteria used in robust depth-based estimation of a mean and/or covariance matrix.\nFor f, corresponding to the Total Variation divergence and discriminator being a logistic regression, they proved that the robust estimate can achieve the minimax rate, although there could be difficulties to optimize the criterion. Then the authors showed that for the JS-divergence with discriminator in the form of a one-layer neural network we can get robust and optimal estimate, while the criterion itself can be efficiently optimized.\n\nComments\n- it could be good to define what Tau is right after formula (3). Analogously for the class of probability distributions $mathcal{Q}$ in (4), in for $\\tilde{\\mathcal{Q}}$ in (5) \n- page 3, line 12 from above: \u201cand f\u2019(t) = e^{t-1}.\u201d In fact, here we should use $f^*(t)$\n- page 3, proposition 2.1, subsection 1 of the proposition: $\\tilde{\\mathcal{Q}}$ instead of $\\tilde{Q}$ should be used as a notation for a class of probability distributions\n- in (12) the authors unexpectedly introduced a new notation $D$. I guess they should specify right after formula (12) what is $D$\n- theorem 3.1. If it is possible, it could be good at least to speculate on how $C, C\u2019$ depend on $c$ in the displayed formula\n- axis labels in figure 2 are almost impossible to read. This somehow should be improved\n- in table 1 we clearly see that TV-GAN is better for some part of problems, and JS-GAN is better for another part of problems. Why? Any comments? At least intuition?\n- page 8, \u201c On the other hand, JS-GAN stably achieves the lowest error in separable cases and also shows competitive performances for non-separable ones.\u201d Why? Any comments?\n\nConclusion\n- in general, the paper is well written\n- it contains sufficient number of experiments to prove that the proposed approach is reasonable\n- the connection between GANs based on f-divergence and robust estimation seems to be important. Thus I\u2019d like to proposed to accept this paper\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}