{"title": "An interesting but somewhat incomplete paper on poisoning graph embeddings.", "review": "This paper is a timely work on poisoning random walk based graph embedding methods. In particular, it shows how to derive a surrogate loss function for DeepWalk. Even though the analysis method and the algorithm proposed is somewhat loose, I think this paper do a good contribution towards the adversarial attack problem for important models.\n\nThere are still some room to improve in this paper. One problem is that since the paper proposes a surrogate loss L_{DW3}, it would be natural to analysis its gap from L_{DW1}. In this paper I can only see some empirical results on this issue. Another issue is that the algorithm the paper used is another approximation towards L_{DW3} by an additional sampling method. And the overall strategy can be far away from the true optimal solution for maximizing L_{DW3}. Still there's no analysis on that issue. A potential drawback for the method proposed is that its complexity is O(NE), which can be quite expensive when the graph is big, and # edges is \\Omega(NlogN).\n\nThe experiments in this paper is convincing. It seems that the method proposed is way better than its competitors when removing edges. Is there any further intuition on that? Why the method is not so good when we add edges? Moreover, the black-box attack scenario requires more justification. What is the relative performance gain for A_{DW3} against other attacks in the black-box setting?\n\nOverall, this paper targets on an important issue in machine learning. My main concern is that it leaves too many questions behind their algorithms. Still some effort is required to improve the paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}