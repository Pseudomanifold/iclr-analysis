{"title": "Good insights on unifying ideas under GVFs; but questions on algorithmic improvements.", "review": "The authors analyze various RL algorithms under the purview of Generalized Value Functions (Sutton et al. 2011). Specifically, the successor feature vector, the policy-gradient theorem, the option-value function, and the gradients for manager/worker in FeUdal Networks are all represented crisply in terms of GVFs. The detailed proofs in the Appendix and the mathematical rigor are appreciated.\n\nA major motivation for unifying ideas under GVFs is to facilitate development of new algorithms, but I find the paper slightly less convincing on this front. With respect to different sections:\n\n\u2022\t[GVF for PG]: Rewriting the policy-gradient theorem with 2 GVFs, the authors propose to improve on the baseline algorithm with bootstrapping on both the critic and the actor-gradients. Although the results on a small domain look interesting, I would have enjoyed some discussion on the scalability aspects and integration/comparison with more recent actor-critic algorithms. \n\n\u2022\t[GVF for Options]: The authors claim that \u201cThe GVF view highlights the fact that the option-value function depends .... and a long-term signal summarizing the performance of other options\u201d. I believe that the definition of the option-value defined in Bacon et al. (2017) makes this dependence quite clear \u2013 it has an expectation over the option-value of all options. Therefore, I\u2019m unable to appreciate the importance of writing option-value in GVFs and/or deriving an algorithmic improvement out of that. \n\n\u2022\t[GVF for FuN]: Using the GVF view, the authors propose to use a different prediction variable (v) for the policy \u2013 i.e. instead of difference in state representation at (t+c) and (t) as used in FuN, they use a discounted (by gamma_hat) sum of these differences. Is this interpretation correct? If yes, could you provide some intuition for how this amounts to an algorithmic improvement over FuN? The Atari results definitely don\u2019t show the improvement empirically.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}