{"title": "Work that introduces new ultrafine-grained classification dataset with a somewhat incremental model", "review": "Summary:\nThis paper introduces a new dataset consisting of images of various objects placed on store shelves that are labeled with object boundaries and what are described as \u201cultrafine-grained\u201d class labels. The accompanying task is to predict the labels of each object given the individual images as well as their spatial layout relative to each other. To solve this task, a deep structured model is used consisting of CNN features for each image which are fed into a linear-chain CRF. To better deal with the large number of classes, pairwise potentials are represented as the multiplication of two lower-rank matrices which represent a sort of \u201cclass embedding\u201d for each potential label. Training efficiency is improved by considering an objective based on a form of piecewise pseudolikelihood, which allows for training-time inference to be conducted with linear complexity relative to the number of labels. This objective also allows for easy use of batch normalization for the input features to the CRF model. This model/training procedure are compared against a number of models/training procedures to demonstrate its utility.\n\nComments:\nArguably, the primary contribution of this paper is the introduction of a new \u201cultrafine-grained\u201d classification dataset which additionally allows for context to be utilized during prediction. This an interesting task, and it\u2019s clear where being able to make such classifications is useful. The task is somewhat limited in scope, however. It\u2019s unclear to me how models developed for this specific task would contain insights or be useful for other tasks - the utility of any models developed for this task seem limited to this exact task. If you have any other examples where inputs might be structured in this way, this would be good to add to the paper.\n\nThe model introduced is interesting, but its novelty is limited. It\u2019s mostly a synthesis of ideas from previous work - CNN-based features, using a CRF to model correlations among labels, and approximating the full likelihood with pseudolikelihood. The interesting additions to these ideas are the fact that an \u201cembedding\u201d is learned for each class and that using the pseudolikelihood during training allows for batch norm to be applied in an easy way. Neither of these is a ground-breaking insight, but they are interesting nonetheless. I am somewhat surprised that the use of batch norm during training but not during testing did not hurt performance - a discussion of why this is the case would be good to have. For the most part, I think the experimentation is sufficiently rigorous - comparisons are made against a variety of baselines, and the new model trained with the specified training procedure outperforms the other alternatives. The one additional comparison I would have liked to see would have been against a model that pairwise potentials from the input features using a neural network-based model (for example, the one used in [1] - this seems like a rather glaring omission.\n\nOther Comments:\n-Since you ran a cross-validation, you should add confidence intervals to your reported numbers\n-One additional dataset detail I was hoping to see that you didn\u2019t provide is the mean/standard deviation of the number of instances per class,\n-Your appendix contains a number of interesting ablation studies - you really should report the numbers for these as well\n-The title of your paper is somewhat misleading - it\u2019s hard to argue that the form of class embedding you use is a \u201cdeep\u201d class embedding since it\u2019s just a matrix of parameters that are learned during training.\n\nOverall, I\u2019m not convinced the model/training procedure by themselves would be fully worthy of publication, but the fact that a new dataset is introduced with a challenging variant of standard classification tasks adds merit to this work.\n\n[1] Ma, Xuezhe, and Eduard Hovy. \"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF.\"\n\n\nREVISION:\nThe other reviewers raised some concerns that I had overlooked (especially regarding novelty of using matrix factorization to generate your potentials). Given these, I do not think that this paper is in a state where it is ready to be accepted. Proper citations and analysis of your approach will be needed first.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}