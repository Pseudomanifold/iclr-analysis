{"title": "A large visual QA on plots dataset that should be useful", "review": "\n\n\nThe authors propose a large dataset for the task of visual\nunderstanding of plots. Compared to the two previous datasets recently\npublished (FigureQA and DVQA), their proposed dataset (DIP) involves a\nsignificantly larger vocabulary, and realistic words, and the\nquestions and answers (as well as plots) can be more diverse and\ncomplex.\n\nThe novelty of the work lies (see below) in some aspects of creating\nthe data, though new ideas are somewhat marginal, considering the work on previous\ndatasets. The paper is clearly written.  I think the dataset will be very useful.\n\n\n-- Because all questions are generated via templates in particular\n   (and plots also follow a certain generation process, albeit noisy),\n   I am concerned that it would ultimately be easy to invert the\n   process to crack the dataset. Any intuitions why this is unlikely?\n   For instance, eventually Jeopardy was tackled (better than human\n   champion) with a large engineered system..\n\n-- Please provide example mistakes of techniques you tried, eg on the \ntable answering problem. \n\n-- Section 5: why not report performance on the 'test-hard' split (in\n   supplementary), since it's mentioned at all?\n\n-- What is the human performance on your data? Some rough accuracy\nrates will be useful, preferably on each question type.  This would\nhelp understand the room for progress (This is done for FigureQA data).\n\n-- minor: Why not use more sophisticated techniques (eg neural-based\n   relation learning, the work is cited) for the last stage table\n   reasoning?\n\n\nIdeas, novelty: unlike previous work, the plots are generated from\nreal tables (world bank, open government data, ...) and Amazon Turk\nworkers are asked to come up with relatively complex questions, for a\nsmall subset of the generated plots.  Templates for these questions\nare then generated (with some extensive manual work to make sure the\nquestions feel natural), and many plots (270k) and questions (~10\nmillion) are then generated using plot generation and the templates.\n\nThe performances reported are via breaking the problem into stages,\nusing several techniques in a pipeline, from visual elements detection\nto OCR, to table reconstruction, finally table QA. Standard\ntechniques (to best of my knowledge) were used for each stage to\nreport a baseline performance.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}