{"title": "Review of \"Universal Lipschitz Functions\"", "review": "This paper introduces GroupSort. The motivation is to find a good way to impose Lipschitz constraint to the learning of neural networks. An easy approach is \"atomic construction\", which imposes a norm constraint to the weight matrix of every network layer. Although it guarantees the network to be a Lipschitz function, not all Lipschitz functions are representable under this strong constraint. The authors point out that this is because the activation function of the network doesn't satisfy the so called Jacobian norm preserving property.\n\nThen the paper proposes the GroupSort activation which satisfies the Jacobian norm preserving property. With this activation, it shows that the network is not only Lipschitz, but is also a universal Lipschitz approximator. This is a very nice theoretical result. To my knowledge, it is the first algorithm for learning a universal Lipschitz function under the architecture of neural network. The Wasserstein distance estimation experiment confirms the theory. The GroupSort network has stronger representation power than the other networks with traditional activation functions.\n\nAdmittedly I didn't check the correctness of the proof, but the theoretical argument seems like making sense.\n\nDespite the strong theoretical result, it is a little disappointing to see that the GroupSort doesn't exhibit any significant advantage over traditional activation function on image classification and adversarial learning. This is not surprising though.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}