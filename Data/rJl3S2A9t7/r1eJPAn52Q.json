{"title": "Advantages of the proposed method over SVRG + policy gradient method are unclear.", "review": "This paper proposes a new policy gradient method for reinforcement learning.\nThe method essentially combines SARAH and trust region method using Fisher information matrix.\nThe effectiveness of the proposed method is verified in experiments.\n\nSARAH is a variance reduction method developed in stochastic optimization literature, which significantly accelerates convergence speed of stochastic gradient descent.\nSince the policy gradient often suffers from high variance during the training, a combination with variance reduction methods is quite reasonable.\nHowever, this work seems to be rather incremental compared to a previous method adopting another variance reduction method (SVRG) [Xu+2017, Papini+2018].\nMoreover, the advantage of the proposed method over SVRPG (SVRG + policy gradient) is unclear both theoretically and experimentally.\n[Papini+2018] provided a convergence guarantee with its convergence rate, while this paper does not give such a result.\nIt would be nice if the authors could clarify theoretical advantages over SVRPG.\n\nMinor comment:\n- The description of SVRG updates in page 2 is wrong.\n- The notation of H in Section 3.1 (\"ODE analysis\") is not defined at this time.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}