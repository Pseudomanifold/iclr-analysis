{"title": "A paper that needs work in terms of motivation, exposition, and evaluation", "review": "(apologies for this belated review)\n\nSummary\n\nThe authors consider the task of imputing missing data using variational auto-encoders. To do so, they assume a fixed pre-trained generative model, perform variational inference to infer a posterior on latent variables given a partial image, and then use this approximate posterior to predict missing pixels. They compare a variety of parameterizations of the variational distribution to HMC inference, and evaluate on MNIST, Celeb-A and the Anime data. \n\nComments\n\nThere are many things about this paper that I don\u2019t understand. My main concern is that I fail to follow why the authors are interested in this task. In what settings would we be interested in performing non-autoencoding variational inference in order to impute missing data? Moreover, in cases where are interested in performing such imputations, what would we like to use the results for? This paper seems like a nice demo, but I\u2019m not entirely convinced I see a compelling application. \n\nMy second concern is about the baselines that are considered. If I were interested in carrying out this inference task, my inclination would not be to run an HMC chain to convergence, but instead to do something like annealed importance sampling (AIS), where at each step I run an iteration of HMC on a large batch of samples on a sequence of target densities that interpolate between the prior and full joint p(x, Z). If computational cost is a concern, I imagine this would not be more expensive than training a density estimator. Moreover, whereas HMC is generally not known to be a good method for estimating marginal likelihoods, AIS methods generally perform much better.\n\nFinally I find the language used in this paper confusing. Cross-coding seems a misnomer for the technique that the authors propose. Isn\u2019t this simply a form of variational inference in which q\u03c8(Z) approximates p\u03b8(\u0396 | x)? The term \u201c-coding\u201d suggests that we somehow define an encoder that accepts the query as input. Moreover, isn\u2019t the XCoder network just a neural density estimator? \n\nFinally, Lemma 1 seems like a really roundabout way of deriving a lower bound. The authors could instead just write:\n\n\tlog p(x)\n\t>=\n\tE_q(Z,Y)[log p(x, Y, Z) - log q(Z, \u03a5)]\n\t=\n\tE_q(Z,Y)[log p(x | \u0396) + log p(Y | Z) + log p(Z) - log q(Z) - log p(\u03a5 | Z)]\n\t=\n\tE_q(Z,Y)[log p(x | \u0396) + log p(Z) - log q(Z)]\n\t=\n\tE_q(Z)[log p(x, \u0396) - log q(Z)]\n\nThis avoids confusing terminology such as cross-coding, and shows that what the authors are doing is in fact just variational inference. Am I missing something here?\n\nI am also confused about how the comparison to HMC is set up. If you\u2019re training q\u03c8(Z), then you presumably need generate a certain number samples at training time. Shouldn\u2019t you add this number of samples number of samples you generate in HMC, in order to get a more apples to apples comparison in terms of the amount of computation performed? As it stands, it is hard to evaluate whether these methods are given a similar number of samples. \n\nFinally, I am not quite sure what to make of the experimental evaluation. We see some scatter plots on MNIST with a 2D latent space, and some faces of celebrities in which there is arguably some sample diversity, although most of this diversity arises in blurry looking hairstyles. However, since the authors condition on the eyes, rather than, say, the nose or mouth, it is hard to know how good a job the network is doing at generalizing to multiple plausible faces. \n\nOverall, I find it difficult to judge the merit of this paper. Is this task in fact hard? Is it useful? Are the results good? Maybe the authors can give us some additional guidance on these questions.\n\nQuestions\n\n- I\u2019m a bit worried that not all the samples that we see in Figure 6 may have equally high probability under the posterior. Could the authors compute and report importance weights?\n\n\tW = p(x, Z) / q(Z)\n\t\n- Could the authors say something about the effective sample size that we obtain when using the learned distribution q(Z) as a proposal? \n\t\n\tESS = (\u03a3_k w^k)^2 / (\u03a3_k (w^k)^2)\n\n- Should it be the case that the ESS is low, and the weights are high variance, could the authors generate a sufficient number of samples to ensure the the ESS = 25 (i.e. the number of images in the figure) and then show the 25 highest-weight samples (or resample 25 images with probability proportional to their weight)?\n\n\t\nMinor \n\n\n- Equation (3): There\u2019s an extra p_\u03b8 in the first integral\n\n- In the proof in Appendix 6.1 \n\n\tKL[ q\u03c8(Z) \u2016 p\u03b8(Z | x) ] + KL[ q\u03c8(Y | Z) \u2016 p\u03b8(Y | Z, x)]\n\nit would be clearer to explicitly denote the expectation over q\u03c8(Z)\n\n\tKL[ q\u03c8(Z) \u2016 p\u03b8(Z | x) ] + E_q\u03c8(Z)[ KL[ q\u03c8(Y | Z) \u2016 p\u03b8(Y | Z, x)] ]\n\t\n(I had to google lecture notes to find out that this expectation is sometimes implicit, which \nas far as I know is not very standard). \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}