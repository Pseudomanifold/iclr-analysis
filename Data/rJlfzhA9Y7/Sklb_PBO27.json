{"title": "More work needs to be done before acceptance.", "review": "In this paper, the authors proposed a competitive, partial observable environment. The authors also use a distributed policy gradient algorithm, together with 4 techniques, to solve this environment. \n\nThe authors don't have any comparison to other algorithms so it's hard to tell whether it outperforms existing algorithms. More related work should also be included in this paper.\n\nFor writing quality, the authors' explanation of their environment is quite clear. However, the author's explanation to policy gradient (i.e., Eq.5 - Eq.9) is confusing. What is R? What is pi(s), as pi is the policy and only pi(a|s) is known? \n\nIn Algorithm 1, the authors use replay buffer to improve its sample efficiency and stability. However, the policy gradient requires the (state, action) pairs must be sampled using the current policy and the distribution of (s, a) in the replay buffer might differ from real distribution. It's very interesting to address this distribution mismatch problem but I didn't see any efforts the authors have put to solve this issue. Acceptance wouldn't be recommended if the authors don't make their points clear. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}