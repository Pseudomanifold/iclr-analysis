{"title": "Interesting and important research questions, unconvincing results", "review": "This paper sets to understand whether pretraining word embeddings for\nprogramming language code by using NLP-like language models has an\nimpact on extreme code summarization task (i.e., generate/predict the\nname of a function based on its body).\n\nI think the paper asks some important questions, however the execution\nof the research and the results presented are not convincing.\n\nI think the area is relevant and the research questions are worth\npursuing; however the work as it is presented in the paper needs\nimprovement to be accepted for publication.\n\nPros:\n* The study of language models for programming language code\n* Pretraining is performed for 3 different languages (C, Java, Python) - target task is in Python\n\nCons:\n* Strange claims of speedup and performance improvement\n* Inconclusive results\n\nSome suggestions for improvement:\n\n* The section on language models pretraining is very sparse, more\n  details are needed.\n\n* The claims of speedup and improvement are strange. Speedup refers to\n  the training speed, I suppose. The performance of the downstream\n  task is never discussed. Only the validation loss is shown and all\n  the performance \"improvement\" is discussed on these graphs, which I\n  found strange. Also, the graphs have their y-axes starting at\n  non-zero values. I personally prefer graphs that start at zero and\n  if there is a need to \"zoom-in\" find a way to \"zoom-in\" to the part\n  of the graph that is important.\n\n* In general the paper writing and reporting on the experiments sounds\n  ad-hoc and not well thought-out.\n\n* I don't agree with many of the explanations in the paper. For\n  example (page 6), it's not true that the extreme summarization task\n  does not require much of the syntactic information (there are\n  submission at the current ICLR'19 that show exactly the opposite,\n  encoding based on syntactic information is useful). The model\n  studied in the paper does NOT use any syntactic information, it\n  treats the code like a sequence of tokens.\n\n* The last question in Section 6 is not a Yes/No question, the answer\n  is phrased as a Yes/No question.\n\nI encourage the authors to pursue the research questions, however in a\nmore systematic and with better methodology.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}