{"title": "Incremental empirical study", "review": "The paper presents some experiments using pre-trained code embeddings on the task of predicting a method name from the code of method body. The paper is well written and the motivations and the design of empirical study are clear.\n\nThe empirical results of validation loss in Figure 1 is reporting the behaviour of random initialization of embedding. From the plots of 10 projects we may derive a couple of claim: (i) the validation loss of random initialization after 10 epochs may increase and get unstable, (ii) random initialization after 5-10 epochs may reach the same loss as pre-trained embeddings. The working assumption is that pre-trained embedding should speed-up the learning process. The empirical results show that it is not just a matter of reducing the training time but also of performance. The discussion is neglecting to comment this behaviour that looks not compliant with the working assumptions. \n\nMinor comment. The reference [Allamanis et al., 2016] is pointing to arxiv.org despite the fact that the work is published as Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}