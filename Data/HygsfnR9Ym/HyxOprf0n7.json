{"title": "Well-presented idea but evaluation seems preliminary", "review": "Revision:\nThe authors have thoroughly addressed my review and I have consequently updated my rating accordingly.\n\nSummary:\nModel-free reinforcement learning is inefficient at exploration if rewards are\nsparse / low probability.\nThe paper proposes a variational model for online learning to backtrack\nstate / action traces that lead to high reward states based on best previous\nsamples.\nThe backtracking models' generated recall traces are then used to augment policy\ntraining by imitation learning, i.e. by optimizing policy to take actions that\nare taken from the current states in generated recall traces.\nOverall, the methodology seems akin to an adaptive importance sampling\napproach for reinforcement learning.\n\nEvaluation:\nThe paper gives a clear (at least mathematically) presentation of the core idea\nbut it some details about modeling choices seem to be missing.\nThe experimental evaluation seems preliminary and it is not fully evident when\nand how the proposed method will be practically relevant (and not relevant).\n\nMy knowledgable of the previous literature is not sufficient to validate the\nclaimed novelty of the approach.\n\nDetails:\nThe paper is well written and easy to follow in general.\n\nI'm not familiar enough with reinforcment learning benchmarks to judge the\nquality of the experiments compared to the literature as a whole.\nAlthough there are quite a few experiments they seem rather preliminary.\nIt is not clear whether enough work was done to understand the effect of the\nmany different hyperparameters that the proposed method surely must have.\n\nThe authors claim to show empirically that their method can improve sample\nefficiency.\nThis is not necessarily a strong claim as such and could be achieved on\nrelatively simple tests.\nIn the discussion the authors claim their results indicate that their approach\nis able to accelearte learning on a variety of tasks, also not a strong claim.\n\nThe paper could be improved by adding a more clear explanation of the exact way\nby which the method helps with exploration and how it affects finding sparse\nrewards (based on e.g. Figure 1).\nIt seems that since only knowledge of seen trajectories can be used to generate\npaths to high reward states it only works for generating new trajectories\nthrough previously visited states.\n\nQuestions that could be clarified:\n- It is not entirely obvious to me what parametric models are used for the\nbacktracking distributions.\n- Does this method not also potentially hinder exploration by making the agent\nlearn to go after the same high rewards / Does the direction of the variational\nproblem guarantee coverage of the support of the R > L distribution by samples?\n- What would be the effect of a hyperparameter that balances learning the recall\ntraces and learning the true environment?\n- Are there also reinforcement learning tasks where the proposed methods'\nimprovement is marginal and the extra modeling effort is not justified (e.g.\ndue to increase complexity).\n\nPage 1: iwth (Typo)\nPage 2: r(s_t) -> r(s_t, a_t)\nPage 6: Prioritize d (Typo)\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}