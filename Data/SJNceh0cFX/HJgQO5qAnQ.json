{"title": "This paper proposes a neural network architectures with locally dense and globally sparse connections. Using dense units a population-based evolutionary algorithm is used to find the sparse connections between modules.", "review": "The problem that the paper tackles is very important and the approach to tackle it id appealing. The idea of regarding the history as a tree looks very promising. However, it\u2019s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events. \n\nUsing neural network if an interesting choice for capturing the influence probability and its timing.\n\nThe authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks? \nThe experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works. This could have made the paper much stronger.\n\nIt was nice that the paper iterated and reviewed the possible inference and learning ways. There is one more way. Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.\n\nThe paper can benefit from a proofreading. There are a few typos throughout the paper such as:\nReference is missing in section 2.1\nPage 2 paragraph 1: \u201can neural attention mechanism\u201d\n\n[1] Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades, AISTATS 2015", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}