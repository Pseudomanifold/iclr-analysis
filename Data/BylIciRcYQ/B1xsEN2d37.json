{"title": "Good theoretical paper", "review": "This paper analyzed the global convergence property of SGD in deep learning based on the star-convexity assumption. The claims seem correct and validated empirically with some observations in deep learning. The writing is good and easy to follow.\n\nMy understanding of the analysis is that all the claims seem to be valid when the solution is in a wide valley of the loss surface where the star-convexity holds, in general. This has been observed empirically in previous work, and the experiments on cifar10 in Fig. 2 support my hypothesis. My questions are:\n\n1. How to guarantee the star-convexity will be valid in deep learning?\n2. What network or data properties can lead to such assumption?\n\nAlso, this is a missing related work from the algorithmic perspective to explore the global optimization in deep learning: \n\nZhang et. al. CVPR'18. \"BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning\".\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}