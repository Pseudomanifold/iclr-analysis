{"title": "Interesting work which makes Gumbel-softmax relaxation work in GAN-based text generation using a relational memory", "review": "Overall:\nThis paper proposes RelGAN, a new GAN architecture for text generation, consisting of three main components: a relational memory based generator for the long-distance dependency modeling, the Gumbel-Softmax relaxation for training GANs on discrete data, and multiple embedded representations in the discriminator to provide a more informative signal\nfor the generator updates.\n\nQuality and Clarity:\nThe paper is well-written and easy to read. \n\nOriginality :\nAlthough each of the components (relational memory, Gumbel-softmax) was already proposed by previous works, it is interesting to combine these into a new GAN-based text generator. \nHowever, the basic setup is not novel enough. The model still requires pre-training the generator using MLE. The major difference are the architectures (relational memory, multi-embedding discriminator) and training directly through Gumbel-softmax trick which has been investigated in (Kusner and Hernandez-Lobato, 2016). \n\nSignificance:\nThe experiments in both synthetic and real data are in detail, and the results are good and significant.\n\n-------------------\nComments:\n-- In (4), sampling is known as non-differentiable which means that we cannot get a valid definition of gradients. It is different to denote the gradient as 0.\n-- Are the multiple representations in discriminator simply multiple \u201cEmbedding\u201d matrices?\n-- Curves using Gumbel-softmax trick + RM will eventually fall after around 1000 iterations in all the figures. Why this would happen?\n-- Do you try training from scratch without pre-training? For instance, using WGAN as the discriminator\n\n\nRelated work:\n-- Maybe also consider to the following paper which used Gumbel-softmax relaxation for improving the generation quality in neural machine translation related?\nGu, Jiatao, Daniel Jiwoong Im, and Victor OK Li. \"Neural machine translation with gumbel-greedy decoding.\" arXiv preprint arXiv:1706.07518 (2017).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}