{"title": "Good paper", "review": "TL;DR. a generalization of the mixup algorithm to any layer, improving generalization abilities.\n\n* Summary\n\nThe manuscript generalizes the mixup algorithm (Zhang et al., 2017) which proposed to interpolate between inputs to yield better generalization. The present manuscript addresses a fairly more general setting as the mixup may occur at *any* layer of the network, not just the input layer. Once a layer is chosen, mixup occurs with a random proportion $\\lambda\\in (0,1)$ (sampled from a $\\mathrm{Beta}(\\alpha,\\alpha)$ distribution).\n\nA salient asset of the manuscript is that it avoids a pitfall of the original mixup algorithm: interpolating between inputs may result in underfitting (if inputs are far from each others: the interpolation may overlap with existing inputs). Interpolating deep layers of the networks makes it less prone to this phenomenon.\n\nA sufficient condition for Manifold Mixup to avoid this underfitting phenomenon is that the dimension of the hidden layer exceeds the number of classes.\n\nI found no flaw in the (two) proofs. Literature is well acknowledged. In my opinion, a clear accept.\n\n* Major remarks\n\n- There is little discussion in the manuscript about which layers should be eligible to mixup and how such layers get picked up by the algorithm. I would suggest elaborating on this.\n- References: several preprints cited in the manuscript are in fact long-published. I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.\n- I find the manifold mixup idea to be closely related to several lines of work for generalization abilities in machine learning (not just for deep neural networks). In particular, I would like to read the authors' opinion on possible connection to the vicinal risk minimization (VRM) framework, in which training data is perturbed before learning, to improve generalization (see, among other references, Chapelle et al., 2000). I feel it would help improve supporting the case of the manuscript and reach a broader community.\n\n* Minor issues\n\n- Tables 1 and 3: no confidence interval / standard deviation provided, diminishing the usefulness of those tables.\n- Footnote, page 4: I would suggest to add a reference to the consistency theorem, to improve readability.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}