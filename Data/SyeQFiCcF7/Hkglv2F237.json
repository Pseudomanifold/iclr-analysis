{"title": "Limited Contribution, Unclear results", "review": "Authors present an adaptation of Capsule Networks for pairwise learning tasks. The pose vectors of final capsule layers for each tower is concatenated and passed through a fully connected layer to calculate the embedding for each tower's input. Then a contrastive loss based on a distance metric is optimized for the embeddings. An interesting regularizer (?) is used which is a dropout based on Capsule activation for connecting last layer capsules to the fully connected layer. \n\nPros:\n\nThe literature review is rich and complete. In general authors explain details of the previous techniques as they use them too which is a good writing technique and improves the readability.\n\nBy utilizing Capsules authors avoid a rigorous preprocessing as it is common with the community. As I understand they do not even use face landmarks to align images.\n\nMeasured by the optimized loss, the proposed method achieves significant improvement upon baseline in the small At&t dataset.\n\nCons:\n\nThe contribution of this work is not on par with ICLR standard for conference papers. Specially since SDropCapsNet (the added dropout) is seems to be auxiliary (gives a slight boost only in LFW without double margin).\n\nThe method used for reporting results is unjustified and not compareable to prior work. For face verification, identification one should report at least an ROC curve based on a threshold on the distance or nearest neighbor identification results which are standards in the literature. Where as they only report the contrastive loss of their model and their own implementation of baselines and Figure 3 which does not clearly show any advantage for CapsNet Siamese networks.\n\n\nQuestion:\nThe architecture description for last layer is vague. In text 512 is mentioned as the input dimmension, 512 is 16*32, Figure 1 shows 9 features per capsule or 3x3 kernels over capsules where it has to be fully connected? Also it says last layer is 128 dimmension where the text implies it should be 20. Could you please explain the role of 20?\n\nIs table 1 the final contrastive loss achievable for each model?\n\nHave you tried just gating the pose parameters of last layer by their activation (multiply to the tanh) rather than using a stochastic dropout?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}