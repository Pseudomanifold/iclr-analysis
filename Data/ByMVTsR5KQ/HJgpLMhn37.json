{"title": "Review - Adversarial Audio Synthesis ", "review": "\n\n*Pros:*\n-\tEasily accessible paper with good illustrations and a mostly fair presentation of the results (see suggestions below).\n-\tIt is a first attempt to generate audio with GANs which results in an efficient scheme for generating short, fixed-length audio segments of reasonable (but not high) quality.\n-\tHuman evaluations (using crowdsourcing) provides empirical evidence that the approach has merit.\n-\tThe paper appears reproducible and comes with data and code.\n\n*Cons*:\n-\tPotentially a missing comparison with existing generative methods (e.g. WaveNet). See comments/questions below ** \n-\tThe underlying idea is relatively straightforward in that the proposed methods is a non-trivial application of already known techniques from ML and audio signal processing.\n\n*Significance*: The proposed GAN-based audio generator is an interesting step in the development of more efficient audio generation and it is of interest to a subcommunity of ICLR as it provides a number of concrete techniques for applying GANs to audio.\n\n*Further comments/ questions:*\n-\tAbstract/introduction: I\u2019d suggest being more explicit about the limitations of the method, i.e. you are currently able to generate short and fixed-length audio.\n-\tSpecGAN (p 4): I\u2019d suggest including some justification of the chosen pre-processing of spectrograms (p. 4, last paragraph). \n-\t** Evaluation:  The paper dismisses existing generative methods early in the evaluation phase but the justification for doing so is not entirely clear to me: Firstly, if the inception score is used as an objective criterion it would seem reasonable to include the values in the paper. Secondly, as inception scores are based on spectrograms it could potentially favour methods using spectrograms directly (SpecGAN) or indirectly (WaveGAN, via early stopping) thus putting the purely sample based methods (e.g. WaveNet) at a disadvantage. It would seem fair to pre-screen the audio before dismissing competitors instead of solely relying on potentially biased inception scores (which was probably also done in this work, but not clearly stated\u2026)? Finally, while not the aim of the paper, it would have been beneficial to discuss and understand the failures of existing methods in more detail to convince the reader that a fair attempt has been made to getting competitors to work before leaving them out entirely. \n-\tResults/analysis: It is unclear to me how many people annotated the individual samples? What is the standard deviation over the human responses (perhaps include in tab 1)? Consider including a reflection on (or perhaps even test statistically) the alignment between the qualitative diversity/quality scores and the subjective ratings to justify the use of the objective scores in the training/selection process.\n-\tRelated work: I think it would provide a better narrative if the existing techniques are outlined earlier on in the paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}