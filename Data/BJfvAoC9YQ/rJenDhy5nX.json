{"title": "Continual learning approach with increasing computational cost over time", "review": "This paper proposes a continual learning approach which transforms intermediate representations of new data obtained by a previously trained model into new intermediate representations that are suitable for a task of interest.\nWhen a new task and/or data following a different distribution arrives, the proposed method creates a new transformation layer, which means that the model\u2019s capacity grows proportional to the number of tasks or data sets being addressed over time. Intermediate data representations are stored in memory and its size also grows.\nThe authors have demonstrated that the proposed method is robust to catastrophic forgetting and it is attributed to the feature transformation component. However, I\u2019m not convinced by the experimental results because the proposed method accesses all data in the past stored in memory that keeps increasing infinitely. The authors discuss very briefly in Section 5.2 on the performance degradation when the memory size is restricted. In my opinion, the authors should discuss this limitation more clearly on experimental results with various memory sizes.\n\nThe proposed approach would make sense and benefit from storing lower dimensional representations of image data even though it learns from the entire data over and over again.\nBut it is unsure the authors are able to claim the same argument on a different type of data such as text and graph.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}