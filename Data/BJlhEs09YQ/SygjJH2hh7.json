{"title": "Recovery of true weights for a special class of CNNs by reduction to tensor decomposition", "review": "Summary:\n\nThis paper analyzes the problem of learning a very special class of CNNs: each layer consists of a single filter, applied to non-overlapping patches of the input. Specifically, the task is: given data generated from a planted CNN of this kind with gaussian inputs, recover the ground truth weights. In the main result (Theorem 4.8), the authors show that the true weights can be approximately recovered, with high probability, by applying rank-one tensor decomposition to a special tensor derived from the generated data. Importantly, achieving a good approximation requires (a) observing more examples than the total number of network parameters, and (b) (somewhat confusingly) the kernels must be large relative to the square of the network depth.\n\nReview:\n\nThe paper is clearly written. The results seem mathematically interesting and substantial. The connection to practical deep CNNs may be overstated however.  The nature of the result is ultimately that this very special CNN learning problem is really tensor decomposition in disguise. But why does the reduction in this special case teach us something fundamental about CNN training? One approach to addressing this issue would be to demonstrate that this class of networks can achieve reasonable performance on a baseline task (hence captures something important about the general case). However, this evaluation is not given.\n\nComments:\n\n1. It seems that a stronger result than 4.7 (hence 4.8) is true. Looking at (A.14) on p. 14, I believe you should have that for $i=1, \\dotsc, p$, $\\phi_{\\i(i)}^\\prime (x)$ are iid copies, since $x \\sim N(0, I)$. Thus, the vector $v$ itself is a constant, and the population tensor $T$ is rank-one.  So, if you define instead $\\alpha_{CNN} = \\EE[\\phi_{\\i(1)}^\\prime(x)]$, you get $T = L_{CNN}$. If I'm confused, can the authors please help clear up my confusion?\n\n2. In order for the bound in 4.8 to be meaningful, you must have $D^2 \\leq d_\\min$. But since $d_\\min \\leq p^{1/D}$, this implies awkwardly that the input dimension be exponential in the depth of the network: $p \\geq D^{2D}$.  This should be mentioned, as it significantly challenges the claim that the results apply to arbitrarily deep networks.\n\n3. Can the authors comment on the challenges for extending these results, say to multiple kernels per layer, or non-gaussian inputs? The latter seems hard, but maybe the former translates to higher rank tensor decomposition?\n\n4. Can the authors include a line in the numerical experiment figures indicating the lower bound from 4.8? This way it is possible to judge how sharp the result is.\n\n5. Perhaps a different activation than ReLU should be used in the numerical experiments, since it violates the smoothness assumption.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}