{"title": "Interesting direction, but lacks practical relevance", "review": "==== Summary ====\n\nThis paper proposes a learning method for a restricted case of deep convolutional networks, where the convolutional layers are limited to the non-overlapping case (receptive field = stride) and having just a single output channel per layer. Additionally, the method is analyzed only for the case of normally distributed inputs and realizable settings, i.e., there exists some set of weights that correctly solve the learning problem. The proposed method is based on a rank-1 tensor approximation to a tensor \u201csummary\u201d of the training set and is proven to result in a good estimation to the ground-truth parameters when the number of samples exceeds the number of parameters.\n\n==== Detailed Review ====\n\nThe paper is well written, easy to follow, and I generally liked reading it. More specifically, I have found the method of aggregating the training set into a tensor that directly reflects the parameters of the model to be quite elegant. Though the general learning scheme is quite similar to previous works (see below), the way it leverages tensor decomposition is unique to the best of my knowledge. Additionally, unlike previous works which dealt with \"shallow\" non-linear models (e.g., neural networks with a single hidden layer), this work manages to address the learning of deep non-linear models, at least to some degree. Despite the above, the paper has two significant shortcomings:\n\n1. The results on their own are limited to a very restrictive case of neural networks that has little to no practical relevance. While it is known that non-overlapping ConvNets are significantly less expressive than overlapping ones [1], they can at least approximate any function if given a sufficient number of hidden channels [2], but the restriction to a single channel per layer means that they can represent only a limited set of functions. I would have expected the paper to include a short discussion on these facts given its subject. The lack of relevance to practical networks would not be that bad if the paper included a sketch of how DeepTD could be extended to more complex networks. It would be helpful if the authors would comment on how might this method could be generalized to these cases.\n\n2. The novelty of the method and the strength of the theoretical results are a bit lacking. First, the use of Stein\u2019s lemma with E[y * x] for the estimation of the parameters of a non-linear model was used before in [3,4], and moreover in both cases they were able to extend their results to non-gaussian inputs and more complex models using higher moments, i.e. E[y * x * x] and E[y * x * x * x]. The submitted paper does cite these papers but does not directly discuss the similarities between them. While these works also rely on tensor decompositions and Stein's theorem,  DeepTD makes a different and novel use of tensor decompositions, but it does so in a way that is highly tuned to non-overlapping single-channel networks (i.e., the tensor T(x) is shaped according to the size of the kernels), and does not seem to be easily extendable to other settings. Additionally, the actual guarantees of theorem 4.8 show that for non-linear networks the bound on the error does not decrease to zero (even with infinite samples). Moreover, it seems that under common settings the resulting bound is meaningless (i.e., a negative lower bound for a positive quantity). I actually found it interesting that despite this bound, the reported empirical results were actually quite good \u2014 perhaps there is more to uncover here and maybe this method has potential beyond what can currently be proven. The only case where it seems to be relevant is when the activations are linear (or with smoothing factor so low which they are practically linear). However, under this trivial case the population tensor T = E[y*x] = E[grad(f_cnn)] is precisely the rank-1 tensor of the kernels, which makes me think that perhaps DeepTD can merely be seen as a kind of linear relaxation to the learning problem. The general point here that though there is a clear distinction between DeepTD and prior works, the delta is not significant, and though the results seem promising, they do not seem fully fleshed out.\n\nIn conclusion, while I do like the general direction that the paper takes with learning the parameters of a neural network, and it does seem like it could lead to exciting results in the future, I do not think it is ready for publication in its current form. Nevertheless, I strongly encourage the authors to keep developing this work (e.g., extend it to multi-channel networks, improve the bounds, drop gaussian input assumption).\n\n[1] Sharir et al. On the Expressive Power of Overlapping Architectures of Deep Learning. ICLR 2018. \n[2] Cohen et al. On the Expressive Power of Deep Learning: A Tensor Analysis. COLT 2016. \n[3] Janzamin et al. Beating the Perils of Non-Convexity:\nGuaranteed Training of Neural Networks using Tensor Methods. Arxiv preprint.\n[4] Sedghi et al. Provable Tensor Methods for Learning Mixtures of Generalized Linear Models. AISTATS 2016.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}