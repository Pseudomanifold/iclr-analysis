{"title": "The paper analyze connection between RNN and TT decomposition by incorporating nonlinearity. The theoretical results are very interesting while novelty is limited. ", "review": "This paper extends the work of TT-RRN [Khrulkov et al., 2018] to further analyze the connection between RNN and TT decomposition by incorporating generalized nonlinearity, i.e., RELU, into the network architectures. Specifically, the authors theoretically study the influence of generalized nonlinearity on the expressivity power of TTD-based RNN, both theoretical result and empirical validation show that generalized TTD-based RNN is more superior to CP-based shallow network in terms of depth efficiency. \nPros:\n1. This work is theoretically solid and extend the analysis of TT-RNN to the case of generalized nonlinearity, i.e. ReLU.\n\n2. The paper is well written and organized.\n \nCons:\n1. The contribution and novelty of this paper is incremental and somehow limited, since the analysis TT-RNN based on the product nonlinearity already exists, which make the contribution of this paper decreased.\n\n2. The analysis is mainly on the particular case of rectifier nonlinearity. I wonder if the nonlinearities other than the RELU hold the similar properties? The proof or discussion on the general nonlinearities is missing.\n\nOther comments:\n1. The authors said that the replacement of standard outer product with its generalized version leads to the loss of conformity between tensor networks and weight tensors, the author should clarify this in a bit more details.\n\n2. The theoretical analysis relies on grid tensor and restricts the inputs on template vectors. It is not explained why to use and how to choose the those template vectors in practice?\n\n3. A small typo: In Figure 2, \u2018m' should be \u2018M'", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}