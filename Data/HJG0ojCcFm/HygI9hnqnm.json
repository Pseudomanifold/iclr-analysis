{"title": "Interesting exploration into RL for negotiation in coalition games ", "review": "This paper develops a reinforcement learning approach for negotiating coalitions in cooperative game theory settings.  The authors evaluate their approach on two games against optimal solutions given by the Shapley value.\n\nThe work builds upon a substantial and growing literature on reinforcement learning for multiagent competitive and cooperative games. The most novel component of the work is a focus on the process of negotiation within cooperative coalition games. The two game environments studied examine a \"propose-accept\" negotiation process and a spatial negotiation process.\n\nThe main contribution of the work is the introduction of a reinforcement learning approach for negotiation that can be used in cases where unlimited training simulations are available.  This approach is a fairly straightforward application of RL to coalition games, but could be of interest to researchers studying negotiation or multiagent reinforcement learning, and the authors demonstrate the success of RL compared to a normative standard.\n\nMy primary concerns are:\n- The authors advertise the work as requiring no assumptions about the specific negotiation protocol, but the learning algorithms used are different in the two cases studied, so the approach does require fine-tuning to particular cases.\n- Maybe I missed it, but how many training games are required?\n- In what real applications do we expect this learning algorithm to be useful?  \n- The experiments where the RL agents are matched against bots include training against those specific bot types. How does the trained algorithm perform when matched against agents using rules outside its training set?  \n- Since the Shapley value is easily computable in both cases studied.  If the bots are all being trained together, why wouldn't the bots just use that to achieve the optimal solution?\n- Why are only 20 game boards used, with the same boards used for training and testing?  How do the algorithms perform on boards outside the training set?\n\nOverall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}