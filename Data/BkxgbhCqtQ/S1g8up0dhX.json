{"title": "Variational inference with discrete distribution for uncertainty estimation", "review": "This paper proposes runs variational inference with discrete mean-field distributions. The paper claims the proposed method is able to give a better estimation of uncertainty from the model. \n\nRating of the paper in different aspects ( out of 10)\nQuality 6, clarify 5, originality 8, significance of this work 5 \n\nPros: \n\n1. The paper proposes a generic discrete distribution as the variational distribution to run inference for a wide range of models. \n\nCons:\n\n1. When the method begins to use mean-field distributions, it begins to lose fidelity in approximating the posterior distributions. Even the model is able to do a good job in approximating marginal distributions, it is hard to evaluate whether the model is gaining benefit overall. \n\n2. I don't see a strong reason for using discrete distributions. In one dimensional space, a distribution can be approximated in different ways. Using discrete distributions only increases the difficulty of reparameterization. \n\n3. In the experiment evaluation, the algorithm seems only marginally outperforms competing methods. \n\n\nDetailed comments: \n\nIn the motivation of the paper, it cites low-precision neural networks. However, low-precision networks are for a different purpose -- small model size and saving energy. \n\nequation 6 is not clear to me.\n\nIn equation 10, how are these conditional probabilities parameterized? Is it like: z ~ Bernoulli( sigmoid(wz) ) ?\n\nIt is nice to have a brief introduction of the evaluation measure SGR. \n\nIn table 3, 1st column, the third value seems to be the largest, but the fourth is bolded. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}