{"title": "Interesting theoretical results, but connection to the experimental results is not clear", "review": "In many machine learning applications, sorting is an important step such as ranking. However, the sorting operator is not differentiable with respect to its inputs. The main idea of the paper is to introduce a continuous relaxation of the sorting operator in order to construct an end-to-end gradient-based optimization. This relaxation is introduced as \\hat{P}_{sort(s)} (see Equation 4). The paper also introduces a stochastic extension of its method \nusing Placket-Luce distributions and Monte Carlo. Finally, the introduced deterministic and stochastic methods are evaluated experimentally in 3 different applications: 1. sorting handwritten numbers, 2. Quantile regression, and 3. End-to-end differentiable k-Nearest Neighbors.\n\nThe introduction of the differentiable approximation of the sorting operator is interesting and seems novel. However, the paper is not well-written and it is hard to follow the paper especially form Section 4 and on. It is not clear how the theoretical results in Section 3 and 4 are used for the experiments in Section 6. For instance:\n** In page 4, what is \"s\" in the machine learning application?\n** In page 4, in Equation 6, what are theta, s, L and f exactly in our machine learning applications?\n\nRemark: \n** The phrase \"Sorting Networks\" in the title of the paper is confusing. This term typically refers to a network of comparators applied to a set of N wires (See e.g. [1])\n** Page 2 -- Section 2 PRELIMINARIES -- It seems that sort(s) must be [1,4,2,3].\n\n[1] Ajtai M, Koml\u00f3s J, Szemer\u00e9di E. An 0 (n log n) sorting network. InProceedings of the fifteenth annual ACM symposium on Theory of computing 1983 Dec 1 (pp. 1-9). ACM\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}