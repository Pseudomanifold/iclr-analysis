{"title": "Promising results but proposed framework is not general enough for a Riemannian manifold and seems wrong", "review": "1.\tSome motivation of extending the adversarial examples generation on manifold should be there.\n2.\tEven if \\epsilon is small, if x is on a manifold, x+\\epsilon may not, so I am not sure about the validity of the definition in Eq. (7) and what follows from here. One solution is putting the constraint d(x, Exp_x(\\epsilon)) \\leq \\sigma, which implies that g(\\epsilon, \\epsilon) \\leq \\sigma. \nAlso, x and \\epsilon lies in completely different space, \\epsilon should lie on the tangent space at x. So, I don\u2019t understand why x+\\epsilon makes sense? It makes the rest of the formulation invalid as well.\n3.\tI don\u2019t understand why in Eq. (12), d(x, x+\\epsilon)^2 = |m(x)|? Do authors want it to be equal, otherwise, I can not see why this equality is true.\n4.\tIn Lemma 2.3, please make H in \\mathbb{R}^{n\\times n} instead of \\mathbb{R}^n \\times \\mathbb{R}^n (same issue for Lemma 2.4), later does not make sense in this context. Also, why not write |H|=U|\\Sigma|U^T, instead of what you have now. \n5.\tNo need to prove Lemma 2.3 and 2.4. These are well-known results in matrix linear algebra.\n6.\tIt\u2019s nice that the authors generalize to l_p ball and can show FGSM as a special case.\n7.\tSome explanation of Algo. 2 should be there in the main paper given that it is a major contribution in the paper and also authors put a paper more than 8 pages long, so as a reader/ reviewer I want more detailed explanation in the main body.\n8.\tIn Algorithm 1, step 7: \u201cUpdate the parameters of neural network with stochastic gradient\u201d should be updated in the negative direction of gradient.\n9.\tAlgorithm 2 is clearly data driven. So, can authors comment on special cases of Algorithm 2 when we explicitly know the Riemannian metric tensor, e.g., when data is on hypersphere.\n10.\tCan authors comment on the contemporary work https://arxiv.org/pdf/1807.05832.pdf, as the purpose is very similar.\n11.\tThe experimental validation is nice and showed usefulness of the proposed method.\n\n\nPros:\n1.\tA framework to show the usefulness of non-Euclidean geometry, specifically curvature for adversarial learning.\n2.\tNice set of experimental validation.\n\nCons:\n1.\tSome theorems statement can be ignored to save space, e.g., Lemma 2.3 and 2.4. And instead, need some explanation of Algorithm 2 in the main text. Right now, not enough justification of additional page.\n2.\tNot sure about the validity of the main formulation, Eq. (7) and other respective frameworks when data x is on a manifold.\n\nMinor comments:\n1.\tIn page 2, \u201cIn this case, the Euclidean metric would be not rational.\u201d-> \u201cIn this case, the Euclidean metric would not be rational\u201d.\n2.\t \u201cHowever, in a geometric manifold, particularly in Riemannian space, the gradient of a loss function unnecessarily presents the steepest direction.\u201d Not sure what authors meant by \u201cunnecessarily presents\u201d\n3.\tNo need to reprove Lemma 2.2, just give reference to a differential geometry textbook like Chavel or Boothby.\n\nI want the authors to specifically address the cons. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}