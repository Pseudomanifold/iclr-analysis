{"title": "interesting idea and good results", "review": "This paper studied data poisoning attacking for graph neural networks. The authors proposed treating graph structures as hyperparameters and leveraged recent progress on meta-learning for optimizing the adversarial attacks. Different from some recent work on adversarial attacks for graph neural networks (Zuigner et al. 2018; Dai et al. 2018), which focus on attacking specific nodes, this paper focuses on attacking the  overall performance of graph neural networks. Experiments on a few data sets prove the effectiveness of the proposed approach. \n\nStrength:\n- the studied problem is very important and recently attracting increasing attention\n- Experiments show that the proposed method is effective.\n\nWeakness:\n- the complexity of the proposed method seems to be very high\n- the data sets used in the experiments are too small\nDetails:\n-- the complexity of the proposed method seems to be very high. The authors should explicitly discuss the complexity of the proposed method. \n-- the data sets in the experiments are too small. Some large data sets would be much more compelling.\n-- Are the adversarial examples identified by the proposed method transferrable to other graph embedding algorithms (e.g., the unsupervised node embedding methods, DeepWalk, LINE, and node2vec)?\n-- I like Figure 3, though some concrete examples would be more intuitive. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}