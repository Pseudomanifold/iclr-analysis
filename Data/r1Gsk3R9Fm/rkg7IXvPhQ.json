{"title": "Good paper but it is not clear what is the main ingredient behind its success ", "review": "Summary:\n\nThis paper proposes layer wise training of neural networks using classification auxiliary tasks for training each layer. Experiments are presented on CIFAR10 and Imagenet. Accuracies close to end to end training are obtained. \n\nThe layer wise training is repeated for J steps, the auxiliary tasks are trained on top of the shallow one layer (of width M ) with a network  of depth k and width tilde{M}. Layerwise training is done using sgd with momentum, and the learning rate is decayed through epochs. \n\nNote that the layer wise training is done with large width M than typical end to end networks in use. \n\nThe authors argue and test the hypothesis that auxiliary tasks  encourage the linear separability of CNN features. \n\nTo reduce the size of the learned network the author propose a layer wise compression using the filter removal technique of Molchanov et al .\n\nReproducibility:\n\nThis empirical  work has been investigated for a while with mild success, the authors should make their code available to the community to confirm and reproduce  their findings.  I encourage the authors to make their code available during the review/discussion period. \n\n\nSignificance of the work:\n\nFrom reading the paper it is not clear what is the main ingredient that makes this layer wise training  successful, negative results would help in understanding what is important for the accuracy. \n\nSome more ablation studies and negative results will be insightful and here are few suggestions in that direction:\n\n- Authors claim that they used invertible downsampling as max or average pooling  lead to information loss. Does the layer wise training give worst results with average or max pooling? If so please report those numbers to know what is the implications of this choice of pooling.  \n\n- On the width of the networks, seems it is key for the success of the approach.  What if  you train wider networks with J that is small? (  J=3 for instance but much  wider networks, instead of J=8 now for imagenet.)\n\n- To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks (smaller M )?\n\n- Would the accuracy  with the layer wise training reach a  plateau if one uses an architecture with J higher than 8? \n\n- Transferability of the learned features: end to end features are know to be transferable. It would be good to see if this still holds using the network layer wise trained on imagenet for CIFAR10 or other datasets. \n\nOther Questions:\n- Section 3.2 is vague. In Proposition 3.1 and  Proposition 3.2 can you add some text to explain what are the implicitions of the claims? \u201cThus our training permits to extend some results on shallow CNN to deeper CNNs \u2026\u201d which shallow results ?\n\n- \u201cFor k>1 batch normalization was useful \u201c is this only on the auxiliary problems networks  or you used also batch norm for the layer ?\n\n- The ensemble used is Z=\\sum_{j=1}^J 2^j z_j , this uses the network of J layers ,  also the O(J) auxiliary networks  of depth k. Please report the number of parameters for all models (single and ensembles) in Table 1 and Table 2. \n\n- In the conclusion: \u201cThe framework can be extendable to the parallel training \u2026\u201d how would this possible since one needs the output of the first training to do the training of the next layer. can you elaborate on what is meant here?\n\nMinor:\npage 2 bottom have competitorsand -> have competitors and \nthe non linearity rho in equation 1 and throughout the paper put a bracket for its argument \\rho(x) not \\rho x\nPage 6 , Imagenet paragraph : W \u2014> We\nsection 4.2 we define linear separability etc\u2026 a space is missing before Further \nsection 4.3 we report our results .. (Imagenet) a space is missing after ImageNet)\n\nOverall Assessment: \nThis is a good paper, making the code available and adding more ablation studies and explanations of width versus depth and the choice of pooling will make the contributions easier to understand. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}