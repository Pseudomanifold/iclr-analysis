{"title": "decent experiments, limited novelty", "review": "This paper is of reasonable quality and clarity, rather modest originality, perhaps considerable significance in some applications.\n\nStrengths:\n- I think this kind of method could be useful for data of very high dimensionality, when it is not possible to train everything end to end.\n- The experiments seem to be conducted correctly.\n- The paper is well written.\n\nWeaknesses:\n- (minor) Abstract: it's kind of funny to say that CIFAR-10 is a large scale image recognition problem.\n- What the authors are proposing is quite similar to Lee et al. [1], which was not mentioned in the paper as well as a wide range of papers, which were mentioned. I think it is kind interesting for people to revise these techniques from 10 years ago, but this method is just not that novel.\n- The authors highlight that their goal is not using this method as a pre-training strategy, but it would be interesting to see whether it would indeed work better if after the layer-wise training, the whole network would be trained end-to-end.\n- Maths in this paper is mostly decorative. \n- When comparing different models or training methods (e.g. layer-wise trained AlexNet and end-to-end trained AlexNet), it would make sense to do some hyperparameter search. It is very risky to conclude anything otherwise.\n- I would like to see a wall clock time comparison between this and end-to-end training.\n\n[1] Lee et al. Deeply-Supervised Nets. 2015.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}