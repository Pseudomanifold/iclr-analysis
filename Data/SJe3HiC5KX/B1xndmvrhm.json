{"title": "Intriguing formulation and performance; flaws in experiments", "review": "Pros:\n- Paper proposes a somewhat complicated but easy to understand idea for open set classification. Formulation is quite intriguing.\n- Outperforming recent baselines on most scenarios, despite being a linear classifier on fixed CNN features.\n\nCons:\n- Experiment setup somewhat flawed (but the same flaw is in prior work too)\n    To elaborate: DeCAF7 is trained on ImageNet, which gives the underlying network extra categorical information of the 1000 classes. Some of these clases are arguably in the \"unknown classes\" in the open set setting. This may jeopardize the premise since the feature knows those classes are semantically different from known classes. Unfortunately (Busto & Gall, 2017) and (Saito et al., 2018) do this too.\n    This is especially problematic since DeCAF7 has a near-linear relationship to the final sigmoid logits, which are the 1000-way ImageNet class scores. This makes the authors formulation (separate subspaces for known and unknown classes) more easily exploit this leaked information. This is because the 1000-way scores obviously have subspaces for all 1000 ImageNet classes, and by extension, the \"known\" and \"unknown\" classes in the open set setting. \n    If this is true and is the main reason that the proposed method outperforms, I would not consider the conclusion of the paper very informative. Instead, its signifies the need of a better experiment setup for the problem.\n    A way to strengthen the paper is to use a network pre-trained on other datasets (e.g. Places, or a subset of ImageNet) to verify the findings of the paper.\n- Lacks clarity for what is being done at test time. \n    I cannot find whether the final SVM is trained on original DeCAF features, or S and T. If it is the latter, how are the representations of target domain data obtained at test time? Are they d dimentional or 2d dimentional?\n    Can you clarify that the test samples are not used for unsupervised training?\n- Experiment elaborate but feels incomplete.\n    It feels like the authors are proposing 3 variations of the method, and there is not one of them that consistently outperform the others. If so, the paper would lack some ablation analysis that provides insights of what makes the FRODA-SVM outperform prior art. For example, how much do the hyperparameters matter? What happens if e.g. d or lambda1 is very large/small?\n\nClarity:\n- Abstract spends too much time on defining problem setup\n- \"Faster than prior work\" refers to the training time, and excludes the DeCAF feature extraction.\n\nOriginality:\nI am not familiar with the related work.\n\nSignificance:\nIt is quite impressive that a linear model on fixed CNN activations outperforms prior art. However, see the first point in the cons.\n\n\n-----------\nEdit: most of the issues listed in \"cons\" are addressed. Although the additional experiments are not very comprehensive, they can better support the claims. I am bumping up the rating to 7.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}