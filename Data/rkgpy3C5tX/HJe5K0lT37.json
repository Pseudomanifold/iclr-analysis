{"title": "Limited baselines in comparison, cost not clear", "review": "The authors proposed a meta-learning approach which amortizes hierarchical variational inference across tasks, learning an initial variational distribution such that, after a few steps of stochastic optimization with the reparametrization trick, they obtain a good task-specific approximate posterior. The optimization is performed by applying backpropagation through\ngradient updates. Experiments on a contextual bandit setting and on miniImage net show how the proposed approach can outperform a baseline based on the method MAML. Although in miniImagenet the proposed method does not produce\ngains in terms of accuracy, it does produce gains in terms of uncertainty estimation.\n\nQuality:\n\nThe derivation of the proposed method is rigorous and well justified. The experiments performed show that the proposed method can result in gains. However, the comparison is only with respect to MAML and other techniques could have also be included to make it more meaningful. For example,\n\nGordon, Jonathan, et al. \"Decision-Theoretic Meta-Learning: Versatile and\nEfficient Amortization of Few-Shot Learning.\" arXiv preprint arXiv:1805.09921\n(2018).\n\nor the methods included in the related work section, or Garnelo et al. 2018.\n\nThe authors do not comment on the computational cost of the proposed method.\n\nClarity:\n\nThe paper is clearly written and easy to read.\n\nNovelty:\n\nThe proposed method is new up to my knowledge. This is one of the first methods to do Bayesian meta-learning.\n\nSignificance:\n\nThe experimental results show that the proposed method can produce gains. However, because the authors only compare with a non-Bayesian meta-learning method (MAML), it is not clear how significant the results are. Furthermore, the computational cost of the proposed method is described well enough.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}