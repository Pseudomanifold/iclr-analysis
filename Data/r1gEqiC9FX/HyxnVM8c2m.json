{"title": "Good method and theoretical contribution, but not convincing results", "review": "The authors propose a new regularization method for neural networks. The main idea is to reparametrize the neural network after each update by rescaling the weights, without changing the encoded function. The proposed algorithm is proved to converge to a unique canonical representation of the weights. \n\nThe paper is clear and well written. The proof structure in the appendix seems coherent even though I haven't checked all the details.  Moreover, the authors detail the application of the method to all the different building blocks of modern architectures. \n\nUp to my knowledge, the idea is novel. Moreover, it can have a high impact on the robustness of training. However, the results are somewhat disappointing.  While the authors present the method as an alternative to batch normalization, most of the reported results show a better performance for BN. \n\nOne of the drawbacks of batch normalization is it's incompatibility with other regularizers such as Dropout. Did the authors try to combine ENorm with Dropout? \nAnother direction that can be worth to investigate, in the same space of improving the robustness of training, is to try to combine this reparametrization with natural gradient updates. \nAnother question that remains open is the following: Even though the algorithm converges to a unique minimizer, it is not guaranteed that the obtained minimizer is good. Indeed, the authors note in their discussion that the criterion they optimize might be not optimal.   \n\nTo summarize, the idea and theoretical contribution is significant, but the work can be improved.\n\n==================\nAfter rebuttal\n==================\nThe authors provided new experiments supporting the proposed method. I am happy to increase my rating. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}