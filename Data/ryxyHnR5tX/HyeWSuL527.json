{"title": "This paper shows how to accelerate certain popular sparse recovery approaches under certain conditions. However, the contributions seem to be incremental and it is unclear how the technique significantly advance the state of the art.", "review": "Clarity: Paper is generally well written; however, certain theoretical statements (e.g. Theorem 1) are not very precise.\n\nOriginality: Contribution seems to be incremental; the proposed method seems to be a straightforward concatenation of well-known existing results in sparse recovery and nearest-neighbor search.\n\nSignificance: Unclear whether the techniques significantly advance the state of the art.\n\nQuality: Overall, I think this is a promising direction but the idea might not have fully fleshed out.\n\n----\nSummary: \nthe paper proposes a scheme to accelerate popular sparse recovery methods that rely on hard thresholding (specifically, CoSaMP and IHT, but presumably other similar methods can also be used here). The key idea is that if the measurement matrix is normalized, then the k-sparse thresholding of the gradient update can be viewed as solving a k-nearest neighbor problem. Therefore, one can presumably use fast k-NN methods instead of exact NN methods. Specifically the authors propose to use the prioritized DCI method of Li and Malik.\n\nPros: \nreasonable idea to use fast (sublinear) NN techniques in the k-sparse projection step.\n\nCons: \n* It appears that the running time improvement over the baseline IHT (which has Otilde(mn) complexity) heavily depends on the intrinsic dimensionality of A. However, the authors do not characterize this.\n* The authors neglect to mention in the paper that prioritized DCI has a pre-processing time of O(mn), so the final algorithm isn't really asymptotically faster.\n* I cannot parse Theorem 1 (especially, the second sentence). Is epsilon the failure probability of DCI?\n* Experimental results are far too synthetic. In real-life problems k itself is big, so there may be other bottlenecks (least squares, gradient updates, etc) and not necessarily the hard thresholding step.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}