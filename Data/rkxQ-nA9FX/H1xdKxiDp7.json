{"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "review": "* Strengths:\n- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.\n- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.\n\n* Weaknesses:\n- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4\n- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper. However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.\n- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting. I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.\n\nOverall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}