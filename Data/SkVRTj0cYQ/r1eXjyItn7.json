{"title": "Well-motivated problem, but incremental improvement over previous work?", "review": "[Post-rebuttal update] No author response was provided to address the reviewer comments. In particular, the paper's contributions and novelty compared with previous work seem limited, and no author response was provided to address this concern. I've left my overall score for the paper unchanged.\n\n[Summary] The authors propose a protocol for training a model over private user data in a federated setting. In contrast with previous approaches which tried to ensure that a model would not reveal too much about any individual data point, this paper aims to prevent leakage of information about any individual client. (There may be many data points associated with a single client.)\n\n[Key Comments] The submission generally seems polished and well-written. However, I have the impression that it's largely an incremental improvement over recent work by McMahan et al. (2018).\n* If the main improvement of this paper over previous work is the dynamic adaptation of weight updates discussed in Section 3, the experimental results in Table 1 should compare the performance of the protocol with vs. without these changes. Otherwise, I think it would be helpful for the authors to update the submission to clarify their contributions.\n* Updating Algorithm 1 / Line 9 (computation of the median weight update norm) to avoid leaking sensitive information to the clients would also strengthen the submission.\n* It would also be helpful if the authors could explicitly list their assumptions about which parties are trusted and which are not (see below).\n\n[Details]\n[Pro 1] The submission is generally well-written and polished. I found the beginning of Section 3 especially helpful, since it breaks down a complex algorithm into simple/understandable parts.\n\n[Pro 2] The proposed algorithm tackles the challenging/well-motivated problem of improving federated machine learning with strong theoretical privacy guarantees.\n\n[Pro 3] Section 6 has an interesting analysis of how the weight updates produced by clients change over the course of training. This section does a good job of setting up the intuition for the training setup used in the paper, where the number of clients used in each round is gradually increased over the course of training.\n \n[Con 1] I had trouble understanding the precise threat model used in the paper, and I think it would be helpful if the authors could update their submission to explicitly list their assumptions in one place. It seems like the server is trusted while the clients are not. However, I was unsure whether the goal was to protect against a single honest-but-curious client or to protect against multiple (possibly colluding) clients.\n\n[Con 2] During each round of communication, the protocol computes the median of a set of values, each one originating from a different client, and the output of this computation is used to perform weight updates which are sent back to the clients. The authors note that \"we do not use a randomized mechanism for computing the median, which, strictly speaking, is a violation of privacy. However, the information leakage through the median is small (future work will contain such privacy measures).\" I appreciate the authors' honesty and thoroughness in pointing out this limitation. However, it does make the submission feel like a work in progress rather than a finished paper, and I think that the submission would be a bit stronger if this issue was addressed.\n\n[Con 3] Given the experimental results reported in Section 4, it's difficult for me to understand how much of an improvement the authors' proposed dynamic weight updates provide in practice. This concern could be addressed with the inclusion of additional details and baselines:\n* Few details are provided about the model training setup, and the reported accuracy of the non-differentially private model is quite low (3% reported error rate on MNIST; it's straightforward to get 1% error or below with a modern convolutional neural network). The authors say they use a setup similar to previous work by McMahan et al. (2017), but it seems like that paper uses a model with a much lower error rate (less than 1% based on a cursory inspection), which makes direct comparisons difficult.\n* The introduction argues that \"dynamically adapting the dp-preserving mechanism during decentralized training\" is a significant difference from previous work. The claim could be strengthened if the authors extended Table 1 (experimental results for differentially private federated learning) in order to demonstrate the effect of dynamic adaptation on model quality.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}