{"title": "interesting direction but confusing presentation", "review": "The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. \n\nSome of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:\n1.\tThe main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record \u2013 why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork & Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?\n2.\tAnother issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): https://www.aha.org/statistics/fast-facts-us-hospitals. Therefore, are the sizes used in the experiments reasonable?\n3.\tIn the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016\n4.\tThe theoretical analysis of the algorithm is only implied and not stated clearly\n5.\tIn reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?\n6.\tIn describing the experiment it says that \u201cFor K\\in\\{1000,10000} data points are repeated.\u201d This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?\n7.\tSince grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds\n8.\tThe results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.\n\nAdditional comments\n9.\tThe introduction is confusing since it uses the term \u201cfederated learning\u201d as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.\n10.\tIn the abstract the term \u201cdifferential attacks\u201d is used \u2013 what does it mean?\n11.\t\u201cAn independent study McMahan et al. (2018), published at the same time\u201d- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.\n12.\tIn the section \u201cChoosing $\\sigma$ and $m$\u201d it is stated that the higher \\sigma and the lower m, the higher the privacy loss. Isn\u2019t the privacy loss reduced when \\sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. \n13.\tAt the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.\n14.\tIn the experiments (Table 1), what does it mean for \\delta^\\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?\n15.\tThe methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?\n16.\tThe citations are used incorrectly, for example \u201csometimes referred to as collaborative Shokri & Shmatikov (2015)\u201d should be \u201csometimes referred to as collaborative (Shokri & Shmatikov, 2015)\u201d. This can be achieved by using \\citep in latex. This problem appears in many places in the paper, including, for example, \u201cwe make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).\u201d Which should be \u201cwe make use of the moments accountant as proposed by Abadi et al. (2016).\u201d In which case you should use only \\cite and not quote the name in the .tex file.\n17.\t\u201cWe use the same de\ufb01nition for differential privacy in randomized mechanisms as Abadi et al. (2016):\u201d \u2013 the definition of differential privacy is due to Dwork, McSherry, Nissim & Smith, 2006\n18.\tNotation is followed loosely which makes it harder to follow at parts. For example, you use \u201cm_t\u201d for the number of participants in time t but in some cases,  you use only m as in \u201cChoosing $\\sigma$ and $m$\u201d.\n19.\tIn algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. \n20.\tFigure 2: I think it would be easier to see the results if you use log-log plot\n21.\tDiscussion: \u201cFor K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.\u201d \u2013 it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.\n22.\tIn the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan\n\n\nIt is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}