{"title": "A thorough investigation of using recurrent networks with experience replay, with impressive results on Atari", "review": "In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (\u201cburn-in\u201d period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab.\n\nThe proposed technique (dubbed R2D2) is not particularly original (it is essentially \u201cjust\u201d using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method.\n\nI do have a couple of important concerns though. The first one is that a few potentially important changes were made to the \u201ctraditional\u201d settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from \u201cMeta-Gradient Reinforcement Learning\u201d (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the \u201cdespite this\u201d above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more \u201cstandard\u201d Atari settings, even if it would mean using different hyper-parameters on DMLab).\n\nThe second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it.\n\nBesides the above, I have a few additional small questions:\n1. \u201cWe also found no benefit from using the importance weighting that has been typically applied with prioritized replay\u201d: this is potentially surprising since this could be \u201cwrong\u201d, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab)\n2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the \u201cdeep experts\u201d version of IMPALA \u2014 this is not clear in Fig. 3 \u2014 in which case this last question would be irrelevant)\n3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can\u2019t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don\u2019t you report their results on Atari?\n4. Table 4\u2019s caption mentions \u201c30 no-op starts\u201d but you actually used the standard \u201crandom starts\u201d setting, right? (not a fixed number of 30 no-ops)\n\nAnd finally a few minor comments / suggestions:\n- In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula)\n- At top of p. 3 I guess \\bar{delta}_i is the mean of the delta_i\u2019s, but then the index i should be removed\n- In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?)\n- p. 4, \u201cthe true stored recurrent states at each step\u201d: \u201ctrue\u201d is a bit misleading here as it can be interpreted as \u201cthe states one would obtain by re-processing the whole episode from scratch with the current network\u201d => I would suggest to remove it, or to change it (e.g. \u201cpreviously\u201d). By the way, I think it would have been interesting to also compare to these states recomputed \u201cfrom scratch\u201d, since they are the actual ground truth.\n- I think you should mention in Table 1\u2019s caption that the PBT IMPALA is a single network trained to solve all tasks\n- Typo at bottom of p. 7, \u201cIndeed, Table 1 that even...\u201d\n\nUpdate: score updated to 8 (from 7) following discussion below", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}