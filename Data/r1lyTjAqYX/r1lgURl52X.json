{"title": "The proposed RL agent leads to interesting results but the technical details need to be clarified", "review": "Summary: \nLeveraging on recent advances on distributed training of RL agents, the paper proposes the analysis of RNN-based RL agents with experience replay (i.e., integrating the time dependencies through RNN). Precisely, the authors empirically compare a state-of-the-art training strategy (called zero start state) with three proposed training strategies (namely; zero-state with burn-in, stored-state and stored-state with burn-in). By comparing these different strategies through a proposed metric (Q-value discrepancy), the authors conclude on the effectiveness of the stored-state with burn-in strategy which they consider for the training of their proposed Recurrent Replay Distributed DQN (R2D2) agent. \n\nThe proposed analysis is well-motivated and has lead to significant results w.r.t. the state-of-the-art performances of RL agents.\n\nMajor concerns: My major concerns are three-fold:\n- The authors do not provide enough details about some \"informal\" experiments which are sometimes important to convince the reader about the relevance of the suggested insights (e.g., line 3 page 5). Beyond this point, the paper is generally hard to follow and reorganizing some sections (e.g., sec. 2.3 should appear after sec. 3 as it contains a lot of technical details) would certainly make the reading of the paper easier.\n- Hausknecht & Stone (2015) have proposed two training strategies (zero-state and Replaying whole episode trajectories see sec. 3 page 3). The authors should clarify why they did not considered the other states in their study.\n- The authors present results (mainly, fig. 2 and fig. 3) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA, where R2D2 is trained using the aforementioned stored-state with burn-in strategy. It is not clear which are the considered training strategies adopted for the (compared to) state-of-the-art agents (Ape-X and IMPALA). The authors should clarify more precisely this point.\n\nMinor concerns: \n- The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric. It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}