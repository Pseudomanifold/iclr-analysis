{"title": "Agreed with small subspace but not with number of classes. Needs a more thorough study.", "review": "The authors build on recent works that study the spectrum of the Hessian of deep networks (e.g. Sagun et al). Previous work argues that Hessian is approximately low-rank i.e. there are few large eigenvalues and many small eigenvalues. This work argues that after some training, large eigenvectors of the Hessian converges to a subspace and stays there.\n\nIntuitively, this papers message makes sense and is interesting. I also agree with the tiny subspace argument in the paper. However, I am not convinced by a couple of things and I believe further evidence is necessary. \n\n1) Authors claim the top subspace has same rank k (where k=number of classes) and backs this up with linear classifier with 2 class (toy model). It is clear that any noiseless k-class linear classifier has its gradient lie on a k dimensional subspace. Similarly, for a deep network, I agree with hessian of the final layer will be rank k however earlier layers can have different ranks as a function of complexity of the lower level representations. My worry is that perhaps the Hessian of the final layer is somehow dominating over the other ones. Ideally, I would like to see analysis of individual layers to reach a conclusion. Is first layer also approximately rank k?\n\n2) Similarly, we need to see the eigenvalue distribution of the Hessian. Say there is a single very large eigenvalue and all others are small. The same claim would still hold. So it is not clear if number of classes really plays a role from the available experiments. This can indeed happen is the classes are correlated for instance. Authors can perhaps plot gradient over top k/2 subspace to reveal if their claim is specific to number of classes.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}