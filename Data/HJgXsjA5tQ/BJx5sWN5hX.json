{"title": "Interesting experimental results, but less significant theoretical contribution", "review": "This paper presents a class of neural networks that does not have bad local valleys. The \u201cno bad local valleys\u201d implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn\u2019t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.\n\nThe good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that\n* adding skip connections doesn\u2019t harm the generalization.\n* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.\n* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.\n\nHowever, from a theoretical point of view, I would say the contribution of this work doesn\u2019t seem to be very significant, for the following reasons:\n* In the first place, figuring out \u201cwhy existing models work\u201d would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.\n* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally \u201cequivalent\u201d to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17\u2019) it is easy to attain global minima.\n* I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive.\n\nBelow, I\u2019ll list specific comments/questions about the paper.\n* Assumption 3.1.2 doesn\u2019t make sense. Assumption 3.1.2 says \u201cthere exists N neurons satisfying\u2026\u201d and then the first bullet point says \u201cfor all j = 1, \u2026, M\u201d. Also, the statement \u201cone of the following conditions\u201d is unclear. Does it mean that we must have either \u201cN satisfying the first bullet\u201d or \u201cN satisfying the second bullet\u201d, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?\n* The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.\n* Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses.\n* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it\u2019s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.\n* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn\u2019t necessarily satisfy the assumptions?\n* Can you show the \u201cimprovement\u201d of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.\n\nMinor points\n* In the Assumption 3.1.3, the $N$ in $r \\neq s \\in N$ means $[N]$?\n* In the introduction, there is a sentence \u201cpotentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),\u201d which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18\u2019 and Yun et al. 18\u2019).\n* Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as \u201cfor example, in the fully connected network case, this means that all data points are distinct.\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}