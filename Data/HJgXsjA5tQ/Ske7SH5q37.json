{"title": "a breakthrough paper on the loss landscape of neural networks", "review": "The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. \n\nOverall I really enjoy reading the paper. \nThe assumptions to aid the proof are very natural and much softer than the existing literature. As far as I\u2019m concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. \nThe presentation of the paper is intuitive and easy to follow. I\u2019ve also checked all the proof and think it\u2019s brilliantly and elegantly written. \n\nMy only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesn\u2019t influence my recommendation to accept the paper.\n\n\nMinor issues:\nI think it\u2019s better to formally define \u201cbad local valley\u201d somewhere in the paper. From what I read, the definition of \u201cbad local valley\u201d is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. \nIn proof number 4 (of Theorem 3.3), the statement should be \u201cany *principle* submatrices of negative semi-definite matrices are also NSD\u201d, and it\u2019s not true otherwise. But this typo doesn\u2019t influence the proof. \nAlso, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your \u201cbad local valley\u201d. \nIt seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}