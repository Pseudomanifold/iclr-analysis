{"title": "Method that tries to be a feature extractor and a generative model at the same time.", "review": "(best read in typora)\n\nThe authors claim to propose a family of methods and generative models that are suited better for downstream tasks than previously proposed approaches.\n\n## Major points\n\nIt feels as if the proposed method tries to be many things. First, it is used for finding unsupervised representations down stream. Then, it still tries to be a generative model \"of sorts\", which is the reason for the use of variational inference in the first place. Additionally, the approximate posterior necessary to evaluate the ELBO is simultaneously used as a feature extractor.\n\nThe resulting issues are:\n\n  - A \"bad\" variational posterior is used because it is unclear how to get vectorial features otherwise. \n  - An adhoc likelihood function is used, which is not sufficiently well explored theoretically in the paper.  Specifically,\n      - Stochastic generation is claimed to be \"more complex than simple Gaussian\"; the burden of proof is on the authors, as Gaussian density is closed under multiplication. \n      - It appears to be a Monte Carlo approximation to sth that is computable in closed form.\n      - It is not clear if that MC approximation is normalised and if the normalisation is the same at each optimisation step. Does this bias optimisation? What happens to the KL penalty weight?\n  - The ELBO change (prior updating) seems to make the claim that we still have a generative model (as written in the intro) invalid. My intuition is that the KL penalty vanishes for small step rates of the optimiser, reducing the model to that of a noisy auto encoder.\n\n\n## Summary\n\nThe authors want to evaluate variational sequence models for feature extraction for downstream tasks. But why? What is the use of a generative inspired algorithm, when necessary ingredients are discarded? Both goals appear to be at conflict and I am not convinced that the variational ingredient is necessary.\n\nI do not cover the experimental section since the method itself has issues so severe that I don't consider it relevant. \n\n\n## Minor points\n\n- Notation $\\mu_{\\phi_t}$ gives the impression that $\\phi$ is time dependent.\n- Equations (9) and (11) are formatted badly.\n- The approximate posterior used was used first in (Bayer & Osendorfer, \"Learning stochastic recurrent networks\", 2014) not (Chen 2018).\n- Diagrams follow GM notation only half-heartedly.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}