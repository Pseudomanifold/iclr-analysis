{"title": "review", "review": "This paper belongs to the space of treating recommendation as a reinforcement learning problem, and proposed a model-based (cascaded DQN) approach, using a generative adversarial network to simulate user rewards.\n\nPros:\n+ proposed a set of cascading Q functions, to learn a recommendation policy\n+ unified min-max optimization to learn the behavior model and the reward function\n+ interesting idea of using generative adversarial networks to simulate user rewards.\n\nCons: \n- in Figure 6 no comparison with model-free (policy-gradient type) of approaches\n- there is not a lot of detail on the value of the generative adversarial network for the user behavior dynamics, thus this prevents the reader from fully understanding the contribution\n- only 2 datasets are used\n- only 100 users for test users seems few\n- why only 1000 active users were sampled from MovieLens?\n\nPersonally, I would prefer less details on formulating the recommendation problem as an RL problem (as there have been other papers before with a similar formulation) and more detail  on the simulation user reward model, and in general in sections 4 and 5. Also, the experiments could be strengthened.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}