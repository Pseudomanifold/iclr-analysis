{"title": "Systematic theoretical work with some supporting experiments", "review": "Summary:\nI'm not very familiar with the work this is building off of, but my summary is as follows:\nThe authors look at the problem of defining multilayer infinite width neural networks. The main challenge is that the weights (which are in some sense now a function) must be appropriately sampled to ensure that norms don't explode. \n\nThis has only been done for two layers before, and the authors derive how to do this for more than two hidden layers using RKHSs. This initialization is called Win-Win, and is compared to different initializations on a few different datasets.\n\nClarity: This paper is quite technical and hard to follow without knowledge of the prior work. I think the authors could have been a little clearer on some of the challenges. E.g. instead of talking about the weights needing to be \"in the same function space\", it would be helpful to remphasise the norm issue.\n\nComments:\nI'm not sure about the high level motivation for developing networks of this kind. In particular, none of the performance numbers are near state of the art (not necessary for developing new promising methods!) but I don't know exactly what the initialization buys in this setting.\n\nAlso, it would have been nice to see whether the Win-Win initialization is only useful for larger width networks compared to smaller width i.e. do other initialization schemes work better in this latter setting?\n\nThe derivations are interesting though, so I still recommend accept.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}