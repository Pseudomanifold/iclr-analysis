{"title": "Nice idea", "review": "TL;DR. I find the manuscript to contain interesting ideas, yet I believe there is room for improvement.\n\n* Summary\n\nFor any given specific network architecture, the manuscript aims at learning a distribution over the weights (rather than point-wise estimates of the weights). This is achieved through using a two-steps procedure, in which a \"hypernetwork\" is trained to output weights for the network of interest, and a GAN is then used to (adversarially) generate samples from a distribution $Q$ which is assumed not too far (in a KL sense) from a Gaussian prior $P$.\n\n* Major issues\n\nI find the central idea to be of interest to the ICLR community. However I have found a number of shortcomings to be addressed before I could recommend acceptance. The following list is in no particular order.\n\n- References: 20 out of 22 (!) references are preprints, about half of which are 3+ years old. Most of them are now published in proceedings and I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.\n- Links with Bayesian deep learning: I feel this should be more carefully discussed in the manuscript. The sentence \"We have proposed a generative, non-Bayesian solution [...]\" should be explained, as from what I gather HyperGAN samples (GAN-like) weights (i.e., networks) from a distribution $Q$ which is deemed not too far (in the KL sense) from a prior distribution $P$. How is that not Bayesian?\n- Numerical experiments. Table 4: what are the numbers reported? If single evaluation, I do not believe any conclusion may be drawn. If averages over multiple repetitions, no conclusion can be drawn without reporting standard deviations. In addition, I do not quite grasp the purpose of the 1D toy example.\n- Overall, I think the authors should try and make their contributions and method clearer. For example, a pseudo-code of the whole procedure might help readers understand the gist. \n- Architecture specific: I find the claim that HyperGAN explores the manifold of neural nets too strong. As the whole procedure is architecture-specific, I would find more appropriate to change that claim to \"exploring the weights distribution for a specific architecture\".\n- Code availability: the scope of the paper is diminished by the fact that no code is available by the time of review. A toolbox (not disclosing the authors' identities) should be made available to support the manuscript claims. Last sentence (page 8) is likely to be outdated and should be removed. \n\n* Minor issues\n\n- some typos: architecture (caption figure 1), $G(Q(z))$ (missing parenthesis, page 3), sum index $n$ not used in the last equation (page 7).\n- \"Perhaps the first proposed method...\" (page 2). Such imprecise statements must be avoided.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}