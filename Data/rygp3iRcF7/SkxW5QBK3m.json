{"title": "Some important related studies are missing.", "review": "\nI have several concerns about this paper.\n\n[originality]\nSome important related studies are missing.\n\n# Related studies about the perspective of \u201carea\u201d.\nThe consecutive position in sequence is often referred to as \u201cspan\u201d in NLP filed, which is identical to what the authors call \u201carea\u201d in this paper.\nThen, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, \ne.g.,\nWenhui Wang, Baobao Chang, \u201cGraph-based dependency parsing with bidirectional lstm\u201d, ACL-2016.\nMitchell Stern, Jacob Andreas, Dan Klein, \u201cA Minimal Span-Based Neural Constituency Parser\u201d, ACL-2017.\nKenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, \u201cEnd-to-end Neural Coreference Resolution\u201d, EMNLP-2017.\nNikita Kitaev, Dan Klein, \u201cConstituency Parsing with a Self-Attentive Encoder\u201d, ACL-2018.\n\nSimilarly, there are several related studies in image processing field,\ne,g.,\nMarco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, \u201cAreas of Attention for image captioning\u201d, ICCV-2017\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, \u201cImage Captioning with Semantic Attention\u201d, CVPR-2016.\n\n# Related studies about the perspective of \u201cstructured attention\u201d. \nSeveral papers about structured attention have already been proposed, \ne.g.,\nYoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. \u201cStructured Attention Networks\u201d, ICLR-2017.\nVlad Niculae, Mathieu Blondel. \u201cA Regularized Framework for Sparse and Structured Neural Attention\u201d, NIPS-2017.\n\n\nI think the authors should explain the relations between their method and the methods proposed in the above listed papers.\n\n\n[significance]\n# Concern about experimental settings\nThe experimental setting for NMT looks unnormal in the community.\nCurrently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings.\n\n\n# computational cost\nThe authors should report the actual calculation speed by comparing with the baseline method and the proposed method.\nIn Sec. 2.2, the authors provided the computational cost. \nI feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method.\n\n\n\nOverall, the proposed method itself seems to be novel and interesting.\nHowever, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write.\nThus, it is a bit hard for me to strongly recommend this paper to be accepted. \n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}