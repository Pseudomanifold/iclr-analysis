{"title": "interesting approach to generalization, but are the guarantees relevant?", "review": "The paper studies generalization through a general empirical risk minimization procedure with Lipschitz regularization.\nGeneralization is measured through distance of the empirical minimizer function to a true labeling function u_0, or to the minimizer of the expected regularized loss.\n\nThe approach of studying generalization through the lens of Lipschitz stability over the data is interesting,\nand the study directly considers minimizers of regularized optimization objectives, which is different than\nrecent generalization bounds which often provide guarantees on a given network regardless of how it was trained.\n\nHowever, various aspects of the setup seem quite disconnected from practice in the context of deep neural networks,\nand the obtained guarantees are quite weak and not always connected to generalization:\n\n- the approach relies on a constant L_0 in the objective which is assumed known in advance (although it is usually set to 0 in practical methods), and determines the nature of convergence. In particular, the results for small L_0 (referred to as \"noisy labels\") only shows convergence to a minimizer u^* of the expected *regularized* loss, thus does not characterize generalization in the usual sense since u^* is biased. More generally, the distinction between 'clean' and 'noisy' labels is confusing and should be clarified: the paper seems to assume that the true labeling function u_0 may itself produce incorrect labels deterministically even with infinite data, which is an odd way to formulate the learning problem.\n\n- the convergence rates obtained in the paper exhibit a curse of dimensionality (O(n^{-1/m}) where m is the dimensionality of the data manifold). Given that most other bounds for neural networks do not exhibit such a dependence on dimension, this seems to be a weaker guarantee, unless the setting captures an improvement in a different setting. (edit: I removed the previous remark on parametric rates, which was inaccurate) Some of the constants also seem to grow exponentially with m. Either way, this should be discussed in the paper.\n\n- possibly related to the previous point, all the theory in the paper is agnostic to the function class considered, given that it simply considers all lipschitz functions in the variational problem (1). Given that the authors attempt to explain generalization of neural networks, this seems like a non-negligible disconnect since there could be approximation errors. In particular, even if deep networks can perfectly fit training data, it is not clear that they can achieve the best trade-off with the Lipschitz constant in the regularized objective (1).\n\n- the assumptions on the prediction space (simplex) and the used loss functions also seem disconnected from practice (the cross-entropy loss usually includes a softmax). Note that while usual networks can fit randomly labeled data (Zhang et al.), this does not mean their loss is 0. Yet the analysis seem to rely on having zero loss on training points, even in the case of 'noisy labels'.\n\nAdditionally, the use of covering arguments in the input space in the proposed approach is related to the study of generalization through robustness [Xu & Mannor (2010), \"Robustness and Generalization\"]. The authors should discuss the relationship to this work.\n\nMore comments:\n- the term 'converge' in the title is not clear. In the abstract, what does 'verification' mean?\n- Section 2.3: \n  m_0 == m ?\n  How does C grow with m in Theorem 2.7 and Corollary 2.8?\n  What is meant by 'perfect generalization'?\n  Is eq. (6) realistic for classification losses?\n- Section 2.4: clarify what is meant by 'noisy labels'", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}