{"title": "Interesting investigation but insufficient proposition and results", "review": "The paper proposes to investigate the well-known problem of memory network learning and more precisely the difficulty of the attention learning supervision with such models. In the introduction, I strongly agree with the statement saying that while end-to-end memory network has been proposed, it is still very difficult to train such model with an off-the-shelf adaptive gradient descent algorithm and an end-to-end supervised loss.\n\nThe paper proposed to use a model with a 2-level attentive encoding of the memory blocks corresponding to a word and a sentence level. The authors start investigating in section 3 the use of an attention supervision. The authors investigate this attention supervision on the path-finding task of the Babi20 dataset and the Wikihop set of the QAngoroo dataset.\n\nAs secondary supervision signal, the authors proposed to use a 'pseudo-gold chain' reasoning information using the co-occurrences between the named entities of the questions and answers with the passages. It can be argued that this pseudo-gold reasoning chain is mainly possible because of the synthetic nature of the synthesis of the dataset which has been produced using a structured knowledge base.\n\nIn a sense, supervising attention in such way was already suggested in [Bordes and Weston 2015], the novelty seems very limited to me while the analysis provided by this work might be useful as an interesting starting point for further analysis and propositions in this domain.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}