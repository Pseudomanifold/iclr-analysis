{"title": "Interesting \"Relevant domain\" based approximation of ReLU to approximate sparse latent structures", "review": "The paper studies the representational power of two-layer ReLU networks and polynomials for approximating a linear generative model for data with sparsity in the latent vector. They show that ReLU networks achieve optimal rate whereas low degree polynomials get a much worse rate.\n\nOverall, the results are strong, the authors provide a lower bound on the degree of polynomial needed to approximate the model indicating the power of non-linearity. The observation of moving away from uniform approximators is well-motivated. The approximation theorem for ReLU is intriguing and uses new ideas which I have not seen before and are potentially useful in other applications. So far, only rational functions have been able to give such approximation guarantees. However, the motivation for studying sparse linear regression from a representation view-point is not very clear. Ideally, you would like to study representation for more complex models. \n\nQuestions/Comments:\n- Related work is missing prior work at the intersection of kernel methods and neural networks, please update.\n- Define notation before using, for example, \\rho_\\tau^{\u2a02m}\n- Expand proof sketches, they are not very clear, also full proofs are written with not much detail.\n- Is the dependence on \\mu tight? The current dependence sort of suggests that you need the observation matrix to be very close to identity.\n- Proof of Lemma B.1 is unclear, could you explain how you deduce the lemma from the inequality?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}