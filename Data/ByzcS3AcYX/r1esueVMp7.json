{"title": "Good technical ideas, but suffers from clarity issues and weak evaluation.", "review": "Overview: This paper describes an approach to style transfer in end-to-end speech synthesis by extending the reconstruction loss function and augmenting with an adversarial component and style based loss component.\n\nSummary: This paper describes an interesting technical approach and the results show incremental improvement to matching a reference style in end-to-end speech synthesis.  The three-component adversarial loss is novel to this task.  While it has technical merit, the presentation of this paper make it unready for publication.  The technical descriptions are difficult to follow in places, it makes some incorrect statements about speech and speech synthesis and its evaluation is lacking in a number of ways.   After a substantial revision and additional evaluation, this will be a very good paper.\n\nThe title of the paper and moniker of this approach as \u201cTTS-GAN\u201d seems to preclude the fact that in the last few years there have been a number of approaches to speech synthesis using GANs.  By using such a generic term, it implies that this is the \u201cstandard\u201d way of using a GAN for TTS.  Clearly it is not. Moreover, other than the use of the term, the authors do not claim that it is. \n\nWhile the related works regarding style modeling and transfer in end-to-end TTS models are well described, prior work on using GANs in TTS is not.  (This may or may not be related to the previous point.)  For example, but not limited to:\nYang Shan, Xie Lei, Chen Xiao, Lou Xiaoyan, Zhu Xuan, Huang Dongyan, and Li Haizhou, Statistical Parametric Speech Synthesis Using Generative Adversarial Networks Under a Multi-task Learning Framework, ASRU, 2017\nYuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, Text-to-speech Synthesis using STFT Spectra Based on Low- /multi-resolution Generative Adversarial Networks, ICASSP 2018\nSaito Yuki, Takamichi Shinnosuke, and Saruwatari Hiroshi, Training Algorithm to Deceive Anti-spoofing Verification for DNN-based Speech Synthesis, ICASSP, 2017.\n\nSection 2 describes speech synthesis as a cross-domain mapping problem F : S -> T, where S is text and T is speech. (Why a text-to-speech mapping is formalized as S->T is an irrelevant mystery.)  This is a reasonable formulation, however, this is not a bijective mapping.  There are many valid realizations s \\subset T of a text utterance t \\in S.  The true mapping F is one-to-many.    Contrary to the statement in Section 2, there should not be a one-to-one correspondence between input conditions and the output audio waveform and this should not be assumed.  This formalism can be posed as a simplification of the speech synthesis mapping problem.  Overall Section 2 lays an incorrect and unnecessary formalism over the problem, and does very little in terms of \u201cbackground\u201d information regarding speech synthesis or GANs.  I would recommend distilling the latter half of the last paragraph.  This content is important -- the goal of this paper is to disentangle the style component (s) from the \u201ceverything else\u201d component (z)  in x_{aud} by which the resultant model can be correctly conditioned on s and ignore z.\n\nSection 3.2 Style Loss: The parallel between artistic style in vision and speaking style in speech is misplaced.  Artistic style can be captured by local information by representing color choices, brush technique, etc.  Speaking style and prosodic variation more broadly is suprasegmental.  That is it spans multiple speech segments (typically defined as phonetic units, phonemes, etc.).  It is specifically not captured in local variations in the time-frequency domain.  The local statistics of a mel-spectrogram are empoverished to capture the long term variation spanning multiple syllables, words, and phrases that contribute to \u201cspeaking style\u201d.  (In addition to the poor motivation of using low-level filters to capture speaking style, the authors describe \u201cprosody\u201d as \u201crepresenting the low-level characteristics of sound\u201d. This is not correct.)  These filter activations are more likely to capture voice quality and speaker identity characteristics than prosody and speaking style.\n\nSection 3.2: Reconstruction Loss: The training in this section is difficult to follow.  Presumably, l is the explicit style label from the data, the emotion label for EMT-4 and (maybe) speaker id for VCTK.  It is a rather confusing choice to refer to this as \u201clatent\u201d since this carries a number of implications from variational techniques and bayesian inference.  Similarly, It is not clear how these are trained. Specifically, both terms are minimized w.r.t. C but the second is minimized only w.r.t G.  I would recommend that this section be rewritten to describe both the loss functions, target variables, and the dependent variables that are optimized during training.\n\nSection 3.3 How are the coefficients \\alpha and \\beta determined?\n\nSection 3.3 \u201cWe train TTS-GAN for at least 200k steps.\u201d Why be vague about the training?\n\nSection 3.3. \u201cDuring training R is fixed weights\u201d Where do these weights come from? Is it an ImageNet classifier similar with a smaller network than VGG-19?\n\nSection 5: The presentation of results into Table 1 and Table 2 is quite odd.  The text material references Table 1 in Section 5.1, then Table 2 in Section 5.2, then Table 1 in Section 5.3 and then Table 2 again in Section 5.3.  It would be preferable to include the tabular material which is being discussed in the same order as the text.\n\nSection 5: Evaluation.  It is surprising that there is no MOS or naturalness evaluation of this work.  In general increased flexibility of a style-enabled system results in decreased naturalness.  While there are WER results to show that intelligibility (at least machine intelligibility) may not suffer, the lack of an MOS result to describe TTS quality is surprising.\n\nSection 5: The captions of Tables 1 and 2 should provide appropriate context for the contained data.  There is not enough information to understand what is described here without reference to the associated text.\n\nSection 5.1: The content and style swapping is not evaluated.  While samples are provided, it is not at all clear that the claims made by the authors are supported by the data.  A listening study where subjects are asked to identify the intended emotion of the utterance would be a convincing way to demonstrate the effectiveness of this technique.  As it stands, I would recommend removing the section titled \u201cContent and style swapping\u201d as it is unempirical.  If the authors are committed to it, it could be reasonably moved to the conclusions or discussion section as anecdotal evidence.\n\nSection 5.3: Why use a pre-trained WaveNet based ASR model?  What is its performance on the ground truth audio?  This is a valuable baseline for the WER of the synthesized material.\n\nSection 5.3 Style Transfer: Without support that the subject ratings in this test follow a normal distribution a t-test is not a valid test to use here.  A non-parametric test like a Mann-Whitney U test would be more appropriate.\n\nSection 5.3 Style Transfer: \u201cEach listened to all 15 permutations of content\u201d.  From the previous paragraph there should be 60 permutations.\n\nSection 5.3 Style Transfer: Was there any difference in the results from the 10 sentences from the test set, and the 5 drawn from the web?\n\nTypos:\nSection 1 Introduction: \u201cx_{aud}^{+} is unpaired\u201d -> \u201cx_{aud}^{-} is unpaired\u201d\nSection 2: \u201cHere, We\u201d -> \u201cHere, we\u201d\nSection 5.3 \u201cTachotron\u201d -> \u201cTacotron\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}