{"title": "Poorly Written Paper with Unconvincing Contribution", "review": "-- Paper summary --\n\nThe primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration.\n\n-- General Commentary --\n\nThe overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn\u2019t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time?\n\nI have plenty of concerns with the submission itself, listed below:\n\n- First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn\u2019t properly check the paper before submission.\n\n- While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what\u2019s being proposed.  A simple diagram or illustration would have clarified some of the notation at the very least.\n\n- The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (\u2018uncalibrated\u2019, \u2018temp-scal' and \u2018BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot.\n\n- As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include \u2018Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks\u2019 (Bradshaw et al, 2017), and \u2018Calibrating Deep Convolutional Gaussian Processes\u2019 (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models.\u2028\n\n- With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion.\u2028\n\n- I can\u2019t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the \u2018alternative hypothesis\u2019 being mentioned, and the investigation is entirely limited to the offline setting, so I\u2019m not entirely sure what distinction the authors are trying to make here.\u2028\n\n- In the same section, the authors then remark that \u2018Our results open new perspectives to improve the variational approximation\u2026\u2019 and \u2018we believe our results might foster further research in\u2026\u2019, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can\u2019t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers.\u2028\n\n- The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper\u2019s lack of identity and focus.\n\n- There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient.\u2028\n\n- There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by \u2018conf\u2019 in Equation 4.\u2028\n\n- Referring to \u2018datasets\u2019 as \u2018databases\u2019 in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by \u2018uses BNNs\u2019? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission.\n\n- In their discussion of the results, the authors state that \u2018We cannot conclude that BNNs are calibrating at the cost of losing accuracy\u2019, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score.\n\n-- Recommendation --\n\nUnfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference\u2019s standards.\n\nPros/Cons summary:\n\n+  The proposal yields good results in the provided experiments\n-   Minor contributions that are not convincing enough\n-   Muddled presentation of ideas\n-   Dubious or weakly motivated design choices\n-   Poorly written with plenty of typos\n-   Difficult to follow", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}