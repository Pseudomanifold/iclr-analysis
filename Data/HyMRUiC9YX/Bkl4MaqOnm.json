{"title": "Paper that presents an experimental study of adversarial examples transferability with a contribution based on loss smoothing. The study is interesting with good illustrations and potential improvements but seems a bit limited, the conclusions are rather expected or unsurprising.", "review": "This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model. There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...\nThe authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples. Two experimental studies are made for each influence factor from existing architectures. Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.\n\nPros\n-the proposed experimental studies can be interesting to the community\n-many interesting illustrations are provided.\n\nCons\n-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better\n-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.\n-Only two influence factors are studied, again the paper would be more interesting with a more general study\n\nThe paper has an interesting potential but seems a bit limited in its present form.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}