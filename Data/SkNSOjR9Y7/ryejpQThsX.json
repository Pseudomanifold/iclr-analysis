{"title": "Rederivation of REINFORCE Estimator", "review": "Summary:\nThis paper proposes training VAEs with discrete latent variables by importance sampling the expected log likelihood (ELL) term in the ELBO, which is the problematic term since it is not amenable to reparametrization gradients.  For the importance sampling distribution, they choose the variational distribution itself, making the ELL gradient E[(d q(z|x) / d \\theta) \\log p(x|z) / q(z|x)].  Experiments are reported for MNIST and Fashion-MNIST using Bernoulli and categorical latent variables.\n\nCritique:\nThe gradient estimator the paper proposes is the REINFORCE estimator [Williams, ML 1992] re-derived through importance sampling.  The equivalence can be seen just by expanding the derivative of log q in REINFORCE: E[log p(x|z) d log q(z|x)] = E[ (log p(x|z) / q(z|x)) d q(z|x) ], which is the exact estimator the paper proposes.  REINFORCE has been previously used for variational inference [Paisley et al., ICML 2012; Ranganath et al, AISTATS 2014] and deep generative models [Mnih & Gregor; ICML 2014] and recently extended for various control variates [Tucker et al., NIPS 2017].   The equivalence would not be exact if the authors chose the importance distribution to be different than the variational approximation q(z|x), so there still may be room for novelty in their proposal, but in the current draft only q(z|x) is considered.  \n\nConclusion: Due to lack of novelty, I recommend rejection.\n\n\nMiscellaneous points:\n\u201c...there exist no simple solutions to circumvent this problem.\u201d  The Gumbel-softmax trick is fairly simple (although an approximation) [Jang et al., ICLR 2017; Maddison et al., ICLR 2017]. \n\n\u201c...after training q(z|x) is a very good approximation to the true posterior p(z|x).\u201d  That\u2019s not necessarily true.  \n\nEquation #2 should be just equal to Equation #1.\n\n\u201cKingma & Welling (2013) proposed to minimize L(\\theta) using stochastic gradient descent on a training set...\u201d. First uses of stochastic gradient for VI were [Sato, NC 2001; Platt et al., NIPS 2008; Hoffman et al., JMLR 2013].  Kingma & Welling [ICLR  2014] were the first to introduce reparameterized stochastic gradients.\n\nBefore Equation #11, the reference to Equation #4 should be to Equation #5.\n\n\u201c...the weighting...depends only on \\theta_D and not on \\theta_E\u201d (p 4). D and E should be switched.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}