{"title": "review", "review": "This paper is looking at how to extract knowledge from CNNs to help improve explainability and robustness against an adversarial attack.  It is using a known technical call M-of-N rules.\n\nThis problem of explainability of NN's is an important one and rules are a good step in that direction.  \n\n+ The paper is generally well written\n\n- The contribution seems to be relatively small\n- The evaluation is limited, only 1 dataset and only 1 technique evaluated\n\nGeneral advice for work in AI explainability:\nWhen one looks at the problem of AI explainability it is important to describe who the target audience is for the explanation.  Is it a machine learning expert, who wants to debug the model?  Is it an end user who wants to better understand why the prediction was made?  Is it a regulator who is trying to ensure the model's predictions are fair?\n\nEach of these personas will come with different needs and different technical backgrounds, so an assumption that some artifact (a rule set?) is \"explainable\" may apply to one group, but not the other group.  For example, rules are likely to be more explainable to a ML expert, but may not be to an end user, unless they are very small.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}