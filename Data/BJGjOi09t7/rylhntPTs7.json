{"title": "Experimental evaluations are mostly qualitative", "review": "This paper replaces the Gaussian latent variables in standard VAE with the Weibull distribution and therefore presents a VAE solution to nonnegative matrix factorization, under squared Euclidean distance loss function. The literature review summarizes four related work very well. The adopted Weibull distribution provides a tractable inverse CDF function and analytical form of the KL divergence, facilitating VAE inference. In particular, the effects of the entropy term are discussed in detail.  Experiments illustrate the generated data from the model,  the learned part-based dictionaries, and the distributions of latent variables from similar data points.  \n\nQuestions: \n\n1. What is the updating rule for W_f? Is it multiplicative?  In Sec 2.4, The value of W_f is kept to be nonnegative by \"setting negative terms to zero\". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps? \n\n2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?  \n\n3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect? \n\nMinor: \nIn some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}