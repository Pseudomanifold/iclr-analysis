{"title": "Interesting but weaker novelty/experiments/writing", "review": "The paper describes an approach for using graph neural networks (GNN) to perform multi-label classification (MLC). The main idea is to use attentional pooling to project an input graph into a \"label graph\", whose nodes correspond to labels on some MLC problem. Multiple rounds of self-attention/message-passing hops can be performed on the input graph and label graph. Each output label is binary-valued, and is predicted from its corresponding node in the label graph. They evaluate on 6 multi-label sequence classification datasets, and report strong perform over baselines.\n\nThough interesting, I recommend rejection for several reasons:\n\n1) The technical contribution has limited novelty. One (very recent) reference this paper misses is \"Hierarchical Graph Representation Learning with Differentiable Pooling\" by Ying et al. (2018), which uses a very similar mechanism. The field is moving quickly, so references get missed sometimes, however from what I can tell, the graph-coarsening idea presented here isn't that technically distinct from Ying el al.'s. The Mrowca et al. (2018) \"Flexible Neural Representation for Physics Prediction\" is also fairly similar and should probably at least be cited.\n\n2) There aren't strong baselines. This approach is based on GNNs, and the Graph2MLP results, which is similar to previous GNN graph-level classification methods, are fairly strong too. My suspicion is that with some more tuning and tweaking, the results here would be similar to those of Ying et al., Velickovic et al. (2017)'s Graph Attention Nets, and other models which use what Gilmer et al. (2017) terms the \"readout\" function for MLC. Without testing some of these other approaches, how can readers be sure this is approach has value over other approaches? The reviews by Gilmer et al. (2017) and Battaglia et al. (2018) summarize a bunch of alternatives that could be tried, some of which use similar encoder/decoder setups (not with the attentional pooling, however, as far as I know).\n\n3) The writing is fairly dense for what is a fairly straightforward idea. And the paper is over 8.5 pages, with key details in the Appendix. \n\nI believe this approach could be quite powerful, and there was clearly a lot of excellent work that went into this project. But because the GNN area is very active, the bar is high. With a little more innovation on the model side (can the same core model be useful for things beyond MLC as well? I'm guessing it could), better baselines, better scholarship, and condensing the writing, I think this paper can be an important step forward.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}