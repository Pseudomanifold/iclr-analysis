{"title": "Scalable method for the slate-recommendation task ", "review": "This paper pr poses a conditional generative model for slate-based recommendations. The idea of slate recommendation is to model an ordered-list of items instead of modelling each item independently as is usually done (e.g., for computational reasons). This provides a more natural framework for recommending lists of items (vs. recommending the items with the top scores).\n\nTo generate slates, the authors propose to learn a mapping from a utility function (value) to an actual slate of products (i.e., the model conditions on the utility).  Once fitted, recommending good slates is then achieved by conditioning on the optimal utility (which is problem dependant) and generating a slate according to that utility. This procedure which is learned in a conditional VAE framework effectively bypasses the intractable combinatorial search problem (i.e., choosing the best ordered list of k-items from the set of all items) by instead estimating a model which generates slates of a particular utility. The results demonstrate empirically that the approach outperforms several baselines.      \n\nThis idea seems promising and provides an interesting methodological development for recommender systems. Presumably this approach, given the right data, could also learn interesting concepts such as substitution, complementarity, and cannibalization.\n\nThe paper is fairly clear although the model is never formally expressed: I would suggest defining it using math and not only a figure.  The study is also interesting although the lack of publicly available datasets limits the extent of it and the strength of the results. Overall, it would be good to compare to a few published baselines even if these were not tailored to this specific problem.\n\n\nA few detailed comments (in approximate decreasing order of importance):\n\n- Baselines. The current baselines seem to focus on what may be used in industry with a specific focus on efficient methods at test time.\n\n  For this venue, I would suggest that it is necessary to compare to other published baselines. Either baselines that use a similar setup or, at least, strong collaborative filtering baselines that frame the problem as a regression one.\n\n  If prediction time is important then you could also compare your method to others in that respect.\n\n- training/test mismatch. There seems to be a mismatch between the value of the conditioning information at train and at test. How do you know that your fitted model will generalize to this \"new\" setting?\n\n- In Figures: If I understand correctly the figures (4--6) report test performance as a function of training steps. Is that correct?\n\n  Could you explain why the random baseline seems to do so well? That is, for a large number of items, I would expect that it should get close to zero expected number of clicks.\n\n- Figure 6d. It seems like that subfigure is not discussed. Why does CVAE perform worse on the hardest training set?\n\n- The way you create slates from the yoochoose challenge seems a bit arbitrary. Perhaps I don't know this data well enough but it seems like using the temporal aspects of the observations to define a slate makes the resulting data closer to a subset selection problem than an ordered list.\n\n- Section 3. It's currently titled \"Theory\" but doesn't seem to contain any theory. Perhaps consider renaming to \"Method\" or \"Model\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}