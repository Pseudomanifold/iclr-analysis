{"title": "Interesting study but need more experiments. ", "review": "Summary: \nThis paper focuses on a \"generalization\" in deep Q-network (DQN). Specifically, they showed that when features (parameters of DQN) are trained in one environment (default flavour/mode) and then used as an initialization for the same model but for a slightly different environment ( i.e. still captures key concepts of the original environment ) can boost the performance of the model in the new flavour/environment. More importantly, the performance boost is significant when DQN's parameters which are used to initialize the model for the new environment were trained using dropout and L2 regularization in the default flavour/mode.  For the experiments, 4 games: FREEWAY, HERO, BREAKOUT, and SPACE INVADERS which have 13 flavours (combinations of a mode and a difficulty) are used.\n\nStrengths:\n+ This paper is interesting in the sense that it empirically shows that using regularization in training deep RL can be helpful when the goal is the generalization from one flavour of an environment to another one but very similar to the original.\n+ The experiments show that using REGULARIZED FINE-TUNING and FINE-TUNING for a new flavour /mode can help with sample efficiency compared with the models which are trained from scratch [10M frames vs 50M frames and 50M frames vs 100M frames ](Table 3).\n+ The paper is well-written and it can be easily followed.\n\nWeaknesses:\n- In my view, there should be experiments in which the proposed method is compared with other approaches that improve generalization in deep RL like Zhang et al., 2018 and Justesen et al., 2018.\n- According to the paper (at least my understanding) DQN's hyper-parameters are tuned based on default mode/flavour environment. It is possible that results for 'SCRATCH' results (Table 3) can be improved if DQN's hyper-parameters are tuned on the current flavour not default one.\n-The proposed method is only applicable when the default environment and the new one are very similar. If the environments are that similar why even bother to train first on default and then generalize to the new one? Wouldn't be less expensive to just focus on the target environment and find the best model on that?\n\nQuestions:\n- It is mentioned in the paper, that evaluation protocol suggested by Machado et al. (2018) is being followed in this paper. Have you followed the protocol introduced in section 4.2 of Machado et al. (2018)? If yes, the protocol in Machado et al. (2018) is for training time but numbers in Table 1 in this paper are for evaluation time. Can you elaborate?\n- Why only those 6 games were selected for the experiments? \n\nIn summary, I found this paper is interesting but my concern is about the experiments.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}