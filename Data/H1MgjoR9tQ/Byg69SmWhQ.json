{"title": "A way to actually train sequential embedding models", "review": "The main contribution of this paper in practice seems to be a way to initialize the Continuous Matrix Space Model so that training actually converges, followed by a slightly different contrastive loss function used to train these models. The paper explores the pure matrix model and a mixed matrix / vector model, showing that both together improve on simpler methods on many benchmark tasks.\n\nMy main concern is that the chained matrix multiplication involved in this method is not substantially simpler than an RNN or LSTM sentence encoding model, and there are no comparisons of training and inference cost between the models proposed in this paper and conceptually simpler RNNs and LSTMs. The FastSent paper, used here as a baseline, does compare against some deep models, but they choose far more complex baselines such as the NMT encoding, which is trained on a very different loss function. Indeed the models proposed here do not seem to outperform fasttext and fastsent despite having fairly similar computational costs.\n\nI think this paper could use a little more justification for when it's appropriate to use the method proposed here versus more straightforward baselines.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}