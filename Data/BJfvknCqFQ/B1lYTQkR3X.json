{"title": "Solid experimental study of approximate worst and average case input image rotation and translation", "review": "Summary:\nStandard CNN models for MNIST, CIFAR10 and ImageNet are vulnerable with regard\nto (adversarial) rotation and translation of images.\nThe paper experimentally examines different ways of formulating attacks\n(gradient descent, grid search and sampling) and defenses\n(random augmentation, worst-case out of sample robust training,\naggregated classification) for this class of image transformation.\n\nThe main results are:\n- Gradient descent is not effective at generating worst-case rotations /\ntranslations due to nonconcavity of the adversarial objective\n- Grid search is very effective due to low parameter space\n- Sampling and pick the worst is also effective and cheap, for similar reasons\n- L infinity ball pixel perturbation robustness is orthogonal to the examined\ntransformations and does not provide good defense mechanism\n- Just augmenting data with random translation / rotations is not a strong\ndefense\n- Using a worst-case out of sample of 10 for training with an approximation of\na robust optimization objective combined with an aggregated result for\nclassification is a stronger defense\n\nRecommendation:\nThe paper presents a comprehensive study of a relevant class of adversarial\nimage perturbations for state-of-the-art neural network models.\nThe results are a useful pointer towards future research directions and for\nbuilding more robust systems in practice.\nI recommend to accept the paper.\n\nStrong points:\n- The paper is well written, has clear structure and is technically easy to\nunderstand.\n- The question of padding and cropping comes up naturally and is then answered.\n\nOpen questions (things that could potentially be of interest when added):\n- Loss landscapes look like most of the nonconcavity is along the translation\nparameter. Any idea why?\n- What mechanisms within CNN models do or do not learn (generalize) rotation\nand translation from provided data (including augmentation)?\n\nSpecific:\n- Page 2: perturbrbations (Typo)\n- Page 3: witho (Typo)\n- Page 3: Constrained optimization problems typically written as\nmax_{...} \\mathcal L(x', y) s.t. x' = T(...)\n(s.t. for subject to instead of for) but that's matter of taste I guess\n- Page 4: first order -> first-order (consistency)\n- Page 4: tyipcally (Typo)\n- Page 4: occurs most common(ly)\n\nI am not sufficiently knowledgable about the previous literature to ensure that\nthe claimed novelty of the paper is truly as novel.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}