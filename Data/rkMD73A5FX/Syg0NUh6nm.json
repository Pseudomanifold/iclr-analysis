{"title": "interesting paper with a thorough evaluation ", "review": "Summary\n\nThis paper proposes a method named Mahe that can provide hierarchical explanations for a model: including both context-dependent(instance level) and context-free (global) explanations by a local interpretation algorithm. It obtains context-free explanations through generalizing context-dependent interactions to explain global behaviors. The effectiveness is shown through a number of synthetic and real-world data experiments.\n\nThe paper provides an interesting way to get context-free explanations from local explanations. The experiments are well designed and the paper is overall written well. \n\nMajor comments\n\n0. The motivation of the local-MLP models is not convincing. \n\n1. Of particular concern is the computational time cost of the model, as it involves retraining and an exhaustive search through local interactions to get context-free explanations.\n\nThe paper provides no experiments about timing cost to show the relative computational scalability of the proposed method. As Mahe trains MLPs per data sample and searches through all interactions for finding context-free explanations, this raises concerns.\n\n2. The paper includes no baseline comparisons for finding context-free interactions.\n\n3. Non-linear GAM is replaced by linear approximations in the experiments. More experiments showing the advantage of non-linear function approximation is recommended. \n\n4. Minor Comments: In the description, \"L + 1 different levels of a hierarchical explanation which constitutes the context-dependent explanation\", What does L indicate? The order of interactions?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}