{"title": "novelty over early work? different angle to view this problem - sparse learning. Proofs/statements need to more rigorous.", "review": "This paper provides an algorithm that excludes the bad training data in the training process and obtain a more accurate model for both supervised and unsupervised learning problem. The paper gives the theoretical guarantee for mixed linear regression and Gaussian mixture model, and also conducts the experiments for deep image classification and deep generative models.\n\nMajor Concerns:\n1, As said in related work, a soft version of this paper\u2019s method has been proposed in the previous work, and the major seems to be that there is no initialization in the previous work which only leads to local convergence. Therefore, based on my understanding, the only innovation in this paper is that it gives the initialization process so that the algorithm can converge to the global optimal solution. But even this innovation only successes on some specific problems (Section 4-7). There are too few innovations.\n\n2, In Section 4, for mixed linear regression, Theorem 1 and Theorem 2 together can not guarantee the global optimal solution for the algorithm. The author should demonstrate  \u201cstrict inequality\u201d property in the 3rd line in Theorem 2, because it should correspond to the  \u201cstrict inequality\u201d property in the 2nd line in Theorem 1.\n\n3. Another angle to view the target problem in paper is from the outlier detection problem. The sparse learning formulation and theory can be conducted to solve this problem. Many existing theoretical analysis methods and optimization methods can be applied. For example, authors can refer to \n\nA Robust AUC Maximization Framework With Simultaneous Outlier Detection and Feature Selection for Positive-Unlabeled Classification, 2017\n\nThe comparison to these type of methods need to be included. \n\nMinor Concerns:\n1, Theorem 2 does not give the probability, only mentioning \u201chigh probability\u201d. How high? I do not find the probability in the proof as well. The same concern happens to Theorem 4. I think that\n\n2, In Section 6 and 7, the author does not compare with other algorithms, which can not show the advantage of this algorithm.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}