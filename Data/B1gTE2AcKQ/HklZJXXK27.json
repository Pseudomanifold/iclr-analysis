{"title": "Interesting idea. Presentation is unclear and requires more work.", "review": "In this work the authors propose a learning algorithm for inferring generative programs and the concepts associated with the input data. Specifically, the idea is originated from amortised inference where a generative model (the mapping from the latent to the data) is learned simultaneously with the reverse mapping, i.e., the recognition network, in a joint optimisation framework following variational Bayes. In this work the authors further introduce a memory buffer of highly probable latent samples from the recognition model in order to avoid evaluating programs that are unlikely to have generated the data. The main challenge of the algorithm is the handling of the discrete nature of the latent state.\n\nIn general, as having little experience with programs and automata I found the paper genuinely interesting, but also very confusing. On the positive side, the authors managed to nicely motivate their work and tried to make a direct connection to popular approaches like the VAE and the sleep and wake algorithm. However, even after putting a lot of effort in understanding the paper, I am still not confident that I managed to extract all the important information out of the manuscript.\n\n* More specifically, the paper lacks in clarity and the presentation of the main methodology needs improvement. The proposed algorithm is presented in a rather convoluted way and lacks a proper formulation. Apart from the general equation of the ELBO the rest of the methodology looks more like a guide to follow rather than a scientific presentation of a novel idea. The authors need to put significantly more effort in communicating better their work. It cannot be the case that out of a 10-page paper the whole idea is summarised/compressed in the last two small paragraphs of page 4.\n\n* In these paragraphs (end of page 4) the authors describe the proposed idea of having a memory buffer to store latent samples. They also refer to their joint p(z, x). I have a couple of questions here. First of all, what is the form of it? Furthermore, do we use this probability as weights in order to later resample from the memory to train the generative model?\n\n* Also, in Algorithm 1 different parameters in different distributions are all denoted as \\theta. This is very confusing. I really struggle to follow the algorithm as none of the involved distributions is properly defined, and we do not even know what and how many parameters do we need to optimise. I am also confused with the flow of the algorithm. Is it a single objective that we optimise in one go as the gradient update suggests? Are we optimising in turns? What happened to the KL term of the variational bound? When/where is the prior been updated?\n\n* Regarding the update of the recognition network in the sleep phase. Due to the nature of the optimisation scheme the recognition network is being updated after having access to the optimal (up to that point) variational parameters (do you learn a q(z)?) and prior p(z). This is definitely acceptable, however, it would be fair to do a similar two-step update for the VAE. A similar idea has been studied in [Krishnan et al. 2018].\n[Krishnan et al. 2018] \u201cOn the challenges of learning with inference networks on sparse, high-dimensional data\u201d. AISTATS 2018\n\n* In the beginning of Section 3 the authors argue that their work considerably differs from a VAE, because the proposed algorithm tries to recover a distribution of a latent program that generates the observations rather than a latent vector. To my understanding---correct me if I am wrong but it is nowhere mentioned in the main text apart from the caption of Figure 1---a latent program is just a categorical distribution that spits out the alphabet needed to construct the observations. To me this is exactly the same as what VAEs do with just using a categorical variational distribution. The only difference is the discrete nature of the latent variable and the fact that you also try to learn the prior p(z), which you refer to it as inductive bias. Can the authors please comment on that?\n\n* Regarding the discrete latent state and optimisation of VAE the authors use the REINFORCE algorithm. Can a trick similar to Gumbel-softmax [Jang et al. 2017] be exploited for the purposes of this work. Does it make any sense? If yes what are the implications?\n[Jang et al. 2017]. \u201cCategorical Reparameterization with Gumbel-Softmax\u201d. ICLR 2017\n\n* In the experiment in Figure 4 you compare wallclock time of the algorithms. In such a plot I would expect to see different algorithms terminating at different time. This is not the case in the plot, although you explicitly mention that the proposed algorithm requires several times more computation per iteration. Can you please explain? Also, there is no text explaining Figure 7.\n\n---Minor issues---\n* Page 3: \u201cneedn\u2019t\u201d \u2192 \u201cneed not\u201d\n* Page 7: \u201cmarginal likelihood p(x|z)\u201d --> this is not the marginal\n* Page 7: \u201cestimates a lower bound on p(x|z)\u201d \u2192 the lower bound is on the marginal p(x).\n\nOverall, based on my detailed comments above I believe that the paper is not ready for publication. Thus my recommendation is to reject.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}