{"title": "A nice proposed idea and new dataset, but with confusing presentation and evaluation", "review": "This paper introduces a modification to the wake-sleep algorithm that introduces into the recognition network a cache which holds a small number of high-quality proposed latent variables, and uses discrete sampling from this cache instead of using draws from the recognition network directly. It's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.\n\nThere is an extensive introduction and background, which aims to situate this work, but overall it is a bit confusing (and strays a bit afield, e.g. why is there a description of SGD in connection to deep meta-learning? noting that this paper is nearly two pages over length\u2026).  \n\n- Much language is devoted to the idea that a variational autoencoder setting is \"better\" than wake-sleep for learning deep generative models, because it provides a single unifying objective function. This shows up also in language like in table 2 (\"the correct update for q\", whereas the wake-sleep update \"may be heavily biased\"). This doesn't make much sense. There may well be empirical reasons to prefer the performance of a VAE as opposed to wake-sleep, but the issue of whether it \"will converge on a good model\" is clearly an issue in VAEs as well (see, e.g., the \"hacks\" necessary to achieve convergence to a non-spurious optima in [1]). This paper's own experiments seem to show that, for these discrete models, achieving convergence is a non-issue for wake-sleep.\n\n- In section 2.3 it is claimed this \"highlights a bias-variance tension\". This doesn't make any sense; the \"sleep\" update to q in wake-sleep is its own update, and not a biased approximation to the ELBO; there is nothing inherently wrong with the dual objectives used in the wake-sleep agorithm. Each is easily motivated as maximizing a lower bound on the marginal likelihood; the sleep update could alternately be motivated as performing variational inference, simply with a alpha-divergence as the objective.\n\n- There are claims here that the challenges to good VAE performance are due to inaccurate inference networks, but this is not clear at all (see, e.g. [2]).\n\n- Wake-sleep seems to be presented primarily as a straw-man, with little effort to actually benchmark against it or consider its merits. In particular, reweighted wake-sleep [3] and related wake-wake algorithms [4] should address any potential bias concerns, at modest increase in computational cost.\n\n- Given that much of the focus is on how this paper uses a direct estimate of an ELBO, in constrast to wake-sleep, it is completely baffling that this objective function never once appears in the main text of the paper! The only statement of it is in table 2 (which is an incomplete objective, and doesn't include M) and implicitly in the algorithm block itself. Figure 2 is nice as a sales pitch, but it doesn't replace actually writing down the objective functions used for training!\n\n- Why are the latent variables z_i refered to as \"programs\" in section 2.3? This is simply a latent variable, and is a free choice for the model designer depending on problem domain. If the point here is that this is a discrete latent variable, or in some sort of combinatorial / structured space, then say that. \n\n- There is some confusion here about what is a \"prior\" in section 3. A prior is not something which is \"learned\" empirically from data as part of the training (empirical Bayes procedures notwithstanding), A prior is literally a prior belief on parameters. Estimating a marginal distribution p(z), isnot learning a prior, it is doing density / distribution estimation. That's also fine, but (as noted) likely additional regularization is needed. In the VAE setting, the phrase \"marginal posterior\" is often used for q(z) = 1/N \\sum_i q(z_i | x_i), i.e. marginalizing over the data.\n\n- The experiments are all nice examples of discrete domains, and I like the new dataset as a testbed for fitting regular expressions a lot.\n\nIn general, I think this paper would benefit from a much more expanded and clear presentation of the contents of section 3. At the moment, the algorithm is only understandable from a close read of the algorithm block. The core novel idea here is the introduction of the \"memory\" block into the wake-sleep algorithm, but this is only briefly described. Some experiments here which isolate the impact of this memory block on training would be great \u2014 for example, how does performance vary given different sizes of the memory? In the experiments, there is not a huge performance gap between the proposed approach and wake-sleep; it would be good to also compare reweighted wake sleep as a comparison. It would also be good to run the importance sampled variant of the proposed approach (or, even, a variant which directly marginalizes over the contents of the memory). In terms of framing, I don't really see what this paper has to do with \"few-shot program learning\", aside from the fact that the test examples happen to be structured domains (it is arguable whether e.g. the compositional kernels are \"programs\").\n\n\n[1] Bowman et al, Generating Sentences from a Continuous Space\n[2] Shu et al, Amortized Inference Regularization\n[3] Bornschein and Bengio, Reweighted wake-sleep\n[4] Le et al, Revisiting Reweighted Wake-Sleep\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}