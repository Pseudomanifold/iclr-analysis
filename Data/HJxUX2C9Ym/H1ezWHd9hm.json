{"title": "Good idea. Lacking presentation makes it hard to follow.", "review": "n this paper, the authors propose an alternative to the softmax layer \u2014 an observer-classifier framework, where the observer has access to the input features as well as a query vector from the classifier and outputs a binary response, and the classifier (modeled as an LSTM) has access to its queries as well as the binary response from the classifier to output further queries as well as its current decision about the class of the input. This process repeats producing a binary decision tree with individual classes represented at the leaf nodes.\n\nThe paper tackles an interesting problem of interpretability of complex models such as CNN, and the proposed approach appears promising in some domains as reflected in the experiments. However, I had a few concerns about the presentation which affected my understanding of the methodology. The biggest issue for me is the lack of a clear explanation of the proposed framework. Several details are skimmed over such as \u2014 why do we use Gumbel-softmax estimator? What\u2019s the motivation for two different MLPs in the Attribute Predicting Observer? How exactly are the trees created in the visualizations (Fig. 3 and 4)? How do we use the intermediate predictions if we do at all? And so on. Some of these I could make my way through after some effort while some others are still unclear to me. In a paper proposing a new framework, the space could have been better utilized describing the method crisply, but currently the explanation is done within 1.5 pages. In the experiments section, there is little analysis on parameters like the number of binary decision needed. Fig. 2 plots the observations but there is no insight as to why for example, with the same number of classes CIFAR10 requires 6 binary decisions whereas MNIST just 4. A naive understanding would suggest log(N) decisions to reach leaf nodes for N classes. Fig. 2 would be better drawn with logarithmic x-axis, and the legend to really correspond with the lines in the figure. Fig. 3 and 4 occupy a full page and are near impossible to read even on a large monitor. Authors should investigate better summary visualizations and maybe dedicate this space to developing and explaining the framework. I think Fig. 1 does an excellent job at explaining the framework and it would be great if the accompanying text built on it. The results on zero-shot learning are also promising but the section lacks clarity again. \n\nMinor issues: \u201csaliecy\u201d (page 1, spelling), \u201cby first sampling\u201d (page 2, no follow-up), \u201clog o is the unnormalized output\u201d (page 3, should it be o?), \u201ceffects\u201d (page 4, should be affects), \u201ccan trained\u201d (page 4, should be can be trained), \u201cseek for\u201d (page 8, should be seek).\n\nIn summary, even though I liked the idea and the approach to the problem, the presentation is quite lacking, and in its current form, the paper might lose impact just by virtue of being hard to understand and subsequently being hard to reproduce. I would encourage the authors to polish the explanation, clean up the figures, and proof read for spelling and grammar errors before resubmitting to a future conference. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}