{"title": "Very well written paper on an important subject, with clear technical contribution and convincing results", "review": "This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.\nSpecifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of \"depth bases\".\nThese values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.\nThen, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).\nBy unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.\n\nThe paper is clear, well organized, well written and easy to follow.\nEven if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.\nIn particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.\nThe experiments are quite convincing and seem to clearly support the efficacy of the proposed method.\n\nI don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:\n\n1) In page 5, the authors write \"learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations\".\nI don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.\nThe word \"guarantee\" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.\n\n2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.\nGiven this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}