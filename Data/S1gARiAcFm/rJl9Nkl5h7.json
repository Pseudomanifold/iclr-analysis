{"title": " The paper details about a feed-forward NN, called DyMoN, used to model dynamic features of objects that are at multiple phases of transition. The authors need to relate how the Markovian processes are realized within the hidden units and why at all a deep architecture is required.  ", "review": "\n> Even though the paper details the underlying Markovian setup in Section 2, it is unclear to the reader how this knits with the FFNN architecture, for example what are the Markovian functions at hidden layer and output layer. Are they all conditional probabilities? How do you prove that this is what occurs within each node?\n\n> Why is the functional form of f_\\theta in Eq 1? \n\n> How many hidden layers are in place?\n\n> What is the Stochastic dynamical process in Figure A and how is this tethered to DyMon? \n\n> The authors mention an nth-order Markovian process implemention but is this not the case with any fully connected neural network implementation? What the reader fails to see is why DyMoN is different to these already-existing architectures.\n\n> In the teapot example, the authors mention a DyMoN architecture. (Page 8). Is this what is used throughout for all the experiments? If yes, why is it generalizable and if not, what is DyMoN\u2019s architecture? You could open the DyMoN box in Figure 10 (1) and explain what DyMoN consists.\n \n\nSection 2 is the crux of the paper and needs more work - explain the math in conjunction to the \u2018deep\u2019 architecture, what is the 'deep' architecture and why it is needed at all. Then go on to show/prove that the Markovian processes are indeed being realized. \n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}