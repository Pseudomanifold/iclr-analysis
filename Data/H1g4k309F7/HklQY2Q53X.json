{"title": "review of Wasserstein Barycenter Model Ensembling", "review": "Paper overview: Model ensembling techniques aim at improving machine learning model prediction results by i) executing several different algorithms on the same task and ii) solving the discrepancies in the responses of all the algorithms, for each task. Some common methods are voting and averaging (arithmetic or geometric average) on the results provided by the different algorithms. \nSince averaging amounts to computing barycenters with different distance functions, this paper proposes to use the Wassertein barycenter instead of the L2 barycenter (arithmetic average) or the extended KL barycenter (geometric mean). \n\nRemarks, typos and experiences that would be interesting to add: \n     1) Please define the acronyms before using them, for instance DNN (in first page, 4th line), KL (also first page), NLP, etc. \n    2) In practice, when ensembling different methods, the geometric and arithmetic mean are not computed with equal weights ($\\lambda_l$ in Definition 1). Instead, these weights are computed as the optimal values for a given small dev-set. It would be interesting to see how well does the method compare to these optimal weighted averages, and also if it improves is we also compute the optimal $\\lambda_l$ for the Wasserstein barycenter. \n    3) How computationally expensive are these methods? \n    4) So the output of the ensembling method is a point in the word embedding space, but we know that not all points in this space have an associated word, thus, how are the words chosen?\n    5) The image captioning example of Fig.4 is very interesting (although the original image should be added to understand better the different results), can you show also some negative examples? That is to say, when is the Wassertein method is failing but not the other methods.\n\n\nPoints in favor: \n     1)Better results: The proposed model is not only theoretically interesting, but it also improves the arithmetic and geometric mean baselines.\n    2) Interesting theoretical and practical properties: semantic accuracy, diversity and robustness (see Proposition 1). \n\nPoints against: The paper is not easy to read. Ensembling methods are normally applied to the output of a classifier or a regression method, so it is not evident to understand why the 'underlying geometry' is in the word embedding space (page 2 after the Definition 1). I think this is explained in the second paragraph of the paper, but that paragraph is really not clear. I assume that is makes sense to use the word-embedding space for the image caption generation or other ML tasks where the output is a word, but I am not sure how this is used in other cases. \n\nConclusion: The paper proposes a new method for model assembling by rethinking other popular methods such as the arithmetic and geometric average. It also shows that it improves the current methods. Therefore, I think it presents enough novelties to be accepted in the conference.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}