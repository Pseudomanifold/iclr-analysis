{"title": "interesting, but very limited idea", "review": "This method deals with compressing tiny videos using an end-to-end learned approach. However, the paper has a significant number of limitations, which I will discuss below.\n\n1. The method has only been trained on very small videos due to the fact that fully connected layers are used. I don't really understand why was this necessary, and it's not explained in the paper at all. Just this fact makes it completely infeasible for any \"real\" application.\n2.  The evaluation was done on very limited domains. Of huge concern to me is the fact that very good results are presented on the sprites dataset. However, that dataset can be literally encoded by providing an index in a lookup table of sprites, so it's absolutely ludicrous to compare learned methods on that set to general video compression methods. The results look a lot less exciting when looking at the Kinetics 64x64 dataset. \n3. The evaluation (again) is problematic because the results refer to PSNR. PSNR for video is a very overloaded term. In fact, just the way to compute PSNR is not very clear for video. Video compression papers in general compute it in one of two ways: take the mean squared error over all the pixels in the video, then compute PSNR; or compute per frame PSNR then average. Additionally, none of the papers in this domain use RGB, because the human visual system is much more sensitive to detail preservation (the Y/luminance channel) than they are to chroma (color) changes. When attempting to present results for video, I would recommend to use PSNR-Y (and explain which type it is!), while also mentioning which ITU recommendation is used for defining the Y channel (there are multiple recommendations). \n4. It is not very clear how the global code is obtained. It is implied that all frames get processed in order to come up with f, but does this mean that they're processed via an LSTM model, or is there a single fully connected layer which takes as input all frames? In terms of modeling f, it sounds like the hyperprior model from Balle et al is employed, but again it's not clear to me how (is it modelling an entire video or a sequence?). I would really like to see a diagram for the network structure that computes f.\n\nOnt he positives of the paper: I applaud the authors with respect to the fact that they made an effort to explain how the classical codecs were configured and being explicit about the chroma sampling that's employed. \n\nI think all the problems I mentioned above can be fixed, so I don't want to reject the paper per se. If possible, should the authors address my concerns (i.e., add more details), I think this could be an interesting \"toy\" method. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}