{"title": "An interesting way to combine regularized approximate minimal polynomial extrapolation and optimistic online learning.", "review": "This paper provides an interesting way to combine regularized approximate minimal polynomial extrapolation and optimistic methods. I like the idea and the experimental results look promising. However, I have some concerns:\n    - I'm wondering if the comparison with the baseline is fair. Actually, one iteration of Optimistic-AMSGrad is more expensive than one iteration of AMSGrad since it requires to compute m_{t+1}. The authors should explain to what extend this computation is significantly cheaper that a backprop (if it actually is).\n    - The discussion after Theorem 1 is not clear. To me it is not clear whether or not Optimistic-AMSGrad has a better Regret that AMSGrad: you did not compare the *sum* of the two additional term with the term with a $\\log(T)$ (second line of (8) with second lien of (9)). Do you get a better regret that O(\\sqrt{T}), a better constant ? Moreover you did not justify why it is reasonable to assume that each $g_t[i]^2-h_t[i]^2/\\sqrt{v_{t-1}[i]}$ are bounded.\n    - I'm also concerned about the definition of $D_{\\infty}$. Did you prove that this constant is not infinite ? (Reddi et al. 2018) proposed a projected version of their algorithm and did the analysis assuming that the constraints set was bounded. In your Algorithm 2 would you project in Line 8 and 9 or only on line 9 ? I think that the easiest fix would be to provide a projected version of your algorithm and to do your analysis with the standard assumption that the constraints set is bounded.\n    - The description of Alg 2 is not clear. \"Notice that the gradient vector is computed at w_t instead of w_{t\u22121/2}\" why would it be $w_{t-1/2}$ ? in comparison to what ? \"Also, w_{t+ 1/2} is updated from {w_{t\u2212 1/2} instead of w_t.\" Same. The comments are made without any description of the algorithm, fact is, if the reader is not familiar with the algorithm (which is introduced in the following page) the whole paragraph is hard to catch.\n    - Actually, Page 6 you explain how the optimistic step of Daskalikis et al. (2018) is unclear but you can merge the updates Lines 8\nand 9 to $w_{t+1} = w_{t} - \\eta_{t+1} \\frac{4 h_{t+1}}{(1-\\beta_1) \\sqrt{v_t}} - \\eta_t \\\u00b1rac{\\theta_t}{\\sqrt{v_t}} + \\eta_t \\frac{4 h_{t}}{(1-\\beta_1) \\sqrt{v_{t-1}}}$ (Plug line 8 in line 9 and then plug Line 9 at time t) to get a very similar update. If you look more closely at Daskalakis et al. 2018 their guess $m_{t+1}$ is $g_t$. Finally you Theorem 2 is stated in a bit unfair way since you also require $\\beta_1 <\\sqrt{\\beta_2}$, moreover it seems that Theorem 2 is no longer true anymore if, as you says, you impose that the second moment of ADAM-DISZ is monotone adding the maximization step. \n    - About the experiments, I do not understand why there is the number of iterations in the left plots and the number of epochs on the right plots. It makes the plots hard to compare. \n    - You should compare your method to extragradient methods.\n\n\n\nTo sum up, this paper introduce interesting results. The combination of (Scieur et al. 2016) and optimistic online learning is really promising and solid theoretical results are claimed. However, some points should be clarified (see my comments above). Especially, I think that the authors focused too much (sometimes being unfair in their discussion and propositions) on showing how their algorithm is better than (Daskalakis et al. 2018) whereas as they mentioned it \"The goals are different.\" ADAM-DISZ is designed to optimize games and is similar to extragradient. It is know that extragradient methods are slower than gradient methods for single objective minimization because of the extrapolation step using a too conservative signal for single objective minimization.\n\n\nSome minor remarks: \n    - Page One \"NESTEROV'SMETHOD\"\n    -  \"which can be much smaller than \\sqrt{T} of FTRL if one has a good guess.\" You could refer to Section 3 or something else because otherwise this sentence remains mysterious. What is a good guess (OR maybe you could say that standard \"good\" guesses are either the previous gradient or the average of the previous gradients)\n    - \"It combines the idea of ADAGRAD (Duchi et al. (2011)), which has individual learning rate for different dimensions.\"  what is the other thing combined ?\n    - Beginning of Sec 3.1 $\\psi_t$ represent $diag(v_t)^{1/2}$.\n\n===== After Authors Response =====\nAs developed in my response \"On $D_{\\infty}$ assumption \" to the reviewers, I think that the assumption that $D_\\infty$ bounded is a critical issue.\nThat is why I am moving down my grade.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}