{"title": "Simple but nice extension with existing ideas from the literature", "review": "In this manuscript, the authors borrow the idea of \"optimism\" from the online learning literature and apply it to two frequently used methods for neural network training (AMSGrad and ADAM). More or less, replicating the theory known in the literature, they give a regret analysis. The manuscript ends with a comparison of the optimistic methods against their plain counterparts on a set of test problems.\n\nThis is a well-written paper filling a gap in the literature. Through the contribution does not seem significant, the results do support that such extensions should be out there. In addition to a few typos, some clarification on several points could be quite useful:\n\n1) It is not clear why the authors use this particular extrapolation algorithm?\n\n2) If we have the past r+1 gradients, can we put them into use for scaling the next direction like in quasi-Newton methods?\n\n3) The following part of the sentence is not clear: \"... the gradient vectors at a specific time span is assumed to be captured by (5).\"\n\n4) \\nabla is missing at the end of the line right after equation (6).\n\n5) The second line after Lemma 2 should be \"... it does not matter how...\" (The word 'not' is missing.)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}