{"title": "An interesting tree-structured positional embedding", "review": "This work proposes a novel tree structure positional embedding by uniquely representing each path in a tree using a series of transformation, i.e., matmul for going up or down the edges. The tree encoding is used for transformer and shows gains over other strong baselines, e.g., RNNs, in synthetic data and a program translation task.\n\nPros:\n\n- An interesting approach for representing tree structure encoding using a series of transformation. The idea of transformation without learnable parameters is novel.\n\n- Better accuracy both on synthetic tasks and code translation tasks when compared with other strong baselines.\n\nCons:\n\n- Computation seems to be larger given that the encoding has to be recomputed in every decoding step. I'd like to know the latencies incurred by the proposed method.\n\nOther comment:\n\n- I'd like to see experimental results on natural language tasks, e.g., syntax parsing.\n\n- Section 2:  \"we see that is is not at all necessary\" -> that is\n\n- Section 3: Notation is a little bit hard to follow, \":\" for D and U, and \";\" in stacking.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}