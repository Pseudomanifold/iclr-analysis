{"title": "Promising approach for enabling transformers to process tree-structured data", "review": "The authors propose to change the positional encodings in the transformer model to allow processing of tree-structured data.\nThe tree positional encodings summarize the path between 2 nodes as a series of steps up or down along tree branches with the constraint that traveling up a branch negates traveling down any branch.\n\nThe experimental results are encouraging and the method notably outperforms the regular transformer as well as the tree2tree LSTM introduced by Chen et al on larger datasets. \n\nThe current draft lacks some clarity and is low on references. It would also be interesting to see experiments with arbitrary trees or at least regular trees with degree > 2 (rather than just binary trees). While the authors only consider binary trees in this paper, it represents a good first step towards generalizing attention-based models to nonlinear structures.\n\nComments:\n* Would it be possible to use the fact that D_kU = I for the correct branch k? (This happens frequently for binary trees)", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}