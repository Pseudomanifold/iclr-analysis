{"title": "Second order method for pruning multiple layers", "review": "This paper proposes a multi-layer pruning technique based on the Hessian. The main claims are performing better than other second order pruning methods and be principled. \n\n\nMain concerns / comments are:\n-\tPart of the novelty relays on computing the Hessian, and the algorithm goes for very large networks (parameter wise), why? Modern networks do have much fewer parameters and do perform better. How does it behave on those? Would be interesting to see impact on modern networks (e.g., ResNet). \n\n\n-\tPaper claims to be principled (as many others) and being able to address multiple layers at the same time. I do believe first order methods do that as well. Why not compared to them? \n-\tPaper claims little overhead (compared to training and re-training). There is not much on that. Also, following the pipeline [train-prune-retrain] can be substituted by pruning while training with little overhead as in recent papers: (such as Learning with structured sparsity or Learning the number of neurons in DNN both at NIPS2016 or encouraging low-rank at compression aware training of DNN, nips 2017). Compared to those newer methods, this proposal has a drop-in accuracy while those do not. Would be nice to have a discussion related to that. Would be possible to include this into the original training process? \n-\tExperiments are shown in small datasets and non-current networks with millions of parameters which do not reflect current state of the art. I would be interested to see limitations in networks not having fully connected layers with the large majority of (redundant) parameters.\n-\tCompute time is not provided. Please comment on that\n-\tI am not sure if I understand the statement on 'pruning methods can not handle multiple layers'. To the best of my understanding, current pruning methods as those mentioned above do\n\n-\tDifferent to others, the proposed method, given a desired compression ratio can adjust the relevance of each layer. That is interesting, however, what is the motivation behind? Would be interesting to be able to control specifically each layer to make sure, for instance, the latency of each layer is maintained. \n\n\n-\tI am confused with \\lambda, how does this go from percentage to per parameter? Is that guaranteed?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}