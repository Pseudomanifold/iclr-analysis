{"title": "Marginally above acceptance threshold", "review": "The paper proposes a multi-layer pruning method called MLPrune for neural networks, which can automatically decide appropriate compression ratios for all the layers. It firstly pre-trains a network. Then it utilizes K-FAC to approximate the Fisher matrix, which in turn approximates the exact Hessian matrix of training loss w.r.t model weights. The approximated Hessian matrix is then used to estimate the increment of loss after pruning a connection. The connections from all layers with the smallest loss increments are pruned and the network is re-trained to the final model.\n\nStrength:\n1. The paper is well-written and clear. \n2. The method is theoretically sound and outperforms state-of-the-art by a large margin in terms of compression ratio. \n3. The analysis of the pruning is interesting.\n\nWeakness:\n*Method complexity and efficiency are missing, either theoretically or empirically.* \nThe main contribution claimed in the paper is that they avoid the time-consuming search for the compression ratio for each layers. However, there are no evidences that the proposed method can save time. As the authors mention, AlexNet contains roughly 61M parameters. On the other hand, the two matrices A_{l-1} and DS_l needed in the method for a fully-connected layer already have size 81M and 16M respectively. Is this only a minor overhead, especially when the model goes deeper?\n\nOverall, it is a good paper. I am inclined to accept, and I hope that the authors can show the complexity and efficiency of their method.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}