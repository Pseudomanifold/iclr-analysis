{"title": "A clear paper with clear contributions", "review": "Summary: \n(1) This paper proposes the concept of Switchable Normalization (SN), which learns a weighted combination of three popular/existing normalization techniques, Instance Normalization (IN) for channel-wise, Layer Normalization (LN) for layer-wise, and Batch Normalization (BN) for minibatch-wise.\n(2) Some interesting technical details: a) A softmax is learned to automatically determine the importance of each normalization; b) Reuse of the computation to accelerate.  c) Geometric view of different normalization methods.\n(3) Extensive experimental results to show the performance improvement. Investigation on the learned importance on different parts of networks and tasks.\n\n\nComments:\n\nThe writing of this paper is excellent, and contributions are well presented and demonstrated.\nIt is good for the community to know SN is an option to consider. Therefore, I vote to accept the paper. \n\nHowever, the proposed method itself is not significant, given many existing efforts/algorithms; it is almost straightforward to do so, without any challenges. \n\nHere is a more challenging question for the authors to consider: Given the general formulation of normalization methods in (2), it sees more interesting to directly learn the pixel set I_k. The proposed SN can be considered as a weak version to learn the pixel set: SN employs the three candidates set pre-defined by the existing methods, and learns a weighted combination over the  \u201ctemplate\u201d sets. This is easy to do in practice, and it has shown promising results. A natural idea to learn more flexible pixel set, and see the advantages. Any thoughts?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}