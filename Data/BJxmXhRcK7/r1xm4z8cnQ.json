{"title": "Tensor-based soft-sharing MTL, upgraded with TR-decomposition. Maybe practically useful to enhance. But not clearly enough written or evaluated.", "review": "Summary:  This paper studies deep multi-task learning. Prior papers have studied various knowledge sharing approaches for deep multi-task learning including hard and soft sharing schemes. And some soft sharing schemes have used tensor decompositions (including TT, and Tucker). This paper fuses this line of work with the recently proposed Tensor-Ring decomposition in order to obtain Tensor Ring (TR)-based soft sharing for multi-task learning. The results show some improvement over prior deep MTL methods based on other tensor factorisation methods.\n\nStrengths:\n+ Nice extension of existing line of work tensor-factorisation based MTL.\n+ More flexibility for controlling shared/unshared portions of weights compared to DMTRL. \n+ Improves on previous methods results.\n+ Experiments evaluate how MTL methods relate with various amounts of training data on each task.\n\nWeaknesses:\n- Novelty/significance is limited. \n- Writing. Many things are not clearly and intuitively explained. Some claims are not adequately justified. \n- Introduces more hyper parameters to tune.\n- Results may rely on hyper parameter tuning. \n\nComments:\n1. Novelty.  Existing studies already established the template of different tensor factorisation methods (TT, Tucker) being possible to plug into deep networks for different kinds of soft-sharing MTL. Meanwhile, TR decomposition is taken off the shelf; (and as it\u2019s been applied for compression before, this is not the first time TR decomposition has been used in a CNN context either). Therefore this is an A+B paper and a high bar should be met for the additional analysis, insight, or performance improvements that should provided.\n2. Lots of writing issues:\n2.1 Many things are not explained transparently enough at best (or major over-claim at worst). For example: \n2.1.1 Paper claims the benefit that each task can have its own I/O dimensionality. However if TR-decomp is \u201ccircularly connected\u201d TT-decomp (Fig 1), then this seems not to happen automatically. So it should be unpacked more clearly how this is achieved. \n2.1.2 Paper claims favourable ability to use more private cores than TT, where only one core is private. However circular TT would also seem to have one private core by default (the core with a task axis). So I suspect something else is going on, but this is completely unclear and should be explained more transparently. Furthermore it should be justified if whatever modifications do enable these properties are definitely a unique property of TR-decomp, or could also be applied to TT-decomp. \n2.1.3 Statement \u201cTRMTL generalizes to allow the layer-wise weight to be represented by a relatively lager number of latent cores\u201d unclear: generalises what? larger number of cores than what? Than TT? The previous presentation suggests TT and TR should have same number of cores.  \n2.1.4 Statements like \u201cTR enjoys the property of circular dimensional permutation invariance\u201d are made without any explanation about what is the implication of this for neural networks and multi-task learning.  \n2.2 Many claims are inaccurate or not adequately backed up by theory or experiment. EG: (i) Paper claims to include DMTRL as a special case. But it only subsumes DMTRL-TT, not DMTRL-Tucker. Because TR-decomp does not include Tucker-decomp as an exact special case.  (ii) Sentences \u201cTR-ranks are usually smaller than TT-ranks\u201d are assertions without verification. \n2.3 Sentences are taken verbatim from other papers, plagiarism. For example: \u201cTR model is more flexible than TT, because TR-ranks can be equally distributed in the cores, but TT-ranks have a relatively fixed pattern\u201d  is verbatim from Zhao\u201916 TR-decomp paper. \n3. Hyperparameters: This paper apparently gains some practical benefit due to the notion of shared/unshared cores. However, this also introduces  additional hyper parameters (E.g., each layers private proportion \u201cc\u201d) to tune besides the ranks. Unlike the rank that can be pre-estimated by reconstruction error, this one seems to require tuning by cross-validation. This is not scalable. \n4. Hyperparameters+Tuning: Hyperparameters Private proportion, \u201csharing pattern\u201d, IO dimension seem to be tuned by accuracy.( \u201cWe test different sharing patterns and report the ones with the best accuracies\u201d). This is even less scalable, and additional tuning makes it unsurprising it surpasses other models performance.\n5. Insight & Analysis. All the core selection & public/private core selection are treated as black box optimisation. No insight is given about what turns out to be useful to share or not, and how consistent this is, etc.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}