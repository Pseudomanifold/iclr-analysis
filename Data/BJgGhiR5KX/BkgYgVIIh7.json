{"title": "Limited novelty, strong evaluation, other languages and tasks?", "review": "The paper presents an intuitive architecture for learning cross-lingual sentence representations. I see weaknesses and strengths: \n\n(i) The approach is not very novel. Using parallel data and similarity training (siamese, adversarial, etc.) to facilitate transfer has been done before; see [0] and references therein. Sharing encoder parameters across very different tasks is also pretty standard by now, going back to [1] or so. \n(ii) The evaluation is strong, with a nice combination of standard benchmark evaluation, downstream evaluation, and analysis. \n(iii) While the paper is on cross-lingual transfer, the authors only experiment with a small set of high-resource languages, where transfer is relatively easy. \n(iv) I think the datasets used for evaluation are somewhat suboptimal, e.g.: \na) Cross-lingual retrieval and multi-lingual STS are very similar tasks. Other tasks using sentence representations and for which multilingual corpora are available, include discourse parsing, support identification for QA, extractive summarization, stance detection, etc. \nb) Instead of relying on Agic and Schluter (2017), why don\u2019t the authors use the XNLI corpus [2]?\nc) Translating the English STS data using Google NMT to evaluate an architecture that looks a lot like Google NMT sounds a suspicious. \n(v) While I found the experiment with eigen-similarity a nice contribution, there is a lot of alternatives: seeing whether there is a linear transformation from one language to another (using Procrustes, for example), seeing whether the sentence graphs can be aligned using GANs based only on JSD divergence, looking at the geometry of these representations, etc. Did you think about doing the same analysis on the representations learned without the translation task, but using target language training data for the tasks instead? The question would be whether there exists a linear transformation from the sentence graph learned for English while doing NLI, to the sentence graph learned for German while doing NLI. \n\nMinor comments: \n- \u201cTable 3\u201d on page 5 should be Table 2. \n- Table 2 seems unnecessary. Since the results are not interesting on their own, but simply a premise in the motivating argument, I would present these results in-text. \n\n[0] http://aclweb.org/anthology/W18-3023", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}