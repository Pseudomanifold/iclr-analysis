{"title": "non-effective method (works well only with groundtruth information), convoluted writing, improper evaluation metric", "review": "The paper proposes a hybrid model-free and model-based RL agent for the task of navigation. Reaching the target is decomposed into a set of sub-goals, and the plan is updated as the agent explores the environment. The method has been tested in the House3D environment for the task of RoomNav, where the goal is to navigate towards a certain room. \n\nThe idea of integrating RL agents with semantic knowledge is interesting. However, the paper has several major issues that should be addressed in the rebuttal:\n\n(1) The experiment results in Figure 3 and Figure 4 are based on groundtruth room information. The only experiment that is fully automatic is the one in Figure 5. However, there is no difference between the proposed method and the baselines in that case. So the proposed method is not effective without groundtruth information.\n\n(2) The only evaluation metric that is used is \"Success Rate\". That metric is not sufficient for evaluation of navigation agents since it does not include episode length information. All of the results should be based on the protocol mentioned in \"On Evaluation of Embodied Navigation Agents\", arXiv 2018. \n\n(3) There is no termination action according to Appendix B. So the agent does not know if it is at the target or not. It seems the agent will stop if it issues \"stay still\" three times. That is different from termination action. Also, it is confusing what 450 pixels means for a scene classifier that works on the image.\n\n(4) The paper is written in a convoluted way:\n   (a) It is not clear if the semantic model is trained along with the RL model end-to-end or not.\n   (b) Regarding multi-target sub-policies, is there a separate policy for each pair of intermediate targets? \n   (c) Regarding inference and planning on M, what is \\tau exactly? How is the length of the plan determined? \n   (d) Why is the model updated only after a fixed number of steps? That increases the episode length. \n\n(5) The number of T_i's is manually set to 8. That causes serious generalization issues. How do we know how many T_i's exist in a new environment?\n\n\nMinor comments:\n- The paper mentions \"An example of such environments is House3D which contains 45k real-world 3D scenes\". House3D includes only synthetic scenes. They should not be called real-world scenes.\n- How is the reward shaping done?\n\n****\nFinal comments after reading the response and the reviews:\n\nRegarding the fairness of the review, success rate is not sufficient to evaluate navigation agents. A random agent can achieve 100% success if it is given enough time. So it is totally fair to ask for a metric (such as SPL) that is a function of both success rate and episode length. \n\nI am going to increase the rating to 5 since some of my concerns have been addressed. There are still a number of issues:\n\n- The authors did not run the experiments with the termination action. I disagree that this is orthogonal to the focus of the paper. This is not just an additional action. It indicates whether the agent has learned anything or it is just a combination of better obstacle avoidance and luck. The SPL numbers are so low (maximum SPL is 6.19%) so adding the termination action will probably make the method similar to random.\n\n- There is a huge gap between success rate and SPL numbers. For instance, success rate is 66.4, while SPL is 5.84 (note that for some reason the SPL numbers are multiplied by 10 in the table). I doubt that the agent has learned anything meaningful in comparison to the baseline. I understand that the task is hard, but this gap is so huge.\n\n- A separate policy is trained for each sub-target. This doesn't scale. There should be one policy for all targets. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}