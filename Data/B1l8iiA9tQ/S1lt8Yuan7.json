{"title": "The proposed Backdrop is similar to the traditional Dropout method. Overall this paper lacks of novelty and the observed generalization performance does not have convincing justification. ", "review": "This paper proposes a stochastic based method, namely Backdrop, for updating the network structures via backpropagation type methods.  Backdrop inserts masking layers along the network; it acts as the identity in the forward pass, but as randomly masks parts of the backward gradient propagation. The paper claims this approach can significantly improves the overall generalization performance. \n\nAlthough some difference to Dropout is summarized in Section 2, I still feel these two methods have almost the same idea, with just different implementation. Actually this Backdrop seems to have one more limitation in the parameter complexity, as it introduces several mask layers but keep the dense structures from other intermediate layers. \n\nThe proposed Backdrop uses Bernoulli distribution to select active variables. This is the very fundamental way in the conventional Dropout method. On the other hand, the authors do not  provide convincing justification how this can guarantee the improvement in subsequent generalization. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}