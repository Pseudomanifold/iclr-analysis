{"title": "Small modification and not enough comparison to other methods", "review": "The authors propose to apply Dropout only in the backward pass, by applying a mask sampled from a Bernoulli distribution. They claim that this method can help in situations like optimizing non-decomposable losses where minibatch SGD is not viable. \n\nFirst and foremost, the paper has an acknowledgement paragraph that gives information violating, in my sense, the anonymity requirement. \n\nThis being said, I have other concerns with the paper, and this possible violation didn't effect much my rating. \n\nFirst, the authors claim that the proposed method \"is a flexible strategy for introducing data-dependent stochasticity into the gradient\". However, it doesn't seem to me that the sampled dropped nodes are data-dependent. \n\nIt is also not clear to me why the proposed method is better suited to non-decomposable losses and hierarchically structured data than the classical Dropout.\n\nMoreover, while the method is clearly related to Dropout, the paper lacks of comparison to this regularizer. \n\nThis being said, the idea is sound, and can have a good impact in for example combining the good aspects of batch-normalization and dropout. However, the authors structured the paper on a completely different argument that doesn't convince me for the reasons cited above.     ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}