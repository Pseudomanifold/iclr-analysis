{"title": "Experiments need to be improved", "review": "In response to the authors' rebuttal, I have increased my ratings accordingly. I strongly encourage the authors to include those ablative study results in the work. I also strongly recommend an ablative study on importance sampling so as to provide more quantitative results, in addition to Fig. 4. Finally, I hope the authors can consider more advanced importance sampling techniques and explore whether it helps you get better results in even higher dimensions.\n\n=================================\nThis paper proposes several enhancements to a neural network method for computing committor functions so that it can perform better on rare events in high-dimensional space. The basic idea is using a variational formulation with Dirichlet-like boundary conditions to learn a neural committor function. The authors claim to improve a previous neural network based method by i) using a clever parameterization of the neural committor function so that it approximately satisfy the boundary condition; ii) bypassing the difficulty of rare events using importance sampling; and iii) using collective variables as feature engineering.\n\nGenerally I feel this paper is well written and easy to understand, without requiring too much background in physics and chemistry. The application is new to most people in the machine learning community. However, \nthe main contributions of this paper are empirical, and I found the experiments not very convincing. Here are my main concerns:\n\n1. There is almost no ablation study. The parameterization of committor function satisfies the Dirichlet boundary condition, which is aesthetically pleasing. However, it's unclear how much this improves the regularization used in the previous method. Similarly, without importance sampling, will the results actually become worse? What changes if the collective variables are removed? There is even no comparison with the previous neural network based method on computing committor functions, though the authors cited it.\n\n2. In the experiment on extended Mueller potentials, authors use the FEM results as the ground truth. However, it is not clear how accurate those FEM solutions are. Without this being clarified, it is unclear to me that the RMSE and MAE results in Table 1 are meaningful. Maybe try some simpler problem where the committor functions can be computed exactly?\n\n3. In experiments the authors often argue that results will improve when networks become deeper. However, all network architectures used in the paper are narrow and shallow when viewed from the perspective of modern deep learning. If the authors want to stress this point, I would expect to see more experimental results on neural network architectures, where you vary the depth of the network and report the change of results.\n\n4. \"Then we use the result as the initial network to compute the committor function at T = 300K\" => Did you first train a neural committor on samples of T = 800K and use its weights as initialization to the neural committor for T = 300K? Please clarify this more.\n\n5. Finally, I think the importance sampling technique proposed in this paper can be improved by other methods, such as annealed importance sampling. The largest dimension tested in this paper is only 66, which is still fairly small in machine learning, and I don't expect the vanilla importance sampling can work in higher dimensions.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}