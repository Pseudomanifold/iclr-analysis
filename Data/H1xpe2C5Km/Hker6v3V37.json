{"title": "Good paper which seems technically correct. Not sure if the method will generalize well beyond MNIST.", "review": "Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset.\n\nThe paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model?\n\nAdditionally, think about a pseudo-code for improved understandability. \n\nSome minor concerns/ notes to the authors:\n1.\tAt page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters?\n2.\tPage 6; baseline model: Why do you removed the pooling layers?\n3.\tI\u2019m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I\u2019m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar.\n4.\tWhy is the magnitude of lambda1 so different between the two dataset that you used?\n5.\tCould you add the inference times to your tables and discuss that in addition?\n6.\tWhat kind of noise is added to MNIST?\n7.\tWhat is the state-of-the-art performance on the Hippocampus dataset?\n8.\tWhat would be the performance in your experiments with a MaskRCNN segmentation network?\n9.\tI\u2019m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. \n10.\tFor both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix\u2026\n\t\nMy major concern is that both datasets are not dealing with real background noise. I\u2019m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete.\nTo make your results transparent you could think about to publish the code somewhere.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}