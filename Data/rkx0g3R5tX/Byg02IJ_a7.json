{"title": "Interesting idea, need more clarification and detail, not sure if language modeling is good application", "review": "The mutually exclusive assumption of traditional softmax can be biased in case negative samples are not explicitly defined. This paper presents Cooperative Importance Sampling towards resolving this problem. The authors experimentally verify the effectiveness of the proposed approach using different tasks including applying matrix factorization in recommender system, language modeling tasks and a task on synthetic data.\n\nI like this interesting idea, and I agree with the authors that softmax does exist certain problem especially when negative samples are not well defined. I appreciate the motivation of this work from the PU learning setting. It would be interested to show more results in PU learning setting using some synthetic data. I am interested to see the benefit of this extension of softmax with respect to different amount of labeled positive samples.\n\nHowever, I am not completely convinced that the proposed method would be a necessary choice for language modeling tasks.\n--To me, the proposed method has close connection to 2-gram language model. \n--But for language tasks, and other sequential input, we typically make prediction based on representation of very large context. Let\u2019s say, we would like to make prediction for time step t given the context of word_{1:t} based on some recurrent model, do you think the proposed softmax can generally bring sizable improvement with respect to traditional choices. And how?\n\nBy the way, I think the proposed method would also be applicable in the soft-label setting.\n\nFor the experiments part, maybe put more details and discussions to the supplementary material.\nA few concrete questions.\n-- In some tables and settings, you only look at prec@1, why? I expect the proposed approach would work better in prec@K.\n-- Can you provide more concrete analysis fortable 6? Why proposed methods does not work well for syntactic. \n-- Describe a little bit details about MF techniques and hyper-parameters you used. \n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}