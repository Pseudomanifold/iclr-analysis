{"title": "missing critical details in formulation and evaluation", "review": "This paper proposed PMES to relax the exclusive outcome assumption in softmax loss. The proposed methods is motivated from PU settings. The paper demonstrate its empirical metrit in improving word2vec type of embedding models. \n\n- on experiment: \n-- word2vec the window size = 1 but typically a longer window is used for NS. this might not reflect the correct baseline performance. is the window defined after removing rare words? what's the number of NS used? how stop words are taken care of? \n-- would be good to elaborate how CIS in word similarity task were better than full softmax. Not sure what;s the difference between the standard Negative sample objective. Can you provide some quantitative measure?  \n-- what is the evaluation dataset for the analogy task? \n\n-- MF task: the results/metrics suggests this is a implicit [not explicit (rating based)] task but not clearly defined. Better to provide - embedding dimensions, datasets positive/negative definition and overall statistics (# users, movies, sparsity, etc), how the precision@K are calculated, how to get a positive label from rating based dataset (movielens and netflix), how this compares to the plain PU/implicit-matrix factorization baseline. How train/test are created in this task?\n\n\n- on problem formulation:\nin general, it is difficult to parse the technical contribution clearly from the current paper. \n-- in 3.3., the prob. distribution is not the standard def of multi-variate bernoulli distribution.\n-- (6) first defined the support set but not clear the exact definition. what is the underlying distribution and what is the support for a sington means?\n-- it is better to contrast against the ns approximation in word2vec paper and clarify the difference in term of the mathematical terms. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}