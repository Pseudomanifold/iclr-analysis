{"title": "Review", "review": "The paper provides a new system that combines a number of neural networks to predict chemical reactions. The paper brings together a number of interesting methods to create a system that outperforms the state of the art.\n\nGood about this paper: \n - reported performance: the authors report a small but very consistent performance improvement.\n - the authors propose an approach that puts together many pieces to become an effective approach to chemical reaction prediction. \n -\n\nProblematic with this paper\n - this paper is impossible to understand if you only refer to the ten pages of content. There are at least 5 pointers in the paper where the authors refer to the appendix for details. Details in many of these cases are necessary to even understand what is really being done:  p3: rewards p4: message passing functions, p5: updating states, p9: training details. Further, The paper has some details that are unnecessary - e.g. the discussion of global vs. local network on p4 - this could go into the appendix (or be dropped entirely)\n\n - the model uses a step-wise reward in the training procedure (p3) -> positive reward for each correct subaction. It is not clear from the paper whether the model requires this at test time too (which should not be available). It's not clear what the authors do in testing. I feel that a clean RL protocol would only use rewards during training that are also available in testing (and a final reward)\n\n - eq 7: given there is an expontential in the probability - how often will the sampling not pick the top candidate? feels like it will mostly pick the top candidate. \n\n - eq 9: it's unclear what this would do if the same pair of atoms is chosen twice (or more often)\n\n - the results presented in table 3: it appears that GTPN alone (and with beam search) is worse than the previous state of the art. only the various post processing steps make it better than the previous methods. It's not clear whether the state of the art methods in the table use similar postprocessing steps or whether they would also improve their results if the same postprocessing steps were applied. \n \n\nminor stuff: \np2: Therefore, one can view GTPN as RL -> I don't think there is a causality. Just drop \"Therefore\"\np2: standard RL loss -> what is that? \neq. 2: interestin gchoice to add the vectors - wouldn't it be easier to just concatenate?\np4: what does \"NULL\" mean? how is this encoded?\np4 bottom: this is quite uncommon notation for me. Not a blocker but took me a while to parse and decrypt.\np5: how are the coefficients tuned?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}