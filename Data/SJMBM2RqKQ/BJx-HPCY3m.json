{"title": "Review", "review": "The paper addresses the problem of lifelong learning of neural networks - a setting where learning is performed on a continuously arriving new tasks without having access to previously encountered data.\nAuthors propose a method that prevents catastrophic forgetting typical for naive application of stochastic gradient descent by preventing supposedly important weights to change (in either soft of hard manner), where the weight importance is assessed by its signal to noise ratio estimated from the corresponding (approximate) posterior distribution.\nAuthors evaluate their approach on a set of image classification datasets and find it superior to the PackNet baseline as well as few simpler ones.\n\nThe idea of using uncertainty estimates obtained from Bayesian training to adjust weight updates is natural and potentially very promising. \nHowever, to me this paper does not seem to investigate the idea sufficiently deep.\n\nThe weight pruning or hard masking variant of the method depends on a very important hyperparameter p (size of the mask) which is unclear how to set beforehand. \n\nI also struggle with understanding the weight regularisation or soft masking variant. \nAuthors seem to get their inspiration in the idea of assumed density filtering, where the posterior for 1:T-1 is approximated and used a prior for task T (last sentence on page 5).\nAt the same time, in Algorithm 2, line 6 the prior is defined as the standard BBB mixture prior and not the approximate posterior from the previous task.\nQuite oddly, parameters of the _approximate posterior_ are being quadratically regularalized to not deviate from parameters of the _approximate posterior_ from the previous task. \nThis deviates from the original idea and requires additional justification.\nBesides that, I find the way this regularisation is applied potentially problematic for the variance parameter (last term in eq. 6).\nHere authors apply the regularisation to the parameter of the softplus transformation they use, but scale it with the inverse std deviation which is the \u201cclassical\u201d parametrisation. The choice of parametrisation was not discussed, however, clearly different parametrizations may lead to very different results. \n\nOn the experimental side, I have two major issues:\n1. The datasets considered are very small, authors could consider using ImageNet, especially given that they already work with 224x224 images.\n2. The only prior work used as a baseline is PackNet, while there is no reason why other established methods such as EWC are not applicable.\n\nMinor comments:\nThe middle expression in eq. 5 seems to miss the -log p(D_T | D_{1:T-1}) term which does not change the latter expression (since it does not depend on parameters theta).\nPage 3: \u201ccitestochastic methods\u201d, a citation seems to be missing.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}