{"title": "Good motivation, minor contributions in term of algorithms", "review": "Motivated from leveraging the uncertainty information in Bayesian learning, the authors propose two algorithms to prevent forgetting: Pruning and Regularization. Experiments on several sequential learning tasks show the improved performance.\n\nQuality:  The description on the related work is comprehensive. The proposed algorithms seem easy to follow. \n \nClarity: Low \n\nThe contributions in terms of algorithms are clearly presented. However, the writing can be largely improved.\n\n(1) Some claims are improper:  I don't think it's accurate to say that most of lifelong learning is non-Bayesian (In introduction), and EWC is derived from a Bayesian perspective, and Variational Conditional Learning is a very Bayesian approach.\n\n(2) Please proofread the submission: \nTypos: e.g.,  \"Beysian\", \"citestochastic methods\";  \nStyle: x is not bold occasionally, but has the meaning given the context. \n\nOriginality: It seems to be the first work that leverages the variance in Bayesian Neural Nets (BNN) to prevent forgetting. My understanding that EWC also consider the variance, but in a different way. \n\nSignificance: \nIt is good to consider variance/uncertainty for lifelong learning, and should be encouraged.\nHowever, the comparison to the representative algorithms or state-of-the-art is missing in this submission. For example, EWC/IS, or method in [*].  Is it possible to run the experiments on more standard datasets, such as [*].\n\n[*] Overcoming Catastrophic Forgetting with Hard Attention to the Task, ICML 2018\n\n\nQuestions:\n1. In (6), there are three terms on the right side, it seems the 2nd term include the 3rd term, why do we need to add the 3rd term again?\n\n2. \"Once a task is learned, an associated binary mask is saved which will be used at inference to recover key parameters to the desired task. The overhead memory caused by saving the binary mask (less than 20MB for ResNet18), is negligible given the fact it completely eliminates the forgetting\"\n\nTo me, saving a binary mask means saving \"partial\" model. First, this is additional parameter saving. Second, in the inference stage, one can recover the corresponding best model using the mask, how close is it to cheating? (Perhaps I am not an expert in lifelong learning). \nCan you put the model size of ResNet18, so that the readers can understand 20MB is small/negligible compared to the full model. \n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}