{"title": "this paper uses DPI in a wrong way", "review": "This paper proposes a justification to one observation on VAE: \"restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization\". The explanation given in this work is based on Gaussian mean-field approximation.\n\nI had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example\n\n- the sentence under eq. (2)\n- the sentence \"Bacause the identity of the datapoint can never be learned by ...\" What is the identity of a dat point?\n\nIt looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods. Somehow, those connections are not clear to me.\n\nBesides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.\n\nAs in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z). \n\nHowever, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model). Either case, I don't think we can have the inequality in eq. (5).  ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}