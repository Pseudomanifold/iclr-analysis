{"title": "Official review", "review": "Update:\n\nI appreciate the effort put by the authors into improving the paper. The revised draft is much better than the initial one. But I agree to AnonReviewer2 in that the degree to which this paper has to be modified goes beyond what the review process (even at ICLR) assumes. It is wrong to submit a very unfinished paper and then use the review period to polish it and add results. This incurs unnecessary extra load on the review process.\n\nThe added 2D results are toy-ish and somewhat confusing (I am not sure I understand what the meshgrids are and what do they tell us). Generally, some toy examples are good to illustrate the method, but they are not enough as a serious evaluation. The paper should have more results on complex datasets, like for instance ImageNet or LSUN or CIFAR or so, and should have comparisons to existing VAE-GAN hybrids, like VAE-GAN. Also, since a lot of the authors\u2019 motivation seems to come from psychophysics, showing some application to that might be a good way to showcase the value of the method (although this may not go well if submitting to machine learning conferences). \n\nI encourage the authors to further strengthen the paper and resubmit to another venue.\n\n-----\n\nThe paper proposes a model for image generation that combines an autoencoder with a generative adversarial network (GAN). The GAN is used to enforce that interpolations between latent vectors of two samples from the training set are decoded to realistic images. The method is applied to attribute manipulation and interpolation of face images.\n\nPros:\n1) A simple and reasonable formulation\n2) Visually good reconstruction of samples and convincing interpolation between samples on the CelebA-HQ dataset.\n3) Good qualitative facial attribute manipulation results on the CelebA-HQ dataset.\n\nCons:\n1) Experimental evaluation is very limited. There is just one dataset and only qualitative results. This is unacceptable: the method should be evaluated on more datasets and there should be quantitative results. I do realize it is not trivial to get quantitative, but it is possible. For instance, a user study can always be performed. But I believe one could also come up with simpler-to-measure metrics for at least some of the reported tasks. There is no comparison to other methods for facial attribute manipulation (for instance, StarGAN).\n2) There is no ablation study. To which extent is each of the model components important? For instance, the interpolation adversarial loss, the discriminator/generator balancing term, the network architecture (autoencoder discriminator)?\n3) As I understand, it is impossible to randomly sample directly from the model, only interpolate/modify existing images. This is a difference from most of prior work. It should be discussed clearly.\n4) Related work discussion is quite brief and misses some relevant work, for instance Adversarial Autoencoders (Makhazani et al., ICLR 2016, somewhat related) or Adversarially Constrained Autoencoder Interpolations (Berthelot et al., arxiv 2018, it\u2019s concurrent, but could be good to discuss).\n5) Writing is not of very high quality. There are typos, grammatical issues, and questionable statements. The manuscript should be significantly improved. Specific comments:\n- The structure is quite strange. There is no separation between the method and the experiments, the related work comes in very late and is very brief. This is all not critical, but confusing.\n- A typo in the title: should be \u201cencourages\u201d\n- Second sentence of the introduction should be supported with evidence (for instance references)\n- \u201cTwo unsupervised neural network based algorithms, the Autoencoder (AE; Hinton & Salakhutdinov\n2006) and Generative Adversarial Network (GAN; Goodfellow et al. 2014), are at present the most popular tools in generative modeling.\u201d - Vanilla autoencoders are not very popular tools for generative modeling. Variational autoencoders and some other flavors are.\n- \u201cUnsupervised neural network approaches are often ideal generative models because they require little or no tweaking of network architectures to handle entirely new datasets.\u201d I do not really get this sentence. What is the alternative to unsupervised generative models? Why do unsupervised approaches not require tweaking? (In my experience, they very well benefit from tweaking.)\n- \u201c\u2026 a lack of certain constraints on the generative capacity of current neural-network based generative models make it challenging to infer structure from their latent generative representations.\u201d What does \u201ca lack of certain constraints\u201d mean? There are some constraints, for instance the latent space is usually forced to correspond to a fixed distribution. Moreover, there is a lot of work on disentangling that also aims to find structure in latent spaces (for instance, InfoGAN).\n- \u201cand promotes convexity in the model\u2019s generative capacity.\u201d What is convexity in the capacity? I do not think this is grammatical.\n- In 1.1 the mathematical notation seems wrong. Does X really denote a set? In what follows it seems that X is used interchangeably for three different things: a sample from the dataset, the set of training samples, and the space the samples come from.\n- \u201cbidirectionality of adversarially generated data.\u201d What is bidirectional data?\n- \u201cAEs tend to produce blurry images due to their pixel-wise error functions (Larsen et al., 2015)\u201d Perhaps this was intended to refer to VAEs. AEs can generate perfectly sharp images if given enough capacity. \n- \u201cmethod more greatly resembles the original data than other GAN-based methods\u201d Method does not resemble data\n- \u201cDue to their exact latent-variable inference, these architectures may also provide a useful direction for developing generative models to explore latent-spaces of data for generating datasets for psychophysical experiments.\u201d This is mentioned a few times, but never supported\n- Acknowledgements should not be in the review version (can violate anonymity)\n\n5) Minor: Why is a Gaussian around the midpoint used for interpolations? Why not all convex combinations of two, or possibly more, samples? \n\nTo conclude, the paper presents quite good qualitative results on the CelebA-HQ dataset, but has problems with the thoroughness of the experimental evaluation, discussion of the related work, and presentation. The paper cannot be published in its current form. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}