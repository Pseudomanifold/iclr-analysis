{"title": "A very interesting and rather clear paper on quantized ReLU neural networks", "review": "The authors propose in this paper a series of results on the approximation capabilities of neural networks based on ReLU using quantized weights. Results include upper bounds on the depth and on the number of weights needed to reach a certain approximation level given the number of distinct weights usable. The paper is clear and as far as I know the results are both new and significant. My only negative remark is about the appendix that could be clearer. In particular, I think that figure 2 obscures the proof of Proposition 1 rather than the contrary. I think it might be much clearer to give an explicit neural network approximation of x^2 for say r=2, for instance.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}