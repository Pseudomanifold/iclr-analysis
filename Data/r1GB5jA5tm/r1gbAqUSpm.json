{"title": "Nice idea for important problem, but requires validation on algorithm speed.", "review": "This paper proposes adversarial sampling for pool-based active learning, which is a sublinear-time algorithm based on 1) generating \u201cuncertain\u201d synthetic examples and 2) using the generated example to find \u201cuncertain\u201d real examples from the pool. I liked the whole idea of developing a faster algorithm for active learning based on the nearest neighborhood method. However, my only & major concern is that one has to train GANs before the active learning process, which might cost more than the whole active learning process.  \n\nPros: \n- This paper tackles the important problem of reducing the time complexity needed for active learning with respect to the pool size. I think this is a very important problem that is necessary to be addressed for the application of active learning.\n-The paper is well written and easy to understand.\n-I think the overall idea is novel and useful even though it is very simple. I think this work has a very promising potential to be a building block for future works of fast active learning.\n\nCons:\n-There is no theoretical guarantee on the \"uncertainty\" of obtained real examples. \n-The main contribution of this algorithm is computational complexity, but I am not very persuaded by the idea of using the GAN in order to produce a sublinear (faster) time algorithm for active learning, since training the GAN may sometimes take more time that the whole active learning process. Explicitly describing situations where the proposed method is useful seems necessary. I would expect the proposed algorithm to be beneficial when there is a lot of queries asked and answered, but this seems unlikely to happen in real situations.  \n-Empirical evaluation is weak, since the algorithm only outperforms the random sampling of queries. Especially, given that sublinear nature of the algorithm is the main strength of the paper, it would have been meaningful to evaluate the actual time spent for the whole learning process including the training of GANs. Especially, one could also speed-up max entropy criterion by first sampling subset of data-points from the pool and evaluating upon them. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}