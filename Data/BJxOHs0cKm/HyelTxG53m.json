{"title": "Worth publishing work deserving a more rigorous presentation", "review": "The authors study generalization capabilities of neural networks local minimums thanks to a PAC-Bayesian analysis that grasps the local smoothness properties. Even if some assumptions are made along the way, their analysis provides a metric that gives insight on the accuracy of a solution, as well as an optimization algorithm. Both of these result show good empirical behavior.\n\nHowever, despite my favorable opinion, I consider that the paper presentation lacks rigor at many levels. I hope that the criticism below will be addressed in an eventual manuscript.\n\nIt is confusing that Equations (4) and (9) defines slightly differently \\sigma*_i(w*,\\eta,\\gamma). In particular, the former is not a function of \\eta. \n\nThe toy experiment of Figure 1 is said to be self-explainable, which is only partly true. It is particularly disappointing because these results appear to be really insightful. The authors should state the complete model (in supplementary material if necessary). Also, I do not understand Figures (b)-(c)-(d): Why the samples do not seem to be at the same coordinates from one figure to the other? Why (d) shows predicted green labels, while the sample distribution of (b) has no green labels?\n\nIt is said to justify the perturbed optimization algorithm that Theorem 1 (based on Neyshabur et al. 2017) suggests minimizing a perturbed empirical loss. I think this is a weak argument for two reasons:\n(1) This PAC-Bayes bounds is an upper bound on the perturbed generalization loss, not on the deterministic loss.\n(2) The proposed optimization algorithm is based on Theorem 2 and Lemma 3, where the perturbed empirical loss does not appear directly.\nThat being said, this does not invalidate the method, but the algorithm justification deserves a better justification\n\nThere is a serious lack of rigor in the bibliography:\n- Many peer-reviewed publications are cited just as arXiv preprints\n- When present, there is no consistency in publication names. NIPS conference appears as \"Advances in Neural ...,\", 'NIPS'02\", \"Advances in Neural Information Processing Systems 29\", \"(Nips)\". The same applies to other venues.\n- Both first name initials and complete names are used \n- McAllester 2003: In In COLT\n- Seldin 2012: Incomplete reference\n\n Also, the citation style is inconsistent. For instance, the first page contains both \"Din et al, (2007) later points out...\" and \"Dziugaite & Roy (2017) tries to optimize...\" \n\nTypos:\n- Page 3: ...but KL(w*+u | \\pi) => KL(w*+u || \\pi)\n- In this/our draft: Think to use another word if the paper is accepted\n- Line below Equation (5): \\nabla^2 L => \\nabla L (linear term)\n- it is straight-forward -> straightforward\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}