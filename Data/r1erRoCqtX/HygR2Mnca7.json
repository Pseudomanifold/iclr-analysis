{"title": "limited novelty and experimental results", "review": "This paper discussed a non-uniform sampling strategy to construct minibatches in SGD for the task of learning embeddings for object associations. An example throughout the paper is learning embeddings for a set F of focus entities and set C of context entities. In general, for focus update, the algorithm draws for each minibatch certain amount of positive samples (i,j), i \\in F and j \\in C. Then for each positive pair, we select certain amount of negative samples (i,j\u2019) for j\u2019 \\in some uniformly randomly selected subset C\u2019. The same algorithm is implemented for context update, and the training alternates between the two. The authors choose similar positive object in one minibatch since it\u2019s more efficient. Therefore, LSH hashing is used to point similar items to similar keys. Two similarity measures are used here, Jaccard similarity and cosine similarity. Some experiments are demonstrated on synthetic data and two real datasets to show the effectiveness of their method.\n\nConcerns:\n1.\tEvery piece of the method has been well studied, and the combination of them proposed in this paper does not seem very novel.\n2.\tAlgorithm 4, which is the hashing for Jaccard similarity, seems wrong. Only using iid exponentials cannot make collision probability equal Jaccard similarity.\n3.\tLittle experiments on real datasets. No comparison with other non-uniform minibatch construction methods (there should be some).\n4.\tNo quantitative analysis. \n5.\tStructure of the paper could be improved. For example, it\u2019s better to put section 4 and 6 together.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}