{"title": "Interesting analysis, but a bit confusing in some parts", "review": "This paper presents an analysis of the capacity of different models of deep neural networks with and without parameter quantization. The capacity of a network is calculated as the maximum of the mutual information between the real output and the neural network output, considering both of them as random variables. For the estimation of the network capacity the network is trained to maximize the mutual information given random inputs and random labels. As shown in Fig. 2 (a), the capacity of the network corresponds to the maximum number of random samples that a network can memorize. The capacity can be divided by the number of parameters, so that we can estimate how many bits of information every parameter can memorize, and therefore having an estimation of the maximum parameter quantization that we can use without losing memorization capabilities. Interestingly, the measured capacity seems to not depend on the number of parameters, but on the architecture of the network. Experiments on real data seem to show that the estimation made is conservative and when the aim of the network is generalization, the number of bits per parameter can be reduced further.\n\nPros:\n- The idea of evaluating the network capability of memorization for estimating the quantization of the parameters make sense to me and is novel in my knowledge.\n- With the proposed experiments, one can have an idea about a possible quantization of the network parameters that is independent of the data that we want to train on.\n\nCons:\n- The presentation of the paper is sometimes confusing. The conclusion gives a better explanation of the work than the introduction. Sec. 3.3 talks about weight perturbation, but no figure or experiments are shown until section 5.2 Figures are often very far from the corresponding text. In general, I found difficult to keep track of where we are in the paper.\n- Some parts of the text should be improved for a better comprehension. For instance in equ.1 \\hat{Y} is not defined. Also in the same section, the authors talk about mutual information of a trained network, but in my understanding the mutual information should be between two random variables. See also additional comments.\n- The connection between memorization and generalization is not clear. The authors say that the estimation obtained with memorization is conservative, but it would be good to have a better understanding of that. Finally we want to speed-up the network with quantization and if the estimation is too conservative, we will be force to empirically test the quantization again, and therefore the proposed estimation would not really help.\n- The proposed experiments can help us to estimate the amount of quantization of a given network independently of the data, which is quite nice. However, this estimation is still based on relatively long experiments.\n\nOverall evaluation:\nThe proposed idea and experiments seem interesting, however the presentation of the paper needs some extra work to make it easier to read and understand. It took me several passes before understanding the paper.\nI am willing to increase my score if the authors will be able to improve the quality of the presentation of the text and the experiments.\n\nAdditional comments:\n- From the experiments the estimated bits per parameter for memorization are: 2.3 bit/param for the fully connected networks, 3 bits/param for CNN and 3.7 for RNN. The idea is that more the parameters are \"shared\" and more bits they require. Now, based on your observations, the actual capacity when using quantized parameters is of 6, 8 and 10 respectively. So, why there is a discrepancy between estimation and real experiments?\n- In the last paragraph of sec. 3.1 the authors explain the training protocol for the experiments. I do not fully understand the need of those three phases, and the meaning of the boundary values for the hyper-parameters.\n- In Fig. 4 N_{in} is confusing because before you used N for the number of samples and n_{in} for the number of input dimensions.\n- From Fig. 3(a), it seems that for fully connected it is needed at least a 5 bits quantization to keep full accuracy. Why in the text you always mention 6 bits instead? ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}