{"title": "A good setup but very thin on relevant conceptual details", "review": "\nThis paper indeed asks a very pertinent question about quantifying the effect of  parameter quantization on a neural net's performance. The authors take a very principled approach towards exploring this by splitting up their experiments for (fully connected) DNN, CNN and RNN.  And they define the notion of \"network capacity per parameter\" which they show is varying in a small interval around 2 for all architectures and data set sizes. \n\nBut the overall analysis of the paper is pretty thin and its hard to extract any specific insight out of this paper except to say that RNN's performance is hit the most by parameter quantization possibly because it had the largest capacity per parameter. \n\nBut this conclusion comes from a context which has multiple unconvincing issues surrounding it which I list below, \n\n1. \nIsnt plot 2(b) essentially saying that there is something wrong with this notion of \"capacity per parameter\" because it doesnt seem to be very sensitive to data size or architecture? Across a wide range of changes it seems to be hardly varying and that seems to suggest that it is possibly sensing some approximate invariant rather than a discriminator between the different situations! And I cant see any plot showing how this capacity vs parameters changes when quantization is done. Shouldnt that have been the most crucial thing to demonstrate? \n\n2.\nThe basic notion of \"capacity\" comes from the equation (1) which seems tied to using 0/1 labels and binary data vectors. But most experiments (figure 6 and 7 say) in this paper do not seem to be in this setup. Given this why should any of the insights in section 3.1 transfer to the rest of the paper? \n\n3.\nTo the best of my understanding it seems to me that a lot is going on in how the quantization is done and how the training plays with this. This is only lightly mentioned in the beginning of section 3.2 and this is hardly any information to understand exactly as to how quantization has been implemented in training. I strongly feel that a lot more needs to be said about it and there should have been a clear pseudocode explaining this process. For all I know at this point, literally all conclusions depend on this implementation detail and that is missing. \n\n4.\nEquation 3 looks somewhat confusing to me. The RHS is a random variable/function. Then what does it mean to say that the this random function is being plotted in Appendix C? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}