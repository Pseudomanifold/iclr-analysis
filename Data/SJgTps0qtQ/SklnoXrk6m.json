{"title": "Curriculum design for dynamics randomization with a Bandits style method during training. ", "review": "The paper looks at the problem of generalization across physical parameter varaition in learning for continuous control. The paper presents a method to develop a sampling based curriculum over env. settings for training robust agents. \n\n\n* The paper makes an interesting observation on inadvertent generalization in robust policy learning. \nHowever, the examples in both the cartpole and the pendulum cases seem not to be watertight. \nFor instance, the authors claim that \nBut from a dynamical system perspective in both cases, the controller is operating near limits. \nThe solution and subsequent generalization depend more on the topology of the solution space. \nA heavy Pendulum is an overdamped system and required the policy to operate at the limits of action to generate momentum for swing up. Hence a solution for a lighter pendulum in implicitly included. Similarly, the rolling ball is an underdamped system, and where the policy operates near zero limits in light ball case to prevent the system from going unstable. Adding mass results in damping which makes it easier. In this case, as well the solution space is implicitly contained.\n\n\nBut this is not a novel observation. Similar observations have been made for Robust control and Model-Reference Adaptive Control. \nThe paper also overlooks a number of related works in model-free randomization [4], adaptive randomization [3], adversarial randomization [5,6]. The method also does not compare with model-based methods for adaptive policy learning and iLQR based methods to handle this problem [2, 7].\n\n\nThe argument that the method is model-free is perhaps not as acceptable since the model parameters need to be known apriori for adaptation. The policy itself may be model-free but that is a design choice. \nA good experimental evaluation for this is generalization across known unknowns and unknown unknowns. \n\n\n* The algorithm itself is reasonable but the problem setup and choice of a discrete dynamics parameter choices are questionable. The bandit style method operates over a discrete decision set. \nIt also assumes in the multi-parameter setting that they are independent, which may not be true very often. \n\nThe algorithm proposed itself isnt novel, but would have been justified if the results supported the use of such a method. \n\n* Experiments are quite weak. \nBoth the experimental domains are rather simplistic with smooth nonlinear dynamics. There are more sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2].  \n\nIt would be useful to see how tis method works in more complicated domains and how the performance compares with simpler methods such as joint brute-force randomization both in performance and in computation.  \n\nQuestions: \n1. Please provide details of Algorithm 1. How are the quantities K and M related? \n2. What is the process of task initialization? What information is required and what priors are used. Uniform prior over what range?\n\n\nIn summary, the authors explore an interesting adaptive curriculum design method. However, in its current form, the work needs more thought and empirical evaluation for the sake of completeness. \n\n\nReferences:\n1. Model Reference Adaptive Control [https://doi.org/10.1007/978-1-4471-5102-9_116-1\n]\n2. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems [https://arxiv.org/abs/1707.04674]\n3. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles [https://arxiv.org/abs/1610.01283]\n4. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\n[https://arxiv.org/abs/1610.01283]\n5. Certifying Some Distributional Robustness with Principled Adversarial Training [https://arxiv.org/pdf/1710.10571.pdf]\n6. Adversarially Robust Policy Learning: Active Construction of Physically-Plausible Perturbations [http://vision.stanford.edu/pdf/mandlekar2017iros.pdf]\n7. Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization [https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf]\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}