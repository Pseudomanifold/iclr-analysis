{"title": "Straightforward paper, but unclear what we learn from it", "review": "*Update after discussion period*\nI remain unconvinced. The authors failed to address my clearly articulated request for a more thorough analysis of additional networks trained on ImageNet (e.g. ResNet), which I don't think is asking for too much given a discussion period of three weeks.\n\n\nSummary:\nThe authors find that (1) DNNs exhibit orientation selectivity in many of their hidden layers' units, (2) in the intermediate layers this selectivity emerges during training, concurrently with the network's ability to generalize, and (3) ablating orientation-selective units in the early layers impairs a network's generalization performance. \n\nStrengths:\n+ Very straightforward and easy to follow \n+ Technically sound\n\nWeaknesses:\n- Feels trivial\n- The claims seem to be too general\n\nConclusion:\nI'm torn on the paper. On the one hand, it reports some potentially interesting observations (e.g. trajectory of emergence of orientation selectivity over training). On the other hand, I'm not really sure what we learn from the paper.\n\n\nSpecific comments:\n\n- The result seems trivial. How should a network be able to recognize objects without detecting edges of certain orientation (which implies orientation selectivity)?\n\n- Related to the previous point, pretty much every single (supervised or unsupervised) learning objective investigated so far has produced orientation selectivity, so it seems pretty well established that orientation selectivity is somehow useful. The interesting question is what needs to be done on top of it in order to get a representation useful for object recognition, but in this respect the paper does not contribute anything.\n\n- The results are mostly on CIFAR-10 and only one network (VGG-16) trained on ImageNet is considered. Given the generality of the claims (\"orientation selectivity [plays] a causally important role in object recognition\" \u2013 abstract), the authors would have to show that their results also hold for other high-performing networks on ImageNet, and not just VGG-16 (sort of, see next point). Otherwise, an appropriate conclusion would be that orientation selectivity plays a causal role in the functioning of VGG-16 and some networks trained on CIFAR-10.\n\n- The analysis meant to establish causality (section 3.4) produces pretty mixed results on VGG-16 (Fig. A6b), where ablating the top 50% orientation-selective units in some layers has a *smaller* effect than ablating the rest. How do the authors explain this result?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}