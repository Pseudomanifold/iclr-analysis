{"title": "Interesting perspective connecting many machine learning objectives", "review": "This paper introduces an information-theoretic framework that connects a wide range of machine learning objectives, and develops its formal analogy to thermodynamics. \nThe whole formulation attempts to align graphical models of two worlds P & Q and is expressed as computing the minimum possible relative information (using multi-informations)  between the two worlds. Interestingly, this computation consists of four terms of mutual information, each of which is variationally bounded by a meaningful functional: entropy, rate, classification error, distortion. Finding points on the optimal feasible surface leads to an objective function with the four functionals, and this objective is shown to cover many problems in the literature. The differentials of the objective bring this framework to establish formal analogies between ML and thermodynamics: the first law (the conservation of information), the Maxwell relations, and the second law (relative entropy decrease). \n\nThe main contribution of this paper would be to provide a novel and interesting interpretation of previous ML techniques using an objective function in an information theoretic viewpoint. Drawing the objective from the tale of two worlds and connecting them with existing techniques is impressive, and the analogies to thermodynamics are reasonable. I appreciate this new perspective of this paper and think this direction is worth exploring for sure. The terms and relations derived in the course of this work might be useful for understanding or analyzing ML models.  \n\nOn the other hand, this paper is not easy to follow. It\u2019s written quite densely with technical details omitted, and in some parts lacking proper explanations, contexts, and implications. \nE.g., \n- In section 2, why the world Q is what we want?\n- Among the mutual information terms, it\u2019s not clear why I(Z_i; X_i, Theta) need to be minimized. After the chain rule, while the part of I(Z_i;Theta | X_i) needs to be minimized, isn\u2019t that I(Z_i; X_i) needs to be maximized? \n- The functionals and their roles (Section 2.1) need to be more clarified.\n- In the first paragraph of Section 3, why is that \u201cany failure of the distributional families \u2026. feature surface\u201d?\nFor a broader audience, I recommend the authors to clarify with more explanations, possibly, with motivating examples.\n- Formal analogies to thermodynamics (Section 4) are interesting, but remains analogies only without any concrete case of usefulness. The implications of the first and second laws are not explained in detail, and thus I don\u2019t see their significance.  In this sense, section 4 appears incomplete. I hope they are clarified. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}