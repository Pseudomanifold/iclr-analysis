{"title": "Mutual information-based representation learning with additional tricks for performance gain.", "review": "This paper presents a representation learning approach based on the mutual information maximization. \nThe authors propose the use of local structures and distribution matching for better acquisition of representations (especially) for images.\n\nStrong points of the paper are: \n* This gives a principled design of the objective function based on the mutual information between the input data point and output representation. \n* The performance is gained by incorporating local structures and matching of representation distribution to a certain target (called a prior).\n\nA weak point I found was: \nThe local structure and evaluation are specialized for classification task of images. \n\nQuestions and comments.\n* Local mutual information in (6) may trivially be maximized if the summarizer f (E(x) = f \\circ C(x) with \\psi omitted for brevity) concatenates all local features into the global one.\nHow was f implemented? Did you compare this concatenation approach?\n* Can we add DIM like a regularizer to an objective of downstream task? \nIt would be very useful if combining an objective of classification/regression or reinforcement learning with the proposed (8) is able to improve the performance of the given task.\n* C^(i)_\\psi(X) in (6), but X^(i) in (8): are they the same thing?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}