{"title": "Nice idea. Need more clarification.", "review": "This paper proposes a privacy framework where a privatizer, according to the utility and secret specified by users, provides a sanitized version of the user data which lies in the same space as the original data, such that a utility provider can run the exact algorithm it uses for unsanitized data on the sanitized data to provide utility without sacrificing user privacy. The paper shows an information theoretic bound on the privacy loss and derives a loss function for the privatizer to use. It then proposes an algorithm for the privatizer, evaluates its performance on three scenarios.\n\nThe paper investigated on an interesting problem and proposed a nice solution for synthetic data generation. However, I think the proposed framework and how the example scenarios fit into the framework needs to be described clearer. And more experimental evaluations would also help make the result more solid.\n\nMore detailed comments:\n- Do the user and privatizer need to know what the machine learning task is when doing the sanitization? Is it ok for the privatizer to define utility in a different way as the machine learning task? For example, as a user, I may want to hide my emotion, but I\u2019m ok with publishing my gender and age. In this case, can I use a privatizer which defines secret as gender and utility as (gender, age)? And will the synthetic data generated by such a privatizer be equally useful for a gender classifier (or an age classifier)? It would be good if it is, as we don\u2019t need to generate task-specific synthetic data then.\n- I think it might be interesting to see the effect of the privatizer when utility and secrecy are correlated (with a potentially different level of correlation). \n- It\u2019s not clear to me where the privatizer comes into the picture in the subject-within-subject example. It seems like users here are people whose face appear in front of the mobile device, so they probably won\u2019t be able to privatize their face image, yet the device won\u2019t be able to tell if users are in the consenting group without looking at their faces. I think it\u2019s better if more clarification on how each of the three scenarios fits into the proposed framework is provided.\n- Can different user have different secret? \n- In the experiment, it might be better to try different models/algorithms for the utility and secrecy inferring algorithm, to demonstrate how the privatizer protects secrecy under different scenarios.\n- I think there might be some related work on the field of fairness and transparency where we sometimes want the machine learning models to learn without looking at some sensitive features. It would be nice to add more related work on that side. \n- It\u2019s better to give more intuition and explanation than formulas in Section 3. \n- There are a few typos (e.g. Page2, 3rd paragraph, last sentence: \u201cout\u201d-> \u201cour\u201d; Equation (4), I(S, Q) should be I(S; Q)?; Page 8, 2nd paragraph, 1st line \u201cFigures in 5\u201d -> \u201cFigure 5\u201d) that need to be addressed. Texts in some figures, like Figure 2 and 3, might be enlarged. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}