{"title": "An intriguing but preliminary investigation into RNN dynamics and generalisation", "review": "Post-rebuttal update:\nThe authors have clarified their main messages, and the paper is now less vague about what is being investigated and the conclusions of the experiments. The same experimental setup has been extended to use CIFAR-10 as an additional, more realistic dataset, the use of potentially more powerful LSTMs as well as GRUs, and several runs to have more statistically significant results - which addresses my main concerns with this paper originally (I would have liked to see a different experimental setup as well to see how generalisable these findings are, but the current level is satisfying). Indeed, these different settings have turned up a bit of an anomaly with the GRU on CIFAR-10, which the authors claim that they will leave for future work, but I would very much like to see addressed in the final version of this paper. In addition some of the later analysis has only been applied under one setting, and it would make sense to replicate this for the other settings (extra results would have to fit into the supplementary material).\n\nI did spot one typo on page 4 - \"exterme\", but overall the paper is also better written, which helps a lot. I commend the authors on their work revising this paper and will be upgrading my rating to accept.\n\n---\n\nThe authors investigate the hidden state dynamics of RNNs trained on a single task that mixes (but clearly separates) pattern recognition and memorisation. The authors then introduce two curricula specific to the task, and study how the trained RNNs behave under different deviations from the training protocol (generalisation). They show that under the curriculum that exhibited the best generalisation, there exist more robust (persisting for long time periods) fixed/slow points in the hidden state dynamics. They then extend the optimisation procedure developed by Sussillo & Barak for continuous-time RNNs in order to find these points. Finally, they use this method to track the speed of these points during the course of training, and link spikes in speed to one of the curricula which introduces new classes over time.\n\nUnderstanding RNNs - and in particular how they might \"generalise\" - is an important topic of research. As done previously, studying RNNs as dynamical systems is a principled way to do so. In this line of work some natural objects to look into are fixed points and even slow points (Sussillo & Barak) - how long they can persist, and how large the basins of attraction are. While I believe the authors did a reasonable job following this through, I have some concerns about the experimental setup. Firstly, only one task is used - based on object classification with images - so it is unclear how generalisable these findings are, given that the authors' setup could be extended to cover at least another task, or at least another dataset. MNIST is a sanity check, and many ideas may fail to hold when extended to slightly more challenging datasets like CIFAR-10.\n\nSecondly, as far as I can tell, the results are analysed on one network per setting, so it is hard to tell how significant the differences are. While some analyses may only make sense for single networks, e.g. Figure 3, a proper quantification of some of the results over several training runs would be appropriate.\n\nFinally, it is worth investigating LSTMs on this task. This is not merely because they are more commonly used than GRUs, but they are strictly more powerful - see \"On the Practical Computational Power of Finite Precision RNNs for Language Recognition\", published at ACL 2018. Given the results in this paper and actually the paper that first introduces the forget gate for LSTMs, it seems that performing these experiments solely with GRUs might lead to wrong conclusions about RNNs in general.\n\nThere are also more minor spelling and grammatical errors throughout the text that should be addressed. For example, there is a typo on the task definition on page 2 - \"the network should *output* a null label.\"", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}