{"title": "Learned memory structure differs due to training paradigm", "review": "This paper titled <don't judge a book by its cover - on the dynamics of recurrent neural networks> studies how different curriculum learning results in different hidden state dynamics and impacts extrapolation capabilities. By training a 200-GRU-RNN to report the class label of a MNIST frame hidden among noisy frames, authors found different training paradigms resulted in different stability in memory structures quantified as stable fixed points. Their main finding is that training by slowly increasing the time delay between stimulus and recall creates more stable fixed point based memory for the classes.\n\nAlthough the paper was clearly written in a rush, I enjoyed reading it for the most part. These are very interesting empirical findings, and I can't wait to see how well it generalizes.\n\n# I find the title not very informative. Connection from 'Book' to 'Curriculum' is weak.\n\n# The task does not have inherent structure that requires stable fixed points to solve. In fact, since it only requires maximum 19 time frames, it could come up with weird strategies. Since the GRU-RNN is highly flexible, there would be many solutions. The particular strategy that was learned depends on the initial network and training strategy.\n\n# How repeatable were these findings? I do not see any error bars in Fig 2 nor table 1.\n\n# How sensitive is this to the initial conditions? If you use the VoCu trained network as initial condition for a DeCu training, does it tighten the sloppy memory structure and make it more stable?\n\n# I liked the Fig 2b manipulation to inject noise into the hidden states.\n\n# English can be improved in many places.\n\n# Algorithm 1 is not really a pseudo-code. I think it might be better to just describe it in words. This format is unnecessarily confusing and hard to understand.\n\n# Does the backtracking fixed/slow point algorithm assume that the location of the fixed point does not change through training? Wouldn't it make more sense to investigate the pack-projection of desired output at each training step?\n\n# PTMT, PMTP, and TaCu are not described well in the main text.\n\n# The pharse 'basin of attraction' is losely used in a couple of places. If there isn't an attractor, its basin doesn't make sense.\n\n# Fig 4 is not very informative. Also is this just from one network each?\n\n# Fig 5 is too small!\n\n# page 2: input a null label -> output a null label\n\n# it would be interesting to see how general those findings are on other tasks, e.g., n-back task with MNIST.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}