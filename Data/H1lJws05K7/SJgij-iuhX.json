{"title": "Good results; providing some insights on the selection of activation function.  ", "review": "Good results; providing some insights on the selection of activation function.  \n\nThis paper builds upon two previous works B.Poole etc. and S.S. Schoenholz etc. who initialized the study of random initialized neural network using a mean field approach (or central limit theorem.)\nThe two principal results of this paper are \n1. Initializing the network critically on the edge of chaos.  \n2. Identifying some conditions on the activation functions which allow good \"information flow\" through the network.   \n\nThe first result is not new in general (already appeared in Schoenholz etc. and many follow up mean field papers). However, the results about ReLU (initializing (weigh_variance, bias_variance)=(2, 0)) seems to be new. The author also shows that the correlations converge to 1 at a polynomial rate (proposition 3), which is interesting. \n\nThe second one is a novel part of this paper (proposition 5). If I understand correctly, the authors are trying to identify a class of activation functions (and suitable hyper-parameters) so that the network can propagate the sample-to-sample correlations (i.e. kernel) almost isometrically (please correct me if I am wrong). This is only possible 1) the activation functions are linear; OR 2) in the regime q->0, where the activation function has small curvature (i.e. almost linear). I think the results (and insights) are quite interesting. However, I don't think the authors provides enough theoretical or empirical evidence to support the claim that such activation functions can perform better.  \n\n\n\ncons:\n1. I don't think the experimental results are convincing enough for the reasons below:\n    1.1. All experiments are conducted over MNIST with testing accuracy around 96%.  The authors should consider using large datasets (at least Cifar10).\n    1.2 The width (<=80) of the network is too small while the theory of the paper assumes the width approaches infinity. Width>=200 should be a reasonable choice. It should be possible to train a network with depth~200 and width ~200 and batch_size~64 in a single machine.   \n    1.3. Figure 6(b) seems unconvincing. ReLU network should be trainable with depth>=200; see figure 4 of the paper: \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" \n\n\n2. The claim that swish is better than tanh because the latter suffers from vanishing of gradients is unconvincing. It has been shown in Schoenholz etc and many follow-up papers that ultra-deep tanh networks (>=1000 layers) can be trained with critical initialization. \n\n3. Again, I don't think it is convincing to make the conclusion that swish is better than ReLU based on the empirical results on MNIST. \n\n4. Using a constant learning rate (lr=0.001) for all depths (and all widths) is incorrect. I believe the gradients will explode as depth increases. Roughly, the learning rate should decay linearly with depth (and width) when the network is initialized critically.  \n\n\nIn sum, the paper has some interesting theoretical results but the empirical results are not convincing.  \n\n\nOther comments:\n1. The authors should explain the significance and motivation of proposition 4. In particular, explain why we need f(x)~x. \n2. Consider replacing \"Proposition 4\" by  \"Theorem\", since it is the main result of the paper.  \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}