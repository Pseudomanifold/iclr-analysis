{"title": "Needs more work", "review": "Synopsis: The paper proposes a neural network based model that integrates a\nsubmodular function. The proposal combines gradient based optimization technique\nwith submodular framework named `Differentiable Greedy Network' (DGN). The\nproposal is focused on retrieval tasks especially directed towards the subtask\nof retrieving evidence sentences given a claim on the FEVER dataset. The\nproposal involves building a neural network that recursively selects sentences\nthat support the claim with a greedy algorithm that maximizes the submodular\nfunction. The proposal core of the proposal exploits sums of concave composed\nwith monotone modular functions (SCMM) and ideas from gumbel-softmax based\nliterature to build a submodular function. The proposed submodular function has\nlearnable parameters and the submodular function uses a softmax with a\ntemperature hyperparameter. The paper shows that the proposal is competitive and\noutperfoms a couple of baselines. \n\nPositives: The paper is well motivated and proposes fairly well grounded\nsolution. \n\nProblems: \n* While the paper seems to be well motivated, some of the claims are\njustified in a hand-waving manner and could be better justified with either a\nmore theoretical exposition or empirical analysis. For eg., the utility of ReLU,\nwhile the theoretical justification is clear, how important was the\nnon-linearity in the entire scheme of DGN? Or the relation to attention\narchitectures - while the proposed method is superior to simple attentional architectures \ncan this be related to decomposable attention [1] or the hierarchical attention architecture[2]?\n\n* Ablation between different families of claims: I suppose the submodular\nfunction is going to be very useful when the number of evidences are more than\ntwo. The paper could have benefitted from ablation between different types of\nclaims: claims with one/two sentence as evidence vs claims with more than two\nsentences as evidences.\n\n* The claim that greedy DGN outperforming conventional discrete opitmization\n  algorithms is strong given sparse comparisons to conventional techniques. \n\nQuestions:\n\n* From the experiments in the paper it seems like the temperature is\n  a very delicate hyperparameter and the range from $(3, 6)$ seems like the\n  function is not close to argmax. Would this mean that the trained model is\n  suboptimal? I would also suggest the authors to kindly add the ablations of\n  temperature (with annealing) to the appendix. \n\n* The descriptions of encoder and deepencoder are very confusing. Is the only\n  addition to the deep encoder is an additional non-linear layer? In any case,\n  Table 2 suggests the deep encoder performs comparably to DGN, apart from the\n  problems with class imbalance, given its performance, is it correct to say\n  that deepencoder learns robust representations? In other words, the advantages\n  of DGN are still not clear. Is the greedy algorithm helping in selecting the\n  k sentences? If so, how many of the cases have k>2 and was DGN better there? \n\n* What happens when the similarity is computed using say euclidean distance?\n  How would the temperature hyperparameter change?\n\n\n\nOther Issues: \n* The sections in paper need rewriting, especially section 4. \n\n* Comparisons: it would have been ideal to have the comparisons to attention\n  based approaches (simple attention, hierarchical attention) with standard\n  optimization techniques. \n\n\n[1] Parikh, Ankur P., et al. \"A decomposable attention model for natural language inference.\" arXiv preprint arXiv:1606.01933 (2016).\n[2] Yang, Zichao, et al. \"Hierarchical attention networks for document classification.\" Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}