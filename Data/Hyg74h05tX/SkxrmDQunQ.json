{"title": "Three ingredients for more powerful flow-based model", "review": "This paper offers architectural improvements for flow-based models that enable them to be very competitive with autoregressive models in terms of bits/dim metrics while still providing efficient sampling scheme. The three main contributions are the use of variational dequantization scheme, more powerful element-wise bijections (mixture of logistic CDF), and multi-head self-attention in the dependency structure. \nThe two first contributions are in my opinion the most interesting as:\n- variational dequantization demonstrates the improvement that one can obtain by redefining part of the image processing that has been overlooked before;\n- the inversion of element-wise bijection without closed form inverse can be efficiently approximated with bisection (binary search).\nThe performances achieved by the resulting model are in my opinion a stepping stone in the area of flow-based models and encouraging as to their potential. \nThe ablation study suggest that each contribution by themselves only improve slightly the model but that their simultaneous application results in a stronger boost in performance, which I can't explain from the paper. Nonetheless, some this ablation study was useful in tearing apart the contribution of each of several pieces of the model (missing pieces being gated convolutions, dropout, and instance normalization), although without explaining them.\nAlthough flow-based model can intuitively sample faster than autoregressive models, the measure of sampling time is a bit interesting as an actual evidence of that claim. But the analysis of sampling time should be done on same hardware as to fair comparison before it can be a convincing argument. \nConcerning variational dequantization, is there a reason coupling layer architecture was used instead of potentially more powerful model with less convenient inverses such as inverse autoregressive flow?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}