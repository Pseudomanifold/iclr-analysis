{"title": "interesting improvements for RealNVP/Glow models, but not well analysed", "review": "The paper improves upon the Real NVP/Glow design by proposing better dequantization schemes and more expressive forms of coupling layers. I really like Real NVP models, which I think are a bit underappreciated. Thus, I\u2019m happy that there are papers trying to improve their performance.  However, I wish this was done with more rigour.\n\nThe paper makes 3 claims about the current flow models: (1) it is suboptimal to use additive uniform noise when dequantizing images, (2) affine coupling layers are not expressive enough, and (3) the architectures fail to capture global image context. I\u2019ll comment on these claims and proposed solutions below.\n\n(1) I agree with the reasoning behind the need for a better dequantization distribution. However, I think the authors should provide an evidence that the lower bound is indeed loose when q is uniform. For example, for the CIFAR-10 model, the authors calculated a gap of 0.025 bpd when using variational dequantization. What would this gap be when using uniform q?  Maybe, a clear illustration of the dequantization effect on a simpler dataset or a toy example would be more useful.\n\n(2) My main concern about the mixture CDFs coupling layer is how much bigger the model becomes and how much slower it trains. I find this analysis crucial when deciding whether 0.05 bpd improvement as reported in Table 1 is worth the hassle.\n\n(3) As a person not familiar with the Transformer, I couldn\u2019t understand how exactly self-attention works and how much it helps the model to capture the global image context. Also, I think this problem needs a separate illustration on a dataset of larger images.  \n  \nThe experiments section is very weak in backing up the identified problems and proposed solutions. Firstly, I think it is more clear if the ablation study is done in reverse: instead of making Flow++ and removing components, start with the vanilla model and then add stuff.  Secondly, it\u2019s not clear if these improvements generalize across datasets, e.g. when images are larger than 32x32. Though, larger inputs may lead to huge models which are impossible to train when the resources are quite limited. That\u2019s why I find it important to report how much complexity is added compared to the initial Real NVP. Also, I think it\u2019s a well-known fact that sampling from PixelCNN models is slow unlike for Real NVPs, so I don\u2019t find the results in Table 3 surprising or even useful. \n\nTo conclude, I find this paper unfinished and wouldn\u2019t recommend its acceptance until the analysis of the problems and their solutions becomes better thought out.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}