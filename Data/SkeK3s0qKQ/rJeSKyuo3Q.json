{"title": "A simple novel idea for improving exploration in DRL", "review": "The main idea of this paper is to propose a heuristic method for exploration in deep reinforcement learning. The work is fairly innovative in its approach, where an episodic memory is used to store agent\u2019s observations while rewarding the agent for reaching novel observations not yet stored in memory. The novelty here is determined by a pre-trained network that computes the within k-step-reachability of current observation to the observations stored in memory. The method is quite simple but promising and can be easily integrated with any RL algorithm.\n\nThey test their method on a pair of 3D environments, VizDoom and DMLab. The experiments are well executed and analysed. \n\nPositives:\n-\tThey do a rigorous analysis of parameters, and explicitly count the pre-training interactions with the environment in their learning curves.\n-\tThis method does not hurt when dense environmental rewards are present.\n-\tThe memory buffer is smaller than the episode length, which avoids trivial solutions.\n-\tThe idea of having a discriminator assess distance between states is interesting.\n\nQuestions and critics:\n-\tThe tasks explored in this paper are all navigation based tasks, would this method also apply equally successfully to non-navigation domains such as manipulation? \n-\tMy main concern is that the pre-training of the embedding and comparator networks directly depends on how good the random exploration policy is that collects the data. In navigation domains it makes sense that the random policy could cover the space fairly well, however, this will not be the case for more complex tasks involving more complex dynamics.\n-\tIt was surprising to me that the choice of k does not seem to be that important. As it implicitly defines what \u201cnovelty\u201d means for an environment, I would have expected that its value should be calibrated better. Could that be a function of the navigation tasks considered?\n-\tThe DMLab results are not great or comparable to the state-of-the-art methods, which may hinder interpreting how good the policies really are. This was perhaps a conscious choice given they are only interested in early training results, but that seems like a confound.\n-\tThe architecture does not include an RNN which makes certain things very surprising even though they shouldn't (e.g. firing, or moving around a corner, are specifically surprising for ICM) as they cannot be learnt, but perhaps if they had an RNN in the architecture these would be easy to explain? Would be interesting to see what are the authors thoughts on this (apart from their computational complexity argument they mention)?\n-\tHaving the memory contain only information about the current episode with no information transfer between episodes seems a bit strange to me, I would like to hear the motivation behind this?\n-\tThe fact that the memory is reset between episodes, and that the buffer is small, can mean that effectively the method implements some sort of complex pseudo count over meta-states per episode? \n-\tThe embedding network is only trained during the pre-training phase and frozen during the RL task. This sounds a bit limiting to me: what if the agent starts exploring part of the space that was not covered during pre-training? Obviously this could lead to collapses when allowing to fine-tune it, but I feel this is rather restrictive. Again, I feel that the choice of navigation tasks did not magnify this problem, which would arise more in harder exploration tasks.\n-\tI think that alluding that their method is similar to babies\u2019 behaviour in their cradle is stretched at best and not a constructive way to motivate their work\u2026\n-\tIn Figure 6 and 7, all individual curves from each seed run are shown, which is a bit distracting. Perhaps showing the mean and std would be a cleaner and easier-to-interpret way to report these results?\n\nOverall, it is a simple and interesting idea and seems quite easy to implement. However, everything is highly dependent on how varying the environment is, how bad the exploration policy used for pre-training is, how good the embeddings are once frozen, and how k, action repeat and memory buffer size interact. Given that the experiments are all navigation based, it makes it hard for me to assess whether this method can work as well in other domains with harder exploration setups.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}