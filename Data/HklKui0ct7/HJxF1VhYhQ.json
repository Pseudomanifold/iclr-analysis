{"title": "A variance reduction technique for learning from logged bandit feedback. Proposes a surrogate policy that can be used on top of IPS and POEM.", "review": "Summary:\nThe paper considers the problem of learning from logged bandit feedback, and focuses on the problem of the ratio of the target policy and the logged policy (the basis of algorithms such as inverse propensity scoring). The paper proposes a surrogate policy to replace the logged policy with known parametrization, with a policy obtained by maximum likelihood estimation on the observed data. The authors present theoretical arguments that the variance of the value function estimate is reduced. Empirical experiments show that the surrogate policy can be used to improve IPS and POEM, and also works when the logging policy is unknown.\n\nThe paper analyses an important and interesting problem which is critical to many practical applications today. The proposed solution is modular, and the empirical experiments point to its usefulness. The theoretical analysis, while not fully explaining the proposed approach, provides comfort that there is reduced variance when using the maximum likelihood surrogate.\n\nOverall comments:\n- page 3, Section 3: It is unclear why the assumption that we know the logging policy, as well as its optimal parameter is a sensible one. In particular, the first paragraph seems to indicate that the surrogate policy some somehow the same parameterization and $\\hat{\\beta}$ is in the same space as $\\beta^*$, and just a different parameter. On one hand the authors seem to indicate that they know everything about the logging. On the other hand they seem to want to claim that not knowing the logging policy is ok. What happens when there is a model mismatch between the logging policy and the surrogate policy? Please expand on these two assumptions.\n- page 4, Section 3.1: It might be useful to have a toy example which exactly matches the requirements of Theorem 3.9, such that you can present empirical intuition about the terms in (3.13). In particular: what is the effect of assuming a deterministic reward? How does (3.14) grow? Why is the reduction of MSE greater than $\\xi(n)$?\n- Theorem 3.9: Please present the result that MLIPS is asympotically unbiased explicitly. Furthermore, the current proof of this main theorem should be structured better, so that it can be properly checked.\n\nMinor issues/typos:\n- page 3, above (3.1): In specific, we --> In particular, we\n- Figure 1: the legend is very confusing, making it totally unclear what the text is talking about. Please match text, caption and legend.\n- Section 4.3: please say that the data is the multilabel datasets of Swaminathan and Joachims in Table 1.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}