{"title": "interesting approach with inconclusive results", "review": "This paper presents an DFA-based approach to constrain certain behavior of RL agents, where \"behavior\" is defined by a sequence of actions. This approach assumes that the developer has knowledge of what are good/bad behavior for a specific task and that the behavior can be checked by hand-coded DFAs or PDAs. During training, whenever such behavior is detected, the agent is given a negative reward, and the RL state is augmented with the DFA state. The authors experimented with different state augmentation methods (e.g. one-hot encoding, learned embedding) on 3 Atari tasks.\n\nThe paper is clearly written. I also like the general direction of biasing the agent's exploration away from undesirable regions (or conversely, towards desired regions) with prior knowledge. However, I find the results hard to read.\n\n1. Goal. The goal of this work is unclear. Is it to avoid disastrous states during exploration / training, or to inject prior knowledge into the agent to speed up learning, or to balance trade-offs between constraint violation and reward optimization? It seems the authors are trying to do a bit of everything, but then the evaluation is insufficient. For example, when there are trade-offs between violation and rewards, we expect to see trade-off curves instead of single points for comparison. Without the trade-off, I suppose adding the constraint should speed up learning, in which case learning curves should be shown.\n\n2. Interpreting the results. 1) What is the reward function used? I suppose the penalty should have a large effect on the results, which can be tuned to generate a trade-off curve. 2) Why not try to add the enforcer during training? A slightly more complex baseline would be to enforce with probability (1-\\epsilon) to control the trade-off. 3) Except for Fig 3 right and Fig 4 left, the constraints doesn't seem to affect the results much (judging from the results of vanilla DQN and DQN+enforcer) - are these the best settings to test the approach?\n\nOverall, an interesting and novel idea, but results are a bit lacking.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}