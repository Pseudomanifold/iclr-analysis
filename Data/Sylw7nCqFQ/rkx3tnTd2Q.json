{"title": "An interesting approach for one-shot learning in images", "review": "This paper is about a novel framework to address one-shot learning task by augmenting the support set with automatically generated images. The framework is composed by two networks: a deformation network which is responsible to generate synthetic images and an embedding networks which projects images into an embedding space used to perform the final classification. Compared to previous work, the idea is similar in spirit to recent work that synthesise new images (Wang et. al. CVPR 2018) with GANs, but exploit a simpler idea based on the fusion of two images directly into the image domain with weights generated by a network. Extensive experiments on the standard ImageNet1K and miniImageNet datasets are reported, with a comprehensive comparison of the state of the art and several ablation studies. The proposed method achieves better performance than the state of the art.\n\nStrengths:\n+ The paper is almost well written, with comprehensive related works and it is easy to read except for few details (see below).\n+ The method is simple and is extensively studied. I appreciated the extensive ablation study and the discussion that follows it.\n\nWeaknesses:\n- The approach has some novelty in the method of generating new images and in the framework itself. Beside that, the idea of generating images was introduced previously (e.g. Wang et al. 2018) and the embedding network is a standard one-shot embedding with prototypical loss (Snell et al. 2017).\n- The clarity on few details can confuse the reader and needs improvement on the use of the gallery.\n- The comparison with the state of the art may be unfair due to the addition of the gallery images, which are proved in the ablation studies to be essential to the better performance of the proposed method. \n\nIn particular:\n- It is not clear to me how the gallery images are exactly used. Since the ablation study (sect 6.2) reported in (2) that the performance is worse using images from the support set and (3) the improved performance comes from the diversified images outside the support set, it may be that just the addition of the gallery images is the reason to have better performance. Also it is not clear to me if they are fixed all the time in advance during the meta training, if they change. It would be interesting to see an experiment where the gallery is available in the support set of the compared state of the art works and the baselines, and see if the performance is improved as well.\n\n- Regarding the clarity, beside the use of the gallery, several small issues should be improved:\n  * The paper use the term deformed images (until sect ~4) and then synthetic images for the generated images; \n  * Figure 2 is introduced early but explained late in the paper and it is not clear how the second and the third images should be interpreted; \n  * In the related work section, Wang et al. 2018 is said to generate imaginary images in contrast to realistic images of the proposed method not synthetic, however the images are clearly not real and are synthetically generated from the heuristic and the weight of the network. Moreover, the paper uses the term synthetic images later, so the comment is misleading.   \n\nMinor issue:\n- In the abstract, loose -> lose\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}