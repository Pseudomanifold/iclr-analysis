{"title": "Strong results on some benchmarks but novelty and justification for method lacking", "review": "This paper addresses the problem of few-shot learning, specifically by incorporating a mechanism for generating additional training examples to supplement the given small number of true training examples in an episode. The example generation method takes as input a support set image and a gallery image (which can be any random image) and produces a new synthesized support example that is a linear weighted combination of the two input images (in terms of pixel patches) and is assumed to belong to the same class as the support set example input. The synthesis process is parameterized as an \u201cimage deformation network\u201d whose weights must be learned. Additionally, there is an embedding network that takes as input the full support set (with true and synthesized examples) and is used in a prototypical-net fashion (Snell 2017) to classify the given query set. A set number of gallery images are created from the training base classes and this is the fixed set of gallery images that is used for both training and testing. The image deformation network and embedding network weights are trained end-to-end using the standard meta-learning loss on the query set given the support set. Experiments are conducted on the ImageNet-1K Challenge and Mini-Imagenet benchmarks.\n\nPros:\n- Sizable improvement on ImageNet-1K benchmark relative to previous work.\n- Detailed ablation study is done to display the benefits of choices made in model.\n\nCons:\n- Idea is not that novel relative to all the recent work on learning to supplement training data for few-shot learning (Hariharan 2017, Wang 2018, Gao 2018, and Schwartz 2018). Additionally, it is not made clear why we expect the idea proposed in this paper to be better relative to previous ideas. For example, Delta-Encoder (Schwartz 2018) also supplements training data by working in image-space rather than in feature-space (as done in the other previous work mentioned). One big difference seems to be that for the Delta-Encoder work, the image generation process is not trained jointly with the classifier but this does not seem to have a big impact when we compare the performance between Delta-Encoder and proposed model on Mini-Imagenet benchmark.\n- Benefit in Mini-Imagenet benchmark is not very large. Compared to Delta-Encoder (Schwartz 2018), the proposed model does worse for 1-shot case and intersects the confidence interval for 5-shot case.\n\nRemarks:\n- Could add citation for NIPS 2018 paper 'Low-shot Learning via Covariance-Preserving\nAdversarial Augmentation Networks' (https://arxiv.org/pdf/1810.11730.pdf)\n- Delta-Encoder (Schwartz 2018) paper should be mentioned in related work and contrasted against.\n- How many supplemented examples are used per class? I believe it is n_aug=8 but this could be stated more clearly.\nAdditionally, how many supplemented examples are used in the previous work that are being compared to? It would be useful to have this information and to make sure the comparison is consistent in that the number of supplemented examples are comparable across different models that synthesize examples.\n- In algorithm 1, it seems that only CELoss is used to update parameters of embedding network but on page 6, it says \"we found that using additional cross-entropy loss speeds up convergence and improves recognition performance than using prototypical loss solely\"?\n- At many points in the paper, 'prob image' is used instead of 'probe image'.\n- Page 10: 'Our IDME-Net has different react...' => 'Our IDME-Net has different output...'\n- Page 10: '...why our model work' => '...why our model works'\n\nGao et al. Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks. 2018.\nHariharan et al. Low-shot Visual Recognition by Shrinking and Hallucinating Features. 2017.\nSchwartz et al. Delta-Encoder: an effective sample synthesis method for few-shot object recognition. 2018.\nSnell et al. Prototypical Networks for Few-Shot Learning. 2017.\nWang et al. Low-Shot Learning from Imaginary Data. 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}