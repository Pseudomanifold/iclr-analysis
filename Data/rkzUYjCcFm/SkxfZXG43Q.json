{"title": "Poor readibility, lack of insight and fundamental background knowledge, reject", "review": "summary--\nThe paper focuses on improving object localization, though the title highlights \"interpreting deep neural network\" which is another area. It analyzes the classifier weights for image classification, and compute the derivative of the feature maps from the network for a sensitivity map of the image. Then it learns a simple linear mapping over the sensitivity map for bounding box regression. Experiments report competitive performance.\n\nHowever, there are several major concerns.\n\n1) The paper appears misleading from multiple claims. For example, [abstract] \"common approaches to this problem involve the use of a sliding window,... time consuming\". However, current state-of-the-art methods accomplish detection in a fully convolutional manner using CNN, and real-time performance is achieved. the paper claims that \"computer vision can be characterized as presenting three main tasks... (1) image classification, (2) image localization and (3) image detection\". This appears quite misleading. There are way more topics, from low-level vision to mid-level to high-level, e.g., stereo, boundary detection, optical flow, tracking, grouping, etc. Moreover, just in \"localization\", this could be object localization, or camera localization. Such misleading claims do not help readers learn from the paper w.r.t related work in the community.\n\n\n2) The approach \"is rooted in the assertion that any deep CNN for image classification must contain, implicit in its connection weights, knowledge about the location of recognized object\". This assertion does not appear obvious -- an reference should be cited if it is from other work. Otherwise, recent work shows that deep CNN can overfit random training data, in which case it is hard to imagine why the object location can be implicitly captured by the CNN [R1]. Similarly, the paper claims that \"once weights are found, the gradient... with regard to X would provide information about the sensitivity of the bounding box loss function with regard to the pixels in the images\". This is not obvoius either as recent work show that, rather than the whole object, a part of it may be more discriminative and captured by the network. So at this point, why the gradient can be used for object location without worrying that the model merely captures a part? \n\n[R1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals, Understanding deep learning requires rethinking generalization, ICLR 2017.\n[R2] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Learning deep features for discriminative localization, CVPR 2016.\n\n3) The paper admits in Section 2.2 that \"we have not yet done a formal comparison of these two approaches to constructing the sensitivity map\". As the two approaches are suggested by the authors, why not comparing in this paper. It makes the paper less self-contained and not ready to publish. A formal comparison in the rebuttal may improve the rating of the paper.\n\n4) In Equation 3, how to represent the bounding box coordinate? Are they any transforms? What does it mean by \"bias weights\"? Are they different from Cartesian coordinates, or the one used in Equation (2)?\n\n\n5) The experiments are not convincing by merely reporting the metric of IoU>0.5 without any in-depth analysis. Perhaps some visualization and ablation study improve the quality of experiments.\n\n6) In Section 3.2, why using two different aggregation methods for producing the final sensitivity map -- max-pool along the channel for PACAL VOC 2017 dataset and avg-pool for ImageNet dataset, respectively? Are there some considerations?\n\n7) In Table 1, it shows the proposed method outperforms the other methods significantly, achieving 41% better than the second best method. However, there is no in-depth analysis explaining why the proposed method performs so well for this task. Moreover, from Figure 1 and Figure 3, it is straightforward to ask how a saliency detection model performs in object detection given that the images have clean background and objects are mostly centered in the image.\n\n8) What does it mean by \"CorLoc (mAP)\" in Table 2? As defined in Equation 4, CorLoc acounts the portion of detection whose IoU greater than 0.5 compared to the ground-truth. But mAP accumulates over a range of IoU threshold and precision across classes.\n\n9) As the proposed method is closely related to the CAM method, how does CAM perform on these datasets? This misses an important comparison in the paper.\n\n\n10) The readability of the paper should be improve. There are many typos, for example --\n1. What does \"...\" mean above and below Equation (2)?\n2. inconsistent notation, like $w_{ki}$ and ${\\bf w}_{ki}$ in Equation (2).\n3. conflicted notation, w used in Equation 2 and Equation 3.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}