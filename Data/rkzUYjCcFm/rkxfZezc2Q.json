{"title": "Reasonable but more evaluation needed", "review": "Summary\nThe paper presents a method to perform object localization by computing sensitivity of the network activations with respect to each pixel. The key idea is that the representation for classification implicitly contains object localization information since the object classification is done by detecting features of an object in an image. The localization information is extracted as a form of sensitivity map which indicates each pixel\u2019s contribution to the final classification decision, and is subsequently used for regressing the bounding box. The proposed method outperforms other baseline methods and achieves reasonable performance when compared with slower state-of-the-art deep learning techniques for object localization.\n\nStrengths\n-\tFor object localization, this technique can provide faster results.\n-\tThe paper proposes a simple sensitivity measure which works well in identifying the pixels which are important for object classification, and provides relevant information for localization.\n-\tThe paper suggests a simple linear mapping from sensitivity maps to object bounding box coordinates, which can be learnt from a fairly small ground truth localization data.\n\nWeaknesses\n-\tThe idea of utilizing back-propagated sensitivity map is not novel for weakly-supervised object localization [1,2], as the proposed method just uses a simpler sensitivity function with linear regression.\n-\tThe paper mentions the approach being \u2018very fast\u2019, but they do not show any quantitative comparisons in fair and similar settings with other techniques. The reviewer strongly suggests to provide testing time measures.\n-\tIn ImageNet experiment, test data split in the paper is not held out as they use a pre-trained VGG network which is trained on the full ImageNet dataset.\n-\tThe title claims to interpret deep neural networks, but the proposed approach just uses sensitivity map for object localization task, without any analysis for interpretation. Such interpretations have already been extensively studied in [1,2].\n-\tThe idea of utilizing classification network for localizing an object is new, but the ideas of weakly-supervised object localization is already explored in [1,2,3,4,5,6]. The reviewer recommends to cite and provide valid comparison with [3,4,5,6].\n-\tMore detailed experimental results, i.e. accuracy across different categories are required. The reviewer also recommends ablation studies to compare with bounding box heuristics and sensitivity measures as used in [1, 2].\n-\tNo details about reproduction of results are provided in the paper.\n\nPossible Questions\n-\tWhen computing a sensitivity map from a CNN output vector or an attention map, is the sensitivity calculated with respect to each activation in the output vector? How is a single sensitivity map computed from the attention map, which contains a number of activations?\n\nMinor Comments\n-\tIn the description of g\u2019(\\eta_i), g\u2019(\\eta i) should be g\u2019(\\eta_i).\n-\t\u201c\u2026\u201d around equations should be replaced by \u201c: equation,\u201d.\n\nReferences\n[1] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016. \n[2] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. See https://arxiv.org/abs/1610.02391 v3, 7(8), 2016. \n[3]  Cho et al., Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals, CVPR 2015\n[4] Yi et al., Soft Proposal Networks for Weakly Supervised Object Localization, ICCV 2017\n[5] Oquab et al., Is object localization for free? \u2013 Weakly-supervised learning with convolutional neural networks, CVPR 2015\n[6] Li et al., Weakly Supervised Object Localization with Progressive Domain Adaptation, CVPR 2016\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}