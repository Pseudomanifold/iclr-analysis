{"title": "Method that seem to work in practice, but needs better comparison and has issues with presentation", "review": "The paper presents a method for training a probabilistic model for Multitask Transfer Learning. The key idea is to introduce a latent variable \"z\" per task which to capture the commonality in the task instances. Since this leads to an intractable likelihood the authors use the standard ELBO with a Variational Distribution over \"z\" defined as a Gaussian + Inverse Autoregressive Flow. For classification, the authors also show that they can combine the model with the main idea in Prototypical Networks. \n\nThe experiments evaluate on three different task, the comparison against MAML on the toy problem is quite interesting. However, the results on the Mini-Imagenet suggest that the main contributors to the better performance are the Prototypical Networks idea and the improved ResNet. Additionally, the authors compare against MAML only on the toy task and not on their synthetic dataset. I think that the experiments need better comparisons (there have been published an improved version of MAML, or even just add results from your own implementation of MAML with the same ResNet on the 3rd task as well). \n\nA major issue is that the model presented is not really a Hierarchical Bayesian model as being strongly presented. It is much more a practical variational algorithm, which is not bad by no means, but I find its \"interpretation\" as a Hierarchical Bayesian method as totally unnecessary and making the paper significantly harder to read and follow than it needs to be. This is true for both the base model and the model + ProtoNet. I think that the manuscript itself requires more work as well as a better comparison of the method to baseline algorithms.\n\n\nSection 2.2:\n\nThe authors start by introducing a \"Hierarchical Bayes\" model over the parameters of a Neural Network for multi-task learning. By defining the model parameters to be an implicit function of some low-dimensional noise and the hyper-parameter they shift the inference to the noise variable \"z\". One issue, which I won't discuss further, is that this defines a degenerate distribution over the parameters (a fact well known in the GAN literature), which seem counter-intuitive to call \"Bayesian\". Later, since the parameters \"w\" has vanished from the equation the authors conclude that now they can change the whole graphical models such that there is actually no distribution over the parameters of a Neural Network, while the hyper-parameter IS now the parameters of a Neural Network and the latent variable is an input to it. Mathematically, the transformation is valid, however, this no longer corresponds to the original graphical model that was described earlier. The procedure described here is essentially a Variational Model with latent variable \"z\" for each task and the method performs a MAP estimation of the parameters of the Generative Model by doing Variational Inference (VAE to be exact) on the latent \"z\". There is nothing bad about this model, however, the whole point of using a \"Hierarchical Bayes\" for the parameters of the Network serves no purpose and is significantly different to the actual model that is proposed. \n\nIn section 2, the prior term p(a) in equation 7 and Algorithm 1 is missing.\n\nSection 3:\n\nThe authors argue that they add yet another level of hierarchy in the Graphical Model with a further latent variable \"v\", which is unclear fundamentally why you need it as it can be subsumed inside \"z\" (from a probabilistic modelling perspective they play similar roles). Additionally, they either do not include a prior or on \"v\" or there is a mistake in the equation for p(S|z) at the bottom of page 4. The main motivation for this comes from the literature where for instance if we have a linear regression and \"v\" represents the weights of the last linear layer with a Gaussian Prior than the posterior over \"v\" has an analytical form. After this whole introduction into the special latent variable \"v\", the authors actually use the idea from Prototypical Networks. They introduce a valid leave-one producer for training. However, the connection to the latent variable \"v\" which was argued to be the third level of a Hierarchical Bayes model is now lost, as the context c_k is no longer a separate latent variable (it has no prior and in the original Prototypical Network although the idea can be interpreted in a probabilistic framework it is never presented as a Hierarchical Bayes).  \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}