{"title": "Quite a lot of experiments, but the choice of r(y|x) is not well justified, and some theoretical issues", "review": "Thank you for an interesting read.\n\nThe paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial \"reconstruction\" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. \n\nExperiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed \"reconstruction\" loss. Improvements has been shown on reconstructing input images with significant numbers.\n\nOverall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.\n\nThere is one theoretical issue for the defined \"reconstruction\" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). \n\nThis is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].\n\nI think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). \n\n1. Indeed matching p_{\\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \\approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.\n\n2. The authors mentioned that picking r(y|x) = \\delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section \"...we used a combination of reflecting 10% pad and the random crop to the same image size...\". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. \n\nIn summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).\n\n[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.\n[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}