{"title": "Simple technique, good results, not enough substance.", "review": "Summary: Derive sentence representations from a bidirectional LSTM encoder trained to distinguish real sentences from fake ones. Fake sentences are derived from real ones by swapping two words or dropping a single word (yielding two different models). The resulting representations are applied to various sentence classification tasks by using them as input to a logistic regression classifier trained for the task. Results are generally better than similar experiments performed with SkipThought vectors trained on the same Toronto BookCorpus.\n\nThis is a reasonable idea, and the win over SkipThought is quite convincing, but the paper is short on substance, and parts are confusing or superfluous. Some problems and questions:\n\n1) Most of section 2 could be omitted, since it doesn\u2019t really add insight to the well-established idea of pre-training parameters on an auxiliary task. \n\n2) Section 3 calls the Conneau et al (2017) transfer approach supervised. It also distinguishes between semi-supervised approaches that \u201cdo task-specific adaptation using labeled data\u201d and unsupervised approaches (including the current one) that also must do exactly that.\n\n3) In 4.2, does the 3-layer MLP have non-linearities in its hidden layers? If so, it\u2019s not equivalent to a single linear layer as claimed, regardless of whether a non-linearity is applied to its output. If not, there is no point in using 3 layers.\n\n4) Section 5 gives only minimal descriptions of the tasks - often just acronym and type, presumably because they are borrowed from Conneau et al (2017, 2018). More information needs to be provided.\n\n5) Section 6 should show the best results from the Conneau et al papers for calibration.\n\n6) Were the baseline systems also supplied with Glove word embeddings? Do they have the same number of parameters?\n\n7) Details of the logistic regression classifier?\n\n8) Why train on your method on only 1M sentences, since training is fast? Wouldn\u2019t using more text give better results?\n\n9) Given the recent very strong results from the ELMo paper (which you cite), the current paper doesn\u2019t seem complete without some attempt to replicate this as a baseline - eg, use a deeper encoder, combine state vectors through layers, etc. These features aren\u2019t incompatible with your objective, which might make for an interesting extension.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}