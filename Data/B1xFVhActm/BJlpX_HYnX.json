{"title": "Nice simple idea but insufficient execution and discussion ", "review": "Summary:\n=======\nThe paper proposes a discriminative training formulation for learning sentence representations, where a classifier is required to distinguish between real and fake sentences. The sentences are encoded with a Bi-LSTM and the resulting sentence representations are then used in a number of sentence-level tasks (classification, entailment, and retrieval). The experiments show benefits on most tasks compared to Skip-Thought and FastSent baselines, and the information captured by the representations is analyzed with probing tasks, showing that they are better at capturing certain kinds of information like the presence or order of words. \n\nThe paper proposes a simple and fairly effective approach for learning sentence encoders. The basic idea is appealing and the experimental results are fairly good. However, at present it seems like more work is required for delivering a comprehensive evaluation and analysis. My main concerns with the paper are the insufficient comparison with prior work, its lack of clarity and organization in certain places, and the limited amount of work. Please see below detailed comments on these and other points, as well as suggestions for how to improve some of these issues.  \n\n\nMajor comments:\n==============\n1. Better baselines and comparisons: \n- The results are compared only with SKip-Thought and (the weaker) FastSent. However, there are far better models by now. First, already in the Skip-Thought paper there is a version combining Naive Bayes bi-gram features which performs much better on some benchmarks, for example that version would be better than the paper's results on MR (80.4). \n- Moreover, there have been many newer papers with better results on many of the tasks [1, 2, 4, and references therein]. At the very least, mention should be made that there are better published results, and ideally there should be some comparison to the more relevant papers [1, and maybe others].  \n\n2. Paper organization and clarity:\n- I found Section 2 to be unnecessarily lengthy and disorganized. It mixes motivation with modeling, introduces excessive notation, sometimes without clearly defining it (what is L_{aux}? Why is U in eq. 2 not defined on first usage?), and digresses to weakly related discussions (the link to GANs seems vague and the relation to Introspective Neural Networks is not made clear). The last paragraph is largely redundant with the introduction. \n- There is also a statement that seems just wrong: \"maximizing the data likelihood P(enc(x,\\theta_1)|\\theta_1,\\thera_3)\" -- the data likelihood is P(X | ...). Maximizing the encoding of x can be trivially achieved by simply having a constant encoding whose probability is 1. \n- The entire Section 2 can be condensed to one or two paragraph, essentially deriving the discriminative training task in equations (1) and (2). \n- On the paper organization level, this lengthy section is followed by the related work and then section 4 on \"training tasks for encoders\". There is again redundancy between section 4 and 2. Consider merging sections 2 and 4 into one Methodology section, where the general task is formulated, the sentence encoding (Bi-LSTM with max-pooling) and binary classifier (the MLP) are defined, and the fake sentence generation is described. This would make a better flow and remove excessive text. \n\n3. Motivation and advantages of the approach:\n- The approach is motivated by shortcomings of sentence encodings based on language modeling, such as Skip-Thought, which are computationally intensive due to the large output space and the complicated decoding process. This is an appealing motivation, although there have also been simpler methods for sentence representations that work as well as or better than Skip-Thought [1, 2]. \n- The second motivation is not clear to me, and the claim that \"the training text collection should include many instances of sentences that have only minor lexical differences but found in completely different contexts\" needs more support, either theoretical or empirical. Why wouldn't a language model be able to distinguish such differences?\n- The advantages of the binary classification task make sense. The point about forcing the encoder to track both syntax and semantics is interesting. Have you tried to analyze whether this indeed happens? The probing tasks are a good way to evaluate this, but most of them are syntactic, except SOMO and perhaps CoordInv and BShift. Still, more analysis of this point would be good. \n- One concern with generating fake sentences by swapping words is that it would not apply to languages with free word order. Have you considered how well your approach would work on other languages? \n\n4. Relevant related work: \n- The fake data generation resembles noise used in denoising auto-encoders. A recent application is in unsupervised neural machine translation [3], but there is relevant prior work (see references in [3]). \n- The binary classification task resembles that in [1], where they train a classifier to distinguish between the representation of a correct neighbor sentences and incorrect sentences. \n\n5. Ideas for more experiments and analysis:\n- The results are fairly good by using only 1M sentences. How good would they be with the full corpus? What's the effect of training data size on the method? \n- Table 4 is providing nice examples showing how the fake sentence task generates better sentences representations. Can this be measured on a larger set of examples in aggregate? Why is t-SNE needed for calculating the neighbor rank? \n- Proving tasks are very interesting, but the discussion is limited. A more detailed discussion and analysis would be useful. \n- Consider other techniques for generating fake sentences. \n\n\nMinor comments:\n==============\n- Related work: the Skip-Thought decoder is a unidirectional LSTM and not a bidirectional one as mentioned, right? \n- Related work: more details on supervised approaches would be useful. \n- Section 4.1: how many fake exampels are generated from every real example? Have you experimented with this? \n- Section 4.2 mentions 2 hidden layers in the MLP but figure 3 indicates 3 layers. \n- Is there a reason to use multiple layers without a non-linearity in the MLP? This seems unusual. In terms of expressivity, this is equivalent to using one larger linear layer, although there might be some benefit in optimization. \n- Table 1 seems unnecessary as there is no discussion of how dataset statistics refer to the results. It's enough to refer to previous work. \n- What are some results missing in table 2, specifically SKipthought (1M) on COCO datasets? \n- The paragraph on sentence encoder implementation mentions a \"validation set accuracy of 89 for word shuffle\". Which validation set is that? How is convergence determined for word drop? \n- In analyzing sentence lengths, figure 2 shows the fake sentence to be similar to SKip-Thought on short sentences in SST. Do you have any idea why? Also, fake sentence is better than Skip-Thought on all lengths in MR, not just longer sentences, so I'm not sure there's any signal there. \n- Figure 3: what is the test set for WordShuffle? \n- The idea to create negative samples focused towards specific phenomena sounds like a good way to go\n\n\nWriting, grammar, etc.:\n======================\n- Introduction, paragraph 3, last sentence: start with \"The\". \n- Introduction, paragraph 4, first sentences: discriminative training task fake sentence detection -> discriminative training task *of* fake sentence detection\n- Motivation: an useful -> a useful; we assumes -> we assume; then number -> the number; this much -> this is much\n- Motivation: do not differ -> do not differ much? \n- Related work: skip-gram -> skip-gram model; Training Skipthought model -> Training a Skipthought model\n- Section 4: Prior work use -> Prior work uses/used \n- Section 4.2: space between \"Multi-layer Perceptron\" and \"(MLP)\". This also happens with other acronyms. \n- Page 6: Our models, however, train -> are trained \n- Table 3 caption: is bigram in -> is bigram; is co-ordination is -> is-coordination \n- Page 7: The analysis ... also indicate*s* ... but do*es* not ... \n- Figure 3 caption: classification/proving task -> tasks \n- References: fix capitalization in paper titles\n\n\nReferences\n==========\n[1] Logeswaran and Lee, An efficient framework for learning sentence representations\n[2] Khodak et al., A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors\n[3] Artetxe et al., Unsupervised Neural Machine Translation\n[4] Arora et al., A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}