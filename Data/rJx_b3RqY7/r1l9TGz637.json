{"title": "Ok paper, some nice comparisons, but too similar to existing models", "review": "This paper presents a variant of the adversarial generative modeling\nframework, allowing it to incorporate an inference mechanism. As such it is\nvery much in the same spirit as existing methods such as ALI/BiGAN. The\nauthors go through an information theoretic motivation but end up with the\nstandard GAN objective function plus a latent space (z) reconstruction\nterm. The z-space reconstruction is accomplished by first sampling z from\nits standard normal prior and pushing that sample through the generator to\nget sample in the data space (x), then x is propagated through an encoder\nto get a new latent-space sample z'. Reconstruction is done to reduce the\nerror between z' and z.\n\nNovelty: The space of adversarially trained latent variable models has\ngrown quite crowded in recent years. In light of the existing literature,\nthis paper's contribution can be seen as incremental, with relatively low novelty. \n\nIn the end, the training paradigm is basically the same as InfoGAN, with\nthe difference being that, in the proposed model,  all the latent\nvariables are inferred (in InfoGAN, only a subset of the latent\nvariables are inferred) . This difference was a design decision on the part of the InfoGAN\nauthors and, in my opinion, does not represent a significantly novel\ncontribution on the part of this paper.  \n\nExperiments: The experiments show that the proposed method is\nbetter able to reconstruct examples than does ALI -- a result is not\nnecessarily surprising, but is interesting and worth further\ninvestigation. I would like to understand better why it is that latent\nvariable (z) reconstruction gives rise to better x-space reconstruction.\n\nI did not find the claims of better sample quality of AIM over ALI to be\nwell supported by the data. In this context, it is not entirely clear what\nthe significant difference in inception scores represents, though on this, the\nresults are consistent with those previously published\n\nI really liked the experiment shown in Figure 4 (esp. 4b), it makes the\ndifferences between AIM and ALI very clear. It shows that relative to ALI,\nAIM sacrifices coherence between the \"marginal\" posterior (the distribution\nof latent variables encoded from data samples) and the latent space\nprior, in favor of superior reconstructions. AIM's choice of trade-off is\none that, in many contexts, one would happy to take as it ensures that\ninformation about x is not lost -- as discussed elsewhere in the paper.\nI view this aspect of the paper by far the most interesting. \n\nSummary,\nOverall, the proposed AIM model is interesting and shows promise, but I'm\nnot sure how much impact it will have in light of the existing literature\nin this area. Perhaps more ambitious applications would really show off the\npower of the model and make it standout from the existing crowd. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}