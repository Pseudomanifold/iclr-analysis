{"title": "interesting idea with potential", "review": "\nThe paper addresses the problem of providing saliency-based visual explanations of deep models tasked at image classification. More specifically, instead of generating visualizations directly highlighting the image pixels that support the the decision of an image belonging to class A, it generates \"contrastive\" visualizations indicating the pixels that should be added or suppressed in order to support the decision of a image belonging to class A and not to class B.\n\nThe method formulates the generation of these contrastive explanations through a generative adversarial network (GAN), where the discriminator D is the image classification model to be explained and the generator G is a generative model trained to produce images from the dataset used to train D.\n\nExperiments on the MNIST and fashion-MNIST datasets compares the performance of the proposed method w.r.t. some methods from the literature.\n\n\nOverall the manuscript is well written and its content is relatively easy to follow. The idea of generating contrastive explanations through a GAN-based formulation is well motivated and seems novel to me.\n\nMy main concern with the manuscript are the following:\n\ni) The proposed method seems to be specifically designed for the generation of contrastive explanations, i.e. why the model predicted class A and not class B. While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?\n\nii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel. See Kim et al., NIPS'16, Dhurandhar et al., arXiv:1802.07623. Moreover, regarding the presented results on the MNIST dataset (Sec 4.1) where some of the generated explanations highlight gaps to point differences between digit classes. The work from Samek et al., TNNLS'17 and  Oramas et al., arXiv:1712.06302 seem to display similar properties in their explanations without the need of explicit constractive pair-wise training/testing. The manuscript would benefit from positioning the proposed method w.r.t. these works.\n\niii) Very related to the first point, in the evaluation section (Sec.4.1) the proposed method is compared against other methods in the literature. Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.\n\niv) Finally, the reported results are mostly qualitative. I find the set of provided qualitative examples quite reduced. In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.\nIn addition, I recommend complementing the presented qualitative comparisons with quantitative evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302).\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}