{"title": "important topic but weak results", "review": "This paper studies the characteristics of representations and their roles in neural network expressiveness. The results  are overall not very impressive. \n\n1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank. The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information. Only some heuristic results are obtained for them without rigorous theory. It would be better if these heuristic arguments can be formed as theorems as well.\n\n2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers. That is, the dependence between z and y does not vary with regularizers but the one between z and x does. Is this a coincidence or a general phenomenon? Is there a theoretical explanation? \n\n\n3. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}