{"title": "Empirical Study of Robustness of (small) Ensembles of Neural Nets", "review": "Summary. The paper considers the robustness of neural nets against adversarial attacks. More precisely, the authors experimentally investigate the robustness of ensembles of neural nets. They empirically show that adversarially trained ensembles of 2 neural nets are more robust than ensembles of 2 adversarially trained neural nets.\n\nPros.\n* Robustness of neural nets is a challenging problem of interest for ICLR\n* The paper is easy to read\n* Experimental results compare different algorithms for 2 neural nets\n\nCons.\n* The study is experimental\n* It is limited to gradient-based attacks\n* It is limited to ensembles of size 2\n* The Ensemble2Adv is a single NN model and not an ensemble model. \n\nEvaluation.\nThe problem is significant and the use of ensemble methods for robustness against adversarial attacks is a promising line of research. The experimental study in this paper opens new lines of research in this direction. But, in my opinion, the paper is not ready for publication at ICLR. Detailed comments follow but the study is limited to k=2; the main finding is limited to the comparison between bagging two adversarially trained neural nets (SeparateEnsemble2Adv) and learning adversarially the average of two neural nets (Ensemble2adv). In my opinion, Ensemble2adv is a single model of double size and not an ensemble model thus somehow contradicting the main claim of the paper.\n\nDetailed comments.\n* Introduction, end of \u00a72, it is said that non-gradient based attacks are still effective. But in the sequel you only consider gradient-based attacks and never discussed this question.\n* Introduction, contributions, it should be made clear at the beginning of the paper that you will consider ensembles of size 2 and only gradient-based attacks.\n* Section 2. The momentum-based attack should be cited and could be considered. \"Boosting adversarial attacks with momentum, Dong et al, CVPR18\"\n* Section 3, \u00a72, the discussion on ensemble methods is unprecise. Ensemble methods have different objectives. For instance, Bagging-like methods  aim at reducing the generalization error while others as Boosting aim at augmenting the capacity of individual models.\n* Section 3. Here is my main concern on this paper. The classical method would be bagging of neural nets with different initializations. The neural nets could be adversarially trained. This would lead to the so-called SeparateEnsemble2Adv. Here, the authors consider another method. Their method can be viewed as k(=2) copies of the same neural network with different initializations and an additional layer computing the average of the k(=2) outputs. Then adversarially learn the obtained model which leads to the so-called Ensemble2Adv algorithm. This algorithm is not an ensemble method as such. In my opinion for k=2, it is equivalent to doubling the size of a neural net, adding averaging of the outputs, and adversarially training the obtained neural net.\n* Note recent advances in ensemble NNs with papers such as Averaging weights leads to wider optima ..., Izmailov et al, UAI18; Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, Garipov et al, arXiv:1802.10026\n* Section 4.1. Here comes the limitation k=2. The case k=4 is considered in table 1 but is not discussed elsewhere in the paper.\n* Section 4.1. I am not convinced by DoubleAdv. It is one way of doubling the size of a neural net but I am not convinced that this is the more efficient. As said before, in my opinion, Ensemble2Adv is another way for doubling the size. And many more should exist.\n* Section 5. In my opinion, the main comparisons should concern SeparateEnsemble2Adv and Ensemble2Adv. Also other methods doubling the size should be considered. \n* Section 5. For k greater than 2, SeparateEnsemblekAdv should be the better method because the adversarial learning phase could be easily parallelized.\n* I am not convinced by the discussion in Section 6.\n* Typos. and -> an l-13, p5; IFGSM5, l-19 p6; then l-6 p7; to due l-6 p9\n* Biblio. Please give complete references\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}