{"title": "The spatial value decomposition is just successor representation?", "review": "The paper is quite clearly written and the authors has explained the ideas nicely. There are also some convincing results that that the proposed algorithm, spatial monte-carlo method improved agent's performance in several continuous control tasks.\n\nHowever, a critical problem I found is that the so-called 'spatial value decomposition' seems just the same as the the well known successor representation. (i.e. to learn the expected discounted future state occupancy G(s, a, s'), and the reward function R(s)). Learning successor representation instead of Q-function has been investigated quite extensively by Kulkarni et al. in Deep Sucessor Reinforcement Learning (https://arxiv.org/pdf/1606.02396.pdf). So, I'm afraid I do not see enough novelty in this manuscript.\n\nI would be happy to discuss further if the authors could point out how the proposed method is different from the deep successor RL one. Even if they are different enough, I think the manuscript would be much strengthened if the authors could mention the relationship (difference?) between successor representation and the spatial value decomposition.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}