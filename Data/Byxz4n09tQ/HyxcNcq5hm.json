{"title": "More explanation and clarification on experiments required", "review": "This paper focused on training a small network with a pre-trained large network in a student-teacher strategy, which also known as knowledge distillation. The authors proposed to use a separately trained GAN network to generate synthetic data for the student-teacher training. \n\nThe proposed method is rather straightforward. The experimental results look good, GAN generated data help train a better performed student in knowledge distillation. However, I have concerns about both motivations and experiments. \n\n1. The benefits of GAN for generating synthetic data to assist supervised training are still mysterious, especially when GAN is separately trained on the same dataset without more information introduced. I would love the authors to clarify why GAN generated data are particularly effective for knowledge distillation. Does GAN generated data also help standard supervised training? I would expect following experiments: use mixture of training and GAN data to train teacher and student network by standard supervised loss without knowledge distillation, and compare with values in table 1. \n\n2. The performance of the proposed method depends on the quality of GAN. To help me further understand the quality of GAN, I hope to see the following experiments to compare with scores in table 1.\ni) The accuracy of supervised trained teach and student on GAN generated image. \nii) The classification accuracy on test data by the classifier trained in AC-GAN. \n\n3. I would like the authors to clarify their experiments to convince me the comparison in table 1 is fair.\ni) How many data and iterations in total are used for standard training and knowledge distillation with/without GAN data? Does the better performance come from synthetic data, or come from exploiting more data and training for longer time?\nii) Related to i), In figure 1 (a), how many data and iteration for each epoch? It would help if the standard supervised training curve for student can be provided. \niii) The experiments have a lot of hyperparameters, for example, the weight \\alpha, the temperature T,  optimizer, the learning rate, learning rate decay, the probability p_fake. These hyperparameters are different for each experimental setting. How are they chosen?\n\n4. Please explain conceptually why the proposed compression score is better than inception score. \n\n5. The paper is missing a conclusion section. The following papers introduce adversarial training for knowledge distillation. Though it is not necessary to compare with them in experiments as they are complicated method and the usage of GAN is different from this paper, I think it is still worth to mention them in related work. \nWang et al. 2018 Adversarial Learning of Portable Student Networks\nXu et al. 2018 Training Student Networks for Acceleration with Conditional Adversarial Networks \n\n================ after rebuttal ====================\nI appreciate the authors' response and slightly raise the score. It is a good rebuttal and it has clarified several things. I like the authors' explanation on why GAN is particularly good in a student-teacher setting. The explanation reminds me of the mixup data augmentation paper from last year. I also like the additional experiments which clearly show the benefits of GAN data augmentation. \n\nHowever, I still think it is borderline for several reasons.\n1. As the other reviewer has pointed out, CIFAR-10 is a bit too toy and some models (like LeNet for Figure 2) cannot really show the advantage of the method. I would suggest try ImageNet, and use more recent networks for ablation study.  \n2. As the other reviewer has pointed out, the compression ratio can be impractical. The compression ratio  depends on student-teacher training, which can take a relatively long time. \n3. I would suggest the following experiments that may strengthen the paper. I would consider these as a plus, not necessarily related to my current evaluation. \ni) Try not use GAN, but use mixup (linear interpolation of samples) as data augmentation, and go through the student-teacher training.\nii) Try evaluate the effect of generator structure for data augmentation. Does the generator have to be very strong? The GAN generated results did not improve supervised learning may suggest the generator is not necessarily to be strong.  \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}