{"title": "An principled approach with weak empirical results", "review": "This work proposes a new (biased) gradient estimator to learn a discrete auto-encoders. Similarly to the gumbel-softmax estimator this paper proposes to use the gumbel-max trick and the reparametrization trick but instead of relaxing the argmax by a softmax, the authors derive a formula for the gradient based on direct loss optimization to compute the gradient through the argmax.\n\nPros:\n- The approach is well motivated and the proof of theorem 1 which gives the formula of the new gradient estimator seems correct.\n\nCons:\n- The principle downside of the proposed approach is that it requires to compute the value of the objective for several values of z, which makes it more computationally expensive than gumbel-softmax. Could the author compare the different estimators in terms of running time instead of epoch for example in fig2. it seems like RELAX would perform similarly or better in terms of wall-clock time.\n\n- [1] also proposed an estimator that requires evaluating the objective for different values of z, and showed that it is unbiased and optimal (lowest variance). I think the authors should mention this related work and how their approach differs. I also think the author should compare their work to [1].\n\n- Since both gumbel-softmax and the proposed approach are biased, could the authors give some intuitions on why they believe their approach is better.\n\n- I believe the expectation of the right-hand side of equation (9) can be computed in closed form by using a formula similar to eq (4) and (5), which replace the expectation by a sum over the possible values of z. This will lead to a gradient estimator with no variance, can the author comment on this ?\n\n- I think the bias induced by the mean-field approximation of the decoder should be investigated more thoroughly. Could the authors plot the gap as a function of n for example ? What happens if we also increase the number of category ? (there is a typo in this section it should be k^n instead of n^k) ? Can they compare to gumbel-softmax, is there a threshold at which gumbel-softmax becomes better ?\n\n- It's not clear on what setting is the variance plotted in fig 1. is computed ? Is it computed on the discrete VAE experiment ? if so how many latent variables and category ? Could the bias also be provided ? Could it be compared to gumbel-softmax with varying temperature ?\n\n- The experiments are a bit toyish, it's not clear what happens when the task are more complex, the architecture for the encoder and decoder are deeper or the latent space is bigger. In particular the authors only consider linear encoder and decoder when comparing the ELBO of different methods.\n\n- In the semi-supervised settings what happens if we don't set the perturbed level to the true label ?\n\nConclusion:\nThe experiments are quite toyish and the approach is more computationally expensive than gumbel-softmax. More experiments should be done to clearly show the advantage of this method compared to gumbel-softmax.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}