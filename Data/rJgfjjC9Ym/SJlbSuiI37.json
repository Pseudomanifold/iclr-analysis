{"title": "Borderline paper", "review": "This paper proposes to use 8/4-bit approximation of activations to save the memory cost during gradient computation.  The proposed technique is simple and straightforward. On the other hand, the proposed method only saves up to a constant cost of the storage. With the constant factor (4x, 8x) depending on whether fp16 or fp32 is used during computation. Notably, there is a small but noticeable accuracy drop in the final trained model using this mechanism.\n\nThe alternative method, gradient checkpointing, can bring sublinear memory improvement, with at most 25%  compute overhead, with no loss of accuracy drop.\n\nAs a result, the proposed method has a limited use case. The author did mention, during the response that the method could be combined further with the sublinear checkpointing. However, since sublinear checkpointing already brings in significant savings, it is unclear whether low bit compression is necessary.\n\nGiven the limited technical novelty(can be described as oneliner \"store forward pass in 4/8 bit fixed point\"),  limited applicable scenarios, and limited improvement it can buy(4x memory saving with accuracy drop), I think this is a boarder-line paper\n\nOn the positive side, the empirical result could still be interesting to some readers in the ICLR community, the paper could be further improved by comparing more numerical representations, such as fp16 and other floating point formats such as unum.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}