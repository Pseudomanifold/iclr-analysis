{"title": "A good first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities", "review": "The high-level problem this paper tackles is that of endowing RL agents with recursive reasoning capabilities in a multi-agent setting, based on the hypothesis that recursive reasoning is beneficial for the agents to converge to non-trivial equilibria.\n\nThe authors propose the probabilistic recursive reasoning (PR2) framework for an n-agent stochastic game. The conceptual difference between PR2 and non-correlated factorizations of the joint policy is that, from the perspective agent i, PR2 augments the joint policy of all agents by conditioning the policies of agent i's opponents on the action that agent i took. The authors derive the policy gradient for PR2 and show that it is possible to learn these action-conditional opponent policies via variational inference in addition to learning the policy and critic for agent i.\n\nThe proposed method is evaluated on two experiments: one is an iterated matrix game and the other is a differential game (\"Max of Two Quadratics\"). The authors show in the iterated matrix game that baselines with non-correlated factorization rotate around the equilibrium point, whereas PR2 converges to it. They also show in the differential game that PR2 discovers the global optimum whereas baselines with non-correlated factorizations do not.\n\nThis paper is clear, well-motivated, and well-written. I enjoyed reading it. I appreciated the connection to probabilistic reinforcement learning as a means for formulating the problem of optimizing the variational distribution for the action-conditional opponent policy and for making such an optimization practical. I also appreciated the illustrative choice of experiments that show the benefit of recursive reasoning. \n\nCurrently, PR2 provides a proof-of-concept of recursive reasoning in a multi-agent system where the true equilibrium is already known in closed form; it remains to be seen to what extent PR2 is applicable to multi-agent scenarios where the equilibrium the system is optimizing is less clear (e.g. GANs for image generation). Overall, although the experiments are still small scale, I believe this paper should be accepted as a first step towards endowing deep reinforcement learning agents with recursive reasoning capabilities.\n\nBelow are several comments.\n\n1. Discussion of limitations: As the authors noted in the Introduction and Related Work, multi-agent reinforcement problems that attempt to model opponents' beliefs often become both expensive and impractical as the number of opponents (N) and the recursion depth (k) grows because such complexity requires high precision in the approximate the optimal policy. The paper can be made stronger with experiments that illustrate to what extent PR2 practically scales to problems with N > 2 or K > 1 in terms of how practical it is to train.\n2. Experiment request: To what extent do the approximation errors affect PR2's performance? It would be elucidating for the authors to include an experiment that illustrates where PR2 breaks down (for example, perhaps in higher-dimensional problems).\n3. Minor clarification suggestion: In Figure 1: it would be clearer to replace \"Angle\" with \"Perspective.\"\n4. Minor clarification suggestion: It would be clearer to connect line 18 of Algorithm 1 to equation 29 on Appendix C.\n5. Minor clarification suggestion: In section 4.5: \"Despite the added complexity\" --> \"In addition to the added complexity.\"\n6. Minor clarification: How are the importance weights in equation 7 reflected in Algorithm 1?\n7. Minor clarification: In equation 8, what is the significance of integrating over time rather than summing?\n8. Minor clarification: There seems to be a contradiction in section 5.2 on page 9. \"the learning outcomes of PR2-AC and MASQL are extremely sensitive to the way of annealing...However, our method does not need to tune the the annealing parameter at all...\" Does \"our method\" refer to PR2-AC here?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}