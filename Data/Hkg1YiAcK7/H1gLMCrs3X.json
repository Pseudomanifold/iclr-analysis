{"title": "Avoiding Mode Collapse?", "review": "This paper presents an learning by teaching (LBT) framework to train implicit generative models. Instead of using discriminator as in GANs, LBT adopts an explicit likelihood estimator as the student, which is formulated as a bilevel optimization problem: \n1) maximize the log-likelihood of the generated samples; \n2) maximize the log-likelihood evaluated on the training data. \nThe authors argue that LBT avoids the mode collapse problem intrinsically as the missing modes will be penalized by the second objective. I have some concerns on this.  Why teaching an explicit likelihood can help learn an implicit one? \n\nSuppose the explicit likelihood estimator is a single Gaussian, but the real distribution has multiple modes, fitting such the generated data and the training data on this likelihood will not help to avoid missing modes. \n\nFrom the empirical results, it is clear that LBT-GAN is better than LBT. From the objective in (8), it seems the true reason is  the P_E and D together representing a mixture model, which may fit the training data better. \n\nIn Figure 2.(b), the Intra-mode KL divergence of LBT-GAN seems to be unstable during the training, is this caused by the joint training of discriminator with the estimator. Can you discuss this?\n\nIn Table 1, the authors just copied the results of VEEGAN. Indeed, in our implementation, DCGAN and VEEGAN can be much better than the reported one. The authors have not tried the effort to tune the results of baselines. \n\nRecently, the Least square GAN has been purposed to address the mode collapse as well. I suggested the authors should empirically compare with it as well.\n\nGenerally, the paper is well-written. The idea is interesting, however, the motivation, analysis and empirical results are not convincing enough to fully support their claim. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}