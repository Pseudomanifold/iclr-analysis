{"title": "interesting idea, empirical results are not convincing", "review": "The paper introduces a modification of batch normalization technique. Original\nbatch normalization normalizes minibatch examples using their mean and standard deviation. \nThe proposed version of batch normalization, called diminishing batch normalization, normalized \nexamples in the current minibatch using  mean and standard deviations that are weighted average \nof mean and standard deviation from the current and all previous minibatches. The authors prove convergence of \nbatch gradient descent with diminished batch normalization. Also, the authors show empirically\nthat Adagrad optimization with diminishing batch normalization can find a better local minimum than\nAdagrad optimization with original batch normalization.\n\nThe idea of diminishing normalization is very sound. However I was not convinced that it gives empirical advantage.\nThe paper says that Table 1 shows \"the best result obtained from each choice of \\alpha^m \". Probably the numbers in \nthis table were obtained using some particular choice of the number of epochs. Unfortunately I didn't find in the \npaper any details about the choice of the number of epochs. If we choose the number of epochs that minimize validation\nloss, then according to Figures 4(a) and 3(a), if \\alpha^m=1 then the validation loss is minimized around epoch 55\nand corresponding test error should be less than 2.2%. But the corresponding top left entry in Table 1 has error 2.7%. \n\nAdditional technical remarks:\n1. The abstract says \"we also show the sufficient and necessary conditions for the step sizes and diminishing weights to ensure the convergence\". I didn't find necessary conditions in the paper.\n\n2. The authors claim that they are not aware of any prior analysis of batch normalization. The papers at https://arxiv.org/abs/1805.11604 and \nhttps://arxiv.org/abs/1806.02375 , published initially in 5-6/2018, provide interesting theoretical insights on batch normalization.\n\n3. Sentence after equation (2): change from D_1 to D.\n\n4. Usually batch normalization is applied before non-linear activation. According to equations 3-6, the paper applies \nbatch normalization after the nonlinear activation. My understanding is that convergence proof relies on the former architecture. Does section 5 use the former or the latter architecture? \n\n5. I am not sure that fully connected neural network is an efficient architecture for MNIST dataset. I would like to \nto see experiments with CNN and MNIST. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}