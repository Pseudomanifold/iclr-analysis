{"title": "A momentum based approach for batch normalization with asymptotic convergence analysis", "review": "The authors propose a momentum based approach for batch normalization and provide an asymptotic convergence analysis of the objective in terms of the first order criterion. To my understanding, the main effort in the analysis is to show that the sequences of interest are Cauchy. Some numerical results are reported to demonstrate that the proposed variant of BN slightly outperforms BN with careful adjustment of some hyper parameter. The proposed approach is incremental, and the theoretical results are somewhat weak.\n\nThe most important issue is that the zero gradient of the objective function does not imply that it attains an (even local) minimum point. As for the 2-layer case, the objective function can be nonconvex in terms of the weight parameters with stationary points being saddle points, it is crucial to understand whether an iterative algorithm (GD or SGD) converges to a minimum point rather a saddle point. Thus, the first order criterion alone is not enough for this purpose, which is why extensive studies are carried out for nonconvex optimization (e.g., using both first and second order criteria for convergence [1]) and considering the specific structure of neural nets [2].\n\nThe analysis is somewhat confusing. The authors assume that the objective of interest have stationary points (\\theta*, \\lambda*), and also show that the sequence of the norm of gradient convergence to zero, with the \\lambda^(m) converges to \\bar{\\lambda}. What is the relationship between \\lambda* and \\bar{\\lambda}? It is not clear whether they are the same point or not. Moreover, since there is no converge of the parameter, it is not clear what the convergence for the \\lambda imply here, as we also discussed above that the zero gradient itself may mean nothing.\n\nIn addition, the writing need improvements. Some statements are not accurate. For example, on page 3, after equation (2), the authors state \u201cThe deep network \u2026\u201d, though they mentioned it is for a 2-layer net. Also, more explicit explanation and definitions are necessary for notations. For example, it is clearer to define explicitly the parameters with \\bar (e.g., for \\lambda) as the limit point. \n\n[1] Ge et al. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition.\n[2] Li and Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}