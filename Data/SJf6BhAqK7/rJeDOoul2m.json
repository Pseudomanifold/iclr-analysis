{"title": "A work lacking clarity", "review": "This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.\nThe method is tested on several scenarios and datasets, showing promising results in prediction accuracy.\n\nThe idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. \n\nRelated to this latter point, the use of the term \u201cBayesian nonparametric\u201d is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn\u2019t present any element related to parameters inference, uncertainty estimation, \u2026 Even the fact that the method uses an algorithm illustrated in [1] doesn\u2019t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. \n\nFinally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.  \n\n\n\n\n\n[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012\n\n[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. 2016.\n[3] Ji, Pan, et al. \"Deep subspace clustering networks.\" Advances in Neural Information Processing Systems. 2017.\n[4] Jiang, Zhuxi, et al. \"Variational deep embedding: An unsupervised and generative approach to clustering.\" IJCAI 2017\n[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. \"Semantic autoencoder for zero-shot learning. CVPR 2017.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}