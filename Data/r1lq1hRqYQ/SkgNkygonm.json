{"title": "Limited technical merit and significance", "review": "This paper applies IRL to the cases of multiple tasks/environments and multimodal input features involving natural language (text) and vision (images). It is interesting to see the better performance of their proposed approaches with language-conditioned rewards over language-conditioned policies. The paper is written well.\n\nI view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.\n\nConsidering the use of deep learning that can handle highly complex images and text, the practical significance of this work can be considerably improved by grounding their work in real-world context and/or larger-scale environments/tasks, as opposed to simulated environments in this paper. See, for example,\n\nM. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, Large-scale cost function learning for path planning using deep inverse reinforcement learning, The International Journal of Robotics Research, 2017. \n\nThe authors say that \"The work of MacGlashan et al. (2015) requires an extensively hand-designed, symbolic reward function class, whereas we use generic, differentiable function approximators that can handle arbitrary observations, including raw images.\" What then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?\n\n\n\nMinor issues\n\nPage 2: a comparison to in Section 6 to as an oracle?\nPage 3: What is rho_0?\nPage 7: In order compare against?\nPage 7: and and indicator on?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}