{"title": "Writing needs improvement; many handwavy explanations", "review": "I have read the author's response, and I would like to stick to my rating. From the authors' response on the convergence issue, the result from [1] does not directly apply since the activation function that the authors use in this paper is relu (not linear). Having said that, authors didn't find any issues empirically.\n\nQ7: Yes, I agree that the result depends on the gradient structure of the relu activations. But my point was that, it is still a calculation that one has to carry out, and the insight we gain from the calculation seem computational: that one can regularize jacobian norm easily. True, but is that necessary? Or in other words, can we use techniques (not-so) recent  implicit regularization literature to analyze KFAC? I still think that the work is good, these are just my questions.\n====\n\nThe paper investigates how weight decay (according to the authors, this is done by scaling weights at each iteration) can be used as a regularizer while using standard first order methods and KFAC. As far as I can see, the experimental conclusion seem pretty consistent with other papers that the authors themselves cite (for eg: Neelakantan et al. (2015);  Martens & Grosse, 2015. \n\nIn page 2, the authors mention the three different mechanisms by which weight decay has a regularizing effect. First, what is the definition of \"effective learning rate\"? If the authors mean that regularization just changes the learning rate in some case, that is true. In fact, it is only true while using l2-norm. I looked through the paper, and I couldn't find one. Similarly, I find point #1. to be confusing: why does reducing the scale of the weights increase the effective learning rate? (This confusion carries over to/remains in section 4.1.). The sentence starting (in point #1.) with \"As evidence,\", what is the evidence for? Is it for the previous statement that weight decay helps as a regularizer? Looking at Figure 1., Table 1., I can see that weight decay is actually helpful even with BN+D. In fact, the improvement provided by weight decay is uniform across the board. \n\nThe conclusion of mechanism 1 is that for layers with BN, weight decay is implicitly using higher learning rate and not by limiting the capacity as pointed out by van Laarhoven (2017). The two paragraphs below (12) are contradictory or I'm missing something: first paragraph says that \"This is contrary to our intuition that weight decay results in a simple function.\" but immediately below, \"We show empirically that weight decay only improves generalization by controlling the norm, and therefore the effective learning rate.\" Can the authors please explain what the \"effective learning rate\" argument is?\n\nProposition 1 and theorem 1 are extensions from Martens & Gross, 2015, I didn't fully check the calculations. I glanced through them, and they mostly use algebraic manipulations. The main empirical takeaway as the authors mention is that: weight decay in both KFAC-F and KFAC-G serves as a complexity regularizer which sounds trivial (assuming Martens & Grosse, 2015) since in both of these cases, BN is not used and the fact that weight decay is regularization using the local norm. \n\nIf I understand correctly, KFAC is an approximate second order method with the approximation chosen to be such that it is invariant under affine transformations. Are there any convergence guarantees at all for either of these approaches? Newton's method, even for strongly convex loss functions, requires self-concordance to ensure convergence, so I'm a bit skeptic when using approximate (stochastic) Jacobian norm. \n\nSome of the plots have loss values, some have accuracy etc., which is also confusing while reading. I strongly suggest that Figure 1 be shown differently, especially the x-axis! Essentially weight decay improves the accuracy about 2-4% but it is hard to interpret that from the figure.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}