{"title": "Review Relational Forward Models for Multi-Agent Learning ", "review": "This paper proposes to use graph neural networks in the scenario of multi-agent reinforcement learning (MARL). It tackles two current challenges, learning coordinated behaviours and measuring such coordination.\n\nAt the core of the approach are graph neural networks (a cite to Scarselli 2009 would be reasonable): acting and non-acting entities are represented by a graph (with (binary) edges between acting-acting and acting-nonacting entities) and the graph network produces a graph where these edges are transformed into a vectorial representation, which then can be used by a downstream task, e.g. a policy algorithm (as in this paper) that uses it to coordinate behavour. Because the output of the graph network is a structurally identical graph to the input, it is possible to interpret this output.\n\nThe paper is well written, the main ideas are clearly described. I'm uncertain about the novelty of the approach, at least the way the RFM is utilized in the policy is a nice idea (albeit, a-posteriori, sounds straight forward in the context of MARL). Similarly, using the graph output for interpretations is an obvious choice). Nevertheless, showing empirically that the ideas actually work gives the paper a lot of credibility for being a stepping stone in the area of MARL.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}