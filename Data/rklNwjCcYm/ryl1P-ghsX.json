{"title": "well written paper with marginal contribution", "review": "This paper proposes to use multi-head attention module with bi-LSTM for sequence labeling name entity recognition. The author carefully introduce the model structure, followed by extensive experiments to demonstrate the advantage of the model. I think the novelty of this paper is not obvious, or at least the authors didn\u2019t clearly demonstrate the contribution of this paper. Multi-head attention mechanism, bi-LSTM are both well established methods. There are also existing works which have already applied related techniques to the NER problem. I\u2019m willing to change my score if the authors can clarify the contribution of this paper.\n\nTo add more comments, I think the building blocks in this draft, including bi-LSTM, self-attention, character and word level CNN, neural network for NER, are all well established mechanisms in the community. There are already many use cases of one or combination of some of these modules. Neither the modules themselves nor the way of combining them are novel to me, though I appreciate the effort to build the system and test its performance against other methods.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}