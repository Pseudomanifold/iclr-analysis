{"title": "An OT-based regularization of the loss of seq2seq models", "review": "This paper propose to add an OT-based regularization term to seq-2-seq models in order to better take into account the distance between the generated and the reference and/or source sentences, allowing one to capture the semantic meaning of the sequences. Indeed, it allows the computation of a distance between embeddings of a set of words, and this distance is then used to define a penalized objective function.\nThe main issue with this computation is that it provides a distance between a set of words but not a sequence of words. The ordering is then not taken into account. Authors should discuss this point in the paper.\nExperiments show an improvement of the method w.r.t. not penalized loss.\n\nMinor comments:\n- in Figure 1, the OT matching as described in the text is not the solution of eq (2) but rather the solution of eq. (3) or the entropic regularization (the set of \"edges\" is higher than the admissible highest number of edges).\n- Introduction \"OT [...] providing a natural measure of distance for sequences comparisons\": it is not clear why this statement is true. OT allows comparing distributions, with no notion of ordering (see above).  \n- Table 1: what is NMT?\n- first paragraph, p7: how do you define a \"substantial\" improvement of the scores?\n- how do you set parameter $\\gamma$ in the experiments? Why did you choose \\beta=0.5 for the ipot algorithm?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}