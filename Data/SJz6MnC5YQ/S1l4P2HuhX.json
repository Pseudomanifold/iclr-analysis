{"title": "Interesting work with some odd issues on implementation and results", "review": "The paper presents an approach for translating graphs in one domain to graphs in the same domain using a GAN approach. A graph Translator approach is defined and a number of synthetic data sets and one real-world data set are used to evaluate the approach. Most of the paper is written well, though there are some odd sentence structure issues in places. The paper could do with a thorough check for grammatical and spelling mistakes. For example you miss-spell NVIDIA.\n\nThe main concerns with the work:\n1) Equation 2 is used to minimise the distance between graphs from X and graphs in Y. Yet, the main metric which is used to evaluate the paper is this distance. This would seem to give an unfair advantage to your approach. I would also be concerned about the fact that later you use this for stating if a graph represents good or hacker activity. If you have drawn translated graphs towards real graphs, how do you know that you haven\u2019t pulled a good graph closer to a hacker graph? This is more concerning considering work which came out of NIPS which suggested that GAN\u2019s tend to favour producing similar output rather than spreading it evenly over the domain.\n\n2) It isn\u2019t entirely clear what your results are trying to show. Presumably P, R, AUC and F1 are generated from the results produced from your Discriminator? Were each of the other approaches optimised against your discriminator or not? Also, it is unclear as to what the Gold Standard method is - we\u2019re only told that its a classifier, but what type and how constructed?\n\n3) Your approach seems to be \u2018fixed\u2019 in the set of nodes which are in both in the input and output graphs - needing to be the same. This would seem significantly limiting as graphs are rarely of the same node set.\n\n4) Although you comment on other graphs approaches being limited to very small graphs, you do not test your approach on graphs with over 150 nodes. These would also seem to be very small graphs in comparison to real-world graphs. Further evaluation on larger graphs would seem to be essential - how long would it take on graphs with 10^6 nodes?\n\n5) The real-world dataset seems rather odd and not fully explored. Given that you have this data it is surprising that you didn\u2019t complete the loop by showing that you could take data from before a hack attempt and show that you could predict that in the future you had a hack attempt. Perhaps this is due to the fact that you didn\u2019t have the ground-truth data in here to show a graph going from good to bad? But if not it would have been good to have shown, either through this data or some other, how your approach does match in with real-world results.\n\nGiven the points above, I would be very concerned on an approach which used the above to identify a future hacking attempt.\n\nSome more specific comments on the paper:\n- \"The tremendous success of deep generative models on generating continuous data like image and audio\u201d - it is not clear what this continuous data is.\n\n- Hard to parse : \u201cwhich barely can be available for the accounts worth being monitored.\u201d\n\n- \u201cThis requires us to learn the generic distribution of theft behaviors from historical attacks and synthesize the possible malicious authentication graphs for the other accounts conditioning on their current computer networks\u201d - given that these historical attacks are (hopefully) rare, is there enough data here to construct a model?\n\n- Please define GCNN\n\n- \u201cOur GT-GAN is highly extensible where underlying building blocks, GCNN and distance measure in discriminator, can be replaced by other techniques such as (Kipf & Welling, 2017; Arjovsky et al., 2017) or their extensions.\u201d - this sounds more like a feature of what you have contributed rather than a contribution in its own right.\n\n- In the context of synthetic data, what is ground-truth?\n\n- Hard to parse \u201cModern deep learning techniques operating on graphs is a new trending topic in recent years.\u201d\n\n- Hard to parse \u201cHowever, these methods are highly tailored to only address the graph generation in a specific type of applications such as molecules generation\u201d \n\n- Hard to parse \u201cExisting works are basically all proposed in the most recent year,\u201d\n\n- \u201cTypically we focus on learning the translation from one topological patterns to the other one\u201d -> \u201cTypically we focus on learning the translation from one topological pattern to the other\u201d\n\n- It\u2019s not clear in equation 1 how you represent G_X. Only much later is it mentioned about adjacency matrix.\n\n- Hard to parse \u201cDifferent and more difficult than graph generation designed only for learning the distribution of graph representations, for graph translation one needs to learn not only the latent graph presentation but also the generic translation mapping from input graph to the target graph simultaneously.\u201c\n\n- Hard to parse \u201cgraph translation requires to learn\u201d\n\n- Hard to parse \u201cin most of them the input signal is given over node with a static set of edge and their weights fixed for all samples\u201d\n\n- \u201cwe propose an graph\u201d -> \u201cwe propose a graph\u201d\n\n- Hard to parse \u201cThe two components of the formula refers to direction filters as talked above\u201d\n\n- Hard to parse \u201cNext, graph translator requires to\u201d\n\n- \u201cas shown in Equations equation 7 and Equations equation 6,\u201d -> \u201cas shown in Equation 6 and Equation 7\u201d\n\n- Hard to parse \u201cThe challenge is that we need not only to learn the\u201d\n\n- Figure 2 would seem to need more explanation. \n\n- The end of section 3.3 is a bit vague and lacks enough detail to reproduce.\n\n- \u201cour GT-GAN is able to provide a scalable (i.e., O(N2)) algorithm that can generate general graphs.\u201d - what sizes have you tested this up to?\n\n- Hard to parse \u201cwe randomly add another kjEj edges on it to form the target graph\u201d\n\n- \u201cThe goal is to forecast and synthesize the future potential malicious authentication graphs of the users without any historical malicious behaviors, by the graph translator from normal to malicious graph trained based on the users with historical malicious-behavior records.\u201d - This isn\u2019t entirely clear. Are you trying to create new malicious graphs or show that a current graph will eventually go malicious?\n\n- \u201cAll the comparison methods are directly trained by the malicious graphs without the conditions of input graphs as they can only do graph generation instead of translation.\u201d - not clear. For the synthetic data sets how did you choose which ones were malicious?\n\n- \u201cGraphRNN is tested with graph size within 150. GraphGMG, GraphVAE is tested within size 10 and RandomVAE is tested on graphs within size 150.\u201d -> \u201cGraphRNN and RandomVAE are tested with graph up to size 150. GraphGMG, GraphVAE is tested with graphs up to  size 10.\u201d\n\n- \u201cHere, beyond label imbalance, we are interested in \u201clabel missing\u201d which is more challenging.\u201d - \u201cmissing labels\u201d?\n\n- \u201cIn addition, we have also trained a \u201cgold standard\u201d classifier based on input graphs and real target\ngraphs.\u201d - need to say more about this.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}