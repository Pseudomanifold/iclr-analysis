{"title": "Cool idea, memory usage could be analysed deeper", "review": "The authors address the problem raised by applying a fully attentional network (FAN) to model music. \nThey argue clearly for the need of relational positional embedding in that problem (instead of absolute positional as in vanilla FAN), and highlight the quadratic memory footprint of the current solution (Shaw et al. 2018).\n\nThe main contribution of the paper is a solution to this, consisting in a smart idea (sect 3.4.1 and 3.4.2) which allows them to compute relative embeddings without quadratic overhead.\nThe model performs indeed better than Shaw et al.'s on the single data-set they compared both. On the other one, the argument is that Shaw et al. 2018 cannot be applied because the sequences are too long.\n\nI have two concerns with the paper:\n\t1/ it is very hard to read at times. In particular, the main contribution took me several passes the understand. I list below a few recommendations for improvement \n\t2/ the main argument is that the model requires less memory and is faster. However, the only empirical evidence in that direction is given in the introduction (Sect 1.1., second paragraph).\n\t\tThe following points remain unclear to me:\n\t\t\ta) why can't the Relative Transformer be applied to Piano-e composition. What is the maximal length that is possible?\n\t\t\tb) how much faster / less memory is the relative music transformers? The only data-point is in Sect 1.1., which seems indeed impressive (but then one wonders why this is not exploited further). A deeper analysis of the comparative memory footprint would greatly strengthen the paper in my opinion.\n\t\t\t\nWhy \"music\" relative transformers? Nothing in the model restrict it to that use case. The use of FAN over audio has been explored with limited success, one of the reasons being that - similarly to this use-case here - audio sequences tend to be longer than text.\n\t\t\nminor comments:\n\t- abstract, ln9: there seems to be a verb missing\n\t- p1,ln-2: \"dramatic\" improvements seems to be exaggerated\n\t- p2,ln11: \"too long\". too long for what?\n\t- p4,ln15: (Table 1). is one sentence by itself. Also, a clear explanation of that table is missing\n\t- p5,item 2: an explanation in formula would be helpful for those not familiar with reshaping\n\t- Fig3: it seems very anecdotical. Similar green bloxes might be placed on the left plot\n\t- sect4.1.1,ln3. that sentence does not parse\n\t- Table 2: what is cpsi?\n\t- $l$ is nicer formatted as $\\ell$\n\t- care should be taken to render the Figures more readable (notably the quality of Fig 4, and labels of Fig 7)\n\t- footnotes in Figures are not displayed (Table 2 and 4)\n\t- the description of the human evaluation leaves some open questions. I could not come up with 180 ratings (shouldn't it be 180 * 3 ratings?). Also, at least the values of Relative Transformer vs other 3 models should be shown (or all 6 comparisons). Here you call \"relative transformer\" your model, previously you used that term to refer to (Shaw et al. 2018).\n\t\twhen reporting statistical significance, there are some omissions which should be clarified.\n\t- (Shaw et al. 2018) has been published at NAACL. For such an important citation, you should update the reference from the arxiv version.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}