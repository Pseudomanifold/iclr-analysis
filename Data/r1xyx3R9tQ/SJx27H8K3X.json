{"title": "Prototypical Examples on Small Datasets", "review": "## Strength\n\nThis paper explores ways of identifying prototypes with extensive qualitative and quantitative empirical attempts. \n\n## Weakness\n\n### Not practical\n\nThe authors report that \u201cremoving individual training examples did not have a measurable impact on model performance\u201d. However, this seems not to be supported by experiments.\nFirst, it is not clear what exactly models do they use in Section 4, e.g. ResnetV2 with how many layers? Learning rate schedules? \nSecond, why is the baseline models on CIFAR-10 perform so bad (<90%) even with 100% data?\nThird, with `\"adv\" metric, we need to perform adversarial-example attacks before training, which has little value in practice. \n\n### Datasets\n\nThey only conduct quantitative experiments (section 4) on relatively small datasets (i.e. MNIST, Fashion-MNIST and CIFAR-10). It is not clear how it will generalize to more realistic settings. \n\n## Most confusing typos\n\n1. Section 4, paragraph 5, \"However, we find that training only on the most prototypical examples gives extremely high accuracy on the other prototypical examples.\" Is there a missing \"than\"? It's confused.\n2. The description of Figure 6 is not clear enough. Especially there is no explanation to (d, e, f). \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}