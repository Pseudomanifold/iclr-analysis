{"title": "Many hacks and heuristics. ", "review": "In this paper, the authors propose a heuristic method to overcome the exploration in RL. They store trajectories which result in novel states. \nThe final state of the trajectory is called goal state, and the authors train a path function which given a state and a subgoal states (some states in the trajectory) the most probably action the agent needs to take to reach the subgoal. These way they navigate to the goal state. The goal state is claimed to be achieved if the feature representation stoping state is close to goal (or subgoal for subgoal navigation).\n\n\nThe authors mainly combine a few previous approaches \"Self-Imitation Learning,\" \"Automatic Goal Generation for Reinforcement Learning Agents,\" and \"Curiosity-driven exploration by self-supervised prediction\" to design this algorithm which makes this approach less novel.\n\nGeneral comment; there are variable and functions in the paper that are not defined, at least at the time, they have been used. The Rooms environment is not described. What is visit_times[x] and x is not a wall? What is stage avg reward? and many others\n\nThe main idea of the algorithm is clear, but the description of the pieces is missing.\n\nIt is not clear in stochastic setting how well this approach will perform. \n\nThe authors state that\n\"Among different choices of the modeling, we choose inverse dynamics (Pathak et al., 2017) as the environment model, which has been proved to be an effective way of representing states under noisy environments.\"\nI took a look at this paper and could not find neither proof or quantification of \"effective\"-ness. Please clarify what the meaning this statement is.\n\nWhy s=s' is ambiguous to the inverse dynamics?\n\nWhat is the definition of acc in fig2?\n\nwhy (consin+1)^3/8 is chosen?\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}