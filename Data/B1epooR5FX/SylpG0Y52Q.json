{"title": "Potentially interesting idea, not well explained and justified", "review": "This paper proposes using predicted variables(PVars) - variables that learn\ntheir values through reinforcement learning (using observed values and\nrewards provided explicitly by the programmer). PVars are meant to replace\nvariables that are computed using heuristics.\n\nPros:\n* Interesting/intriguing idea\n* Applicability discussed through 3 different examples\n\nCons:\n* Gaps in explanation\n* Exaggerated claims\n* Problems inherent to the proposed technique are not properly addressed, brushed off as if unimportant\n\nThe idea of PVars is potentially interesting and worth exploring; that\nbeing said, the paper in its current form is not ready for\npublication.\n\nSome criticism/suggestions for improvement:\n\nWhile the idea may be appealing and worth studying, the paper does not address several problems inherent to the technique, such as:\n\n- overheads (computational cost for inference, not only in\n  prediction/inference time but also all resources necessary to run\n  the RL algorithm; what is the memory footprint of running the RL?)\n\n- reproducibility\n\n- programming overhead: I personally do not buy that this technique -\n  at least as presented in this paper - is as easy as \"if statements\"\n  (as stated in the paper) or will help ML become mainstream in\n  programming. I think the programmer needs to understand the\n  underpinnings of the PVars to be able to meaningfully provide\n  observations and rewards, in addition to the domain specific\n  knowledge. In fact, as the paper describes, there is a strong\n  interplay between the problem setting/domain and how the rewards should be\n  designed.\n\n- applicability: when and where such a technique makes sense\n\nThe interface for PVars is not entirely clear, in particular the\nmeaning of \"observations\" and \"rewards\" do not come natural to\nprogrammers unless they are exposed to a RL setting. Section 2 could\nprovide more details such that it would read as a tutorial on\nPVars. If regular programmers read that section, not sure they\nunderstand right away how to use PVars. The intent behind PVars\nbecomes clearer throughout the examples that follow.\n\nIt was not always clear when PVars use the \"initialization function\"\nas a backup solution. In fact, not sure \"initialization\" is the right\nterm, it behaves almost like an \"alternative\" prediction/safety net.\n\nThe examples would benefit from showing the initialization of the PVars.\n\nThe paper would improve if the claims would be toned down, the\nlimitations properly addressed and discussed and the implications of\nthe technique honestly described. I also think discussing the\napplicability of the technique beyond the 3 examples presented needs\nto be conveyed, specially given the \"performance\" of the technique\n(several episodes are needed to achieve good performance).\n\nWhile not equivalent, I think papers from approximate computing (and\nperhaps even probabilistic programming) could be cited in the related\nwork. In fact, for an example of how \"non-mainstream\" ideas can be\nproposed for programming languages (and explained in a scientific\npublication), see the work of Adrian Sampson on approximate computing\nhttps://www.cs.cornell.edu/~asampson/research.html\nIn particular, the EnerJ paper (PLDI 2011) and Probabilistic Assertions (PLDI 2014).\n\nUpdate: I maintain my scores after the rebuttal discussion.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}