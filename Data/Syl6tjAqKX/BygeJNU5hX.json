{"title": "Interesting Paper, underwhelming experiments", "review": "This paper introduces a \"behavior module\" (BM) which is a small network that encodes preferences over actions and runs parallel to the fully connected layers of a policy. The paper shows this architecture working in Atari games, where the same policy can be used to achieve different action preferences over a game while still playing well. It also includes a thorough recap of past modular approaches. \n\nThe motivation for the BM is that we may want deep networks to be able to decompose \"strategy\" and \"behavior\", where behavior may influence decisions without affecting the performance. In this framework, the BM is trained on a reward of correctness + personalized \u201csatisfaction\u201d.\n\nThe experiments model behavior as preferences over how many actions to play simultaneously. The trained BMs can be transferred to new tasks without finetuning. The ideas here also have some similarity to the few shot learning literature.\n\nComments on the experiments:\n1. The Table 2  do not show a smooth interpolation between reward scaling and AMSR vs BD. This is surprising because the performance on the game should be highest went it is weighted to the most. This indicates to me that the results are actually high variance, the 0.8 vs 0.88 in stage 2 of 0.25r vs 0.5r means that is probably at least +/- 0.08 standard deviation. Adding standard deviations to these numbers is important for scientific interpretability.\n2. I expect some BMs should perform much better than others (as they have been defined by number of actions to play at once). I would like to see (maybe in the appendix) a table similar to table 2 for for individual BMs. I currently assume the numbers are averaged over all BMs.\n3. Similarly, I would like to see the BD for BM0 (e.g., if a policy is not optimized for any behavior, how close does it get to the other behaviors on average). This is an important lower bound that we can compare the other BD to. \n4. An obvious baseline missing is to directly weight the Q values of the action outputs  (instead of having an additional network)  by the designed behavior rewards. There is an optimal way to do this because of experimental choices. \n\nQuestions:\n1.For BM2, you write \" Up and Down (or Right and Left)\" did you mean \"Up and Right\"? How can Up and Down be played at the same time?\n\nOverall, this paper uses neuroscience to motivate a behavior module. However, the particular application and problem settings falls short of these abstract \"behaviors\". Currently, the results are just showing that RL optimizes whatever reward function is provided, and that architectural decomposition allows for transfer, which was already showed in (Devin 2017). An experiment which would better highlight the behavior part of the BM architecture is the following:\n1. Collect datasets of demonstrations (e.g. on atari) from different humans.\n2. Train a policy to accomplish the task (with RL)\n3. Train BMs on each human to accomplish the task in the style of each human.\nThis would show that the BMs can capture actual behavior. \n\nThe dialog examples discussed in the abstract would also be very exciting.\n\nIn conclusion, I find the idea interesting, but the experiments do not show that this architecture can do anything new. The abstract and introduction discuss applications that would be much more convincing. I hope to see experiments with a more complex definition of \"behavior\" that cannot be handcoded into the Q function.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}