{"title": "Elegant synthesis approach to a new interesting domain of representing 3D shapes", "review": "This paper presents a methodology to infer shape programs that can describe 3D objects. The key intuition of the shape programs is to integrate bottom-up low-level feature recognition with symbolic high-level program structure, which allows the shape programs to capture both high-level structure and the low-level geometry of the shapes. The paper proposes a domain-specific language for 3D shapes that consists of \u201cFor\u201d loops for capturing high-level regularity, and associates objects with both their geometric and semantic attributes. It then proposes an end-to-end differentiable architecture to learn such 3D programs from shapes using an interesting self-supervised mechanism. The neural program generator proposes a program in the DSL that is executed by a neural program execution module to render the corresponding output shape, which is then compared with the original shape and the difference loss is back-propagated to improve the program distribution. The technique is evaluated on both synthetic and ShapeNet tasks, and leads to significant improvements compared to Tulsiani et al. that embed a prior structure on learning shape representations as a composition of primitive abstractions. In addition, the technique is also paired with MarrNet to allow for a better 3D reconstruction from 2D images.\n\nOverall, this paper presents an elegant idea to describe 3D shapes as a DSL program that captures both geometric and spatial abstractions, and at the same time captures regularities using loops. CSGNet [Sharma et al. 2018] also uses programs to describe 2D and 3D shapes, but the DSL used here is richer as it captures more high-level regularities using loops and also semantic relationships such as top, support etc. The idea of training a neural program executor and using it for self-supervised training is quite elegant. I also liked the idea of guided adaption to make the program generator generalize beyond the synthetic template programs. Finally, the results show impressive improvements and generalization capability of the model.\n\nCan the authors comment on some notion of completeness of the proposed DSL? In other words, is this the only set of operators, shapes, and semantics needed to represent all of ShapeNet objects? Also, it might be interesting to comment more on how this particular DSL was derived. Some of the semantics operator such as \u201cSupport\u201d, \u201cLocker\u201d, etc. look overly specific to chair and tables. Is there a way to possibly learn such abstractions automatically?\n\nWhat is the total search space of programs in this DSL? How would a naive random search perform in this synthesis task?\n\nI also particularly liked the decomposition of programs into draw and compound statements, and the corresponding program generator decomposition into 2 steps BlockLSTM and StepLSTM. At inference time, does the model use some form of beam search to sample block programs or are the results corresponding to top-1 prediction?\n\nWould it be possible to compare the results to the technique presented in CSGNet [Sharma et al. 2018]?  There are some key differences in terms of using lower-level DSL primitives and using REINFORCE for training the program generator, but it would be good to measure how well having higher-level primitives improve the results.\n\nI presume the neural program executor module was trained using a manually-written shape program interpreter. How difficult is it to write such an interpreter? Also, how easy/difficult is to extend the DSL with new semantics operator and then write the corresponding interpreter extension?\n\nMinor typos:\npage 3: consists a variable \u2192 consists of a variable\npage 5: We executes \u2192 We execute\npage 6: synthetica dataset \u2192 synthetic dataset\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}