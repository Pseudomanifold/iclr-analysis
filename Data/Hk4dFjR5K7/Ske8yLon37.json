{"title": "interesting attack", "review": "In this paper, the authors proposed a new attack using deformation. The results are quite realistic to the naked eyes (at least for the example shown).  The idea is quite simple, generate small displacement and resample (interpolate) image until the label flips.\n\n- I think this is a good contribution, It is a kind of attack we should consider.\n- One thing which is good to consider is the type of interpolation. I believe the success rate would be different for linear versus say B-spline interpolation. Also, the width of the smoothing applied to the deformation field has an impact. The algorithm is straightforward, there is no reason to experiment with those.\n\n- It is useful to report pixel displacement in Table 1. The reported values are not intuitive, the **average** displacement for Inception-v3 is 0.59.  Here is my back of envelope conversion of 0.59 which is probably off:\n\n299 (# pixels of the smaller axis 299 for the Inception) x 1/2 (image are centered) x 0.59 =  88 pixels\n\nThis is huge! I think I am calculating something incorrectly because in Fig3,4 those displacements are not big. \n\n- The results of Table 2 is interesting. Why a networked trained with PGD is more robust to ADef attack that a network trained adversarially with Adef?\n\n\n\n\nMinor:\n- The paper is a bit nationally convoluted for no good reason, the general idea is straightforward. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}