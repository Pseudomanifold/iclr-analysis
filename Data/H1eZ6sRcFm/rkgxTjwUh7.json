{"title": "Review", "review": "This paper focuses on applying variational autoencoders to text data. A well-known problem in this domain is posterior collapse, where the posterior \"collapses\" to an uninformative prior distribution and the decoder becomes a standard language model which does not utilize the latent code. This paper assembles a few tricks to improve VAEs for text, including using a self-attention-based encoder, a mixture-of-Gaussians prior on the latent code, and a \"multi-task\" loss which attempts to reconstruct a BoW representation in addition to the original sentence. Some experiments are carried out on the Yelp dataset, including some qualitative study.\n\nThe issue of posterior collapse is important and modeling the global structure of text with a latent variable is a valuable endeavor. This paper provides some promising results in that direction. My main issue with the paper is that it is primarily the combination of a few pre-existing tricks to the text-VAE problem. Bringing together multiple tricks and showing that they can enable significant progress is potentially valuable, but only if the results are obviously better or they represent a big step forward in an important application (for example, showing convincing sentence interpolations, attribute vector arithmetic, autoencoding of very long text sequences, etc). This paper does not demonstrate any sufficiently strong result; instead, it just provides some limited evaluation that things get a little better along with some basic qualitative analysis of potential benefits of using a mixture prior. Apart from this, the paper claims that combining these tricks avoids \"weakening the decoder\" but uses a decoder with word dropout in all experiments, and also claims they decrease the number of hyperparameters, which is not shown in any rigorous way. The proposed additions also are described in insufficient detail - how are the bag of word representations constructed? How are they predicted? What is the additional loss term? How is it combined to the standard reconstruction loss for the autoregressive decoder? Etc. Finally, it is missing discussion (and comparison, as baselines) to a great deal of prior work on avoiding posterior collapse. As-is, the paper is not strong enough for publication. Below, I go into a bit more detail about specific issues and suggestions.\n\nSpecific comments:\n\n- The paper blames posterior collapse on teacher forcing (\"Because decoders for textual VAEs are trained with \u201cteacher forcing\u201d (Williams & Zipser, 1989), they can be trained to some extent without relying on latent variables.\") This is a misleading claim. Even with teacher forcing, it could potentially be advantageous to the model to utilize the latent (recall that the latent can capture information about the entire input sequence, so p(x_t | x_1, ... x_{t - 1}) is less informative than p(x_t | x_1, ... x_{t - 1})), but because of the architecture/learning dynamics/hyperparameters/etc the model tends to choose to \"ignore\" z. This is an independent problem of teacher forcing. The main issue caused by teacher forcing is exposure bias, which is a related but different issue. Teacher forcing is not necessary for training latent variable models with autoregressive decoders, it just happens to work well because it's a remarkably good search strategy.\n- \"existing models use a LSTM as the encoder, it is known that this simple model is not sufficient for text generation tasks (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017).\" I assume you are citing these papers because they deal with attention and show that attention improves sequence-to-sequence models. The fact that attention helps is because the decoder is provided with additional conditioning information, not because the encoder is different (in Bahdanau and Luong the decoder remains a simple LSTM).\n- There are many missing related works (and baselines) for avoiding posterior collapse, such as \"Avoiding Latent Variable Collapse with Generative Skip Models\", \"Adversarially Regularized Autoencoders\", \"Variational Lossy Autoencoder\", etc.\n- Your equation for p(x|z) at the beginning of 3.1.1 is a bit odd because you are talking about a deterministic z, and using it to point out that z might not learn good global features. However, when z is a stochastic latent variable the equation would be exactly the same. Writing it in this way does not make this distinction clear. A deterministic z can indeed capture interpretable global structure (see Figure 2 of Sutskever et al. 2014). If you want to claim that using a stochastic z may _better_ capture global structure, I would recommend making this argument from an information theoretic perspective (as was done in \"Variaional Lossy Autoencoder\", for example).\n- That weaking the decoder \"requires additional hyper-parameters specifying decoder capacity\" is not really a compelling arugment in your paper since your approach also requires additional hyperparameters (number of mixture components, coefficients for additional loss terms, etc.)\n- You call posterior collapse a \"trivial local minimum\". Are you sure it's a local minimum? This implies that the gradient of the loss w.r.t. every one of the parameters is zero at this point. I think you are using \"local minimum\" in a loose sense; please don't do this.\n- What are pseudo-inputs? Do you basically mean they are model parameters which are updated w.r.t. the loss function?\n- \"Recent research into text generation has found that simple LSTMs do not have enough capacity to encode information from the whole text.\" Citation needed!\n- I think what you are calling \"self-attention\" is actually the \"feed-forward attention mechanism\" from \"Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems\".\n- \"We simply summarize word representation of all words in the input text and project this vector with a linear layer.\" I don't think this adequately describes what you are doing. How is this summarization done? Via a histogram of the words which are present? An average of the words' embedding vectors? Same with the multi-task part. What exactly are you predicting? How?\n- I presume your BLEU scores in Table 1 are reflecting the BLEU score for the input vs. the reconstructed output. Please clarify.\n- In the appendix it is written that \"We applied 0.4 word dropout for input text to the decoder for our model and the model from Bowman et al. (2015).\" This is definitely \"weakening the decoder\" - that is the primary motivation for word dropout. I don't think you can claim your approach can avoid weakening the decoder if all of your results are with word dropout.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}