{"title": "Somewhat weak contribution", "review": "The paper describes a generative model for natural language, proposing various modifications to the previous work, including: a modified encoder which combines a self-attention model and a bag-of-words model, a modified multi-modal prior, and a modified training procedure. The claim is that the proposed improvements alleviate the \"posterior collapse\" issue, where the decoder ignores the information from the encoder.\n\nTechnical significance:\nAs the paper acknowledges, the proposed modifications have already been introduced in the literature and the current paper applies them to the natural language modeling task, showing that: combining self-attention and bag-of-word encoders, using an additional training task, and using a multi-modal prior improve performance (e.g. as measured by BLUE scores). From this perspective, the contribution is somewhat limited.\n\nEmpirical significance:\nThe main focus is avoiding \"posterior collapse\" and, unfortunately, this point is not illustrated strongly. Do the baselines show mode collapse and the work improves is that respect? Something is missing here: maybe a (toy) data-set where mode-collapse arises when training baselines, but not when the present modifications are introduced (as another comment points out), maybe training curves showing that the modifications provide a mode stable training process, etc.\n\nClarity:\nThe paper is generally well written, with the proposed modifications being clearly explained. The non-VAE sequence prediction task could be described in a bit more details where it's being used, i.e. in section 4.2. or in the caption of Table 1.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}