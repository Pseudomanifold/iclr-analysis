{"title": "Nice evaluations, empirically sound methodology, but no new model", "review": "This is a nice paper. It makes novel contributions by investigating (a) the problem of skewed dataset distributions in neural program synthesis, specifically program induction from given I/O pairs, and (b) the extent to which making them uniform would improve model performance. \n\nThe paper argues that there are inevitable and artificial sparsities as well as skews in existing datasets (e.g. pruning illegal I/O pairs, naive random sampling tends not to generate complex nested control-flow statements), and the principled way to minimize these sparsities and skews is to make distributions over salient random variables uniform. The authors evaluate their hypothesis empirically on two flavors of neural program synthesis methods: program inductions on explicit DSL represented by Karel, and implicit differentiable neural program synthesizers (such as stack, RAM, GPU as cited in section 2) represented by a Calculator example. In evaluations, they construct few challenging \u201cnarrower\u201d datasets and show baseline models perform significantly worse than models trained on datasets with uniform distributions (by 39-66 pp). Along this line, they also show uniform models consistently perform much better than baseline ones on other out-of-distribution test sets. To show how bad a model would perform if it were trained on a skewed training set, they train models on narrower datasets and evaluate them on different narrower sets.\n\nThe strength of this paper are:\n(1) It has complete and empirically sound evaluations: both showing how much better uniform models would be and how much worse non-uniform models would be.\n\n(2) Although we might doubt the salient random variables are handcrafted and rejection sampling wouldn\u2019t make the dataset completely uniform, they include evaluations on out-of-distribution datasets (e.g. CS106A dataset in section 5.2) to show that uniform models still perform better and thus their sampling scheme does cover some non-obvious sparsities and skews.\n\n(3) Despite the doubt on efficiencies of rejection sampling, they include both a proof and empirical results (section 8.3 and 8.4) to show they need sample O(1/\u03b5) times before finishing.\n\nWeaknesses:\n(1) No new model. This work has solely using the existing model from Bunel et al. (2018) in the Karel domain and didn\u2019t propose a new model that illustrates possibly a way to utilize/demonstrate the uniformity of dataset.\n\n(2) The calculator example is relatively too trivial to represent the whole genre of implicit differentiable neural program synthesizer (e.g. stack, GPU, RAM). \n\n(3) No statistical tests (such as chi-square test) to support the claim about uniformity (even on chosen salient variables) \n\nQuestions:\n(1) What if the distribution of real-world programs are skewed and neural synthesizers are supposed to take advantage of their skewness?\n\n(2) Why would you claim the calculator example is not a program synthesis task while intending to use it to represent another genre of program synthesis methods?\n\nSuggestions:\n(1) To show that current salient random variables do not make the dataset theoretically uniform but are still approximate enough, why not construct some distinct held-out salient variables (such as memory/grid/marker query times, executing time) from existing ones, construct narrower test sets accordingly, and hopefully show uniform models still perform significantly better than baseline?\n\n(2) In section 8.2, why not write the proportionality statement in two lines so that people wouldn\u2019t be confused to think Pr[X=x] = 1 while intending to show Pr[X=x] \u221d 1(an arbitrary constant) so that Pr[X] is uniform?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}