{"title": "Review", "review": "This paper is written reasonably clearly, and appears to be original.\n\nThis paper proposes a coupled optimization for simultaneously compressing a dataset (picking an informative subset) and learning. My major concern is that the problem itself is not formulated (nor solved) properly.\n\nThis is most easily uncovered by looking at the solution of the optimization given an oracle that provides h*. The algorithm would then pick z* by simply choosing the K lowest objective function values (indeed this is one of the block-coordinate steps). These are simply the data that the single candidate function h* fits best, and they may be very close / redundant with one another. There is no step in the procedure that forces the subset to capture the learning problem itself.\n\nFurther, each \"F-step\" requires solving a feasibility problem that is just as hard as solving the original learning problem on the full dataset. Even if I set aside that concern, there is still the matter of the arbitrary threshold epsilon, which the paper provides no guidance on. Indeed the F-step may not even be feasible if epsilon is set incorrectly. The authors incorrectly state the solution to the F-step for linear regression (for the same reason).\n\nI am also not certain the main result (proposition 4 / corollary 1) provides any useful theoretical result. In particular, for K = 1 (given an appropriate choice of epsilon) it's not hard to pick an h* such that g(h*, z*) = 0 for all N. It's also very odd that the proposition requires a lower bound on K, when increasing K just increases the optimal objective g(h*, z*) -- having a lower bound seems to suggest increasing K should result in a better objective g. The proposition also provides no constraint / assumption on epsilon, which is also odd.\n\nFinally, the paper does not compare its method to any other compression scheme -- even random subsampling. The experimental results on compression are meaningless without a baseline to say how easily compressed these datasets are.\n\nIn addition to the above major concerns, the paper does not cite any papers from the vast body of literature on coresets and model compression.\n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}