{"title": "Interesting viewpoint, but could use more examples", "review": "In this paper, the authors relate the architectures of recurrent neural\nnetworks with ODEs and defines a way to categorize the RNN architectures by\nlooking at non-linearity order and temporal memory scale. They further\npropose QUNN, a RNN architecture that is more stable and has less complexity\noverhead in terms of input dimension while comparing with LSTM. \n\nAlthough this paper provides a new view point of RNN architectures and relates\nRNNs with ODEs, it fails to provide useful insight using this view point.\nAlso, it is not clear what advantage the new proposed architecture QUNN has\nover existing models like LSTM or GRU. \n\nThe paper is well presented and the categorization method is well defined.\nHowever, how the order of non-linearity or the length of temporal memory\naffect the behavior and performance of RNN architectures are not studied.\n\nIt is proved that QUNN is guaranteed existence and its Jacobian eigen values\nwill always have zero real part. It would be easier to understand if the\nauthors could construct a simple example of QUNN and conduct at least some \nsynthetic experiments.\n\nIn general I think this paper is interesting but could be extended in various\nways. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}