{"title": "A simple network compression strategy combining weight and activation pruning.", "review": "The main contribution of the paper is an integral model compression method that handles both weight and activation pruning. Increasing the network weight and activation sparsity can lead to more efficient network computation.  The authors show in the paper that pruning the network weights alone may result in a decrease in activation sparsity, which may not necessarily improve the overall computation. The proposed solution is a 2-stage process that first prunes the weights and then the activation. \n\nPros:\n\n- The results show that the proposed method is effective in reducing the number of multiply-and-accumulate (MAC) compared to weight pruning alone. The improvements are consistent across multiple network architectures and datasets.\n- It also shows that weight pruning alone leads to a slight increase in the number of non-zeros activation.\n\nCons:\n\n- A simple approach with limited novelty.\n- Related work should include other compression techniques, such as low-rank approximation,  weight quantization and varying hidden layer sizes.\n- There is no comparison with other model compression techniques mentioned above.\n ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}