{"title": "Simple idea and lack of experiments", "review": "This paper proposes to compress the deep learning model using both activation pruning and weight pruning. Combining both sparsities, the MACs are significantly reduced. \n\nMy main concern is that there is no time comparison. The experiments only show the reduction in terms of the number of non-zeros in weights and activation as well as the MACs. Typically, to deal with sparse activations and sparse weights, there are some overhead computations such as computing indices. Also, dense matrix-matrix(vector) multiplications can be faster by using specially designed libraries.  I would suggest the authors show the improvement for the proposed compression approach in terms of wall-clock time, in CPU, GPU or other hardware platforms. \n\nThe pruning method seems straight-forward to me. I am wondering how to choose the winner rate for each layer. It seems to take a quite long time to pick a set of winner rates for a deep neural network. \n\nThe paper is easy to read in general. However, it is not clear to me how such a compression approach can speed up the training or the inference of deep learning models in practice. \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}