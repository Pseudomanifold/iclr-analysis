{"title": "Lack of baselines and insufficient analysis", "review": "This paper presents an approach of using GANs for text generation by training a Skip-thought sentence embedding model and applying GANs to distinguish embeddings from the training corpus encoded with a skip thought encoder and sentence embeddings generated by a CNN.\n\nWhile the paper gives some initial experimental results in table 1, there are major gaps in the experiments. For example, there are no comparisons with maximum-likelihood trained models or comparisons with other existing GAN architectures. Without these comparisons, it's hard to see the benefit of having the skip-thought architecture compared to the existing text GAN literature such as SeqGAN and MaskGAN.\n\nThe improvements gained with Wasserstein and gradient penalties are interesting but results on more datasets and longer samples are needed, especially section 4.2 would benefit from quantitative and human evaluation.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}