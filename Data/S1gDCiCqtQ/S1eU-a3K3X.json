{"title": "Review", "review": "This paper proposes an unsupervised method for subgoal discovery and shows how to combine it with a model-free hierarchical reinforcement learning approach. The main idea behind the subgoal discovery approach is to first build up a buffer of \u201cinteresting\u201d states using ideas from anomaly detection. The states in the buffer are then clustered and the centroids are taken to be the subgoal states.\n\nClarity:\nI found the paper somewhat difficult to follow. The main issue is that the details of the algorithm are scattered throughout the paper with Algorithm 1 describing the method only at a very high level. For example, how does the algorithm determine that an agent has reached a goal? It\u2019s not clear from the algorithm box. Some important details are also left out. The section on Montezuma\u2019s Revenge mentioned that the goal set was initialized using a \u201ccustom edge detection algorithm\u201d. What was the algorithm? Also, what exactly is being clustered (observations or network activations) and using what similarity measure? I can\u2019t find it anywhere in the paper. Omissions like this make the method completely unreproducible. \n\nNovelty:\nThe idea of using clustering to discover goals in reinforcement learning is quite old and the paper does a poor job of citing the most relevant prior work. For example, there is no mention of \u201cDynamic Abstraction in Reinforcement Learning via Clustering\u201d by Mannor et al. or of \u201cLearning Options in Reinforcement Learning\u201d by Stolle and Precup (which uses bottleneck states as goals). The particular instantiation of clustering interesting states used in this paper does seem to be new but it is important to do a better job of citing relevant prior work and the overall novelty is still somewhat limited.\n\nSignificance:\nI was not convinced that there are significant ideas or lessons to be taken away from this paper. The main motivation was to improve scalability of RL and HRL to large state spaces, but the experiments are on the four rooms domain and the first room of Montezuma\u2019s Revenge, which is not particularly large scale. Existing HRL approaches, e.b. Feudal Networks from Vezhnevets et al. have been shown to work on a much wider range of domains. Further, it\u2019s not clear how this method could address scalability issues. Repeated clustering could become expensive and it\u2019s not clear how the number of clusters affects the approach as the complexity of the task increases. I would have liked to see some experiments showing how the performance changes for different numbers of clusters because setting the number of clusters to 4 in the four rooms task is a clear use of prior knowledge about the task.\n\nOverall quality:\nThe proposed approach is based on a number of heuristics and is potentially brittle. Given that there are no ablation experiments looking at how different choices (number of clusters/goals, how outliers are selected, etc) I\u2019m not sure what to take away from this paper. There are just too many seemingly arbitrary choices and moving parts that are not evaluated separately.\n\nMinor comments:\n- Can you back up the first sentence of the abstract? AlphaGo/AlphaZero do well on the game of Go which has ~10^170 valid states.\n- First sentence of introduction. How can the RL problem have a scaling problem? Some RL methods might, but I don\u2019t understand what it means for a problem to have scaling issues.\n- Please check your usage of \\cite and \\citep. Some citations are in the wrong format.\n- The Q-learning loss in section 2 is wrong. The parameters of the target (r+\\gamma max Q) are held fixed in Q-learning.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}