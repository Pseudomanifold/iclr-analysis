{"title": "Very interesting contribution", "review": "This paper proposes a new sequence to sequence model where attention is treated as a latent variable, and derive novel inference procedures for this model. The approach obtains significant improvements in machine translation and morphological inflection generation tasks. An approximation is also used to make hard attention more efficient by reducing the number of softmaxes that have to be computed.  \n\nStrengths:\n- Novel, principled sequence to sequence model.\n- Strong experimental results in machine translation and morphological inflection.\nWeaknesses:\n- Connections can be made with previous closely related architectures.\n- Further ablation experiments could be included. \n\nThe derivation of the model would be more clear if it is first derived without attention feeding: The assumption that output is dependent only on the current attention variable is then valid. The Markov assumption on the attention variable should also be stated as an assumption, rather than an approximation: Given that assumption, as far as I can tell the (posterior) inference procedure that is derived is exact: It is indeed equivalent to the using the forward computation of the classic forward-backward algorithm for HMMs to do inference. \nThe model\u2019s overall distribution can then be defined in a somewhat different way than the authors\u2019 presentation, which I think makes more clear what the model is doing:\np(y | x) = \\sum_a \\prod_{t=1}^n p(y_t | y_{<t}, x, a_t) p(a_t | y_{<t}, x_ a_{t-1}).  \nThe equations derived in the paper for computing the prior and posterior attention is then just a dynamic program for computing this distribution, and is equivalent to using the forward algorithm, which in this context is:\n \\alpha_t(a) = p(a_t = a, y_{<=t}) = p(y_t | s_t, a_t =a) \\sum_{a\u2019} \\alpha_{t-1}(a\u2019) p(a_t = a | s_t, a_{t-1} = a\u2019) \n\nThe only substantial difference in the inference procedure is then that the posterior attention probability is fed into the decoder RNN, which means that the independence assumptions are not strictly valid any more, even though the structural assumptions are still encoded through the way inference is done. \n[1] recently proposed a model with a similar factorization, although that model did not feed the attention distribution, and performed EM-like inference with the forward-backward algorithm, while this model is effectively computing forward probabilities and performing inference through automatic differentiation.\n\nThe Prior-Joint variant, though its definition is not as clear as it should be, seems to be assuming that the attention distribution at each time step is independent of the previous attention (similar to the way standard soft attention is computed) - the equations then reduce to a (neural) version of IBM alignment model 1, similar to another recently proposed model [2]. These papers can be seen as concurrent work, and this paper provides important insights, but it would strengthen rather than weaken the paper to make these connections clear. \n\nThe results clearly show the advantages of the proposed approach over soft and sparse attention baselines. However, the difference in BLEU score between the variants of the prior or posterior attention models is very small across all translation datasets, so to make claims about which of the variants are better, at a minimum statistical significance testing should be done. Given that the \u201cPrior-Joint\u201d model performs competitively, is it computationally more efficient that the full model? \n\nThe main missing experiment is not doing attention feeding at all. The other experiment that is not included (as I understood it) is to compute prior and posterior attention, but feed the prior attention rather than the posterior attention. \n\nThe paper is mostly written very clearly, there are just a few typos and grammatical errors in sections 4.2 and 4.3. \n\nOverall, I really like this paper and would like to see it accepted, although I hope that a revised version would make the assumptions the model is making clearer and make connections to related models clearer. \n \n[1] Neural Hidden Markov Model for Machine Translation, Wang et al, ACL 2018. \n[2] Hard Non-Monotonic Attention for Character-Level Transduction, Wu, Shapiro and Cotterell, EMNLP 2018. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}