{"title": "Limited novelty", "review": "This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented.\n\nThe motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. \n\nMinor question\n- What is the difference between negative LL's and reconstruction losses in experiments?\n- The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1?\n\n\nReferences\n[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.\n[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}