{"title": "Distortion and Obfuscated Gradients", "review": "The paper addresses the robustness of deep neural networks to adversarial example attacks. It uses a bilateral filtering as a preprocessing step to recover clean data from adversarial ones. It can also get combined with adversarial training and be trained end-to-end. The paper is well written, the background and introduction is clear. However I have comments about their implementation and experimental results:\n\n1. They are claiming to have a very high distortion while their model can still perform well.  They have to make sure obfuscated gradients has not happened and they have implemented the back-propagation from attack to defense correctly. \n\n2. Also if they had consider black-box threats as well as white-box one, it would have been more informative of how their method actually performs. Specially, in order to check whether obfuscated gradients has happened, the black-box threats are better choice, which they have not tried.  \n\nSo considering the above, there might be an issue in what they are claiming, so they might reconsider their method. Maybe reviewing their codes by some experts in this topic can give a good evaluation. \n\n(I have to mention that I am not an expert in Adversarial networks)\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}