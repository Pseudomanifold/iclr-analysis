{"title": "Good paper, experimental validation must be improved", "review": "Even though many people have considered prunning as architecture search, it has not been explored enough so far. This paper comprises a good approach for compression of achitectures using pruning. Based on the uniqueness of the topological ordering of commonly used neural networks (feed forward, skip connection), the paper proposes a simple and easily manipulable vector (sequence) representation for a wide class of neural networks. Instead of using RNN networks, such long seqeunce representation are mapped to a continuous embedding by 1D-CNN. While training this embedding, for the purpose of compression, predictors needed for compression are jointly trained with embedding.\nConsequently, the proposed method presents a possiblity of including many other constraints during the architecture search.\n\nIn the specification of layers, the layer type is just appointed to an integer variable, even though in reality the layer type is a categorical variable. This choice is ok for standard neural network layers, where effectively the choice is between a single catecorical aspect. However, for more sophisticated layer configurations, where you may need many categorical choices, this model choice will not be adequate and will likely lead to artificially biased design choices. The authors should explain the limitations of this model design and propose methods these limitations can be tackled.\n\nThe overall model achieves quite good result in compression. On CIFAR10 the model show good performance as compared to existing compression methods. It should be noted that other methods start with a given stucture, so their search space is more limited than this paper's approach. Specifically, compared to those methods, the search space for the proposed paper is larger because although the number of layers is fixed, the connections between layers give more freedom to the compression algorithm.\n\nCurrently, the number of experiments is borderline. They are enough to indicate the potentials of this approach. However, additional experiments would be welcome. For one, it would be to evaluate the proposed model in more challenging setups: evaluate on ImageNet dataset, using some of the recent architectures (e.g., ResNet, VGGnet, and so on). What is more, for more compressed architectures with better accuracy, when searching for a compressed architecture global optimization methods like Bayesian Optimization is worth to try, for instance using the recently proposed BOCK (Oh et al, ICML 2018).\n\nSome additional comments.\n- For a finite number of edges the number of possible graphs (including the valid architecture) are finite when the input is finite and pooling reduces the feature map size. Thus, it seems that the statement in theorem 3.2 is rather trivial and it is not worth calling it a Theorem.\n\nOverall, this was an interesting paper to read and worth of acceptance, provided that the proposed method delivers also in more competitive experimental settings.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}