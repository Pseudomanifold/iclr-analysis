{"title": "Interesting results. Normalization is not necessary to train deep resnets.  ", "review": "\nThis paper shows that with a clever initialization method ResNets can be trained without using batch-norm (and other normalization techniques).  The network can still reach state-of-the-art performance.\n\n\nThe authors propose a new initialization method called \"ZeroInit\" and use it to train very deep ResNets (up to 10000 layers). They also show that the test performance of their method matches the performance of state-of-the-art results on many tasks with the help of strong data augmentation. This paper also indicates that the role of normalization in training deep resnets might not be as important as people thought. In sum, this is a very interesting paper that has novel contribution to the practical side of neural networks and new insights on the theoretical side. \n\nPros:\n1. The analysis is not complicated and the algorithm for ZeroInit is not complicated.  \n2. Many people believe normalization (batch-norm, layer-norm, etc. ) not only improves the trainability of deep NNs but also improves their generalization. This paper provides empirical support that NNs can still generalize well without using normalization. It might be the case that the benefits from the data augmentation (i.e., Mixup + Cutout) strictly contain those from normalization. Thus it is interesting to see if the network can still generalize well (achieving >=95% test accuracy on Cifar10)  without using strong data-augmentation like mixup or cutout. \n3.Theoretical analysis of BatchNorm (and other normalization methods) is quite challenging and often very technical. The empirical results of this paper indicate that such analysis, although very interesting, might not be necessary for the theoretical understanding of ResNets.  \n\n\nCons:\n1.The analysis works for positively homogeneous activation functions i.e. ReLU, but not for tanh or Swish. \n2.The method works for Residual architectures, but may not be applied to Non-Residual networks (i.e. VGG, Inception)  ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}