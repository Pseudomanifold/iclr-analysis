{"title": "The method presented is partially based on interesting observations, and it obtains good empirical results (tough not better than competition in general). However, the presentation is somewhat misleading: the method includes normalization elements not discussed, and some of its components are not justified and not tested empirically in isolation.", "review": "Summary: \nA method is presented for initialization and normalization of deep residual networks. The method is based on interesting observations regarding forward and backward explosion in such networks with the standard Xavier or (He, 2015) initializations. Experiments with the new method show that it is able to learn with very deep networks, and that its performance is on a par with the best results obtained by other networks with more explicit normalization.\nAdvantages:\n-\tThe paper includes interesting observations, resulting in two theorems,  which show the sensitivity of traditional initializations in residual networks\n-\tThe method presented seems to work comparable to other state of the art initialization + normalization methods, providing overall strong empirical results. \nDisadvantages:\n-\tThe authors claim to suggest a method without normalization, but the claim is misleading: the network has additive and multiplicative normalization nodes, and their function and placement is at least as \u2018mysterious\u2019  as the role of normalization in methods like batch and layer normalization.\no\tThis significantly limits the novelty of the method: it is not \u2018an intialization\u2019 method, but a combination of initialization and normalization, which differ from previous ones in some details. \n-\tThe method includes 3 components, of which only one is justified in a principled manner. The other components are not justified neither by an argument, nor by experiments. Without such experiments, it is not clear what actually works in this method, and what is not important.\n-\tThe argument for the \u2018justified\u2019 component is not entirely clear to me. The main gist is fine, but important details are not explained so I could not get the entire argument step-by-step. This may be a clarity problem, or maybe indicate deeper problem of arbitrary decisions made without justification \u2013 I am not entirely sure. Such lack of clear argumentation occurs in several places\n-\tExperiments isolating the contribution of the method with respect to traditional initializations are missing (for example: experiments on Cifar10 and SVHN showing the result of traditional initializations with all the bells and whistles (cutout, mixup) as the zeroInit gets.\n\nMore detailed comments:\nPage 3:\n-\tWhile I could follow the general argument before eq. 2, leading to the conclusion that the initial variance in a resnet explodes exponentially, I could not understand eq. 2. What is its justification and how is it related to the discussion before it? I think it requires some argumentation.\nPage 4:\n-\tI did not understand example 2) for a p.h. set. I think an argument, reminder of the details of resnet, or a figure are required.\n-\tI could not follow the details of the argument leading to the zeroInit method:\no\tHow is the second design principle \u201cVar[F_l(x_l)] = O( 1/L) justified?\nAs far as I can see, having Var[F_l(x_l)] = 1/L will lead to output variance of (1+1/L)^L =~e, which is indeed O(1). Is this the argument? Is yes, why wasn\u2019t it stated? Also: why not smaller than O(1/L)?\no\tFollowing this design principle several unclear sentences are stated:\n\uf0a7\tWe strive to make Var[F_l(x_l)] = 1/L, yet we set the last convolutional layer in the branch to 0 weights. Does not it set Var[F_l(x_l)] = 0, in contradiction to the 1/L requirement?\n\uf0a7\t \u201cAssuming the error signal passing to the branch is O(1),\u201d \u2013 what does the term \u201cerror signal\u201d refer to? How is it defined? Do you refer to the branch\u2019s input?\n\uf0a7\tI understand why the input to the m-th layer in the branch is O(\\Lambda^m-1) if the branch input is O(1) but why is it claimed that \u201cthe overall scaling of the residual branch after update is O(\\lambda^(2m-2))\u201d? what is \u2018the overall scaling after update\u2019 (definition) and why is it the square of forward scaling?\n-\tThe zero Init procedure step 3 is not justified by any argument in the proceeding discussion. Is there any reason for this policy? Or was it found by trial and error and is currently unjustified theoretically (justified empirically instead). This issue should be clearly elaborated in the text. Note that the addition of trainable additive and multiplicative elements is inserting the normalization back, while it was claimed to be eliminated. If I understand correctly, the \u2018zeroInit\u2019 method is hence not based on initialization (or at least: not only on initialization), but on another form of normalization, which is not more justified than its competitors (in fact it is even more mysterious: what should we need an additive bias before every element in the network?)\nPage 5:\n-\tWhat is \\sqrt(1/2) scaling? It should be defined or given a reference.\nPage 6:\n-\tIt is not stated on what data set figure 2 was generated.\n-\tIn table 2, for Cifar-10 the comparison between Xavier init and zeroInit shows only a small advantage for the latter. For SVHN such an experiment is completely missing, and should be added.\no\tIt raises the suspect the the good results obtained with zeroInit in this table are only due to the CutOut and mixup used, that is: maybe such results could be obtained with CutOut+Mixup without zero init, using plain Xavier init? experiments clarifying this point are also missing.\nAdditional missing experiments:\n-\tIt seems that  ZeroInit includes 3 ingredients (according to the box in page 4), among which only one (number 2) is roughly justified from the discussion.  Step 1) of zeroing the last layer in each branch is not justified \u2013why are we zeroing the last layer and not the first, for example? Step 3 is not even discussed in the text \u2013 it appear without any argumentation. For such steps, empirical evidence should be brought, and experiments doing this are missing. Specifically experiments of interest are:\no\tUsing zero init without its step 3: does it work? The theory says it should.\no\tUsing only step 3 without steps 1,2. Maybe only the normalization is doing the magic?\nThe paper is longer than 8 pages.\n\nI have read the rebuttal.\nRegarding normalization: I think that there are at least two reasonable meanings to the word 'normalziation': in the wider sense is just means mechanism for reducing a global constant (additive normalization) and dividing by a global constant (multiplicative normalization). In this sense the constant parameters can be learnt in any way. In the narrow sense the constants have to be statistics of the data. I agree with the authors that their method is not normalization in sense 2, only in sense 1. Note that keeping the normalization in sense 1 is not trivial (why do we need these normalization operations? at least for the multiplicative ones, the network has the same expressive power without them).  I think the meaning of normalization  should be clearly explained in the claim for 'no  normalization'.\nRegarding additional mathematical and empirical justifications required: I think such justifications are missing in the current paper version and are not minor or easy to add. I believe the work should be re-judged after re-submission of a version addressing the problems.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}