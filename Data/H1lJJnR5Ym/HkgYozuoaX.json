{"title": "Promising method but poor evaluation and presentation", "review": "My apologies for posting late, I was seriously injured around the reviewer deadline.\n\n---------------------------------\n\nThe authors propose \"random network distillation,\" a method that adds an additional reward based on a proxy for \"exploration\" to the RL task at hand. The method works by including an extra term in the reward during training. The term is calculated as follows. A randomly initialized network is created during rollout generation. Another network is initialized as well, and during rollouts is trained to predict the output of the randomly initialized network applied to the states. The agent then uses a measure of the prediction loss as an intrinsic reward. These rewards are then included as part of the trajectory, and are predicted separately for training purposes.\n\nThe authors find that when you combine these intrinsic rewards with agents trained at extremely large scale (~2 billion frames per training run!) it is possible to perform very well on Montezuma's revenge and other sparse reward tasks.\n\nOverall, the paper has great potential - it presents the first algorithm to solve a challenging sparse reward RL task. However, while the method itself is promising, the weak baselines (in particular, the lack of evidence disentangling the benefits of larger scale / more frames vs the benefits of the proposed method) and unclear presentation make me unable to yet recommend the paper for acceptance.\n\nPositive:\n - The work reaches the state-of-the-art on several sparse reward tasks, most notably Montezumas revenge\n - On Montezumas revenge, the method is able to pass through the first level, and explore the vast majority of rooms.\n - The reward mechanism seems to be novel\n\nNegative:\n - All previous work used more than an order of magnitude more frames in training. From the experiments given, it is impossible to distinguish the impact of RND vs larger scale training\n - The baselines are not very strong: The forward dynamics baseline does significantly worse on Montezumas revenge than the previous results in Ostrovski et al and Bellemare et al, even using more than an order of magnitude more frames.\n - Important experimental details lack adequate descriptions\n - Tables and figures are not written with adequate details\n\nDetails of negative feedback:\n\nMajor:\n-------------\nUnclear baselines and questionable improvement on SOTA:\n\n - Previous work (the neural density functions of Ostrovski et al or the CTS scheme of Bellemare et al.) used significantly fewer (~100 million and ~150 million respectively vs ~2 billion) frames of experience in solving Montezumas Revenge, which makes this method\u2019s benefit somewhat incomparable to previous methods given the sampling regime it operates in.\n - It is important to disentangle the impacts of:\n\n   (1) Using many more (an order of magnitude) frames than previous methods\n   (2) The presented RND bonus method\n\n  and it is impossible to separate these without further extensive experimentation with previous methods. The main claim of the paper is that the RND bonus is a better method for solving hard exploration games; this needs to be shown through a rigorous comparison.\n - The fact that the forward dynamics does worse than vanilla PPO (and the previous results in Ostrovski et al and Bellemare et al) on Montezuma's revenge brings the strength of the used baseline into question\n\n\nOverall, the experimental details are greatly lacking:\n\n - The way that the value function is trained (i.e. the objective function) is never explained in the paper. The value function in PPO is typically (according to the baselines repository) trained at each step to fit (GAE advantage + previous value), but in the paper this is not elaborated on.\n - If this is indeed the case, then the statement that the extrinsic value function fits a stationary distribution on page 5 should be fixed.\n - In Table 4 the $\\lambda$ hyperparameter is listed, but is not described at all in the paper. I am guessing that it is the corresponding GAE hyperparameter, but I am not sure as the GAE method is never written about or cited throughout the paper.\n - The paper is not written in a way that is accessible to people that do not closely follow the line of work on sparse rewards. For example, though it is possible to infer, the paper never explicitly defines the intrinsic reward $i_t$ in the main paper text. The exact mechanism through which the \"forward dynamics\" baseline is never given.\n\n\nTables and figures do not give sufficient detail to know what they are describing:\n\n - Table 5 states that the values given are means, but does not say how many samples each mean was generated from until Table 6. The contents of Table 6 should be in the figure captions; it is important to understand how many samples graphs are generated with\n - Similarly, the way that the shaded regions are calculated should be included up front in the first figure with them in it. At first I believed that the intervals were confidence intervals, but they are actually standard deviations.\n - How are the graph lines calculated? I am not sure, but they look like they have been smoothed out - the captions should indicate this if so. If they are smoothed, are the standard deviations calculated before or after smoothing?\n\nMinor:\n-------------\n - Figure 7 has only 3 random seeds compared. To make comparisons between the RND RNN and CNN policies methods you should use more seeds/samples.\n - On page 2 it is said that previous exploration methods are difficult to scale; a (very short) explanation on why would be appreciated\n - On page 4, it would be good to explain why one would be concerned that episodic rewards can leak information about the task to the agent\n - It would be interesting to plot the RND exploration bonus over time as training iteration progresses; this could give some insight into training dynamics that we cannot see from looking at reward trajectories alone.\n - It would be good to include experimentation around understanding if there is a benefit to using this technique in dense reward tasks.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}