{"title": "Review", "review": "This paper studies ReLU model, or equivalently, one-layer-one-neuron model, for the classification problem. This paper shows if the data is linearly separable, gradient descent may converge to either a global minimum or a sub-optimal local minimum, or diverges. This paper further studies the implicit bias induced by GD and SGD and shows if they converge, they can have a maximum margin solution. \n\nComments:\n1. Using ReLU model for linearly separable data doesn't make sense to me. When ReLU is used, I expect some more complicated separable condition. \n2. This paper only studies one-layer-one-neuron model, which is a very restricted setting. It's hard to see how this result can be generalized to the multiple-neuron case.\n3. The analysis follows closely with previous work in studying the implicit bias for linear models.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}