{"title": "Experimental results are too weak", "review": "In the paper, the authors try to propose an adaptive learning rate method called predictive local smoothness.  They also do some experiments to show the performance. \n\nThe following are my concerns:\n\n1. The definition of the L(x_t) is confusing. In (8), the authors define L(x_t), and in (10), the authors give another definition.  Does the L(x_t) in (10) always guarantee that (8) is satisfied? \n\n2. In theorem 1, \\mu^2 = \\frac{1}{n} \\sum_{i=1}^n L_i^2(x_t) + \\frac{2}{n^2}  \\sum_{i<j}^n L_i(x_t) L_j(x_t) > v. It looks like that \\mu > (1-\\rho^2) v, no matter the selection of \\rho.  Why?\n\n3. How do you compute L_i(x_t)  if x is a multi-layer neural network?\n\n4. The experimental results are too weak. In 2018, you should at least test your algorithm using a deep neural network, e.g. resnet. The results on a two-layer neural network mean nothing. \n\n5. sometimes, you algorithm even diverge. for example, figure 3 second column third row.  \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}