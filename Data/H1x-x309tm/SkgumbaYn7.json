{"title": "This paper investigates the convergence condition of Adam-type optimizers in the unconstrained non-convex optimization problems.", "review": "The main theory points out two scenarios causing Adam-type optimizers to diverge, which extends Reddi et al's results. \n\nThe theorem in this paper applies to all Adam-type algorithms, which combine momentum with adaptive learning rates and thus are more general as compared to the recent papers, such as Zhou et al's. The relationship between optimizers' effective step size, step size oscillation and convergence is well demonstrated and is interesting.\n\nRemarks:\n1. The main theorem and proof are based on the non-convex settings while the examples to demonstrate the convergence condition are simple convex functions.\n\n2. The message delivered by MNIST experiment is limited, is not clear and is not very relevant to the main part of the paper. It would be better to compare these algorithms in larger deep learning tasks.\n\nTypo:\nPage 5, section 3.1: Term A is a generalization of term alpha^2 g^2 (instead of just alpha^2) for SGD.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}