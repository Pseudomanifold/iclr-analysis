{"title": "Review", "review": "This paper studies the problem on computing the spectrum of singular values of linear convolutional layers. This is an important problem with abundant applications on regularizing deep neural networks. However, there are several technical issues need to be addressed in its current form. \n\nFirst, in the section \"Summary of Results\", at first read of the paper I found it very confusing why the time complexity of computing the spectrum is a function of n, where n is the size of the input feature map. Intuitively, since the size of the convolutional kernel is m x m x k x k, it is expected that the time complexity is expressed as a function of (m, k). Later I realized that this is due to the unnecessary and redundant 0 padding in section 2.1 that leads to this artifact. I understand that in order to apply the described Fourier transform technique it is necessary to introduce the large nxn filter, which is of the same size as the input, but it also introduces redundant computation. This fact further emerges in the introduction of matrix A in Eq (1). \n\nMore importantly, I think the authors didn't perform a detailed analysis on using the basic definition of convolutional filter to compute its spectrum, and this is the reason why they reached a misleading conclusion that simple SVD takes O(n^6 m^3) time. Specifically, each convolution operation corresponds to a inner product operation, so we can reshape the input 3D tensor with shape m x n x n into a 2D matrix, with shape n^2 x mk^2, denoted as X. Note that this creates a unnecessary redundancy in the input feature map, but it does not create redundant weight for the convolutional kernel. As a comparison, the introduced matrix A in the paper is heavily redundant. Similarly, for m channels, we can reshape the 4D convolutional kernel with shape m x m x k x k into a 2D matrix, with shape mk^2 x m, denoted as K. Then the usual convolution layer can be described as the following linear system: Y = X K, where Y with shape n^2 x m is the output, and can be easily reshaped into size m x n x n. Hence to compute the spectrum of the convolution layer corresponds to computing the singular values of the 2D matrix K with size mk^2 x m. Hence a naive application of SVD directly gives us the solution in time O(m^3 k^2) (Note that the time complexity of SVD for matrix with size a x b is O(min{a^2 b, a b^2})), which is much smaller than the one given in the paper O(m^3 n^2) since k << n. \n\nIn experiment the authors made unfair comparison between their proposed method and the full matrix method: the full matrix A is fully redundant, due to its circulant pattern. As this implies a highly redundant information, nobody will form and compute matrix A explicitly in practice. So the time improvements demonstrated in the experiment section are meaningless. A valid baseline would be to compare the proposed method with the one introduced above. But in this case I would imagine the proposed method to be worse due to its unnecessary 0 padding leading to the worst time complexity. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}