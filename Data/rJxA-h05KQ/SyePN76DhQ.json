{"title": "Review", "review": "# Summary\n\nThe paper proposes a new method for computing output uncertainty estimates in DNNs for classification problems. The authors claim that replacing the softmax output layer for an Inhibit Softmax layer (a modified softmax with an additional constant additive term in the denominator, proposed in Saito et al., 2016) matches the state-of-the-art methods for uncertainty estimation an outperforms them in out-of-distribution detection tasks.  While the original idea is simple, the final method is convoluted, including several modifications (section 3.1.) to deal with an ill-behaved optimization objective, making it difficult to evaluate why the method works in the end. Finally, the experimental section needs to be extended. The proposed metrics are presented as the gold standard to measure uncertainty calibration; however, it is known that this is itself an open problem due to the fact that ground truth labels about uncertainty are not available. The paper should be extended with experiments that use a simulated data to gain a further understanding of what kind of uncertainty it is capture by the model, as well as considering additional metrics like the brier score, histograms of predictive entropy, \u2026. Also, several state-of-the-art inference methods for BNNs that improve the results of the original backprop paper are missing in the experimental section.\n\n# Details\n\nThe key idea is to use an Inhibited softmax output layer (instead the classical softmax) to get a better estimation of the output uncertainty (epistemic, aleatoric and distributional). They claim that the method matches other state-of-the-art methods in the field and outperform them in out-of-distribution detection tasks. However, the reason why this may be true is unclear and the experimental section needs to be strengthen. \n\nRegarding the inhibited softmax layer, they propose to decompose the log-likelihood in two term: the log-likelihood of the classical softmax plus a term modeling a certainty score that is maximized. What the method is doing is adding a term to all the probabilities which effectively is a simple smoothing of the output probabilities, leaving out some probability for when the test data is far from the training one. This value is fix as a hyper-parameter.  In particular \u201cc\u201d dictates the probability of the instance belonging to a new class, and so, fixing it ad-hoc means that you determine how far a test data should be from the training set in order to consider it out-of-distribution. It seems then a bad idea to fix this with independence of the dataset/model we are looking at. The paper would benefit from a deeper analysis of the method and connections with other methods in the literature, e.g. connections to the open world classification problem would be of interest.\n\nAuthors should elaborate on Section 3.1.. This section summarized 4 modifications that seems to be needed in order for the proposed method to work. Each of these modifications are explained in just a short paragraph, and it seems that choosing correctly these values is critical for the final performance of the algorithm. The number of hyper-parameters to tune increases making more difficult to understand why the method works (despite the analysis in the appendix). \n\nIn the experimental section, I would encourage the authors to propose some experiments using synthetic data where they have more control about the uncertainty of the generated data. These experiments would allow to visualize several properties of the uncertainty estimator and offer more insights to the reader. I would also like to see how the current method performs in the low data regime: when you have a small training set and an over parameterized DNN. In this case, I would expect that the method overfit since all the uncertainty of the lower levels is collapsed before reaching the output. Finally, in recent years, several advances in variational inference have been proposed that improve upon the basic version of Backprop (Blundell et al., 2015), e.g., https://arxiv.org/pdf/1703.01961.pdf, https://arxiv.org/pdf/1511.06233.pdf \u2026 that should be included in the experimental section.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}