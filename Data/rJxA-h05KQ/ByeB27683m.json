{"title": "Interesting idea, but needs more careful analysis.", "review": "The authors present inhibited softmax, a modification of the softmax through adding a constant activation which under a particular interpretation provides a measure for uncertainty. The method has no learnable parameter, works with only a single forward operation, does not need additional data for training, and does no input \"preprocessing.\"  The authors present experiments on vision problems and sentiment analysis.\n\nThe idea is to add a constant pre-softmax class activation c. In section 3 the authors explain why this \"serves as an uncertainty estimator.\" I read this paragraph several times, I do not see how they reach this conclusion. They point out that \"P_c(x) is maximized only for cases from training distribution.\" Unless we have a reasonable justification that P_c(x) would emit small values on the other samples, assuming this alone does not tell us anything useful.\n\nThe authors chose to split the new formulation to S(x)_i times P_c(x) in equation (2). If we keep the original equation and expand the cross-entropy term we get:\n\nL_i = - x_(y_i) + log(sum(exp(x_i))+exp(c)).\n\nWhat c does, is that it puts a lower bound on the true class activation and an upper bound on the incorrect class activation. The first term increases the activation of the true class, the second term cancels the first term once x_(y_i) >> c. The second term also decreases the other classes' activations if they are >> c and > x_(y_i)), but once they go below c they are no longer penalized. If we instead write the softmax as a hard max, it becomes more clear why this is the case.\n\nL_i = - x_(y_i) + max({x_i}+{c})\n\nUnlike the original cross-entropy loss that squashes other probabilities to 0 and the correct class to 1, the inhibited softmax would be happy if the other probabilities are less than some threshold t > 0 (which is a function of c) and the correct class probability > t.\n\nFrom this perspective, this approach is just another way to penalize the predictions similar to [1] in which the authors penalize the negative entropy of the prediction to encourage a more uniform prediction. That method also has no additional learnable parameters, works with only a single forward pass, requires no more out-of-distribution data, and has no input \"preprocessing.\" \n\nIn section 3.1 the authors:\n- (i) Propose to remove the bias in the final layer of the neural network to eliminate data-independency. Removing the bias from the final layer should have minimal effect on the ability of network in inducing data-independent bias -- the bias of the final layer can be directly absorbed in the bias of the layer immediate before that.\n- (ii) the second paragraph that discusses replacing the penultimate layer with a kernel function is not clear. \n- (iii) The assertion that x_i increases boundlessly is wrong. Adding a constant value to all of x_i's does not change the loss of the original cross-entropy term -- it cancels out in the probability; furthermore, the gradient of - log P_c converges to zero quickly when max(x_i) >> c. If we expand the term - log P_c  to - log(sum(exp(x_i))) + log(sum(exp(x_i))+exp(c)), and then simplify it to - max(x_i) + max({x_i}+{c}), we'll see that when x_i is sufficiently larger than c, the first term and the second term cancel out. This cancelling effect happens immediately with a hard max when max(x_i)>c, but for a softmax, it is more gradual, but quick nonetheless. Therefore the x_i's should not be increasing boundlessly.\n- (iv) Applying l2 regularization only on the weights of the final layer merely pushes the high-magnitude activation to the layer immediate before it. Unless we regularize all the layers, this should have minimal effect.\n\nIt seems, empirically, that the deeper the network, the more the overconfidence problem [2]. The overconfidence makes the detection of out-of-distribution samples in modern neural networks particularly more difficult. The first matter that concerns me in the evaluation of this work is that the networks under study, in addition to being non-standard, are much shallower than the common neural networks that are used for evaluation of out-of-distribution samples. For instance, (Wide)-ResNets are commonly used in previous work such as [3] to evaluate the method for OOD detection. The OOD problem in the shallow networks is a relatively easier problem than the OOD detection in deep networks. The second concern is that the set of chosen datasets for in-distribution and out-of-distribution are far from challenging: the datasets are so different that a linear classifier based on the local image statistics can separate them. Some more challenging datasets that appear in the OOD detection literature are CIFAR100, TinyImageNet, and SUN.\n\nQuality. The provided theoretical analysis are either incomplete or need a more careful review by the authors. See above.\n\nClarity. The paper is overall easy to follow and understand. However, there are a few instances that should be clarified. The paragraph that provides intuition on why inhibited softmax serves as an uncertainty estimator could be improved. The paragraph explaining the adjustment by using a kernel activation function is not clear. The explanation of the experiments (the three bullet points in section 4) can be improved by adding more details. Specifically, the measures should be explained properly.\n\nOriginality. The three main contributions are, (i) the mathematical explanation, which as discussed earlier is not clear (to me) and seems insufficient by itself. (ii) The additional adjustments, which as discussed earlier were not convincing, and (iii) the benchmarks comparing the proposed idea with some of the recent methods that I believe do not reflect a conclusive picture because of the two stated concerns. The idea of inhibited softmax itself comes from another publication (Saito et al. in the paper).\n\nI would be happy to change my rating if I have misunderstood parts of the paper or have made a mistake in my review.\n\nReferences.\n[1] G. Pereyra, G. Tucker, J. Chorowski, \u0141. Kaiser, and G. Hinton, \u201cRegularizing Neural Networks by Penalizing Confident Output Distributions,\u201d ICLR Work., 2017\n[2] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, \u201cOn Calibration of Modern Neural Networks,\u201d ICML, 2017.\n[3] S. Liang, Y. Li, and R. Srikant, \u201cEnhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,\u201d ICLR, 2018.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}