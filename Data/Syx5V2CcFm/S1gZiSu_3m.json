{"title": "Comparison over related work should be clarified. Measure of convergence rate should be justified", "review": "Non-convex optimization is a hot topic since many machine learning problems can be formulated as non-convex problems. In this paper, the authors propose a universal stage-wise algorithm for weakly convex optimization problems. The idea is to add a strongly convex regularizer centered at an iterate of previous stage to the objective function. This builds a convex function which can be optimized by any standard methods in the convex optimization setting. The authors developed convergence rates in expectation in terms of the gradient of envelope. Empirical results are also reported to show the effectiveness of the method.\n\nComments:\n\n(1) The weakly-convex concept considered in this paper is very similar to the bounded non-convexity considered in the paper (Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter) (not cited). In particular, the Natasha paper also developed a multi-stage algorithm for bounded non-convexity optimization problems by adding strongly-convex regularizers centered at iterates of previous stages. The authors should discuss more extensively the related work to clarify their novelty.\n\n(2) The convergence rate is measured by $\\nabla\\phi_\\gamma(x_\\tau)$. However, according to (3) , this only guarantees an upper bound on $\\text{dist}(0,\\partial\\phi_\\gamma(\\text{prox}_{\\gamma\\phi_\\gamma}(x_\\tau)))$. The output of the algorithm is $x_\\tau$ instead of $\\text{prox}_{\\gamma\\phi_\\gamma}(x_\\tau)$. Is it possible to derive an upper bound on $\\text{dist}(0,\\partial\\phi_\\gamma(x_\\tau))$?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}