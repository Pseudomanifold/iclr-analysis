{"title": "Novelty is limited", "review": "\nSummary: \nThe paper proposes to use Riemannian stochastic gradient algorithm for low-rank tensor train learning in deep networks. \n\nComments:\nThe paper is easy to follow. \n\nC1.\nThe novelty of the paper is rather limited, both in terms of the convergence analysis and exploiting the low-rank structure in tensor trains. It misses the important reference [1], where low-rank tensor trains have been already discussed. Section 3 is also not novel to the paper. Consequently, Sections 2 and 3 have to be toned down. \n\nSection 4 is interesting but is not properly written. There is no discussion on how the paper comes about those modifications. It seems that the paper blindly tries to apply the low-rank constraint to the works of Chung et al. (2014) and Luong et al. (2015).  \n\n[1] https://epubs.siam.org/doi/abs/10.1137/15M1010506\nSteinlechner, Michael. \"Riemannian optimization for high-dimensional tensor completion.\" SIAM Journal on Scientific Computing 38.5 (2016): S461-S484.\n\nC2.\nThe constraint tt_rank(W) \\leq r in (5) is not a manifold. The equality is needed for the constraint to be a manifold.\n\nC3.\nUse \\langle and \\rangle for inner products. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}