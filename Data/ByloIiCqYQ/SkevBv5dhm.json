{"title": "Paper that addresses a new application with Deep models: binary code for vulnerability detection. One key contribution is to create a dataset for the community actually built from an existing dataset for source code vulnerability detection. A model of variational-autoencoders maximizing a divergence between positive and negative distributions is also proposed - good results on the proposed datasets are reported.", "review": "The paper proposes a method to classify vulnerable and non-vulnerable binary codes where each data instance is a binary code corresponding to a sequence of machine instructions. The contributions include the creation of a new dataset for binary code vulnerability detection and the proposition of an architecture based on a supervised adaptation of variational auto-encoder, built upon the result of a sequential information,  \nand using a regularization term to better discriminate positive from negative data. An experimental evaluation on the data proposed is presented, including several baselines, the results show the good behavior of the method.\n\nPros:\n-Presentation of new application of representation learning models\n-Construction of a new dataset to the community for binary software vulnerability detection\n-The proposed model shows a good performance\nCons:\n-The presentation of the dataset is for me rather limited while it is a significant contribution for the authors, it seems to be an extension of an existing dataset for source code vulnerability detection.\n-From the last remark, it is unclear for me if the dataset is representative of binary code vulnerability problem\n-The proposed architecture is reasonable and maybe new, but I find it natural with respect to existing work in the literature.\n\nComments:\n\n-If providing a new dataset is a key contribution, the authors should spend more time to present the dataset. What makes it interesting/novel/challenging must be clarified. \nThis dataset seems actually built from the existing NDSS18 dataset for source code vulnerability detection. If I understood correctly, the authors have compiled (and sometimes corrected) the source to create binaries, then they use the labels in NDSS18 to label the binary codes obtained. \nThis a good start and can be useful for the community.\nHowever the notion of vulnerability is not defined and it is difficult for me to evaluate the interest of the dataset.\nI am not an expert in the field, but I am not that convinced that vulnerability for binary codes is necessary related to vulnerability that can be detected from source codes.\nIndeed, one can think that some vulnerability may appear in binary codes that cannot be detected from source codes: e.g. use of unstable libraries, problems with specific CPU architectures, problems du to different interpretation of standard.\n\nThe current version of dataset seems to be a data where one tries to find the vulnerability that can be detected from code. It would be interesting here to know if detecting the vulnerabilities are easier from source code or from binary code.\n\nIt could be good if the authors could discuss more this point.\n\n-The architecture proposed by the authors seems to use a sequential model (RNN or other) as indicated in Fig.2, the authors should precise this point.\nThe architecture is general enough to work on other problems/tasks - which is good - but the authors focus on the binary vulnerability code dataset in the experiments.\n\nIf the authors think that their contribution is to propose a general method for sequence classification, it could be good to apply it on other datasets.\nOtherwise, something maybe more specific to the task would be useful.\nIn particular, there is no clear discussion to justify that variational autoencoders are better models for the task selected, it coud be good to argue more about it.\n\nThat being said, having non fixed priors and trying to maximize the divergence between positive and negative distributions are good ideas, but finally rather natural.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}