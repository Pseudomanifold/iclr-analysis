{"title": "A generative model for images", "review": "After the rebuttal: I appreciate the authors' effort to revise the paper. The revision made clear that the data produced by the proposed generative model is not linearly separable in general while the theory (Theorem 2) still holds.\n\nI am keeping my original evaluation as there still seems to be a lack of stronger experimental evidence. The fact that the classification algorithm motivated by the generative model can do as well as a similar-sized ConvNet does not quite support that the generative model itself is good -- getting a good classifier is still an easier task than getting a good generative model. \n\n=====================\n\nThis paper proposes a new generative model for natural images. Based on the architecture of the generative model, a \u201clayer-wise clustering\u201d algorithm for image classification is proposed and theoretically shown to converge to an optimal classifier. Experimentally, the algorithm is shown to have similar performances as a baseline CNN on CIFAR-10.\n\nThe main novelty of this paper is the proposed hierarchical generative model and the associated algorithm. One interesting feature is that the network obtained by this algorithm is entirely linear except for the ReLU-pool part. However, the ReLU-pool does not serve as a typical nonlinearity / pooling I believe; rather it sounds to me like a specially tailored step for the theoretical results, which under the \u201cpatch orthonormality\u201d assumption is guaranteed to recover the previous layer. Therefore, it surprises me a little bit that the algorithm actually works reasonably well on CIFAR-10. However, as the baseline it compares with is still below \"typical\", I do want to see if this algorithm can be scaled up to match the performance of more complicated (at least pre-ResNet) models such as VGG.\n\nThe theoretical result looks appealing, but I feel like the magic more or less comes from the strong assumptions. In particular, in expectation the output image is just a *linear* operator on the initial (m_0 x m_0 x C_0) one-hot semantic variable. Also, the patch orthonormality assumption implies that intermediate semantics can be perfectly recovered by the (clustering + conv with centroids + ReLU-pool) step, as we are just recovering a partition of a group of orthonormal vectors.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}