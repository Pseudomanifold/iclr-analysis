{"title": "Novel solution to an important problem, but needs further details and experimentation", "review": "This paper attempts to the solve  data-set coverage issue common with Inverse reinforcement learning based approaches - by introducing a meta-learning framework trained on a smaller number of basic tasks. The primary insight here is that there exists a smaller set of unique tasks, the knowledge from which is transferable to new tasks and using these to learn an initial parametrized reward function improves the coverage for IRL. With experiments on the SpriteWorld synthetic data-set, the authors confirm this hypothesis and demonstrate performance benefits - showcasing better correlation with far fewer  number of demonstrations.\n\nPros:\n+ The solution proposed here in novel - combining meta-learning on tasks to alleviate a key problem with IRL based approaches.\nThe fact that this is motivated by the human-process of learning, which successfully leverages tranferability of knowledges across a group of basic tasks for any new (unseen) tasks, makes it quite interesting.\n+ Unburdens the needs for extensive datasets for IRL based approach to be effective\n+ To a large extent, circumvents the need of having to manually engineered features for learning IRL reward functions\n\nCons:\n- Although the current formulation is novel, there is a close resemblance to other similar approaches  - mainly, imitation learning. It would be good if the authors could contrast the differences between the proposed approach and approach based on imitation learning (with similar modifications). Imitation learning is only briefly mentioned in the related work (section-2), it would be helpful to elaborate on this. For instance, with Alg-1 other than the specific metric used in #3 (MaxEntIRLGrad), the rest seems close similar to what would be done with imitation learning?\n- One of main contributions is avoiding the need for hand-crafted features for the IRL reward function. However, even with the current approach, the sampling of the meta-learning training and testing tasks seem to be quite critical to the performance of the overall solution and It seems like this would require some degree of hand-tuning/picking. Can the authors comment on this and the sensitivity of the results to section of meta-learning tasks and rapid adaption?\n- The results are limited, with experiments using only the synthetic (seemingly quite simple)  SpriteWorld data-set. Given the stated objective of this work to extend IRL to beyond simple  cases, one would expect more results and with larger problems/data-sets.\n\t- Furthermore, given that this work primarily attempts to improve performance with using meta-learned reward function instead of default initialization - it might make sense to also compare with method such as Finn 2017, Ravi & Larochelle 2016.\n\nMinor questions/issues:\n> section1: Images are referred to as high dimensional observation spaces, can this be further clarified?\n>  section3: it is not immediately obvious how to arrive at eqn.2. Perhaps additional description would help.\n> section4.1 (MandRIL) meta-training: What is the impact/sensitivity of computing the state visitation distribution with either using the average of expert demos  or the true reward? In the reported experiments, what is used and what is the impact on results, if any ?\n> section4.2: provides an interesting insight with the concept of locality of the prior and establishes the connection with Bayesian approaches.\n> With the results, it seems like that other approaches continue to improve on performance with increasing number of demonstrations (the far right part of the Fig.4, 5) whereas the proposed approach seems to stagnate - has this been experimented further ? does this have implications on the capacity of meta-learning ?\n> Overall, given that there are several knobs in the current algorithm, a comprehensive sensitivity study on the relative impact would help provide a more complete picutre", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}