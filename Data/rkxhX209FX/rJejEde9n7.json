{"title": "need further improvement", "review": "Summary: This paper proposes an integration of active learning for multi-task learning with policy search. This integration is built on an existing framework, EPOpt, which each time samples a set of models and a set of trajectories for each model. Only trajectories with the bottom \\epsilon percentile returns will be used to update the multi-task policy. This paper proposes a way to improve the sample-efficiency so that fewer trajectories will be sampled and fewer trajectories will be loss. \n\nIn general, the paper presentation is easy to follow. The idea is well motivated of why an active learning integration is needed. The related work is a bit too narrow, e.g. work [1] on the same approach like EPOpt or meta-learning (for model adaptation) [2] (and others more on this topic)\n\n[1] T. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-Ensemble Trust-Region Policy Opti\nmization. In ICLR, 2018.\n\n[2] C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In ICML, 2017.\n\nIn overall, I have major concerns regarding to the proposed framework.\n- Active learning is a method that is in general known to be an optimal trade-off between exploration vs. exploitation in finding a global optimal solution. That means, the proposed use of linear stochastic bandits is trying to find an optimal arm \\theta^* (the worst trajectory) that gives the highest reward (the lowest return). In my opinion, integrating this idea naively into EPOpt to sample a set of trajectories would only aim to find the worst trajectory among all trajectories from all models. This is clearly not enough to say \"finding ALL the WORSE regions among trajectory space\" to improve the policy. Therefore, a new way of integration or a new objective should be used in order to make a principled framework. \n\n- The statement over sample-efficiency gain vs. EPOpt in Section 4 is too loose which is not based on any detailed analysis or further theoretical results.\n\n- The experiment results are not well presented: there is no results for EPOpt in Fig. 1; \n\n\nMinor comments:\n\n- Algorithm 1: argument of GetTrajectory (in LEARN) should be \\theta_i, instead of \\pi_\\theta_i?.\n\n\nIn conclusion, the proposed framework is not yet principled. Experiment results are too preliminary and not well presented. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}