{"title": "Possibly useful malware detector but unclear paper and uncharacterized black box labels in dataset", "review": "This paper attempts to train a predictor of whether software is malware. Previous studies have emulated potential malware for a fixed number of executed instructions, which risks both false negatives (haven\u2019t yet reached the dangerous payload) and false positives (malware signal may be lost amidst too many other operations). This paper proposes using deep reinforcement learning over a limited action space: continue executing a program or halt, combined with an \u201cevent classifier\u201d which predicts whether individual parts of the program consist of malware. The inputs at each time step are one of 114 high level \u201cevents\u201d which correspond to related API invocations (e.g. multiple functions for creating a file). One limitation seems to be that their dataset is limited only to events considered by a \"production malware engine\", so their evaluation is limited only to the benefit of early stopping (rather than continuing longer than the baseline malware engine). They evaluate a variety of recurrent neural networks for classifying malware and show that all significantly underperform the \u201cproduction antimalware engine\u201d. Integrating the event classifier within an adaptive execution control, trained by DQN, improves significantly over the RNN methods. \n\nIt might be my lack of familiarity with the domain but I found this paper very confusing. The labeling procedure (the \"production malware engine\u201d) was left entirely unspecified, making it hard to understand whether it\u2019s an appropriate ground-truth and also whether the DRL model\u2019s performance is usable for real-world malware detection. \n\nAlso, the baseline models used an already fairly complicated architecture (Figure 3) and it would have been useful to see the performance of simple heuristics and simpler models. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}