{"title": "Good discussion, improved comparisons needed", "review": "This paper suggests the use of learned transformation networks, embedded within introspective networks to improve classification performance with synthesized examples.\n\nThe authors cite a number of related works, and give a good introduction to both introspective learning, as well as the particular usage for large variation resistance. This discussion forms the majority of the paper, and while the explanation seems clear it would be nice to have a stronger dedicated section on exact relations to both GAN and potentially VAE mathematically. Any kind of discussion which could tie the thorough derivation to some of the contemporaries in generative modeling would help the accessibility of the paper.\n\nMy primary concerns are from the experimental sections of the paper. The setup of the problem seems such that any strong baseline could be directly tested, since the final CNN is ultimately trained in a two stage setup on the aggregated dataset (as per subsection Baselines). Here it is also worth mentioning DAgger https://ri.cmu.edu/pub_files/2010/5/Ross-AIStats10-paper.pdf / https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf which is used in another context but in a similar way for improving imitation and online learning. \n\nMany of the baseline methods for table 1 seem far from what I would consider \"strong\" baselines. Given that the core proposal of the paper is improving classifiers, the importance of having high quality baselines cannot be overstated. Particularly, baseline CNN numbers for MNIST, SVHN, and CIFAR-10 are far from what has been seen in simple papers such as Wide ResNet https://arxiv.org/abs/1605.07146, ResNet https://arxiv.org/abs/1512.03385, Universum Perscription (which bears some resemblence to this work in high level concept) https://arxiv.org/abs/1511.03719, or even older work such as Maxout Networks http://proceedings.mlr.press/v28/goodfellow13.pdf . Particularly, these papers show examples of simple CNNs which outscore the best values reported in this table, on the same datasets. Running the same setup but with these improved classifiers as baselines would make a much stronger support for the core hypothesis that ITN can be used to improve strong classifiers.\n\nTable 2 seems to me an improper comparison. Methods such as zero-shot or meta-learning type approaches seem much more appropriate for testing cross-generalization improvement. In some sense, none of the tested methods besides ITN should even be able to cross-generalize well, so the fact that ITN does better here is not surprising to me. While this is a benefit of ITN as stated, seeing a comparison to methods deisgned for cross-generalization as well would make the results of Table 2 much stronger.\n\nTable 3 also seems have improper comparisons, in that there are a large number of works using semi-supervised generative models (Improved Techniques for Training GANS, which is already cited, SS-VAE https://arxiv.org/abs/1406.5298, Temporal Ensembling https://arxiv.org/abs/1610.02242, VAT https://ieeexplore.ieee.org/abstract/document/8417973/, Ladder Networks http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks, Auxiliary Deep Generative Models https://arxiv.org/abs/1602.05473, Manifold Tangent Classifier https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier) for improved classification in low data domains. Adopting the same settings and comparing directly to these methods would greatly strengthen this section as well, as simple classifiers (as shown in many of these previous papers) are not generally great baselines for semi-supervised modeling.\n\nIn addition, there should be a direct case where becoming robust to these kinds of transformations fails. For example, if my classification task is the rotation/position of a repositioned MNIST digit, becoming robust to these types of transformations may be harmful. An experiment or discussion about when robustness to large data variation might be harmful would be a good inclusion as well. As a more general comment, this method seems applicable outside of image domains, and it would be interesting to see it applied in other settings though it is likely outside the scope of this particular paper.\n\nFormatting of the paper (specifically spacing between sections and subsections) seems a bit off in general. If the authors are applying /vspace tricks to shrink spaces in the format, I would recommend taking a closer look at how and where, to make spacing more consistent over the whole document. Comparing to past ICLR papers (best papers, or high rated from past conferences) to see how they approach formatting could improve the visual appeal of this paper.\n\nOverall, this paper has a lot in its favor. The experimental section is thorough, if not as strong as I would like. The derivation of the method and motivation is clear, and there are a lot of avenues explored. However, as it currently stands the experimental sections should be stronger to really prove out the core claim \"Our method, ITN strengthens the classifiers by generating unseen variations with various learned transformations.\" compared to other methods using generative and semi-supervised methods in a similar vein. In addition, the clarity and approachability of the paper could be improved by drawing a relation to parallel related work such as GAN or VAE in more detail.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}