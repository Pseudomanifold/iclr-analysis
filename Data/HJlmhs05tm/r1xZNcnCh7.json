{"title": "An interesting combination of the recently developed techniques for a better algorithm", "review": "\nIn this paper, the authors extend the framework proposed by Kim&Bengio 2016 and Dai et.al. 2017, which introduce an extra step to fit a generator to approximate the current model for estimating the deep energy model. Specifically, the generator is fitted by reverse KL divergence. To bypass the difficulty in handling the entropy term, the authors exploit the Deep INFOMAX formulation, which introduces one more discriminator. Finally, to obtain better samples, the authors inject the Metropolis-adjusted Langevin algorithm within the learned generator to generate samples in latent space. They demonstrate the better performances of the proposed algorithms in both synthetic and real-world datasets, and apply the learned model for anomaly detection task.\n\nThe paper is well-written and does a quite good job in combining several existing algorithms to obtain the ultimate algorithm. The algorithm achieves quite good empirical performances. However, the major problem of this paper is the novelty. The algorithm is basically an extension of the Kim&Bengio 2016 and Dai et.al. 2017, with other existing learning technique. Maybe the only novel part is combining the MCMC with the learned generator for generating samples. However, the benefits of such combination is not well justified empirically. Based the figure 4, it seems the MCMC does not provide better samples, comparing to directly generate samples from G_z. It will be better if the authors can justify the motivation of using MCMC step. \n\nSecondly, it is reasonable that the authors introduce the gradient norm as the regularization to the objective for training stability. However, it will be better if the effect of the regularization for the energy model estimation can be discussed. \n\nMinor:\nThe loss function for potential in Eq(3) is incorrect and inconsistent with the Algorithm 1. I think the formulation in the Algorithm box is correct.  \n\nIn sum, I personally like the paper as a nice combination of recently developed techniques to improve the algorithm for solving the remaining problem in statistics. The paper can be better if the above mentioned issues can be addressed\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}