{"title": "Restricted/simplified version of network in network by Lin et. al. without clear benefits", "review": "Paper summary:\n\nThis paper proposes a method to scale the activations of a layer of neurons in an ANN depending on the inputs to that layer. The scaling factor, called modulation, is computed using a separate weight matrix and activation function. It is multiplied with each neuron's activation before applying its non-linearity. The weight matrix of the modulator is learned alongside the other weights of the network by backpropagation. The authors evaluate this modulated neural unit in convolutional neural networks, densely connected CNNs and recurrent networks consisting of LSTM units. Reported improvements above the baselines are between 1% - 3%.\n\nPro:\n\n+ With some minor exceptions the paper is clearly written and comprehensible.\n+ Experiments seem to have been performed with due diligence.\n+ The proposed modulator is easy to implement and applicable to (almost) all network architectures.\n\nContra:\n\n- Lin et. al. (2014) proposed a network in network architecture. In this architecture the output of each neural unit is computed using a small neural network contained in it and thus arbitrary, input-dependent activation functions can be realized and learned by each neuron. The proposed neural modulation mechanism in the paper at hand is in fact a more restricted version of the network-in-network model and the authors should discuss the relationship of their proposal to this prior work.\n\n- When comparing the test accuracy of CNNs in Fig. 4 the result is questionable. If training of the vanilla CNN was stopped at its best validation loss (early stopping), the difference in accuracies would have been marginal. Also the choice of hyper-parameters may significantly affect the outcome of the comparison experiments. More experiments would be necessary to prove the advantage of this model over a wide range of hyper-parameters.\n\nMinor points:\n\n- It is unclear whether the modulator weights are shared along the depth of a CNN layer, i.e. between feature maps.\n\n- Page 9: \"Our modification enables a network to use previous activity to determine its current sensitivity to input [...]\" => A vanilla LSTM is already capable of doing that using its input gate.\n\n- Page 9: \"[...] the ability to adjust the slope of an Activation Function has an immediate benefit in making the back-propagation gradient dynamic.\" => In fact ReLUs do not suffer from the vanishing gradient problem. Furthermore DenseNets already provide a short-path for the gradient flow by introducing skip connections.\n\n- The discussion at the end adds little value and rather seems to be a motivation of the model than a discussion of the results.\n\nRating:\n\nMy main concern is that the proposed modulator is a version of the network in network model restricted to providing a scaling factor. Although the authors motivate this model biologically, I do not see sufficient empirical evidence to believe that it is advantageous over the full network in network model by Lin et. al. I would recommend to add a direct comparison to that model to a future version of this paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}