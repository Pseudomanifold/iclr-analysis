{"title": "modulating scalar applied per-neuron", "review": "The paper introduces a new twist to the activation of a particular neuron. They use a modulator which looks at the input and performs a matrix multiplication to produce a vector. That vector is then used to scale the original input before passing it through an activation function. Since this modulating scalar can look across neurons to apply a per-neuron scalar, it overcomes the problem that otherwise neurons cannot incorporate their relative activation within a layer. They apply this new addition to several different kinds of neural network architectures and several different applications and show that it can achieve better performance than some models with more parameters.\n\n\nStrengths:\n- This is a simple, easy-to-implement idea that could easily be incorporated into existing models and frameworks.\n- As the authors state, adding more width to a vanilla layer stops increasing performance at a certain point. Adding more complex connections to a given layer, like this, is a good way forward to increase capacity of layers.\n- They achieve better performance than existing baselines in a wide variety of applications.\n- The reasons this should perform better are intuitive and the introduction is well written.\n\nWeaknesses:\n- After identifying the problem with just summing inputs to a neuron, they evaluate the modulator value by just summing inputs in a layer. So while doing it twice computes a more complicated function, it is still a fundamentally simple computation.\n- It is not clear from reading this whether the modulator weights are tied to the normal layer weights or not. The modulator nets have more parameters than their counterparts, so they would have to be separate, I imagine.\n- The authors repeatedly emphasize that this is incorporating \"run-time\" information into the activation. This is true only in the sense that feedforward nets compute their output from their input, by definition at run-time. This information is no different from the tradition input to a network in any other regard, though.\n- The p-values in the experiment section add no value to the conclusions drawn there and are not convincing.\n\nSuggested Revisions:\n- In the abstract: \"A biological neuron change[s]\"\n- The conclusion is too long and adds little to the paper\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}