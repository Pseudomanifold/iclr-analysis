{"title": "Interesting work but not good enough", "review": "The paper addresses the problem of continual learning from a sequence of supervised tasks. The main contribution of the paper are the following. The work:\n* puts the problem in uniform general framework in which many of the state-of-the-art methods fit\n* identifies some drawbacks of some the state of the art methods (namely EWC- Kirkpatrick et al., 2017 and GEM-Lopez-Paz et al., 2017) . \n* proposes two versions of an approach to address these drawbacks \nThe main identified problem in EWC and GEM is that both methods result in a zero penalty when both the previous and current models misclassify a sample, even if they make different mistakes.\nTo solve this problem, the authors propose to keep a memory of randomly sampled data from the previous task, and distil the knowledge from the optima of the previous tasks to the current one using a KL penalty. The first version considers computing this penalty from the whole sequence of tasks, while the second randomly selects one task at each iteration and uses it to compute the penalty, paying some overall accuracy for plasticity.\n\nWhile the paper is clear and well written, I have some concerns about it's quality, originality and significance.\n\nOriginality: The work seems to me to be very related to LwF. The main difference is that while LwF uses only the new data for the KL penalty, this paper keeps a memory of previously seen data to compute this loss. \n\nSignificance: While the way the authors approached the problem seems well structured and motivated, and is based on a sound observation, the authors limited themselves to experiments where the task data have similar structure. I am not sure how significant the improvement of the method would be in the more challenging and realistic setting where the data comes from different domains.  I am more specifically skeptical about the stochastic version of the algorithm in that case.\n\nQuality: \n* The proposed algorithm stores some data from previous tasks along with the outputs of the corresponding optimal model. While knowledge distillation as proposed would result in a non zero penalty when the new model makes different prediction, it still doesn't take advantage of the new information to probably correct the previous models prediction when it is wrong. Why not keeping the ground truth labels instead? This won't increase the memory requirement, and may give better results. It would be interesting to compare against such a method. \n* I think selecting the samples to keep in memory randomly could to be suboptimal.  Other selection methods can be considered. A previous work:  iCaRL: Incremental Classifier and Representation Learning, Rebuffi et al. 2017, gives way to select representative samples. It would be interesting to see the effect of such a selection on the results. \n\nOverall, while the paper doesn't present any significant flaw, it doesn't add much to the continual learning literature either, which explains my rating.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}