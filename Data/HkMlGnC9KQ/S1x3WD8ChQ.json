{"title": "Well written with interesting findings, but limited novelty", "review": "Regularizing RKHS norm is a classic way to prevent overfitting. The authors\nnote the connections between RKHS norm and several common regularization and\nrobustness enhancement techniques, including gradient penalty, robust\noptimization via PGD and spectral norm normalization. They can be seen as upper\nor lower bounds of the RKHS norm.\n\nThere are some interesting findings in the experiments. For example, for\nimproving generalization, using the gradient penalty based method seems to work\nbest.  For improving robustness, adversarial training with PGD has the best\nresults (which matches the conclusions by Madry et al.); but as shown in Figure\n2, because adversarial training only decreases a lower bound of RKHS norm, it\ndoes not necessarily decrease the upper bound (the product of spectral norms).\nThis can be shown as a weakness of adversarial training if the authors explore\nfurther and deeper in this direction.\n\nOverall, this paper has many interesting results, but its contribution is\nlimited because:\n\n1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has\nbeen well studied by previous literature. This paper simply applies these\nresults to deep neural networks, by treating the neural network as a big\nblack-box function f(x).  Many of the results have been already presented in\nprevious works like Bietti & Mairal (2018).\n\n2. In experiments, the authors explored many existing methods on improving\ngeneralization and robustness. However all these methods are known and not new.\nIdeally, the authors can go further and propose a new regularization method\nbased on the connection between neural networks and RKHS, and conduct\nexperiments to show its effectiveness.\n\nThe paper is overall well written, and the introductions to RKHS and each\nregularization techniques are very clear. The provided experiments also include\nsome interesting findings. My major concern is the lack of novel contributions\nin this paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}