{"title": "Interesting approach, but explanation can be clearer, and scope is limited.", "review": "Summary\n\nThe authors aim to do continual learning to solve dependent tasks using \"single-stage end-to-end learning\". The resulting \"Unicorn\" agent trains on all tasks simultaneously. The idea is to use multi-task \"off-policy learning\", which uses (old) trajectories (experience) from task A to help learning on a related task B. Authors further distinguish between goals (inputs to Q) and tasks (different reward functions). A goal might a color/shape of an object to pick up.\n\nThe core model is a UVFA that learns a goal-conditioned Q-function Q(s,a,g). \n\nSome technical aspects: \n- use n-step returns.\n- when training Q on goal g_i, authors also trajectories that were generated using Q conditioned on another goal(s) g_j. They then truncate the returns for task i when an action taken is not optimal under Q(s,a,g_j) conditioned on goal j. The intuition (seems) to be that this \n- authors do not use experience replay or a target Q-function, since the parallelized implementation is reported to be stable enough.\n- unicorn sees all train task reward functions during training (but not hold out task rewards).\n- unicorn is tested on several 3d maze environments with key-lock etc semantics. The tasks / goals seem simple, and the dependency is defined by changing colors / shapes of objects to be picked up. Authors argue unicorn has to learn to relate task rewards to these goal features.\n- unicorn is compared against baselines that 1) do single-task learning (expert) 2) learn on a sum of task rewards (glutton), 3) uniformly random baseline. \n- authors show that 1) unicorn performs better on train tasks 2) performs better on hold-out tasks. Also, authors show results for zero-shot transfer learning, with adding abstract tasks (extra reward for picking up any object) improving performance, \n\nPro\n- Simple approach (e.g., no experience replay etc), and uses only a limited set of techniques (e.g., reward truncation). \n- Reward performance suggests the model has more properly related goal features to different payoffs.\n- Analysis of qualitative behavior is nice.\n\nCon\n- The writing is a bit dense in places, e.g., the discussion of baselines is a bit hard to read.\n- Description of algorithm is wrapped in long text, a clear algorithm box would make the approach much clearer.\n- Not clear what kind of hyperparameters are introduced / used / tuned for Unicorn. \n- Authors say \"deep dependency\", but this seems to just refer to different colors / shapes between objects in the env used in the paper. How is \"dependency\" between goals and tasks defined in general? \n- The experimental setting seems a bit limited, authors only show results on a single domain, and do not offer rigorous definitions. This makes the scope of the paper rather limited.  \n\nReproducibility: \n- It's not clear what the variance in the baseline performance is (variance only shown for unicorn).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}