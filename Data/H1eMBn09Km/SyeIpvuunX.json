{"title": "Application paper of GAN based on known techniques", "review": "The paper produces a heat-map of ride-share requests in four cities in the USA. For each city 'block' they produce a time-sequence of 2016 images representing a week-long run from combining each 5-minute interval. This is used with a GAN to produce new data. The techniques applied, although not commonly used in the context of ride sharing / hailing, have been used extensively in other literature.\n\nSome major points on the paper:\n1) A GAN approach is normally used to generate more data when enough real data is not obtainable. However, here you only use one week of data from a much larger set. Surely, it would be better to make use of all the weeks available?\n\n2) It is not clear how the heat-maps once produced could be used in the future. There is a hint in the results section about how they can be converted back to ride requests, but this is not clearly defined.\n\n3) There are a number of cases where you state that some approach has been found to be better. However, no evidence is presented for how you determined this to be true.\n\n4) The conversion of data to heat-maps has been used extensively in prior research. Although I'm not directly aware of the use in machine learning I am aware of the use in transport - \"Interactive, graphical processing unit- based evaluation of evacuation scenarios at the state scale\". The novelty here seems to be the application to this specific problem.\n\nMore specific points:\n- \"Our real ride request data sets consist of all the ride requests for an entire week for the four cities.\" - it's not clear - are all four cities used to train one model?\n\n- \"Hence the week-long data should be quite representative.\" - This fails to take into account such things as national holidays or other major events such as sports. Did your chosen week contain one of these?\n\n- \"Hence we believe the ride request data sets also reflect the overall urban mobility patterns for these cities.\" - This is a huge assumption, which would seem to need evidence to back it up.\n\n- \"and lump together all the ride requests within each interval.\" - Presumably you mean that all time values are to the granularity of 5 minutes? \n\n- \"We arbitrarily sized each block to represent an image of 24\u000224 pixels\" - this seems particularly small.\n\n- \"Each image of that block is labeled with a time interval (for our experiments, the hour in a day).\" - Can the variability within an hour not make this more difficult? \n\n- \"We find that small networks are appropriate for the training data\" - evidence to support this.\n\n- \"This network is pre-trained on the training data\" - which training data are you referring to?\n\n- \"This is found to increase the efficiency of the training process\" - evidence?\n\n- \"In this work we set the block size for each of the four cities to be\n1200 \u0002 1200 meters\" - how was this value arrived at?\n\n- You state that GPUs were no more efficient, it would be good to see more analysis of this.\n\n- \"To help enhancing the scalability\" -> \"To help enhance the scalability\"\n\n- \"and other useful functions\" - such as?\n\n- Figure 4 would probably work better as a speedup graph.\n\n- \"Running times for sampling ride requests from the trained models and stitching the images of all the blocks together are significantly less than the training times, and are not included in these results.\" - at least some figures to give an idea of scale should be provided.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}