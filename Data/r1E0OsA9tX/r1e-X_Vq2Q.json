{"title": "The paper presents a novel approach to addressing the problem of overfitting when training large networks on small data sets. The main idea is to use a dependent prior on the network weights (instead of commonly used independent normal priors) in order to allow the weights \"learning from the experience of others\".", "review": "Starting from a simple neural network with only one hidden layer and a single output, the basic idea of approximate empirical Bayes (AEB) method is proposed, defining a matrix-variate normal prior distribution with a Kronecker product structure, so as to capture correlations between the row and column vectors of the weight matrix. Then, a block coordinate descent algorithm for solving the optimization problem is proposed. It consists of alternating three steps to obtain the optimal solutions of model parameters, row and column covariance matrices.\n\nThe current method is investigated and tested on three data sets for both classification (MNIST & CIFAR10) and regression (SARCOS) tasks. Encouraging experimental results demonstrate that the correlation learning in the weight matrix significantly improves performance when the training set size is relatively small. It is also shown that the proposed AEB method does not seem sensitive to the size of mini-batches and its combination with other generalization methods can lead to better results in some cases.\n\n Strengths:\n\n This paper is mostly well written and overall is easy to follow. It clearly reveals that correlation in the weight matrix plays a crucial role in better generalizing on small training sets.\n\n Minor comments:\n\n * The authors state that it is straightforward to extend the proposed method to more sophisticated models with various structures, such as CNN. Perhaps a bit more detail should be given in the main text.\n * Fig. 6 on page 12 is not explicitly mentioned in the main text. It seems a bit confusing.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}