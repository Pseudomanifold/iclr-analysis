{"title": "Generating Images from Sounds Using Multimodal Features and GANs", "review": "Summary:\n\nThis paper addresses the problem of generating images from sound. The general idea is to use conditional GANs. In particular, two stacked conditional autoencoder GANs, where the autoencoders have a U-Net architecture. First, sound features are mapped into multimodal features that contain image feature/class information. Such multimodal features condition the generation of image features with an initial GAN, and the image features condition the generation of the output image with a second GAN. Although the problem itself is rather difficult, the solution is almost entirely based on previous work. The most novel part of the paper is the learning of the multimodal features. The final results are not very compelling, the literature review is very limited. There is a very high-level description of the approach with very few details, which leave the reader with a lot of unanswered questions. There is no attempt to compare with previous work, the architecture is not studied in depth with an ablation study, or compared with more interesting baselines, other than verifying that images can be generated from features (something not very surprising, given StackGUN!). Probably the more important verification is that the multimodal feature can embody some class information. Overall it seems to be a limited contribution.\n\nComments on quality, clarity, originality and significance:\n\nThis paper provides a high-level description for an approach to a problem that is relatively difficult to address. The paper is not very well motivated, and therefore lacks clarity and leaves the reader with a lot of unanswered questions. The literature review is limited as it is the set of results and comparison with previous works.\n\nThe paper addresses an important problem; however, I feel the work is not very significant because it does not reveal new techniques, nor produces compelling results, nor performs a deep analysis.\n\nI think the problem is interesting, but a deeper analysis is needed to lift this contribution up to a significant one.\n\nBelow is a summary of some of the additional questions gathered while reading the paper:\n\nWhy there is the need for the multimodal features at all? Why can\u2019t the sound be converted into class labels and then StackGAN can generate images?\n\nWhy do you need a feature-to-feature GAN? Why not generating images directly from the generated multimodal features? Motivations are not provided clearly.\n\nUnclear architecture from Figure 1. The Feature-to-Feature GAN and the Feature-to-Image GAN have the same architecture? What does the encoder and decoder do? How are they organized? \n\nLooks like every piece is trained alone, no end-to-end learning, right? Please clarify that point.\n\nNo attempt to compare with other approaches has been made. Also, no effort to formulate a baseline model. What would happen if one were to use solely the features generated by SoundNet?\n\nWould you be able to compare your multimodal features with those generated by Ngiam et al. (2011), for instance?\n\nSection 3 refers to a 90/10 training/evaluation split but then it is unclear in what experiments that exact split is used.\n\nNo description on hyperparameters.\n\nNo complexity, no architecture details, (also no equations that could provide more details).\n\nIt should be clarified what it means one-to-one conversion. It is brought up in several points in the paper, but it is never clear what it means and therefore how it relates to what the Author intends to stress. \n\nIt is unclear why by performing first a feature-to-feature mapping and later a feature-to-image mapping the one-to-one conversion problem should be addressed. In StackGAN the problem addressed is the resolution increase. The problem addressed by this paper is unclear.\n\nUnclear what is the \u201censemble effect\u201d, and what is the motivation for upsampling a feature vector in two dimensions.\n\nWhat image features were used to generate the images in Figure 3? Which architecture was used and how was it trained? Where these the same multimodal features used in the full architecture?\n\nThe paragraph motivating the need for multimodal features is unclear.\n\nHow are the three type of losses weighted for learning the multimodal layers? No discussion provided on that.\n\nUnclear why a multimodal vector should be upsampled in 2 dimensions. \n\nThe training procedure and loss for training the feature-to-feature conditional GAN is not explained.\n\nDespite the difficulty of the problem, the generated images do not look compelling.\n\n16 references do not seem enough by todays\u2019 high-quality standard conferences.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}