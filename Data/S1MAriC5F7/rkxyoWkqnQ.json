{"title": "well written paper but with only little novelty and missing baseline comparison", "review": "The paper describes a simple, asynchronous way to parallelize successive halving. In a nutshell, this method, dubbed ASHA, promotes a hyperparameter configuration to the next rung of successive halving when ever possible, instead of waiting that all configurations of the current rung have finished. ASHA can easily be combined with Hyperband which iteratively runs different brackets of successive halving. The empirical evaluation shows that the proposed method outperforms in the most cases other parallel methods such as Vizier and population based training.\n\n\nOverall, the paper is well written and addresses an interesting and important research problem.\nHowever, the paper contains only little novelty and proposes a fairly straight-forward way to parallelize successive halving.\nEven though it shows better performance to other approaches in the non-sequential setting, its performance seems to decay if the search space becomes more difficult (e.g CNN benchmark 1 and 2 in Section 4.2), which might be due to its increasing number of mispromoted configurations. \n\nBesides that the following points need to be clarified:\n\n1) I am bit worried that asynchronous Hyperband performs consistently worse than asynchronous successive halving (ASHA). Why do we need Hyperband at the first place if a fixed bracket for ASHA seems to work better?\n   It would be interesting to see, how the performance of ASHA changes if we alter its input parameters.\n\n2) Why is PBT not included for the large-scale experiments in Section 4.3 given that it is a fairly good baseline for the more difficult search space described in Section 4.2?\n\n3) Even though I follow the authors' argument that the parallel version of Hyperband described in the original paper is slower than ASHA / asynchronous Hyperband, it doesn't promoted suboptimal configurations and hence might achieve a better performance at the end. Why is it not included as a baseline?\n\n4) I am also missing a reference and comparison to the work by Falkner et al., that introduced a different way to parallelize Hyperband which can make use of all workers at any time. \n\n\nFalkner, Stefan and Klein, Aaron and Hutter, Frank\nBOHB: Robust and Efficient Hyperparameter Optimization at Scale\nProceedings of the 35th International Conference on Machine Learning (ICML 2018)", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}