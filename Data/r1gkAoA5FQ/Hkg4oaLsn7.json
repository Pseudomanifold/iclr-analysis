{"title": "Missing relevant comparisons, evaluations, and references", "review": "This paper addresses long-text generation, with a specific task of being given a prefix of a review and needing to add the next five sentences coherently.  The paper proposes adding two discriminators, one trained to maximize a cosine similarity between source sentences and target sentences (D_{coherence}) and one trained to maximize a cosine similarity between two consecutive sentences.  On some automatic metrics like BLEU and perplexity, an MLE model with these discriminators performs a little bit better than without.\n\nThis paper does not include any manual evaluation, which is critical for evaluating the quality of generated output, especially for evaluating coherence and cohesion.  This paper uses the task setup and dataset from \"Learning to Write with Cooperative Discriminators\", Holtzman et al., ACL 2018.  That paper also includes many specified aspects to improve the coherence (from the abstract of that paper \"Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.\").  But this paper:\n--Does not compare against the method described in Holtzman et al., or any other prior work\n--Does not include any human evaluations, even though they were the main measure of evaluation in prior work.\n\nThis paper states that \"To the best of our knowledge, this paper is the first attempt to explicitly capture cross-sentence linguistic properties, i.e., coherence and cohesion, for long text generation.\"  There is much past work in the NLP community on these.  For example, see:\n \"Modeling local coherence: An entity-based approach\" by Barzilay and Lapata, 2005 (which has 500+ citations). \nIt has been widely studied in the area of summarization, for example, \n\"Using Cohesion and Coherence Models for Text Summarization\", Mani et al., AAAI 1998, and follow-up work.\nAnd in more recent work, the \"Learning to Write\" paper that the dataset and task follow from addresses several linguistically informed cross-sentence issues like repetition and entailment.  \n\nThe cosine similarity metric in the model is not very well suited to the tasks of coherence and cohesion, as it is symmetric, while natural language isn't.  The pair:\n\"John went to the store to buy some milk.\"\n\"When he got there, they were all out.\"\n\nand \n\n\"When he got there, they were all out.\"\n\"John went to the store to buy some milk.\"\n\nwould have identical scores according to a cosine similarity metric, while the first ordering is much more coherent than the second.\n\nThe conclusion says \"we showed a significant improvement\": how was significance determined here?\n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}