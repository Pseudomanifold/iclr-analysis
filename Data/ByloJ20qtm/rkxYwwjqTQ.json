{"title": "Interesting Model and Incremental Improvement on Synthetic Datasets and Problematic Problem Definition", "review": "This paper presents an LSTM-based model for bug detection and repair of a particular type of bug called VarMisuse, which occurs at a point in a program where the wrong identifier is used. This problem is introduced in the Allamanis et al. paper. The authors of the paper under review demonstrate significant improvements compared to the Allamanis et al. approach on several datasets.\n\nI have concerns with respect to the evaluation, the relation of the paper compared to the state-of-the-art in automatic program repair (APR), and the problem definition with respect to live-variable analysis.\n\nMy largest concern about both this paper and the Allamanis et al. paper is how it compares to the state-of-the-art in APR in general. There is a large and growing amount of work in APR as shown in the following papers:\n[1] L. Gazzola, D. Micucci, and L. Mariani, \u201cAutomatic Software Repair: A Survey,\u201d IEEE Transactions on Software Engineering, pp. 1\u20131, 2017.\n[2] M. Monperrus, \u201cAutomatic Software Repair: A Bibliography,\u201d ACM Comput. Surv., vol. 51, no. 1, pp. 17:1\u201317:24, Jan. 2018.\n[3] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, \u201cDo automated program repair techniques repair hard and important bugs?,\u201d Empir Software Eng, pp. 1\u201347, Nov. 2017.\n\nAlthough the proposed LSTM-based approach for VarMisuse is interesting, it seems to be quite a small delta compared to the larger APR research space. Furthermore, the above papers on APR are not referenced.\n\nThe paper under review mostly uses synthetic bugs. However, they do have a dataset from an anonymous industrial setting that they claim is realistic. In such a setting, I would simply have to trust the blinded reviewers. However, the one industrial software project tells me little about the proposed approach\u2019s effectiveness when applied to a significant number of widely-used software programs like the ones residing in state-of-the-art benchmarks for APR, of which there are at least the following two datasets:\n[4] C. L. Goues et al., \u201cThe ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,\u201d IEEE Transactions on Software Engineering, vol. 41, no. 12, pp. 1236\u20131256, Dec. 2015.\n[5] R. Just, D. Jalali, and M. D. Ernst, \u201cDefects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs,\u201d in Proceedings of the 2014 International Symposium on Software Testing and Analysis, New York, NY, USA, 2014, pp. 437\u2013440.\n\nThe above datasets are not used or referenced by the paper under review.\n\nMy final concern about the paper is the formulation of live variables. A variable is live at certain program points (e.g., program statements, lines, or tokens as called in this paper). For example, from Figure 1 in the paper under review, at line 5 in (a) and (b), object_name and subject_name are live, not just sources.  In the problem definition, the authors say that \"V_def^f \\subseteq V denotes the set of all live variables\", which does not account for the fact that different variables are alive (or dead) at different points of a program. The authors then say that, for the example in Figure 1, \"V_def^f contains all locations in the program where the tokens in V appear (i.e., tokens in the Blue boxes), as well as token sources from line 1\u201d. The explanation of the problem definition when applied to the example does not account for the fact that different variables are alive at different program points. I\u2019m not sure to what extent this error negatively affects the implementation of the proposed model. However, the error could be potentially quite problematic.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}