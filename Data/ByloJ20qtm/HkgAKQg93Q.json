{"title": "Weak baseline and lack trade-offs discussion makes it hard to say if idea is good.", "review": "Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present.\n\nThis paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug.\n\nUnfortunatley this leads to a number of weaknesses:\n - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.\n - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the >80% range)\n - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on.\n - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines?\n\nStrengths of the paper are:\n - Well-written and easy to follow and understand.\n - Evaluation on several datasets.\n - Interesting architecture for bug-localization if the idea really works.\n\n[1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}