{"title": "Decent idea with very good validation", "review": "The paper proposes to train exploration policies for contextual bandit problems through imitation learning on synthetic data-sets. An exploration policy takes the decision of choosing an action on each time-step (balancing explore/exploit) based on the history, the confidences of taking different actions suggested by a policy optimizer (bet expert policy given the history). The idea in this paper is to generate many multi-class supervised learning data-sets and sun an imitation learning algorithm for training a good exploration policy. I think this is a novel idea and I have not seen this before. Moreover some intuitive features for training the exploration policy, like the historical counts of the arms, the time-step, arms rewards variances are used on top the the confidence scores from the policy optimizer. It is shown empirically that these extra features add value. \n\nOverall I think this is a well-written paper with very thorough experimentation. The results are also promising. It would be interesting to gain some insights from the learnt policy, in order to improve hand-designed policies. For example, in a few data-sets it would be interesting to see whether the learnt policy is similar to epsilon greedy in the early stages and switches to greedy after a point, or which of the hand-designed strategies like bagging/cover is the learnt policy most similar to in terms of choice of actions, however I am not sure how such an analysis can be done.  It would also be fair to discuss the offline training time and online run-time of the algorithm with respect to others.  Also, I think the paper should provide a brief introduction to imitation learning, as it is commonly not known in the bandit community. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}