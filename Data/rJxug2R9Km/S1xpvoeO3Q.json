{"title": "Overall, given the novelty of the idea and the good results, I am inclined to accept, with major modifications.", "review": "This paper proposes a new method (Melee) to explore on contextual bandits. It uses a supervised full information data set to evaluate (using counterfactual estimation) and select (using imitation learning) a proper exploration strategy. This exploration strategy is then used to augment an e-greedy contextual bandit algorithm.\n\nThe novelty is that the exploration strategy is learned from the data, as opposed to being engineered to minimize regret. The edge of Melee stems from the expected improvement for choosing an action against the standard bandit optimization recommendation.\n\nPros:\n- using data to learn exploration strategy in tis manner is a novel idea for bandits\n- good experimental results\n- well written paper\n\nCons:\n- Practical impact may be minimal. This setting is seldom encountered in reality.\n- No comparison with Thompson sampling bandits, which also use data in devising an exploration strategy. I suggest authors compare to better suited bandits and exploration strategies, beyond basic e-greedy and UCB.\n- Article assumes knowledge of imitation learning. which is not a given in bandit literature. I suggest a simple explanation or sketch of the imitation algorithm.\n- Theoretical guarantees questionable. Theorem 1 talks about \"no-regret algorithm\". you then extend this notion and claim \"if we can achieve low regret .... then ....\". It is unclear to me how this theorem allows you to make such claim. A low regret is > no-regret, and hence a bound on no-regret may not generalize to low regret.\n- May want to add noise to augmentation data, to judge robustness of method.\n\nOverall, given the novelty of the idea and the good results, I am inclined to accept, with major modifications. Improvements of the method and analysis are likely to follow. Given the flaws though, I am not fighting for this paper.\n\nMinor comments:\nsec 2.1: you may want to explain why you require reward to be [0,1]\nAlg 1: explain Val and rho in algorithm.\nsec 2.3: what is \"ergo\". Also, you may want to refer to f as \"function\" and to pi as \"policy\". referring to f as policy may be confusing (even though it is a policy). For example: \"(line 8) on which it trains a new policy\"\nEnd of 2.4: \"as discussed in 2.4\" should be \"in 2.3\"\nsec 3.3: why is epsilon=0 the best? is it because synthetic data has no noise? This result surprises me.\n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}