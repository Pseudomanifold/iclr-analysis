{"title": "Good results, some questions", "review": "This work proposes a method for parametrising relation matrices in graph neural networks (GNNs) using text. The model is applied in a relation extraction task, and specific dataset subsections are identified to test and analyse \u201chopping\u201d behaviour: a model\u2019s ability to combine multiple relations for inferring a new one.\n\nStrengths\n- strong results\n- testing on both bag-level / single-level relation extraction\n- Insights via multiple ablations \u2014 variation of the number of layers, exploring densely connected data subsections with cycles to identify examples for multi-hop inference\n- evaluation on new, human-annotated test set\n\n\nIssues\n- evaluation only on one task, and one dataset (although, with more detail).\n- unclear how/why the specific result metrics and their particular range of precision are chosen. Why is F1/Acc only reported for the human-labelled part, but not for the distantly labelled part? Why is precision cut off at ~25% recall? A comprehensive aggregate measure across all levels of recall would be more informative, e.g. PR-AUC., and consistently applied in all experiments.\n- task varies from previous versions of the task, posing a potential problem with comparability. What is the motivation behind augmenting the dataset with \u201cNA\u201d labels? Why is the task from previous work altered to predicting relationships between _every_ entity pair? Also unclear: is predicting \u201cNA\u201d actually part of the training loss, and of the evaluation? Is training of the previous \u201cbaseline\u201d models adapted accordingly?\n- some claims appear too bold and vague \u2014 see below.\n- comparatively small modelling innovation\n- There is similar prior work to this, most prominently Schlichtkrull et al. 2017 [https://arxiv.org/pdf/1703.06103.pdf] who evaluate on Fb15k-237, but also De Cao et al. 2018 [https://arxiv.org/pdf/1808.09920.pdf, published within 1 month before ICLR submission deadline] who evaluate on Wikihop. These previous methods in fact do the following: \u201cMost existing GNNs only process multi-hop relational reasoning on pre-defined graphs and cannot be directly applied in natural language relational reasoning.\u201d It would be good to work out differences to these previous models in more detail, especially to Schlichtkrull et al. 2017.\n- unclear how big the specific contribution of the language-generated matrices is. I would normally not obsess about such a baseline, but it seems that the \u201cgenerate using NL\u201d aspect is a core to the paper (even in the title), and this isn\u2019t worked out clearly.\n\n\nMore comments / questions:\n- \u201cmulti-hop relational reasoning is indispensable in many natural language processing tasks such as relation extraction\u201d. This isn\u2019t clear to me, \u201cindispensable\u201d is a very strong wording.\n- \u201cstate-of-the-art baselines\u201d. SOTA defines the best previous work. How can there be several baselines (plural) which are all best? What does SOTA mean when a slight redefinition of the task is proposed, as in this work?\n- \u201cRelation extraction from text is a classic natural language relational reasoning task\u201d \u2014 reference would be useful. \n- not a big issue, though this does sound somewhat contradictory: 1) \u201cthe number of layers K is chosen to be of the order of the graph diameter\u201d. 2) \u201cWe treat K as a hyperparameter\u201d\n- not clear: is LSTM indeed exactly the same as GP-GNN with K=1? I assume there is a difference, as the LSTM encodes the entire sentence at once, conditioning entity representations on their local context, whereas in GP-GNN this would not be the case.\n- The distinction to Context-Aware RE (CARE) is not clear. The authors argue that CARE models co-occurrence of multiple relations, but is this not what a multi-hop relation extraction _should_ learn? It is also not clear how GP-GNN differs in this respect.\n- It would be interesting to compare with a model which does not use language to define relation matrices (A), but learns them directly as parameters (independently from the text). \n- It would be interesting to see an analysis of the matrices A_{i,j}. What does the text generate, and how can one find this out? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}