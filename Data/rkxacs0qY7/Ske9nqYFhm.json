{"title": "Interesting paper on functional variational inference but more analysis is needed re the approximations", "review": "This paper presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally (i.e. through a stochastic process) rather than via a prior (e.g.) over weights. The paper is motivated by the maximization of the evidence lower bound defined on stochastic processes, which is itself motivated as the minimization of the KL between the approximate posterior and the true posterior processes. \n\nThe paper relies heavily on Th 1, which is proved in the appendix, which states that the KL divergence between two stochastic processes is equivalent to the supremum of the marginal KL divergence over all finite sets of input locations. This yields a GAN-like objective where the ELBO is minimized wrt the input subsets and maximized wrt the approximate posterior. Obviously, as the former minimization is unfeasible, the authors proposed two additional approximations: (1) Restrict the size of the subset to search for; (2) replace the mimimization step with a sampling/average procedure. From the theoretical standpoint, I believe ths is the major defficiency of the paper, as these approximations are not justified and it is not clear, theoretically, how they relate to the original objective. In fact, for example on the case of Gaussian process priors, it looks too good to be true that one can have a KL-divergence over low-dimensional distributions instead of handling N-dimensional (fully coupled) distributions. It is unclear what is lost here (whereas in well-known sparse variational methods such as that of Titsias, one knows how the sparse model relates to the original one). \n\nOnly the first experiment compares to a GP model, where it is shown that the solution given by fBNN (which was seeded with the GP solution) is not better (if not slightly worse than the GP\u2019s). As recommended by Matthews et al (2018), all the experiments should compare to a base GP model.\n\nOther Comments:\nThe paper claims that the method estimates reliable uncertainties. However, there is not an objective evaluation of this claim (as in the predictive posteriors are well-calibrated). \nWhy aren\u2019t hyper-parameters estimated using the ELBO?\nIn Figure 2, why are the results so bad for BBB? This is very surprising.\nHow does the approach relate Variational Implicit Processes (Ma et al, 2018)?\nMost of the experiments in the paper assume 1 hidden layer. In the case of deeper architectures, how can one specify a prior over functions that is \u201cmeaningful\u201d?\nMost (all?) the experiments are specific to regression. Is there any limitation for other likelihood models?\nHow does the approach compare to inference in implicit models?\nIn the intro \u201cpractical variational BNN approximations can fail to match the predictions of the corresponding GP\u201d. Any reference for this?\nI believe the paper should also relate to the work of Matthews et al (2015)\n\n\nReferences\n(Ma et al, 2018) Variational Implicit Processes \n(Matthews et al, 2018) Gaussian process behavior in wide deep neural networks\n(Matthews et al, 2015) On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes\n\f\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}