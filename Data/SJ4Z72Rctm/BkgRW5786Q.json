{"title": "Need clearer motivation for algorithm. Lots of little issues need fixing", "review": "The authors introduce Divergence Correction (DC) for the problem of transfer learning by composing policies. There approach builds on GPI with a maximum entropy objective. They also prove that DC solves for the max-entropy optimal interpolation between two policies and derive a practical approximation for this algorithm. They provide experimental results in a gridworld problem and study their approximate algorithm in two continuous control problems.\n\nWhile this paper has some interesting ideas (combining GPI with a Max-Entropy objective and DC), these ideas are not properly motivated. The main problem seems to be clarity. One big problem is that the paper never defines the notion of a notion of optimality (or near-optimality). Also, considering that the DC algorithm is one of the main contributions of the paper it is barely motivated. Theorem 3.2 is presented with almost no explanation about how DC was derived. Why do the authors believe that DC is a good idea on a conceptual level? It's very interesting that the paper presents cases where previous approaches (Optimistic and GPI) don't perform well. But the authors don't explain why they believe DC should perform well in these cases. \n\nThe authors make the unjustified claim in the abstract that their approach has \"near-optimal performance and requires less information\". I say this is unjustified because they only try this approach on three benchmarks. In addition, there should be situations where DC also performs poorly since there are known hardness results for solving MDPs. Admittedly, those results may not apply if the authors are making assumptions that are not being clearly discussed in the paper.\n\nMinor Comments:\n1. In the abstract, \"requiring less information\" is very imprecise. Are you referring to sample complexity?\n2. In the introduction, \"can consistently achieve good performance\" is imprecise. What is the notion of near-optimality? What does consistent mean? Having experimental results on 3 tasks doesn't seem to be enough to me to justify this claim.\n3. In the introduction (and rest of the paper), please don't call Haarnoja et al.'s approach optimistic. Optimism already has another widely used meaning in RL literature. Maybe call it \"Uncorrected\".\n4. In section 2.2, the authors introduce \\pi_1, \\pi_2, ... , \\pi_n but never actually use that notation. This section does not clearly explain how GPI works.\n5. In Theorem 3.1, the authors should introduce Q^1, Q^2, ... , Q^n and define the policies in terms of the action-value functions. Also, the statement of this theorem is not self contained, what is the reward function of the MDP? The proof below should be called a proof sketch.\n6. The paper mentions that extending to multiple tasks is possible. Is it trivial? What is the basic idea? It seems straightforward but it might be helpful to explicitly state the idea.\n7. In Theorem 3.2, how was C derived? Please add some commentary explaining the conceptual idea.\n8. In Table 1, what is f(s, a|b)? I don't see where this was defined?\n9. CondQ is usually referred to as UVFA in the literature.\n10. Section 3 really needs a conclusion statement.\n11. Section 4 is very unclear and hard to follow.\n12. In figure 1f, what is LTD? It's never defined. I'm guessing it's DC.\n13. All of the figures are too small and some are not clear in black and white.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}