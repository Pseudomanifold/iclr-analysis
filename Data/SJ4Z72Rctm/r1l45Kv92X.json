{"title": "Interesting work, but need further improvement", "review": "\n-- Contribution, Originality, and Quality --\n\nThis paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5).\n\nThese two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting.\n\n-- Clarity --\n\nI have two major complaints about the clarity of this paper. \n\n1) Section 4 of the paper is not well written and is hard to follow.\n\n2) Some notations in the paper are not well defined. For instance\n\n2a) In page 3, the notation \\delta has not been defined.\n2b) In page 6, both notation V_{\\theta'_V} and V'_{\\theta_V} have been used. I do not think either of them has been defined. \n\n-- Pros and Cons --\n\nPros:\n\n1) The proposed approaches and the experiment results are interesting.\n\nCons:\n\n1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference.\n\n2) The paper is not very well written, especially Section 4.\n\n3) For Theorem 3.2, why not prove a variant of it for the general multi-task case?\n\n4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}