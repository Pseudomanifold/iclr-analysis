{"title": "Good Paper", "review": "This paper proposes using Divergence Correction to compose max ent policies. Based on successor features, this method corrects the optimistic bias of Haarnoja 2018. The motivation for composing policies is sound. This paper addresses the problem statement where policies must accomplish different linear combinations of different reward functions. This method does not require observation the reward weights.\n\nAs shown in the experiments, this method outperforms or equally performs past work in both tabular and continuous  environments. The paper is well written and discusses prior work in an informative manner. The tabular examples provide good visualizations of why the methods perform differently.\n\nMinor:\n- Figure 1.e: Why does the Optimistic transfer have high regret when the caption says that \"on the LU task, optimistic transfers well\"\n- Figure 1.i states \"Neither GPI nor the optimistic policies (j shows GPI, by the Optimistic policy is similar)\" but Figure1.j is labeled DC T, is this a typo?\n- Figure 2: Many typos:  \"(b) Finger position at the en (of the trajectoriesstard ting from randomly sampled start states)\"\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}