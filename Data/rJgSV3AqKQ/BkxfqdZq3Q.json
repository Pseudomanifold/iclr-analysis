{"title": "An emperical study on several methods for adjusting learning rate ", "review": "The paper reports the results of testing several stepsize adjustment related methods including  vanilla SGD, SGD with Neserov momentum, and ADAM. Also, it compares those methods with hypergradient and without. The paper reports several interesting results. For instance, they found hypergradient method on common optimizers doesn't perform better that the fixed exponential decay method propose by Wilson et al. (2017). \n\nThough it is an interesting paper, but the main issue with this paper is that it lacks enough innovation with respect to theory or empirical study. It is not deep or extensive enough for publishing at a top conference. \n  \nOn page 3, it will be better to explain why use mu = 0.9, beta, etc. Why use CIFAR-10, MNIST?\n\nThe URL in References looks out of bound. \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}