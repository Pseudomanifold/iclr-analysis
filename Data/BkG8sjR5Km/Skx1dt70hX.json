{"title": "Review", "review": "Summary: The authors use competition as a way to train agents in a complex continuous team-based control task: a 2 player soccer game. Agents are paired randomly into a team of 2 and play another team of 2. The key aspect of the proposed algorithm is the use of population based training.\n\nStrong Points\n-\tThe authors propose a convincing methodology for speeding up learning in coordinated MARL.\n-\tThe Nash Averaging approach suggested for evaluating in the presence of cycles is interesting and a useful tool for evaluation when there are no easy baselines\n-\tThe authors do convincing ablation studies to show that the PBT is the most important part of the learning algorithms and does well even when paired with a simple feed forward model\n\nQuestions\n-\tThe authors use reward shaping of the form: \u201cWe design shaping reward functions {rj : S \u00d7 A \u2192 R}j=1,...,nr P , weighted so that r(\u00b7) := nr j=1 \u03b1j rj (\u00b7) is the agent\u2019s internal reward and, as in Jaderberg et al.\u201d I\u2019m not sure I follow how this works, without the additional dense shaping in the soccer game the reward is 0/1 depending on if one\u2019s team wins or loses, so won\u2019t one\u2019s rewards always be perfectly correlated with those of one\u2019s teammates and perfectly anticorrelated with those of the other team? Does this only work with the dense shaping (e.g. vel-to-ball)?\n-\tI would like to see which of the PBT controlled hyperparameters actually matter for the increase in training speed. Do the learning rates matter (since they\u2019re also being changed by the Adam optimizer as training goes) or is it about the discount factor/entropy regularizer?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}