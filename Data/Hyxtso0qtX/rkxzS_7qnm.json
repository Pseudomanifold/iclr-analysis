{"title": "Cool idea, but there are issues in evaluation and results are weak overall", "review": "The paper proposes a novel exploration strategy for self-supervised imitation learning. An inverse dynamics model is trained on the trajectories collected from a RL-trained policy. The policy is rewarded for generating trajectories on which the inverse dynamics model (IDM) currently works poorly, i.e. on which IDM predicts actions that are far (in terms of mean square error) from the actions performed by the policy. This adversarial training is performed in purely self-supervised way. The evaluation is performed by one-shot imitation of an expert trajectory using the IDM: the action is predicted from the current state of the environment and the next state in the expert\u2019s trajectory. Experimental evaluation shows that the proposed method is superior to baseline exploration strategies for self-supervised imitation learning, including random and curiosity-based exploration. \n\nOverall, I find the idea quite appealing. I am not an expert in the domain and can not make comments on the novelty of the approach. I found the writing mostly clear, except for the following issues:\n- the introduction has not made it crystal clear that the considered paradigm is different from e.g. DAGGER and GAIL in that expert demonstrations are used at the inference time. A much wider audience is familiar with the former methods, and this distinction should have be explained more clearly.\n- Section 4.2.: \u201cAs opposed to discrete control domains, these tasks are especially challenging, as the sample complexity grows in continuous control domains.\u201d - this sentence did not make sense to me. It basically says continuous control is challenging because it is challenging. \n- I did not understand the stabilization approach. How exactly Equation (7) forces the policy to produce \u201cnot too hard\u201d training examples for IDM? Figure 4 shows that it is on the opposite examples with small L_I that are avoided by using \\delta > 0.\n- Table 1 - it is a bit counterintuitive that negative numbers are better than positive numbers here. Perhaps instead of policy\u2019s deterioration you could report the relative change, negative when the performance goes down and positive otherwise?\n\nI do have concerns regarding the experimental evaluation:\n- the \u201cDemos\u201d baseline approach should be explained in the main text! In Appendix S.7 I see that 1000 human demonstrations were used for training. Why 1000, and not 100 and not 10000?  How would the results change? This needs to be discussed. Without discussing this it is really unclear how the proposed method can outperform \u201cDemos\u201d, which it does pretty often.\n- it is commendable that 20 repetitions of each experiment were performed, but I am not sure if it is ever explained in the paper what exactly the upper and lower boundaries in the figures mean. Is it the standard deviation? A confidence interval? Can you comment on the variance of the proposed approach, which seems to be very high, especially when I am looking at high-dimensional fetch-reach results?\n- the results of \u201cHandReach\u201d experiments, where the proposed method works much worse than \u201cDemos\u201d are not discussed in the text at all\n- overall, there is no example of the proposed method making a difference between a \u201cworking\u201d and \u201cnon-working\u201d system, compared to \u201cCuriosity\u201d and \u201cRandom\u201d. I am wondering if improvements from 40% to 60% in such cases are really important. In 7 out of 9 plots the performance of the proposed method is less than 80% - not very impressive. \"Demos\" baseline doesn't perform much better, but what would happen with 10000 demonstrations?\n- there is no comparison to behavioral cloning, GAIL, IRL. Would these methods perform better than learning IDM like \"Demos\" does?\n\nI think that currently the paper is slightly below the threshold, due to evaluation issues discussed above and overall low performance of the proposed algorithm. I am willing to reconsider my decision if these issues are addressed.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}