{"title": "good training exploration, somewhat limited scope of experimental conditions", "review": "This paper presents a system for self-supervised imitation learning using a RL agent that is rewarded for finding actions that the system does not yet predict well given the current state.  More precisely, an imitation learner I is trained to predict an action A given a desired observation state transition xt->xt+1; the training samples for I are generated using a RL policy that yields an action A to train given xt (a physics engine evaluates xt+1 from xt and A).  The RL policy is rewarded using the loss incurred by I's prediction of A, so that moderately high loss values produce highest reward.  In this way, the RL agent learns to produce effective training samples that are not too easy or hard for the learner.  The method is evaluated on five block manipulation tasks, comparing to training samples generated by other recent self-supervised methods, as well as those found using a pretrained expert model for each task.\n\nOverall, this method exploration seems quite effective on the tasks evaluated.  I'd be curious to know more about the limits and failures of the method, e.g. in other types of environments.\n\nAdditional questions:\n\n- p.2 mentions that the environments \"are intentionally selected by us for evaluating the performance of inverse dynamics model, as each of them allows only a very limited set of chained actions\".  What sort of environments would be less well fit?  Are there any failure cases of this method where other baselines perform better?\n\n- sec 4.3 notes that the self-supervised methods are pre-trained using 30k random samples before switching to the exploration policy, but in Fig 2, the success rates do not coincide between the systems and the random baseline, at either samples=0 or samples=30k --- should they?  if not, what differences caused this?\n\n- figs. 4, 5 and 6 all relate to the stabilizer value delta, and I have a couple questions here:  (i) for what delta does performance start to degrade?  At delta=inf, I think it should be the same as no stabilizer, while at delta=0 is the exact opposite reward (i.e. negative loss, easy samples).  (ii) delta=3 is evaluated, and performance looks decent for this in fig 6 --- but fig 4 shows that the peak PDF of \"no stabilizer\" is around 3 as well, yet \"no stabilizer\" performs poorly in Fig 5.  Why is this, if it tends to produce actions with loss around 3 in both cases?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}