{"title": "interesting and potentially useful paper", "review": "The paper proposes an exploration strategy for deep reinforcement learning agent in continuous action spaces. The core of the method is to train an inverse local model (a model that predicts the action that was taken from a pair of states) and its errors as an exploration bonus for a policy gradient agent. The intuition is that its a good self-regulating strategy similar to curiosity that leads the agents towards states that are less known by the inverse model. Seeing these states improves the . There are experiments run on the OpenAI gym comparing to other models of curiosity. The paper is well written and clear for the most part.\n\npros:\n- the paper seems novel and results are promising\n- easy to implement\ncons:\n- seems unstable and not clear how it would scale in a large state space where most states are going to be very difficult to learn about in the beginning like a humanoid body.\n- only accounts for the immediately controllable aspects of the environment which doesn't seem to be the hard part. Understanding the rest of the environment and its relationship to the controllable part of the state seems beyond the scope of this model. Nonetheless I can imagine it helping with initial random motions. \n- from (6) the bonus seems to be unbounded and (7) doesn't seem to fix that. Is that not an issue in general ? Any intuition about that ?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}