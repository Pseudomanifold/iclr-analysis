{"title": "Novel idea for exploration in RL, good empirical results, can benefit from more clarity and evidence", "review": "Summary:\n\nThe paper proposes the novel idea of using contingency awareness (i.e. the agent\u2019s understanding of the environment dynamics, its perception that some aspects of the environment are under its control and ability to locate itself within the state space) to aid exploration in sparse-reward reinforcement learning tasks. They obtain great results on hard exploration Atari games and a new SOTA on Montezuma\u2019s Revenge (compared to methods which are also not using any external data). They use an inverse dynamics model with attention, (trained with self-supervision) to predict the agent\u2019s actions between consecutive states. This allows them to approximate the agent\u2019s position in 2D environments, which is then used as part of the state representation to encourage efficient exploration. One of the main strengths of this method is the fact that it achieves good performance on challenging tasks without the expert demonstrations or environment simulators. I also liked the discussion part of the paper and the fact that it emphasizes some of the limitations and avenues for future work. \n\nPros:\nGood empirical results on challenging Atari tasks (including SOTA on Montezuma\u2019s Revenge without extra supervision or information)\nTackles a long-standing problem in RL: efficient exploration in sparse reward environments\nNovel idea, which opens up new research directions\nComparison experiments with competitive baselines\n\nCons:\nThe choice of extra loss functions is not very well motivated \nSome parts of the paper are not very clear\n\nMain Comments:\nMotivation of Extra Loss Terms: It is not very clear how each of the losses (eq 5) will help mitigate all the issues mentioned in the paragraph above. I suggest providing more detailed explanations to motivate these choices. In particular, why are you not including an entropy regularization loss for the policy to mitigate the third problem identified? This has been previously shown to aid exploration. I also did not see how the second issue mentioned is mitigated by any of the proposed extra loss terms.\nRequest for Ablation Studies: It would be useful to gain a better understanding of how important is each of the losses used in equation 5, so I suggest doing some ablation studies.\nCell Loss Confusion: Last paragraph of section 3.1: is there a typo in the formulation of the per cell cross-entropy losses? Is alpha supposed to be the action a? Otherwise, this part is confusing, so please explain the reasoning and what supervision signal you used. \nState Representation: Section 3.2 can be improved by adding more details. For example, it is not explained at all what the function psi(s) contains and how it makes use of the estimated agent location. I would suggest moving some of the details in section 4.2 (such as the context representation and what psi contains) earlier in the text (perhaps in section 3.2). \n\n\nMinor Comments:\nPlots: It would be helpful to give more details about the plots. I suggest labeling the axes. Is the x-axis number of frames, steps or episodes? How many runs are used to compute the mean? What do the light and dark colors represent? What smoothing process did you use to obtain these curves if any? Figure 2, why is there such a large drop in performance on Montezuma\u2019s Revenge after 80M? Something similar seems to happen in PrivateEye, but much earlier in training and the agent never recovers. \nTables: I would suggest reporting results in the tables for more than 3 seeds given that these algorithms tend to have rather high variance. Or at least, provide the values for the variance. \nAppendix A, Algorithm 1: I believe this can be written more clearly. In particular, it would be good to specify the loss functions that you are optimizing. There seems to be some mismatch between the notation of the losses in the algorithm and the paper. It would also help to define alpha, c, psi etc. \nFootnote on page 4: you may consider using a different variable instead of c_t to avoid confusion with c (used to refer to the context representation). \nAppendix D, Algorithm 2: is there a reason for which you aren\u2019t assigning the embeddings to the closest cluster instead of any cluster that is within some range? \n\n\nReferences:\nThe related work section on exploration and intrinsic motivation could be improved by adding more references such as:\nGregor et al. 2016, Variational Intrinsic Control\nAchiam et al. 2018, Variational Option Discovery Algorithms\nFu et al. 2017, EX2: Exploration with Exemplar Models for Deep Reinforcement Learning\nSukhbaatar et al. 2018, Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play\nEysenbach et al. 2018, Diversity is all you need: learning skills without a reward function\n\n\nFinal Decision:\n\nThis paper presents a novel way for efficiently exploring environments with sparse rewards. \nHowever, the authors use additional loss terms (to obtain these results) that are not very well motivated. I believe the paper can be improved by including some ablation experiments and making some parts of the paper more clear, so I would like to see these additions in next iterations of the paper. \n\nGiven the novelty, empirical results, and comparisons with competitive baselines, I am inclined to recommend it for acceptance. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}