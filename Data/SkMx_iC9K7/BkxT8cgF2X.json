{"title": "interesting idea but poorly written and experiments not well-executed", "review": "Contributions:\n\nThe main contribution of this paper is the proposed DelibGAN for text generation. The framework introduces a coarse-to-fine generator, which contains a first-pass decoder and a second-pass decoder. Instead of using a binary classifier, the discriminator is a multiple instance discriminator. Two different variants of DelibGAN are proposed, with experiments showing that DelibGAN-II performs the best.\n\nStrengths:\n\n(1) Novelty: I think this paper contains some novelty inside. Using a coarse-to-fine generator is an interesting idea. However, as I noted below, the paper is not well-executed.\n\nWeaknesses:\n\n(1) Presentation: This paper is easy to follow, but poorly written.\n\nFirst, the paper is too repetitive. For example, the two-pass decoding process has been repeatedly mentioned too many times in the paper, such as the paragraph above Eqn. (8). Please be concise. \n\nSecond, when citing a paper, there should be a space between the word and the cited paper. For example, in the first paragraph of the introduction section, instead of \"(RNN)(Mikolov et al., 2011)\", it should be \"(RNN) (Mikolov et al., 2011)\". This should be corrected for the whole paper.\n\nThird, in the first paragraph of the introduction section, I am not sure why (Wang & Wan, 2018a) is cited here. This is not the first paper that points out the problem. One should refer to [a], which is also not cited in the paper. \n\nMissing reference: I also encourage the authors citing [b] since it is directly related to this work, which is about using GAN for text generation.  \n\n[a] Sequence Level Training with Recurrent Neural Networks, ICLR 2016\n[b] Adversarial Feature Matching for Text Generation, ICML 2017\n\n(2) Evaluation: My main concern lies in the experimental evaluation, with detailed comments listed below.  \n\nQuestions:\n\n(1) In Algorithm 1, there exists the pretraining process of G_1 & G_2. However, it is not clear to me how this this pretraining is implemented, since the output of G_1 is not observed, but is hidden and imagined by the model. So, what is the training signal for pretraining G_1? Can the authors provide more details? Please clarify it. \n\n(2) In experiments, why the authors only compare with SeqGAN, SentiGAN & MaskGAN? One would naturally ask how the proposed model compare with RankGAN, TextGAN and LeakGAN. For example, what are the corresponding results of RankGAN, TextGAN & LeakGAN in Table 1 to 5? This should not be difficult to compare with, based on the availability of Texygen.\n\n(3) Besides using the perplexity, the authors also use the novelty and diversity terms defined in (11) & (12) for evaluation. This is good. However, why not using some commonly used metrics in the literature, such as BLEU and self-BLEU? I agree these metrics are also not perfect, but it will be helpful to also report these results for benchmark purposes. \n\n(4) Instead of using datasets like HappyDB & SSTB, it would be helpful to also report results on some benchmark datasets such as COCO captions and EMNLP News as used in the LeakGAN paper.  What are the results looking like on these two datasets?\n\n(5) The results in Table 1 & Figure 2 is misleading. They do not necessarily mean the proposed model is better, as the NLL value only measures how the generated sentence look like a real sentence, but it does not measure the diversity of generated sentences. For example, a model that only repeatedly produces one very realistic sentence would also achieve a very high NLL score.  \n\n(6) Table 3 & 5 shows the human evaluation results. However, how this is performed is not clear at all. For example, is the human evaluation performed using AMT? How many samples are used for human evaluation? Also, how many workers has one sentence been evaluated by? Without all these details, how reliable this result is is questionable.\n\nMinor issues:\n\n(1) In Eqn. (3), since only two classes exist, there is no need to say \"softmax\", use \"sigmoid\" function is enough for illustration. \n\n(2) In the line below Eqn. (5), \"bias\" => \"baseline\"\n\n(3) In Eqn. (3), there is \")\" missing. \n\n(4) In Figure 2, there are two methods \"SentiGAN\" and \"DelibMLE\" with the same green color. Which one is which is hard to see. \n\n(5) In the first paragraph of related work, MaskMLE => MaskGAN.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}