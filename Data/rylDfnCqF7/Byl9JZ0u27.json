{"title": "Neither the objective nor the model but the optimization procedure may be the key to training VAE", "review": "General:\nThe paper tackles one of the most important problems of learning VAEs, namely, the posterior collapse. Typically, this problem is attacked by either proposing a new model or modifying the objective. Interestingly, the authors considered a third option, i.e., changing the training procedure only, leaving the model and the objective untouched. Moreover, they show that in fact the modified objective (beta-VAE) could drastically harm training a VAE.\n\nI find the idea very interesting and promising. The proposed algorithm is very easy to be applied, thus, it could be easily reproduced. I believe the paper should be presented at the ICLR 2019.\n\nPros:\n+ The paper is written in a lucid manner. All ideas are clearly presented. I find the toy problem (Figure 2) very illuminating.\n+ It might seem that the idea follows from simple if not even trivial remarks. But this impression is fully due to the fashion the authors presented their idea. I am truly impressed by the writing style of the authors.\n+ I find the proposed approach very appealing because it requires changes only in the optimization procedure while the model and the objective remain the same. Moreover, the paper formalizes some intuition that could be found in other papers (e.g., (Alemi et al., 2018)).\n+ The presented results are fully convincing.\n\nCons:\n- It would be beneficial to see samples for the same latent variables to verify whether the model utilizes the latent code. Additionally, a latent space interpolation could be also presented.\n- The choice of the stopping criterion seems to be rather arbitrary. Did the authors try other methods? If yes, what were they? If not, why the current stopping criterion is so unique?\n- The proposed approach was applied to the case when the prior is a standard Normal. What would happen if a different prior is considered?\n\nNeutral remark:\n* Another problem, next to the posterior collapse, is the \u201chole problem\u201d (see Rezende & Viola, \u201cTaming VAEs\u201d, 2018). A natural question is whether the proposed approach also helps to solve this issue? One possible solution to that problem is to take the aggregated posterior as the prior (e.g., (Tomczak & Welling, 2018)) or to ensure that the KL between the aggregated posterior and the prior is small. In Figure 4 it seems it is the case, however, I am really curious about the authors\u2019 opinion on this matter.\n* Can the authors relate the proposed algorithm to the wake-sleep algorithm? Obviously, the motivation is different, however, I find these two approaches a bit similar in spirit.\n\n--REVISION--\nI would like to thank the authors for their comments. In my opinion the paper is very interesting and opens new directions for further research (as discussed by the authors in their reply). I strongly believe the paper should be accepted and presented at the ICLR.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}