{"title": "Review", "review": "This work looks into the phenomenon of posterior collapse, and shows that training the inference network more can reduce this problem, and lead to better optima. The exposition is clear. The proposed training procedure is simple and effective. Experiments were carried out in multiple settings, though I would've liked to see more analysis. Overall, I think this is a nice contribution. I have some concerns which I hope the authors can address.\n\nComments:\n - I think [1] should be cited as they first mentioned Eq 5 and also performed similar analysis.\n - Were you able to form an unbiased estimate for the log of the aggregate posterior which is used extensively in this paper (e.g. MI)? Some recents works also estimate this but they use biased estimators. If your estimator is biased, please add a sentence clarifying this so readers aren't mislead.\n - Apart from KL and (biased?) MI, a metric I really would've liked to see is the number of active/inactive units as measured in [2]. I think this is a more reliable and very explainable metric for posterior collapse, whereas real-valued information-theoretic quantites can be hard to interpret.\n\nQuestions:\n - Has this approach truly completely solved posterior collapse? (e.g. can you show that the mutual information between z and x is maximal or the number of inactive units is zero?) \n - How robust is this approach to the effects of randomness during training such as initialization and use of minibatches? (e.g. can you show some standard deviations of the metrics you report in Table 1?)\n - (minor) I wasn't able to understand why the top right is optimal, as opposed to anywhere on the dashed line, in Figures 1(b) and 3?\n\n[1] Hoffman, Matthew D., and Matthew J. Johnson. \"Elbo surgery: yet another way to carve up the variational evidence lower bound.\" \n[2] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. \"Importance weighted autoencoders.\"\n\n--REVISION--\n\nThe paper has significantly improved since the revision and I am happy to increase my score. I do still think that the claim of \"preventing\" or \"avoiding\" posterior collapse is too strong, as I agree with the authors that \"it is unknown whether there is a better local optimum that [activates] more or all latent units\". I would suggest not to emphasize it too strongly (ie. in the abstract) or using words like \"reducing\" or \"mitigate\" instead.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}