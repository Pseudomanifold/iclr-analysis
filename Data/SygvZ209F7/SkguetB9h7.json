{"title": "An important step in our understanding of biologically plausible learning.", "review": "Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively). In particular, they examine two methods for breaking the weight symmetry required in backpropagation: feedback alignment and sign-symmetry. They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance.\n\nThe paper is clear, well motivated, and significant in that it advances our understanding of how recently proposed biologically plausible methods for getting around the weight symmetry problem work on large datasets.\n\nIn particular, I appreciated: the clear introduction and explanation of the weight symmetry problem and how it arises in the context of backprop, the thorough experiments on two large scale problems, the clarity of the presented results, and the discussion about future directions of study.\n\nMinor comments:\n- s/there/therefore in the first paragraph on page 2\n- The authors claim that their conclusions \"largely disagree with results from Bartunov et al 2018\". I would suggest a slight rewording here: the authors' results *extend* our understanding of Bartunov et al 2018. They do not disagree in the sense that this paper also finds that feedback alignment alone is insufficient to train large models on ImageNet.\n- Figure 1: I was expecting to see a curve for performance of feedback alignment on AlexNet\n- Figure 1: The colors are hard to follow. For example, the two shades of purple represent the two FA models, which makes sense, but then there are two separate hues (black and blue) for the sign-symmetry models. Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models. This would make it easier to group the related models.\n- Figure 2: Would be nice if these colors (for backprop/FA/SS) matched the colors in Figure 1.\n- Figure 3: Why is there such a small change in the average alignment angle (2 degrees?) I found that surprising.\n- Figure 3: The right two panels would be clearer on the same panel. That is, instead of showing the std. dev. separately, show it as the spread (using error bars) on the plot with the mean. This makes it easier to get a sense if the distributions overlap or not.\n- Figure 3 (b/c): Could also use the same colors for BP/SS as Figs 1 and 2.\n- Figure 3 (caption): I think the blue/red labels in the caption are mixed up for panel (a).", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}