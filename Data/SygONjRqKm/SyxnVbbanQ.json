{"title": "Amortized Context Vector Inference for Sequence-to-Sequence Networks", "review": "Summary:\nThis paper tries to introduce stochasticity over context representation to enable the neural networks to search for effective representation so as to avoid getting trapped into local optima. The basic idea is to formulate the posterior probability of context representation via a mixture gaussian model. The standard attention model SA can be thought of as a special case of the proposed method. The proposed method outperforms a number of baselines on ADS, video captioning, and machine translation datasets.\n\nStrength:\n  - Generalize better via considering uncertainty and exploring more search space with negligible computational overheads\n  - Outperforms a number of baselines.\n\nComments:\n  - For video captioning task, various methods perform quite similar on test set, but vary a lot on validation set? \n  - Any chances to compare bounds of different variational methods from optimization perspective?\n  - In order to further validate the generalization capacity, any experiments on low-source data or domain adaptation?\n  - Compare with transformer?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}