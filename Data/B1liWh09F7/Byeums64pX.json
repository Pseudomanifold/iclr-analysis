{"title": "Good experimental design and clarity of writing, but limited novelty, experimentation, and analysis", "review": "Summary of the paper: This work builds on the adversarial autoencoder (AAE) proposed by Makhzani et al 2015 and the adversarially regularized auto encoder (ARAE) proposed by Kim et al. 2017. It does this by integrating Vaswani et al. 2017\u2019s transformer model and self-attention (Parikh et al. 2016 and many others) and the AAE and ARAE methodologies. The authors call this SALSA (Self Attentive Latent Space Based Adversarial Text Generation). Basically SALSA combines these parts that are successful on their own and shows on the Google Sentence Compression dataset that ARAE and AAE can be beaten with the inclusion/injection of Transformer + self-attention. Their models show longer and higher-quality sentence generation with more diversity. For some of the BLEU/Human Eval metrics though, ARAE seems to perform better. SALSA seems to aid in alleviating mode collapse (something ARAE struggles dramatically to do).\n\nPreliminary Comments:\n  * Paper is easy-to-read and organized.\n  * Experimental design is well-described especially in how the dataset was preprocessed; something many papers in this area fail to do.\n  * Evaluation metrics are discussed in detail, which is a plus.\n\nDrawbacks:\n  * Novelty is limited: SALSA is an aggregation of already proposed methodologies. \n  * The experimentation could\u2019ve been more robust. \n        - It seems as though there\u2019s only one model that is evaluated here. Trying different models (varying HPs or just random seeds) or providing justification of how this specific model was chosen would be helpful. If I missed this, please correct me.\n        - Varying the dimensionality of the noise vector, how often it is copied would be helpful.\n        - Discussion on why copying the noise vector T times rather than generating a smaller dimensional noise vector (or the same dimensional noise vector) at each step would strengthen the paper.\n  * Statistics of the human evaluation, standard errors/variances of how the humans evaluated the sentences would strengthen the results more.\n  * The discussion of experimental results leaves a little to be desired. I\u2019d like to know what a 0.1 improvement in grammaticality, semantic consistency, fluency etc means. Standard errors like I mentioned before and/or examples of this would go a long way to helping that.\n  * The authors discuss how SALSA aids in \u2018better performance\u2019 on long/complex sentences. A definition of what is long and complex in conjunction with evaluation across BLEU/Self-BLEU/PPL/R-PPL/3 Human metrics would strengthen the paper dramatically.\n  * Looking at SALSA-ARAE vs ARAE in Table 2, ARAE has much higher BLEU scores. Some discussion at why this is the case would also improve the paper.\n\nMinor Issues:\n  * There are many typos in the text, so another proof-read would be good.\n  * Figures and tables are not space-efficient and do not need to be so large; placed in the manners they are. This empty space could be used to add more discussion/further experimentation. Figure 1 can be rotated to give you 6-8 lines more. Figures 2 and 3 can be put side by side without reducing the font too much.\n  * The bolding of certain numbers in the tables are confusing on first glance because you\u2019re obviously comparing SALSA-AAE to AAE and SALSA-ARAE to ARAE, but the tables have both without a divider, so on first glance I was confused (wasn\u2019t the best performance in the row). So including a double lined divider on each table would be helpful.\n  * A few papers have been listed multiple times in the references (AAE and ARAE papers).\n  * Related work section: an inclusion of a few of the more recent GAN papers for text is missing. I think some discussion of other VAE approaches to text such as Bowman et al. 2016: Generating Sentences from a Continuous Space among others would be good. I think citing Kingma and Welling 2014 Auto-Encoding Variational Bayes and Rezende et al. 2014 Stochastic Backpropagation and Approximate Inference in Deep Generative models is important in the first mention of VAEs.\n\nConcluding Thoughts:\n  * This paper has limited model novelty and doesn\u2019t overcome this by having great experiments or analysis.\n  * This paper has the building blocks for great experimentation and analysis, and if those are completed well, has the ability to make strong claims regarding the impact of SALSA on a variety of metrics.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}