{"title": "Interesting idea but not novel enough and lack of insights", "review": "Contributions:\n\nThe main contribution of this paper is the incorporation of the Transformer architecture for adversarial latent-code-based text generation. Specifically, Transformer is used for the encoder and decoder design of AAE and ARAE. This is different from previous GAN models designed for text generation, where LSTM is used for generator, and LSTM or CNN is used for discriminator.\n\nWeaknesses:\n\n(1) Novelty: I think the novelty of this paper is somehow limited. Basically, instead of using LSTM and CNN, the main novelty is the usage of Transformer for encoder and decoder design. From the modeling perspective, there is no novelty, using the same AAE and ARAE models proposed by previous work. \n\nIt the paper is well-written and well-executed, this paper still has the chance to be a good paper, however, I think this paper is quite thin. The contents seems not enough to support a paper.\n\n(2) Presentation: \n\nFirst, in the related work section 2.4, I think the authors should discuss related works not just AAE & ARAE. SeqGAN, RankGAN, TextGAN, LeakGAN and MaskGAN is another research line that needs to be discussed.\n\nSecond, there is no need to make Figure 1 & 2 & 3 this large. It really just wastes a lot of spaces. Table 1 also wastes a lot of spaces. If getting rid of these, the contents in this paper is really thin. \n\nThird, there are several typos in the paper. Please proofread the paper carefully.\n\n(3) Evaluation: Please see my detailed comments listed in the Questions below. \n\nQuestions:\n\n(1) This paper is interested in adversarial latent-code-based text generation. I have a general question about this paper. In the introduction section, the authors mentioned the problem of exposure bias, and then say GAN is a promising approach to alleviate this problem. This is true. However, doing adversarial learning on the latent code seems not help alleviate this exposure bias problem, since still no sequence-level guidance is provided. The model is still generating next word based on previous ground-truth words, with the difference being that an adversarially learned latent code is also injected. Can the authors justify why this model is useful and how it can alleviate exposure bias?\n\n(2) I think the Transformer used in this work can be also adopted for SeqGAN etc. Do the authors think SeqGAN, MaskGAN etc will also benefit from using Transformer?  \n\n(3) In section 4.3, the authors show qualitatively SALSA generates longer and more diverse text. Can the authors provide some intuition why this is the case?\n\n(4) I think the sample size used in the human evaluation is too small. Only 18 sentences are used. A larger-scale human evaluation is needed. How reliable and what are the variances of the scores in Table 5? \n\nMinor issues:\n\n(1) AAE and ARAE has been cited twice in the references. Please correct it. \n\n(2) Not all the boldings in Table 2-5 are correct. Usually, bolding is used for the best performing model. Please correct them. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}