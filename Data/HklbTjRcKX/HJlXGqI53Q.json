{"title": "Interesting empirical work on compression in Resnets with partially inconclusive results", "review": "-> Summary\n\nThe authors propose to extend the analysis of Shwartz-Ziv & Tishby on the information bottleneck principle in artificial neural network training to realistic large-scale settings. They do so by replacing otherwise intractable quantities with tractable bounds in forms of classifiers for I(y;h) and Pixel CNNs for I(x;h). In conclusion, they observe two phases during training, one that maximizes mutual information between input and hidden representation and a second one that compresses the representation at the end of training, in line with the predictions from toy tasks of Shwartz-Ziv & Tishby.\n\n-> Quality\n\nThe paper is very well written, all concepts are well-motivated and explained.\n\n-> Significance\n\nThe main novelty is to replace intractable quantities in the analysis of the information bottleneck with tractable bounds in form of auxiliary models. The idea is neat and makes a lot of sense. On the other hand, some of the results and the bounds themselves are well-known and can thus not be considered novel. The main contribution is thus the empirical analysis itself and given some overly confident claims on qualitative results and missing ablation on the quantitative side, I am not convinced that the overall results are very conclusive.\n\n-> Main Concerns\n\nThe authors make a lot of claims about the qualitative diversity of samples from deeper layers h4 of the network as compared to h1 and h2. However, I do not agree with this. When I look at the samples I see a lot of variations early in training and also in layers h1 and h2. The difference to h4 seems marginal at best and not as clear cut as the authors present it. Thus, these claims should be softened.\n\nIn figure 1 I tend to say that samples at epoch 1 are more varied than at epoch 200. In figure 5 (b) seems pretty color invariant and not only (f) as claimed. In fact (f) seems pretty stable and consistent to me.\n\nThe bound in equation (2) might be quite loose, depending on the quality of the classifier or pixel CNN. Even though there is no way to test this, it should be discussed.\n\nWhat is the effect of weight decay here? I suspect that weight decay plays a crucial role in the final compression phase observed in e.g. figure 3 (c), but might not be a necessary condition to make the network generalize. An ablation experiment verifying or falsifying this statement would be important to conduct and without it I am not convinced that the shown curves are conclusive.\n\n-> Minor\n\n- You seem to use a weird math font, is this on purpose? It does not seem to be the ICLR standard.\n- The bound in equation (2) is a standard variational bound and has been used many times, the authors make it sound like it is their contribution. You should maybe cite basic work and recent work on variational information bottleneck here.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}