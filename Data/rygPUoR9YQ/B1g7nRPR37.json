{"title": "Incremental novelty, weak experimental evaluation, simple test cases ", "review": "In this paper a method of generating compositional images using conditional GANs in proposed. The proposed GAN model is conditioned on two input images concatenated channel-wise and is aimed to generate images from a target distribution of images whose content is a composition of the two input images. In all settings, it is assumed that the ground truth foreground masks of the inputs and the target composite images are available; these segmentation masks are used throughout the learning and the backgrounds are removed for simplicity (explained in paragraph 2 of section 3.3, paragraph 1 of section 3.4, etc)\n\n-----Pros-----\n\n-Being able to generate compositional images is an ambitious goal and interesting application.\n-The Supplemental Material in the appendix provides good visualizations.\n\n-----Cons-----\n\n* This paper does not provide a coherent technical novelty. Essentially, the proposed method combines several prior works and uses different tricks to make the final compositional image. Here are some of them:\n\n-A conditional GAN similar to Goodfellow 2016 and Mirza & Osindero (2014) and Isola et al 2017\n-A relative appearance flow network (RAFN) which has an encoder-decoder architecture similar to Zhou et al. 2016\n-A spatial transformer network(STN) Jaderberg et al 2015\n-A self-supervised inpainting network of Pathak et al 2016\n-An Inference Refinement Network which is similar to Azadi et al 2017\n\n* The applicability and extendibility of the proposed method is not demonstrated: \n-Does the proposed method only work with two input images? \n-How would the proposed method work if three or more input images were provided (similarly concatenated channelwise). What is the actual application of this model if only works with two images? \n-How can one extend it to a more general use case? \n-Why is it better to generate compositional images conditioned on images rather than lingual phrases describing the scene?\n\n* The experimental result does not support the claims of the paper for generating compositional images. Evaluation based on known criteria is not done and no comparative study with prior work is conducted.\n\n- The test cases are very simplistic. While the paper claims about dealing with challenging object compositional problems such as \u201c3D object translations\u201d and \u201cview points\u201d and \u201cocclusions\u201d there are only two test cases: (a)chair and desk (b) bottle and basket. These are very narrow test cases. Even in these two test cases the results are not satisfying. While chair can be behind the desk it can also be next to it! And many diverse situations can happen for composing a chair and a desk. However, all the provided results show only one compositional pose and these modes of diversity have never appeared in the results. The same issue exists with \u201cbottle and basket\u201d. The generated examples only show a bottle inside a basket. This is no different than if the GAN was conditioned on the lingual phrase of \u201cbottle inside basket\u201d rather than images.\n\n-The generated images are not evaluated based on any known criteria of image generation. For example, Inception Score (IS) could be used to quantitatively evaluate (a) the quality of the generated images and (b) their diversity. Other known evaluation criteria are Fr\u00e9chet Inception Distance (FID), precision, recall, F1 score, etc. Even just a simple log-likelihood is not measured to quantitatively show the performance of the proposed approach.\n\n* I strongly suggest improving the the presentation of the paper. There are many formulations which are taken from prior GAN works, there is no need to repeat writing these formulations. Summarizing those formulations or putting them in the appendix can open up space so that you can bring in some of the qualitative results inside the main manuscript.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}