{"title": "Improved estimators for correcting label shifts, but experiments can be improved", "review": "- The authors consider the problem of learning under label shifts, where the label proportions p(y) and q(y) of the training and test distributions differ, while the conditionals p(x|y) and q(x|y) are equal. They build upon the work by Lipton et al. 18 on estimating label proportion weights q(y)/p(y) using the confusion matrix, by proposing an improved estimator with regularization. They show that their estimator provides better weight estimates compared to the unregularized version, and it also gives better prediction accuracies under large label shift scenarios. \n\n- One question I have about this approach is the choice of h in the confusion matrix estimation. Since the theory holds for any fixed hypothesis h, is there any guidance on how we should pick h? The authors seem to use the same model class for the weight estimation and predictions in the experiments. How would using a simpler h for weight estimation (e.g., linear logistic regression) affect the results presented here? \n\n- The Dirichlet shifts described with only the parameter alpha is not particularly intuitive in conveying the size of shifts. The CIFAR10 and MNIST datasets contain about 6000 examples per class. How would a large shift with alpha=0.01 change the distribution, especially for the smallest class how many samples are retained? This can help the readers judge when the correction of label shifts are helpful. \n\n- To clarify, in the experiments for Figure 4 using Minority-Class shifts, with p=0.001, is it true that there are less than 100 training examples for each of the minority classes in the training set? This seems like a very extreme shift. \n\n- I also have trouble understanding Figure 3. RESNET-18 should give >90% accuracy on the original CIFAR10, but in 3b we see accuracies around 75% for small shifts. Also how is the F1-score in 3c computed? Is it micro-averaged or macro-averaged F1? Either way an F1 score below 20% is very low for the unweighted classifier, since RESNET-18 should give fairly good classification accuracy on each class separately if it has >90% overall accuracy. \n\n- The paper is quite solid in motivating the need for better weight estimators for reweighing label proportions and their derivations, and manage to show improvements over the unregularized estimator. Details on the experiments should be improved to give the readers better ideas on when correcting for label shifts help. Right now it looks like it only helps for cases with fairly extreme shifts. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}