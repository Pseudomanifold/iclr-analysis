{"title": "Good paper, contribution moderate, experiments promising", "review": "My summary: A new model, the UT, is based on the Transformer model, with added recurrence and dynamic halting of the recurrence. The UT should unite the computational universality properties of Neural Turing Machines and Neural GPU with good performance on disparate language and algorithmic tasks.\n\n(I have read your author feedback and have modified my rating according to my understanding.)\n\nReview:\nThe paper is well written and proofread, concrete and clear. The model is quite clearly explained, especially with the additional space of the supplementary material, appendices A and B (note fig 4 is less good quality than fig 2 for some reason) -- I\u2019m fine with the use of the Supp Mat for this purpose.\n \nThe experiments have been conducted well, and demonstrate a wide range of tasks, which seems to suggest that the UT has pretty general purpose. The range of algorithmic tasks is limited, e.g. compared to the NTM paper.\nI miss any experimental details at all on training.\nI miss a comparison to Neural GPU and Stack RNN in 3.1, 3.2.\n\nI miss a proof that the UT is computationally equivalent to a Turing machine. It does not have externally addressable, shared memory like a tape, and I\u2019m not sure how to transpose read/write heads either.\n\nThe argument that the UT offers a good balance between inductive bias and expressivity is weak, though it may be the best one can hope for of a statistical model in a way. I note that in 3.1, the Transformer overfits, while it seems to underfit in 3.3 (lower LM and RC accuracy, higher LM perplexity), while the UT fare well, which suggests that the UT hits the balance better than the Transformer, at least.\n\nFrom the point of view of network structure, it seems natural to lift further constraints on the model: \nwhy should width of intermediate layers be exactly equal to sequence length?\nwhy should all hidden state vectors be size $d$, the size of the embeddings chosen at the first layer, which might be chosen out of purely practical reasons like the availability of pre-trained word embeddings?\n\nWhat is the contribution of this work? It starts from the Transformer, the ACT idea for dynamic halting in recurrent nets, the need for models fit for algorithmic tasks. \nThe UT\u2019s building blocks are near-identical to the Transformers (and the paper is upfront and does a good job of explaining these similarities, fortunately)\n- cf eq1-5: residuals, multi-headed self attention, and layer norm around all this. \n- shared weights among all such units\n- encoder-decoder architecture\n- autoregressive decoder with teacher forcing\n- decoder units like the encoder\u2019s but with extra layer of attention to final output of encoder\n- coordinate embeddings\nThe authors may correct me, but I believe that the UT with FC layers is exactly identical to the Transformer described in Vaswani 2017 for T=6. \nSo this paper introduces the idea of varying T, interprets it as a form of recurrence, and adds dynamic halting with ACT to that. Interestingly, the recurrence is not over sequence positions here.\nThis contribution is not major, on the other hand the experimental validation suggests the model is promising.\n\nTypos and writing suggestions\nabove eq 8: masked such that -> masked so that\neq 8: dimensions of O and H^T are incompatible: d*V, m*d; to evacuate the notation issue for transposition, cf footnote 1, here and elsewhere, you could use either ${^t A}$ or $A^\\top$ or $A^\\intercal$. You could also write $t=T$ instead of just $T$.\nsec3.3 line -1: designed such that -> designed so that\nTowards the beginning of the paper, it may be useful to stabilise terminology for $t$: depth (as opposed to width for $m$), time steps, recurrence dimension, revisions, refinements\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}