{"title": "Solid and empirically promising model which merges Transformer and recurrent models but without strong intuitive or theoretical support to back up its claims.", "review": "This paper describes a transformer with recurrent structure to take advantage of self-attention mechanism. The number of recurrences can be dynamically determined through ACT-like halting depending on the difficulty of the input. A series of experiments on language modeling tasks have been demonstrated to show promising performances.\n\nThe overall concerns about this paper is that while the performances are quite promising, the theoretical claims and comparisons in the discussion section are of question. The authors attempt to provide connections to other networks (i.e., Natural GPU, RNN) since UT is an amalgamation of both transformers and RNN, they sound a little \u201chand-wavy\u201d (i.e., comments about UT effectively interpolating between the feed-forward, fixed-depth Transformer and a gated recurrent architecture). In short, while empirically completely acceptable, intuitively or theoretically it is hard to grasp why UT is superior other than the dynamic/sharing layers across t (not time). I believe that improving this aspect could make this paper even better. Based on the comments below and the responses with the authors, I am willing to improve my score.\n\nPros:\n1.\tThe best of both worlds from parallelizable transformer and recurrent structure for repeated self-attention mechanism. Essentially, the \u201cdepth\u201d of the transformer can vary if we \u201cunroll\u201d the recurrent stacks.\n\n2.\tExtensive experiments showing the performance of UT.\n\n3.\tAnalysis of the effect of the recurrent aspect of UT and how it can vary depending on the task difficulty.\n\nComments/cons:\n1.\tI am having trouble understanding the \u201cuniversal\u201d aspect of the transformer. Is this because the variability of the depth of UT (since \u201cgiven sufficient memory\u201d was mentioned)? If so, such characteristic of \u201ccomputational universality\u201d does not seem much unique to UT compared to infinite memory for a transformer or a simple RNN across stack (i.e., input is the while sequence and recurrent step is through the stack analogous to UT stack). Please comment on this.\n\n2.\tIt is nice to see many experiments, but without preexisting knowledge about the datasets and their tasks, I can only make relative judgements based on the provided comparisons against other methods. It would be nice to see slightly more detailed descriptions of each task (particularly LAMBADA LM), not necessarily in the main paper (due to space) but in the appendix if possible for improved self-containedness. \n\n3.\tIn the discussion, the crucial difference between UT and RNN is that RNN is stated to be that RNN cannot access memory in the recurrent steps while UT can. This seems to be the case for not just UT but any Transformer-type model by construction.\n\n4.\tThe authors stated that the \u201crecurrent step\u201d for RNN is through time (as the authors stated) while the \u201crecurrent step\u201d in UT is not through time. While this claim is completely correct itself, the RNN\u2019s inability to access memory in its \u201crecurrent steps\u201d was compared with how UT could still access memory throughout its \u201crecurrent steps\u201d. In this sense, we may argue that the UT cannot access memory across its own t (stacking across t). I am not sure if it is fair to make such implications by putting both \u201crecurrent steps\u201d to be of same nature and pointing out one\u2019s weakness. Perhaps the authors could comment on this.\n\nMinor:\n1.\tTable 2.: Best Stack-RNN for 1 attractor is the highest but not bold-faced.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}