{"title": "Well-motivated approach to an interesting problem", "review": "This paper is concerned with the problem of finding adversarial examples for an ensemble of classifiers. This is formulated as the task of finding noise vectors that can be added to a set of examples in such a way that, for each example, the best ensemble element performs as badly as possible (i.e. it\u2019s a maximin problem).\n\nThis is formulated as a two-player game (Equation 1), in which the above description has been relaxed slightly: Equation 1 seeks a *distribution* over noise vectors, instead of only one. This linearizes the game, so that we can seek a mixed Nash equilibrium. Given access to a best response oracle, Algorithm 1 results in such a mixed Nash equilibrium. This is pretty standard stuff (see e.g. \u201cRobust Optimization for Non-Convex Objectives\u201d in NIPS\u201917, or \u201cA Reductions Approach to Fair Classification\u201d in ICML\u201918), but the application of this approach to this problem is novel and interesting.\n\nIn Section 2.1, the authors seek to show that they can get provable guarantees for *linear* classifiers, provided that there exists a \u201cpure strategy Nash equilibrium\u201d, which is a set of noise vectors for which *every* classifier misclassifies *every* example. These conditions seem to me to be so strong that I\u2019m not sure that this section is really pulling its weight.\n\nOn the subject of Section 2.1, the authors might consider whether an analysis based on \u201cTwo-Player Games for Efficient Non-Convex Constrained Optimization\u201d (on arXiv) could be used here: convert Equation 1 into a constrained optimization problem by adding a slack variable, then reformulate it as a non-zero-sum game, in which one player uses the zero-one loss, and the other uses e.g. the hinge loss.\n\nWhile Algorithm 1 makes an unrealistic oracle assumptions, and I didn\u2019t find Section 2.1 fully satisfying, I think that overall the theoretical portion of the paper is sufficiently convincing that one should be surprised if their experiments don\u2019t show good performance (which they do--extremely good performance, in fact). Overall, this is an interesting and important problem, and a well-motivated approach that seems to work well in practice. I think Section 2.1 is a bit weak, but this is a relatively minor issue.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}