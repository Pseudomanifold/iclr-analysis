{"title": "Interesting and well-written paper but insufficient motivation", "review": "Summary: The authors provide a method to attack multiple classifiers, with the key insight that it is insufficient to attack a simple average of the multiple classifier outputs; creating adversarial examples which can fool each classifier independently leads to more success in attacking any defenses that has access to multiple classifiers. Note that white-box access to all the classifiers is assumed. \n\nClarity: Paper is well written and claims are clear and substantiated. \n\nOriginaility: The paper's technical contribution seems limited. They suggest performing PGD to estimate the best response, which is similar to previous work. However, the authors do multiple rounds of this, with different weights on the multiple classifiers at each step. \n\nConcerns: \n(1) Ensembles have mostly been proposed for black-box attacks. The setting where there are multiple classifiers and all of these weights are accessible to the attacker seems unrealistic. What's the advantage for a defense to commit to a set of trained classifiers before hand? \n\n(2) Security concerns aside; it is not surprising that it's possible to find an attack that works for multiple classifiers at the same time, and I believe this has been done in prior work. The theoretical contribution is limited and the technique proposed is just a small modification of existing gradient based algorithms.\n\n(3) The experimental evaluation is against previous work which tried to solve a different problem (black box based attacks).  Hence, they are not convincing. \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}