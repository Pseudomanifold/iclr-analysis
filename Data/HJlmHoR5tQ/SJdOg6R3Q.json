{"title": "Good empirical results, but overall lacking in clarity with potentially problematic issues", "review": "Summary/Contribution:\nThis paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.\n\nPros:\n    - The approach outperform AIRL by a convincing margin on the crippled ant problem, while obtaining comparable/favorable performance on other benchmarks.\n\nCons:\n    - The justification for using the empowerment maximization framework to learn the shaping parameters is unclear. The formulation introduces a potentially confounding factor by biasing the policy optimization which clouds the experimental picture. \n\nJustification for rating:\nThis paper presents good empirical results, but without a clear identification of the source of improvement. I lean on the side of rejecting unless the authors can better eliminate any potential bias in their formulation (see question below). The justification for combining the empowerment maximization objective is also unclear while being integral to the novelty of the proposed method. \n\nQuestions I could not resolve from my reading:\n    - The \"imitation learning benchmark\" numbers in Table 2 are different from the original AIRL paper. Do the authors have an explanation as to why? Is this only due to a difference in the expert performance?\n    - Can the authors confirm that in the transfer experiments, the policy is optimized with only the transfered reward and no empowerment bonus? Otherwise, can the authors comment on whether the performance benefits could be explained by the additional bonus.\n    - In equation (12), \\Phi is optimized as an (approximate) mutual information, not a value function, so it is not clear why this term approximates the advantage (I suspect this is untrue in EAIRL as V* is recovered at optimality in the AIRL/GAN-GCL formulation). Can the authors comment?\n    - Why is w* unnormalized? Unless I am misunderstanding something, in the definition immediately above it, there is a normalization term Z(s). \n\nOther comments:\n    - \"AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near optimal rewards function\" -> This characterization is strange since on some tasks AIRL(s,a) outperforms or is within one standard deviation of EAIRL (e.g. on Half Cheetah, Ant, Swimmer, Pendulum).\n    -  \"Our experimentation highlights the importance of modeling discriminator/reward functions.. as a function of both state and action\". AIRL(s) is better on both the pointmass and crippled-ant task than AIRL(s,a). Can the authors clarify?\n    - \"Our method leverages .. and therefore learns both reward and policy simultaneously\". Can the authors clarify in what sense the reward and policy is being learned simultaneously in EAIRL where it is not in AIRL?\n    - In all the tables, the authors' approach is bolded as oppose to the best numbers. I would instead prefer that the authors bold the best numbers to avoid confusion.\n\n- Typos:\n    - \"the imitation learning methods were proposed\"\n    - \"quantify an extent to which\" \n    - \"GAIL uses Generative Adversarial Networks formulation\"\n    - \"grantee\"\n    - \"no prior work has reported the practical approach\"\n    - \"but, to\"\n    - \"(see (Fu et al., 2017))\"\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}