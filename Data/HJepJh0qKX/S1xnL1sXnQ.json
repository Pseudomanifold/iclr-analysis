{"title": "An experimental paper with many problems in the setting and analysis", "review": "This paper proposes a specific measure of difficulty for training examples called \u201ceasiness\u201d. Easiness is based on training the model N times and counting the number of times an example is classified correctly. Based on this measure, they introduce \u201cmatching rate\u201d as a measure of similarity of two architectures. Two architectures are suggested to be similar if the set of easy and hard examples is similar. The rest of the paper presents comparisons of architectures. Considering the problems below, I don\u2019t see any reliable contribution in this paper.\n\n- Why this specific definition of easiness? Can you compare to simply using \u201closs\u201d as a measure for the difficulty of an example?\n- e_t seems to be measuring the variance of training on a single example. If there is only one example that is always classified correctly, the denominator can be simplified to K. It doesn\u2019t tell us how many training iterations it takes to fit that example.\n- Why this specific formulation for \u201cmatching rate\u201d? Why not a more common measure of similarity between sets such as intersection over union (IoU)? Can you suggest any references using a similar similarity score?\n- Numbers in Table 1 do not seem particularly big to support the claim in section 4 that \u201c...CNNs start learning from the *same* examples even if CNN architectures are different\u201d. 0.20 is definitely bigger than random 0.1 for the matching rate but it still means only a 20% match.\n- Random 0.1 is redundant in table 1.\n- In section 4, define \u201ccontradicted patterns\u201d.\n- Are all images in Figure 1 for one model? How does it compare to visualizing examples according to their loss?\n- The conclusion in section 5 says \u201c... different CNNs start learning from similar patterns\u201d. As mentioned above, \u201ceasiness\u201d and consequently \u201cmatching rate\u201d do not provide information about the progress of training and only final trained models. Regardless, this conclusion does not seem particularly unexpected or informative.\n- Section 6 proposes to test a model on data with a different structure from data provided in training. This is a distribution mismatch and the model is not trained to handle.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}