{"title": "Good technical report of applying decomposition methods to weights of LSTM. But lack of originality and significance. ", "review": "\n[PROS]\n\n[quality]\n\nThe paper did extensive experimental studies on compressing weights of LSTM with two matrix decomposition methods (SVD and NMF). Their experiments cover three NLP downstream tasks and two benchmark datasets, and performed the decomposition methods on state-of-the-art language models. They explored effects of different methods on different weights of LSTM, and also examined how the results are influenced by different norms (used in the objective function of decomposition method). The experimental studies are very solid and convincing---they can well support their claims. \n\n[clarity]\n\nThe paper is clearly-written and has a very detailed appendix.  \n\n[CONS]\n\n[originality]\n\nOne major weakness of this paper is its lack of originality. The paper directly applied two well-known and popularly used methods---SVD and NMF---to the weights of LSTM, but did not provide any insights on the fitness of these methods. For example, what the non-negativity means in the case of LSTM weights? \n\nMoreover, why LSTM? The way of applying these methods in this paper has nothing to do with LSTM\u2019s unique properties that distinguish it from other neural models. None of the experimental findings seem tied to any of LSTM\u2019s properties. We can always expect a speed-up in runtime but a drop in performance after applying these methods to another neural model. As long as we still maintain certain rank, the performance will not drop much. The effect of using different norms seems only dependent on the properties of the norms and the matrices, but not anything related to the structure of a LSTM. \n\n[significance]\n\nAs mentioned in [quality], the experiments are extensive and results are convincing. But the results are largely predictable as discussed in [originality], and not necessarily tied to LSTM. That being said, this submission is certainly a really high-quality technical report, but does not seem significant enough to be accepted to ICLR. \n\n[questions for authors]\n\nWhy are experiments only performed on CPUs but not GPUs? I understand that the time complexity does not depend on the device, but the actual runtime does---because of massive cores in GPUs. So when people apply large RNNs like ELMO, they would prefer using GPUs. So do methods proposed in this work also provide any edge in that case? ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}