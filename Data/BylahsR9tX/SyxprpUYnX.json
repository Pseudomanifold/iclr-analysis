{"title": "Detailed experiments and nice work but it seems the idea is not new", "review": "This paper focuses on compressing pretrained LSTM models using matrix factorization methods, and have detailed analysis on compressing different parts of LSTM models.\n\nIt seems the idea is very similar to [1]. Both use matrix factorization to compress pretrained RNN models. The method is not exactly the same and the experiments are different tasks. It would be nice if it could be added as a baseline.\n\nThe LSTMP model proposed in [2] also uses matrix factorization as a method to do LSTM model compression and speedup. The difference here is that this paper focuses on compression of a pretrained model. However, when fine-tuning is used, the difference becomes less.  So I think for comparison, another baseline is needed, the LSTM model with projection layer that is trained from scratch. \n\n[1] Prabhavalkar, Rohit, et al. \"On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition.\" arXiv preprint arXiv:1603.08042 (2016).\n[2] Sak, Ha\u015fim, Andrew Senior, and Fran\u00e7oise Beaufays. \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling.\" Fifteenth annual conference of the international speech communication association. 2014.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}