{"title": "Interesting exploratory research, some more examples are desired", "review": "This paper investigates internal working of RNN, by mapping its hidden states\nto the nodes of minimal DFAs that generated the training inputs and its \nabstractions. Authors found that in fact such a mapping exists, and a linear\ndecoder suffices for the purpose. \nInspecting some of the minimal DFAs that correspond to regular expressions, \ninduced state abstractions are intuitive and interpretable from a viewpoint of\ntraining RNNs by training sequences.\n\nThis paper is interesting, and the central idea of using formal languages to\ngenerate feeding inputs is good (in fact, I am also doing a different research\nthat also leverages a formal grammar with RNN).\n\nMost of the paper is clear, so I have only a few minor comments:\n\n- In Figures 4 and 5, the most complex MDFA of 14 nodes does not have the\n  lowest testing accuracies. In other words, testing accuracies is not\n  generally proportional to the complexity of MDFA. Why does this happen?\n\n- As noted in the footnote in page 5, state abstraction is driven by the idea\n  of hierarchical grammars. Then, as briefly noted in the conclusion, why not\n  using a simple CFG or PCFG to generate training sequences? \n  In this case, state abstractions are clear by definition, and it is curious\n  to see if RNN actually learns abstract states (such as NP and VP in natural\n  language) through mapping from hidden states to abstracted states.\n\n- Because this paper is exploratory, I would like to see more examples\n  beyond only the two in Figure 6. Is it possible to generate a regular \n  expression itself randomly to feed into RNN?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}