{"title": "Interesting idea, serious clarity problems", "review": "This paper aims to show that an RNN trained to recognize regular languages effectively focuses on a more abstract representation of the FSA of the corresponding language. \n\nUnderstanding the type of information encoded in the hidden states of RNNs is an important research question. Recent results have shown connections between existing RNN architectures and both weighted (e.g., Chen et al., NAACL 2018, Peng et al., EMNLP 2018) and unweighted (Weiss et al., ACL 2018) FSAs. This paper asks a simple question: when trained to recognize regular languages, do RNNs converge on the same states as the corresponding FSA? While exploring solutions to this question is potentially interesting, there are significant clarity issues in this paper which make it hard to understand it. Also, the main claim of the paper \u2014 that the RNN is focusing on a low level abstraction of thew FSA \u2014 is not backed-up by the results.\n\nComments:\n\n\u2014 The authors claim that the RNN states map to FSA states with *low* coarseness, but Figure 3b (which is never referred to in text\u2026) shows that in most cases the ratio of coarseness is at least 1/3, and in some cases > 1/2. \n\n\u2014 Clarity:\nWhile the introduction is relatively clear starting from the middle of section 3 there are multiple clarity issues in this paper. In the current state of affairs it is hard for me to evaluate the full contribution of the paper.\n\n- The definitions in section 3 were somewhat confusing. What is the conceptual difference between the two accuracy definitions? \n\n- When combining two states, does the new FSA accept most of the strings in the original FSAs? some of them? can you quantify that? Also, figure 6 (which kind of addresses this question) would be much more helpful if it used simple expressions, and demonstrated how the new FSA looks like after the merge.\n\n- section 4 leaves many important questions unanswered:\n1. Which RNN was used? which model? which parameters? which training regime? etc.\n2. How were the expressions sampled? the authors mention that they were randomly sampled, so how come they talk about DATE and EMAIL expressions?\n3. What is the basic accuracy of the RNN classifier (before decoding)? is it able to learn to recognize the language? to what accuracy? \n\n- Many of the tables and figures are never referred to in text (Figure 3b, Figure 5)\n\n- In Figure 6, there is a mismatch between the regular expression (e.g., [0-9]{3}\u2026.) and the transitions on the FSA (a-d, @).\n\n- How come Figure 3a goes up to 1.1? isn\u2019t it bounded by 1? (100%?)\n\n- The negative sampling procedure should be described in the main text, not the appendix. Also, it is not clear how come shuffling the characters is considered an independent distribution.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}