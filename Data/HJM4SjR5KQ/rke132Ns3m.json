{"title": "A method to model multiple sequential data sources, with interconnections among them obeying a weighted undirected graph. The paper is not well organized and the model description is not convincing. Potentially important empirical exploration is missing.", "review": "The authors propose a method to model sequential data from multiple interconnected sources using a mixture of common pool of HMM's. The composition of the mixture for each data source is both sparse and similar to that of other data sources that are close to it, with the closeness determined by the weights of the edges of an undirected graph.\n\nThe introduction section is unfocused and sloppy. HMM's are well understood models in machine learning but the paper falls short in explaining the particular distributed scenario of interest. The narrative jumps from sports, to neuroscience to wireless communications to fMRI,  without mentioning the common denominator. The proposed model section lacks also some focus, jumping from distributed sparse representations to multitask learning. The key concept here seems to be the poorly defined concept of 'a sparse HMM mixture over a large dictionary of HMM's'. The formalization of this rather complicated object is not well justified and leaves a lot of guesswork to the reader. \n\nInstead of maximizing the likelihood, an alternative objective function (4) is proposed as maximizing the inner products of posterior probability vectors at each node. The authors probably try to say something sensible but the sloppy notation is not very helpful here.   \n\nThe way authors introduced the graph structure into their cost function creates potentially a flexibility. However, the authors could have spent more energy on explaining why sparseness of the mixtures should be a desirable property for the problems they hope to solve with the model. The graph structure could potentially be used without necessitating sparsity, so opting for sparsity needs to be justified.\n\nOne also suspects that the authors could have written a clearer generative model instead of modifying the maximum likelihood criterion for learning. This would have enabled the readers to appreciate the generative model better.\n\nMoreover, the parameter $\\lambda$ controls how much effect the graphical interrelations is to have on the final learned parameters of the model. The authors however do not present a detailed examination of empirical results of varying $\\lambda$, and only suffice to determine it with cross-validation. A more interpretive stance towards lambda would confer increased understanding of how sparsity functions in this model. The cluster analysis at the end of the experiments indeed provide a more tangible demonstration of how sparsity affects the results obtained and why it might be desirable.\n\nOverall, the paper is not of sufficient quality to be presented at ICLR.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}