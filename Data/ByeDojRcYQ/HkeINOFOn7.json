{"title": "Interesting paper, some concerns with formalism and objective. Missing baselines. ", "review": "This paper proposes a distributed policy gradient method for learning policies with large, collaborative, homogeneous swarms of agents. \n\nFormalism / objective: \nThe setting is introduced as a \"collaborative Markov team\", so the objective is to maximise total team reward, as expressed in equation (3). This definition of the objective seems inconsistent with the one provided at line (14): Here the objective is stated as maximising the agent's return, L_n, after [k] steps of the agent updating their parameters with respect to L_n, assuming all other agents are static. I think the clearest presentation of the paper is to think about the algorithm in terms of meta-learning, so I will call this part the 'inner loop' from now on. \nNote (14) is a very different objective: It is maximising the return of an agent optimising 'selfishly' for [k] steps, rather than the \"collaborative objective\" mentioned above. This seems to break with the entire premise of collaborative optimisation, as it was stated above. \nMy concern is that this also is reflected in the experimental results: In the food gathering game, since killing other agents incurs \"a small negative reward\", it is never in the interest of the team to kill other team-mates. However, when the return of individual agents is maximised both in the inner loop and the outer loop, it is unsurprising that this kind of behaviour can emerge. Please let me know if I am missing something here. \n\nOther comments: \n-The L_n(theta, theta_n) is defined and used inconsistently. Eg. compare line (9), L_n(theta_n, theta), with line below, L_n(theta, theta_n). This is rather confusing \n-In equation (10) please specific which function dependencies are assumed to be kept? My understanding is that \\theata_n is treated as a function of theta including all the dependencies on the policies of other agents in the environment? \n-Related to above, log( pi_\\theta_n ( \\tan_n)) in line 16 is a function of all agents policies through the joint dependency on \\theta. Doesn't that make this term extremely expensive to evaluate? \n-Why were the TRPO_kitchensink and A3C_kitchensink set up to operate on the minimum reward rather than the team reward as it is defined in the original objective? It is entirely possible that the minimum reward is much harder to optimise, since feedback will be sparse. \n-The survival game uses a discrete action space. I am entirely missing MARL baseline methods that are tailored to this setting, eg. VDN, QMIX, COMA etc to name a few. Even IQL has not been tried. Note that MADDPG assumes a continuous action space, with the gumble softmax being a common workaround for discrete action spaces which has not been shown to be competitive compared to the algorithms mentioned above. \n-Algorithmically the method looks a lot like \"Learning with Opponent Learning Awareness\", with the caveat that the return is optimised after one step of 'self-learning' by each agent rather than after a step of 'Opponent-learning'. Can you please elaborate on the similarity / difference? \n-Equation (6) and C1 are presented as contributions. This is the standard objective that's commonly optimised in MARL when using parameter sharing across agents.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}