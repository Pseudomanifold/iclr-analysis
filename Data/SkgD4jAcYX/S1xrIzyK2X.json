{"title": "Incremental idea; problems in defining the layer; lack of analysis; weak baselines and marginal improvements", "review": "This paper presents adaptive ReLUs and a convolutional setup termed ConvReLU, which aims to alleviate the dying Relu problem.  To reduce the number of newly introduced parameters by their activation function and avoid overfitting, ConvReLU utilizes a parameter sharing scheme.  Three image classification and three language classification datasets are used to validate the proposed method. \n\nIn general, this paper is well-written and easy to follow. However,  the presented idea of adaptive ReLUs is somewhat trivial.  \nOne main concern is that the paper lacks some theoretical analysis and there is no any experiment analysis to backup authors' claim on that the proposed activation is better in preventing the problem of dying of ReLu.   At least, authors need to qualitative demonstrate the idea by visualizing the learned filters among different activations.  Deeper discussions on why the proposed activation function is better than the existing ones are needed. \n\nIn addition,  the ConvReLU has some flaws.  The way that authors define a \"layer\" is strange.  Conventionally, the last component of a Conv layer is the activation/nonlinearity.   However, the first component of a ConvReLU is the AdaReLU function.   At first glance, it seems two setups are equivalent.  However, due to the parameter sharing scheme, I don't know how authors define the values of post-activation (the output after nonlinearity).   Authors also need to provide details on how the ConvReLU is implemented.\n\nThe experiment results are not convincing.  Although authors compare their performances to different activation on six datasets, the improvement is very marginal.  Since the proposed ConvReLU introduces new parameters, it is hard to tell, the improved performances come from the method itself or the increasing of the number of parameters.  Moreover, authors also need to report the performance average over multiple runs and the standard division.  This will help to make sure the improvement does not come from the different initialization.   \n\nFor the text classification task, authors need to justify the reason for choosing the VGG network structure?   The reported performance is even worse than state-of-the-arts three years ago.  For instead, please see the paper \"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim from ACL 2014.  The performance of a non-static CNN achieves 81.5 while the best-reported performance here is 80.39.   Similarly, please see \"Character-level Convolutional Networks for Text Classification\" for AG and Yelp.   \n\nFurthermore, the setting of the partial replacement strategy is also strange.   I am not persuaded that by replacing the activation of the first dense layer only can lead to a consistent performance gain.    Authors also need to report the speed of training of their method compared regular ReLU.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}