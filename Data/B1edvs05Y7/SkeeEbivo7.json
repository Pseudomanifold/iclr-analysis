{"title": "Good results, however, I have questions about the algorithm", "review": "In the paper, the authors combine the federated method, sparse compression, quantization and propose Sparse Binary Compression method for deep learning optimization.  Beyond previous methods, the method in this paper achieves excellent results in the experiments. The paper is written very clearly and easy to follow. \n\nThe following are my concerns,\n1. In the introduction, the authors emphasize that there is a huge compression in the upstream communication. How about the downstream communication, I think the server should also send gradients to clients. The averaged gradient is not compressed anymore. \n\n2. I think the method used in the paper is not federated learning. Federated learning averages the models from multiple clients. however, in the paper, the proposed methods are averaging gradients instead. It is called local updates, and is a well-known tradeoff between communication and computation in the convex optimization.\n\n3. I want to point out that the similar local update (federated learning) technique has already explored, and proved not work well. In [1] the authors showed that deploying the local update simply may lead to divergence. Therefore, the iterations of the local update are constrained to be very small. e.g. less than 64.  Otherwise, it leads to divergence. I also got similar results in my experience.  The temporal sparsity in the paper looks very small. I am curious about why it works in this paper.\n\n4. Another issue is the results in the experiments. It is easy to find out that resnet50 can get 76.2% on Imagenet according to [2]. However, the baseline is 73.7% in the paper.  I didn't check the result for resnet18 on cifar10 or resnet34 on cifar 100, because people usually don't use bottleneck block for cifars.\n\n5. In Table 2, Federated average always has worse results than other compared methods. Could you explain the reason?  If using federated average is harmful to the accuracy, it should also affect the result of the proposed method. \n\n\n[1] Zhang, Sixin, Anna E. Choromanska, and Yann LeCun. \"Deep learning with elastic averaging SGD.\" Advances in Neural Information Processing Systems. 2015\n[2]https://github.com/D-X-Y/ResNeXt-DenseNet", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}