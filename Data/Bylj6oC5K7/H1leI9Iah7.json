{"title": "An interesting approach, not sure how significant.", "review": "The paper studies how logit regularization techniques can affect adversarial robustness. The authors study the recently proposed technique of adversarial logit pairing (ALP) and attempt to understand the different effects it has on the resulting model and how each of them can affect robustness. They argue that a significant part of ALP's success stems from regularizing the logits and that alternative logit regularization methods can lead to similar robustness. Finally, they propose an alternative to ALP that explicitly disentangles logit similarity and logit regularization to train models that are claimed to be more robust than state-of-the-art models.\n\nBefore continuing with my review, I would like to note that the effectiveness of ALP in increasing robustness is still being debated. The robustness of ALP models has not been verified by a third party. The models released by the authors were shortly found to vulnerable to simply running a standard attack for more steps (https://arxiv.org/abs/1807.10272). The ALP authors claim that this is due to releasing a smaller model for a competition but they still have not released another model that is claimed to be robust. Nevertheless, I believe that this situation should not affect how the current paper is reviewed. The idea of regularizing the logits is natural and precisely understanding its effect on the model is a valuable research direction.\n\nOverall I like the idea of the paper. I agree that the effect of logit regularization methods on the model is unclear and attempting to dissect these techniques is a very important research direction. On the other hand, this is a very tricky area to perform such experiments. Accurately measuring the robustness of a model is a challenging task. It is therefore hard to understand if an increase in adversarial accuracy is due to a true increase in the robustness of a model or due to the attack being used performing sub-optimally when the setting is modified.\n\nMost of the results of the paper revolve around rather fine-grained differences in the adversarial accuracy of <5% (e.g. Figure 1, Table 1). Such small differences might in fact be artifacts of the PGD attack used performing sub-optimally. Moreover, for some of the results, it is clear that they are indeed an artifact of the attacks performing sub-optimally. For instance the left of Figure 2, label smoothing is reported to achieve high accuracy again PGD yet in Section 5.3 it is reported that more steps can reduce the accuracy to <15%. It is thus clear that the claim about label smoothing increasing robustness is misleading.\n\nI believe it is essential that the authors validate theirs results, at the very least Table 1, by evaluating the models on different attacks.\nGiven that the models regularizes the logits, evaluating an attack optimizing the logits directly (e.g. the CW attack, https://arxiv.org/abs/1608.04644) is essential. Moreover, given that the gradients of the loss appear to not be informative, an attack based on finite differences is also crucial here (see SPSA attack of https://arxiv.org/abs/1802.05666).\n\nMoreover, I find the reasoning behind why should logit regularization techniques increase robustness very limited. The change in logit distributions (Figure 1,2) are rather marginal and do not suggest fundamental differences of the underlying models.\n\nI believe that additional experiments and justifications of the results are needed before the paper is considered for acceptance. I thus recommend rejection at this time.\n\n[UPDATE after reading the public discussion]: It appears the someone already suggested running SPSA as an attack. I really appreciate the authors evaluating the attack and reporting their findings. Indeed it seems that the reported robustness from logit regularization does not stand up to an SPSA attack. As a result, I think it is fairly clear that logit regularization has a limited effect on the robustness of the model and is only making PGD attacks less effective. I update my score to a clear reject.\n\nMinor comments to authors:\n-- I would suggest remove the sentence close to the end of the intro about ALP being perhaps the most robust defense. As I outlined above, the robustness of ALP has not been convincingly verified yet. Similarly for the text above equation 2. Perhaps most importantly, the last paragraph before Section 4 where you state that \"...ALP _works_...\" in a very definitive tone.\n-- Paragraph after (5): the logits of the other classes are not guaranteed to increase. Some of them might decrease in the process of making the loss as large as possible.\n-- I like equation (10) where you attempt to decompose that effect of similarity and regularization. It would be interesting to understand the interplay between lambda and beta.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}