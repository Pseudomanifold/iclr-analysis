{"title": "Performs worse than adversarial training", "review": "This paper presents a new adversarial defense based on \"cleaning\" images using a round trip through a bidirectional gan.  Specifically, an image is cleaned by mapping it to latent space and back to image space using a bidirectional gan.  To encourage the bidirectional gan to focus on the semantic properties, and ignore the noise, the gan is trained to maximize the mutual information between z and x, similar to the info gan.\n\nPros:\n\t1. The paper presents a novel (as far as I am aware) way to defend against adversarial attacks by cleaning images using a round trip in a bidirectional gan\n\nCons:\n\t1. The method performs significantly worse than existing techniques, specifically adversarial training.\n\t\ta. The authors argue \"Although better than FBGAN, adversarial training has its limitation: if the attack method is harder than the one used in training(PGD is harder than FGSM), or the perturbation is larger, then the defense may totally fail. FBGAN is effective and consistent for any given classifier, regardless of the attack method or perturbation.\"\n\t\tb. I do not buy their argument, however, because one can simply apply the strongest defense (PGD 0.3 in their results) and this outperforms their method in *all* attack scenarios.  And if someone comes out with a new stronger attack there's no guarantee their method will be strong defense against that method\n\t2. The paper is not written that well.  Even though the technique itself is very simple, I was unable to understand it from the introduction, and didn't really understand what they were doing until I reached the 4th page of the paper. \n\t\n\nMissing citation:\nPixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples  (ICLR 2018)\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}