{"title": "review on \"Learning Multimodal Graph-to-Graph Translation for Molecule Optimization\"", "review": "This paper proposed an extension of JT-VAE [1] into the graph to graph translation scenario. To help make the translation model predicting diverse and valid outcomes, the author added the latent variable to capture the multi-modality, and an adversarial regularization in the latent space. Experiment on molecule translation tasks show significant improvement over existing methods.\n\nThe paper is well written. The author explains the GNN, JT-VAE and GAN in a very organized way. The idea of modeling the molecule optimization as translation problem is interesting, and sounds more promising (and could be easier) than finding promising molecule from scratch. \n\nTechnically I think it is reasonable to use latent variable model to handle the multi-modality. Using GAN to align the distribution is also a well adapted method recently. Thus overall the method is not too surprising to me, but the paper executes it nicely. Given the significant empirical improvement, I think this paper has made a valid contribution to the area.\n\nRegarding the results in Table 1, I\u2019m curious why the VSeq2Seq is better than JT-VAE and GCPN (given the latter two are the current state-of-the-art)? \n\nAnother thing I\u2019m curious about is the \u2018stacking\u2019 of this translation model. Suppose we keep translating the molecule X1 -> X2 -> X3 ...  using the learned translation model, would the model still gets improvement after X2? When would it get maxed out?\nOr if we train with \u2018path\u2019 translation (i.e., train with improvement path with variable length), instead of just the pair translation, would that be helpful? I\u2019m not asking for more experiments, but some discussion might be useful.\n\n[1] Jin et.al, Junction tree variational autoencoder for molecular graph generation, ICML 2018\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}