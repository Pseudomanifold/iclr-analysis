{"title": "Interesting paper", "review": "The paper studies the problem of representation learning in the context of hierarchical reinforcement learning by building on the framework of  HIRO (Nachum et al. (2018)). The papers propose a way to handle sub-optimality in the context of learning representations which basically refers to the overall sub-optimality of the entire hierarchical polity with respect to the task reward. And hence, the only practical different from the HIRO paper is that the proposed method considers representation learning for the goals, while HIRO was directly using the state space.\n\n\nI enjoyed reading the paper. The paper is very *well* written. \n\nExperimental results:  The authors perform the series of experiments on various high dimensional mujoco env, and  show that the representations learned using the proposed method outperforms other methods (like VAEs, E2C etc), and can recover the controllable aspect of the agent i.e the x, y co-ordinate. This is pretty impressive result.\n\nSome questions:\n\n[1] Even though the results are very interesting, I'm curious as to how hard authors try to fit the VAE baseline. Did authors try using beta VAEs (or variants like InfoVAE) ?  Since the focus of the entire paper is about representation learning (as well as the focus of the conference), it is essential to make sure that baselines are strong. I would have suspected that it would have been possible to learn x,y co-ordinate in some cases while using improved version of VAE like beta VAE etc.\n\n[2] One of the major intuitions behind sub-optimality is to learn representations that can generalize, as well as can be used for continual learning (or some variant of it!). This aspect is totally missing from the current paper. It would be interesting to show that the representations learned using the proposed method can transfer well to other scenarios or can generalize in the presence of new goals, or can be sample efficient in case of continual learning. \n\nI think, including these results would make the paper very strong.  (and I would be happy to increase my score!).\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}