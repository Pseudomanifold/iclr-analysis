{"title": "Simple modification with relatively robust gains", "review": "The authors in this work present an approach to policy optimization that relies on an alternative policy formulation based on normalizing flows. This is a relatively simple modification (this is no criticism) that essentially uses the same TRPO algorithm as previous approaches, but a different mechanism for generating the distribution over actions. The crux of the authors\u2019 approach is detailed in equations (6) and (7), although it could have been useful to see more of the discussion of the architecture from appendix B in the actual text of the paper.\n\nThe authors then go on to analyze the properties and expressiveness of the resulting properties and show that it is more capable of capturing complex interactions than a simple Gaussian. It was somewhat unclear, however, in section 4.2 what the exact form of the policies being compared are. Is this a simple example with only the parameters of the Gaussian, or was the Gaussian parameterized by a multi-layer model? Further, one thing I would also have liked to see the authors question more is, for the problems they attack, whether this expressiveness is more useful \u201cduring exploration\u201d or for the ultimate performance of the final policy.\n\nThe authors, finally, show that this approach is able to out-perform the alternative Gaussian policy. Ultimately this approach seems to be a simple modification (or replacement) of the standard policy formulation, and one that seems to lead to good performance gains. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}