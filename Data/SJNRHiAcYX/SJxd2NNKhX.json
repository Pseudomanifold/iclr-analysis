{"title": "simple idea but not totally clear", "review": "This paper generalizes basic policy gradient methods by replacing the original Gaussian or Gaussian mixture policy with a normalizing flow policy, which is defined by a sequence of invertible transformations from a base policy.\n\nAlthough the concept of normalizing flow is simple, and it has been applied to other models such as VAE, there seems no work on applying it for policy optimization. Thus I think this method is itself interesting.\n\nHowever, I find the paper written in a way assuming readers very familiar with related concept and algorithms in reinforcement learning. Thus although one can get the general idea on how the method works, it might be difficult to get a deeper understanding on some details.\n\nFor example, normalizing flows are defined in Section 4, and then it is directly claimed that normalizing flows can be applied to policy optimization, without giving details on how it is actually applied, e.g., what is the objective function? and why one needs to compute gradients of the entropy (Section 4.1)?\n\nAlso, in the experiments, it is said that one can combing normalizing flows with TRPO without describing the details. I can't get how exactly normalizing flows + TRPO works.\n\nThe experiments also talk about 2D bandit problem, and again, without any descriptions. BTW, in the Section 4.3, what does [-1, 1]^2 mean? (I have seen {-1, 1}^2, but not [-1, 1]^2).\n\nIt seems that the authors only use the basic normalizing flow structures studied in Rezende&Mohamed (2015) and Dinh et al (2016). However, there are more powerful variants of normalizing flows such as the Multiplicative Normalizing Flows or the Glow. I wonder how good the results are if these more advanced versions are used. Maybe they can uniformly outperform Gaussian policy?\n\nUpdate:\nI feel the idea of this paper is straightforward, and the contribution is incremental. To improve the paper, stronger experiments need to be performed. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}