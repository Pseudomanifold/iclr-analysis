{"title": "Poor novelty, clarity and significance", "review": "This paper proposed a new regularization for semi-supervised learning called tangent-normal adversarial regularization. It can be decomposed into two parts, namely the tangent adversarial regularization and the normal adversarial regularization. Both of them are related to the smoothness assumption in a more general form.\n\nThe novelty seems really incremental as a combination of two existing concepts and thus there is no clear conceptual improvement. The two parts as the building blocks are even already included in the deep learning book.\n\nThe clarity is OK but not so good. The introduction is very long but not so informative. In fact, the description and discussion are even misleading. The first paragraph gives the background information of SSL in 12 lines without a single citation. I think a minimal background is necessary, while this paragraph is too much in such a way that it gives something every body knows and is a waste of the most precious space.\n\nThe 3 streams of SSL are misleading: the 1st and 3rd are traditional generative methods and GANs which overlap a lot, and some important streams such as co-training are completely ignored. Even though the paper is claimed to focus on the 2nd stream, i.e., regularization, it focused only on the geometric regularization and ignored any other regularization for instance entropy regularization (by Prof. Bengio!) and expectation regularization (by Prof. McCallum!) that belong to information-theoretic regularization.\n\nThe 3 assumptions are also strange. To me, there is 1 assumption and 2 implementations of that assumption. As far as I know, the basic assumptions in SSL are all based on 3 concepts of consistency originated from LGC and VAT. Since SSL has a long history and is/was a very hot topic, the authors should survey the literature more carefully.\n\nThe significance is even not OK. As claimed in the paper it focuses on the 2nd stream, the proposed method should mainly be compared with similar regularization approaches. VAT is the origin of perturbation consistency but no longer SOTA. There are many following methods, temporal ensembling from ICLR 2017, mean teacher from NIPS 2017, smooth neighbor teacher graph from CVPR 2018, and compact clustering via label propagation from ICML 2018, just to name a few. Again, the authors should survey the literature more carefully and then consider how to revise their paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}