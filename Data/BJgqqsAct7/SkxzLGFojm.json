{"title": "Good paper provided the authors have answers to some technical questions.", "review": "This paper gives the first nonvacuous generalization bounds for\nmeaningful Imagenet models.  These bounds are given in terms of the\nbit length of compressions of learned models together with a method\nfor taking into account symmetries of the uncompressed parameters.\n\nThese bounds are nonvacuous only when the compressed models are small\n--- on the order of 500 Kilobytes.  State of the art compressed models\nof this size achieve Imagenet accuracies slightly better than Alexnet,\n16% error for top 5, and this paper reports a nonvacuous\ngeneralization guarantees of 89% error for top 5.  While there is\nstill a large gap between the actual generalization and the guarantee,\nthis would still be a significant accomplishment.\n\nI have one major concern.  The generalization bound involves adding an\nempirical loss and a regularization term computed from a KL\ndivergence.  I am convinced that the authors have correctly handles\nthe KL divergence term.  But the paper does not contain sufficient\ndetail to determine if the authors correctly handle the empirical loss\nterm.  It is NOT correct to use the training loss of the\n(deterministic) compressed model.  The generalization bound requires\nthat the training loss be measured under the parameter noise of the\nposterior distribution.  The paper needs to be clear that this has\nbeen done. The comments in Appendix B on noise robustness are\ndisturbing in this regard.\n\nIf the training loss  has been calculated correctly in the bound,\nthe results are significant.\n\nAssuming correctness, I would comment that the Catoni bound, while sqeaking\nout all available tightness, is very opaque.  I might be good to\nconsider the more transparent bounds, claimed to be essentially the\nsame, given in McAllester's tutorial.  If the more transparent bounds\nachieve equivalent numerical results, they would make the nature of\nthe bounds clearer.\n\nAnother comment involves a largely ignored detail in (Dzuigaite and\nRoy 17). Their bounds become vacuous if they center their Gaussian\nprior at zero.  Instead they center the prior on the initial value of\nthe parameters.  This yields a dramatic improvement in the bound.  In\nthe context of the present paper, this suggests a modification of the\nprior distribution on the compressed model.  We represent the model by\nfirst selecting the r code values.  I think a distribution could be\ndefined on the code book that would improve its log probability, but I\nwill ignore that.  Given the r code values we can define a\ndistribution over the possible compressed representations of a weight\nw_i in terms of a prior on w_i defined in terms of its initial value.\nThis gives a probability distribution over the compressed\nrepresentation.  Using log probability of the compressed\nrepresentation should then be a significant improvement on the first\nterm in (8).  This shift in the prior on compressed models has no\neffect on the second term of (8) so things should only get better.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}