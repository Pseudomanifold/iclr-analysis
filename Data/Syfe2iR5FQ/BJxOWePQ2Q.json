{"title": "Not enough technical details and missing theory", "review": "In this paper, the authors propose to organize all parameters of a multilayer neural network in a higher order tensor (8 dimension) and use low-rank tensor decomposition models (Tucker and TT/MPS formats) to compress it. They applied this approach to a very specific architecture, the stacked hourglass architecture of Newell et all (2016) full convolutional neural network and used in the specific problem of human pose estimation.\nThe idea of the paper is simple and technically sounded but unfortunately, I found it not well presented with many important technical details missing. Also, there are not any theoretical justification for the method to work, the results are restricted to a very specific network architecture and only one application, which limits their generalization to a broader class of problems. Below, I provide the main comments about the paper:\n\nMajor issues:\n-\tThe paper claims that, for lower compression rates, the proposed method outperforms the uncompressed version of the network. What is the justification for this behavior? Is it related to a regularization effect? How this effect depends on the number of available samples? I think the authors should provide some theoretical insights about this behavior or, at least, a deeper experimental study.\n-\tThe authors do not give details on how the tensorized network is optimized/trained. They state that \u201cAll models were trained for 110 epochs using RMSprop (Tieleman & Hinton, 2012). The learning rate was varied from from 2.5e-4 to 1e-6 using a Multi-Step fixed scheduler.\u201d But It is not clear if a back-propagation strategy can be still used after tensorization and what are the updating rules for the parameters in the tensor decomposition model. I think this missing information must be included in the paper, at least as an appendix.\n-\tThe title of section 2.2, \u201cFully-tensorized architecture\u201d suggests that all design parameters of the network are compressed through a tensor decomposition, however, in the implementation details section, they state that total number of parameters is 15,830,976 where 14,555,776 are in the weights tensor and 1,675,200 (based convolutions) are not considered as part of the tensor decomposition model. It is not clear, why this particular tensorization approach is used.\n-\tThey compared the results of the compressed approach against an uncompressed architecture but reducing the number of parameters by choosing 64 channels of residual blocks instead of 128 as used in the compressed version. I think this difference in number of parameters prevents from obtaining a fair comparison of the results. Why the authors have chosen fewer parameters in the uncompressed version? If it is because there is a memory size limitation, I would suggest to use 64 channels in the compressed version too, so the results can be fairly compared.\n\nMinor comments:\n-\tTensor notation is a bit strange in this paper. Tensors are usually denoted by capital calligraphic letters or underlined bold capital letters but in this work the authors use calligraphic with tilde, which makes the notation a little bit overcrowded. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}