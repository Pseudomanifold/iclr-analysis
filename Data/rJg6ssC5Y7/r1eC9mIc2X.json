{"title": "Good initiative at its beginning stage", "review": "As the paper claims there is no common accept system for benchmarking deep learning optimizer. It is also hard to repeat others' results. The paper describes a benchmarking framework for deep learning optimizer. It proposes three performance indicators, and includes 20 test problems and a core set of benchmarks. \n\nPro: \n1) It is a very relevant project. There is a need for unified benchmarking framework. In traditional optimization field, benchmarking is well studied and architectured. See an example at http://plato.asu.edu/bench.html\n2) The system is at its early stage, but its design seems complete\n3) The paper shows some performance of vanilla SGD, momentum, and Adam\n\nCon:\n1) It will take tremendous efforts to convince others to join the party and contribute\n2) It only support tensorflow right now\n3) Writing can be better\n\nIn Figure 1, make sure the names of components are consistent: either all start with nouns or verbs. The whole picture is not too illustrative. \n\n\n\nCan switch the order of Figure 2 and Figure 3?\n\nIn Table 1, the description of ALL-CNN-C has a '?'. Is it intended?\n\nWhy not explain Table 2? \n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}