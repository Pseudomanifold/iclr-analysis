{"title": "This paper presents an extremely comprehensive comparison of sentence representation methods.", "review": "Only a handful of NLP tasks have an ample amount of labeled data to get state-of-the-art results without using any form of transfer learning. Training sentence representation in an unsupervised manner is hence crucial for real-world NLP applications.\nContextualized word representations have gained a lot of interest in recent years and the NLP and ML community could benefit from such detailed comparison of such methods.\n\nThis paper's biggest strength is the experimental setting.  The authors cover a lot of ground in comparing a lot of the recent work, both qualitatively and quantitatively -- there are a lot of experiments.\nI do understand the computational limitations of the authors (as they mention on HYPERPARAMETER TUINING) and I do agree with their statement \u201c The choice not to tune limits our ability to diagnose the causes of poor performance when it occurs\u201d.\nExtensive hyper-parameter tuning can make a substantial different when dealing with NN models, maybe the authors should have considered dropping some of the tasks (the article has more than enough IMHO) and focus on a smaller sub set of tasks with proper hyper-parameter tuning.\nTable 2 is very interesting, the results suggesting that we are indeed very far from fully robust sentence representation method. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}