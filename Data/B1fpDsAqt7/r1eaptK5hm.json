{"title": "The paper proposes to combine different task-level modules in a progressive way for the task for VQA. The model achieved state-of-the-art performance.", "review": "The paper proposes to learn task-level modules progressively to perform the task of VQA. Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model. The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better. The results are mainly shown on VQA 2.0 set, with a good amount of analysis.\n\n- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization. I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection. I do not have major comments about the paper itself, although I did not check the technical details super carefully.\n\n- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component. \n\n- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)? \n\n- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules. For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships? This can help us not only better understand the models, but also the dataset (VQA) and the task in general. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}