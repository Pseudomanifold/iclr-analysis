{"title": "An interesting idea, but slightly contrived and lacking empirical support", "review": "The paper develops a novel solution for federated learning under three constraints, i.e. no data pooling (which distillation violates), infrequent communication (which iterative distributed learning violates), and modest-sized global model (which ensemble model violates). This is acknowledgedly a kind of unique setting, and the proposed solution does fit it well.\n\nHowever, I have the following two main concerns\n1. The major attack on distillation from an ensemble is that it needs to pool data across all sources which has cost and privacy concerns. However I'm not entirely convinced this \"data pooling\" is really necessary. One could argue distillation might as well be performed with simply an extra dataset that could be collected (sampled) elsewhere.\nPlus, even though the proposed solution doesn't need to do \"data pooling\", it is effectively doing \"model pooling\" which may has its own costs and issues, e.g. the assumptions that one has access to all the parameters of the local models, and that all those local models should more or less be homogeneous to allow such pooling to happen, might not hold.\n\n2. The idea of applying Beta-Bernoulli Process to uncover the underlying global model from a pool of local models is interesting. But I would very much like to see comparisons to some other simpler baselines, e.g. using dictionary learning to extract the common set of basis shared among the local models, or perhaps the slightly fancier DP-means (Kulis & Jordan, 2012)? Especially the lack of a meaningful improvement over the compared baselines from the empirical studies makes me wonder whether the BBP is indeed fit for purpose or even necessary for this task.\n\nSome other questions/comments,\n1. I'd be interested to see what the authors think about the connection between their proposed PFNM to Hinton's dropout, which could also be interpreted as performing an implicit \"model pooling\" over an ensemble of local models sharing weights among each other.\n\n2. After introducing the notation for \"-j\", I'd suggest not to abuse \"j\" to keep denoting (dummy) indices in summations (e.g. Eq.(7), (8), etc.) - I might prefer swapping it with e.g. \"j'\" in $B^{j'}_{i,l}$, $v_{j'l}$ and $\\sigma^2_{j'}$ to avoid confusions.\n\n3. When the number of batches J gets larger, which means a smaller batch size and therefore also a larger variance among the local models, would it be beneficial to also increase the noise variances $\\sigma_j$ accordingly to allow a better fit?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}