{"title": "Need improvement in presentation", "review": "This paper studied the problem of reducing the training time of neural networks in a data-parallel setting where training data is stored in multiple servers. The main goal is to reduce the network traffic of exchanging the gradients at each step. Gradient dropping achieves this goal by exchanging globally only the sparse and large gradients but accumulating the small gradients locally at each step. It was shown in previous work that this approach can have slow convergence. This paper improves this technique by exploring 3 simple heuristics, SUM, PARTIAL, and ERROR, for combing the global sparse gradients and the locally computed gradients during updates. Experimental results showed that in machine translation tasks, the proposed method achieved significant faster time while the final quality of the model is not affected.\n\nReducing communication cost in distributed learning is a very important problem, and the paper proposes an interesting improvement upon an existing method. However, I think it\u2019s necessary to provide more intuitions or theoretical analysis behind the introduced heuristics. And it might be better to conduct experiments or add more discussion in tasks other than machine translation. The paper is generally easy to follow but the organization needs to be improved. \n\nMore details in terms of presentation:\n- Section 2.1 and Algorithm 1 reviewed gradient dropping which is the main related work to this paper but it is in the middle of the related work section. The notations were also not clearly stated (e.g. the notation of | . |, Gt, and AllReduce). It\u2019s better for algorithm blocks to be more self-contained with notations briefly described inside them. I suggest re-organizing it by combing Section 2.1 with Section 3 into a single technical section and providing a clear definition of the notations.\n- In Section 3, the first part of Algorithm 2 is identical to Algorithm 1, so it makes sense to keep only one of them and explain only the difference. \n- In Section 4 and 5, it is unclear what the baseline method is referring to. Is it DGC or training with a single machine? The result in Figure 1 seems less important. I would suggest placing it and Section 5.1 in a less significant position. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}