{"title": "Nice and Relevant Results", "review": "Summary:\n========\nThe paper presents rates of convergence for estimating nonparametric functions in Besov\nspaces using deep NNs with ReLu activations. The authors show that deep Relu networks,\nunlike linear smoothers, can achieve minimax optimality. Moreover, they show that in a\nrestricted class of functions called mixed Besov spaces, there is significantly milder\ndependence on dimensionality. Even more interestingly, the Relu network is able to\nadapt to the smoothness of the problem.\n\nWhile I am not too well versed on the background material, my educated guess is that the\nresults are interesting and relevant, and that the analysis is technically sound.\n\n\n\nDetailed Comments:\n==================\n\n\nMy main criticism is that the total rate of convergence (estimation error + approximation\nerror) has not been presented in a transparent way. The estimation error takes the form\nof many similar results in nonparametric statistics, but the approximation error is\ngiven in terms of the parameters of the network, which depends opaquely on the dimension\nand other smoothness parameters. It is not clear which of these terms dominate, and\nconsequently, how the parameters W, L etc. should be chosen so as to balance them.\n\n\nWhile the mixed Besov spaces enables better bounds, the condition appears quite strong.\nIn fact, the lower bound is better than for traditional Holder/Sobolev classes. Can you\nplease comment on how th m-Besov space compares to Holder/Sobolev classes? Also, can\nyou similiarly define mixed Holder/Sobolev spaces where traditional linear smoothers\nmight achieve minimax optimal results?\n\n\nMinor:\n- Defn of Holder class: you can make this hold for integral beta if you define m to be\nthe smallest integer less than beta (e.g. beta=7, m=6). Imo, this is standard in most\ntexts I have seen.\n- The authors claim that the approximation error does not depend on the dimensionality\n  needs clarification, since N clearly depends on the dimension. If I understand\n  correctly, the approximation error is in fact becoming smaller with d for m-Besov\n  spaces (since N is increasing with d), and what the authors meant was that the\n  exponential dependnence on d has now been eliminated. Is this correct?\n\nOther\n- On page 4, what does the curly arrow notation mean?\n- Given the technical nature of the paper, the authors have done a good job with the\n  presentation. However, in some places the discussion is very equation driven. For e.g.\n  in the 2nd half of page 4, it might help to explain many of the quantities presented in\n  plain words.\n\n\n\nConfidence: I am reasonably familiar with the nonparametric regression literature, but\nnot very versed on the deep learning theory literature. I did not read the proofs in\ndetail.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}