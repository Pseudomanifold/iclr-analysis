{"title": "Theorem 2.1 -2.2 are interesting", "review": "Overall I found that the paper does not clearly compare the results to existing work. There are some new results, but some of the results stated as theorems are immediate consequence of existing work and a more detailed discussion and comparison is warranted. I will first give detailed comments on the establishing the relationship to existing work and then summarize my evaluation. \n\n\u2014\u2014\u2014\u2014\nDetailed comments on contributions and relationships to existing work.\n\nA. Theorem 2.1 establishes the limit of the regularized solutions as the maximum margin separator. \nThis result is a generalization the analogous results for linear models Theorem 3 in Rosset et al. (2004) \u201cBoosting as a regularized path to maximum margin separator\u201d and Thm 2.1 in Rosset Zhu Hastie \u201cmargin maximizing loss functions\u201d  (the later paper missing from references, and that paper generalizes the earlier result for multi-class cross entropy loss). \nMain difference from earlier work:\n1. extends the results for linear models to any homogeneous function \n2. (minor) the previous results by Rosset et al. were stated only for lp norms, but this is a minor generalization since the earlier work didn\u2019t at any point use the lp-ness of the norm and immediately extends for any norms. \n\nSecondly, Theorem 2.2 also gives a bound on deviation of margin when the regularization is not driven all the way to 0. I do think this theorem would be differently stated by making the explicitly showing dependence of suboptimal margin \\gamma\u2019 on lambda and the sub optimality constant of loss. This way, one can derive 2.1 as a special case and also reason about what level of sub-optimality of loss can be tolerated. \n\nB. Theorem 3.1 derives generalization bounds of learned parameters in terms of l2 margin. \n\u2014this and many similar results connecting generalization to margins have already been studied in the literature (Neyshabur et al. 2015b for example covers a larger family of norms than just l2 norm). Specially an analogous bound for l1 margin can also be found in these work which can be used in the discussions that follow. \n\nC. Theorem 3.2: This result to my knowledge is new, but also pretty immediate from definition of margin. The proof essentially follows by showing that having more hidden units can only increase the margin since the margin is maximized over a larger set of parameters. \n\nD. Comparison to kernel machines: Theorem 3.3 seems to be the paraphrasing of corollary 1 in Neyshabur et al (2014). But the authors claim that the Theorem 3.3 also holds when \u201cthe regularizer is small\u201d. I do not understand what the authors are referring to here or how the result is different form existing work. Please clarify\n\n-----------\nIn summary, The 2.1-2.2 on extension of the connection between regularized solution and maximum margin solution to general homogeneous models and to non-asymptotic regimes \n-- this is in my opinion key contribution of the paper and an important result. But there is not much new technique in terms of proof here\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}