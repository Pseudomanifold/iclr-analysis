{"title": "SRL composition functions for Sentence Embedding Tasks", "review": "This work proposes a view on tasks requiring pairs of sentence representations from the perspective of statistical relational learning (SRL), where there exist a multitude of composition functions for pairs of entity vectors. The authors systematically categorise different types of composition functions and apply them in tasks for testing sentence representations, with two sentence representation pretraining tasks.\n\nStrengths:\n- proposition of a unifying view onto two mostly separate strands of research\n- principled way of thinking about the requirements to a relational vector composition model\n- systematicity - several composition functions are categorised and systematically compared\n- breadth: comparison of a large variety of composition functions on multiple tasks with two separate sentence embedding pretraining tasks\n- good practice: significance testing of results. \n\nWeaknesses:\n- empirical differences are marginal, but authors claim to \u201csignificantly improve the state of the art [\u2026]\u201d in the abstract, which I do not see as justified. The major source of variation appears to be the pretraining task chosen, not the composition function.\n- The chosen scope is limited: here sentence representations cannot depend on one another, whereas this is commonplace in practice, e.g. via per-token attention on the other sequence, or in conditional LSTMs.\n- the observations on expressiveness of composition functions in SRL are not new. It would have been interesting to see particular ways in which these observations differ from SRL when lifted over to the new context of sentence representations, rather than entity pairs.\n- The proposed method boils down to combining relatively simple components in a straightforward manner, little innovation in terms of methodology.\n- related work mostly discusses word-level representations, whereas the paper is about sentence-level representations and SRL.\n- particular formulations and claims are in several points unclear, vague, imprecise or too broad \u2014 see other comments below.\n\n\nOther comments:\n- imprecise: \u201creasoning over its input embeddings\u201c. Can this be made more specific? \n- Section 2 unclear: \u201cadditive and weak\u201c interaction. What does that mean?\n- Section 2,  bullet point 3: imprecise, and motivation unclear. Perhaps: formulate in terms of computational complexity?\n- Broad claim - be more specific: \u201cThe ComplEx model yields state of the art performance on knowledge base completion\u201c \u2014 other models have been proposed, many of which outperform ComplEx on specific datasets, so the claim in its full generality cannot hold.\n- strong wording in abstract: \u201cwe prove that textual relational models implicitly use compositions from baseline SRL models\u201d. In my personal view this is not strong enough a theoretical result to \u201cprove\u201d it.\n- speculative/unclear: \u201cexpressive compositions can also be helpful to improve interpretability and evaluation of sentence embeddings by providing sentence level analogies\u201d\n- speculative and vague: \u201cFrom our previous analysis, we believe this composition function favours the use of contextual/lexical similarity rather than high-level reasoning and can penalise representations based on more semantic aspects. This bias could harm research since semantic representation in an important next step for sentence embedding\u201d\n- Section 4.2: initially unclear goal of this section\n- Four of the Transfer evaluation tasks only use one embedding - h1. While it is interesting to have results also on these tasks, these are somewhat unrelated to the main topic of the paper\n- some more details on significance tests would be useful. Normality assumption? Number of re-runs?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}