{"title": "An interesting general surrogate reward which has wide applicability, and can be flexibly included alongside a variety of algorithms.", "review": "## Summary\n\nThe authors present work that shows how to deal with noise in reward signals by creating a surrogate reward signal. The work develops a number of results including: showing how the surrogate reward is equal in expectation to the true reward signal, how this doesn't affect the fixed point of the Bellman equation, how to deal with finite and continuous rewards and how the convergence time is affected for different levels of noise. They demonstrate the value of this approach with a variety of early and state-of-the-art algorithms on a variety of domains,, and the results are consistent with the claims.\n\nIt would be useful to outline how prior work approached this same problem and also to evaluate the proposed method with existin approaches to the same problem. I realise that this is the first method that estimates the confusion matrix rather than assuming it is known a priori but there are obvious ways around this, e.g. the authors first experiment assumes the confusion matrix is known, so this would be a good place to compare with other competing techniques. Also, the authors have a way of estimating this, so they could plug it into the other algorithms too.\n\nI also have some concerns about the clarity and precision of the proofs, although I do not have any reason to doubt the Lemma/Theorem correctness (see below).\n\nThe weakest part of the approach is in how the true reward is estimated in order to estiamate the confusion matrix. It uses majority vote (which is only really possible in the case of finite rewards with noise sufficiently low that this will be a robust estimate). Perhaps some other approaches could be explore here too.\n\nFinally, there is discussion about adversarial noise in rewards at the beginning but I am not sure the theory really addresses it nor the evaluations.\n\nNonetheless, given that I do not know whether the claim of originality is true (in terms of the estimation of the confusion matrix). If it is, then the work is a significant and interesting advance, and is clearly widely applicable in domains with noisy rewards. It would be interesting to see a more tractable approach for continous noise too, but this would probably involve assumptions (smoothness? Gaussianity?), and doesn't impact the value of this work.\n\n## Detailed notes\n\nThere is a slight sloppiness in  notation in equation (1). This uses \\tilde{r} as a subscript of e, but r is +1 or -1 and the error variables are e_+ and e_- (not e_{+1} and e_{-1}).\n\n\nThe noise levels in Atari (Figure 3) show something quite interesting which could be commented upon. For noise below 0.5 the surrogate reward works roughly  similarly to the noisy reward, but when the noise level goes above this, the surrogate reward clearly exploits the increased information content (similar to a noisy binary channel with over 0.5 noise). This may have  implications for adversarial noise.\n\nThere are also some issues with the proofs which I spotted outlined below:\n\n### Lemma 1 proof\nThe proof of Lemma 1, I think, fails to achieve its objective. The first pair of equations is not a rewrite of equation (1). I believe that the authors intend for this to be a consequence of Equation (1) but do not really demonstrate this clearly. Also, the authors seem to switch between binary rewards -1 and +1 and two levels of reward r- and r+ leading to some confusion. I would suggest the latter throughout as it is more general but involves no more terms.\n\nI suggest the following as an outline for the proof. It would help for them to define what they mean by the different rhats (as they currently do) and explain that these values are therefore:\n\n  rhat- = [(1 - e+) r- - e- r+ ]/(1 - e+ - e-)\n  rhat+ = [(1 - e-) r+ - e+ r-]/(1- e+ - e-)\n\nfrom equation (1). What is left is for them to actually prove the Lemma, namely that the expected value of rhat is:\n\n  E(rhat ) = p1(rhat=rhat-) rhat- + p(rhat=rhat+) rhat+ = E(r)\n\nwhere the probabilities relate to the surrogate reward taking their respective values. And just stylistically, I would avoid writing \"we could obtain\" and simply write \"we obtain\".\n\nLemma 2 achieves this more clearly with greater generality.\n\n\n### Theorem 1 proof\nAt the end of p13, the proof of the expected value loses track of the chosen action a. I would suggest the authors replace: $$\\mathbb{P}'(s,s',\\hat{r})$$ with $$\\mathbb{P}'(s,a, s',\\hat{r})$$ then define it. Likewise $$\\mathbb{P}(s,s')$$ should be $$\\mathbb{P}(s,a,s')$$ (and also defined).\n\nI am also a little uncomfortable with the switch from: $$max_{b \\in \\mathcal{A}} | Q(s',b) - Q*(s',b)|$$ in the second to last line of p13, which refers to the maximum Q value associated with some state s', to  $$||Q-Q*||_{\\infty}$$ in the next line which is the maximum over all states and actions. The equality should probably be an inequality there too.\n\nThroughout this the notation could be much better defined, including how to interpret the curly F and how it acts in the conditional part of an expectation and variance.\n\nFinally, there is a bit too free a use of the word \"easily\" here. If it were easy, then the authors could do it more clearly I think. Otherwise, please refer to the appropriate result in the literature.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}