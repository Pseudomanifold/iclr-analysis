{"title": "Using multiple generators to properly capture the target (data) distribution", "review": "In this work, the authors propose to use multiple generators to estimate the target distribution. Especially, it assumes the case that the range of generators is non-convex and the target distribution doesn't fall into it. To solve this issue, the multiple generators are convex combined to do better approximation and an incremental training process is proposed to train multiple generators one by one.\n\n1) Using multiple generators seems reasonable based on the authors assumption (non-convex of the range of generators), but is  this assumption based on having a perfect discriminator? Could you assume a similar case for the discriminator?\n\n2) In figure 3, it is shown each generator tries to improve the estimated target distribution. However, it is not clear what generator generates what samples. It would be better to use different colors for different generators. If I assume that the red samples are from the first generator, why the second image (top right) shows slightly shifted samples compared to the first image (top left). As far as I understand, the first generator is fixed after it is converged.\n\n3) It is shown that the (convex) weights for generators are fixed to 1, is there any reason to fixed it?\n\n4) On page 3, the equation in section 2.1 looks like missing $w_{n+1}$, could you confirm this?\n\n5) is the Original GAN exactly the Ian's original GAN or WGAN?\n\n6) Have you tried this approach using small sized generator (having  small number of parameters)?\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}