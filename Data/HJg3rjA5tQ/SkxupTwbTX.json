{"title": "Interesting idea but the paper needs work", "review": "This paper proposes a way to define f-divergences for densities which may have different supports. While the idea itself is interesting and can be potentially very impactful, I feel the paper itself needs quite a bit of work before being accepted to a top venue.  The writing needs quite a bit oof polish for the motivations to clearly stand out. Also, some of the notation makes things way more confusing that it should be. Is it possible to use something other than p() for the noise distribution, since the problem itself is to distinguish between p() and q(). I understand the notational overload, but it complicates the reading unnecessarily. I have the following questions if the authors could please address:\n\n1) The inequality of Zhang et a. (2018) that this paper uses seems to be an easy corollary of the Data Processing Inequality :https://en.wikipedia.org/wiki/Data_processing_inequality Did I miss something? Can the authors specify if that is not the case?\n\n2) In terms of relevance to ICLR, the applications of PCA, ICA and training of NNs is clearly important. There seems to be a significant overlap of Sec 5.3 with Zhang et al. Could the authors specify what the differences are in terms of training methodology vis-a-vis Zhang et al? It seems to me these are parallel submissions with this submissions focussing more on properties of Spread Divergences and its non deep learning applications, while the training of NNs and more empirical evidence is moved to Zhang et al. \n\n3) I am having a tough time understanding the derivation of Eq 25, it seems some steps were skipped. Can the authors please update the draft with more detail in the main text or appendix ?\n\n4) Based on the results on PCA and ICA, I am wondering if the introduction of the spread is in some ways equivalent to assuming some sort of prior. In the PCA case, as an exercise to understand better, what happens if some other noise distribution is used ? \n\n5) I do not follow the purpose of including the discussion on Fourier transforms. In general sec 3 seems to be hastily written. Similarly, what is sec 3.2's goal ? \n\n6) The authors mention the analog to MMD for the condition \\hat{D}(p,q)=0  \\implies p =q. From sec 4, for the case of mercer spread divergence, it seems like the idea is that  \"the eigenmaps of the embedding should match on the transformed domain\" ? What is [a,b] exactly in context of the original problem? This is my main issue with this paper. They talk about the result without motivation/discussion to put things into context of the overall flow, making it harder than it should be for the reader. I have no doubt to the novelty, but the writing could definitely be improved.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}