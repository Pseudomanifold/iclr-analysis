{"title": "The paper considers uniform approximations of functions by ReLU neural networks. The authors derive bounds on the depth and width of such networks to achieve a certain degree of accuracy (\\eps) over classes of functions such as polynomials, sinusoidals, oscillatory textures, etc.", "review": "A main theme of the paper is showing that constant-width ReLU networks with depth increasing ploy-logarithmically in 1/\\eps can achieve the desired accuracy over the classes considered. \n\n- It seems to me that a major claim of the paper is that previous results did not have constant-width approximations. \n\nHowever, this does not seem accurate to me. For example, for polynomials, the statement \u201cthe width of the approximating network does not grow with the degree of the polynomial as is the case in Yarotsky (2016)...\u201d does not seem true. The constructions in Yarotsky (2016) which much of the present work appears to be based on, in fact, allow for a constant-width approximation of polynomials of degree \u201cn\u201d in \u201cd\u201d-variables, over the cube, with \n\nwdith = 9, and depth ~ m \\log m [\\log (1/\\eps) + \\log m] where m = n + d. (*)\n\nThis bound is quite similar to Prop. 3.3 (with maybe even sharper dependence on \u201cm\u201d: m(\\log m)^2 versus m^2 in the paper. PS. I would also  double-check C in Prop. 3.3 which could be growing logarithmically in m.)\n\n(*) can be seen by inspecting the arguments around (14)-(15) in Yarotsky (2016) and noting that c_1 ~ \\log m in that argument. For example, considering d=1 which is the focus of the present paper, Yarotsky (2016) shows a constant-width approximation to the product function (x,y) \\mapsto xy which can be used to build a constant width-approximation to the monomial of the highest degree in the polynomial by recursive composition. All other monomials can be accessed serially at various depths of that architecture.\n\n- Much of the subsequent results in the paper are based on this constant-width approximation of polynomials as the authors point out. This is not that surprising given the Taylor approximation. For example, in Theorem 4.1, the first few lines of the proof show that the cosine can be approximated with a polynomial of degree m = O(\\log (1/\\eps)). Combining this with the polynomial approximation result one gets a constant-width approximation with \ndepth ~ \\log(1/\\eps)^2 or \\log(1\\eps)^2 \\log \\log (1/\\eps) \ndepending which version of the polynomial approximation result one believes (m \\log m versus m as the prefactor as discussed above).  In other words, it appears to me that the proof of Theorem 4.1 can be shortened considerably. It would be a corollary of the polynomial approximation. \n\n- A novelty of the present work over Yarotsky (2016) that the authors point out is avoiding skip connections in Proposition 3.1. (Perhaps this is true also for Prop. 3.3? Not discussed.) I haven\u2019t checked the details here, but assuming correctness, I agree that it is quite interesting. I am not sure however if it is a significant improvement over the existing results.\n\n- The discussion of Section 5 and 6 might be new in this context and somewhat interesting. However, at least that of Section 5 again seems to be a natural byproduct the polynomial approximation result.\n\n- I would like to point out that there are other results on finite-width approximation by ReLU networks, establishing some quite sharp bounds, for example,\nHanin and Sellke 2017: arXiv:1710.11278\nYarotsky 2018: arXiv:1802.03620\nIn light of these, it wouldn\u2019t be that accurate to claim that the issue of constant-width approximation is considered for the first time in the present paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}