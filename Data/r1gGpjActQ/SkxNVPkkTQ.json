{"title": "good results, okay paper", "review": "In this paper, the authors propose an extension to the Non Autoregressive Translation model by Gu et. al, to improve the accuracy of Non autoregressive models as compared to the autoregressive translation models.\nThe authors propose using hints which can occur as\n1. Hidden output matching by incurring a penalty if the cosine distance between the representation differ according to a threshold. The authors state that this reduces same output word repetition which is common for NART models\n2. Reducing the KL divergence between the attention distribution of the teacher and the student model in the encoder-decoder attention part of the model.\n\nWe see experimental evidence from 3 tasks showing the effectiveness of this technique.\n\nThe strengths of this paper are the speedup improvements of using these techniques on the student model while also improving BLEU scores. \nThe paper is easy to read and the visualisations are useful.\n\nThe main issue with this paper is the delta contribution as compared to the NART model is Gu et. al. The 2 techniques, although simple, don't make up for technical novelty.\nIt would also be good to see more analysis on how much the word repetition reduces using these techniques quantitatively, and performance especially on longer length sequences.\n\nAnother issue is the comparison of latency measurements for decoding. The authors state that the hardware and the setting under which the latency measurements are done might be different as compared to previous numbers. Though still impressive speedup improvements, it somehow becomes fuzzy to understand the actual gains.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}