{"title": "Limited novelty and unclear motivation", "review": "The paper addresses the incremental few-shot learning problem where a model starts with base network and then introduces the novel classes, building a connection between novel and base classes via an attention module.\n\nStrengths:\n+ clear writing. \n+ the experiments are compared with related work and the ablation studies can verify the effectiveness of the proposed (or \"introduced\" would be a precise term) recurrent BP.\n\nWeakness:\n\n- [Novelty]\nThe paper title is called attention attractor network, which shares very relevance to previous CVPR work (Gidaris & Komodakis, 2018). So the first thing I was looking for is the clear description of the difference between these two. Unfortunately, in related work, authors mention the CVPR work without stating the difference (last few lines in Section 2). As such, I don't see much novelty in the paper compared with previous work. Eqn. (7)-(10) explicitly describes the attention formula. What's the distinction from the CVPR work?\n\n- [Motivation of the regularizer using Recurrent BP is not clear]\nThe use of recurrent BP is probably the most distinction from previous work. However, I don't see a clear description on why such a technique is necessary.\n\nStarting from the first line in Section 3.3, \"since there is no closed-form of the regularizer in Eqn (13)\", E needs BPTT or the introduced recurrent BP. This part is simply a re-adaption of other algorithms. A very simple question is, how about use other regularizers to replace Eqn (13)? \n\n- [Some experiments missing]\nThe experiments section 4.6 uses a case of None and \"best WD\" to address some of my concerns. This is good. Does the \"gamma random\" indicates only E is used without the ||W||^2? why the best WD for one-shot is zero? This implies the model is best for applying no weight decay?\n\nWhat's the effect of using the recurrent BP technique to the CVPR work? Is there some similar improvement? If yes, then the paper makes some contribution by the regularization. If not, what's the reason?\n\nHow about using the truncated BPTT with a larger T?\n\nIn general, I think the recurrent BP part should be the highlight of the paper and yet authors fail to spread such a spirit in the abstract or title. And there are some experiments missed as I mentioned above.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}