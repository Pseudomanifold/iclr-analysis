{"title": "Although the proposed general formulation is itself interesting, some of the arguments are not sound, and the proposed scheme is somehow similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).", "review": "\n[pros]\n- It proposes a general formulation of GAN-type adversarial learning as in (1), which includes the original GAN, WGAN, and IPM-type metrics as special cases.\n- It also proposes use of the penalty term in terms of the Lipschitz constant  of the discriminative function.\n\n[cons]\n- Some of the arguments on the Wasserstein distance and on WGAN are not sound.\n- Theorem 3 does not make sense.\n- The proposed scheme is eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017).\n\n[Quality]\nI found some weaknesses in this paper, so that I judge the quality of this paper not to be high. For example, the criticisms on the Wasserstein distance in Section 2.3 and in Section 4.4, as well as the argument on WGAN at the end of Section 3.1, is not sound. The claim in Theorem 3 does not make sense, if we literally take its statement. All these points are detailed below.\n\n[Clarity]\nThe main paper is clearly written, whereas in the appendices I noticed several grammatical and spelling errors as well as unclear descriptions.\n\n[Originality]\nDespite that the arguments in this paper are interesting, the proposed scheme is somehow eventually similar to the gradient-penalty-based formulation in Gulrajani et al. (2017), with differences being introduction of loss metrics $\\phi,\\varphi,\\psi$ and the form of the gradient penalty, $\\max \\|\\nabla f(x)\\|_2^2$ in this paper versus $E[(\\|\\nabla f(x)\\|_2-1)^2]$ in Gulrajani et al. (2017). This fact has made me to think that the originality of this paper is marginal.\n\n[Significance]\nThis paper is significant in that it would stimulate empirical studies on what objective functions and what types of gradient penalty are efficient in GAN-type adversarial learning.\n\nDetailed comments:\n\nIn Section 2.3, the authors criticize use of the Wasserstein distance as the distance function of GANs, but their criticism is off the point. It is indeed a problem not of the Wasserstein distance itself, but of its dual formulation.\n\nIt is true mathematically that $f$ in equation (8) does not have to be defined outside the supports of $P_g$ and $P_r$ because it does not affect the expectations in (8). In practice, however, one may regard that $f$ satisfies the condition $f(x)-f(y)\\le d(x,y)$ not only on the supports of $P_g$ and $P_r$ but throughout the entire space $\\mathbb{R}^n$. It is equivalent to requiring $f$ to satisfy the 1-Lipschitz condition on $\\mathbb{R}^n$, and is what WGAN (Arjovsky et al., 2017) tries to do in its implementation of the \"critic\" $f$ via a multilayer neural network with weight clipping.\n\nOne can also argue that, if one defines $f$ only on the supports of $P_g$ and $P_r$, then it should trivially be impossible to obtain gradient information which can change the support of $P_g$. The common practice of requiring the Lipschitz condition throughout $\\mathbb{R}^n$ is thus reasonable from this viewpoint. This is therefore not the problem of the Wasserstein distance itself, but the problem regarding how the dual problem is implemented in learning of GANs. In this regard, the discussion in this section, as well as that in Section 4.4, is misleading.\n\nOn optimizing $k$, I do not agree with the authors's claim at the end of Section 3.1 that WGAN may not have zero gradient with respect to $f$ even when $P_g=P_r$. Indeed, when $P_g=P_r$, for any measurable function $f$ one trivially has $J_D[f]=E_{x\\sim P_g}[f(x)]-E_{x\\sim P_r}[f(x)]=0$, so that the functional derivative of $J_D$ with respect to $f$ does vanish identically. \n\nI do not understand the claim of Theorem 3. I think that the assumption is too strong. If one literally takes \"$\\forall x \\not= y$\", then one can exchange $x$ and $y$ in the condition $f(y)-f(x)=k\\|x-y\\|$ to obtain $f(x)-f(y)=k\\|y-x\\|$, which together would imply $k=0$, and consequently $f$ is constant. One would be able to prove that if there exists $(x,y)$ with $x \\not= y$ such that $f(y)-f(x)=k\\|x-y\\|$ holds then the gradient of $f$ at $x_t$ is equal to $k(y-x)/\\|x-y\\|$ under the Lipschitz condition.\n\nAppendix G: Some notations should be made more precise. For example, in the definition of J_D the variable of integration $x$ has been integrated out, so that $J_D$ no longer has $x$ as its variable. The expression $\\partial J_D/\\partial x$ does not make any sense. Also, $J_D^*(k)$ is defined as \"arg min\" of $J_D$, implying as if $J_D^*(k)$ were a $k$-Lipschitz function.\n\nPage 5, line 36: $J_D(x)$ appears without explicit definition.\n\nPage 23, lines 34 and 38: Cluttered expression $\\frac{\\partial [}{\\partial 2}]$ makes the statements not understandable. It also appears on page 24 several times.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}