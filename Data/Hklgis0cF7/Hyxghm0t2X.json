{"title": "Not well written", "review": "This paper realizes the radial basis function in deep CNNs by leveraging the Mahalanobis distance between a convolutional feature vector and the corresponding center. The method is implemented for the image classification and segmentation tasks against several types of attack methods and demonstrates good robustness.\n\nAlthough the results against adversarial attacks are promising, the paper is not well written. Especially, the notations in Section 2 are not clearly defined which baffled me a lot on how this method functions. For instance, in Equation 4, what does w_k stand for? Why there are K activation functions in a CNN, are they different? What is the meaning of a dot product between a w_k and an activation function?\n\nAdditionally, there lacks detail on how to train the transformation matrix (P in Equation 3 or T in Equation 1), and the following sentence confused me a lot: \"To enforce T's positive semi-definiteness, using the eigenvalue decomposition, it can be decomposed into T'T\". I understand why T needs to be PSD matrix, but how can eigenvalue decomposition decompose T into T'T? And how is this achieved during the training of a CNN? I think the authors should revise this part carefully to demonstrate the proposed methods more clearly.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}