{"title": "Reasonable idea but the technical details are quite unclear", "review": "This paper presents the following approach to domain adaptation. Train a source domain RNN. While doing inference on the target domain, first you run the source domain RNN on the sequence. Then while running the target domain RNN, set the hidden state at time step i, h^t_i, to be a function 'f' of  h^t_{i-1} and information from source domain \\psi_i; \\psi_i is computed as a convex combination of the state of the source domain RNN, h^s_{i}, and an attention-weighted average of all the states h^s{1...n}. So in effect, the paper transfers information from each of source domain cells -- the cell at time step i and all the \"collocated\" cells (collocation being defined in terms of attention). This idea is then extended in a straightforward way to LSTMs as well. \n \nDoing \"cell-level\" transfer enables more information to be transferred according to the authors, but it comes at a higher computation since we need to do O(n^2) computations for each cell.\n\nThe authors show that this beats a variety of baselines for classification tasks (sentiment), and for sequence tagging task (POS tagging over twitter.)\n\nPros:\n1. The idea makes sense and the experimental results show solid \n\nCons:\n1. Some questions around generalization are not clearly answered. E.g. how are the transfer parameters of function 'f' (that controls how much source information is transferred to target) trained? If the function 'f' and the target RNN is trained on target data, why does 'f' not overfit to only selecting information from the target domain? Would something like dropping information from target domain help?\n\n2. Why not also compare with a simple algorithm of transferring parameters from source to target domain? Another simple baseline is to just train final prediction function (softmax or sigmoid) on the concatenated source and target hidden states. Why are these not compared with? Also, including the performance of simple baselines like word2vec/bow is always a good idea, especially on the sentiment data which is very commonly used and widely cited. \n\n3. Experiments: the authors cite the hierarchical attention transfer work of Li et al (https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16873/16149) and claim their approach is better, but do not compare with them in the experiments. Why?\n\nWriting:\nThe writing is quite confusing at places and is the biggest problem with this paper. E.g.\n\n1. The authors use the word \"collocated\" everywhere, but it is not clear at all what they mean. This makes the introduction quite confusing to understand. I assumed it to mean words in the target sentences that are strongly attended to. Is this correct? However, on page 4, they claim \"The model needs to be evaluated O(n^2) times for each sentence pair.\" -- what is meant by sentence pair here? It almost leads me to think that they consider all source sentence and target sentences? This is quite confusing. \n\n2. The authors keep claiming that \"layer-wise transfer learning mechanisms lose the fine-grained cell-level information from the source domain\", but it is not clear exactly what do they mean by layer-wise here. Do they mean transferring the information from source cell i to target cell i as it is? In the experiments section on LWT, the authors claim that \"More specifically, only the last cell of the RNN layer transfers information. This cell works as in ART. LWT only works for sentence classification.\" Why is it not possible to train a softmax over both the source hidden state and the target hidden state for POS tagging? \n\nnits:\npage 4 line 1: \"i'th cell in the source domain\" -> \"i'th cell in the target domain\". \"j'th cell in target\" -> \"j'th cell in sourcE\".\n\n\nRevised: increased score after author response.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}