{"title": "Good empirical results on transfer learning; writing could be clearer", "review": "== Quality of results ==\nThis paper's empirical results are its main strength. They evaluate on a well-known benchmark for transfer learning in text classification (the Amazon reviews dataset of Blitzer et al 2007), and improve by a significant margin over recent state-of-the-art methods. They also evaluate on several sequence tagging tasks and achieve good results.\n\nOne weakness of the empirical results is that they do not compare against training a model on the union of the source and target domain. I think this is very important to compare against.\n\nNote: the authors cite a paper in the introduction \"Hierarchical Attention Transfer Network for Cross-domain Sentiment\nClassification\" (Li et al 2018) which also achieves state of the art results on the Amazon reviews dataset, but do not compare against it. At first glance, Li et al 2018 appear to get better results. However, they appear to be training on a larger amount of data for each domain (5600 examples, rather than 1400). It is unclear to me why their evaluation setup is different, but some clarification about this would be helpful.\n\n== Originality ==\nA high level description of their approach:\n1. Train an RNN encoder (\"source domain encoder\") on the source domain\n2. On the target domain, encode text using the following strategy:\n  - First, encode the text using the source domain encoder\n  - Then, encode the text using a new encoder (a \"target domain encoder\") which has the ability to attend over the hidden states of the source domain encoder at each time step of encoding.\n\nThey also structure the target domain encoder such that at each time step, it has a bias toward attending to the hidden state in the source encoder at the same position.\n\nThis has a similar flavor to greedy layer-wise training and model stacking approaches. In that regard, the idea is not brand new, but feels well-applied in this setting.\n\n== Clarity ==\nI felt that the paper could have been written more clearly. The authors set up a comparison between \"transfer information across the whole layers\" vs \"transfer information from each cell\" in both the abstract and the intro, but it was unclear what this distinction was referring to until I reached Section 4.1 and saw the definition of Layer-Wise Transfer.\n\nThroughout the abstract and intro, it was also unclear what was meant by \"learning to collocate cross domain words\". After reading the full approach, I see now that this simply refers to the attention mechanism which attends over the hidden states of the source domain encoder.\n\n== Summary ==\nThis paper has good empirical results, but I would really like to see a comparison against training a model on the union of the source and target domain. I think superior results against that baseline would increase my rating for this paper.\n\nI think the paper's main weakness is that the abstract and intro are written in a way that is somewhat confusing, due to the use of unconventional terminology that could be replaced with simpler terms.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}