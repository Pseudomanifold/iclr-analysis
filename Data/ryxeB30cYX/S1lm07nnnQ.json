{"title": "Limited novelty, but good experimental results", "review": "The paper proposes to quantize activation outputs in FGSM training. The algorithm itself is not novel. The straight through approach for training quantized network has been used in previous papers, as also pointed out by the authors. The new thing is that the authors found that quantization of activation function improves robustness, and the approach can be naturally combined with FGSM adversarial training. Experimental results show comparable (and slightly worse) results compared to adversarial training with PGD, while the proposed approach is faster in training time. \n\nI have the following questions/comments: \n\n1. Why not do SQA with PGD-adversarial training? If SQA+FGSM performs similar to PGD training, SQA+PGD might perform even better. \n\n2. There are several important papers missing in the discussion/comparisons: \n- Quantization improves robustness has been reported in a previous paper: \"Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions\". How does the proposed algorithm compare with this paper? \n- Adding stochastic noise in each layer has been used in some recent papers: \"Towards Robust Neural Networks via Random Self-ensemble\". It will be good to include into discussions. \n\n3.  I can't find the comparison between PGD-training and SQA on MNIST. Are they also comparable on MNIST? Showing results on more datasets will make the conclusion more convincing.  If the benefit of the proposed approach is training time, showing the scalability on ImageNet will make the argument stronger. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}