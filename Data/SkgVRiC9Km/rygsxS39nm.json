{"title": "good work", "review": "In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. \n\n \n(1) a grammar error at \"provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.\"\n\n(2) a grammar error at \"This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk\"\n\n(3) The sentence \"For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss\" is a little confusing to me. \"h(1)_k, ..., h(N)_k\" is only for one hidden layer, rather than \"each hidden layer\". Right?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}