{"title": "Thorough experimental evaluation of a simple method to prune neural networks before training. The same idea seems to have been already proposed in the literature.", "review": "This work introduces SNIP, a simple way to prune neural network weights before training according to a specific criterion. SNIP identifies prunable weights by the normalised gradient of the loss w.r.t. an implicit multiplicative factor \u201cc\u201d on the weights, denoted as the \u201csensitivity\u201d. Essentially, this criterion takes two factors into account when determining the relevance of each weight; the scale of the gradient and the scale of the actual weight. The authors then rank the weights according to their sensitivity and remove the ones that are not in the top-k. They then proceed to train the surviving weights as normal on the task at hand. In experiments they show that this method can offer competitive results while being much simpler to implement than other methods in the literature.\n\nThis paper is well written and explains the main idea in a clear and effective manner. The method seems to offer a viable tradeoff between simplicity of implementation and effective sparse models. The experiments done are also extensive, as they cover a broad range of tasks: MNIST / CIFAR 10 classification with various architectures, ablation studies on the effects of different initialisations, visualisations of the pruning patterns and exploration of regularisation effects on a task involving fitting random labels.\n\nHowever, this work has also an, I believe important, omission w.r.t. prior work. The idea of using that particular gradient as a guide to selecting which parameters to prune is actually not new and has been previously proposed at [1]. The authors of [1] considered unit pruning but the modification for weight pruning is trivial. It is worth pointing out that [1] is also discussed in one of the other citations of this work, namely [2]. For this reason, I believe that the main contribution of this paper is more on the thorough experimental evaluation of an existing idea rather than the proposed sensitivity metric.\n\n\nAs for other general comments:\n\n- The authors argue that SNIP can offer training time speedups by only optimising the remaining parameters. In this spirit, the authors might also want to discuss about other works that seem relevant to this task, e.g.  [3, 4]. They also allow for pruned and sparse networks during training (thus speeding it up), without needing to conform to a specific sparsity pattern. \n\n- SNIP seems to be a good candidate for applying it to randomly initialised networks; nevertheless, a lot of times we are also interested in pruning pre-trained networks. Given that SNIP is relying on the magnitude of the gradient to determine relevance, how good does it handle this particular case (given that the magnitude of the gradients is close to zero at convergence)?\n\n- Why is the normalisation of the magnitude of the gradients necessary? The normalisation doesn\u2019t change the relative ordering so we could simply just rank according to |g_j(w; D)|.\n\n- While the experiment at section 5.6 is interesting, the result is still dependent on the a-priori chosen cut-off point \u201ck\u201d. For this reason it might be worthwhile to plot the behaviour of the network as a function of \u201ck\u201d. Furthermore, the authors should also refer to [5] as they originally did the same experiment and showed that they can obtain the same behaviour without any hyper parameters.\n\n[1] Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment.\n[2] A Simple Procedure for Pruning Back-Propagation Trained Neural Networks.\n[3] Learning Sparse Neural Networks through L_0 Regularization.\n[4] Generalized Dropout.\n[5] Variational Dropout Sparsifies Deep Neural Networks.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}