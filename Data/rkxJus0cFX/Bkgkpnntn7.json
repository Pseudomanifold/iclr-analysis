{"title": "Good implementation optimizations in a important practical problem, but relatively incremental contribution", "review": "Quality and clarity:\nThe paper proposes an approach to reduce the communication bandwidth and overhead in distributed deep learning. The approach leverages on previous work (mainly the residual gradient compression (RGC) algorithm), and proposes several implementation optimizations. From what I can read, it is the basic RGC algorithm that is used, but with some clever optimization to improve the performance of it. \n\nThe quality of the paper is good, it is well-written and easy to read. The evaluation of the proposed approach is well done, using several diverse datasets and models, and executed on two different parallel systems. However, the reasons why RGC and qRGC sometimes have better accuracy than SGD needs to be analyzed and explained. \n\nOriginality and significance:\nThe originality of the paper is relatively low (optimization of an existing algorithm) and the contributions are incremental. However, the paper addresses an important practical problem in distributed learning, and thus can have a significant practical impact on how distributed deep learning systems are implemented.\n\nPros:\n* Addresses an important issue. \n* Good performance.\n* Good evaluation on two different systems. \n\nCons:\n* Limited contribution. Although I like implementation papers (very important), I think the contribution is to low for ICLR.\n\nMinor:\n* In general, the figures are hard to read (the main problem is to small text)\n* Compression in the title is slightly misleading, since it's mainly selection that is done (top-0.1% gradients). Although the values are packed in a data structure for transmission, it's not compression in a information theory perspective.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}