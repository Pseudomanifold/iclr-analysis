{"title": "Brings together existing work and adds a bit, somewhat limited experiments", "review": "[Relevance] Is this paper relevant to the ICLR audience? yes\n\n[Significance] Are the results significant? somewhat\n\n[Novelty] Are the problems or approaches novel? reasonably\n\n[Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations.\n\n[Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat\n\n[Clarity] Is the paper well-organized and clearly written? yes, except the experiments\n\nConfidence: 2/5\n\nSeen submission posted elsewhere: No (but I did find it on arXiv after writing the review)\n\nDetailed comments:\n\nIn this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (\u201cnormal\u201d RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases.\n\n=== Comments\n\nI found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the \u201cnon-decaying time series\u201d observations.\n\n However, I had difficult coming up with an example of a \u201cstatic decay feature\u201d. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a \u201cstatic decay\u201d feature rather than just a \u201cstatic\u201d feature.)\n\nMy main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While \u201cmeaningfulness\u201d may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.\n\n=== Typos, etc.\n\nThe plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}