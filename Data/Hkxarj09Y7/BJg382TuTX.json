{"title": "Modified LSTM cell updates considering different feature types", "review": "This paper proposes a new type of recurrent neural network which takes into account five different features: in addition to the prevalent dense features, the author(s) also consider(s) sparse features, time features, global static and changing features. The differences in feature types are reflected in the cell state or output state update rules. Experiments on a modified UCI dataset and a proprietary dataset show that the proposed model outperforms the time-variant LSTM (TLSTM).\n\nPros:\n1. By decomposing the cell state into different components for different feature types (dense vs sparse, short term vs long term) and update them in different manners, the model takes advantage of the feature type information.\n2. By updating sparse feature related cell states only when sparse features are present, it could be potentially computationally cheaper than treating everything as dense (although in the paper due to more parameters the proposed model is actually slower).\n\nCons:\n1. The contributions are not significant. It seems that TLSTM already \"accounts for asynchronous feature sampling\" (Sec 1) and the novelty here lies most in how sparse features are treated.\n2. The presentation is sometimes confusing. For example, in Figure 5 which presents the main results, what's the \"relative change in F1 score\" and what's the unit in the plot? If it's percentage the gains seem to be too small and could be potentially due to the additional parameters. Besides, what does \"group sampling\" mean exactly? Furthermore, legend seems to be missing.\n3. Crucial implementation details are missing. The paper mentions that \"the number of features grows\" in the proposed model. Are sparse features and static features used or not in TLSTM? \n4. What's the difference between a static decay feature (Sec 3.5) and decay features (Sec 3.2)? Isn't the static decay feature varying with time as well?\n\nMinor comments:\n1. Figure 1 and 5 are too small and hard to read.\n2. Sec 3.3, \"updated based on Equation 1 and Equation 2\", but none of the equations are numbered in this paper.\n3. Some discussions on the proprietary dataset seem to be irrelevant. I'd rather see how are sparse features generated for the UCI dataset.\n4. The decay function $g= 1 / log (e + \\alpha^T x_t^{\\delta})$, how can we make sure that as time passes it decreases as time passes?\n\n\nOverall, I think explicitly taking into account different feature types in the LSTM cell update rules is interesting, but the contributions of this paper compared to TLSTM are not significant enough for acceptance, and the presentation can be made more clear.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}