{"title": "Review", "review": "\ufffc\nTo my understanding, this paper builds on prior work from Chow et al. to apply Lyapunov-based safe optimization to the policy-gradient setting. This seems is similar to work by Achiam 2017. While this work seems like an interesting framework for encompassing several classes of constrained policy optimization settings in the Lyapunov-based setting, I have some concerns about the evaluation methodology. \n\nIt is claimed that the paper compares against \u201ctwo baselines, CPO and the Lagrangian method, on several robot locomotion tasks, in which the agent must satisfy certain safety constraints while minimizing its expected cumulative cost.\u201d Then it is stated in the experimental section \u201cHowever since backtracking line-search in TRPO can be computationally expensive, and it may lead to conservative policy updates, without loss of generality we adopt the original construction of CPO to create a PPO counterpart of CPO (which coincides with SPPO) and use that as our baseline.\u201d This seems directly to contrast to the earlier statement which states that it is unclear how to modify the CPO methodology to other RL algorithms. Moreover, is this really a fair comparison? The original method has been modified to form a new baseline and I\u2019m not sure that it is \u201cwithout loss of generality\u201d. \n\nAlso, it is unclear whether the results can be accepted at face value. Are these averaged across several random seeds and trials? Will performance hold across them? What would be the variance? Recent work has shown that taking 1 run especially in MuJoCo environments doesn\u2019t necessarily provide statistically significant values. In fact the original CPO paper shows the standard deviations across several random seeds and compares directly against an earlier work in this way (PDO). Moreover, it is unclear why CPO was not directly compared against and neither was the \"equivalent\" baseline not compared on similar environments as in the original CPO paper. \n\nComments:\n\nFigure 3 is difficult to parse, the ends of the graphs are cut off. Maybe putting the y axis into log format would help with readability here or having the metrics be in a table.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}