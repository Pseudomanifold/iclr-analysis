{"title": "entirely reasonable paper, but novelty is unclear, empirical verification incomplete", "review": "In this paper, authors compare different ways to enforce stability constraints on trajectories in dynamic RL problems. It builds on a recent approach by Achiam et al on Constrained Policy Optimization (oft- mentioned \"CPO\") and an accepted NIPS paper by Chow which introduces Lyapunov constraints as an alternative method. While this approach is reasonable indeed, the novelty of the approach is questionable, not only in light of recent papers but older literature: inference of Markov Decision Processes under constraints is referred to and has been known a long time. Furthermore, the actual tasks chosen are quite simple and do not present complex instabilities. Also, actually creating a Lyapunov function and weighing the relative magnitude of its second derivative (steep/shallow) is not trivial and must influence the behavior of the optimizer. Also worth mentioning that complex nonlinearities might imply that instabilities in the observed dynamics are not seen and learned unless the space exploration is conservative. That is, comparison of CPO and Lagrangian constraint based RL with Lyapunov based method proposed depends on a lot of factors (such as those just mentioned) that are not systematically explored by the paper.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}