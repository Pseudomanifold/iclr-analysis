{"title": "A method to deal with the problem of fixed input image sizes in CNNs classifiers", "review": "This paper presents a strategy to overcome the limitation of fixed input image sizes in CNN classifiers. To this end, the authors incorporate some local and local attention modules, which fit inputs of arbitrary size to the fixed-size fully connected layer of a  CNN. The method is evaluated on three public classification benchmarks: CIFAR-10, ImageNet and Kaggle-Furniture128. \n\nThe results are better than those of the baseline architecture with fixed input size.\n\nEven though the need of handling arbitrary input size is an interesting problem, I have several major concerns about this paper:\n\n- One of the main problems of this paper is its presentation, both the writing and methodology. The writing is very poor, with continuous errors and many wrong definitions and concepts. For example, authors talk about \u2018data argumentation\u2019, \u2018pooling reduces the size of the hidden layers\u2019,\u2019back-to-back convolutional layers\u2019\n\nFurther, the paper is not well structured, which makes it very hard to follow.\n\nMethodology:\n\nAnother major concern is that I do not see how this approach allows the network to be input-size independent. If one looks at table 1, in both AIN-121 and AIN-169 the GAIL module employs kernel sizes equal to M/32xN/32, with M and N denoting the input image sizes. In this case, for each image, the kernel size will be different and, consequently, the number of learnable parameters. It is not clear to me how this is solved in this paper, as it ultimately results in a \u2018different\u2019 architecture for each different input size.\n\nWhen doing the sum on the proposed module, what does the result represent? absolute sum? mean of the sum? I also believe that a lot of information is lost when performing this operation (for example going from 32 to 1), in addition of the other spatial reductions during the network forward pass. Please comment on this and give a more detailed information about the proposed module.\n\nEvaluation: \nIn CIFAR-10, authors say that \u2018keep MOST of the setting similar to ResNet\u2019. What is then difference with the training with ResNet? For a fair comparison both settings should remain the same. In addition, what is the benefit of evaluating this approach on CIFAR-10, as the images are all of the same size? Furthermore, improvement is marginal with respect to the baselines (and it is not clear what is the reason behind the improvement), while increasing the model complexity by nearly 50%.\n\nKaggle-Furniture128: Why the learning is stopped exactly at epochs 38 and 53? Is this the same for all the networks? DenseNet and ResNet are pre-trained with what dataset?\n\nImageNet: In table 4, while the results for the baselines are evaluated on the validation set, the test set is used for evaluating the proposed approach. Furthermore, some results on the test set are obtained with \u2018augmentations\u2019. The reported values should correspond to the original test set without any kind of modification.\n\nMinor comments:\n\nThe authors assess the input fixed-size problem as a main problem in image processing. Despite being a limitation, some other image processing tasks, such as semantic segmentation, do not suffer from this problem, as CNNs are fully convolutional, and can accommodate images of arbitrary size.\n\nMany inconsistencies between terms: LAIL and then LAIN and GAIL and GAIN.'", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}