{"title": "Need more experiments to justify FoF and comparison to previous work ", "review": "This paper proposes a \"representation flow layer\" for action recognition, to learn hidden motion representation in a CNN end-to-end. The authors also come up with a concept termed \"flow-of-flow\". The paper is interesting, but I have several concerns.\n\n1. The concept of 'flow-of-flow' needs more justification. As shown in Table 5, flow-of-flow performs much worse than a single flow layer. After adding a conv layer in between, the performance gets better. I assume this is because the estimated flow are noisy, directly compute flow on them again making the result worse. It is the same as applying TVL1 twice. Adding the extra conv layer can help smoothing the feature map. So how about justify another configuration \"flow-conv\"? I just want to see if 'flow-of-flow' makes sense. \n\nActually, as pointed out by authors, flow-of-flow usually performs worse due to inconsistent optical flow results. I think the reason it gets a little bit higher accuracy here is because over-fitting. \n\nSo one more experiment of only \"flow-conv\", will help making the paper solid. \n\n2. The caption of figure 5 need more clarification. Is it TVL1 twice and FoF? or simply TVL1 and flow? Because subfig (b) and (c) do not align well. I am assuming this is computing the flow twice. \n\n3. I have several concerns for Table 9. \n\n(1) The authors do not provide results on HMDB dataset (2nd column). Is it because the performance is inferior to R(2+1)D or simply have no time before the deadline? This result is important, so please complete the table. \n\n(2) I don't see obvious advantage of the proposed approach over R(2+1)D and S3D. Especially for S3D, the performance is similar but S3D is faster. So the contribution of the paper is limited.  \n\n(3) If we compare the proposed approach to I3D, I3D is still better. Besides, there are several recent work reporting better performance on these two datasets. For example, ARTNet (Appearance-and-Relation Networks for Video Classification, CVPR18) report a 78.7 score on Kinetics. \n\n(4) The authors report their performance for 2D CNN and 2+1D CNN, but what about 3D CNN? It is also important to show. \n\n4. Since another goal of this paper is to improve efficiency, so I want to see a comparison, at least a discussion, to two previous literature. These two literature are the first work to propose to learn optical flow inside a CNN for action recognition. In these two literature, the authors incorporate FlowNet-like architectures to compute optical flow and feed them as input to the temporal stream. Because the FlowNet-like architecture can be quite shallow, their efficiency (fps) is very high, and the performance is on par with state-of-the-art.\n\n(1) Yi Zhu, et al, Hidden Two-Stream Convolutional Networks for Action Recognition\n(2) Laura Sevilla-Lara, et al, On the Integration of Optical Flow and Action Recognition\n\n\nIn conclusion, \n\n(1) In terms of novelty, the flow layer is adapted from TVNet, and the FoF concept need more clarification. \n(2) In terms of performance, there is no obvious advantage over previous work like S3D and R(2+1)D. And the experiments right now seem not complete. \n\nHence, I recommend a initial rating of 5. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}