{"title": "A decent contribution but requires more focus.", "review": "The paper considers cooperative multiagent learning tasks with identical individual agents. To solve the dilemma between individual and social optimality, an additive internal reward function is evolved on a slower timescale than policy learning.  This slower task was carried out within the so-called PBT multiagent training environment using genetic algorithms. \n\nThe paper relies heaviiy on evolutionary-related motivation and analogy, and hints at lessons that can be learned regarding the evolution of cooperation in wider contexts. As an RL  person, I must admit that I found this perspective too central and somewhat distractive of the essential algorithmic idea. As mentioned, the latter is to add an internal reward function, learned using the social reward as a fitness signal. \n\nThe proposed scheme makes a number of specific choices of architecture and algorithms along the way, which make it hard to give full credibility to conclusions found regarding the effectiveness of different components. \n\nSome details of the implementation are not clear, such as the reference to stop-gradient after equation (3). Later on that page an LSTM network is mentioned, for which I did not find details. The appendix could give a more detailed description of the implementation.\n\nOverall I found the presentation somewhat confusing, and not focused enough from the algorithmic learning perspective.\n    \n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}