{"title": "A marginally novel method to estimate classification confidence on novel data distributions; Experimental results need to be more comprehensive and they are not conclusive enough. ", "review": "The authors proposed two methods to deal with estimating classification confidence on novel unseen data distributions. The first idea is to use ensemble methods as the base approach that helps in identifying uncertain cases and then using distillation methods to reduce the ensemble into a single model mimicking behavior of the ensemble. They propose a generalization of this idea, that is to also perform distillation on a more generic unsupervised data distribution (than the supervised one that is used in training the ensemble). It is not clear whether this distribution should overlap with the novel distribution as a requirement or not. The second idea is to use a novelty detector classifier and weight the network output by the novelty score. \nMy major concern is that the comparison doesn't seem to be sufficiently comprehensive. The main method that is used to compare against is (Kendall & Gal, 2017), in which the main aim seems to be reducing uncertainty and improving generalization error under i.i.d. assumptions. This is different from the main focus of the paper, which is to better estimate classification confidence on novel data distributions. It seems that other approaches, such as \"Calibration methods\" (Guo et al. 2017) are better aligned with the focus of the paper, and should be considered instead. \nMy other concern is that the novelty seems to be marginal: either extending distillation methods in a very natural form, or weighting the network output using a novelty detector. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}