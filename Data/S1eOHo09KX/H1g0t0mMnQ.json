{"title": "budgeted feature acquisition to train networks - seems similar to RADIN", "review": "This paper presents a novel method for budgeted cost sensitive learning from Data Streams.\nThis paper seems very similar to the work of Contrado\u2019s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively \u201cpurchasing\u201d the most valuable features for the current datapoint under evaluation according to a budget. \n\nIn this process, a sample (S_i) with up to \u201cd\u201d features arrives for evaluation.  A partially revealed feature vector x_i arrives at time \u201ct\u201d for consideration.  There seems to exist a set of \u201cknown features\u201d that that are revealed \u201cfor free\u201d before the budget is considered (Algorithm 1).  Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility.  When the stopping condition is reached, a prediction is made.  After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated.\n\nThe ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado\u2019s RADIN paper.   The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks.\nThe value of this paper is in the idea that we can learn online and in a cost sensitive way.  The most compelling example of this is the idea that a patient shows up at time \u201ct\u201d and we would like to make a prediction of disease in a cost sensitive way.  To this end I would have liked to have seen a chart on how well this algorithm performs across time/history.  How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).\n\nAm I correct in assuming there are some base features that are revealed \u201cfor free\u201d for all samples?  If so how are these chosen?  If so how does the number of these impact the results?  \n\nIn Contrado\u2019s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset \u201ccardio.\u201d  Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset?  Did you actually re-implement RADIN or just take the numbers from their paper?  In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).\n\nWith respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm.  For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost.  The second example uses computational cost vs relevance gain.  This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost.  With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). \n\n In reality, these costs would be bundled.  You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience.  Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers.  Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront.  It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey.  The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage.  To show the value of your work, a better discussion of the cost savings would be appreciated.             \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}