{"title": "Batch normalization for RNNs with binary and ternary weights", "review": "* Summary\nThis paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering.\n\n\n* Strengths\n- I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering).\n- Encouraging results on specialized hardware implementation.\n\n\n* Weaknesses\n- Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution.\n- All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough.\n- Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims \"binaryconnect method fails\" and \"our method [...] outperforms all the existing quantization methods\" seem unfounded (Section 5.5).\n- In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified.\n- In the introduction on page 2 is argued that the proposed method \"eliminates the need for multiplications\" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? \n- The notation is quite confusing. For starters, in Section 2 you mention \"a fixed scaling factor A\" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace \"=\" with \"\\approx\". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation.\n\n\n* Minor Comments\n- Abstract: What is ASIC? It is not referenced in Section 6.\n- Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. \n- Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time.\n- Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point.\n- The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results.\n- Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is?\n- Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}