{"title": "Theoretical contribution is limited.", "review": "Prons: \nThis paper provides a simple and economic technique to accelerate adaptive stochastic algorithms. The idea is novel and preliminary experiments are encouraging.\n\nCons: \n1.\tThe theoretical analysis for AAMSGrad is standard and inherits from AMSGrad directly. Meanwhile, the convergence rate of AAMSGrad merely holds for strongly convex online optimization, which does not match the presented experiments. Hence, the theoretical contribution is limited. \n2.\tThe current experiments are too weak to validate the efficacy of the proposed accelerated technique. We recommend the authors to conduct more experiments on various deep neural networks. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}