{"title": "Interesting and potent interpretable LSTM without an actual interpretation in terms of the problem: the model claims to make variable/temporal variable importance but without actually interpreting the quality.", "review": "This paper describes a recurrent model (LSTM specifically, but generalizable) which can produce variable-wise hidden states that can be further used for two types of attentions: 1) variable importance for the importance of each variable (not accounting for time), and 2) temporal importance of each variable for the importance of each variable over time. The proposed NN model (IMV-LSTM) does not seem to directly provide such importance. Rather, the outputs are \u201cdecomposed\u201d for each variable/time that allows probabilistic inference on top of this.\n\nOne of my main concerns (described in Cons/Comments below) is how it is not straightforward to grasp the quality of variable importance and temporal variable importance results despite this is the key strength of this paper. If this comes from my lack of understanding, I would appreciate if the authors could provide a little more explanation.\n\nPros:\n1.\tThe overall quality of the paper is decent and mostly clear.\n2.\tThe experiments are quite extensive.\n3.\tThe fact that each variable should have different level of importance is interesting and practical.\n\nCons/Comments:\n1.\tThe term \u201ctensor\u201d is used throughout the paper to describe the stacked matrices. While this is not technically wrong to describe 2>-dimensional structures, this term could potentially imply (and make the readers to expect) tensor-based schemes such as tensor decomposition. This is not necessarily bad, but to me, \u201ctensor\u201d and \u201cvariable-wise correspondence\u201d do not seems to be associated too deeply since the \u201ctensor\u201d used in IMV-LSTM is a stack of matrices that are also independently used with respect to each other.\n\n2.\tThe variable importance experiments seem quite extensive and thorough, especially the lists of variable-wise temporal importance matrices provided in the appendix. However, the authors could provide some significance or relevance of the findings with respect to any domain knowledge or literature, it may help further appreciate and interpret the quality of the variable importance which is quite subjective to non-experts. Such information may not even need to be in the main paper; including a short description in the appendix.\n\n3.\tRelated to comments (2), the difference between IMV-Full and IMV-Tensor is hard to interpret since neither one is always better than the other (i.e., IMV-Full > IMV-Tensor in some experiments, vice versa). While the key difference is speculated to be from how the LSTM handles the variables, I am curious how this related to the differences in the results and how the differences variable importance results (i.e., Fig.3) can be in at least speculated.\n\nQuestions:\n1.\tShould \\tilde{h}_t in Figure 1 (a) be \\tilde{h}__{t-1} since this hidden state is from t-1? The figure itself currently implies that the hidden state for t is used, but this is computed from x_t using U_j. With \\tilde{h}__{t-1}, it follows Eq.(1).\n\n2.\tIn Equation set 2 for IMV-Tensor, are W and U (not W_j and U_j) also in tensor forms so each variable and hidden state get transformed correspondingly (i.e., W_1 for h^1_{t-1}, U_1 for x^1_t).\n\n3.\tThe IMV-Tensor version of IMV-LSTM (related to the question above) can be considered as a set of parallel LSTMs, one for each variable. Such independence could also be inferred from Figure 1. If that\u2019s the case, where do the variables \u201cinteract\u201d with each other? Is this happening in the later stage where the hidden states across variable/time are aggregated in the attention stage (Eq.(8) and on)?\n\n4.\tUp until Eq.(8), n was used for the variable index where n = 1,\u2026,N. In Eq.(8), it seems to be still used as the variable index (i.e., h_T^n and g^n), but it is also a set of possible values for a random variable z_{T+1}. Is n used the same way for z_{T+1} as well? I am slightly confused on how z is used. Also, (just to clarify), if we use N variables, we are using y_t as well (i.e., [x_t^1,\u2026,x_t^{N-1}, y_t])?\n\n5.\tf_agg: Is this for aggregating over instances? For \\bar{\\alpha}^n, I\u2019m guessing this is aggregated over instances for variable n for t=1,\u2026,T_1.\n\n6.\tI am not too familiar with the notion of \u201ctime-lag\u201d in the experiments. If the authors could explain this a little bit, I would appreciate it.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}