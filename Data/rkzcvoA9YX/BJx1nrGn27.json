{"title": "Strong results but paper is based somewhat on minor changes from previous work", "review": "This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples. It proposes an extension to Learning to Compare (Sung 2017), where Learning to Compare learns a relation module that is parametrized to product a similarity score between whole feature-maps of embedded support and query set examples. Rather than measure similarity between examples at the feature-map level, this work proposes measuring similarity at the object-level. Specifically, an object is represented by each (i,j) position on the final feature-map derived through the embedding network. Then to compare a support image and query image, we compare pairwise across all (i,j) positions on the final feature-map between support and query images by computing object similarity features. The pairwise object similarity features are then summed and mapped to a similarity probability between the support and query example. Note that the network structure used for object comparison here is derived from the Relation network from Santoro 2017. The model is then trained in the same way as Learning to Compare, where we want support and query examples with the same class label to have similarity probability of 1. Experiments are conducted on Omniglot and Mini-Imagenet, comparing the proposed method to previous work.\n\nPros:\n- Decent improvement in Mini-Imagenet results relative to Learning to Compare (close to +9% for 1-shot and +5% for 5-shot relative to Learning To Compare) .\n- One concern I had was that improvement in performance was due to working with 224 x 224 images in this paper (rather than resized 84 x 84 images as most previous work does with Mini-Imagenet); however, additional comparison was performed with Learning To Compare where that model also uses 224 x 224 images but there was no improvement there just by using larger image input, thus establishing that larger image input is not a reason for the improved performance.\n\nCons:\n- The proposed idea is not particularly novel, as it is basically the Learning to Compare model but with pairwise comparison at each location in the final feature map rather than comparing the feature maps as a whole.\n\nRemarks:\n- Figure 4 does not seem to be that useful. It is used to comment that overfitting is not an issue by showing training and testing performance on Omniglot dataset but maybe would be more useful if Learning to Compare was also shown in those plots to compare the level of overfitting between the two models.\n\nSantoro et al. A simple neural network module for relational reasoning. 2017.\nSung et al. Learning to Compare: Relation Network for Few-Shot Learning. 2017.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}