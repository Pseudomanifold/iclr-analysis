{"title": "Repeating the old story from other papers, quit limited novelty, lacking solid experiments", "review": "The main concerns come from the following parts:\n\n\n(1) Repeating the old story from other papers:\nA large part of math is from previous works, which seems not enough for the ICLR conference.\nIt is very surprising that the authors totally ignore the latest improvements in neural network compression. Their approach is extremely far away from the state of the art in terms of both methodological excellence and experimental results. The authors should read through at least some of the papers I list below, differentiate their approach from these pioneer works, and properly justify their position within the literature. They also need to show a clear improvement on all these existing pieces of work. \n\n(2) quite limited novelty:\nIn my opinion, the core contribution is replacing SGD with Adam.\nFor network compression, it is common to add L1 Penalty to loss function. The main difference of this paper is change SGD to Adam, which seems not enough. \n\n(3) lacking solid experiments:\nIn section Experiment, the authors claim \"Finally, we show the trade-off for pruning Resnet-50 on the ILSVRC dataset.\", but I cannot find the results. \n\nIs the ResNet-32 too complex for cifar-10? Of course, it can be easily pruned if the model is too much capacity for a simple dataset.  Why not try the Resnet-20 first?\n\n[1] C. Louizos et al., Bayesian Compression for Deep Learning, NIPS, 2017\n[2] J. Achterhold et al., Variational Network Quantization, ICLR, 2018", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}