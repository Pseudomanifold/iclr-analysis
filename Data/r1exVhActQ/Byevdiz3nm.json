{"title": "an interesting perspective on the L1 regularization of neural network", "review": "This paper discusses the effect of L1 penalization for deep neural network. In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. \n\nThe perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. If the coefficients of the linear equation are distributed in general positions, then the number of variables should not be larger than the number of equations. \n\nWhile I mostly like the paper, I would like to point out some possible issues:\n\nmain concerns: \n\n1. the columns of V may not be independent during the optimization(training) process. In this situation, I am not quite sure if the assumption of \u201cgeneral position\u201d still holds. I understand that in literatures of Lasso and sparse coding it is common to assume \u201cgeneral position\u201d. But in those problems the coefficient matrix is not Jacobian from a learning procedure. \n\n2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. It is against the empirical observation that when lambda is extremely small, effect of the regularizer tends to be almost zero. Can authors also show this effects empirically, i.e., when the regularization coefficients decrease, the nnz does not vary much? (Maybe there is some optimization details or approximations I missed?)\n\nSome minor notation issues:\n1. in theorem 1: dim(W^{(j)})=d should be dim(vec(W^{(j)}))=d\n2. in theorem 1: Even though I understand what you are trying to say, I would suggest we describe the jacobian matrix V in details. Especially it is confusing to stack vec(X^J) (vec(W^j)) in the description.\n3. the notations of subgradient and gradient are used without claim\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}