{"title": "main contribution: contribution of multi-modal dataset, evaluation code for learning representation tasks and results on dataset", "review": "The paper looks into contribution of data set for multi-modal learning using X-Flight simulator in various settings. The authors also contribute code for evaluation of the learning representation tasks and present the results for the data using various setups from autoencoders to dynamics model, using sensor only data and combining image and sensor data, and predicting various timesteps.\n\nImprovements\nMultimodal datasets have been made available previously in Image, video, text combinations, where the outcome was clear (for e.g learning caption etc.), however, in this dataset, the task is more challenging (for e.g predicting the various sensor readings or landing outcome). The paper would benefit from \n- adding clarification on the Learning tasks, as some of the descriptions/settings and result discussion need more explanation. An e.g predicting the timesteps ahead can be meaning different things, depending on when the start time is, sampling rate and the time to land. \n- measure of the scale where only MSE is mentioned for the tasks in the results\n- why the time with lower latent dimensions was same as with higher\n- the explanation for some of the measure being out of whack for some settings is attributed to challenges with the data set and e.g. is provided for images with nighttime landing. A quantitative number around such cases/for the e.g. in the training data, and test data would be good\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}