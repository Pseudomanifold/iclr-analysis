{"title": "limited insight and novelty", "review": "In this paper, the authors propose POLO, a reinforcement learning algorithm which has access to the model of the environment and performs RL to mitigate the planning cost. For the planning, POLO uses the known model of the environment up to a fixed horizon H and then use an approximated value function in the leaf nodes. This way, instead of planning for an infinite horizon, the planning is factored to a shorter horizon, resulting in lower computation cost.\n\nThe novelty and motivation behind this approach is limited. Similar or even more general approach for discrete action space is introduced in \"Sample-Efficient Deep RL with Generative Adversarial Tree Search\" where they also learn the model of the environment and additionally consider the error due to the model estimation. There is also a clear motivation in the mentioned paper while I could not find a convincing one for the current paper. \nPutting the novel limitation aside,  both of these paper, the current paper, and the paper I mentioned, suffer from very lose estimation bounds. Both of these works bound somewhat similar (not the same) things via L_inf error of value function which in practice does not necessarily result in useful or insightful upper bounds (distribution dependent bound is desired). Moreover, with the assumption of knowing the environment model, the implication of the current work is significantly limited.\n\nThe authors do a good job of writing the paper and the paper is clear which is appreciatable.\n\nIn equation 6 the authors use log-sum-exp and claim it corresponds to UCB, but they do not provide any evidence to support their claim. \n\nIn addition, the Bayesian linear regression in the tabular setting is firstly proposed in Generalization and Exploration via Randomized Value Functions and beyond tabular setting (the setting in the current paper) was proposed in Efficient Exploration through Bayesian Deep Q-Networks. \n\nThe claims in this paper are not strong enough and the empirical study does not strongly support or provide sufficient insight. For example experiments in section 3.2 does not provide much insight beyond common knowledge.\n\nWhile bridging the gap between model based and model free approaches in RL are significantly important research directions in RL, I do not find the current draft significant enough to shed sufficient light into this topic.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}