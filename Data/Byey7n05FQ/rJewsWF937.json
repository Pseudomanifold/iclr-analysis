{"title": "Lucid paper with nice ideas, but problem setting not completely clear", "review": "This paper was a joy to read.   The description and motivation of the POLO framework was clear, smart, and sensible.  The fundamental idea is to explore the interplay between value-function estimation and model-predictive control and demonstrate how they benefit one another.  None of these ideas is fundamentally new, but the descriptions and their combination is very nice.\n\nAs I finished the paper, though, I was left with a lingering lack of understanding of the exact problem setting that is being addressed. The name is cute but didn't help clarify.    As I understand it:\n- we have a correct dynamics model (I'm assuming that's what \"nominal dynamics model\" means) and a good trajectory optimization algorithm\n- the agent has limited online cognitive capacity\n- there is no opportunity for offline computation\nIf offline computation time were available, then we could run this algorithm (or your favorite other RL algorithm) in the agent's head before taking any actions in the actual world.   That does not seem to be the setting here, although it does seem to me that you might be able to show that POLO is a good algorithm for finding a value function, offline, with no actual interaction with the world.\n\nSo, fundamentally, this paper is about action under computational time constraints.   One strategy would be for the robot to use 7 of its cores to run your favorite approximate DP / RL algorithm in parallel with 1 core that's used for action selection.  Why is that worse than your algorithm 1?\n\nSetting this question aside, I had some other comments:\n- It is better *not* to use \"trajectory optimization\" and \"model-predictive control\" interchangeably.  I can use traj opt in other circumstances (e.g. with open loop trajectory following) and could use other planners for MPC.\n- Some version of lemma 2 probably (almost certainly) already exists somewhere in the literature;  I'm sorry, though, that I can't point you to a concrete reference.\n- The argument about MPC letting us approximate H Bellman backups is plausible, but seems somewhat subtle;  it would be good to elaborate it in some more detail.\n- The set of assertions and experiments is very nice.\n- Why are no variances shown in figure 3?   Why does performance seem to degrade after a certain horizon.\n\nThis paper doesn't seem really to be about learning representations.  I don't know if that's important to the ICLR decision-making.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}