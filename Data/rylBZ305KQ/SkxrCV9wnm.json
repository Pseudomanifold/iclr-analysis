{"title": "Limited novelty on the modeling side + lack of motivation on the relevance of the overall problem", "review": "The paper proposes to model shifts in language use over time with a recurrent neural network architecture augmented with stochastic variables. Stochastic variables are sampled at the beginning of each time segment and are supposed to capture variability introduced by the time shift. The proposed architecture is mainly compared to approaches that model evolution of \u201cword embeddings\u201d through time and the authors seem, throughout the paper, to contrast their approach to these streams of works. Experimental results show the proposed model architecture outperform RNNs augmented with dynamic word embedding models at test time, in language modeling settings.\n\nThe first drawback of the paper is the limited novelty of the architecture. These types of architectures are well-known and have already been used in a variety of other tasks (Fraccaro et al. https://arxiv.org/pdf/1605.07571.pdf, Serban et al. (https://arxiv.org/pdf/1612.00377.pdf) , etc...). Therefore, the contribution of this paper is to apply this architecture to model shifts in language over time.\n\nFurthermore, it is not clear to me how the proposed model can be used to track evolution of semantic similarities between words over time as well as detecting topical changes. Estimating dynamic word embeddings has the positive aspect of enhancing interpretability by modeling the evolution of emergent \u201ctopics\u201d across time. This seems to me one of interesting aspects in modeling time shift in language and, for this reason, it has been previously extensively studied. How would that be possible under the proposed approach ? How to extract the evolution of the semantic field of a specific term across time under the proposed model ? In this aspect, contrasting the current architecture with models based on dynamic word-embeddings harm the paper.\n\nFollowing up on these last considerations, the relevance of the problem tackled in this paper seems a bit weak/unclear to me. Why would it be important to form model of language that are robust across time-spans ? Couldn\u2019t we argue that the same models could be \u201cretrained\u201d on new data as it comes in which would lead to study (language) models that can continually learn in an efficient fashion ?\n\nIn summary, the main points of concern are:\n- the limited novelty on the modeling side\n- the lack of proper motivation on the importance of the addressed problem \n\nMinor remarks:\n\n- Yao et al. (2018) \u201cuse alternate optimization that breaks the flow of gradient through time\u201d , unclear as their method can be optimized using stochastic gradient descent, as the Yao et al. point out. Maybe one drawback is parameter efficiency.\n\n- Learning a transition function in the latent space has also been proposed in Fraccaro et al. (https://arxiv.org/pdf/1605.07571.pdf), and should be cited.\n\n- What would these stochastic variables capture ? Why it is a good idea for the current problem ?\n\n- Page 4: DiffDtime -> DiffTime\n\n- How do you compute PPL in your setting ? Is it the upper bound coming from the ELBO ? It would be crucial to see evolution of the KL divergence during time, as a hint of how much information is encoded into the latent variables.\n\n- \u201crecursive inference\u201d, e.g. either finetuning q for a particular x or at test time is related to Salimans et al. (https://arxiv.org/pdf/1410.6460.pdf) and therefore should be cited.\n\n- I cannot really see that \u201cDRLM-F significantly improves long-term performances on NYT\u201d. From the graph, I can see a gap of ~1,1.5 perplexity point from 126 to ~124.5 which doesn\u2019t seem striking to me.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}