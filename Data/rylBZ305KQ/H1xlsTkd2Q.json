{"title": "A straightforward extension of recurrent neural network language model with stochastic hidden variables. A simplified version of stochastic RNN.", "review": "This paper presents a straightforward extension of recurrent neural network language model (RNNLM) by considering the stochastic hidden variables rather than deterministic hidden variables in traditional RNNLM. The motivation is to capture the shift of meaning in language over time based on this stochastic modeling. This RNN model is developed with twofold considerations. One is to represent the latent transition while the other is to perform the reconstruction. Two separate RNNs were run to fulfill the whole model. The key of this paper is the regularization term in Eq. (3) which constrains the transition of two neighboring hidden states.  This paper considers the global latent variables at each time step as well as the transition function over different time steps. The transition of latent variables in two neighboring steps is modeled by a Gaussian distribution with the mean driven by latent variable in former step which is similar to what dynamic topic model was doing.\n\nPros: A simplified version of stochastic RNN which adopts a Gaussian distribution for state transition with neural network based mean parameter. Idea is interesting. Solution is meaningful.\n\nCons: \n1. The stochastic or variational recurrent neural networks have been published before. Citations to these previous works are missing in this paper.\n2. A variant of stochastic RNN with a number of assumptions and simplifications. Two RNNs were used and separately trained. The transition probability from z_{t-1} to z_t is independent of observation x. This is odd.\n3. In variational neural network, there are an encoder and a decoder for recognition and generation. This work is missing such a structure. It is difficult to explain how latent variable is obtained and how new word is predicted.\n4. Although the experimental results show improvement, the justification is still not enough. The comparison with other variational RNN (e.g. the work entitled \"Sequential Neural Models with Stochastic Layers\") is missing. The visualization of latent information is required.\n3. Typo: The sentence in the second line of Conclusion \".. conditioned and ...\" is incomplete.\n\nRemarks:\n1. Deriving a hybrid criterion and joint optimization over the parameters (for encoder and decoder) is suggested. \n2. The interpretation about how the training data are related to the stochastic transition is required.\n3. Suggest to figure out the details of Eq. (3) instead of simply citing Krishnan et al. (2017).", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}