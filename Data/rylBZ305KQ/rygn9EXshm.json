{"title": "Little awareness to previous attempts to dynamic language models", "review": "This paper aims to improve sequential prediction using LSTM by incorporating\nanother latent variables {z_t} that follow distinct dynamics rather than LSTM\nitself. \nExperimental results on comparing with naive LSTM and dynamic word embedding\napproaches confirmed that the proposed method works better than these \nbaselines.\n\nBasically the proposed method is sensible, but this paper has two clear\ndrawbacks: (1) it is not compared with similar previous attempts; and (2)\nthe proposed architecture is not guaranteed to represent \"global\" contextual\nbehavior rather than local contextual one.\n\n(1) There have been numerous attempts in sequential modeling, especially\nlanguage models, to incorporate a global and contextual factor into prediction.\nEarly attempts used latent dirichlet allocation combined with n-gram models;\nafterwords, LSTM became prevalent, and in fact there exists a famous baseline\nof Latent LSTM allocation [1]. Therefore, it is minimally required for this\npaper to compare the proposed model with latent LSTM.\n\n(2) Latent LSTM above has a clear advantage that there is an explicit mechanism\nof latent topic distribution that governs a specific sequence (global variable).\nHowever, the proposed method in this paper, contextual behavior is determined\nonly locally, and might be quite prone to overfit to exceptional observations,\nin this case, words. Of course, it might handle a global context by chance,\nbut the architecture does not guarantee it. Therefore, such a potential\ndisadvantage should be experimentally investigated by a comparison with a\nsuitable baselines noted above.\n\nI agree with the authors that the contexual behavior should be continuous\nand flexible, such as pursued in this paper. Thus a challenge is how to\nsuitably incorporate a global context into such a representation.\nFinally, I would like to know what kind of function g is learned through this\nexperiment. Investigating into the strengths and weaknesses of the learned\nfunction g is also beneficial for a more flexible language models in the \nfuture.\n\n[1] Zaheer, M., Ahmed, A., & Smola, A. J. (2017). Latent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence Data. In International Conference on Machine Learning (pp. 3967-3976).\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}