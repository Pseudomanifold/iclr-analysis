{"title": "Review", "review": "This paper proposes a method, so-called MixFeat that can mix features and labels. This method is in a similar line of the methods such as mixup and manifold mixup. \n\npros)\n(+) The proposed method looks simple and would have low computation cost at inference phase.\n(+) The experiment of evaluating the possibility of reducing the number of datasets looks good.\n\ncons)\n(-) The advantages of the proposed method are not clarified. There should be at least one insight why this method can outperform others.\n(+) Decomposition of r and theta in eq.(1) looks interesting, but there is no supporting ground to grasp the implicit meaning of this idea. Why the parameters a and b are reparameterized with r and theta?\n(-) Figure 1 is not clearly illustrated and confusing. Just looking at the figure, one can understand mixup is better than others.\n(-) This paper does not contain any results validated on ImageNet dataset. This kind of method should show the effectiveness on a large scale dataset such as ImageNet dataset.\n\ncomments)\n- It would be better to compare with Shake-type method (shake-drop (https://arxiv.org/pdf/1802.02375.pdf), shake-shake) and SwapOut (https://arxiv.org/pdf/1605.06465.pdf). \n- The performance of PyramidNet in Table 1 looks different from the original one in the original paper (https://arxiv.org/pdf/1610.02915.pdf).\n\nThe paper proposes an interesting idea, but it does not provide any insights on why it works or why the authors did like this. Furthermore, the experiments need to contain the results on a large scale dataset, and from the formulation eq.(1), the proposed method looks similar to a single-path shake-drop or shake-shake, so the authors should compare with those methods.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}