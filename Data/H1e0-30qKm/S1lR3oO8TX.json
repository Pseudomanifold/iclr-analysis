{"title": "review", "review": "[EDIT]: I have updated my score after the author response and paper revision.\n=============================\n\n[I was asked to step in as a reviewer last minute. I did not look at the other reviews].\n\n-------------------------------\nSummary\n-------------------------------\nThis paper proposes to learn disentangled latent states under the GAN framework. The core idea is to partition the latent states into N partitions, and correspondly have N Siamese networks that pull the generated images with the same latent partition towards each other, along with a contrastive loss which ensures generated images with different latent partitions to be different. The authors experiment with two setups: in the \"unguided setup\" training is completely unsupervised, while in the \"guided\" setup, there is some weak supervision to encourage different partitions to learn different factors.\n\n-------------------------------\nEvaluation\n-------------------------------\nWhile the motivation is nice, I find the results (especially in the unguided setup) underwhelming. This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different. Results with weak supervision (their method for injecting weak supervision was very nice) are more impressive. However, there is no comparison against existing work. Learning disentangled representations with deep generative models is very much an active area. Here are some recent papers:\n\nhttps://openreview.net/references/pdf?id=Sy2fzU9gl\nhttps://arxiv.org/abs/1802.05822\nhttps://arxiv.org/abs/1802.05983\nhttps://arxiv.org/abs/1802.04942\n\nImportantly, there are no quantitative metrics. I do not think this work is ready for publication.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}