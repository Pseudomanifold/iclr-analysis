{"title": "An interesting heuristic and some interesting derivations, but there's a gap between the two.", "review": "This paper proposes a simple heuristic for tuning HMC's parameters: just optimize the expected log-density of the Lth sample. It seems to work reasonably well on the problems the authors evaluate on.\n\nThis heuristic is arrived at by a somewhat roundabout derivation, which I found interesting (although many of the same ideas are implicit in Salimans et al. (2014; \"MCMC & VI: Bridging the Gap\")). But ultimately this derivation comes to a head at this very heuristic argument:\n\n\u201c\u2026since qL(zL; \u03c6) converges to p\u03b8(z|x) as L increases, we expect the effect of H[qL(zL; \u03c6)] on \u03c6 to be small and that most of the similarity of qL(zL; \u03c6) to p\u03b8(z|x) will be captured by the first term in the RHS of (15). Therefore, we propose to tune \u03c6 by optimizing the tractable objective given by the first term\u2026\u201d: \n\nI don\u2019t see why this argument applies to the entropy term and not to the log-joint term. In particular, If q_L has really converged to p(z|x), then there\u2019s no point optimizing \u03c6 either, right?\n\nHere\u2019s a concrete example of how I could imagine this procedure going wrong: make q(z0) a delta at the latent vector z* that maximizes the log-joint, and set the step size of the Hamiltonian simulation to 0. This will make the entropy term (which is ignored) -\u221e, maximize the log-joint term, and I think it even makes D^L_{KL}=0.\n\nIt seems like this isn\u2019t what actually happens experimentally, though\u2014perhaps I\u2019m missing something?\n\nRegarding the experiments, a natural baseline would be something akin to the approach of Hoffman (2017; \"Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo\u201d), simply initializing the HMC sampler with a mean-field Gaussian. I would expect this to produce worse results for small numbers of steps, since the variational Gaussian would choose a single mode, but I\u2019m curious how the quantitative metrics would compare.\n\nSome more minor points:\n\n* \u201cVariational auto-encoders (VAEs) (Kingma & Welling, 2014) are DGMs trained by using mean-field\nVI with a Gaussian parametric distribuion and amortization.\u201d I disagree with this terminology\u2014DGMs trained with, say, IAF are routinely called VAEs.\n\n* Section 3.1: It might be good to clarify that you\u2019re describing exact Hamiltonian integration, whereas in practice one always uses a discretized numerical integrator. (The leapfrog integrator is reversible and preserves volume, but doesn\u2019t conserve energy, so this does make the results a bit more complicated.)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}