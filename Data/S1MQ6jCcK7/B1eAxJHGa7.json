{"title": "Interesting Approach with Insufficient Results", "review": "This paper presents an apparently original method targeted toward models training in the presence of low-quality or corrupted data. To accomplish this they introduce a \"mixture of correlated density network\" (MCDN), which processes representations from a backbone network, and the MCDN models the corrupted data generating process. Evaluation is on a regression problem with an analytic function, two MuJoCo problems, MNIST, and CIFAR-10.\n\nThis paper's primary strength is that the proposed method is a tool quite distinct from recent work, in that it does not use bootstrapping or solely use corruption transition matrices. The paper is typeset well. In addition to this, the experimentation has unusual breadth.\n\nHowever, the synthetic regression task is a nice proof-of-concept, but thorough regression evaluation could perhaps include the Boston Housing Prices dataset or some UCI datasets.\n\nThe hamartia of this paper is that it does not provide sufficient depth in its computer vision experiments. For one, experimentation on CIFAR-100 would be appreciated.\nIn the CIFAR-10 experiments, they consider one label corruption setting and lack experimentation on uniform label corruptions.\nThe related works has thorough coverage on label corruption, but these works do not appear in the experiments. They instead compare their label corruption technique to mixup, a general-purpose network regularizer. It is not clear why it is thought the \"state-of-the-art technique on noisy labels\"; this may be true among network regularization approaches (such as dropout) but not among label correction techniques. For this problem I would expect comparison to at least three label correction techniques, but the comparison is to one technique which was not primarily designed for label corruption.\n\n\nNitpicks:\n-In the related works we are told that a smaller learning rate can improve label corruption robustness. They train their method with a learning rate of 0.001; the baseline gets a learning rate of 0.1.\n-The larger-than-usual batch size is 256 for their 22-4 Wide ResNets, and at the same time they do not use dropout (standard for WRNs of this width) and use less weight decay than is common. Is this because of mixup? If so why is the weight decay two orders of magnitude less for your approach compared to the baseline? How were these various atypical parameters chosen?\n-They also use gradient clipping for their method, which is extremely rare for CIFAR-10 classification. Why is this necessary?\n-This document could be cleaner by eschewing the Theorem of this paper, which \"states that a correlation between two random matrices is invariant to an affine transform.\" For this audience, I suspect this theorem is unnecessary. Likewise the three lines expended for the maths of a Gaussian probability density function could probably be used for other parts of this paper.\n-\"a leverage optimization method which optimizes the leverage of each demonstrations is proposed. Unlike to former study,\" -> \"a leverage optimization method which optimizes the leverage of each demonstration is proposed. Unlike a former study,\"\n-\"In the followings,\" -> \"In the following,\"\n\nEdit: The updated results need consistent baselines. For example, the method of [7] should be consistently compared against.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}