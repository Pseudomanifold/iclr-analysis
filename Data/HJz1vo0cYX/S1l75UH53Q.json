{"title": "Behavior of the new loss at the beginning of the training and weird empirical results", "review": "This paper describes a new method to calibrate the accuracy and confidence of the prediction. The authors propose a variance-weighted confidence-integrated loss function to balance the accuracy and confidence via stochastic inference. The empirical results show the proposed method performs better than previous related methods. \n\nCompared to the previous work Pereyra et al. (2017) and Lee et al. (2018), the novelty of this paper is to use different $\\alpha_i$ for different training example instead of a global hyper-parameter $\\beta$. When the variance of prediction is high, the loss weights more on the cross-entropy term with uniform distribution and when the variance of prediction is low, the loss weights more on the cross-entropy loss with ground-truth. \n\nI wonder how this new loss works at the beginning of the training. At the beginning of the training, the variance of the prediction for most samples is high, so the new loss forces the prediction to be close to the uniform distribution. How could the model learn the prediction distribution by optimizing this loss?\n\nThe new loss requires multiple forward passes in order to compute $\\alpha_i$ while the previous method Eq. 10 only requires one pass. So the computational cost of the new method is $n$ times of the previous method where $n$ is the number of forward passes in the new method. It seems unfair to use the same number of epochs for the new and previous methods. I wonder what the comparison would be if training the previous method for longer.\n\nI\u2019m not familiar with the literature of the calibration scores and cannot tell whether the improvement is significant or not. However, the test accuracy of DenseNet on CIFAR-100 is much lower than the number in Huang et al. (2016). And the new loss only achieves a significant improvement on DenseNet while the improvement is marginal on the other architectures. I wonder if the significant improvement is because of some implementation mistakes.\n\nOverall, the idea to use data-dependent weights is interesting and reasonable. However, some issues in terms of both methodology and experiments need to be clarified.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}