{"title": "This could be an interesting paper but currently requires a lot of experimental and writing improvements", "review": "The paper proposes to use successor features for the purpose of option discovery.  The idea is to start by constructing successor features based on a random policy, cluster them to discover subgoals, learn options that reach these subgoals, then iterate this process.  This could be an interesting proposal, but there are several conceptual problems with the paper, and then many more minor issues, which put it below the threshold at the moment.\n\nBigger comments:\n1. The reward used to train the options (eq 5) could be either positive or negative.  Hence, it is not clear how or why this is related to getting options that go to a goal.\n2. Computing SRs only for a random policy seems like it will waste potentially a lot of data. Why not do off-policy learning of the SR while performing  the option?\n3. The candidate states formula seems very heuristic. It does not favour reaching many places necessarily (eg going to one next state would give a 1/(1-gamma) SR value)\n4. Fig 5 is very confusing. There are large regions of all subgoals and then subgoals that are very spread out.  If so many subgoals are close by, why would an agent explore? It could  just jump randomly in that region for a while. It would have been useful to plot the trajectory distribution of the agent when using the learned options to see what exactly the agent is doing\n5. There are some hacks that detract from the clarity of the results and the merits of the proposed method. For example, options are supposed to be good for exploration, so sampling them less would defeat the purpose of constructing them, but that is exactly what the authors end up doing. This is very strange and seems like a hack. Similarly, the use of auxiliary tasks makes it unclear what is the relative merit of the proposed method. It would have been very useful to avoid using all these bells and whistles and stick as closely as possible to the stated idea.\n6. The experiments need to be described much better. For example, in the grid worlds are action effects deterministic or stochastic? Are start state and goal state drawn at random but maintained fixed across the learning, or each run has a different pair? Are parameters optimized for each algorithm?  In the plots for the DM Lab experiments, what are we looking at? Policies? End states? How do options compare to Q-learning only in this case? Do you still do the non-unit exploration? The network used seems gigantic, was this optimized or was  this the first choice that came to mind? Would this not overfit? What is the nonlinearity?\n\nSmall comments:\n- This synergy enables the rapid learning Successor representations by improving sample efficiency. \n- Argmax a\u2019 before eq 1\n- Inconsistent notation for the transition probability p\n- Eq 3 and 4 are incorrect (you seem to be one-off in the feature vectors used)\n- Figure 2 is unclear, it requires more explanation\n- Eq 6 does not correspond to eq 5\n- In Fig 6 are the 4 panes corresponding top the 4 envs.? Please explain. Also this figure needs error bars \n- It would be useful to plot not just AUC, but actual learning curves, in order to see their shape (eg rising faster and asymptoting lower may give a better AUC). \n- Does primitive Q-learning get the same number of time steps as *all* stages of the proposed algorithm? If not, it is not a fair comparison\n- It would be nice to also have quantitative results corresponding to the experiments in Fig 7.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}