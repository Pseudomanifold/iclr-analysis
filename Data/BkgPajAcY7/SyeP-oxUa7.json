{"title": "interesting investigation with worthwhile contribution; some suggested areas of improvement", "review": "This paper is about exploring better baselines for sentence-vector representations through randomly initialized/untrained networks. I applaud the overall message of this paper that we need to evaluate our models more thoroughly and have better baselines. The experimentation is quite thorough and I like that you\n1) explored several different architectures\n2) varied the dimensionality of representations\n3) examine representations with probing tasks in the Analysis section. \n\nMain Critique\n- In your takeaways you say that, \u201cFor some of the benchmark datasets, differences between random and trained encoders are so small that it would probably be best not to use those tasks anymore.\u201d I don\u2019t think this follows from your results. Just because current trained encoders do not perform better than random encoders on these tasks doesn\u2019t in itself mean these tasks aren\u2019t good evaluation tasks. These tasks could be faulty for other reasons, but just because we have no better technique than random encoders currently, doesn\u2019t make these evaluation tasks not worthwhile. Perhaps you could further examine what features (n-gram, etc.) it takes to do well on these tasks in order to argue that they shouldn\u2019t be used.\n- In your related work section you say that \u201cWe show that a lot of information may be crammed into vectors using randomly parameterized combinations of pre-trained word embeddings: that is, most of the power in modern NLP systems is derived from having high-quality word embeddings, rather than from having better encoders.\u201d Did you run experiments with randomly initialized embeddings? This paper (https://openreview.net/forum?id=ryeNPi0qKX) finds that representations from LSTMs with randomly initialized embeddings can perform quite well on some transfer tasks. I think in order to make such a claim about the power of high-quality word embeddings you should include numbers comparing them to randomly initialized embeddings.\n\nQuestions\n- Did you find that your results were sensitive to the initialization technique used for your random LSTMs / projections?\n- Do you have a sense of why random non-linear features are able to perform well on these tasks? What kind of features are the skip-thought and InferSent representations learning if they do not perform much better? It\u2019s interesting that many of the random encoder methods outperform the trained models on word content. I think you could discuss these Analysis section findings more.\n\nOther Critiques\n- In the introduction, instead of simply describing what is commonly done to obtain and evaluate sentence embeddings, it would be better to include a sentence or two about the motivation for sentence embeddings at all.\n- The first sentence, \u201cSentence embeddings are learned non-linear recurrent combinations of pre-trained word embeddings\u201d, doesn\u2019t seem to be true as BOE representations are also sentence embeddings and CNNs/transformers could also work. \u201cNon-linear\u201d and \u201crecurrent\u201d are not inherent requirements for sentence embeddings, but just techniques that researchers commonly use.\n- In the second paragraph of introduction instead saying \u201cNatural language processing does not yet have a clear grasp on the relationship between word and sentence embeddings\u2026\u201d it might be better to say \u201cNLP researchers\u201d or the \u201cNLP community\u201d instead of \u201cNLP\u201d as a field doesn\u2019t have a clear grasp.\n- In the introduction: \u201cIt is unclear how much sentence-encoding architectures improve over the raw word embeddings, and what aspect of such architectures is responsible for any improvement.\u201d It would be also good to mention that it\u2019s unclear how much the training task / procedure also is affects improvements.\n- You could describe more about applications of reservoir computing in your related work section as it\u2019s been used in NLP before.\n- I don\u2019t think you actually ever describe the type of data that InferSent is trained on, only that it is \u201cexpensive\u201d annotated data. It might be useful to add a sentence about natural language inference for clarity.\n- In the conclusion, change \u201cperformance improvements are less than 1 and less than 2 points on average over the 10 SentEval tasks, respectively\u201d to  \u201cperformance improvements are less than 2 percentage points on average over the 10 SentEval tasks, respectively\u201d\n- It would be nice if you bolded/underlined the best performing numbers in your results tables.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}