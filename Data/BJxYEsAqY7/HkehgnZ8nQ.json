{"title": "A review", "review": "In this paper, the authors present two methods, Sequential and Parallel-FEED for learning student networks that share architectures with their teacher.\n\nFirstly, it would be a good idea to cite https://arxiv.org/abs/1312.6184, it precedes knowledge distillation and is basically the same thing minus a temperature parameter and a catchy name.\n\nThe paper could do with some further grammar/spell checks.\n\nIt isn't clear to me where the novelty lies in this work. Sequential-FEED appears to be identical to BANs (https://arxiv.org/abs/1805.04770) with an additional non-linear transformation on the network outputs as in https://arxiv.org/abs/1802.04977. Parallel-FEED is just an ensemble of teachers; please correct me if I'm wrong.\n\nThe experimental results aren't convincing. There aren't any fair comparisons. For instance, in table 6 a WRN-28-10(sFEED) after 5 whole training iterations is compared to a WRN-28-1(BAN) after 1. It would be good to run BAN for as many iterations. A comparison to attention transfer (https://arxiv.org/abs/1612.03928) would be ideal for the ImageNet experiments. Furthermore, if one isn't interested in compression, then Table 4 indicates that an ensemble is largely preferable.\n\nThis work would benefit from a CIFAR-10 experiment as it's so widely used (interestingly, BANs perform poorly on CIFAR-10), also a task that isn't image classification would be helpful to get a feel of how the method generalizes.\n\nIn summary I believe this paper should be rejected, as the method isn't very novel, and the experimental merits are unclear.\n\nPros:\n- Simple method\n- Largely written with clarity\n\nCons:\n- Method is not very novel\n- No compared thoroughly enough to other work", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}