{"title": "Regularize interpolation or regularize manifold?", "review": "Main idea:\nThis paper investigates the desiderata for a successful interpolation:\n1) Interpolation looks realistic;\n2) The interpolation path is semantically smooth. \nAn adversarial regularizer is proposed to achieve 1), and in practice 2) may also satisfied.  \nTo evaluate the method, they introduce a synthetic dataset with line images and compare with different autoencoder methods without the interpolation regularization.\nFor real data, they show that the interpolation regularized autoencoder (i.e. ACAI) leads to a better unsupervised representation.\n\nQuestions:\n1. Do we really need every interpolated point to be realistic (i.e. similar to a data point in the train-set)? I believe that there exists an interpolation between two totally different objects can never be observed.  \n2. Do we need interpolation points to form a semantically smooth morphing? I guess this is a desired property for continuous generators, but it seems not necessary in general.\n3. The gamma in the 2nd term in (1) is confusing. If gamma = 1, I understand it forces to predict alpha = 0 since x is real. But if gamma < 1, the average in data space may be very blurry thus not realistic at all. How does gamma affect the optimization?\n4. ACAI looks very similar to LSGAN: by giving \"0\" label to real data and \"alpha\" label to fake data; in LSGAN, alpha = 1.\nHave you tested a LSGAN like regularizer? \n5. The baselines are not representative: since ACAI introduces an adversarial regularizer, you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder. \n\nAfter rebuttal:\nSee the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}