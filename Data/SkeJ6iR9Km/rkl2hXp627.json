{"title": "Potentially Interesting", "review": "in this work the authors propose to replace Gaussian distribution over latent variables in standard variational autoencoders (VAEs) with a sparsity inducing spike-and-slab distribution. While taking the slab to be Gaussian, the authors approximate the spike with a scaled sigmoid function, which is then reparameterized through a uniform random variable. The authors derive an extension of the VAE lower bound to accommodate KL penalty terms associated with spikes. The variational lower bound (VLB) is optimized stochastically using SGD (with KL-divergence computed in closed form). Results on benchmarks show that as compared to standard VAE, the proposed method achieves better VLB for higher number of latent dimensions. Classification results on latent embeddings show that the proposed method achieves stable classification accuracy with increasing number of latent dimensions. Lastly the authors visualize sampled data to hint that different latent dimensions may encode interpretable properties of input data.\n\nOriginality and significance: In my opinion, the approach taken in this work does not constitute a major methodological advancement; the VLB authors derive is a relatively straight-forward extension of VAE's lower bound. \n\nPros:\nThe paper is well-written and easy to follow. \nThe idea of having a sparse prior in latent space is indeed relevant, \nThe approximation and reparameterization of the spike variable is however functionally appealing. \nPotentially useful for semi-supervised learning or conditional generative modeling.\n\nConcerns:\nThe authors show various empirical results to highlight the performance of their approach, but I am still not sure where it is best to use sparse embeddings that are induced by the proposed approach vs. those of standard VAE (or other of its sparse variants e.g., rectified Gaussian priors by Tim Salimans).  For instance in all experiments VAE seems to be competitive or better for low-dimensional latent space, so one may ask, why is it necessary to go to a higher number of latent variables? In a VAE setup, one can simply tune the number of latent dimensions through cross-validation, as one would probably need to do to tune the prior sparsity parameter in the proposed method. \n\nI am also wondering if the disparity between VAE and proposed method w.r.t. classification performance for increasing number of latent dimensions vanishes as more labeled data is used for training? Fig. 11 in appendix seems to indicate that. \n\nLastly I am not sure how we can expect to always converge to interpretable encodings since there is nothing explicit in the objective function to encourage interpretable solutions. Perhaps samples such as those shown in the paper can also be generated by modulating VAE embeddings?\n\nMaybe the proposed approach offers potential for tasks such as semi-supervised learning or conditional generative modeling, but the current set of empirical results does not allow one to draw any conclusions there. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}