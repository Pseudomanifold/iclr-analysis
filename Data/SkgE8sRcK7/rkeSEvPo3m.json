{"title": "Interesting but not good enough for now", "review": "This paper proposes a combination of Evolutionary methods and variational representation learning to improve the sample efficiency of RL methods.\nThey train a VAE on environment frames, as well as an action-conditioned Dynamics model to predict the next frames, and these form the representations fed into a policy network which is trained through ES.\n\nOverall, I find the problem setting interesting, and they try to tackle Atari games instead of simpler domains.\nThe use of CMA-ES instead of NES is a good improvement, and the way they motivate using VAE representations to obtain manageable representation sizes is well put forward.\n[Edit: as mentioned by the other reviewers, this extension isn't as novel, given Ha et al's work, hence that reduces my confidence about accepting this paper further...]\n\nHowever, this paper suffers from several issues in its current state:\n1.\tIts presentation is overly detailed about known literature. Section 2 goes in low-level details which are not necessary. It covers ES methods even though a citation to Salimans et al. 2017 would have been sufficient. Section 2.1 is a really complete coverage of CMA-ES, which should really just be a citation of the actual paper again or should be in the Appendix, this doesn\u2019t warrant 1.5 pages of the main text.\n2.\tThe actual model presentation is too succinct and split into Section 2.2 and Section 3 (network architectures and parameters). It is never clear how many parameters are optimized by CMA-ES (I counted ~8200 parameters if the MLP of size 256 x 32 x n_a is used). Algorithm 3 however was extremely clear and helpful to fully understand the method.\n3.\tThere is no clear evaluation of the performance of the VAE representations and of the RNN dynamics model. Did they actually learn to represent anything at all? Figure 4 is not sufficient in providing evidence supporting this. \u2028Compare this to Higgins et al. 2017, which used VAEs which represented enough information to perform at the same performance as non-variational representations.\n4.\tThis feeds into the biggest issue with the current results:\u2028The proposed method works rather badly, obtaining worst performance than ES on 35 out of 51 games (68%). On most of these games, the proposed method does not seem to be able to get off the ground at all. \u2028Why is that the case? Obviously if the VAE+RNN do not represent the games well enough, the performance will be bad. Did the policy learning with CMA-ES converge well? (seeing learning curves might help)\u2028The fact that no gradients are passed back from the Policy to the VAE/RNN clearly emphasises that issue (The policy only affect the data on which the representations are periodically retrained on).\n\nIn conclusion, even though I feel this paper tries to tackle an interesting problem, the results are not sufficient to support them as of now.\n\nTypos:\n-\t\u201cDonates\u201d instead of \u201cdenotes\u201d in a few places.\n\nReferences:\n-\tHiggins et al., 2017: https://arxiv.org/abs/1707.08475 \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}