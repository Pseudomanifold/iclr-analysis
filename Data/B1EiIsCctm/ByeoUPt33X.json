{"title": "Interesting investigation of discrete latent variable in VAE objective, but limited discussion.", "review": "This paper describes an optimal transport approach for learning autoencoders with discrete and continuous latent variables.\nWasserstein autoencoder (WAE) is developed for better use of the discrete latent variable.\nExperiments using MNIST dataset show how it works.\n\nStrong point of the paper: \nThe empirical investigation to the reason why discrete latent variable is discarded in the VAE objective is interesting.\n\nWeak points:\n* The evaluation is limited to only MNIST datasets.\n* What encourages the use of latent variable is unclear (see below).\n\nThe reviewer has several questions and comments.\n* The choice of \\lambda may depend on the dimensionality of x. I am curious how this affects when applied to other datasets.\n* WAE is different from VAE in three ways: \n(1) l2 reconstruction error written as \\|x - y\\|_2^2, \n(2) MMD with IMQ kernels instead of KL divergence between variational distribution q and prior p, and \n(3) latent variable k being marginalized out.\nThe paper should elaborate more on which factor heps the use of discrete latent variable.\n* Figure 5 (c) is hard to read. Different marker shape (rather than its size) should improve the clarity to distinguish the samples from P and Q.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}