{"title": "A-GEM is a a clear improvement over the previous approach (GEM)", "review": "The paper is well-written, with the main points supported by experiments.  The modifications to GEM are a clear computational improvement.\n\nOne complaint: the \"A\" in A-GEM could stand for \"averaging\" (over all task losses) or \"approximating\" (the loss gradient with a sample).  Both ideas are good.  However, the paper does not address the question: how well does GEM do when it uses a stochastic approximation to each task loss?  (Note I'm not talking about S-GEM, which randomly samples a task constraint; rather, approximate each task's constraint by sampling that task's examples).\n\nAnother complaint: reported experimental results lack any associated idea of uncertainty, confidence interval, empirical variation, etc.  Therefore it is unclear whether observed differences are meaningful.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}