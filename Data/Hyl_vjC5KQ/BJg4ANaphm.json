{"title": "Good idea, exposition can be much improved", "review": "The paper considers the problem of hierarchical reinforcement learning, and proposes a criterion that aims to maximize the mutual information between options and state-action pairs.\n\nThe idea of having options partition the state-action space is appealing, because this allows options visit the same states, so long as they act differently, which is natural. The authors show empirically that the learned options do indeed decompose the state-action space, but not the state space.\n\nThere is a lot in the paper already, but the exposition could be much improved. Many of the design choices appear very ad hoc, and some are outright confusing. Some detailed comments:\n\n* I got really confused in Section 3 re: advantage-weighted importance sampling. Why do this? If the option policies are trying to optimize reward, won\u2019t they become optimal eventually (or so we usually hope in RL)? This section seems to assume that the advantage function is somehow given. It also doesn\u2019t look like this gets used in the actual algorithm, and in fact on page 5 it is stated that \u201cwe decided to use the on-policy buffer in our implementation\u201d. Then why introduce the off-policy bit at all, and list it as a contribution?\n* Please motivate the choices. The paper mentions that one of its contributions are options with deterministic policies. This isn\u2019t a contribution unless it addresses some problem that stochastic policies fail at. For example, DPG allows one to address continuous control problems.\nSame with using information maximization. The paper literally states that \u201can interpretable representation can be learned by maximizing mutual information\u201d. Representation of what? MI between what?\n* Although the qualitative results are nice (separation of the state-action space), empirical results are modest at best. This may be ok, because based on the partition of the state-action space it seems that the option policies learn diverse behaviors in the same states. Maybe videos visualizing different options from the same states would be informative.\n* Please add more discussion on why the options are switched at every step", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}