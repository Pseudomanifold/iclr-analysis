{"title": "Theoretical and algorithmic novelty is not enough. Exhaustive domain/empirical evidence is expected. Needs work.", "review": "Following the summary comment, the main issue is -- the idea makes sense in terms of using fair representation learning for confounding variable analysis. However, several aspects needs to be improved. \n\n1) Evidence for linguistic feature predicting age seems not enough i.e., the error reported in page 8 is approx. 40% of the overall range of the age in the two datasets considered; Hence the confounder SNR might not be strong to show significant changes/patterns in disentanglement scores (of Hardt et al) -- which is why all the deltas in Tables 1, 2 are very close. \n\n2) There is an implicit influence of the good-ness of input/linguistic features? How to account for that in the performance summaries? Also, any specific reason for choosing 2 and 5 groups/bins for age? \n\n3) The models in Table 2 should 'not' have significant differences in the ideal case -- is this correct? i.e., the disentanglement scores are doing reasonable job for all settings? Also, the authors suggest the improvements are mostly significant -- this should be more precise (back to exhaustive evaluations thing). \n\n4) The scale of disentanglement is difficult to interpret. Shouldn't the score be normalized by the group size to fix the range across all comparisons? \n\n5) What is the basis for presenting the four models? And it seems the simplest model is working best because, for the size of the dataset (small, unlike standard CV data), the non-simple models are over-parameterized and more non-linear (for the lack of a better phrase) to train?  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}