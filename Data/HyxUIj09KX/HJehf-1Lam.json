{"title": "Interesting take on neural networks from a measure-theoretic viewpoint, however not easy to follow for a non-expert", "review": "The paper provides a new framework \"S-System\" as a generalization of hierarchal models including neural networks. The paper shows an alternative way to derive the activation functions commonly used in practice in a principled way. It further shows that the landscape of the optimization problem of neural networks has nice properties in the setting where the number of input/hidden units tending to infinity and the neurons satisfy certain diversity conditions.\n\nOverall, the paper presents super interesting ideas that can potentially lead to a deeper understanding of the fundamentals of deep learning. However, for a general reader it is a hard-to-follow paper. Without a full understanding of the various domains this paper presents ideas from, it is hard to verify and fully understand the claims. I believe the paper would be better appreciated by an audience of a mathematical journal. As an alternative, I would encourage the readers to split the paper and possibly simplify the content by using a running example (more concrete than the one of MLP used) to explain the implications as well as assumptions.\n\nA clearer, more accessible presentation is necessary so that a non-expert can understand the paper's results. Thus, I vote to reject. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}