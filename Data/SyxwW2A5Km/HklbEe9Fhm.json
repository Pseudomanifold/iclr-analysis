{"title": "Official review", "review": "Summary:\nThe authors apply the self-attention mechanism, a.k.a. transformer, to improve the representations of multi-field categorical features in recommendation systems. Unlike the previous approaches in which multi-field features are simply concatenated, the proposed method more actively combines those features improving the final performance.\n\nStrengths:\n+ It is reasonable to apply the permutation-invariant self-attention mechanism to the multi-field features as orders of the fields should not matter.\n+ The method achieves the state-of-the-art performance on two datasets.\n\nWeaknesses:\n- The paper lacks the technical novelty as it does not propose any novel technique. Rather, it simply applies an existing technique to a new type of dataset.\n- More extensive analyses on the learned representation would improve the paper.\n- As the authors argue, the method can be used upon other existing state-of-the-art networks. Showing the improvement on other methods would improve the paper. Currently, the authors only present improvement on a simple MLP.\n\nQuestions:\nTo apply the self-attention, the embeddings of the field features should be projected in the same space. I wonder if this physically makes sense. I wonder how they are embedded in the features and relate to each other. I would suggest to include some analysis on the features while putting some rows of Table 3 to the appendix since many of these rows are not directly related to the method itself.\n\nOverall, I like the idea of the paper. However, the paper lacks the technical novelty and presents only limited experiments and analysis. I would suggest the authors include more analyses on the learned representations.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}