{"title": "Useful idea, though the contribution is a bit marginal", "review": "This paper considers how to augment training data by applying class-preserving transformations to selected datapoints.\nIt proposes improving random datapoint selection by selection policies based on two metrics: the training loss \nassociate with each datapoint (\"Loss\"), and the influence score (from Koh and Liang that approximates leave-one-one test loss). The authors consider two policies based on these metrics: apply transformations to training points in decreasing \norder of their score, or to training points sampled with probability proportional to score. They also consider two \nrefinements: downweighting observations that are selected for transformation, and updating scores everytime \ntransformations associated with an observation are added. \n\nThe problem the authors tackle is important and their approach is natural and promising. On the downside, the theoretical \ncontribution is moderate, and the empirical studies quite limited. \n\nThe stated goals of the paper are quite modest: \"In this work, we demonstrate that it is possible to significantly reduce the \nnumber of data points included in data augmentation while realizing the same accuracy and invariance benefits of \naugmenting the entire dataset\". It is not too surprising that carefully choosing observations according suitable policies \nis an improvement over random subsampling, especially, when the test data has been \"poisoned\" to highlight this effect. \nThe authors have demonstrated that two intuitive policies do indeed work, have quantified this on 3 datasets. \n\nHowever they do not address the important question of whether doing so can improve training time/efficiency. In other words, the authors have not attempted to investigate the computational cost of trying to assign importance scores to each observation. Thus this paper does not really demonstrate the overall usefulness of the proposed methodology.\n\nThe experimental setup is also limited to (I think) favor the proposed methodology. Features are precomputed on images using a CNN, and the different methods are compared on a logistic regression layer acting on the frozen features. The existence of such a pretrained model is necessary for the proposed methods, otherwise one cannot assign selection scores to different datapoints. However, this is not needed for random selection, where the transformed inputs can directly be input to the system. A not unreasonable baseline would be to train the entire CNN with the augmented 5%,10%, 25% datasets, rather than just the last layer. Of course this now involves training the entire CNN on the augmented dataset, rather than just the last layer, but how relevant is the two stage training approach that the authors propose?\n\nIn short, while I think the proposed methodology is promising, the authors missed a chance to include a more thorough analysis of the trade-offs of their method.\n\nI also think the paper makes only a minimal effort to understand the policies, the experiments could have helped shed some more light on this.\n\nMinor point:\nThe definition of \"influence\" is terse e.g. I do not see the definition of H anywhere (the Hessian of the empirical loss)", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}