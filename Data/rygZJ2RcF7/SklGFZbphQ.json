{"title": "border-line paper", "review": "The paper demonstrates that we can harness (semantically meaningful) features learned by a pre-trained autoencoder AE to define a determinisc transformation (e.g. math operations on latent space) to transform one distribution A into another distribution B.\nThe original AE was pre-trained on a larger distribution that includes both A and B.\n\nA key contribution of this paper is the interesting demonstration that this method (called Neuron Editing) allows us to perform a transformation T that transforms  pre-treatment observations into post-treatment observations, which is useful in the medical or biological setting.\n\n+ Novelty\nNeuron editing is essentially a common technique of performing arithmetics in the latent space e.g. King - Man + Woman = Queen (in NLP) or Man wearing sunglass - Man + Woman = Woman wearing sunglasses (e.g. in image domain e.g. in Alec Radford et al. 2015).\nTherefore, the novelty is limited.\n\n+ Significance\nThe main contribution of this paper is the empirical demonstration that such transformation T is better defined, rather than learned directly from data (e.g. via GANs).\n\nI should note that I'm not too familiar with the biology datasets in Sec. 3.2 and Sec 3.3 in order to fully appreciate the practical impact of Neuron Editing.\n\n+ Clarity\n\nI think some key reasons behind why Neuron Editing works could be more clearly presented.\nThat is, the key here is we use pre-trained AEs to perform a pre-defined transformation.\nI think the key might not be whether we use GANs or not, it is how we use them.\nI guess if we use ALI (i.e. training a GAN concurrently with an AE) to perform Neuron Editing, the result should work as well.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}