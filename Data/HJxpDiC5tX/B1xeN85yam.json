{"title": "nice data collection but no technical contribution", "review": "The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos.\n\nThe review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task.\n\nThe collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.\n\nThe numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings.\n\nThe result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq.\n\nIt is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data.\n\nHere are some minor details:\n\np.6.\n\nnote that there must be a blank between the 'e' characters to avoid collapsing ...\n--> this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order.\n\nTo explain why modeling characters with CTC is problematic, ...\n--> this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}