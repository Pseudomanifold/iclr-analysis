{"title": "Engineering Marvel", "review": "The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR.\n\nThe authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions:\n\n- the authors argue for \"phonemes and ctc\", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary.\n- why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own \"homonym\" argument applies, too, and \"mop\" (or \"mom\") and \"pop\" should be mapped to the same \"viseme\" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier.\n- how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system?\n- LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference?\n- is the data going to be available?\n- what is a \"production-level speech decoder\"? how come your model \"is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques\" if Google does essentially the same (\"in production\")?\n- in Section 1, you say that \"by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts\". in Section 5, you demonstrate the \"generalization power of our V2P approach\"and find that it \"is able to generalize well\" - please clarify\n- \"speech impaired patients\" often have non-canonical articulation, the proposed system may not work well for them\n- it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. \n- finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}