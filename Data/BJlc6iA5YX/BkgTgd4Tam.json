{"title": "Needs further clarifications", "review": "I have to emphasize first that this is not my area of expertise so I am going to review it as an outsider. \n\nThe authors argue that the checkerboard phenomenon can be exploited to make neural networks robust against adversarial attacks. They propose to enhance the checkerboard pattern by first adding a layer, called Artificial Checkerboard Enhancer (ACE), and then evading the attacks by zero-padding the image. The authors\u2019 argument is that enhancing the checkerboard phenomenon will make attacks more targeted towards certain pixels, which can be evaded by shifting the image. \n\nOverall, I think the paper is difficult to read and is not suitable for publication. In terms of clarity, the authors do not use precise terminology that would allow the reader to reproduce their work. They allude to vague statements. For example, they introduce two KEY terminologies that are repeatedly used throughout the paper but are not properly defined (see for instance the \u201cdefinition\u201d of \u201cGradient Overlap\u201d in Appendix C). \n\nIn addition, in terms of the experiements, it certainly does not help to say that they were \u201creproduced by [the authors themselves]\u201d. What does this mean?\n \nIn terms of originality, I agree with the first reviewer that the defense strategy seems to be easily breakable. The authors propose that they enhance the checkerboard phenomenon so that adversarial attacks become easier to implement by targeting individual pixels (the pixels in the checkerboard artifacts). Then, they pad the image with zero pixels to shift it to the right. I don\u2019t understand how shifting the pixels would make it harder to attack (especially when the adversary knows the system). \n\nIt would be really appreciated if the authors elaborate on the following points to help me understand their contribution: \n- The entire discussion about ACE in Section 4.1 is ad-hoc and not well-motivated. Why would ACE enhance the checkerboard patterm? Can you please explain why it works? This is not mentioned anywhere in the paper. The experiment in Section 4.2 helps a bit but it does not answer this question. \n\n- What wouldn't an adversary remove the padded pixels before generating the attack? In defense strategies, it is often assumed that the adversary knows the system. Can you please explain why that is not possible in this setting? \n\n- \n\nIn Figure 4, the axes are \\bar i and \\bar j in the main body, but they are x and y in the figure. Please use the same notation. ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}