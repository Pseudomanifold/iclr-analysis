{"title": "Review", "review": "This paper proposes an alternative training paradigm for DNIs: instead of training an auxiliary module to approximate the gradient provided by the following modules of the original model, they train it to approximate directly the final output of the original model. Although the approach does not seem to improve significantly, if at all, over Sobolev training of these modules (denoted 'critic' in the paper), this method seems simpler and offer side benefits which seem to be the main contribution of this paper. \nIndeed Table 4 and 5 show for example how they can, after training the full model, extract a submodel requiring significantly less computation and parameters with little loss in the performance. Alternatively, they also propose a method to do progressive inference for similar reasons. \nThe ensembling result is interesting, but the figure 4 is not necessarily clear on the significance of this result, especially since the standard deviation is not shown in this figure.\nThe idea proposed in this paper is interesting, however, the experiments are restricted on relatively small and simple architectures and limited on two very similar datasets (CIFAR-10 and CIFAR-100), making the argument less compelling. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}