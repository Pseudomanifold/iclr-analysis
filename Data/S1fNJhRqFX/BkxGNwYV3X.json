{"title": "The reviewer does not understand why equation 2 solves the exploration-exploitation tradeoff.", "review": "This paper proposes new algorithms (QUCB and QUCB+) to handle the exploration exploitation tradeoff in Multi-Armed Bandits and more generally in Reinforcement Learning. The first idea of the paper is to use an Upper Confidence Bound based on the variance estimation rather than an exploration factor based on a concentration inequality such as Hoeffding inequality. The variance estimation is done using Quantile Regression. The second idea of the paper is to adapt the proposed algorithm, QUCB, to the case of asymmetrical distributions. \\sigma+ is defined as the standard deviation of the quantiles higher than the median,. \\sigma+ isused in place of \\sigma in QUCB. Synthetic experiments are done for MAB, and Atari game are used to test DQN-UCB+ for RL.\n\nMajor concern:\n\nThe authors claim that the main advantage of QUCB is that no counter is needed to evaluate the Upper Confidence Bound, and therefore that QUCB can be used in RL where the number of state-action pairs is too high to be stored. Indeed, QUCB does not use counts on the number of times each action has been selected. However, the upper confidence bound that is used in QUCB (see eq 2, line 5 of algorithm 2) is not optimistic in face of uncertainty but reckless: the arm, that is selected, is the arm with the highest mean plus standard deviation.  The reviewer understands that to obtain an accurate estimate of means, the arms with high variance need to be sampled more than the arms with low variance, as in UCB-V.  The reviewer does not understand why equation 2 solves the exploration-exploitation tradeoff. No theoretical analysis of the algorithm is given. No convincing arguments are provided. At a minimum, \n1/ the authors have to explain why it could work. May be based on Chebyshev's inequality or Follow the Perturbed Leader ?\n2/ QUCB has to be compared with UCB (Auer et al 2002) in the experiments.\n3/ The variance and/or the distribution of arms have to be different in the experiment. Notably it could be interesting to launch an experiment where the best arm has a low variance while the worst arm has an high variance.\n\n\nMinor concerns:\n\nTheorem 1 is not used in QUCB, so I suggest removing it.\nq_j is not defined and not initialized in algorithm 1 and 3. \nc_t is not defined in algorithm 3. \nThe right reference for UCB is Finite Time Analysis of the Multi-Armed Bandit Problem, P. Auer, N. Cesa-Bianchi, P. Fischer, 2002.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}