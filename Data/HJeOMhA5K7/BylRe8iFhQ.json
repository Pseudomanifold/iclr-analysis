{"title": "Interesting problem, not well executed idea", "review": "The  paper  introduces  a  method  to  incorporate  human  advises  to  deep  learning  by  extending  Column  Network  (CLN)  -  a  powerful  graph  neural  network  for  collective  classification. \n\nThe  problem  is  quite  interesting  and  is  practical  in  real-world. However, I have some concerns:\n\nCorrectness\n==========\nIn the main modification to the CLN in Eq (3), the rule-based gates are introduced to every hidden layer. However, the functional gradient with respect to the \"advise gradient\" is only computed for the last layer (at the end of Section 3). The exponential gates may cause some instability issue due to its unboundedness. \n\nEvaluation\n=========\nThe  questions  in  experiment  (Can  K-CLNs  learn  efficiently/effectively  with  noisy  sparse  samples?)  do  not  support  the  problem  statement  about  human  advice  incorporation.  Thus,  all  they  did  in  the  experiment  is  trying  to  compete  against  CLN.\n\nI would believe that the improvement (which I trust is real) depends critically on the quality and quantity of the human-crafted rules, much in the same way that feature engineering plays the major roles in the classical structured output prediction. Hence more details about the rules set used in experiments should be given.\n\nPresentation\n===========\nIn  the  experiment  part,  the  authors  need  to  describe  their  model  configuration.  The  presentation  of  datasets  consumes  a  lot  of  space  and  can  be  reduced (e.g., using a table).  This  paper  displays  many  unnecessary  figures  that  consumes  a  lot  of  space.  The  paper  provides  some  unnecessary  text  highlights  in  bold.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}