{"title": "Method needs some clarification. ", "review": "Summary: This paper presents a way to combine existing factorized second order representations with a codebook style hard assignment. The number of parameters required to produce this encoded representation is shown to be very low. Like other factorized representations, the number of computations as well as the size of any intermediate representations is low. The overall embedding is trained for retrieval using a triplet loss. Results are shown on Stanford online, CUB and Cars-196 datasets.\n\nComments:\n\nReview of relevant works seems adequate. The results seem reproducible. \n\nThe only contribution of this paper is combining the factorized second order representations  of (Kim et. al. 2017) with a codebook style assignment (sec. 3.2). Seems marginal.\n\nThe scheme described in Sec. 3.2 needs clarification. The assignment is applied to x as h(x) \\kron x in (7). Then the entire N^2 D^2 dimensional second order descriptor h(x) \\kron x \\kron h(x) \\kron x is projected on a N^2 D^2 dim w_i. The latter is factorized into p_i, q_i \\in \\mathbb{R}^{Nd}, which are further factorized into codebook specific projections u_{i,j}, v_{i,j} \\in \\mathbb{R}^{d}. Is this different from classical assignment, where x is hard assigned to one of the N codewords as h(x), then projected using \\mathbb{R}^d dimensional p_i, q_i specific to that codeword ?\n\nIn section 4.1 and Table 2, is the HPBP with codebook the same as the proposed CHPBP ? The wording in \"Then we re-implement ... naively to a codebook strategy\"  seems confusing.\n\nThe method denoted \"Margin\" in Table 4 seems to be better than the proposed approach on CUB. How does it compare in terms of efficiency, memory/computation ?\n\nIs it possible to see any classification results? Most of the relevant second order embeddings have been evaluated in that setting.\n\n\n===============After rebuttal ===============================\n\nAfter reading all reviews, considering author rebuttal and AC inputs, I believe my initial rating is a bit generous. I would like to downgrade it to 4. It has been pointed out that many recent works that are of a similar flavor, published in CVPR 2018 and ECCV 2018, have slightly better results on the same dataset. Further, the only novelty of this work is the proposed factorization and not the encoding scheme. This alone is not sufficient to merit acceptance. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}