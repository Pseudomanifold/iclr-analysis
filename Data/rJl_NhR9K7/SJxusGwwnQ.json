{"title": "Overall method is not presented", "review": "This paper presents a methodology to bring together independent subspace analysis and variational auto-encoders. Naturally, in order to do that, the authors propose a specific family of prior distributions that lead to subspace independence the Lp-nested distribution family. This prior distribution is then used to learn disentangled and interpretable representations. The mutual information gap is taken as the measure of disentanglement, while the reconstruction loss measures the quality of the representation. Experiments on the sPrites dataset are reported, and comparison with the state of the art shows some interesting results.\n\nI understand the limitations of current approaches for learning disentangled representations, and therefore agree with the motivation of the manuscript, and in particular the choice of the prior distribution. However, I did not find the answer to some important questions, and generally speaking I believe that the contribution is not completely and clearly described.\nP1) What is the shape of the posterior distribution?\nP2) How does the reparametrization trick work in your case?\nP3) How can one choose the layout of the subspaces, or this is also learned?\n\nMoreover, and this is crucial, the proposed method is not clearly explained. Different concepts are discussed, but there is no summary and discussion of the proposed method as a whole. The reader must infer how the method works from the different pieces. \n\nWhen discussing the performance of different methods, and even if in the text the four different alternatives are clearly explained, in figure captions and legens the terminology changes (ISA-VAE, ISA-beta-VAE, beta-VAE, beta-ISA-VAE, etc). This makes the discussion very difficult to follow, as we do not understand which figures are comparable to which, and in which respect.\n\nIn addition, there are other (secondary) questions that require an answer.\nS1) After (10) you mention the subspaces v_1,...v_l_o. What is the formal definition of these subspaces?\nS2) The definition of the distribution associated to ISA also implies that n_i,k = 1 for all i and k, right?\nS3) Could you please formally write the family of distributions, since applying this to a VAE is the main contribution of your manuscript?\nS4) Which parameters of this family are learned, and which of them are set in advance?\nS5) From Figure 4 and 5, I understand that the distributions used are of the type in (7) and not (10). Can you comment on this?\nS6) How is the Lp layout chosen?\nS7) Why the Lp layout for ISA-beta-VAE in Figure 5 is not the same as in Figure 4 for ISA-VAE?\nS8) What are the plots in Figure 4? They are difficult to interpret and not very well discussed.\n\nFinally, there are a number of minor corrections to be made.\nAbstract: latenT\nEquation (3) missig a sum over j\nFigure 1 has no caption\nIn (8), should be f(z) and not x.\nBefore (10), I understand you mean Lp-nested\nI did not find any reference to Figure 3\nIn 4.1, the standard prior and the proposed prior should be referred to with different notations.\n\nFor all these reasons I recommend to reject the paper, since in my opinion it is not mature enough for publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}