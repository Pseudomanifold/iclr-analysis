{"title": "Review of \"Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning\"", "review": "This paper introduces a new method for visualizing states which are likely to lead to various outcomes (e.g., high/low reward or a particular agent action). This method is based on a VAE modified to produce samples which not only allow reconstruction of the input, but also contain salient input features (as measured by guided backpropagation) and which will result in the same agent output as the original input. The paper then goes on to use this method to generate inputs for Atari games which result in particular rewards (high/low), actions (e.g., left), or uncertainty (states in which different actions can either lead to very good or bad states). \n\nThe paper is clear and well-written, and a large number of generated samples are provided (especially in the Appendix, which contains examples for all games in the ALE for a variety of outcomes). The method for regularizing the samples is novel to my knowledge. However, while the method for generating samples is interesting, this paper contains several critical issues (detailed below). I therefore think that this paper needs more work and is not yet ready for publication in a venue such as ICLR. \n\nMajor comments: \n\n1) While the visualizations are indeed interesting, it\u2019s unclear whether these visualizations provide information about particular agents or about the task itself. This is a critical distinction, especially in light of recent results (cited by the authors) which suggest that many current visualization methods produce the same outputs regardless of the model studied [1, 2, 3]. Moreover, this issue is especially important given the use of guided backpropagation in the sample generation method itself.\n\n2) Assuming the visualizations are model-specific, it\u2019s unclear what (if any) falsifiable statements can be generated from these visualizations. Section 4.3 takes a stab at this, stating that the visualizations revealed that a particular agent didn\u2019t understand the oxygen mechanic in Sequest, which was confirmed by rolling out the agent (though no data is presented along with this anecdote). This paper would be dramatically strengthened by more experiments like this section (with provided data, of course). Without such a link to the agent\u2019s ultimate behavior (and by extension, ground truth), it\u2019s unclear whether these visualizations actually lead to any understanding at all.\n\n3) Further experiments are needed to demonstrate the benefit of the proposed method over simply analyzing a replay buffer. The experiment included in the manuscript based on pixel differences is flawed as pixel differences are a poor measure of mode collapse since they are sensitive to simple image transformations such as translation. This issue has been investigated in the context of GANs, in which pixel differences were found to work only for centered images such as face datasets [4]. In order to evaluate whether the proposed method is actually advantageous over analyzing the replay buffer, a perceptual metric would need to be used.\n\nMinor comments:\n\n1) The claim in the third paragraph of Section 4.5 that unused conv1 weights lead to more easily \u201cdistracted\u201d models is unsubstantiated.\n\n\n[1] Adebayo J, Gilmer J, Muelly M, Goodfellow I, Hardt M, Kim B. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292. 2018 Oct 8.\n\n[2] Hooker S, Erhan D, Kindermans PJ, Kim B. Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. 2018 Jun 28.\n\n[3] Kindermans PJ, Hooker S, Adebayo J, Alber M, Sch\u00fctt KT, D\u00e4hne S, Erhan D, Kim B. The (Un) reliability of saliency methods. arXiv preprint arXiv:1711.00867. 2017 Nov 2.\n\n[4] Arora S, Zhang Y. Do GANs actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224. 2017 Jun 26.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}