{"title": "Interesting Generalization of G/V/advantage function but some clarifications are needed. ", "review": "In this paper, the authors present how to integrate replay buffer and on-policy trust region policy optimization (TRPO) by generalizing Q/V/advantage function and then empirically show the proposed method outperforms TRPO/DDPG.\n\nThe generalization of advantage function is quite interesting and is well written. One minor issue is that d^{\\pi_n} (s) is confusing since it appears after ds. \n\nThe theory in Section 3.1 makes sense. However, due to the limitation in Theorem 1 that $\\theta$ is the joint parameters, applying Theorem 1 can be difficult. In Eq (25), what is the $\\theta$ here? And what does $\\nabla_\\theta \\pi_n$ mean? Does $\\pi_n$ uses $\\theta$ for computation? One of the problems of using replay buffers in on-policy algorithms is that the stationary distribution of states changes as policy changes, and at least the writing doesn't make it clear on how to solve distribution mismatching issue. Further explanation on Eq (25) might help. If the distributions of states are assumed to match, then the joint distribution of states and actions may mismatch so additional importance sampling might help, as suggested in [1] Eq (3). \n\nAnother problem is on the barrier function. In Eq (26), if we only evaluate $\\rho_b(\\theta)$ (or its gradient w.r.t. $\\theta$) at the point $\\theta_old$, it doesn't differ with or without the barrier function. So in order to show the barrier function helps, we must evaluate $\\rho_b(\\theta)$ (or its gradient) at a point $\\theta \\neq \\theta_old$. As far as I know, the underlying optimizer, K-FAC, just evaluates the objective's (i.e., $\\rho_b$) gradients at $\\theta_old$. Both Conjugate Gradient (CG), which TRPO uses, and K-FAC are trying to solve $F^{-1} g$ where $g$ is the gradient of the objective at the current point. \n\nThe experiments show significant improvement over TRPO/DDPG. However, some experiments are also expected.\n1. How is the proposed algorithm compared to PPO or Trust PCL? \n2. How does the barrier function help? More importantly, what's the comparison of the barrier function to [1] Eq (5)? \n\nThe proposed algorithm seems more like a variant of ACKTR instead of TRPO since line search is missing in the proposed algorithm and the underlying optimizer is K-FAC instead of CG.\n\nRef:\n[1]: Proximal Policy Optimization Algorithms, by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}