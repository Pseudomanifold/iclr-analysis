{"title": "Seems a trivial extension of TRPO", "review": "The paper tries to bring together the replay buffer and on-policy method. However, the reviewer found major flaws in such a method.\n\n- Such replay buffers are used for storing simulations from several policies at the same time, which are then utilised in the method, built upon generalised value and advantage functions, accommodating data from these policies.\n\nIf the experience the policy is learning from is not generated by the same policy, that is off-policy learning. \n\nIn the experiment part, the replay buffer size is often very tiny, e.g., 3 or 5. The reviewer believes there may be something wrong in the experiment setting. Or if the reviewer understood it incorrectly, please clarify the reason behind such a tiny replay buffer.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}