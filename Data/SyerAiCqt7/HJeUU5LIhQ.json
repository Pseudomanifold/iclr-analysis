{"title": "The authors discuss a hierarchical Bayesian framework for clustering sparse sequences. They use data from a restaurant loyalty program to identify users (rows) and weeks of visits (columns) under the assumption that user visits to a restaurant will be sparse across weeks. ", "review": "The paper is very poorly written. It is hard to understand what the real contribution is in this paper. \nThe connection of the model with HMM is not clear. The literature review has to be rewritten.\n\nTo the reader, it sounds that the authors are confused with the fundamentals itself: mixture model, Bayesian models, inference. \n\n> Mixture models can be based on any of the exponential family distributions - Gaussian just happens to be the most commonly used.\n> Again if this is a Bayesian model, why are #clusters not inferred? The authors further mention that in their Pystan implementation K clusters were spun too quick. What was the K used here? Was it set to a very large value or just 3? Did the authors eventually use the truncated infinite mixture model in Pystan?\n> The authors mention their model is conceptually similar to EM but then end up using NUTS. \n> Why is a url given in Section 2.3 instead of being given in the references? \n> Provide a plate model describing Section 3.2.", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}