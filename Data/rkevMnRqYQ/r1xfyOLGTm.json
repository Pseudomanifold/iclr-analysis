{"title": "An interesting idea but less convincing experimental results", "review": "This work proposes a way to infer the implicit information in the initial state using IRL and combine the inferred reward with a specified reward to achieve better performance in a few simulated environments where the specified reward is not sufficient to solve the task. The main novelty of this work is to reformulate the Maximum Causal Entropy IRL objective using just the initial state as the end state of an expert trajectory to infer the underlying preference. Overall the proposed approach is impressive and the intuition behind the paper is novel and easy to understand.\n\nMy main concerns are the following:\n- All the simulated experiments are able to demonstrate the effectiveness of the method, though they seem to be a bit too simplistic, e.g. known dynamics. As mentioned in Section 7, more real-environment experiments would make this method a lot stronger.\n- The way of choosing the distribution s_{-T} seems to require some sort of human preference, e.g. in the apple collection case,  s_{-T} has to be sampled from the distribution where there's no apple in the basket in order to make the algorithm to work. This assumption seems to make the implicit information of the initial state not so *implicit*. Besides, it's unclear how to choose the horizon T. It would be interesting to see how the value of T affects the performance.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}