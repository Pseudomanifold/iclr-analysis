{"title": "Interesting idea: use a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. ", "review": "Often in a deep generative model with multiple latent variables, the structure amongst the latent variables is pre-specified before parameter estimation. This work aims to learn the structure as part of the parameters. To do so, this work represents all possible dependencies amongst the latent random variables via a learned binary adjacency matrix, c, where a 1 denotes each parent child relationship.\n\nEach setting of c defines a latent variable as the root and subsequent parent-child relationships amongst the others. To be able to support (up to N-1) parents, the paper proposes a neural architecture where the sample from each parent is multiplied by the corresponding value of c_ij (0'd out if the edge does not exist in c), concatenated and fed into an MLP that predicts a distribution over the child node. The inference network shares parameters with the generative model (as in Sonderby et. al). Given any setting of c, one can define the variational lower-bound of the data. This work performs parameter estimation by sampling c and then performing gradient ascent on the resulting lower-bound.\n\nThe model is evaluated on MNIST, Omniglot and CIFAR where it is found to outperform a VAE with a single latent variable (with the same number of latent dimensions as the proposed graphVAE), the LadderVAE and the FCVAE (VAE with a fully connected graph). An ablation study is conducted to study the effect of number of nodes and their dimensionality.\n\nOverall, the paper is (a) well written, (b) proposes a new, interesting idea and (c) shows that the choice to parameterize structure via the use of auxillary random variables improves the quality of results on some standard benchmarks.\n\nComments and questions for the authors:\n* Clarity\nIt might be instructive to describe in detail how the inference network is structured for different settings of c (for example, via a scenario with three latent variables) rather than via reference to Sonderby et. al.\n\nWhat prior distribution was used for c?\n\nFor the baseline comparing to the LadderVAE, what dimensionalities were used for the latent variables in the LadderVAE (which has a chain structured dependence amongst its latent variables)? The experimental setup keeps fixed the latent dimensionality to 80 -- the original paper recommends a different dimensionality for each latent variables in the chain [https://arxiv.org/pdf/1602.02282.pdf, Table 2] -- was this tried? Did the ladderVAE do better if each latent variable in the chain was allowed to have a dimensionality?\n\n* Related work\nThere is related work which leverages Bayesian non-parametric models to learn hierarchical priors for deep generative models. It is worth discussing for putting this line of work into context. For example:\nhttp://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_Nonparametric_Variational_Auto-Encoders_ICCV_2017_paper.pdf\nand more recently: https://arxiv.org/pdf/1810.06891.pdf\n\nIn the context of defining inference networks for generative models where the latent variables have structure, Webb et. al [https://arxiv.org/abs/1712.00287] describe how inference networks should be setup in order to invert the generative process.\n\n* Qualitative study\nNotable in its absence is a qualitative analysis of what happens to the data sampled from the model when the various nodes in the learned hierarchy are perturbed holding fixed their parents. Have you attempted this experiment? Are the edge relationships sensible or interesting?\n\nIs there a relationship between the complexity of each conditional distribution in the generative model and the learned latent structure? Specifically, have you experimented to see what happens to the learned structure amongst the latent variables if each conditional density is a linear function of its parents?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}