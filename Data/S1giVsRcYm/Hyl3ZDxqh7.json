{"title": "An interesting work but with some flaws ", "review": "This paper proposed a new exploration strategy, based on the successor representation (SR), which can be used as a pseudo bonus in reinforcement learning. The authors also showed the connection between the state visit count and the SR, in the tabular case. Finally, the proposed algorithm had been tested on simulated examples, and several hard exploration Atari domains.\n\nIn general, there are some interesting ideas in this paper, while the empirical justification may not be strong enough. My pros and cons are summarized as follows. \nPros:\n- The idea of using SR for pseudo count in deep RL is novel.\n- Theorem 1 shows the interesting connection between state visit count and the proposed SR.\n- The experiments on Atari games show some promise for using SR (but not that much).\nCons:\n- There are a few inconsistencies regarding the use of SR. For example, the tabular case used the minus l1 norm as the reward bonus; however, the Atari case instead set the bonus to be the reciprocal of the l2 norm. \n- Other than the Montezuma's Revenge, it's difficult to draw the conclusion that using SR can generally lead to better exploration performance, based on the last two columns of Table 2.\n- The definition of loss L_{SR} is a bit unclear: Is there something similar to the Bellman equation you can say about SR? I also don't quite understand the motivation for the architecture between \\phi and \\psi in Figure 1.\n- A few small comments/questions are listed as follows.\n  1. When discussing the impact of the introduced auxiliary task, it would be more convincing to show the performance of games other than Montezuma's Revenge.\n  2. Why is it true that \"... because a reward of 1 is observed...\", in the second paragraph of Section 4?\n  3. What is the value of \\tau in the loss L_{TD} on Atari domains?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}