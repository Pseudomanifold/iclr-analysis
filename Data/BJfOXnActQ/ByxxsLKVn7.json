{"title": "Good paper", "review": "TL;DR. Significant contribution to meta-learning by incorporating latent metrics on labels.\n\n* Summary\n\nThe manuscript builds on the observation that using structured information from the labels space improves learning accuracy. The proposed method --CAML-- is an instance of MAML (Finn et al., 2017), where an additional embedding is used to characterize the dissimilarity among labels.\n\nWhile quite natural, the proposed method is supported by a clever metric learning step. The classes are first represented by centroids and an optimal mapping $\\phi$ is then learnt by maximizing a clustering entropy (similarly to what is performed in a K-means-flavored algorithm, though this connection is not made in the manuscript). A conditional batch normalization (Dumoulin et al., 2017) is then used to model how closeness (in the embedding space $f_\\phi$) among labels is taken into account at the meta-learning level.\n\nExisting literature is well acknowledged and I find the numerical experiments to be convincing. In my opinion, a clear accept.\n\n* Minor issues\n\n- I would suggest adding a footnote explaining why Table 1 reports confidence intervals and not just standard deviations. How are constructed those intervals?\n- Section 3.2 bears ambiguity as the manuscript reads \"We first define centroids [...]\" depending on $f_\\phi$ which is then defined as the argument of the minim of the entropy term. What appears as a circular definition is merely the effect of loose writing yet I am afraid it would confuse readers. I would suggest to rewrite this part, maybe using a pseudo-code to better make the point that $f_\\phi$ is learnt.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}