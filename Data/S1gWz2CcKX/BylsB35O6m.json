{"title": "Interesting ideas and results but lacking clarity and focus. ", "review": "This paper proposes a multi agent life simulators as an environment for RL. The environment is procedurally generated, with possibly many different game dynamics including foraging, and combat. They train deep RL agents in this environment and show various emergent behaviors such as exploration, and niche development. Additionally, they propose a tournament competition scheme to evaluate different populations of agents against each other.\n\nThis paper has a number of interesting findings, but overall lacks polish and coherence. The writing is verbose and informal in many places. There are many details not included -- for example specifics on combat targeting, how RL agents are trained, and information how to parse figures (what do colors mean?).\n\nPro\n+ Interesting idea, and demonstration of a system. From the intro, I believe an environment such as this is will be fruitful to study.\n+ Results seem preliminary but are interesting. In particular, the finding that agents generalize and thus perform better on tournament selection when trained in larger population is intriguing as well as the exploration results with population count!\n+ Reproducibility: authors claim they will release environment simulator code.\n\nCon\n- Paper can be considerably tightened. It is currently quite long (9.5 pages vs the suggested 8 page). There are also a lot of details included that don't seem core to the message. For example -- the multiple types of API / IPC communication, much of the RPGs section.\n- Some areas of writing could be improved, either too casual, or sloppy. For example -- various names are not capitalized in bibliography.\n- Examples of imprecise / casual writing: \"good performance without discounting, but training was less stable.\" What does \"less stable\" actually mean? \"postprocess trajectories using a discount factor\" this is part of the REINFORCE algorithm -- postprocessing, to me, implies modifying the observations. The term \"numerical collapse\" is not a term I am aware of.\n- It is unclear what is shown in many of the figures. What are the colors in figures 8,9,10 for example? \n- Lots of details and ongoing work put in which distracts from a clear message. For example, why was \"entity targeting\" included? It doesn't appear to be described and the results shown in figure 10 are confusing. I would consider stepping back, and figuring out what one thing you want to show the reader, then drop all detail not around that point.\n- Lacking a conclusion of somesort. Ideally there would be something to pull the whole paper together.\n- use of terminology -- unclear why neural mmo is name of this environment. This is not a MMO, nor does the environment have anything \"neural\" related -- one can train reinforcement learning agents without neural network function approximators on it for example. I would consider renaming.\n\nIn its current form, I do not recommend accepting this paper but I do encourage the authors to continue working on it to both tighten the writing and presentation as well as continue to show interesting results via RL experiments.\n\n\n\nEDIT: See bellow, raised score from 4 --> 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}