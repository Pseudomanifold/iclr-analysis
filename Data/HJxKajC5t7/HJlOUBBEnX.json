{"title": "Simple but not vey novel bianrization method for weights and activations", "review": "The authors propose to use a scaled hyperbolic tangent function (soft quantization function) to mimic the hard quantization function, and gradually sharpen it during the training process to diminish the gap between the soft and hard binarization functions. The authors also propose a more efficient batch normalization method.\n\nThis method is simple yet general as the soft quantization function can be used for both weights and activations, which is a desirable property. One major concern is that the proposed continuous relaxation training trick is previously studied and  used in  applications like hashing and this may not be treated as an inspiring technical contribution.  Moreover, some more recent state-of-the-art approaches achieve better results (e.g., [1]) than XNOR-NET, and discussions on these methods may further improve the proposed method. Yet another concern is that the accuracy results  (top1-37.84/top5-64.06) for XNOR-NET (Bw=1, Ba=1, Bbn=32) in this paper is much lower that the reported (top1-44.2/top5-69.2) in the original XNOR-NET paper. Even the results of the proposed method is much lower than the original XNOR-NET results. This may raise concern on the efficacy of the proposed method whether the proposed method is good or not. Can the authors use the same setting as the original XNOR-NET paper and do the comparison?\n\n[1]. Towards accurate binary convolutional neural network, NIPS2017. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}