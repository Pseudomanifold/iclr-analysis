{"title": "nice contribution", "review": "Summary\nThe authors propose a relatively simple approach to mine noisy parallel sentences which are useful to greatly improve performance of purely unsupervised MT algorithms.\nThe method consists of a) mining documents that refer to the same topic, b) extracting from these documents parallel sentences, c) training the usual unsup MT pipeline with two additional losses, one that encourages good translation of the extracted parallel sentences and another one forcing the distribution of words to match at the document level.\n\nNovelty: the approach is novel.\n\nClarity: the paper is clearly written.\n\nEmpirical validation: The empirical validation is solid but limited. The authors could further strengthen it by testing on low-resource language pairs (En-Ro, En-Ur).\nIt would also be useful to report more stats about the retrieved sentences in tab. 1 (average length compared to ground truth, BLEU using as reference the translation of a SoA supervised MT method, etc.)\n\nQuestions\n1) Sec. 3.2 is the least clear of the paper. The notation of eq. 7 is quite unclear because of the overloading (e.g., P refers to both the model and the empirical distribution).\nI am also unclear about this constraint about matching the topic distribution: as far as I understood, the model gets only one gradient signal for the whole document. I find then surprising that the authors managed to get any significant improvement by adding this term.\nRelated to this term, how is it computed? Are documents translated on the fly as training proceeds? Could the authors provide more details?\n\n2) Have the authors considered matching sentences to any other sentence in the monolingual corpus as opposed to sentences in the comparable document?\n ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}