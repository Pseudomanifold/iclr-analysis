{"title": "Interesting approach; not sure if really scales to long horizon problems", "review": "The paper introduces an interesting approach to model learning for imitation and RL. Given the problem of maintaining multi-step predictions in the context of sequential decision making process, and deficiencies faced during planning with one-step models [1][2], it\u2019s imperative to explore approaches that do multi-step predictions. This paper combines ideas from learning sequential latent models with making multi-step future predictions as an auxiliary loss to improve imitation learning performance, efficiency of planning and finding sub-goals in a partially observed domain.\n\nFrom what I understand there are quite a few components in the architecture. The generative part uses the latent variables z_t and LSTM hidden state h_t to find the factored autoregressive distribution p_\\theta. It\u2019s slightly unclear how their parameters are structured and what parameters are shared (if any). I understand these are hard to describe in text, so hopefully the source code for the experiments will be made available.\n\nOn the inference side, the paper makes a few choices to make the posterior approximation. It would be useful to describe the intuitions behind the choices especially the dependence of the posterior on actions a_{t-1}:T because it seems like the actions _should_ be fairly important for modeling the dynamics in a stochastic system.\n\nIn the auxiliary cost, it\u2019s unclear what q(z|h) you are referring to in the primary model. It\u2019s only when I carefully read Eq 7, that I realized that it\u2019s p_\\theta(z|h) from the generator. \n\nSlightly unsure about the details of the imitation and RL  (MPC + PPO + Model learning) experiments. How large is the replay buffer? What\u2019s the value of k? It would be interesting how the value of k affects learning performance. It\u2019s unclear how many seeds experiments were repeated with.\n\nOverall it\u2019s an interesting paper. Not sure if the ideas really do scale to \u201clong-horizon\u201d problems. The MuJoCo tasks don\u2019t need good long horizon models and the BabyAI problem seems fairly small.\n\n- Minor points\n\nSec 2.3: not sensitive *to* how different\nAlgorithm 2: *replay* buffer\n\n[1]: https://arxiv.org/abs/1612.06018\n[2]: https://arxiv.org/abs/1806.01825", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}