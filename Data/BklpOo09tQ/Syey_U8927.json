{"title": "Overall good paper", "review": "Paper summary: The paper presents a 2-step approach to generate strong adversarial examples at a far lesser cost as compared to recent iterative multi-step adversarial attacks. The authors show the improvements of this technique against different attacks and show that the robustness of their 2-step approach is comparable to the iterative multi-step methods. \n\nThe paper presents an interesting technique, is nicely written and easy to read. The fact that their low-cost 2-step method achieves is robust enough to iterative multi-step methods that are expensive is significant.  \n\nPros: \n1) The technique is low-cost as compared to other expensive techniques like PGD and IFGSM \n2) The technique tries to use the categorical distribution of the generated example in the first step to generate an example in the second step, such that the generated image is most different from the first. This is important and different from the most common technique of iteratively maximizing the loss between the generated samples. \n3) The authors show the effetiveness  and improvement of the approach to various attack methods as compared to existing defense techniques\n4) The authors evaluate their technique on MNIST and SVHN datasets\n\n\nCons or shortcomings/things that need more explanation :\n1) It would have been really good to the kind of adversarial examples generated by this technique look like as compared to the examples generated by the other strategies. \n2) In table 2, for the substitute models of FGSM trained on H and S labels (rows 2 and 5), it is unclear why the accuracies are so low when attacked on FGSM (hard) and FGSM(soft) models. \n ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}