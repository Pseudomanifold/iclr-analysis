{"title": "Interesting but somewhat incremental approach; related work a bit weak", "review": "The authors aim to increase diversity in machine translation using a multinomial latent variable that captures uncertainty in the target sentence. Modeling uncertainty with latent variables is of course relatively common in ML, and this work has similarities with latent variables models for MT [Zhang et al., 2016] and for other generation tasks such as dialogue [Serban et al., 2017; etc.]. The key difference is that the authors here use a Mixture of Expert (MoE) approach while most relevant prior works use variational approaches. Experiments show improvements in diversity over variational NMT [Zhang et al., 2016] and decoding-time approaches (e.g., diversity constraints [Vijayakumar et al., 2016]).\n\nOverall, the proposed approach (hard-MoE) is well motivated and the experimental results are relatively promising. I think the authors did a good job analyzing and justifying their approach against the soft version of their model (i.e., soft-MoE causes experts to \u201cdie\u201d during training) and variational alternatives (i.e., variational approaches often have failure modes where the latent variable is effectively ignored.) \n\nHowever, I find related work a bit weak because the problem of producing diverse output has been a much bigger focus in tasks other than MT, such as dialogue and image captioning. The paper glosses over related approaches on these tasks, but the need to model uncertainty for these other tasks is much bigger since source and target are usually not semantically equivalent. So it would have been nice to see argumentative (or even empirical) comparisons with popular models such as VHRED for dialogue [Serban et al., 2017], as many of these models are not intrinsic to either MT or dialogue (the only aspect specific to dialogue in VHRED is context, but it can be set to empty and thus VHRED could have been used as a baseline in the paper.) It would be interesting to compare the work against Serban et al. [2017]\u2019s justification for using a latent variable, which is quite different (see their bit on \u201cshallow generation\u201d, and the idea that their latent variable encapsulates \u201cthe high-level semantic content of of the output\u201d).  \n\nOne technical caveat is that there appears to be some inconsistency in the comparison between human and systems in Table 1. If N is the number of references, then systems are evaluated on N references while the human \u201csystem\u201d on only N-1 because of leave-one-out. While this difference might have less of an impact on \u201caverage oracle BLEU\u201d than standard BLEU, having one less reference might still penalize the human \u201csystem\u201d, and this might partially explain why \u201cbeam search\u2019s average oracle BLEU is fairly close to human\u2019s average oracle BLEU\u201d. The right thing to do would be to evaluate both human and all systems in a leave-one-out approach (i.e., let references [r1 \u2026 rN] and systems [s1 \u2026 sM], then evaluate each element of [s1 \u2026 sM r1] on references [r2 \u2026 rN], etc.). In that manner, all the \u201csystems\u201d including human are consistently evaluated on *exactly* the same references. \n\nMinor comments: \n\n \u201cBy putting the model in evaluation mode during minimization we also speed up training and reduce memory consumption, since the K forward passes have no gradient computation or storage.\u201d In other words, does this mean the algorithm is easy to *parallelize* because sharing parameters is often what kills the effectiveness of parallelized SGD and variants? If so, \u201cparallelizing\u201d is key word to mention here otherwise I don\u2019t see how we can speed that up by increasing K.\n\nFigure 2: performance drops with K approaching 20. What happens with K=50 or 100 or more? This is a bit of a concern because (1) larger K could require a massive amount parallelization and (2) competing approaches such as VHRED can handle latent variables with higher capacities.\n\nPractical considerations subsection is too vague: parameter sharing is not formally/mathematically explained and the work could be hard to reproduce exactly (as there are often different ways to share parameters). \n\nWhy no \u201c#ref covered\u201d for human in Table 1, and why no comparison with Variational NMT? Zhang et al [2016] is the most talked about competing model, so it should probably be evaluated on both settings.\n\nMissed reference: Mutual Information and Diverse Decoding Improve Neural Machine Translation.\nJiwei Li, Dan Jurafsky. https://arxiv.org/abs/1601.00372", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}