{"title": "Interesting paper with marginal results", "review": "This paper proposes changes to the ELBO loss used to train VAEs, to avoid posterior collapse. They motivate their additional components rather differently than what has been done in the literature so far, which I found quite interesting.\nThey compare against appropriate baselines, on MNIST and OMNIGLOT, in a complete way.\n\nOverall, I really enjoyed this paper, which proposed a novel way to regularise posteriors to force them to encode information. However, I have some reservations (see below), and looking squarely at the results, they do not seem to improve over existing models in a significant manner as of now.\n\nCritics:\n1.\tThe main idea of the paper, in introducing a measure of diversity, was well explained, and is well supported in its connection to the Mutual Information maximization framing. One relevant citation for that is Esmaeili et al. 2018, which breaks the ELBO into its components even further, and might help shed light on the exact components that this new paper are introducing. E.g. how would MAE fit in their Table A.2?\n2.\tOn the contrary, the requirement to add a \u201cMeasure of Smoothness\u201d was less clear and justified. Figure 1 was hard to understand (a better caption might help), and overall looking at the results, it is even unclear if having L_smooth is required at all?\n\u2028Its effect in Table 1, 2 and 3 look marginal at best?\u2028\nGiven that it is not theoretically supported at all, it may be interesting to understand why and when it really helps.\n3.\tOne question that came up is \u201chow much variance does the L_diverse term has\u201d? If you\u2019re using a single minibatch to get this MC estimate, I\u2019m unsure how accurate it will be. Did changing M affect the results?\n4.\tL_diverse ends up being a symmetric version of the MI. What would happen if that was a Jensen-Shannon Divergence instead? This would be a more principled way to symmetrically compare q(z|x) and q(z).\n5.\tOne aspect that was quite lacking from the paper is an actual exploration of the latent space obtained. \u2028The authors claim that their losses would control the geometry of the latents and provide smooth, diverse and well-behaved representations. Is it the case?\n\u2028Can you perform latent traversals, or look at what information is represented by different latents?\u2028 \nThis could actually lend support to using both new terms in your loss.\n6.\tReconstructions on MNIST by VLAE seem rather worst than what can be seen in the original publication of Chen et al. 2017? Considering that the re-implementation seems just as good in Table 1 and 3, is this discrepancy surprising?\n7.\tFigure 2 would be easier to read by moving the columns apart (i.e. 3 blocks of 3 columns).\n\nOverall, I think this is an interesting paper which deserves to be shown at ICLR, but I would like to understand if L_smooth is really needed, and why results are not much better than VLAE.\n\nTypos:\n-\tKL Varnishing -> vanishing surely?\n-\tDevergence -> divergence\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}