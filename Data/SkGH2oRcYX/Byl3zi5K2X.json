{"title": "Review for Deep Adversarial Forward Model", "review": "Summary: Model-based RL that work on pixel-based environments tend to use forward models trained with pixel-wise loss. Rather than using pixel-wise loss for an action-conditioned video prediction model (\"Forward Model\"), they use an adversarial loss combined with mutual-information loss (from InfoGAN) and content loss (based on difference in convnet features of VGG network, rather than pixels). They run experiments on video-action sequences collected from an Atari game (Frostbite), and on a Udacity driving dataset.\n\nPros: The introduction and related work section is very well written, and motivation of why one should try adversarial loss for forward models is clear.\n\nWhile I think this work has potential, this paper is clearly not ready for publication, and below are a few suggestions on what I think the authors need to do to improve the work:\n\n(1) The authors emphasize novelty, and being \"first\" a few times in the paper, but fail to mention the large existing work done on video prediction (i.e. [1]), many of which also used these triplet loss or adversarial losses. Sure, those works focus on video prediction, while this work focus on building a \"forward model\"and is supposed to be for model-based RL, but this work has not performed any model-based RL experiments, so from my point of view, it is a video-prediction model contingent on an action input. Regardless, I believe the approach and results should be compared to existing work on video prediction, and similarities and differences to existing approaches should be highlighted. Adding an action-conditioned element to existing video-prediction techniques is also fairly simple.\n\n(2) From reading the intro/related work section, this work is clearly motivated in the direction of model-based RL, and the authors has already used this model for Frostbite. If this method is useful for model-based RL, I would expect to see experimental results for RL, at least for Frostbite (rather than just the training loss in Table 1). Rather than focusing on saying this method is the first to use triplet loss, or the first to use adversarial loss for forward models, I am much more interested in seeing a forward model that works well for RL tasks, since, that's the point right?\n\nAlthough the work is promising, I can only give it a score of 4 at the moment. If the author fixes the writing to include detailed discussion with video prediction literature, with good quantitative and qualitative comparison to existing methods, that is worth 1 extra point. If the author has good results on using this forward model on environments that have previously used older forward models (such as Atari environments in [2] or CarRacing/VizDoom in [3]), and presents those results in a satisfactory way, that may increase my score by another 1-2 points depending on the depth of the experiments. Currently the paper is only < 7 pages, so I believe there is room for more substance.\n\nMinor points:\n- in related work section, should be f_{theta} not f_theta\n\n[1] Denton et al., \"Unsupervised Learning of Disentangled Representations from Video\", (NIPS 2017). https://arxiv.org/abs/1705.10915\n[2] https://arxiv.org/abs/1704.02254\n[3] https://arxiv.org/abs/1803.10122\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}