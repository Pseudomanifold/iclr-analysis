{"title": "Updated review", "review": "EDIT: I thank the authors for providing all clarifications. I think this paper is a useful contribution. It will be of interest to the audience in the conference.\n\nSummary:\nThis paper provides a method to jointly learn from crowdsourced worker labels and the actual data. The key claimed difference is that previous works on crowdsourced worker labels ignored the data. At a higher level, the algorithm comprises maximizing the mutual information gain between the worker labels and the output of a neural network (or more generally any ML model) on the data. \n\nEvaluation:\nI like the idea behind the algorithm. However there are several issues on which I ask the authors to provide some clarity. I will provide a formal \"evaluation\" after that. (For the moment, please ignore the \"rating\". I will provide one after the rebuttal.) \n\n(1) As the authors clarified, one key aspect of the \"information intersection\" assumption is that the crowdsourced labels are statistically independent from the data when conditioned on the ground truth. How strongly does this coincide with reality? Since the work is primary empirical, is there any evidence on this front?\n\n(2) In the abstract, introduction etc., what does it mean to say that the algorithm is an \"early algorithm\"?\n-- Thanks for the clarification. I would suggest using the term \"first algorithm\" in such cases. However, is this the first algorithm towards this goal? See point (3).\n\n(3) The submitted paper misses an extremely relevant piece of literature: \"Learning From Noisy Singly-labeled Data\" (arXiv:1712.04577). This paper also aims to solve the label + features problem together. How do the results of this paper compare to that of this submission?\n\n(4) \"Model and assumptions\" Is the i.i.d. assumption across the values of \"i\"? Then does that not violate the earlier claim of accommodating correlated mistakes?\n\n(5) Recent papers on crowdsourcing (such as Achieving budget-optimality with adaptive schemes in crowdsourcing arXiv:1602.03481 and  A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness arXiv:1606.09632) go beyond restricting workers to have a common confusion matrix for all questions. In this respect, these are better aligned with the realistic scenario where the error in labeling may depend on the closeness to the decision boundary. How do these settings and algorithms relate to the submission?\n\n(6) Page 5: \"Later we will show....\"   Later where? Please provide a reference.\n\n(7) Theorem 3.4, The assumption of existence of experts such that Y^S is a sufficient statistic for Y: For instance, suppose there are 10 experts who all have a 0.999 probability of correctness (assume symmetric confusion matrices) and there are 5 non-experts who have a 0.001 probability of correctness and even if we suppose all are mutually independent given the true label, then does this satisfy this sufficient statistic assumption? This appears to be a very strong assumption, but perhaps the authors have better intuition?\n\n(8) The experiments comprise only some simulations. The main point of experiments (particularly in the absence of any theoretical results) towards bolstering the paper is to ensure that the assumptions are at least somewhat reasonable. I believe there are several datasets collected from Amazon Mechanical Turk available online? Otherwise, would it be possible to run realistic experiments on some crowdsourcing platforms?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}