{"title": "More details on actual learning are required", "review": "Update after feedback: I would like to thank the authors for their detailed answers, it would be great to see some revisions in the paper also though (except new experimental results).\nEspecially thank you for providing details of a training procedure which I was missing in the initial draft. I hope to see them in the paper (at least some of them).\n\nI have increased the rating to 6. Given new experimental results both on real data and forecaster comparison I would like to increase the rating to 7. However, I am not sure that this is fair to other authors who would might not be physically able to provide new experimental results due to computational constraints, please note that the experiments in this paper are rather 'light' in the standards of modern deep learning experiments and can be done within the rebuttal period.  \n====================================================\n\n\nThe paper finds a practical implementation of ideas from Kong & Schoenebeck (2018) for the learning with crowd problem. It proofs the claims from Kong & Schoenebeck (2018) for the specific family of data classifiers and crowd aggregators. From the general perspective, the papers proposes a method for joint training a classifier and a crowd label aggregator with particular consideration of correlated crowd labels. \n\nThe paper is fairly well-written and well-balanced between theoretical and empirical justification of the method. I see 1 major and 1 big issues with the paper.\n\nMajor issue: I am missing details of the actual procedure of training the model. Is MIG set as a loss function for the data classifier NN? Is crowd aggregator trained also as an NN with MIG as a loss function? How do the authors find the optimal p? Also, in order all the provided theory to work all the found data classifier NN, the aggregator and p should be exact maximisers of MIG as far as I understand. How do the author ensure that they find the exact maximisers? Also related to understanding how training works: on p.15 the authors claim \u201cNote that our method can handle this simple correlated mistakes case and will give all useless experts weight zero based on Theorem 3.4.\u201d I have trouble understanding why the proposed method should find these zero weights rather than it is just able to find them?\n\nI am willing to change my judgement if the authors provide convincing details on the training procedure.\n\nBig issue: Experimental settings. \na) Though it is interesting to see the analysis of the method under controlled environments of synthetic crowd labels with different properties that show benefits of the proposed method (such as dealing with correlated crowd labels), it would be also appealing to see the results with real labels, for example, Rodrigues & Pereira (2017) provide Amazon MTurk crowd labels for the LabelMe data\nb) Is the proposed data-crowd forecaster the only method that uses crowd labels on the test data? While it can be argued that it is not straightforward in the test regime to include crowd labels into Crowd Layer, for example, without retraining the neural net, AggNet can use crowd labels without retraining the neural net part. In the presented format, it is unfair to compare the forecaster with the other methods because it uses more information, and essentially, the forecaster is not compared with anything (that uses the same information). It can be compared, at least, with pure Majority Voting, or more advanced pure crowdsourcing aggregation methods. Yes, they won\u2019t use image data, but at least they can use the same amount of crowd label information, which would make a nice comparison with the presented related work and proposed NN: this is what you can get using just image data during test (Crowd Layer, Max-MIG, and others from the current paper), this is what you can get using just crowd labels during test (Majority Voting or, preferably, more advanced pure crowdsourcing aggregators), and this is what you can get using both image and crowd labels during test (the proposed forecaster and AggNet, for example)\n\nQuestions out of curiosity: \ni). Does Max-MIG handle missing crowd labels for some data points? Did the author use missing labels in the experiments?\nii). Both the Dogs vs. Cats and CIFAR-10 datasets have more or less balanced data, i.e., the number of data points belonging to each ground truth class is similar between classes. Is this true for the LUNA16 dataset? If yes, have the authors tried their method with heavily imbalanced data? In my experience, some crowdsourcing methods may suffer with imbalanced data, for example, Crowd Layer does so on some data. This tendency of Crowd Layer is kind of confirmed on the provided Dogs vs. Cats in the na\u00efve majority case, where based on crowd labels the first class dominates the second.\n\nOther questions/issues/suggestions:\n1. Until the formal introduction of the forecaster on page 4, it is not entirely clear what is the difference between the data classifier and data-crowd forecaster. It should be explained more clearly at the beginning that the 3 concepts (data classifier, crowd label aggregator and \"data-crowd forecaster\") are separated. Also some motivation why we should care about forecaster would be beneficial because one can argue that if we could train a NN that would make good enough predictions why we should waste resources on crowd labels. For example, the provided empirical results can be used as an argument for this.\n2. From the introduction it is unclear that there are methods in crowdsourcing that do not rely on the assumption that data and crowd labels are independent given the ground truth labels. As mentioned in related works there are methods dealing with difficulty of data points, where models assume that crowd labels maybe biased on some data points due to their difficulty, e.g., if images are blurred, which violates this assumption.\nAlso the note that considering image difficulty violates the independence assumption could be added on page 3 around \"[we] do not consider the image difficulty\"\n3. The beginning of page 4. I think it would be more clear to replace \"5 experts' labels:\" by $y^{[5]}=$\n4. I suggest to move the caption of Figure 3 into the main text.  \n5. p.3 \"However, these works are still not robust to correlated mistakes\" - Why? \n6. Data-crowds forecaster equation. It would be good to add some intuition about this choice. The product between the classifier and aggregator predictions seems reasonable, division on p_c is not that obvious. This expression presumably maximises the information gain introduced below. Some link between this equation and the gain introduction would be nice. Also, minor point \u2013 it is better to enlarge inner brackets ()_c\n7. The formulation \u201cTo the best of our knowledge, our approach is a very early algorithm\u201d, and namely \u201ca very early algorithm\u201d is unclear for me\n8. Dual usage of \u201cinformation intersection\u201d as an assumption and as something that Max-MIG finds is confusing\n9. Any comments how the learning rates were chosen are always beneficial\n10. Proof of Proposition C.3: \u201cBased on the result of Lemma C.2, by assuming that h \u2217 \u2208 H_{NN} , we can see (h \u2217 , g\u2217 ,p \u2217 ) is a maximizer of max_{h\u2208H_{NN} ,g\u2208G_{W A},p\u2208\u2206_C} MIGf (h, g,p)\u201d \u2013 is expectation missing in the max equation? Is this shown below on page 13? If yes, then the authors should paraphrase this sentence as it does not imply that this is actually shown below\n11. p.12 (and below) \u2013 what is $\\mathbf{C}^m$? Is it $\\mathbf{W}^m$?\n12. p.15 (at the end of proof) $p \\log q$ and $p \\log p$ are not formally defined\n\nMinor:\n1. p.1 \"of THE data-driven-based machine learning paradigm\"\n2. \"crowds aggregator\" -> \"crowd aggregator\"?\n3. p.2 (and below) \"between the data and crowdsourced labels i.e. the ground truth labelS\"\n4. Rodrigues & Pereira (2017) has a published version (AAAI) of their paper\n5. p.2 \"that model multiple experts individually and explicitly in A neural network\"\n6. p.3 \"model the crowds by A Gaussian process\"\n7. p.3 \"We model the crowds via confusion matriCES\"\n8. p.3 \"only provide A theoretic framework and assume AN extremely high model complexity\"\n9. p.4 \"forecast\" for h and g -> \"prediction\"?\n10. p.6 \u201cbetween the data and the crowdsourced labelS\u201d?\n11. p.6 \u201cHowever, in practice, with A finite number of datapoints\u201d\n12. p.6 \u201cthe experiment section will show that our picked H_{NN} and G_{W A} are sufficientLY simple to avoid over-fitting\u201d\n13. p.6 \u201cWe call them A Bayesian posterior data classifier / crowds aggregator / data-crowds forecaster, RESPECTEVILY\u201d\n14. p.6 \u201cTheorem 3.4. With assumptionS 3.1, 3.3\u201d\n15. p.7 \u201cDoctOr Net, the method proposed by Guan et al. (2017)\u201d\n16. p.7 \u201cincluding the naive majority case since naive expert is independent with everything\u201d \u2013 rephrasing is required, unclear what \u201cindependent with everything\u201d means and who is \u201cna\u00efve expert\u201d\n17. Please capitalised names of conferences and journals in References\n18. p.10 \u201cshe labels the image as \u201cdog\u201d/\u201ccat\u201d with THE probability 0.6/0.8 respectively\u201d, \u201c(e.g. B labels the image as \u201ccat\u201d with THE probability 0.5 and \u201cdog\u201d with THE probability 0.5 when the image has cats or dogs)\u201d\n19. p.12 \u201cLemma C.2. (Kong & Schoenebeck, 2018) With assumptionS 3.1, 3.3\u201d, \u201cProposition C.3. [Independent mistakes] With assumptionS 3.1, 3.3\u201d\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}