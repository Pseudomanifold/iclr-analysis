{"title": "Potential to advance the area, but the paper should be improved", "review": "The paper proposes improvements on the area of neural network inference with homomorphically encrypted data, where a major drawback in current applications is the high computational cost.\n\nThe paper makes several minor contributions and, in my opinion, it fails to provide an unified message to the reader. The arguably independent contributions are:\n1. the use of the faster SAEL 3.2.1 over CryptoNet \u2014 not really an innovation per se\n2. flexible representation of message space \u2014 the main contribution\n3. a much more complex network than CryptoNet for an experiment on CIFAR10 \u2014 minor contribution/application of 2\n4. using pre-trained model for embedding images before encryption \u2014 minor contribution\n\nI think the authors should refocus this work on point 2 and 3 only. 1 could simply be a note in the paper, as not a a real contribution and 4 may be even excluded as it goes toward a different direction.\n\nI will mainly focus on the central contribution of the paper \u2014 the data representation \u2014 which in my opinion has the potential to progress this area further. Unfortunately, the current quality of presentation is suboptimal. The choices on architecture and intermediate representations of LoLa in Section 4 are hard to follow, and it is not clear to me when the authors are making a choice or satisfying a constraint. That is, for example when we aim to vs. we have to use one representation in place of another? Since this is the main contribution, I suggest the author to help the presentation with diagrams and formulae in this Section. Each component should be presented in modular fashion. In fact, this is what the authors did in Section 3. Yet, towards reuse of these concepts for machine learning applications, the authors could present their improvements as applied to *neural network layers*. E.g. responding to answers such as: what are the most efficient data representation in input and output if we want to compute a convolutional layer? It would be also interesting to break down cost requirements for each intermediate computation, to highlight bottlenecks and compare alternative choices.\n\nIncidentally, LoLa-Conv improvements appear to be simply due to the use of a conv vs. dense layer as input.\n\nSection 6 explains the use of a pre-trained AlexNet to embed data for learning a linear model (HE-aware) for Caltech 101. In the context of application of HE, the idea is novel to my knowledge, although it is a rather common practice in deep learning. The number reported in this section have no baseline to compare with and therefore it\u2019s hard to evaluate their significance.\n\nA missing reference is [A].\n\nMinors\n* section 4: \u201cthis saves a processing step which saves\"\n* typos in section 6: \u201ca networks\u201d, \u201cin medical imagine\"\n* footnote 4: \u201cconvinient\"\n\n[A] TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}