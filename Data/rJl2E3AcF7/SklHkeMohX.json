{"title": "Need to discuss more about how Doubly Sparse is superior to Sparsely-Gated MoE", "review": "The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.\n\n[+] It reduces computational cost compared to full softmax.\n[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed\n[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this. Besides, in evaluation, the paper only compares Doubly Sparse with full softmax. Why not compare with Sparsely-Gated MoE?\n\nOverall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}