{"title": "New method for large scale softmax inference", "review": "In this paper the authors introduce a new technique for softmax inference. In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert. Then, given the expert, output a particular category. The first level of sparsity comes from the first expert. The second level of sparsity comes from every expert only outputting a limited set of output categories.\n\nThe paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time? I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well. You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?\n\nNits:\n- it wasn't clear how the sparsity percentage on page 3 was defined?\n- can you motivate why you are not using perplexity in section 3.2?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}