{"title": "Interesting study of an overlooked problem", "review": "# Positive aspects of this submission\n\n- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.\n\n- The methodology in Sections 4 and 5 is very thorough and useful.\n\n- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.\n\n# Criticism\n\n- In Section 3, even if the \"l1 + projection\" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).\nSimilarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.\nGiven that you also study \"o-greedy-hit\" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}