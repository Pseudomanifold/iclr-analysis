{"title": "hard to get a handle on due to very limited clarity", "review": "The paper is hard to read, and hard to get a handle on the significance of the result.\n\nThe work describes an error-backpropagation rule based on an approximation of the activation of each neuron, where an error measure is taken from the membrane potential of the output neuron and then backpropagated in auto encoders with a single hidden layer (which severely limits the complexity of backprop). The backprop as proposed is essentially standard backprop, where a continuous approximation of the forward activation is made such that the gradients can be computed. There is mention of applying effectively BPTT, but I do not see that in the model (sec 2.3). \n\nI am also concerned as to whether the comparison with ANNs is correct in section 3.1. Why only one epoch with specific parameters? Do the parameters matter? \n\nThe quality of the work is unclear in places, like whether the comparisons are valid. The clarity is problematic, also because the grammar is off in many places. \n\nI am also highly dubious about the claim that auto encoders can only learn the hidden representations  of only one modality. Citation? Or a detailed explanation. \n\nThe performance shown in Figure 7 for the SNN seems rather poor compared to an ANN, if the comparison is valid. The reported results in Table 1 are confusing, as some are and some are not following from the graphs. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}