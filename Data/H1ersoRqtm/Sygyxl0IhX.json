{"title": "Limited novelty and missing some key experiments", "review": "Note: I changed my original score from 4 to 7 based on the new experiments that answer many of the questions I had about the relative performance of each part of the model. The review below is the original one I wrote before the paper changes.\n\n# Positive aspects of this submission\n\n- The intuition and motivation behind the proposed model are well explained.\n\n- The empirical results on the MethodNaming and MethodDoc tasks are very promising.\n\n# Criticism\n\n- The novelty of the proposed model is limited since it is essentially adding an existing GGNN layer, introduced by Li et al. (2015), on top of an existing LSTM encoder. The most important novelty seems to be the custom graph representation for these sequence inputs to make them compatible with the GGNN, which should then deserve a more in-depth study (i.e. ablation study with different graph representations, etc).\n\n- Since you compare your model performance against Alon et al. on Java-small, it should be fair to report the numbers on Java-med and Java-large as well.\n\n- The \"GNN -> LSTM+POINTER\" experiment results are reported on the MethodDoc task, but not for MethodNaming. Reporting this number for MethodNaming is essential to show the claimed empirical superiority of the hybrid encoder compared to GNN only.\n\n- I have doubts about the usefulness of the proposed model for natural language summarization, for the following reasons:\n\n    - The comparison of the proposed model for NLSummarization against See et al. is a bit unfair, since it uses additional information through the CoreNLP named entity recognizer and coreference models. With the experiments listed in Table 1, there is no way to know whether the increased performance is due to the hybrid encoder design or due the additional named entity and coreference information. Adding the entity and coreference data in a simpler way (i.e. at the token embedding level with a basic sequence encoder) in the ablation study would very useful to answer that question.\n\n    - In NLSummarization, connecting sentence nodes using a NEXT edge can be analogous to using a hierarchical encoder, as used by Nallapati et al. (\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\", 2016). Ignoring the other edges of the GNN graph, what are the theoretical and empirical advantages of your method compared to this sentence-level hierarchical encoder?\n\n    - Adding the coverage decoder introduced by See et al. to your model would have been very useful to prove that the current performance gap is indeed due to the simplistic decoder and not something else.\n\n- How essential is the weighted averaging for graph-level document representation (Gilmer et al. 2017) compared to uniform averaging?\n\n- A few minor comments about writing:\n    - In Table 1, please put the highest numbers in bold to improve readability\n    - On page 7, the word \"summaries\" is missing in \"the model produces natural-looking with no noticeable negative impact\"\n    - On page 9, \"cove content\" should be \"core content\"\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}