{"title": "Interesting results. Some more experimentation needed", "review": "This work presents a recurrent attention model as part of an RNN-based RL framework. The attention over the visual input is conditioned on the the model's state representation at time t. Notably, this work incorporated multiple attention heads, each with differing behavior.\n\nPros:\n-Paper was easy to understand\n-Detailed analysis of model behavior. The breakdown analysis between \"what\" and \"where\" was particularly interesting.\n-Attention results appear interpretable as claimed\n\nCons:\n-Compared to the recurrent mechanism in MAC, both methods generate intermediate query vectors conditioned on previous model state information. I would not consider the fact that MAC expects a guiding question to initialize its reasoning steps constitute a major difference in the overall method.\n-There should be an experiment demonstrating the effect of # of attention heads against model performance. How necessary is it to have multiple heads? At what point do we see diminishing returns?\n-I would also recommend including a citation for :\nSukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"End-to-end memory networks.\" NIPS 2015.\n\n\nGeneral questions:\n-Was there an effect of attention grid coarseness on performance?\n-For the atari experiments, is a model action sampled after each RNN iteration? If so, would there be any benefit to trying multiple RNN iterations between action sampling?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}