{"title": "An interesting visual attention approach.", "review": "Summary.\nThe paper proposes a variant model of existing recurrent attention models. The paper explores the use of query-based attention, spatial basis, and multiple attention modules running in parallel. The effectiveness of the proposed method is demonstrated with various tasks including RL (on Atari games), ImageNet image classification, and action recognition, and shows reasonable performance. \n\nStrengths.\n- An interesting problem in the current CV/RL community.\n- Well-surveyed related work.\n- Supplemental materials and figures were helpful in understanding the idea.\n\nvs. Existing recurrent attention models.\nIn Section 2, the proposed model is explained with emphasizing the differences from existing models, but there needs a careful clarification.\n\nIn this paper, attention weights are computed conditioned on a query vector (which solely depends on the RNN\u2019s state) and the Keys (which are generated by a visual encoder, called vision core). In the landmark work by Xu et al. (ICML \u201815, as already referenced), attention weights are computed very similarly - they used the hidden state of RNN followed by an additional layer (similar to the \u201cquery\u201d) and visual features from CNN followed by an additional layer (similar to the \u201ckeys\u201d). The only difference seems the use of element-wise multiplication vs. addition, but both are common units in building an attention module. Can authors clarify the main difference from the existing recurrent attention models?\n\nTraining details.\nIn the supervised learning tasks, are these CNN bases (ResNet-50 and ResNet-34) trained from scratch or pre-trained with another dataset?\n\nMissing comparison with existing attention-based models.\nThe main contribution claimed is the attention module, but the paper does not provide any quantitative/qualitative comparison from another attention-based model. This makes hard to determine its effectiveness over others. Notable works may include:\n\n[1] Sharma et al., \u201cAction recognition using visual attention,\u201d ICLR workshop 2016.\n[2] Sorokin et al., \u201cDeep Attention Recurrent Q-network,\u201d NIPS workshop 2015.\n[3] Choi et al., \u201cMulti-Focus Attention Network for Efficient Deep Reinforcement Learning,\u201d AAAI workshop 2017.\n\nMinor concerns.\nThe related work section would be helpful if it proceeds the current Section 2.\nTypos", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}