{"title": "Review", "review": "In this paper the authors propose a neural-symbolic architecture, called Neural Logic Machines (NLMs), that can learn logic rules.\n\nThe paper is pretty clear and well-written and the proposed system is compelling. I have only some small concerns.\nOne issue concerns the learning time. In the experimental phase the authors do not state how long training is for different datasets.\nMoreover it seems that the \u201crules\u201d learnt by NSMs cannot be expressed in a logical formalism, isn\u2019t it? If I am right, I think this is a major difference between dILP (Evans et. al) and NLMs and the authors should discuss about that. If I am wrong, I think the authors should describe how to extract rules from NLMs.\nIn conclusion I think that, once these little issues are fixed, the paper could be considered for acceptance.\n\n[minor comments]\np. 4\n\u201ctenary\u201d -> \u201cternary\u201d\n p. 5\n\u201cov varying size\u201d -> \u201cof varying size\u201d\n\u201cThe number of parameters in the block described above is\u2026\u201d. It is not clear to me how the number of parameters is computed.\n\u201cIn Eq. equation 4\u201d -> \u201cIn Eq. 4\u201d\n\np. 16\n\u201cEach lesson contains the example with same number of objects in our experiments.\u201d. This sentence sounds odd.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}