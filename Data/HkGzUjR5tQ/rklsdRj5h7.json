{"title": "Interesting approach for cross-domain / cross-lingual NER with solid empirical results; limited technical novelty", "review": "<Summary>\nAuthors propose the new \u201cDATNet\u201d for the NER task, which extends the base neural model for NER (Bi-LSTM+CRF sequence model with input represented with CharCNN-word embeddings) with the following two main components: (1) GRAD: a language (or resource) adversarial discriminator with adaptive weights that regularize source-target data imbalance, and (2) additional adversarial training approaches that perturb input samples in the embeddings space.\n\nThe paper reports big improvement over their baseline approaches without having to rely on other auxiliary or hand-crafted features. The experiment is performed for various low-resource scenarios (varying training data size). \n\n<Comments>\n- While the idea of applying \u201cdual adversarial\u201d approaches is new in the context of NER, the technical novelty of each component is limited. GRAD, for example, is rather a minor modification of Language Adversarial Discriminator (Kim et al., 2017) with a scalar weight parameter on loss. The empirical superiority of the proposed method (GRAD) over normal AD approaches cannot be claimed either -- for this authors need to report quantitative results of the (Base + AT + F/P-transfer with AD -- or e.g. \\alpha fixed at 0.5 or at some other rate), which I believe is missing in all figures and tables. Visualization of resulting feature distribution (Figure 3) is interesting to look at, but that alone does not suffice. There is no technical novelty in applying adversarial training either, except that it was used in the context of NER.\n\n\n- Authors use both source and target data to train their \u201cbase model\u201d as well (\u201c... we exploit all the source data and target data ...\u201c), e.g. presumably by merging the source and target dataset as well as their embedding matrices, etc. It is important that authors report if there ever is a negative transfer case (e.g. a base model trained with just target data may outperform models trained with source+target data) at varying resource scenarios -- especially at sufficiently-resourced cases.\nIf by any chance their \u201cbase model\u201d refers to in-domain training / in-domain testing results on target, the aforementioned baseline (naive merge of source and target data) is obviously an important baseline to report. I suggest that authors provide these details or clarify. (The confusion comes mainly because some of the SOTA results the authors quote are in-domain training / in-domain evaluation results on respective languages, and some are cross-domain results -- yet they are all under \u201ccross-lingual/domain\u201d columns in Table 2).\n\n- It would be interesting to report the learned optimal \\alpha value for each different setting (at varying r or training size) to see if the authors\u2019 intuition is met. On a related note, from the manuscript alone, it is not entirely clear if \\alpha is a learnable parameter or a tunable hyper parameter -- by context I believe it is a model parameter. If they are tuned, authors need to report these values.\n\n<Nit> \n- Section 3.2.3, \u201c... (GRAD) to enable adaptive weights for [each sample]\u201d \u2192 I think it reads better with [each resource] or [each source and target], unless you meant \\alpha_i for each sentence.\n- Section 3.2.5, \u201c... recently, adversarial samples are [wisely] incorporated\u201d \u2192 [widely]\n- Fonts for figures could be bigger. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}