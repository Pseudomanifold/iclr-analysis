{"title": "good initiative, not mature enough", "review": "The authors propose to optimize a black-box (validation/test) metric by learning to re-weight the training examples. The weights are calculated from a linear model on an auto-encoder-computed embedding, and the parameters of the linear model is found by an Gaussian-Process-Regression-(UCB)-guided global optimization procedure. Experimental results demonstrate that the learnt weights outputs uniform weights.\n\nThe paper is well-written with clear motivations. Nevertheless, the paper is not mature due to the following reasons:\n\n(1) The alternatives are not carefully compared/discussed for the key components of the proposed framework.\n   (1a) Why need the linear model? What if the weights are not calculated from the linear model parameters, but optimized by GPUCB directly?\n   (1b) Why need GPUCB? What if we just do random search or standard simulated annealing for global optimization?\n   (1c) Following (1a) and (1b), what if the weights are optimized directly through random search?\n   (1d) What if, in the case of MNIST, class weights are used instead of example weights\n   (1e) How sensitive is the proposed framework in terms of hyperparameters like p and q, and perhaps other GP parameters?\n\n(2) No comparison on the standard-but-challenging metrics like F1. The authors state in the experiments that it is not the focus of the work, but I do believe that the comparison is meaningful to help understand whether the proposed framework is close to the state-of-the-art in those standard metrics. Otherwise the baseline (uniform weights) is arguably just too weak.\n\n(3) Is the framework just overfitting the validation data set by reusing it to evaluate multiple times? Are there overfitting behavior observed during the reuses?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}