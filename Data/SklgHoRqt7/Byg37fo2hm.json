{"title": "Interesting problem. I think more discussion and experiments to understand better potential overfitting issues are needed here.  I'm intrigued that the approach works as well as it does on the examples given, so there may be something interesting here.", "review": "Pros:\n- Addresses several interesting and important problems all at once: covariate shift, concept drift, mismatch between training loss and test loss.\n- Fairly simple and elegant solution.\n- Multiple examples of the method working.\n- Clearly written\n\nCons:\n- Examples don't feel like full-fledged machine learning examples, where you  tune your learning algorithm on the validation set, as well as the example weights (their approach).  \n- Needs discussion of potential overfitting issues (see comments below).\n\nComments:\n- I think one area that is underexplored in this paper is overfitting.  One issue is overfitting the validation data by having more complicated weight functions. For example, in Figure 3(b), as the embedding dimension goes beyond 14, it seems like the error metric gets worse.  Would be interesting to see a plot of the validation error metric alongside this test error metric.  Also, what happens when we use even larger embedding dimensions -- that should clarify whether this is an overfitting situation, or just a random chance fluctuation.\n- In machine learning contexts, it's standard to try many different ML methods (or at least network architectures) with various hyperparameter settings and regularization methods, yet this isn't discussed at all in the paper.  Would you use the search for alpha as an inner loop in your model search and hyperparameter selection process?  I'd expect there could be additional issues with overfitting the validation set as you used more complicated models.  I think not discussing or investigating this makes the examples feel a little bit more like toys.  I think tuning your learning algorithm settings on a validation set is pretty intrinsic to machine learning approaches.\n- Relatedly, you say \"we impose no regularization on the model parameters\"... does this include things like early stopping, dropout, or other things that are used to prevent overfitting?  This seems just part of the \"no hyperparameter tuning\" setting of the paper.  \n- In the introdution you say \"MOEW . . . reshapes the total loss function to better match the testing metric.  This is similar to the idea of basis expansion, where we approximate the metric function using a linear combination of per example loss functions\".  You make a similar statement in the conclusion. This is an interesting idea, but it doesn't seem to represent what you're doing. This explanation suggests that you are fitting \\alpha's so that the objective function value approximates the validation metric for each theta.  But that's not what you're doing, right?\n- In 3.2, you say that c \"is a constant that normalizes the weights over a (batch from) the training set T\".  Why would you renormalize per batch?  This seems to potentially negate the effect of the reweighting, especially for small batches.\n- It might have been interesting to see if there was any significant differences between hinge loss and cross-entropy loss for the binary case.\n- In the MNIST experiment, am I correctly understanding that the difference between the 300 uniform weighted models you tried was the random initialization of the weights? Was there a lot of variation in performance among these trials?  Folk wisdom makes me think there would not be large performance differences.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}