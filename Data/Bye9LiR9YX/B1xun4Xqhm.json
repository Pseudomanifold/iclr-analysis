{"title": "REMEMBER AND FORGET FOR EXPERIENCE REPLAY REVIEW", "review": "This paper presents a method for forgetting and re-weighting experiences from a buffer during updates. It is well quantified experimentally and has some interesting tricks to improve performance in DDPG and other methods in continuous control which make use of a replay buffer. The authors also present another method \u201cRACER\u201d which makes use of this. \n\nI would like to see this published at some point, particularly because of the interesting results on DDPG. However, while it is interesting and useful, I do I have concerns both on the novelty and experimental comparisons in the current version. For example, RACER seems similar to ACER, yet doesn\u2019t compare to it, making it difficult to understand what is its benefit other than its use of REFER. Moreover, the authors state that without the REFER part (with PER instead), RACER doesn\u2019t work well at all, making it difficult to assess the RACER algorithm on its own. I would suggest if the authors claim that the contribution is the REFER algorithm they assess REFER in ACER on its own to make the main contribution stronger. \n\nRegarding novelty, I suggest that more of the paper can be spent situating the work in the broader scope of experience selection. There were several other methods that could have been compared against \u2014 for example (de Bruin et al., 2015) \u2014which also presents a forgetting method similar to this one. While that work is cited, I don't believe it is sufficiently contrasted against this work.\n\nBelow I will examine various points/thoughts that came up.\n\n+ well experimented, appreciated the use of confidence intervals in the appendix and extensive ablation. However, I\u2019d like to point out that the confidence intervals for some tasks spanned anything from 0 to the max, which did not inspire confidence. However, this may be a problem with the task and not the method, so not a significant problem \n+ clearly a lot of effort went into getting all these experiments and architecting the system which is well appreciated, great job there.\n+ DDPG results are promising and may indicate the problem with DDPG is its off-policy-ness. Nice results there.\n+ For the Re-Fer part, it was a bit unclear why it is 1/c_max < p_T <c_max rather than 0 < p_T < c_max? I suppose this is because you still want to update even if your current policy has not likelihood of that action? It would be nice to point to an explanation from that part of that text even if the intuition is in the appendix, otherwise it\u2019s a bit unclear as to why this is chosen to be the acquisition function. \n+ Along these lines it would be good to see more theoretical examination of on-policiness, rather than a binary threshold of the importance weight. \n+ This paper seemed somewhat unfocused and packed with stuff, almost like two papers together which made things a bit difficult to follow as to what the main contribution is. I believe this detracted from both methods. For example, it was unclear what the benefit of using RACER was vs. say any other method which makes use of REFER. As the authors state, RACER without the ReFer part seems to not really work well at all, which makes me question this part of the contribution. It seems like a more interesting experiment would be to update importance weighted off-policy PG algorithms with the REFER part. This would hone the message which seems to be the main contribution of the paper.\n+ I find it surprising that the authors compared PPO against RACER rather than using ACER which seems like the nearest analogue to this algorithm or IMPALA which seems to have a similar parallelized architecture.\n+ More work could have been cited on experience selection selection, for example:\n\nIsele, D., & Cosgun, A. (2018). Selective Experience Replay for Lifelong Learning.\u00a0arXiv preprint arXiv:1802.10269.\nPan, Yangchen, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White. \"Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains.\"\u00a0arXiv preprint arXiv:1806.04624\u00a0(2018).\n\n(I am aware that these are relatively new works, but after looking at the posting timestamps, I believe the original versions were posted several months at least prior to this publication.)\n\n+ Along these lines I have concerns about the novelty since de Bruin 2015 even uses a similar off-policy metric for forgetting already. There are several differences here, but I\u2019m not sure if they\u2019re significantly novel for publication in its current state. \n\nTypos/Grammar Issues Found:\n\n\u201cHowever, the information contained in consecutive steps is highly correlated, worsening the quality of the gradient estimate, and episodes can be composed of thousands of time step.\u201d \u2014> \u201cHowever, the information contained in consecutive steps is highly correlated, worsening the quality of the gradient estimate, and episodes can be composed of thousands of time step(s).\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}