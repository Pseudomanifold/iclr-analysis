{"title": "Review", "review": "OVERALL COMMENTS:\n\nI haven't had much time to write this, so I'm giving a low confidence score and you should feel free to correct me.\n\nI didn't think this paper was very clear. \nI had trouble grasping what the contributions were supposed to be\nand I had trouble judging the significance of the experiments. \n\nThat said, now that (I think) I understand what's going on,\nthe idea seems well motivated, the connection between the repulsion and the use of label information in other\nGAN variants makes sense to me, and the statements you are making seem (as much as I had time to check them) correct. \n\nThis leaves the issue of scientific significance. \nI feel like I need to understand what specifically contributed to the improvements in table 1 to evaluate significance. \nFirst of all, it seems like there are a lot of other 'good-scoring' models left out of this table. \nI understand that you make the claim that your improvement is orthogonal, but that seems like something that needs to\nbe tested empirically. You have orthogonal motivation but it might be that in practice your technique works for a reason\nsimilar to the reason other techniques work. I would like to see more exploration of this. \nSecond, are the models below the line the only models using spectral norm? I can't tell.\nOverall, it's hard for me to envision this work really seriously changing the course of research on GANs,\nbut that's perhaps too high a bar for poster acceptance.\n\nFor these reasons, I am giving a score of 6.\n\nDETAILED COMMENTS ON TEXT:\n\n> their performance heavily depends on the loss functions used in training.\nThis is not true, IMO. See [1]\n\n\n> may discourage the learning of data structures\nWhat does 'data structures' mean in this case?\nIt has another more common usage that makes this confusing.\n\n> Several loss functions have been proposed\nIMO this list doesn't belong in the main body of the text.\nI would move it to an appendix.\n\n> We assume linear activation is used at the last layer of D\nI'm not sure what this means?\nMy best guess is just that you're saying there is no activation function applied to the logits.\n\n> Arjovsky et al. (2017) showed that, if the supports of PX and PG do not overlap, there exists a perfect discriminator...\nThis doesn't affect your paper that much, but was this really something that needed to be shown?\nIf the discriminator has finite capacity it's not true in general and if it has infinite capacity its vacuous.\n\n\n> We propose a generalized power iteration method...\nWhy do this when we can explicitly compute the singular values as in [2]?\nGenuine question.\n\n> MS-SSIM is not compatible with CIFAR-10 and STL-10 which have data from many classes;\nJust compute the intra-class MS-SSIM as in [3].\n\n> Higher IS and lower FID scores indicate better image quality\nI'm a bit worried about using the FID to evaluate a model that's been trained w/ an MMD loss where \nthe discriminator is itself a neural network w/ roughly the same architecture as the pre-trained image classifier\nused to compute the FID. What can you say about this?\nAm I wrong to be worried?\n\n> Table 1: \nWhich models use spectral norm?\nMy understanding is that this has a big influence on the scores.\nThis seems like a very important point.\n\n\n\nREFERENCES:\n\n[1] Are GANs Created Equal? A Large-Scale Study\n[2] The Singular Values of Convolutional Layers\n[3] Conditional Image Synthesis With Auxiliary Classifier GANs", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}