{"title": "these aren't adversarial examples?", "review": "This paper describes a black-box attack on text classification systems by replacing some of the characters with homoglyphs (similar looking characters). The proposed approach is simple: important words are identified through MCTS, and one random character in each of these important words is modified to create an \"adversarial\" input. Homoglyphs have been an important issue in computer security, and it's great to see its first(?) application to NLP. That said, I have major concerns about the paper which prevent me from recommending its acceptance.\n\ncomments:\n- The paper has serious presentation issues. It contains a lot of irrelevant and repeated information, and several important sections of the paper (e.g., explanations about the datasets and models) are only provided in the appendix. \n\n- My main criticism of this paper is that the proposed method is really only \"adversarial\" to one particular NLP pipeline. As the authors mention, the perturbed words will likely be certainly be converted to UNK tokens during preprocessing. If important words are converted to UNKs, the downstream classification task is meaningless, so we'd expect a poor result (e.g., \"this is a good movie\" ---> \"this is a <unk> movie\"). I'm not sure how you can call this an adversarial example, as to the neural network the label of the input could have actually *changed* as a result of the perturbation.  It seems that we can easily fix this issue by using character-level models (or word representations generated by a contextualized embedding such as ELMo). If we can't, then the authors need to provide experiments that show character-level models are also fooled by homoglyphs. In sum, I don't know what this paper adds to the existing body of work on adversarial example generation in NLP (much of which is not cited in the paper).  \n\n- The algorithm used to find important words assumes access to the probability distribution produced by the black box system, but this may not be true for many (most?) commercial systems.\n\n- More details about the MCTS would have been nice. Could you have simply tried all possible perturbations for a given input? If not, how many forward passes on average does the network need to converge to a good policy? \n\n- Is the greedy baseline using homoglyph perturbations? It seems to just be doing random character replacements....", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}