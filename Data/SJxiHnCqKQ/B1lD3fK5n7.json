{"title": "Interesting ideas but I would have liked to see more analyses and discussion", "review": "This paper makes two main contributions:\n- homoglyph attacks in adversarial examples (changing a character into another character of similar visual shape),\n- the use of Monte Carlo Tree Search (MTCS) for determining the \"important\" words to perturb (the ones that are more likely to change a classifier prediction).\n\nClarity:\nThe paper has detailed background information and is overall clear. A few parts are hard to read though, including the description of the MTCS algorithm (the main contribution).\n\nOriginality:\nThis work is, as far as I can tell, original work. Both contributions are extensions of previous work (instead of 1: flipping random characters, 2: simpler greedy search algorithms).\n\nSignificance of contribution 1:\nHomoglyph attacks are well-known in security, e.g., making someone follow a malicious URL visually looking like a legitimate one. The authors note that this kind of attack can also be used for generating adversarial samples. It is good for the ML community to be aware of these attacks. \nThe authors draw a connection with adversarial images, in that they are also similar to natural images and providing compelling examples (replacing ascii characters by similar cyrillic characters in English). On second thought, I think the connection is partially misleading: it is hard to detect whether an image is adversarial (although there is some recent research on that). With text, it is possible to ensure all characters are in a given alphabet; other characters can be removed, or transliterated, or stripped of diacritics, .... The adversarial samples may fool humans, but may have a harder time fooling a defensive algorithm. Of course, attackers have the upper hand being able to forge more and more complex examples, at the expense of being less and less visually similar. The algorithm then becomes more similar to existing algorithms flipping random characters. This is a point that would have been worth discussing.\nAlso, the research community is now well-aware of adversarial examples and has been reacting. One response is the use of sub-word models (bytes, characters, ...), which are much more robust than word-based models, in that there are very few OOV items. The authors pointed out that re-training with adversarial examples does not make the model much more robust. This is clear for word-based models, less so for sub-word models. I would have liked to see this point at least mentioned.\n\nSignificance of contribution 2:\nWhen generating adversarial examples, attacks should be focused on \"important\" words, the ones that have the most influence on the classifier's prediction. The authors use MCTS to find the most important subsets of words, extending previous greedy strategies focusing on a singleword (Gao et al). The authors show that extending the search space in this way improves the rate of successful attacks over the SOTA. I think this is the more substantial contribution of this paper. But this comes at a price: using a more elaborate search strategy will increase the number of calls to the classifier. The paper does not discuss that trade-off; theoretical and/or experimental analyses would have been nice.\n\nOverall, the paper introduces two interesting ideas to the problem of black-box adversarial example generation for text classifiers. However, the paper does little to discuss trade-offs or potential downsides of the proposed approaches. I also feel the two ideas could be published in separate papers, leaving more space to move some of the interesting results from the appendices into the main articles.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}