{"title": "Interesting approach to hierarchical RL", "review": "This paper proposes a method to train high- and low-level controllers to tackle hierarchical RL.  The novelty is framing hierarchical RL as a problem of training a diverse set of LL controllers such that they can be used by a HL controller to solve high-level tasks.  By dividing state representation into proprioceptive and task-specific, the reward used to train LL and HL controllers are simplified.  Experimental result shows that the method is effective at solving maze environments for both ant and humanoid.\n\nThe good parts:\n- The method of training diversified LL controller and a single high-level controller seems to work unreasonably well.  And one benefit of this approach is that the rewards for both high (sparse) and low (difference) level controllers can be trivially defined.\n\n- The separation of proprioceptive and task-specific states seems to be gaining popularity.  For the maze environment (and any task that involves locomotion), this can be done intuitively.\n\nPlace to improve:\n- Terrain-Adaptive Locomotion (Peng et al. 2016) used a similar approach of phase-indexed motion, as well as selecting from a mixture of experts to generate action sequence for the next cycle.  Perhaps worthwhile to cite.\n\n- In fact, it seems that phase in this work only benefited training of low-level controller for humanoid.  But it should be possible to train humanoid locomotion with using phase information.\n\n- This hierarchical approach shouldn't depend on the selection of state space.  What would happen when LL and HL controllers all receive the same inputs?\n\n- The paper is difficult to follow at places.  Ex. b_phi element of R^d in Section 3.3.  I'm still not sure what is b_phi, and what is d here.\n\n- The choice of K = 10 feels arbitrary.  Since K corresponds to the length of a cycle, it should make sense to choose K such that the period is reasonable compared to average human stride period, etc.  What is the simulation step length?\n\n- Since LL policies control the style of the motion and the only reward it gets is to keep moving, presumably the resulting motion would look unnatural or exhibit excessive energy consumption.  Does \"keep moving\" reward work with other common rewards like energy penalty, etc?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}