{"title": "A long paper with incomplete experiments", "review": "The paper fuses Capsule Networks with Graph Neural Networks. The idea seems technically correct and is well-written. With 13 pages the paper seems really long. Moreover, the experimental part seems to be too short. So, the theoretical and experimental part is not well-balanced.\n\nMinor concerns/ notes to the authors:\n1.\tPage 1: The abbreviation GNN is used before it is defined.\n2.\tPage 2: I guess there is a mistake in your indices. Capital N == n or?\n3.\tPage 4: What is \\mathbf{I}? I guess you mean the identity matrix.\n4.\tPage 4: Could you define/describe C_all?\n5.\tPage 5: Can you describe how you perform the coordinate addition or add a reference?\n6.\tPage 6: The idea to use reconstruction as regularization method is not new. May you can add a respective reference?\n7.\tPage 8: The abbreviations in your result tables are confusing. They are not aligned with the text. For example, what is Caps-CNN for a model?\n\nMy major concern is about your experimental evaluation. Under a first look the result tables looking great. But that\u2019s due to fact, that you marked the two best values in bold type. To be more precise, the method WL is in the most cases better than your proposed method. This makes me wondering if there is a real improvement by your method. It would be easier to decide if you would present the training/inference times and the number of parameters. By having that, I could relate your results regarding an accuracy-complexity tradeoff.  Moreover, your t-SNE and attention visualizations are not convincing. As you may know, the output of a t-SNE strongly dependents on the chosen hyper-parameters like the perplexity, etc. You not mentioned the setting of these values. Additionally, it is hard to decide if your embeddings are good or not because you are not presenting a baseline or referencing a respective work. You are complaining that this is due to the space restrictions. But you have unlimited capacity in the appendix. So please provide some clarifying plots. Finally, I\u2019m also not convinced that your attention mechanism works as expected. It\u2019s again due to missing baseline results and/or a reference. If it\u2019s not possible to add one of them, you could perform an easy experiment where you freeze your fully-connected layers of the attention module to fixed values (maybe such that it performs just an averaging) and repeat your experiments. In case your attention module works as expected you should observe a real change in terms of accuracy and in your visualizations too.\nYou could also think about to publish your code or present further results/plots in a separate blog. \n\nUpdate:\nAccording to the revised version which addresses a lot of my concerns, I vote for marginally above acceptance threshold.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}