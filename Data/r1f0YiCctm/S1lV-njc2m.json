{"title": "The proposed approach has interesting formulation and good performance tradeoff while the main theorems are based on existing works.", "review": "This paper considers the compression of the model parameters in deep neural networks. The authors propose minimal random code learning (MIRACLE), which uses a random sample of weights and the variational framework interpreted by the bits-back argument. The authors introduce two theorems characterizing the properties of MIRACLE, and demonstrate its compression performance through the experiments.\n\nThe proposed approach is interesting and the performance on the benchmarks is good enough to demonstrate its effectiveness. However, since the two main theorems are based on the existing results by Harsha et al. (2010) and Chatterjee & Diaconis (2018), the main technical contribution of this paper is the sampling scheme in Algorithm 1.\n\nAlthough the authors compare the performance trade-offs of MIRACLE with that of the baseline methods quoted from source materials, isn't it possible or desirable to include other competitors or other results for the baseline methods? Are there any other methods, in particular, achieving low error rate (with high compression size)? Little is discussed on why the baseline results are only a few. \n\nminor comment: \n- Eq.(4) lacks p(D) in front of dD.    \n\nPros:\n- Interesting approach based-on the bits back argument\n- Good performance trade off demonstrated through experiments\nCons:\n- Only a few baseline results, in particular, at high compression size\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}