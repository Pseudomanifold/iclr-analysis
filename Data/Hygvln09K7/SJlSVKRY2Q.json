{"title": "Training slow and fast learners using different strategies is an interesting idea. ", "review": "[Summary:]\nThis paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce \u201cmiracle representations\u201d and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.\n\n[Strengths:]\nThe paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. \n\n[Weaknesses:]\n- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].\n- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. \n- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].\n\n[Specific comments:]\n- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. \n-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.\n-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.\n-The use of the term \u201cmiracle\u201d keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one \u201cmiracle X\u201d was defined and used as these are all essentially saying the same thing.\n\nReferences\n[1]https://arxiv.org/abs/1803.02999\n[2]https://arxiv.org/abs/1801.05558\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}