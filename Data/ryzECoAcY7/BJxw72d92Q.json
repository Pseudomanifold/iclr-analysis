{"title": "The technique is sound and demonstrated good performance on a range of RL tasks, however its significance is not fully demonstrated.  ", "review": "Pros:\n1. A nice idea combining universal MDP formulation and Hindsight experience replay for HRL that can deal with hierarchies with more than two levels of policies in continuous tasks.\n2. Good empirical results \n\nCons:\n1. One limitation of this work is that the goal set is known. What if the goals are unknown?\n\n2. The current domains seems relative simple comparing other existing papers on HRL, hence it is hard to tell the significance of the method.\n\n3. It Lacks thorough experimental analysis. Some comments are suggestions are provided here.\n---Since the proposed framework can deal with arbitrary level of hierarchies, it might be better to include include an experiment comparing the more than 2 subgoal layers. This will help understand whether there is any diminishing return by increasing the number of layers.\n\n---What kind of policy representations and hyperparameters of the training algorithm are used? Are they the same for different domains? Some critical details and some ablation test should be provided.\n\n---The paper can also be strengthened if some comparisons to other HRL methods can be included.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}