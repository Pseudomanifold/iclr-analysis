{"title": "Cool Idea, More Evidence Needed", "review": "This paper presents an attention mechanism that computes a weighted sum over not only single tokens but ngrams (phrases). Experiments on WMT14 show a slight advantage over token-based attention.\n\nThe model is elegant and presented very clearly. I really liked the motivation too. \n\nHaving said that, I am not sold on the claim that phrasal attention actually helps, for two reasons:\n\n1) The comparison to previous results is weak. There are more datasets, models, and hyperparameter settings that need to be tested.\n\n2) Phrasal attention essentially adds an additional convolution layer, i.e. it adds parameters and complexity to the proposed model over the baseline. This needs to be controlled by, for example, adding another transformer block to the baseline model. The question that such an experiment would answer is \"does phrasal attention help more than an extra transformer layer?\" In my view, it is a more interesting question than \"does phrasal attention help more than nothing?\"\n\nAlso related to concern (2), I think that the authors should check whether the relative improvement from phrasal attention grows/shrinks as a function of the encoder's depth. It could be that deep enough encoders (e.g. 10 layers) already contain some latent representation of phrases, and that this approach mainly benefits shallower architectures (e.g. 2 layers).\n\n===  MINOR POINTS ===\nIf I understand the math behind 3.1.2 correctly, you're first applying a 1x1 conv to K, and then an nx1 conv. Since there's no non-linearity in the middle, isn't this equivalent to the first method? The only difference seems to be that you're assuming the low-rank decomposition fo the bilinear term at a different point (and thus get a different number of parameters, unless d_k = d_q).\n\nHave you tried dividing by sqrt(d_k * n) in 3.1.1 too?\n\nWhile the overall model is well explained, I found 3.3 harder to parse.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}