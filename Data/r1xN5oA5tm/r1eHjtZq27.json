{"title": "This work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. ", "review": "Phrase-Based Attention\n\nPaper Summary:\n\nNeural translation attention computes latent alignments which pairs input/target positions. Phrase-based systems used to align pairs of spans (n-grams) rather than individual positions, this work explores neural architectures to align spans. It does so by compositing attention and convolution operations. It reports empirical results that compares n-gram to uni-gram attention.\n\nReview:\n\nThis paper reads well. It provides appropriate context. The equations are correct. It lacks a few references I mentioned below. The main weaknesses of the work lies in its motivation and in the \nempirical results.\n\nThis work motivation ignores an important aspect of neural MT: the vectors that attention compares (\u201cqueries\u201d and \u201ckeys\u201d) do not summarizes a single token/unigram. These vectors aggregate information across nearby positions (convolutional tokens, Ghering et al 2017), all previous tokens (recurrent models, Suskever et al 2014) or the whole source sentence (transformer, Vaswani et al 2017). Moreover multiple layers of attention are composed in modern decoders, comparing vectors which integrates information from both source and target. These vectors cannot be considered as the representation of a single unigram from the source or from the target.\n\nThe key-value convolution method 3.1.1 is not different from (Ghering et al 2017) which alternates computing convolution and attention multiple times. The query as kernel is a contribution of this work, it is highly related to the concurrent submission to ICLR on dynamic convolution \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d. This other work however reports better empirical results over the same benchmark.\n\nOn empirical results, it seems that the table does not include recent results from work on weighted transformer (Ahmed et al 2017) or relative attention (Shaw et al 2018). Also a 0.1 BLEU improvement over Vaswani et al seems brittle, is your result averaged over multiple runs, could the base transformer be better with as many parameters/updates as your model?\n\nReview Summary:\n\nThis work starts from the premise that current models attends to unigram representation, which is wrong (keys and values already depends on multiple source/target positions). The empirical results are missing recent improvements. The reported empirical advantage compared to baseline is thin. The most interesting contribution is the query as kernel approach: however the concurrent submission \u201cPay Less Attention with Lightweight and Dynamic Convolutions\u201d obtains better empirical results with a similar idea.\n\nMissing references:\n\nKarim Ahmed, Nitish Shirish Keskar, and Richard Socher. 2017. Weighted transformer network for machine translation. arxiv, 1711.02132.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}