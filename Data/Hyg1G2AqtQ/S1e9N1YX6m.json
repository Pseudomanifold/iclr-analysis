{"title": "Interesting premise, needs more clarity/comparisons", "review": "\nIntroduction: \n\u201cSince the state dynamics and rewards depend on the input process\u201d -> why do the rewards depend on the input process conditioned on the state? \n\nDoes the scenario being considered basically involve any scenario with stochastic dynamics? Or is the fact that the disturbances may come from a stateful process what makes this distinct?\n\nif the input sequence following the action -> vague, would help if this would just be written a bit more clearly. \n\nIs just the baseline input dependent or does the policy need to be input dependent as well? From later reading, this point is still quite confusing. One line says \u201cAt time t, the policy only depends only on (st, zt).\u201d. Another line says that the policy is pi_theta(a|s), with no mention of z. I\u2019m pretty confused by the consistency here. This is also important in the proof of Lemma 1, because P(a|s,z) = pi_theta(a|s). Please clarify this.\n\nSection 4:\n Is the IID version of Figure 3 basically the same as stochastic dynamics? (Case 2)\n\nSection 4.1\n\u201cIn input-driven MDPs, the standard input-agnostic baseline is ineffective at reducing variance\u201d -> can you give some more intuition/proof as to why. \n\nIn Lemma 2, how come the Q function is dependent on z, but the policy is only dependent on s (not even the current and past z\u2019s). \n\nI think the proof of theorem 1 should be included in the main paper rather than unnecessary details about policy gradient. \n\nTheorem 1 and theorem 2 are really some of the most important parts of the paper, and they deserve a more thorough discussion besides the 2 lines that are in there right now. \n\n\nAlgorithm 1 -> should it be eqn 4?\n\nThe meta-algorithm provided in Section 5 is well motivated and well described. An experimental result including what happens with LSTM baselines would be very helpful. \n\nOne question is whether it is actually possible to know what the z\u2019s are at different steps? In some cases these might be latent and hard to infer?\n\nCan you compare to Clavera et al 2018? It seems like it might be a relevant comparison. \n\nThe difference between MAML and the 10 value network seems quite marginal. Can the authors discuss why this is? And when we would expect to see a bigger difference. \n\nRelated work: Another relevant piece of work\nMeta-Learning Priors for Efficient Online Bayesian Regression\n\nMajor todos:\n1. Improve clarity of what z's are observed, which are not and whether the policy is dependent on these or not. \n2. Compare with other prior work such as Clavera et al, Harrison et al. \n3. Add more naive baselines such as training an LSTM, etc. \n4. Provide more analysis of the meta-learning component, how much does it actually help.\n\nOverall impression: \u2028I think this paper covers an interesting problem, and proposes a simple, straightforward approach conditioning the baseline and the critic on the input process. What bothers me in the current version of the paper is the lack of clarity about the observability of z, where it comes from and also some lack of comparisons with other prior methods. I think these would make the paper stronger.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}