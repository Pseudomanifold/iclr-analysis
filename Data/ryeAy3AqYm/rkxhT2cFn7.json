{"title": "This paper contains interesting experiments, but it seem that the proposed methods lacks understanding", "review": "This paper considers adversarial attack and its defense to DQN. Specifically, the authors propose a poisoning attack that is able to fool DQN, and also propose a modification of DQN that enables the use of strong defense. Experimental results are provided to justify the proposed approach.\n\nDetailed comments:\n\n1.  Although the attack approach seems easy to implement, it would be interesting to see why it works. It might make this paper better if the intuition of the UQP is provided. FGSM is a well-known attack for deep learning models. What is the intuition of using the sign of the gradient of the cross-entropy? Since the argmax is a one-hot vector, this cross-entropy seems ill-defined. How to compute the gradient?\n\n2. It would also be interesting to see why taking actions based on the student network enables better defense.  In DADQN, the authors seem to combine a few tricks proposed by existing works together. It might be better to highlight the contribution and novelty of this approach. ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}