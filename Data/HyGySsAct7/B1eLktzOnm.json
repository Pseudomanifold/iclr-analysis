{"title": "The paper is not well-positioned against the existing literature on black-box attack. Its empirical evaluation is somewhat sloppy.", "review": "PAPER SUMMARY:\n\nThis paper introduces a biologically motivated black-box attack algorithm. \nThe target model in this case is DNN applied to the ASR context (automatic speech recognition system). \n\nNOVELTY & SIGNIFICANCE:\n\nThe proposed approach extends the previous genetic approach of (Alzantot et al., 2018) to attack a more complicated ASR system (that handles phrases and sentences). The new contribution here is an add-on momentum mutation component on top of the existing genetic programming architecture of (Alzantot et al., 2018) as illustrated in Figure 3.\n\nThis however appears very incremental seeing that integrating the mutation component into existing system is straight-forward and that mutation is not even a new concept -- it has always been a vital component in genetic programming paradigm.\n\nIt is also unclear how this mutation component improves over the existing work (more on this in the sections below).\n\nAnother issue is this work seems to ignore the recent literature on adversarial black-box attacks to DNN model. To list a few:\n\nChen, P.-Y.; Zhang, H.; Sharma, Y.; Yi, J.; and Hsieh, C.-J. 2017b.\nZOO: Zeroth-order optimization-based  black-box attacks to deepneural networks without training substitute models. \nIn Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (15-26) ACM\n\nCheng,  M.;  Le,  T.;  Chen,  P.-Y.;  Yi,  J.;  Zhang,  H.;  and  Hsieh,C.-J.2018.\nQuery-efficient hard-label black-box attack:  An optimization-based approach. arXiv preprint arXiv:1807.04457\n\nWhile these works have not been used to attacking ASR system, they should be directly applicable to such system since after all, they are black-box attacks. I think the proposed method needs to be compared with these works.\n\nTECHNICAL SOUNDNESS:\n\nI find it surprising that even though the proposed method is claimed to be a black-box attack but in the end, it actually exploits the fact that the target model uses CTC decoder. This pertains specifically to the target model's internal architecture and a black-box attack is not supposed to know this.\n\nCLARITY:\n\nThe paper is clearly written.\n\nEMPIRICAL RESULTS:\n\nI do not understand this statement:\n\n\"That 35% of random attacks were successful in this respect highlights the fact that black box\nadversarial attacks are definitely possible and highly effective at the same time\"\n\nWhy is 35% successful attack rate a positive result? The result tends to suggest that this is an attack with low success rate. \n\nThe 2nd paragraph in 3.2 seems to give a vague explanation: \"the vast majority of failure cases are only a few edit distances away from the target. \n\nThis suggests that running the algorithm for a few more iterations could produce a higher success rate, although at the cost of correlation similarity\".\n\nGiven the above statement, I do not see why the authors didn't actually \"run the algorithm for a few more iterations\" to verify it ...\n\nI am also curious why is the success rate of the proposed method is significantly lower than that of the existing system -- I assume \"single word black box\" is the work of (Alzantot et al., 2018).\n\nI find the empirical evaluation somewhat sloppy: why are the tested method not compared on the same benchmark? How do we interpret the results then?\n\nREVIEW SUMMARY:\n\nThe paper misses the recent literature on black-box attack. The authors need to compare with those to demonstrate the efficiency of their proposed work. I also find the contribution of this paper too incremental & its empirical evaluation appears somewhat sloppy and not convincing (see my specific comments above). ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}