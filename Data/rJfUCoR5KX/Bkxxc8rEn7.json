{"title": "The authors made several claims and provide suggestions on training binary networks. However, the experiments are somewhat not sufficient to support the proposed hypothesis.", "review": "The authors made several claims and provide suggestions on training binary networks, however, they are not proved or theoretically analyzed.  The empirical verification of the proposed hypothesis was viewed as weak as the only two datasets used are small datasets MNIST and CIFAR-10, and the used network architectures are also limited. Much more rigorous and thorough testing is required for an empirical paper which proposes new claims. \n\nTake the first claim \"end-to-end training of binary networks crucially relies on the optimiser taking advantage of second moment gradient estimates\" as an example. As it is known that choice of optimizer is highly dependent on the specific dataset and network structure, it is not convincing to jump to this conclusion using the observations on two small datasets and limited network architectures.  E.g, many binarization papers use momentum for ImageNet dataset with residual networks. Does Adam also outperforms momentum in this case? Similarly, it is also hard for me to judge whether the other conclusions made about weight/gradient clipping, the momentum in batch normalization and learning rate, are correct or not.\n\nSome minor issues are:\n1. In Figure 4, different methods are not run to convergence, and the comparison may not be fair.\n2. The second paragraph in section 4: \"It can be seen that not clipping weights when learning rates are large can completely halt the optimisation (red curve in Figure 5).\" However, in figure 5, the red curve is \"Clipping gradients\", which one is correct?\n3. The authors propose a recipe for faster training of binary networks, is there experiments supporting that training networks with the proposed recipe is faster than the original counterpart? ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}