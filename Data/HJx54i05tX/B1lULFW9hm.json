{"title": "Through analysis on weight-tied autoencoders may benefit from more clear presentation", "review": "This work applies infinite width limit random network framework (a.k.a. Mean field analysis) to study deep autoencoders when weights are tied between encoder and decoder. Random network analysis allows to have exact analysis of asymptotic behaviour where the network is infinitely deep (but width taken to infinite first). This exact analysis allows to answer some theoretical questions from previous works to varying degrees of success. \n\nBuilding on the techniques from Poole et al (2016) [1], Schoenholz et al (2017) [2], the theoretical analysis to deep autoencoder with weight tied encoder/decoder shows interesting properties. The fact that the network component are split into encoder/decoder architecture choice along with weight tying shows various interesting phase of network configuration. \n\nMain concern with this work is applicability of the theoretical analysis to real networks. The autoencoding samples on MNIST provided in the Appendix at least visually do not seem to be a competitive autoencoder (e.g. blurry and irrelevant pixels showing up). \n\nAlso the empirical study with various schemes is little hard to parse and digest. It would be better to restructure this section so that the messages from theoretical analysis in the earlier section can be clearly seen in the experiments.\n\nThe experiments done on fixed learning rate should not be compare to other architectures in terms of training speed as learning rates are sensitive to the architecture choice and speed may be not directly comparable. \n\nQuestions/Comments\n- Without weight tying the whole study is not much different from just the feedforward networks. However, as noted by the authors Vincent et al (2010) showed that empirically autoencoders with or without weight tying performs comparably. What is the benefit of analyzing more complicated case where we do not get a clear benefit from? \n\n- Many auto encoding networks benefit from either bottleneck or varying the widths. The author\u2019s regime is when all of the hidden layers grows to infinity at the same order. Would this limit capture interesting properties of autoencoders?\n\n- When analysis is for weight tied networks, why is encoder and decoder assume to have different non-linearity? It does show interesting analysis but is it a practical choice? From this work, would you recommend using different non-linearities?\n\n- It would be interesting to see how this analysis is applied to Denoising Autoencoders [3], which should be straightforward to apply similar to dropout analysis appeared in Schoenholz et al [2].\n\n[1] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential\nexpressivity in deep neural networks through transient chaos. In Advances in neural information\nprocessing systems, pp. 3360\u20133368, 2016.\n[2] S.S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein. Deep information propagation. 5th International Conference on Learning Representations, 2017.\n[3] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}