{"title": "This paper proposes an regularizer to encourage soft label assignment during self-training. This is a cute idea and leads to notable improvement. The EM interpretation, however, needs more discussions.", "review": "Pros:\nThis paper points out an intrinsic issue in conventional self-training formulation and proposes a simple yet effective reformulation with regularization to resolve the problem. This kind of work should be appreciated.\n\nCons:\n1) The organization and clarity could be improved. \n1-1) When the authors first mentioned Eq. (2), it still leads to sparse (over-confidence) solutions---indeed, the same solutions from Eq. (1). Would it be better to move this modification to Section 4? Or at least clearly mention that this modification alone is not enough.\n1-2) Section 3.3 needs completely rewriting. The p=15% comes out nowhere. What is the meaning of U in UCBST?\n1-3) In Section 4, the authors should clearly state that the regularizer can be included in either step of learning, and then briefly overview the Section instead of directly jumping into Sec. 4.1 and 4.2. Besides, please make R_c(YX_T, Y_T, W) consistent. Some part in Eq. (8) misses X_T. \n1-4) In Eq. (11), does >= \\lambda_t lead to 0?\n \n2) The EM formulation needs more discussion. Specifically, isn't the very right term in the 3rd and 4th row a constant term (i.e. log 1 = 0)? If so, why bother using the variational method and optimize the lower bound? The current formulation seems artificial to me. \n\n3) As the paper mentioned the over-confidence problem, I would hope to see some qualitative studies instead of just qualitative ones---more specifically, the motivations of the paper need to be justified. Besides, I would suggest the authors expanding the experimental section with more analysis and sufficient descriptions about the tasks/datasets. In order to do so, the authors can actually move some contents of Section 4 to Appendix.\n\n4) What are the pros/cons of the 5 different forms of regularizations?\n\nOther comments:\n1) In Section 2, domain adaptation, the author mentioned: \"there exists an interesting ...\". Could the authors give more details as it is not clear to me?\n\n2) Could the authors provide more discussions about how noisy label learning relates to the proposed method? I saw some discussions in Section 4.1, but I would suggest having a section in Appendix about this.\n\n3) Could the authors provide an intuition about the meaning of \\lambda on page 4? Could the authors include the KKT derivation in the Appendix?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}