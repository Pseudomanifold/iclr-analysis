{"title": "Interesting approach but limited experiments and insights ", "review": "Personal Expertise:\nThe reviewer has extensive practical and theoretical experience with deep networks, different activation functions, as well as some practical experience using deep networks to model physical phenomena which are different inverse problems than the more common perception modeling. However, the reviewer is not knowledgeable in using ODEs for deep networks.\nContributions of the paper :\nThe contribution of the paper is in proposing a learnable activation function in form of an ODE which can help to better model highly oscillatory and irregular functions more efficiently. This can be potentially useful for special applications in inverse problems where (by field knowledge) we know highly non-linear and specific activation functions can be more reasonable than the common ReLU and its recent variants.\n\nQuality and composition:\nThe composition of the theoretical part of the paper is clear. The experimental part is very limited though and does not include all the necessary details. Many of the details in the initial sections can be taken to the appendices to make room for more empirical studies. \n\nNovelty, related works:\nThe work seems novel in proposing the specific activation function but in general there are many other works that propose learnable or more elaborate activation functions, neurons, or local parts of networks. It is not so clear how this work is different from those and nor is compared to those works. This includes networks in networks, maxout networks, capsule networks, etc. \n\nCritique of the theories and experiments:\n\nTheoretical Design:\nThe theoretical design and derivation of the paper seem correct, although the reviewer is not an expert on this topic. However, it does not clearly mention why the ODE is not designed and solved for each problem separately. Should there be different design choices for y for each task/dataset? Why not solving the ODE during the training as well? If we are solving the ODE only once and based on some initialization of coefficients, it seems to be equivalent to designing a learnable activation function such as leaky ReLU. In that regard, one could call leaky-ReLU a DifEN?\n\nExperimental Setup:\nThe motivation of the new activation function is for specific use-cases where oscillatory or decaying functions are to be modelled. In that respect, the experimental setup is quite limited and inconclusive.\n- MNIST experiment: to conclusively evaluate the performance of the proposed activation function, it is important to try fixed activation functions on the same architecture as the DifEN and vice versa. \n- Diabetes regression experiment: since the task is not a well-studied regression task, more experiments on various datasets are required to make a conclusion.\n- The learnable activation function can potentially make the network more prone to overfitting, this needs to be tested thoroughly.\n- An important application of the proposed activation function is mentioned to be for model compression. That should be properly tested on problems with large sets of parameters (such as ImageNet networks) and observe if the performance drop due to a decrease in the number of parameters in a standard network is sharper than that of a network with DifEN activations.\n- More tasks and more analysis should be performed for the real-world tasks that the authors mention as the motivation of this work (specifically medical diagnosis or predictions). The analysis should demonstrate and give insight on the extra generalization power that the new activation function brings to those problems.\n- Following on the previous point, it is important to empirically demonstrate for which applications DifEN is useful.\n- Some analyses are missing on when a neuron can accentuate the problem of vanishing and/or exploding gradients in certain configurations of the DifEN parameters. It seems like the situation can arise during the training where the activation function become too steep or too saturated.\n\nSummary judgment:\nAll in all, I think the proposed work has some potential in specific applications, however, the experimental setup does not give a clear and conclusive message of where and how the new activation functions are useful. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}