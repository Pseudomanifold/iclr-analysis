{"title": "Interesting use of denoising based on attractor dynamics in RNNs, but weak experimental validation.", "review": "The authors propose to embed in a recurrent neural network (RNN) a multistage subnetwork that is trained to denoise its own state. This is done with an additional denoising cost term that essentially encourages the recurrent subnetwork to suppress noise during as the recurrence is unfolded in time.\nThe authors first demonstrate the denoising properties of this architecture, and then demonstrate its performance on a series of tasks combining it with regular tanh and GRU recurrent units.\n\nThe paper is clear and the main idea is rather interesting, but the presented experimental validations are arguably weak. The demonstration of the denoising properties of the network is rather superficial, in the sense that it does not give much insight into the functioning of the architecture, despite that presumably being the main goal of the section. In particular, it is not clear where the non-monotonic change in denoising as a function of network size comes from. Based on the attractor neural network literature that the authors cite at the beginning of the paper, it could be due to either the presence of spurious attractors, the absence of fixed-point attractors or the fact that the attractor network is trained above capacity. But the authors never go into a detailed analysis that could reveal the detailed functioning of their architecture and merely mention the hypothesis that for large networks denoising performance would decrease because of overfitting.\nAs for the experiments that are presented in the rest of the paper, while relevant, the types of tasks and datasets on which the proposed architecture is being tested are rather small.\n\nHere are some more specific comments and questions:\n- It would help to clarify the training procedure to explicitly mention in step 3 that training proceeds on sequences with added noise.\n- It is not clear how many times in each experiments step 3 is being repeated for each mini-batch, i.e. what computational overhead is required for the training of the SDRNN compared to a regular RNN.  \n- It is not clear whether the recurrent neural net called RNN with attractors (RNN+A) is indeed an attractor neural network. Does the state of the network indeed always converge to an attractor?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}