{"title": "Interesting idea ", "review": "\n\n[Summary]\nThis paper proposes a Graph-Sequence-to-Sequence (GraphSeq2Seq) model to fuse the dependency graph among words into the traditional Seq2Seq framework.\n\n\n\n[clarity]\nThis paper is basically well written though there are several grammatical errors (I guess the authors can fix them).\nMotivation and goal are clear.\n\n\n[originality]\nSeveral previous methods have already tackled to integrate graph structures into seq2seq models.\nTherefore, from this perspective, this study is incremental rather than innovative.\nHowever, the core idea of the proposed method, that is, combining the word representation, sub-graph state, incoming and outgoing representations seems to be novel.\n\n\n\n[significance]\nThe experimental setting used in this paper is slightly out of the current main stream of NMT research.\nFor example, the current top-line NMT systems uses subword unit for input and output sentences, but this paper doesn\u2019t.\nMoreover, the experiments were performed only on the very small datasets, IWSLT-2014 and 2015, which have at most 153K training parallel sentences.\nTherefore, it is unclear whether the proposed method has essential effectiveness to improve the performance on the top-line NMT baselines.\n\nComparing on the small datasets, the proposed method seems to significantly improve the performance over current best results of NPMT+LM.\n\n\n\nOverall, I like the idea of utilizing sub-graphs for simplicity and saving the computational cost to encode a structural (grammatical or semantic) information.\nHowever, I really wonder if this type of technique really works well on the large training datasets...", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}