{"title": "Decent results but incomplete comparisons", "review": "The paper present interactive parallel exploration (IPE), a reinforcement learning method based on an ensemble of policies and a shared experience pool. Periodically, the highest-return achieving policy is selected, towards which the other policies are updated in a sense of some distance metric. IPE is applicable to any off-policy reinforcement learning algorithm. The experiments demonstrate some improvement over TD3 on four MuJoCo benchmark tasks.\n\nThe method is motivated heuristically, and and it provides some benefits in terms of sample efficiency and lower variance between training trials. However, it is hard to justify the increased algorithmic complexity and additional hyperparameters just based on the presented results. The paper motivates IPE as an add-on that can increase the performance of any off-policy RL algorithm. As such, I would like to see IPE being applied to other algorithms (e.g., SAC or DQN) as a proof of generalizability, and compared to other similar ensemble based algorithms (e.g., bootstrapped DQN).\n\nWhile the improvement in the sample complexity is quite marginal, what I find the most interesting is how IPE-TD3 reduces variance between training trials compared to vanilla TD3. Convergence to bad local optimum can be a big problem, and IPE could help mitigate it. I would suggest including environments where local optima can be a big problem, for example HumanoidStandup, or any sparse reward task. Also the paper does not include ablations, which, given the heuristic nature of the proposed method, seems important.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}