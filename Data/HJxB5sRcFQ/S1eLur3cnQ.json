{"title": "Interesting application of GANS; Applications on interesting datasets but weak comparison with baselines and quantitative analysis. ", "review": "\f\nSummary: This paper presents a novel GAN framework for generating graphic layouts which consists of a set of graphic elements which are geometrically and semantically related. The generator learns a function that maps input layout ( a random set of graphic elements denoted by their classes probabilities and and geometric parameters) and outputs the new contextually refined layout. The paper also explores two choices of discriminators: (1) relation based discriminator which directly extracts the relations among different graphic elements in the parameter space, and (2) wireframe rendering discriminator which maps graphic elements to 2D wireframe images using a differentiable layer followed by a CNN for learning the discriminator. The novel GAN framework is evaluated on several datasets such as MNIST, document layout comparison and clipart abstract scene generation  \n\nPros:\n- The paper is trying to solve an interesting problem of layout generation. While a large body of work has focussed on pixel generation, this paper focuses on graphic layouts which can have a wide range of practical applications. \n- The paper presents a novel architecture by proposing a generator that outputs a graphic layout consisting of class probabilities and polygon keypoints. They also propose a novel discriminator consisting of a differentiable layer that takes the parameters of the output layout and generates a rasterized image representing the wireframe. This is quite neat as it allows to utilize a CNN for learning a discriminator for real / fake prediction. \n- Qualitative results are shown on a wide variety of datasets - from MNIST to clipart scene generation and tangram graphic design generation. I found the clipart scene and tangram graphic design generation experiments quite neat. \n\nCons:\n- While the paper presents a few qualitative results, the paper is missing any form of quantitative or human evaluation on clip-art scene generation or tangram graphic design generation. \n- The paper also doesn\u2019t report results on simple baselines for generating graphic layouts. Why not have a simple regression based baseline for predicting polygon parameters? Or compare with the approach mentioned in [1]\n- Even for generating MNIST digits, the paper doesn\u2019t report numbers on previous methods used for MNIST digit generation. \nInterestingly, only figure 4 shows results from a traditional GAN approach (DCGAN). Why not show the output on other datasets too? \n\nQuestions / Remarks:\n- Why is the input to the GAN not the desired graphic elements and pose the problem as just predicting the polygon keypoints for those graphic elements. I didn\u2019t quite understand the motivation of choosing a random set of graphic elements and their class probabilities as input.  \n    - How does this work for the case of clip-art generation for example? The input to the gan is a list of all graphic elements (boy, girl glasses, hat, sun and tree) or a subset of these?\n    - It is also not clear what role the class probabilities are playing this formulation. \n- In section 3.3.2, it\u2019s mentioned that the target image consist of C channels assuming there are C semantic classes for each element. What do you mean by each graphic element having C semantic classes? Also in the formulation discusses in this section, there is no further mention of C. I wasn\u2019t quite clear what the purpose of C channels is then. \n- I found Figure 3b quite interesting - it would have been nice if you expanded on that experiments and the observations you made a little more. \n\n[1] Deep Convolutional Priors for Indoor Scene Synthesis by Wang et al\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}