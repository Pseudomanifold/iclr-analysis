{"title": "potentially interesting ideas; could be better justified and validated.", "review": "The paper addresses the problem of producing sensible (high) uncertainties on out of distribution (OOD) data along with accurate predictions on in-distribution data. The authors consider a model wherein the weights of the network (\\theta) are drawn from a matrix normal distribution whose parameters are in-turn a (non-linear; parameterized by a another network) function of the covariates (x). Instead of inferring a posterior over theta that then induces the predictive uncertainties, uncertainties here arise from a regularizer that penalizes the distribution over theta from deviating too far from a standard Normal. Experiments present results on toy data, MNIST/not MNIST as well as on adversarial perturbations of MNIST and CIFAR 10 datasets.\n\nThe paper is clearly written and addresses an important problem. The paper presents both an alternate model as well as an alternate objective function. While the authors do report some interesting results, they do a poor job of motivating the proposed extensions. It isn\u2019t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:\n\n1. The proposed model? Is using a conditional weight prior p(\\theta | x) (Eq 3) instead of p(\\theta) (as in BNNs)  necessary for the inflated uncertainties on OOD data? \n\n2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\\theta) =  MN(0, I, I) in the KL sense. Depending on \\lambda, the objective either closely approximates the marginal likelihood or not. It is unclear how important this particular objective is to the results.\n    -  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \\theta and induce predictive uncertainties.  Were they explored and found to be not effective? It would be nice to see how a \u201cgold standard\u201d HMC based inference does on at least the small toy problem of Sec 5.1? \n    - There is also a closely related variant of Eq 3 which we can arrive at by switching the log and the expectation in the first term of Eq 5 and applying Jensen\u2019s inequality \u2014> E_p(\\theta| x)[ln p(y | x, \\theta)] - KL (p(\\theta | x) || p(\\theta)). This would correspond to maximizing a valid lower bound to the marginal likelihood of a BNN model p(y | x, \\theta) p(\\theta), while interpreting p(\\theta | x) as an amortized variational approximation. This variant has the advantage that it provides a valid lower bound on the marginal likelihood, and exploits the well understood variational inference machinery. This also immediately suggests, that the variational approximation , p (\\theta | x)  should probably depend on both x and y rather than only on x and the flexibility of the hyper networks g would govern how well the true posterior over weights \\theta can be approximated. \nComparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.\n\n3.  Or simply to a well tuned \\lambda, chosen on a per dataset basis? From the text it appears that \\lambda is manually selected to trade off accuracy against uncertainty on OOD data. In the real world, one would not have access to OOD data during training, how is one to pick \\lambda in such cases?\n\nDetailed comments about experiments:\n\na) The uncertainties produced by CDN in Figure 2 seems strange. Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data? \n\nb) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison. This forces the VI solution to tend to the MLE, sacrificing uncertainty in the variational distribution. It would be good to include comparisons against VI with \\lambda = 1. \n\n==========\n\nThere are potentially interesting ideas in this paper. However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}