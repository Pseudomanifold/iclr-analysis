{"title": "nice theoretical result about deep linear neural networks", "review": "This paper continues the recent line of study on the convergence behaviour of gradient descent for deep linear neural networks. For more than 2 layers, the optimization problem is nonconvex and it is known strict saddle points exist. The main contribution is a relaxation of the balancedness condition in previous work by Arora et al and a new deficiency margin condition, which together allowed the authors to prove that gradient descent will converge to an epsilon solution in at most O(log 1/epsilon) iterations (under reasonable assumptions on step size and other parameters). Examples on how to satisfy the two conditions are discussed. Overall, the obtained results appear to be a solid contribution beyond our current understanding of deep linear neural networks, and potentially may be helpful towards our understanding of deep nonlinear neural networks.\n\nThis paper is very well-written. The authors gave an elegant short proof for the gradient flow case and spent efforts in proving the discretized version as well. The discussion of related works seems to be appropriate and thorough. \n\nOne thing I would love to see more discussions about is the deficiency margin assumption. I know the authors provided some argument about its necessity in the appendix, but is it possible that under the deficiency margin assumption, the nonconvex optimization problem is really \"trivial\" hence the linear convergence of gradient descent? For instance, can one prove that on this level set there still could be some (strict) saddle-point? And what if Phi is rank-deficient?\n\nIn the paragraph proceeding Section 3.2, the authors mentioned that \"overly small standard deviation will render high magnitude for the deficiency margin, and therefore fast convergence improbable.\" Is there a typo here? Do you mean  a smaller deficiency margin? If not, can you please provide more details?\n\nLastly, it would be great if the authors could complement the theoretical results with some numerical experiments, especially to test the initialization strategies in Section 3.3.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}