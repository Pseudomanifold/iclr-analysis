{"title": "Review", "review": "In this paper, the author proposed an efficient surrogate loss for estimating  Hessian in the setting of Meta-reinforcement learning (Finn.et al, 2017), which significantly reduce the variance while introducing small bias. The author verified their proposed method with other meta-learning algorithms on the Mujoco benchmarks. The author also compared with unbiased higher order gradient estimation method-DiCE in terms of gradient variance and average return. \n\nThe work is essentially important due to the need for second-order gradient estimation for meta-learning (Finn et al., 2017) and other related work such as multi-agent RL. The results look promising and the method is easy to implement. I have two detail questions about the experiment:\n\n1) As the author states, the new proposed method introduces bias while reducing variance significantly. It is necessary to examine the MSE, Bias, Variance of the gradient estimatorsquantitatively  for the proposed and related baseline methods (including MAML, E-MAML-TRPO, LVC-VPG, etc). If the bias is not a big issue empirically, the proposed method is good to use in practice.\n\n2)  The author should add DiCE in the benchmark in section 7.1, which will verify its advantage over DiCE thoroughly.\n\nOverall this is a good paper and I vote for acceptance.\n\n\nFinn, Chelsea, et al. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017.\n\nFoerster, Jakob, et al. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" ICML 2018.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}