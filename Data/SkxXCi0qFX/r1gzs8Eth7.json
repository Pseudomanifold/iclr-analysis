{"title": "Strong paper, Strong accept", "review": "The paper first examines the objective function optimized in MAML and E-MAML and interprets the terms as different credit assignment criteria. MAML takes into account the dependences between pre-update trajectory and pre-update policy, post-update trajectory and post-update policy by forcing the gradient of the two policies to be aligned, which results in better learning properties. \nThought better, the paper points out MAML has incorrect estimation for the hessian in the objective. To address that, the paper propose a low variance curvature estimator (LVC). However, naively solving the new objective with LVC with TRPO is computationally prohibitive. The paper addresses this problem by proposing an objective function that combines PPO and a slightly modified version of LVC.\n\nQuality: strong, clarity:strong, originality:strong, significance: strong,\n\nPros:\n- The paper provides strong theoretical results. Though mathematically intense, the paper is written quite well and is easy to follow.\n- The proposed method is able to improve in sample complexity, speed and convergence over past methods.\n- The paper provides strong empirical results over MAML, E-MAML. They also show the effective of the LVC objective by comparing LVC over E-MAML using vanilla gradient update.\n- Figure 4 is particularly interesting. The results show different exploration patterns used by different method and is quite aligned with the theory.  \nCons:\n- It would be nice to add more comparison and analysis on the variance. Since LVC is claimed to reduce variance of the gradient, it would be nice to show more empirical evidences that supports this. (By looking at Figure 2, although not directly related, LVC-VPG seems to have pretty noisy behaviour)\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}