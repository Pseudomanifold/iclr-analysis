{"title": "Review", "review": "Strength: \n\nThe proposed approach is architecture-independent: the attack is constructed only from the dataset.\n\nWeaknesses: \n\nPaper is not sufficiently well written for a venue like ICLR. \nAttack has very low success rate.\nTo the exception of Figures 4 and 5, many experiments are conducted on MNIST.\n\nFeedback: \n\nExperimental results show that the attack is able to degrade a classifier\u2019s performance by inserting perturbations that are computed on the data only. However, there are no baselines included to compare adversarial evasion rates achieved here to prior efforts. This makes it difficult to justify the fairly low success rate. In your rebuttal, could you clarify why baselines were not used to offer comparison points in the experiments?\n\nFurthermore, strong conclusions are made from the results despite the lack of supporting evidence. For instance, on P10, the attack is said to \u201calso explains why adversarial examples can be universal.\u201d. However, not only does the attack achieve less success than universal adversarial examples would (so it cannot explain all of them) but also does it not share any characteristics with the way universal adversarial examples are crafted. Drawing such a strong conclusion thus most likely needs a lot more supporting evidence. \n\nSeveral directions would improve the content of the paper: \n\n* Complete existing experimental results by being more systematic. For instance, in Section 3.1, measurements are only performed on one pair of MNIST images. Without studying a significant portion of the test set of two datasets, it is very difficult to draw any conclusions from the limited evidence.\n* Perform a human study to have all perturbed images labeled again. Indeed, because of the ways images are perturbed here, it is unclear how much perturbation can be added without changing the label of the resulting input. \n* Study how existing adversarial example techniques modify internal representations. This would help support conclusions made (e.g., about universal perturbations---see above). \n* Rewrite the related work section to scope it better: for instance, Sabour et al. in Adversarial Manipulation of Deep Representations and Wicker et al. in Feature-Guided Black-Box Safety Testing of Deep Neural Networks explore adversaries operating in the feature space. This will also help build better baselines for the evaluation.\n\nAdditional details: \n\nTLDR: typo in the first word\nP1: The following definition of adversarial examples is a bit restrictive, because they are not necessarily limited to vision applications (e.g., they could be found for text or speech as well). \u201cAdversarial examples are modified samples that preserve original image structures\u201d \nP1: The following statement is a bit vague (what is obvious impact referring to?): \u201cExperiments show that simply by imitating distributions from a training set without any knowledge of the classifier can still lead to obvious impacts on classification results from deep networks.\u201d\nP1: References do not typeset properly (the parentheses are missing: perhaps, the \\citep{} command was not used?)\nP2: What is the motivation for including references to prior work in the realm of image segmentation and more generally-speaking multi-camera settings in the related work section? \nP2: Typo in \u201c linear vibration\u201d\nP2: It remains difficult to make a conclusion about humans being robust to the perturbations introduced by adversarial examples. For instance, Elsayed et al. at NIPS 2018 found that time-constrained humans were also misled by adversarial examples crafted to evade ML classifiers: see Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.\nP2: Prior work suggests that the following conclusion is not entirely true: \u201cMost of these kinds of examples are generated by carefully designed algorithms and procedures. This complexity to some extent shows that adversarial examples may only occupy a small percentage for total image space we can imagine with pixel representations.\u201d For instance, Tramer et al. in ICLR 2018 found that adversarial subspaces were often large: \u201cEnsemble Adversarial Training: Attacks and Defenses\u201d.\nP2: Others have looked at internal representations of adversarial examples so the following statement would be best softened: \u201cTo the best of our knowledge, this paper should be the first one that discusses adversarial examples from the internal knowledge representation point of view.\u201d. See for instance, Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning by Papernot and McDaniel.\nP3: Could you add pointers to support the description of human abstraction and sparsity? It reads a bit speculative as is, and adding some pointers would help relate the arguments made to relevant pointers for readers that are less familiar with this topic. \nP3: What is the motivation for including the discussion of computations performed by a neural network layer-by-layer in Section 2?\nP4: Given that saliency maps can be manipulated easily and are only applicable locally, it appears that Figure 1 is too limited to serve as sufficient evidence for the following conclusion: \u201cThis, in some way, proves the point that the knowledge storage and representation of current neural networks are not exactly sparse prototype based.\u201d\nP5: The error rate reported on MNIST is quite low (45%). Even using the Fast Gradient Method, one should be able to have the error rate be as high as 90% on a standard CNN.\nP7: Would you be able to provide references to backup the following statement? \u201cThis is a network structure that is very common.\u201d\nP10: How does the discussion in Section 4.2 relate to the attack described in the submission?\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}