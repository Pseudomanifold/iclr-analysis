{"title": "T(v_a, v_b,.)-addition is an improvement?", "review": "The paper deals with further development of RAND-WALK model of Arora et al. There are stable idioms, adjective-noun pairs and etc that are not covered by RAND-WALK, because sometimes words from seemingly different contexts can join to form a stable idiom. \n\nSo, the idea of paper is to introduce a tensor T and a stable idiom (a,b) is embedded into v_{ab}=v_a+v_b+T(v_a, v_b,.) and is emitted with some probability p_sym (proportional to exp(v_{ab} times context)). The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated. Finally, there exists an expression, PMI3(u,v,w), that shows the correlation between 3 words, and that can be estimated from the data directly. It is proved that Tucker decomposition of that tensor gives us all words embeddings together with tensor T. Thus, from the latter we will obtain a tool for finding embeddings of idioms (i.e. v_a+v_b+T(v_a, v_b,.)).\n\nTheoretical analysis seems correct (I have not checked all the statements thoroughly, but I would expect formulations to be true). The only problem I see is that phrase similarity part is not convincing. I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}