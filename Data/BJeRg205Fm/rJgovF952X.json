{"title": "No novelty, conceptually problematic, and exceeding the page limit", "review": "The paper shows how to model the outputs of neural networks via likelihoods other than commonly used ones. The likelihoods discussed include Beta, Dirichlet and Dirichlet-Multinomial. The paper introduces the gradient computation of these likelihoods and test them in several datasets. \n\nThis paper lacks novelty and has conceptual mistakes. It is a common practice, in Bayesian learning, to model different types of data with different likelihoods. The examples discussed in this paper are very basis and the gradient computation is standard. I do not see anything new. And the authors misunderstand that if you involve some likelihood in training, you can quantify the uncertainty. It is wrong. Uncertainty should be estimated in the posterior inference framework --- you need to integrate the posterior distribution of the (latent) random variables into the test likelihood to obtain the predictive distribution, from which you can identify the confidence levels. That\u2019s why auto-encoding variational Bayes framework is useful and popular.  \nWhat the paper is doing is still the point estimation. \n\nBesides, the paper exceeds the 8-page limit for the content. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}