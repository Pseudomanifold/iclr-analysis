{"title": "Novel and interesting idea, but significant algorithmic and empirical concerns", "review": "Two high-level points about my review before going into the details:\n1. This paper was a thoroughly enjoyable and insightful read. Kudos to the authors for attempting such a comprehensive overview of likelihood-based vs. likelihood-free learning.\n2. I\u2019ll be more than happy to revise my current rating if my concerns are addressed by the authors.\n\nWith regards to the technical assessment of this work, the idea of using a nearest neighbors objective for learning a generative model is both intriguing and appealing. What makes this work even more interesting are its connections with maximum likelihood estimation. Novelty aside, I believe there are major theoretical, algorithmic, and empirical concerns in the current work which I discuss below:\n\nTheorem 1 \n- The third condition is true for location-scale family of distributions e.g., Gaussian. But the distribution learned by a generative model p_theta is far from Gaussian or other location-scale distributions. \n- More importantly, I don\u2019t think the upper bound is tight in practice because the likelihoods can vary significantly across the dataset. Take MNIST for example. Compare the log-likelihoods of an autoregressive model or ELBOs of a VAE across the different classes of digits. Straight digits (like 1s) have much higher log-likelihoods on average than curved digits. \n\nAlgorithm\n- While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it\u2019s hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation.\n- Similarly, I was a bit disappointed by the choice of Euclidean distance in a pixel space as the choice of distance metric. The argument that you do not want to use \u201cauxiliary sources of labelled data or leverage domain-specific prior knowledge\u201d is indeed necessary for fair comparisons, but also points to a limitation of the current approach.\n\nEmpirical evaluation \n- Seems too outdated both in terms of baselines and metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches.\n- While it is arguably well-established that Parzen window estimates are misleading (Theis et al.), that\u2019s the only quantitative estimate in this work (Table 1). Hard to think of any recent published work (last 1-2 years) in generative modeling that even reports these estimates.\n- The baselines in Table 1 are all from 2013-15. Clearly, much has happened in the last 3 years that merit the inclusion of more recent baselines.\n-  Even for sample quality, there has been a lot of research in designing and improving metrics. E.g., Inception scores, Frechet Inception Distance, Kernel Inception Distance. I am not looking for state-of-the-art numbers, showing heavily zoomed out samples without any of these metrics is slightly disingenuous.\n- As mentioned before, reporting the computation time/per iteration and number of iterations for convergence for the proposed algorithm in comparison with other approaches  is important.\n- Similarly, the argument about the method avoiding even the other GAN problems (e.g., vanishing gradients, stability in training) can and should be supported by empirical evidence.\n\nAnalysis and discussion\n- One family of generative models that is crucially missing from this work is normalizing flow models. \n- This is somewhat debatable, but I do not agree that the tradeoff between likelihoods and sample quality is due to model capacity. As far as I can tell, the cited work of Grover et al., 2017 provides evidence contrary to what the authors claim. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. So model capacity isn't necessarily the key differentiating factor (which is same for both training algorithms in their experiments), it's more about the choice of the objective function and the optimization procedure.\n\nMinor points for improving presentation:\n- Section 3 can be made more concise and to the point. I\u2019d be especially interested if the precision and recall discussion in this section and elsewhere can be formalized.\n- Use numbered lists instead of bullets for assumptions in Theorem 1, so that the discussion of the assumptions right after the theorem statement are easy to follow.\n- The citation of Grover et al. seems outdated? The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models.\n- In general, avoid making somewhat hard assertions that are speculative. Some of them I\u2019ve highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}