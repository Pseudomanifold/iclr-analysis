{"title": "Nice idea. Need better experiments.", "review": "Privacy concerns arise when data is shared with third parties, a common occurrence. This paper proposes a privacy-preserving classification framework that consists of an encoder that extracts features from data, a classifier that performs the actual classification, and a decoder that tries to reconstruct the original data. In a mobile computing setting, the encoder is deployed at the client side and the classification is performed on the server side which accesses only the output features of the encoder. The adversarial training process guarantees good accuracy of the classifier while there is no decoder being able to reconstruct the original input sample accurately. Experimental results are provided to confirm the usefulness of the algorithm.\n\nThe problem of privacy-preserving learning is an important topic and the paper proposes an interesting framework for that. However, I think it needs to provide more solid evaluations of the proposed algorithm, and presentation also need to be improved a bit.\n\nDetailed comments:\nI don\u2019t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.\nThe decoder used to measure privacy is very important. Can you provide more detail about the decoders used in all the four cases? If possible, evaluating the privacy with different decoders may provide a stronger evidence for the proposed method.\nIt seems that DNN(resized) is a generalization of DNN. If so, by changing the magnitude of noise and projection dimensions for PCA should give a DNN(resized) result (in Figure 3) that is close to DNN. If the two NNs used in DNN and DNN(resized) are different, I believe it\u2019s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.\nThe abstract mentioned that the proposed algorithm works as an \u201cimplicit regularization leading to better classification accuracy than the original model which completely ignores privacy\u201d. But I don\u2019t see clearly from the experimental results how the accuracy compares to a non-private classifier.\nSection 2.2 mentioned how different kind of layers would help with the encoder\u2019s utility and privacy. It would be better to back up the argument with some experiments.\nI think it needs to be made clearer how reconstruction error works as a measure of privacy. For example, an image which is totally unreadable for human eye might still leak sensitive information when fed into a machine learning model. \nIn term of reference, it\u2019s better to cite more articles with different kind of privacy attacks for how raw data can cause privacy risks. For the \u201cNoisy Data\u201d method, it\u2019s better to cite more articles on differential privacy and local differential privacy.\nSome figures, like Figure 3 and 4, are hard to read. The author may consider making the figures larger (maybe with a 2 by 2 layout), adjusting the position of the legend & scale of x-axis for Figure 3, and using markers with different colors for Figure 4. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}