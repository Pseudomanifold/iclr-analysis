{"title": "Good idea but far from a proper publication", "review": "TargetProp\n\nThis paper addresses the problem of training neural networks with the sign activation function. A recent method for training such non-differentiable networks is target propagation: starting at the last layer, a target (-1 or +1) is assigned to each neuron in the layer; then, for each neuron in the layer, a separate optimization problem is solved, where the weights into that neuron are updated to achieve the target value. This procedure is iterated until convergence, as is typical for regular networks. Within the target propagation algorithm, the target assignment problem asks: how do we assign the targets at layer i, given fixed targets and weights at layer i+1? The FTPROP algorithm solves this problem by simply using sign of the corresponding gradient. Alternatively, this paper attempts to assign targets by solving a combinatorial problem. The authors propose a stochastic local search method which leverages gradient information for initialization and improvement steps, but is essentially combinatorial. Experimentally, the proposed algorithm, GRLS, is sometimes competitive with the original FTPROP, and is substantially better than the pure gradient approximation method that uses the straight-through estimator.\n\nOverall, I do like the paper and the general approach. However, I think the technical contribution is thin at the moment, and there is no dicussion or comparison with a number of methods from multiple papers. I look forward to discussing my concerns with the authors during the rebuttal period. However, I strongly believe that the authors should spend some time improving the method before submitting to the next major conference. I am confident they will have a strong paper if they do so.\n\nStrengths:\n- Clarity: a well-written paper, easy to read and clear w.r.t. the limitations of the proposed method.\n- Approach: I really like the combinatorial angle on this problem, and strongly believe this is the way forward for discrete neural nets.\n\nWeaknesses:\n- Algorithm: GRLS, in its current form, is quite basic. The Stochastic Local Search (SLS) literature (e.g. [1]) is quite rich and deep. Your algorithm can be seen as a first try, but it is really far from being a powerful, reliable algorithm for your problem. I do appreciate your analysis of the assignment rule in FTPROP, and how it is a very reasonable one. However, a proper combinatorial method should do better given a sufficient amount of time.\n- Related work: references [2-10] herein are all relevant to your work at different degrees. Overall, the FTPROP paper does not discuss or compare to any of these, which is really shocking. I urge the authors to implement some or all of these methods, and compare fairly against them. Even if your modified target assignment were to strictly improve over FTPROP, this would only be meaningful if the general target propagation procedure is actually better than [2-10] (or the most relevant subset).\n- Scalability: I realize that this is a huge challenge, but it is important to address it or at least show potential techniques for speeding up the algorithm. Please refer to classical SLS work [1] or other papers and try to get some guidance for the next iteration of your paper.\n\nGood luck!\n\n[1] Hoos, Holger H., and Thomas St\u00fctzle. Stochastic local search: Foundations and applications. Elsevier, 2004.\n[2] Stochastic local search for direct training of threshold networks\n[3] Training Neural Nets with the Reactive Tabu Search\n[4] Using random weights to train multilayer networks of hard-limiting units\n[5] Can threshold networks be trained directly?\n[6] The geometrical learning of binary neural networks\n[7] An iterative method for training multilayer networks with threshold functions\n[8] Backpropagation Learning for Systems with Discrete-Valued Functions\n[9] Training Multilayer Networks with Discrete Activation Functions\n[10] A Max-Sum algorithm for training discrete neural networks", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}