{"title": "Reasonable approach, but lacks novelty and better evaluation", "review": "This paper presented a dialog response generation method using adversarial learning framework. \nThe generator is based on previously proposed hierarchical recurrent encoder-decoder network (HRED), and the discriminator is a bidirectional RNN. \nNoise samples are introduced in generator for response generation.\nThey evaluated their approach on two datasets and showed mostly better results than the other systems. \n\nThe novelty of the paper is limited. \nModeling longer dialog history (beyond the current turn) is not new, this has been used in different tasks such as dialect act classification, intent classification and slot filling, response generation, etc.\nThe generator is based on previous HRED. \nAdding noise to generate responses is somewhat new, but that doesn\u2019t seem to be well motivated or justified. \nWhy adding Gaussian noise improves the diversity or informativeness of the responses is not explained. \nThe idea of discriminator has been widely used recently for language generation related tasks.  What is new here? Is it the word-based metric? Sharing the context and word information with generator?  It would be helpful if the authors can clarify their contribution. \n  \nRegarding using MLE to first generate multiple hypotheses in generator, how is the quality of the n-best responses? \nIs there a way to measure the goodness of the responses in some kind of reranking framework, not necessarily discriminator? \n\nThe results in the table showed the proposed method outperforms the others in terms of those objective metrics. I feel some subjective evaluations are needed to strengthen the paper.\nFrom the samples responses in the table, it doesn\u2019t look like the new method generates very good responses. \n\n\nDetailed comments: \n- Sec 2, before 2.1, last paragraph, \u201cWith the GAN objective, we can match the noise distribution, P(Z_i) to the distribution of the ground truth response, P(X_i+1|X_i).  This needs clarification. \n- Figure 1: caption, \u201cleft\u201d and \u201cright\u201d are misplaced. \n- sec 2.1, last paragraph, without Z_i, the net could still learn a mapping from X_i to Y_i, but would produce deterministic outputs.  I think the authors mean that the system generates a probability distribution P(Y_i|X), the output is the most likely one from that. However, if the output is a distribution, the system can also do some sampling and not necessarily output the top one.  This is not that different from adding noise in the history \u2014 if that\u2019s based on some distribution, then it may still be deterministic. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}