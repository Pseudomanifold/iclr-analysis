{"title": "Reasonable approach but somewhat incremental; weak evaluation setup", "review": "\nThe paper applies conditional GAN to the HRED model [Serban et al., 2016] for dialogue response generation, showing improvements in terms of informativeness and diversity compared to HRED and VHRED [Serban et al., 2017].\n\nThe paper is technically solid and relatively easy to follow and the results are good, but comparisons with previous work (descriptive and experimental) are rather weak. \n\n- Related work is incomplete: The paper specifically argues for the use of GAN to improve diversity in dialogue response generation, but this is not the first paper to do so. [Xu et al., 2017] presents a GAN-like setup that targets exactly the same goal, but that work is not cited in the paper. Same for [Zhang et al., 2018], but the latter work is rather recent (it still should probably be cited).\n\n- Evaluation: There is no evaluation against Xu et al., which targets the same goal. The authors didn\u2019t even compare their methods against baselines used in other GAN works for diverse response generation (e.g., MMI [Xu et al.; Zhang et al.], Li et al.\u2019s GAN approach [Xu et al.]), which makes it difficult to draw comparisons between these related methods. As opposed to these other works, the paper doesn\u2019t offer any human evaluation.\n\n- It would have been nice to include an LSTM or GRU baseline, as these models are still often used in practice and the VHRED paper suggests [Serban et al., 2016; Table 1] that LSTM holds up quite well against HRED (if we extrapolate the results of VHRED vs. LSTM and VHRED vs. HRED). The ablation of GAN and HRED would help us understand which of the two is more important.\n\nIn sum, the work is relatively solid, but considering how much has already been done on generating diverse responses (including 3 other papers also using GAN), I don\u2019t think this paper is too influential. Its main weakness is the evaluation (particularly the lack of human evaluation.)\n\nMinor comments:\n\n- Introduction: \u201cdiversity promoting training objective but their model is for single turn conversations\u201d. \nIf these were \u201csingle turns\u201d, they wouldn\u2019t really be called conversations; that objective has been used with 3+ turn conversations. It can actually be applied to multi-turn dialogue as with any autoegressive generative models. For example, it has been exploited that way as a baseline for multi-turn dialogue [Li et al. 2016](\u201cDeep Reinforcement Learning for Dialogue Generation\u201c). Note it is not a \u201ctraining objective\u201d, but only an objective function at inference time, which is a more valid reason to criticize that paper.\n\n- \u201cWe use greedy decoding (MLE) on the first part of the objective.\u201d Doesn\u2019t that hurt diversity because of MLE? what about using sampling instead (maybe with temperature)?\n\n- Algorithm 1: the P_theta_G don\u2019t seem to match the text of section 2. h_i is in sometimes written in bold and sometimes not (see also Eq 12 for comparison.)\n\n- End of section 2.1: There are multiple Li et al.; specify which one.\n\n- End of section 2.2 and 2.4: extra closing parenthesis after N(0, \u2026))\n\n- Figures are too small to read the subscripts.\n\n[Xu et al. 2017]: Zhen Xu, Bingquan Liu, Baoxun Wang, Sun Chengjie, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural response generation via gan with an approximate embedding layer. EMNLP 2017.\n\n[Zhang et al. 2018]: Zhang, Yizhe & Galley, Michel & Gao, Jianfeng & Gan, Zhe & Li, Xiujun & Brockett, Chris & Dolan, Bill. (2018). Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}