{"title": "limited improvements", "review": "This paper proposes to use subsampling to reduce the computation cost of BN, which buys around 20% of the computational cost. \n- In normal BN, the gradient is propagated through the normalization factor as well, how would that change in the case of subsampled BN?\n- The minimum amount of gains makes it less appealing considering the potential complexity of implementing the algorithm.\n- Did the author compared against cuDNN\u2019s native version of BN? Because random sampling is involved, this will result in less regular patterns of computation, this could likely make the implementation of BN to be less efficient.\nIn summary, the method proposed in the paper is reasonable but could be limited in practice due to only 20% maximum gain can be achieved.\n\nUpdate after the author response:\n\nThe author addressed some of the concerns raised in the review(Thanks for the detailed response), in particular, the comparison to cuDNN.  I still think the paper is still borderline but the results might be of interest to some of the ICLR audience.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}