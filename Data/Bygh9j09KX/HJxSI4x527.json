{"title": "Interesting paper, purely empirical and no novelty", "review": "The paper is well written and easy to follow. It was a nice read for me.\n\nThe paper studies the CNNs like AlexNet, VGG, GoogleNet, ResNet50 and shows that these models are heavily biased towards the texture when trained on ImageNet. The paper shows human evaluations and compares model accuracies when various transformations like cue hypothesis, texture hypothesis (terms coined in the paper) are applied to study texture vs shape importance. The paper shows various results on different models clearly and results are easily interpretable. The paper then proposed a new ImageNet dataset which is called Stylized-ImageNet (SIN) where the texture is replaced with randomly selected painting style.\n\nI believe that this is a good empirical study which is needed to understand why the ImageNet features are good (supervised training) and this can inform research in self-supervision, few shot learning domains.\n\nThe paper is an empirical paper and is presenting a quantitive study of role of texture which others have already presented like Gatys et al. 2017. The paper itself has no novel contributions. The paper notes \"novel Stylized-ImageNet dataset\" and shows that models can learn shape/texture features both but there is not much detail/explanation on why \"Stylized\" is the novel approach and also the methodology of constructing data by replacing with painting from AdaIN style transfer (Huang & Belongie, 2017) is not discussed/explored. More specifically, there is no ablation on other ways this dataset could have been constructed and why style transfer was picked as the choice, why was AdaIN chosen. While the choice is valid, I think these questions need to be answered if we have to consider it \"novel\". Additionally, I would like answers to the following questions:\n\n1. In Figure 4, ResNet50 results are missing. I would be very interested in seeing those results. Can authors show those results?\n2. Did authors study deeper networks like RN101/152 and do the observations about texture still hold?\n3. Did authors consider inspecting if the models have same texture biases when trained on other datasets like COCO? If yes, can you share your results?\n4. In Figure 5, can authors also show the results of training VGG, AlexNet, GoogleNet models on SIN dataset? I believe otherwise the results are incomplete since Fig. 4 shows the biases of these models on IN dataset but doesn't show if these biases are removed by training on SIN.\n5. In Section 3.3, Transfer learning, authors show improvement on VOC 2007 Faster R-CNN . Do authors have explanation on why this gain happens? how's the texture learning in pretext task (like image classification training on SIN dataset) tied to the transfer learning no different dataset?\n6. What are the results of transfer learning on other datasets like COCO, Faster R-CNN?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}