{"title": "To address the sparse reward problems, the authors propose a relabeling strategy called Competitive Experience Reply (CER).  This strategy relabels states, and places learning in the context of an exploration competition between a pair of agents.  The experiments support some parts of authors\u2019 claim well.  However, the experiments are insufficient. ", "review": "The authors propose a states relabeling strategy (CER) to encourage exploration in RL algorithms by organizing a competitive game between a pair of agents. \nTo verify their strategy, they extend MADDPG as their framework. Then, they compare the performance of agents trained with HER, and both variants of CER, and both variants of CER with HER. The experiments show that CER can improve the performance of HER with faster converge and higher accuracy.\n\nMy major concerns are as follows.\n1.\tThe authors may want to conduct more experiments to compare CER with other state-of-the-art methods such as PPO[1]. As illustrated in Figure 1, the performance of HER is better than that of CER. The authors may want to analyze whether CER strategy alone could properly address the sparse reward problems, and why CER strategy can improve HER. The authors have mentioned that CER is \u201corthogonal\u201d to HER. I suggest authors provide more discussions on this statement. \n2.\tThe authors may want to improve the readability of this paper. \nFor example, in Figure 1, the authors may want to clarify the meanings of the axes and the plots. \nThe results shown in Figure 3 are confusing. How can the authors come to the conclusion that the optimal con\ufb01guration requires balancing the batch sizes used for the two agents? \nTo better illustrate the framework of CER, the authors may want to show its flow chart.\n3.\tThere are some typos. For example, in Section 2.1, the authors use T(s\u2019|s,a) without index t; in Section 2.2, the authors use both Q(a,s,g) and Q(s,a,g). \nThere is something wrong with the format of the reference (\u201cTim Salimans and Richard Chen \u2026 demonstration/, 2018.\u201d) in the bottom of page 10.\n\n[1] Schulman J, Wolski F, Dhariwal P, et al. Proximal Policy Optimization Algorithms[J]. 2017.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}