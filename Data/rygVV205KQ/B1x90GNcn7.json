{"title": "Potentially practical improvement of sparse-reward RL using IL, but a bit unclear when it helps", "review": "The submission describes a sort of hybrid between reinforcement learning and imitation learning, where an auxiliary imitation learning objective helps to guide the RL policy given expert demonstrations.  The method consists of concurrently maximizing an RL objective--augmented with the GAIL discriminator as a reward\u2014and minimizing the GAIL objective, which optimizes the discriminator between expert and policy-generated states.  Only expert states (not actions) are required, which allows the method to work given only videos of the expert demonstrations.  Experiments show that adding the visual imitation learning component allows RL to work with sparse rewards for complex tasks, in situations where RL without the imitation learning component fails.\n\nPros:\n+ It is an interesting result that adding a weak visual imitation loss dramatically improves RL with sparse rewards \n+ The idea of a visual imitation signal is well-motivated and could be used to solve practical problems\n+ The method enables an \u2018early termination\u2019 heuristic based on the imitation loss, which seems like a nice heuristic to speed up RL in practice\n\nCons:\n+ It seems possible that imitation only helps RL where imitation alone works pretty well already\n+ Some contributions are a bit muddled: e.g., the \u201clearning with no task reward\u201d section is a little confusing, because it seems to describe what is essentially a variant of normal GAIL\n+ The presentation borders on hand-wavy at parts and may benefit from a clean, formal description\n\nThe submission tackles a real, well-motivated problem that would appeal to many in the ICLR community.  The setting is attractive because expert demonstrations are available for many problems, so it seems obvious that they should be leveraged to solve RL problems\u2014especially the hardest problems, which feature very sparse reward signals.  It is an interesting observation that an imitation loss can be used as  a dense reward signal to supplement the sparse RL reward.  The experimental results also seem very promising, as the imitation loss seems to mean the difference between sparse-reward RL completely failing and succeeding.  Some architectural / feature selection details developed here seem to also be a meaningful contribution, as these factors also seem to determine the success or failure of the method.\n\nMy biggest doubt about the method is whether it really only works where imitation learning works pretty well already.  If we don\u2019t have enough expert examples for imitation learning to work, or if the expert is not optimizing the given reward function, then it is possible that adding the imitation loss is detrimental, because it induces an undesirable bias.  If, on the other hand, we do have enough training examples for imitation learning to succeed and the expert is optimizing the given reward function, then perhaps we should just do imitation learning instead of RL.  So, it is possible that there is some sweet spot where this method makes sense, but the extent of that sweet spot is unclear to me.\n\nThe experiments are unclear on this issue for a few reasons.  First, figure 4 is confusing, as it is titled \u2018comparison to standard GAIL', which makes it sound like a comparison to standard imitation learning.  However, I believe this figure is actually showing the performance of different variants of GAIL used as a subroutine in the hybrid RL-IL method.  I would like to know how much reward vanilla GAIL (without sparse rewards) achieves in this setting.  Second, figure 8 seems to confirm that some variant of vanilla imitation learning (without sparse rewards) actually does work most of the time, achieving results that are as good as some variants of the hybrid RL-IL method.  I think it would be useful to know, essentially, how much gain the hybrid method achieves over vanilla IL in different situations.\n\nAnother disappointing aspect of the paper is the \u2018learning with no task reward\u2019 section, which is a bit confusing.  The concept seems reasonable at a first glance, except that once we replace the sparse task reward with another discriminator, aren\u2019t we firmly back in the imitation learning setting again?  So, the motivation for this section just seems a bit unclear to me.  This seems to be describing a variant of GAIL with D4PG for the outer optimization instead of TRPO, which seems like a tangent from the main idea of the paper.  I don\u2019t think it is necessarily a bad idea to have another discriminator for the goal, but this part seems somewhat out of place.\n\nOn presentation: I think the presentation is a bit overly hand-wavy in parts.  I think the manuscript could benefit from having a concise, formal description.  Currently, the paper feels like a series of disjoint equations with unclear connections among them.  The paper is still intelligible, but not without knowing a lot of context relating to RL/IL methods that are trendy right now.  I feel that this is an unfortunate trend recently that should be corrected.  Also, I\u2019m not sure it is really necessary to invoke \u201cGAIL\u201d to describe the IL component, since the discriminator is in fact linear, and the entropy component is dropped.  I think \u201capprenticeship learning\u201d may be a more apt analogy.\n\nOn originality: as far as I can tell, the main idea of the work is novel.  The work consists mainly of combining existing methods (D4PG, GAIL) in a novel way.  However, some minor novel variations of GAIL are also proposed, as well as novel architectural considerations.\n\nOverall, this is a nice idea applied to a well-motivated problem with promising results, although the exact regime in which the method succeeds could be better characterized.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}