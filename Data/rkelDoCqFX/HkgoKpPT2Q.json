{"title": "Novel model on zero-shot VQA, however, a lot of details is not clear in the paper. ", "review": "[Summary]\nThis paper study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. The authors tackle the problem in two steps, 1) learning a task conditional visual classifier based on unsupervised task discovery, and 2) transferring and adapting the task conditional visual classifier to visual question answering model. With these two steps, the VQA model is formulated as p(a|v(I,q), eta_(q)) where v(I,q) is the visual feature with attention and eta_(q) is the task specification vector. During the pre-training stage, the embedding of task description can be obtained from two sources, word embedding of the name of the answer set, or caption with <blank> which replace the answers which encoded by a GRU. The authors verify the proposed method by re-construct the VQA 2.0 dataset, which the out-of-vocabulary answer appears during pre-training. The experiment shows that the proposed method is better than the baseline methods. \n\n[Strength]\n1. The proposed model is the first work on zero-shot VQA model which can handle out-of-the-vocabulary answers. \n\n2. Experiment results show the proposed method is effective on the proposed splits. \n\n[Weakness]\n1. There are several parts of the paper is super unclear, can the authors answer my following questions? \n\n- What is the model for each module proposed in the paper? There is no model figure or description at all, which make the proposed method is hard to understand or replicate the results. \n\n- Page 4, weakly supervised task regression, what is the new indirect loss, and how to calculate it, could the authors explain more? \n\n- Page 5, wordnet, how to select which node of the WordNet hierarchy to use as the task specification name t_s? \n\n- Page 6, visual description, how to get the visual description? coco caption annotation or generated by some image captioning model? \n\n- Page 6, how to combine these two linguistic knowledge source? \n\n2. Page 5, footnote mention that the ambiguity of the reference is usually resolved by attention model. what if the attention model attends the wrong parts? \n\n3. Given the WordNet answer set W_ts, how to select the answer from this set? will this differ from the visual description?\n\n4. It seems the dataset split is delicately constructed for the proposed method, will the proposed method apply on normal VQA split? \n\n5. What is the performance with standard VQA 2.0 split, such as train on train and test on val. the novel answers could be obtained by enlarging the VQA answer number. Although the novel answer maybe only just a small portion of the test set, it will be worth checking the performance on this split and compare with other methods. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}