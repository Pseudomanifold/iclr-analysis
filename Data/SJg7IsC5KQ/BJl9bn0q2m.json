{"title": "Interesting theoretical analysis for batch normalization ", "review": "This paper provides a theoretical analysis for batch normalization with gradient descent (GDBN) under a simplified scenario, i.e., solving an ordinary least squares problem. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem. Some practical experiments are carried out to justify their theoretical insights. The paper is in general easy to follow. \n\nPros:\nThis paper provides some insights for BN using the simplified model.\n1. It shows that the optimal convergence rate of BN can be faster than vanilla GD.\n\n2. It shows that GDBN doesn't diverge even if the learning rate for trainable parameters is very large. \n\nCons:\n1. In the main theorem, when the learning rate for the rescaling parameter is less than or equal to 1, the algorithm is only proved to converge to a stationary point for OLS problem rather a global optimal. \n\n2. To show convergence to the global optimal, the learning rate needs to be sufficiently small. But it is not specified how small it is. \n\nOverall, I think this paper provides some preliminary analysis for BN, which should shed some lights for understanding BN. However, the model under analysis is very simplified and the theoretical results are still preliminary.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}