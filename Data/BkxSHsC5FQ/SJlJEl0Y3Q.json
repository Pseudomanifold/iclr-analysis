{"title": "Interesting idea but limited comparisons", "review": "This paper presents a hybrid concept of deep neural network and support vector machine (SVM) for preventing catastrophic forgetting. The authors consider the last layer and the softmax function as SVM, and obtain support vectors, which are used as important samples of the old dataset. Merging the support vector data and new data, the network can keep the knowledge on the previous task. The use of support vector concept is interesting, but this paper has some issues to be improved.\n\nPros and Cons\n  (+) Interesting idea \n  (+) Diverse experimental results on six datasets including benchmark and real-world datasets\n  (-) Lack of related work on recent catastrophic forgetting\n  (-) Limited comparing results\n  (-) Limited analysis of feature regularizers\n \nDetailed comments\n- I am curious how we can assure that SVM's decision boundary is similar or same to NN's boundary\n- SupportNet is a method to use some of the previous data. For fair comparisons, SupportNet needs to be compared with other models using previous samples such as GEM [Lopez-Paz and Ranzato, 2017]. \n- Following papers are omitted in related work:\n  1. Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n  2. Shin et al. Continual Learning with Deep Generative Replay, NIPS 2017.\n   Also, the model needs to be compared with two models.\n- There is no result and analysis for feature regularizers. As the authors referred, the features of support vector data continuously change as the learning goes on. So, I am curious how the feature regularizer has effects on the performance.  This can be performed by visualizing the change of support vector features via t-SNE as the incremental learning proceeds   \n- The authors used 2000 support vectors for MNIST, Cifar-10, and Cifar-100. However, this size might be quite large considering their difficulty. \n- How is the pattern of EwC using some samples in the old dataset?\n- iCaRL was evaluated on ImageNet. Is there any reason not to be evaluated on ImageNet? \n- What kind of NNs is used for each dataset? And what kind of kernel is used for SVM?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}