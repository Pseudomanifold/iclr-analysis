{"title": "Good paper, well written and well motivated, good results, no-source code!", "review": "This interesting paper tackles the problem of joint source-channel coding, by means of learning.\n\nFrom 100kft heights, especially given the choice of VIMCO gradient estimates, this is effectively a \"let's embed a source-channel-decoder simulator and differentiate through it\", and find a solution that is better than source|channel factorized classic methods, or hand-tuned approaches.\n\nThe method and results are good. The authors also show some interesting results about the representations learned, about how decoded samples (images) change smoothly when the (discrete) embedding (the-codes) changes over deltas of hamming_d()=1bit. This is very good results IHMO. One limitation of this method is the fixed-code-length.\n\nJumping straight to my main main issue with this paper: no code was made available, at least not at this time.\n\nWhile the authors do provide an extensive appendix with hyper-parameter specs, usually in my experience when dealing with discrete / monte-carlo methods, it's usually rather hard to reproduce results. I really strongly advise the authors to provide fully reproducible code for this paper, to help further research on this topic.\n\nBesides that I have three technical comments / request regarding this paper:\n\n1// the choice of BSC channel - while this is the easiest most natural choice, and we should certainly have results on BSC, I am left wondering why the authors didn't try other more complex / more realistic channels? The authors only mention this as potential area of future research in the last sentence of the conclusions. \n\nThere are several reasons for this comment: first of all, it is well known that even classic joint source-channel coding methods do shine on complex channels, such fading/erasure channels and/or in general channels with correlated error sequences. Such channels are indeed key in modern wireless communications, and are easy to simulate. Given that more-complex channels could be introduced in the channel model p(y_hat|y) -  it would not change the rest of the method - it would be particularly interesting to see what results this method achieve in these more complex environments.\n\n2// I would like to hear more about the choice of VIMCO. Understood the authors statement to \"preserve the hard discreteness\" ~ that said methods like Gumbel-SM and several others also referenced in the paper ~ have been used  successfully to solve for propagating gradients through discrete units. This is where, in my opinion, experiments comparing VIMCO approximation results to at least one other method could allow to decide / validate the best architecture. \n\nThis is also because, in my previous experience, this type of networks with discrete units may be hard to train. I would like to hear from the authors about how stable the training was under different hyper-parameters, and perhaps see some convergence curves for the loss function(s).\n\n3// it's not 100% clear to me where the limitation of fixed code-length come into play from the architecture. Could the authors please point this out clearly?\n\nThank you!\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}