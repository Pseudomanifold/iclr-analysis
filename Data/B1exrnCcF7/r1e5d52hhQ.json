{"title": "Covariates factors are learned from voice and image data using CNNs. A logistic classifier is trained for cross-modal matching from covariates. ", "review": "Authors aim to reveal relevant dependencies between voice and image data (under a cross-modal matching framework) through common covariates (gender, ID, nationality). Each covariate is learned using a CNN from each provided domain (speak recordings and face images), then, a classifier is determined from a shared representation, which includes the CNN outputs from voice-based and image-based covariate estimations. The idea is interesting, and the paper ideas are clear to follow.\n\nPros:\n- New insights to support cross-modality matching from covariates.\n- Competitive results against state-of-the-art.\n-Convincing experiments.\n\nCons:\n-Fixing the output dimension to d (for both voice and image-based CNN outputs) could lead to unstable results. Indeed, the comparison of voice and face-based covariate estimates are not entirely fair due to the intrinsic dimensionality can vary for each domain. Alternatives as canonical correlation analysis can be coupled to joint properly both domains.\n- Table 4 - column ID results are not convincing (maybe are not clear for me).", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}