{"title": "Clear Rejection", "review": "[Summary]\nPU learning is the problem of learning a binary classifier given labelled data from the positive class and unlabelled data from both the classes. The authors propose a new  GAN architecture in this paper called the Divergent Gan (DGAN) which they claim has the benefits of two previous GAN architectures proposed for PU learning: The GenPU method and the Positive-Gan architecture. The key-equation of the paper is (5) which essentially adds an additional loss term to the GAN objective to encourage the generator to generate samples from the negative class and not from the positive class. The proposed method is validated through experiments on CIFAR and MNIST.\n\n[Pros]\n1. The problem of PU learning is interesting.\n2. The experimental results on CIFAR/MNIST suggest that some method that the authors coded worked at par with existing methods.\n\n[Cons]\n1. The quality of the writeup is quite bad and a large number of critical sentences are unclear. E.g.\na. [From Abstract] It keeps the light adversarial architecture of the PGAN method, with **a better robustness counter the varying images complexity**, while simultaneously allowing the same functionalities as the GenPU method, like the generation of relevant counter-examples.\nb. Equation (3) and (4) which are unclear in defining R_{PN}(D, \u03b4)\nc. Equation (6) which says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense.\nd. The distinction between the true data distribution and the distribution hallucinated by the the generator is not maintained in the paper. In key places the authors mix one with the other such as the statement that supp(Pp (Xp )) \u2229 supp(Pn (Xn )) \u2192 \u2205\nIn short even after a careful reading it is not clear exactly what is the method that the authors are proposing.\n\n2. Section 2.2 on noisy-label learning is only tangentially related to the paper and seems more like  a space filler.\n\n3. The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs worse than other PU learning methods which does not make sense. Because of this I am not quite convinced by the experiments.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}