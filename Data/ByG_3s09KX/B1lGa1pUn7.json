{"title": "A useful framework, but may not have enough research novelty", "review": "Review: This paper proposed \"Dopamine\", a new framework for DeepRL.  While this framework seems to be useful and the paper seems like a useful guide for using the framework, I didn't think that the paper had enough scientific novelty to be an ICLR paper.  I think that papers on novel frameworks can be suitable, but they should demonstrate that they're able to do something or provide a novel capability which has not been demonstrated before.  \n\nStrengths: \n\n-Having a standardized tool for keeping replay buffers seems useful.  \n\n-The Dopamine framework is written in Python and has 12 files, which means that it should be reasonably easy for users to understand how it's functioning and change things or debug.  \n\n-The paper has a little bit of analysis of how different settings effect results (such as how to terminate episodes) but I'm not sure that it does much to help us in understanding the framework.  I suppose it's useful to understand that the settings which are configurable in the framework affect results?  \n\n-The result on how sticky actions affect results is nice but I'm not sure what it adds over the Machado (2018) discussion.  \n\nWeaknesses: \n\n-Given that the paper is about documenting a new framework, it would have been nice to see more comprehensive baselines documented for different methods and settings.  \n\n-I don't understand the point of 2.1, in that it seems somewhat trivial that research has been done on different architectures and algorithms.  \n\n-In section 4.2, I wonder if the impact of training mode vs. evaluation mode would be larger if the model used a stochastic regularizer.  I suspect that in general changing to evaluation mode could have a significant impact.  \n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}