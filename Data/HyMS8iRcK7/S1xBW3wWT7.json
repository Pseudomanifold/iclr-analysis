{"title": "Differentiate from TARDIS and show more benchmark results.", "review": "\nSummary:\n\nThis paper introduces a new RNN architecture with external memory for sequence modeling. The proposed architecture (MARNN) is a simplification of TARDIS (Gulcehre et al., 2017). It uses the similar reader-writer tying mechanism, gates to control information flow from previous hidden state and memory. However, it has a simpler addressing mechanism. Authors show results in Character level PTB and a temporal action detection/proposal task.\n\nMajor comments:\n\nMARNN looks like a simplification of TARDIS architecture with Gumbel softmax. The major difference between the two architectures is the addressing mechanism.\n\n1.\tCan the authors clearly differentiate MARNN vs TARDIS?\n\n2.\tAuthors compare against TARDIS only in character level PTB which is actually a task which does not require very long term dependencies. It would be better if authors consider more tasks and directly compare against the TARDIS addressing mechanism to prove that the proposed addressing mechanism is indeed better.\n\n3.\tAuthors should consider more tasks, to show the efficiency of the proposed architecture.\n\n4.\tThe name of the model seems to be too generic. NTM, TARDIS, DNC with recurrent controller can be considered as memory augmented RNN. Please change the name.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}