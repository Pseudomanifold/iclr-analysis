{"title": "Bayesian view on deep network binarization - interesting idea but lacks in clarity and experiments", "review": "The paper treats network binarization as learning a bernoulli probability that each weight is 0 or 1. It uses a hierarchical approach to factor the probability in to parameters shared between weights/kernels/layers. The result is a network that can be used to sample a binary network. The main novelty is that full-precision weights are not needed during training. \n\n- There is a variance issue underlying this idea. The goal is to find a binary network that has high performance, but the probabilistic formulation could have a high variance in terms of the performance of the resulting binary networks. In the experiments, the variance is not shown, rather the authors sample 100 networks and pick the best one based on validation set accuracy? (they just say \"pick the best one\"). Should this be accounted for in the objective function? The prior has very high variance (p=0.5). The authors claim that this \"demonstrates the versatility\" -- how?\n- Related to the above, there is a high variance associated with he REINFORCE estimator. In the appendix, the authors use a baseline value to alleviate this, but no discussion is provided in the main text or the experiments.  \n- For the stochastic version of binary connect, the authors report the best out of 100 trials. I would rather like to see the mean and confidence intervals, for this as well as the proposed method.\n\n- Please provide a comparison of the number of hyper parameters used vs the number of binary network parameters. Is it feasible to store the \"master network\" in memory for small devices? It seems you need more parameters than the original network as you have weight specific parameters + kernel/filter/layers specific parameters? Is there any generalization between these hyperparameters that can be shown in experiments e.g. using a compact hierarchy?\n- More generally, how do you see this method being used in practice? Do you sample each binary network on the device? \n- How is this better than other methods of training binary nets, which have better accuracy than your approach (according to Table 2)?\n- In the experiments, the hierarchical structure used for hyperparameters is not clearly described. \n\n- I found the exposition in Section 3.2 to be very confusing using f(*)  whereas it is very simply described in words. The policy network is hierarchical upon layers/filters/kernels/weights. What is \"s\" in equations (1) etc.?\n- In Section 3.3, I found the connection to MDPs tenuous, whereas it is easy to understand that you are using REINFORCE to estimate the gradient of the expectation. \n- The pseudo-reward is completely ad-hoc. Since this is a \"1-step MDP\" (bandit problem?), the reward is maximized when the probability of w=1 is related to the sign of the gradient. In the end, we seem to have arrived at something that is basically similar to BinaryConnect (i.e. using the sign of the gradient). \n- Only experiments with an uniform prior are shown. Can the prior be used in some ways?\n\n- In the experiments, the activation functions used are not described. \n- Some numbers are missing in Table 3.\n- One advantage of TernaryConnect is that it allows sparsity by allowing zero weights, whereas binary nets only allow +/- 1 weights. That is, Ternary Connect should not be dismissed simply and the performance of Ternary Connect should be shown on all the datasets.\n- The error for VGG on CIFAR-10 is very high compared to SOTA (close to 94% accuracy). \n- The legend on Figure 3 is not readable, the font size can be increased. \n- Page 1 \"regardless of the availability ...\" - citation needed. \n- Page 2, experiments are not a separate contribution.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}