{"title": "Promising work from theoretical standpoint", "review": "This paper is about estimating mutual information in high dimensional settings. This is a very challenging open problem, that is of interest to a diverse set of research communities.\n\nIn this paper, it is theoretically argued that the recent proposed mutual information (lower bound) estimator, MINE, that is based on the Donsker-Varadhan representation of the corresponding KL divergence expression for mutual infortmation, is fundamentally flawed for high dimensions (of discrete variables).  It is further shown that lower bounds for joint entropy are hard to obtain due to exponential sample complexity. So, the authors suggest to instead obtain an upper bound for each entropy term in the mutual information expression; cross-entropy is the suggested upper bound for an entropy term.\n\nI have some basic questions as the following.\n\nSince the recent KL divergence based MI estimator, MINE, is inaccurate in high dimensions, there should be at least a discussion on connections between your estimator and the classic nearest neighbor distances based estimator of Kraskov et al. and the extensions (I suppose, even for discrete variables, one can compute distances to obtain nearest neighbors efficiently). Also, there are kernel functions based estimators.\n\nThere is no discussion in the paper about the errors accumulating from individual entropy terms in the mutual information expression. Kraskov et al. talk about this problem of accumulating errors in their seminal paper and propose not to compute the entropy terms individually. What you are proposing is in contrast to their clever observations.\n\nDoes the analysis on upper bound for entropy term also apply to the conditional entropy in the mutual information expression ? I think, there are more subtleties that should be explained.\n\nSince the proposed approach upper bounds entropy using cross entropy term (i.e. using some machine learning model like a neural network), it is even more important to show solid empirical evaluation, for synthetic as well as real world data.\n\nThere is a subtle difference between estimating mutual information and proposing an upper/lower bound for it. At present, it is not clear if the proposed upper bound of entropy would lead to an overall upper bound or lower bound for the mutual infortmation expression. The latter is important to know both in the context of optimization based on mutual information maximization (it should be lower bound in such case), as well analyzing mutual infortmation to under complex dynamics such as in brain.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}