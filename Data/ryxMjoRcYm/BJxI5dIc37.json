{"title": "somewhat unclear, possible modeling contributions", "review": "The paper considers the problem of solving a continuous-state MDP where the traces of the resulting policy satisfy some Linear Temporal Logic property. The authors use a version of finite-state machines called the Limit Deterministic B\u00fcchi automaton to represent the desired logic. The paper then defines a product MDP that extends the state space of the original MDP to incorporate those logic states in the automaton. Finally, to solve this extended MDP, the paper proposes to use neural fitted Q-iteration. Let n be the number of those logic states. They propose to use n neural networks to represent the value function, one Q-network for each logic state. \n\nI think the problem formulation is a little confusing. It is unclear whether there is a given reward function and the objective is to maximize the discounted cumulative reward, or the objective is to reach some predefined accepting states. It seems to be the latter, but the objective is not clearly stated in the paper. Also, it is unclear to me why the authors choose to define rewards as in equation (4) in their proposed algorithm. There is hardly any explanation on the parameters or justification for their choice. \n\nI think the main contribution of this paper is on the modeling side, that one may enlarge an MDP to incorporate certain logic information, which may be helpful in defining tasks.  There doesn't seem to be a significant contribution on the algorithmic side.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}