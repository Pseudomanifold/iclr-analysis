{"title": "Interesting paper but lacking both context and comprehensive analyses", "review": "Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove).\n\nClarity:\nThe paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither \"online\" nor \"offline\". The mathematical derivations are detailed, which is nice.\n\nOriginality:\nThe work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity.\n\nSignificance:\nI should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm.\nI like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to \"cosine similarity [...] is not backed by a solid theoretical foundation\".\nI am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}