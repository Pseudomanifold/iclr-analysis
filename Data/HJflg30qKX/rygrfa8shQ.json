{"title": "An insightful result", "review": "This paper analyzes the asymptotic convergence of GD for training deep linear network for classification using smooth monotone loss functions (e.g., the logistic loss). It is not a breakthrough, but indeed provides some useful insights.\n\nSome assumptions are very restricted: (1) Linear Activation; (2) Separable data. However, to the best of our knowledge, these are some necessary simplifications, given current technical limit and significant lack of theoretical understanding of neural networks.\n\nThe contribution of this paper contains multiple manifolds: For Deep Linear Network, GD tends to reduce the complexity:\n(1)\tConverge to Maximum Margin Solution;\n(2)\tTends to yield extremely simple models, even for every single weight matrix.\n(3)\tWell aligned means handle the redundancy.\n(4)\tExperimental results justify the implication of the proposed theory.\n\nThe authors use gradient flow analysis to provide intuition, but also present a discrete time analysis.\n\nThe only other drawbacks I could find are (1) The paper only analyze the asymptotic convergence; (2) The step size for discrete time analysis is a bit artificial. Given the difficulty of the problem, both are acceptable to me.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}