{"title": "The contribution looks rather incremental and is not motivated enough neither in theory nor in experiments.", "review": "The authors address the neural networks compression problem and propose to amend the previously known approximations based on different tensor decompositions (CP, Tucker, or tensor train) with a reshaping step, where a tensor is preliminary reshaped to a higher order tensor. However, I do not see why would such modification be beneficial compared to the previously known approximations; this is also not clarified by the experiments. \n\nThe paper contains detailed overview of the related literature. However, I think it would be more interesting to see a detailed elaboration of the proposed idea. Most importantly, the description of the proposed approach -- reshaped tensor decomposition -- looks rather superficial. In particular, I can not see where the improvement (compared with similar approximations without the reshaping) in memory and/or runtime would come from. Let me clarify with an example. Given a matrix A of size N x M and assume that this matrix is actually rank-1, i.e. there exist vectors a and b such that A = a\u2019b. Let's reshape this matrix into, say, an order-4 tensor T with dimensions n x m x k x p. Assume that this tensor T is also rank-1 and there exist vectors x1, x2, x3, x4 such that T is equal to their outer product. Since N+M = n+m+k+p, why would vectors x1, x2, x3, x4 need less memory than vectors a and b? Moreover, would such reshaping transformation preserve the structure of the original tensor, e.g., its low-rank representation? That is, if A is a rank-1 matrix, is the tensor T guaranteed to still be rank-1? Isn't it more difficult to factorize a tensor of higher order?\n\nI am also confused by the author's choice of the baseline for their experiments. First of all, wouldn't it be more informative to compare with the other known compression methods based on tensor decompositions (Lebedev, et al, 2015; Jaderberg, et al, 2014; Kim, et al, 2016)? Although indeed these approaches can be seen as particular cases of the proposed reshaped decomposition, such comparison would demonstrate whether this newer approach (RTD) is better than already existing methods. In particular, Kim et al experimentally demonstrate that their approximation, based on Tucker decomposition, leads to nearly lossless compression (they perform their experiments with publicly available pre-trained networks which shouldn't be difficult to compare to). Why the experimental results presented in this paper are so different? Does the loss of accuracy result from the compression with the Tucker decomposition or is it due to the reshaping step or are there other reasons? I am also not sure what does plain tensor decomposition stand for and why is it considered to be state-of-the-art (references?)? \n\nMinor comments: \nfigure 2: b is a particular case of a\nfigure 2: d - shouldn\u2019t the outer product of two 3-dimensional tensors result in a 6-dimensional tensor?\nmight want to use \\citep to add brackets around citations\ntypo: element-wisely -> element-wise", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}