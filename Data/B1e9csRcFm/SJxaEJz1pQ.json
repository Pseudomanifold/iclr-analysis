{"title": "The Importance of Norm Regularization in Linear Graph Embedding: Theoretical Analysis and Empirical Demonstration", "review": "In this paper, the authors proved that the generalization error of linear graph embedding methods is bounded by the norm of embedding vectors, rather than the dimensionality constraints. Interestingly, along with the analysis of Levy & Goldberg (2014), they found that linear graph embedding methods are probably computing a low-norm factorization of the PMI matrix. Correspondingly,  experimental results are provided to support their analysis.\nOverall, this work is theoretically complete and experimentally sufficient.\n\n1. it is unclear whether the embedding dimensions of all cases (with varying value of \\lambda_r) are fixed as a constant in Fig. 1 - Fig. 3.\n\n2. Figure 4 shows the impact of embedding dimension on the generalization performance. Are these results obtained after 50 SGD epochs? Comparing Fig.4 (a) with Fig.3 (a), we may infer that the results in Fig.3(a) when \\lambda_r = 0 are obtained by setting the embedded dimension as about 10^2. How about the generalization performance during SGD for \\lambda_r = 0 if the embedded dimension is set to be smaller than 10?\n\n3. In Claim 1, the degree d and the dimension D are mixed. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}