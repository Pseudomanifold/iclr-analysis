{"title": "Interesting new problem formulation, not carefully presented and evaluated.", "review": "The paper proposes a framework for training generative models that work on composed data. The models are trained in an adversarial fashion. The authors apply it to decompose foreground/background parts on MNIST images, and to perform sentence composition/decomposition.\n\nHigh level comments:\n* Clarity: In terms of language and writing style, the paper is written very clearly and easy to follow. In terms of presentation, there are some details that are omitted which would have made understanding easier and the work more reproducible.\n* Quality: The idea that is introduced seems intuitive and reasonable, but the experiments does not have enough details to prove that this method works (i.e. no quantitative results presented).  Moreover, the presentation of the method is not very well done (missing details), especially since the authors used the upper limit of 10 pages.\n* Originality: I am not familiar with the literature of generative models to judge this precisely, but according to the related work section it sounds like an original idea that is worth sharing.\n* Significance: I believe the idea of modeling data composition explicitly sounds intuitive and interesting, and it is worth sharing. However, the experimental section does not have enough evidence that it is actually possible to learn this, so it is not clear whether the contribution is significant.\n\nPros:\n-\tinteresting new problem formulation \n-\tsimple and clear language\n-\tthe theoretical analysis in the last section could be interesting more generally in the context of GANs\n-\tthe framework is applied on 2 different modalities: images and text.\n\nCons:\n-\thard to tell whether this approach works since the metrics for evaluation are not specified and there are no quantitative results in the experimental section (only 1 qualitative example per task)\n-\tthe work is not reproducible due to the lack of details (see more explanations below)\n-\tthe theoretical analysis is a standalone piece of the paper, without any discussion about the implications, or making connections to the previous sections.\n\nDetailed comments:\n1.\tI believe the weakest part of this paper is the evaluation section. The authors run their framework on 4 tasks of increasing difficulty. While the MNIST examples make for a nice and intuitive qualitative analysis, the are no quantitative results at all. The only result that is reported for each task is one qualitative picture. The authors make statements such as \u201cThe decomposition network learns to decompose the digits and backgrounds correctly\u201d , \u201cGiven one component, decomposition function and the other component can be learned.\u201d but there is not mention for how these conclusion are made (no metrics, no numbers). Indeed, it is difficult in general to quantify the results of generative models, but most other GAN papers introduce some sort metric that can be used to aggregate the evaluation on an entire dataset. If the authors manually inspected the results, they should at least report how many images they inspected and how many looked correct. \n2.\tAside from evaluation, there are some other details missing from the presentation. The individual details may not be major, but because all of these are missing together, it really affects the overall quality of the paper. For example:\n    \uf0a7\t the authors state: \u201cTo train discriminator(s), a regularization is applied. For brevity, we do not show the regularization term (see Petzka et al. (2017)) used in our experiments.\u201d. For reproducibility purposes, I believe it is important to at least mention the type of regularization, at least in the appendix. \n    \uf0a7\tThere is a parameter alpha used to balance the losses. What values was used in the experiments?\n    \uf0a7\tChoices of models are often not explained. Why did you choose that form for c(o1, o2) in section 3.3? Why DCGAN for component generators, and U-net for decomposition?\n    \uf0a7\tIt is not explained in detail how the Yelp-reviews dataset is altered to achieve coherence. The authors mention that \u201cAs we sample a pair independently, the input sentences are not generally coherent but the coherence can be achieved with a small number of changes.\u201d. However, the specific algorithm by which these changes are made is not specified, and thus it can\u2019t be reproduced.\n3.\tThe theoretical section is an interesting contribution, but the paper just states a list of theorems without making any connections to the applications used before, or a broader discussion about how these fit in the context of GANs more generally.\n4.\tMy understanding is that both datasets used are created by the authors by making alterations to MNIST and Yelp-reviews dataset, thus making them to some extent synthetic datasets suited to fit this problem formulation. I would have like to see how this composition/decomposition works on existing datasets with no alterations. Does it still work? \n5.\tIn section 2.3, in the coherent sentence experimental setting, I don\u2019t fully understand the design of the task. Figure 2 shows an example where composition and decomposition are not symmetric (i.e. composing then decomposing does not go back to the input sentences), although one of your losses is supposed to ensure exactly this cyclic consistency. Why not choose another problem that doesn\u2019t directly violate your assumptions?\n\nMinor issues: \n6.\tFrom the related work section, it is not clear how your approach is different from Azadi et al. (2018). Please include more details.\n7.\tIn section 2.4, you mention using Wasserstein GANs, with no further details about this model (not even a one line description). Without reading their paper, the readers of your paper could not easily follow through this section. The losses further introduced are also not explained intuitively (e.g. what do the two expectation terms in l_g_i represent?).\n8.\tI believe there are some errors in which tasks reference which figures in section 3.3. Should Task 2 refers to Figure 6, and Task 3 to Figure 7?\n9.\tWhat exactly is range(.) in section 4? If this refers to the interval of values that a variable can take, the saying \u201cis a matrix of size |range(Z)| \u00d7 |range(Y )|\u201d doesn\u2019t exactly make sense. Please define formally. \n\nFinal remarks and advice: \nOverall, I believe the paper introduces some interesting ideas. There is definitely value in the problem definition and theoretical analysis. However, I believe the paper needs more work on presentation and evaluation, especially since the authors opted for 10 pages and according to ICLR guidelines \u201cReviewers will be instructed to apply a higher standard to papers in excess of 8 pages.\u201d. Hopefully the above comments will help the authors improve this work!", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}