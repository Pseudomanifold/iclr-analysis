{"title": "Algorithm derivation is reasonable and interesting", "review": "Summary: \nThis paper proposes the method which improves the generalization performance of large-batch SGD by adding the diagonal Fisher matrix noise.\nIn the theoretical analysis, it is shown that gradient descent with the diagonal noise is faster than it with the full-matrix noise on positive-quadratic problems.\nMoreover, the effectiveness of the method is verified in several experiments.\n\nComments:\nThe idea of the proposed method is based on the following observations and assumptions:\n\n- Stochastic gradient methods with small-batch can be regarded as a gradient method with Fisher matrix noise.\n- The generalization ability is comparable between diagonal Fisher and full Fisher matrix.\n- Gradient method with diagonal Fisher is faster than that with full Fisher matrix.\nThis conjecture is theoretically validated for the case of quadratic problems.\n\nIn short, the algorithm derivation seems to be reasonable and the derived algorithm is executable.\nMoreover, experiments are well conducted and the results are also good.\n\n\nMinor comment:\n- There is a typo in the next line of Eq. (2):\n\\nabla_{M_L} (\\theta_k)} -> \\nabla_{M_L} L(\\theta_k)}\n\nIn addition, the notation \"l_i\" is not defined at this time.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}