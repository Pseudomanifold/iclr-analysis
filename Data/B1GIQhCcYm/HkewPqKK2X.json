{"title": "Good formulation, but not novel and short comparison", "review": "==== After rebuttal === \nI thank the authors for responses. I carefully read the response. But it is difficult to find a reason to increase the score. So, I keep my score. \n====================\n\nUnsupervised image-to-image (I2I) translation is an important issue due to various applications and it is still challenging when applied to diverse image data and data where domain gap is large. This paper employs a neural mutual information estimator (MINE) to deal with I2I translation between two domains where there is a large gap. However, this paper contains several issues.\n1. Pros. and Cons.\n   (+) Mathematical definition of I2I translation\n   (+) Application of mutual information for conserving content.\n   (-) Lack of comparison with recent I2I models\n   (-) Lack of experimental results and ablation studies \n   (-) Unclear novelty\n2. Major comments\n   - The novelty of this paper is not clear. Excluding the mathematical definition, it seems that the proposed TI simply combines DCGAN and MINE-based statistical networks. For clarifying the novelty, the detailed architecture and final objective functions can be helpful. \n   - Recent works on unsupervised I2I translation are omitted including UNIT [1], MUNIT [2], and DRIP [3]. Also, the authors need to clarify the main difference of TI-GAN from comparing models.\n   - It is not clear to relate the mathematical definition of domain transfer to one-to-many translation within large domain gap. \n   - It is not clear how to use mutual information (MINE) for learning. There is no explicit definition of loss function considering MINE term. \n   - It is short of comparing other state-of-the art models such as UNIT, MUNIT, DRIP, and AugCycleGAN. They compared their results with CycleGAN only.\n   - Experiments are not enough to support the authors\u2019 insist. There is not any quantitative metric or qualitative result on generating edge-to-shoes. \n   - It is difficult to read due to inconsistent usage of terms (e.g., Figure 3 and 4 (c)s)\n   - For better understanding, it requires to compare the patterns of MINE loss and adversarial loss. \n   - Experiments on more datasets such as animal, season, faces or USPS datasets. \n   - What is the main difference in the results between DCGAN-based and UNet-based models?\n\n\nMinor\n   - cicle_times symbol looks the product between distribution. But it should be defined before being used.\n   - A reference of CycleGAN is incorrectly cited. \n   - There are some typos in the paper.\n   - page 1: dependent \u2192 depend\n   - page 3: by separate \u2192 by separating\n   - page 6: S a I \u2192 S and I\n\n\n1. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks, CoRR, abs/1703.00848, 2017\n2. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, Multimodal Unsupervised Image-to-Image Translation, CoRR. abs/1804.04732\n3. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, Diverse Image-to-Image Translation via Disentangled Representations, ECCV 2018.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}