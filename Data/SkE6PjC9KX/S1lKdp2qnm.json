{"title": "The authors propose an extension to the recently established framework of Neural Processes by adding an attention-based conditioning mechanism which allows the model to better capture dependencies in its conditioning set. ", "review": "This paper is a joy to review, as it is clearly written and has a crisp idea that the authors try to motivate consistently.\nIt extends the framework of neural processes and conditional neural processes by an incremental seeming idea: self attention on the conditioning set and cross attention. What this means in practice is that the model is able to learn a more detailed and structured 'kernel' between query and past data which allows it to identify and model conditional structure better.\n\nThe authors try three main prongs of such attention mechanisms with the multi-head attention appearing to be the most successful one in the experiments.\n\nRegarding experiments, the authors show a 1d function gitting example and various conditional image generation ones, similar to the original examples in the paper. While I find the function fitting exampole quite unconvin cing, it arguably also contains less interesting structure for the model to pick up.\nIn the image generation examples both he quantitative and the qualitative illustrations appear to indicate that a very rich conditioning apparatus (stacked multi head attention) manages to give the model more detailed generative abilities.\nWhile introducing all this machinery seems a bit over-engineered at times, the results do show a benefit. \n\nOverall I find the exposition of the effects of the attention mechanism very well executed and the paper clearly positioned and written. My main complaint would be the incremental nature of the work, as the contributions here are not as significant advances as some preceding ideas that have gone into this work, but still steadily improve on the vision of NP and appear to be necessary steps to push the model forward giving this work validity on its own.\nThe authors discuss a similar mechanism for generation, which while more involved would be a very exciting change from the current framework.  I would have enjoyed seeing more of that in this paper to discuss input and output attention jointly.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}