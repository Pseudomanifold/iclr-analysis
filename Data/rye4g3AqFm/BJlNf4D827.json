{"title": "not surprising", "review": "\nThe authors make a case that deep networks are biased\ntoward fitting data with simple functions.\n\nThe start by examining the priors on classifiers obtained by sampling\nthe weights of a neural network according to different distributions.  They do this\nin two ways.  First, they examine properties of the distribution\non binary-valued functions on seven boolean inputs obtained by\nsampling the weights of a small neural network.  They also empirically compare\nthe labelings obtained by sampling the weights of a network with\nlabelings obtained from a Gaussian process model arising from earlier\nwork.\n\nNext, they analyze the complexity of the functions produced, using\ndifferent measures of the complexity of boolean functions.  A\nfavorite of theirs is something that they call Lempel-Ziv complexity,\nwhich is measured by choosing an arbitrarily ordering of the\ndomain, writing the outputs of the function in that ordering,\nand looking at how well the Lempel-Ziv algorithm compresses this\nsequence.  I am not convinced that this is the most meaningful\nand fundamental measure of the complexity of functions.\n(In the supplementary material, they examine some others.\nThey show plots relating the different measures in the body\nof the paper.  None of the measures is specified in detail in the\nbody of the paper. They provide plots relating these complexity\nmeasures, but they don't demonstrate a very close connection.)\n\nThe authors then evaluate the generalization bound obtained by\napplying a PAC Bayes bound, together with the assumption that\nthe training process produces weights sampled from the distribution\nobtained by conditioning weights chosen according to the random\ninitialization on the event that they fit they fit the training\ndata perfectly.  They do this for small networks and simple datasets.\nThey bounds are loose, but not vacuous, and follow the same order\nof difficulty on a handful of datasets as the true generalization\nerror.\n\nIn all of their experiments, they stop training when the training\naccuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf\nhave found that continuing training past this point further improves test\naccuracy.  The experiments all use architectures that are\nquite dissimilar to what is commonly used in practice, and\nachieve much worse accuracy, so that a reader is concerned\nthat the results differ qualitatively in other respects.\n\nI do not find it surprising that randomly sampling parameters\nof deep networks leads to simple functions.\n\nPapers like the Soudry, et al paper cited in this submission are\ninconsistent with the assumption in the paper that SGD samples\nparameters uniformly.\n\nIt is not clear to me how many hidden layers were used for the\nresults in Table 1 (is it four?).  \n\nI did find it interesting to see exactly how concentrated the\ndistribution of functions obtained in their 7-input experiment\nwas, and also found results on the agreement of the Gaussian process\nmodels with the randomly sampled weight interesting, as far as they\nwent.  Overall, I am not sure that this paper provided enough\nfundamental new insight to be published in ICLR.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}