{"title": "Well-motivated approach, but limited novelty and experiments", "review": "This paper introduces a domain adaptation approach based on the idea of Cyclic GAN. Two different algorithms are proposed. The first one incorporates a semantic consistency loss based on domain-specific classifiers acting on full cycles of the of the generators. The second one also makes use of domain-specific classifiers, but acting either directly on the training samples or on the data mapped from one domain to the other.\n\nStrengths:\n- The different terms in the proposed loss functions are well justified.\n- The results on low-resources supervised domain adaptation indicate that the method works better than the that of Motiian et al. 2017.\n\nWeaknesses:\n- Novelty is limited: The two algorithms are essentially small modification of the semantic consistency term used in Hoffman et al. 2018. They involve making use of both the source and target classifiers, instead of only the source one, and, for the relaxed version, making use of complete cycles instead of just one mapping from one domain to the other. While the modifications are justified, I find this a bit weak for ICLR.\n\n- It is not clear to me why it is worth presenting the relaxed cycle-consistency object, since it always yields worse results than the augmented one. In fact, at first, I though both objectives would be combined in a single loss, and was thus surprised not to see Eq. 5 appear in Algorithm 1. It only became clear when reading the experiments that the authors were treating the two objectives as two different algorithms. Note that, in addition to not performing as well as the augmented version, it is also unclear how the relaxed one could work in the unsupervised scenario.\n\n- Experiments:\n* In 4.1, the authors mention that 10 samples per class are available in the target domain. Are they labeled or unlabeled? If labeled, are additional unlabeled samples also used?\n* In Table 1, and in Table 3, is there a method that corresponds to CyCADA? I feel that this comparison would be useful considering the similarity. That said, I also understand that CyCADA uses both a reconstruction term (as in Eq. 4) and a semantic consistency one, whereas here only a semantic reconstruction term is used. I therefore suggest the authors to also compare with a baseline that replaces their objective with the semantic consistency one of CyCADA, i.e., CyCADA without reconstruction term.\n* In 4.2, it is again not entirely clear if the authors use only the few labeled samples, or if this is complemented with additional unlabeled samples. In any event, does this reproduce the setting used by Motiian et al. 2017?\n* As the argument is that the proposed loss is better than the reconstruction one and that of Hoffman et al. 2018 for low-resource supervised adaptation, it would be worth demonstrating this empirically in Table 2.\n\nSummary:\nThe proposed objective functions are well motivated, but I feel that novelty is too limited and the current set of experiments not sufficient to warrant publication at ICLR.\n\nAfter Response:\nAfter the authors' response/discussion, while I appreciate the additional results provided by the authors, I still feel that the contribution is a bit weak for ICLR.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}