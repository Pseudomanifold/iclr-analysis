{"title": "This paper integrates DPP with GAN and promotes diversity in learning generator distribution", "review": "The paper proposes to introduce DPP into the vanilla GAN loss and uses it as a way to regularize the generator to produce more diverse outputs, in order to combat the mode-collapse problem. Since the proposed method is added as a simple loss regularizer, the approach does not introduce additional parameters, therefore, less training difficulties. The results on synthetic data seems promising, but there is insufficient evaluation being performed on real and larger dataset where the mode collapse problems are more likely to happen.\n\n- Method\nThe proposed methods seem sensible. But there are some critical details missing from the current text that prevents me from assessing this paper clearly. \n\n* How are the \\lambda and v in Eq.6 calculated? It seems to me that you need to estimate the eigenvalues and eigenvectors at every iteration of your training. I am aware of that many DPP-based models suffer from scalability issues. Could you discuss the potential overhead of this procedure? Also in experiments, you claim \"DPP-GAN is cost-free, we observe that the training of GDPP-GAN has an indistinguishable running time than the DCGAN, which are the fastest models to finish training\u201c, which is hard to believe.. could you give more details and analysis on the overhead here?\n\n* In Eq.6 why there are both  \"a diversity magnitude loss L_m\", and \"diversity structure loss L_s\". What do they specifically try to capture respectively? Could you give a geometric interpretation on this part?\n\n* what is the batch size used in your experiments on MNIST and CIFAR-10. It seems to me that the effectiveness of GDPP would rely on batch size used as per my understanding you will estimate the DPP kernel using the current batch of samples (generated or real)? \n\n* Despite the fact that GDPP wants to reduce parameters introduced, it is not very intuitive to understand how it would work to use the features outputted by D as the DPP features as well. As D, as the discriminator itself, is trained to distinguish real from fake, while mimicking the eigenvalues/vectors of real data. How would these two goals be reconciled by the same set of parameters?\n\n- Experiments\nThe results on the synthetic data seem promising, but the results on MNIST and CIFAR-10 are not impressive enough:\n* The visual quality of Figure.9 does not look very appealing. I believe many simple variants DCGAN can produce better quality of images\n* Why your DCGAN baseline on CIFAR-10 only reports inception score around 5 (with high variance, see Figure.5 and Table 3)? I believe vanilla DCGANs can easily attain an IS at 5.5 to 6, as reported in most recent GAN literature.\n* More visual results on CIFAR10 should have been presented in order to demonstrate DPP does generate images with as many classes as existed in CIFAR-10 (which is 10)\n* The results could be much more convincing if the authors could show the generation results and evaluation metrics on larger/more real datasets other than CIFAR-10 and MNIST. See GAN literature in 2018 about what dataset to use.\n\n\n- Presentation\nMost parts of this paper are well written. There are few typos and grammatical errors across the text which I believe are easily fixable. There are some missing details that hinder the understanding of some technical parts of the paper. See above for detailed comments.\n\n- Other\nPromoting diversity in (deep) generative models isn't a new topic. It would be good if the authors could established connections/differences between this work and this line of relevant  works. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}