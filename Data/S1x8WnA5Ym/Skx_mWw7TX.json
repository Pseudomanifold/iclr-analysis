{"title": "The connection between the propsoed regularizer and the DPP is not precise.", "review": "For training GANs, the authors propose a regularizer that is inspired by DPP, which encourage diversity. This new regularizer is tested on several benchmark datasets and compared against other mode-collapse mitigating approaches. \n\n   \nIn section 2, important references to recent techniques to mitigate mode collapse are missing, e.g.\nBourGAN (https://arxiv.org/abs/1805.07674)\nPacGAN (https://arxiv.org/abs/1712.04086)\nD2GAN (https://arxiv.org/abs/1709.03831)\n\nAlso related is evaluation of mode collapse as in \nOn GANs and GMMs (https://arxiv.org/abs/1805.12462) \n\nThe actual loss that is proposed as in (5) and (6), seems far from the motivation that is explained as in Eq (3), using generator as a point process that resembles DPP. This conceptual gap makes the proposed explanation w.r.t DPP unsatisfactory. A more natural approach would be simply add $det(L_{S_B})$ itself as a regularizer. Extensive experimental comparisons with this straightforward regularizer is in order. \n\nIt is not immediate if the proposed diversity regularizer $L_g^{DPP}$ in (5) is differentiable in general, as it involves computing the eigen-vectors. Elaborate on the implementation of the gradient update with respect to this new regularizer. \n \n\nExperiments: \n\n1. The results in Table 3 for stacked-MNIST are very different from VEEGAN paper. Explain why a different setting was used compared to VEEGAN experiments. \n\n2. Similar experiments have been done in Unrolled-GAN paper. Add the experiment from that setting also. \n\n3. In general, split the experiments to two parts: one where the same setting is used as in the literature (e.g. VEEGAN, Unrolled GAN) and the results are compared against those reported in those papers. Another where new settings are studied, and the experiments of the baseline methods are also run by the authors. This is critical to differentiate such cases, as hyper parameters of competing algorithms could have not been tuned as rigorously as the proposed method. This improves the credibility of the experimental results, eventually leading to reproducibility. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}