{"title": "Incremental idea and weak analysis", "review": "Summary\nThis paper proposes to formulate diverse segmentation problems as a guided segmentation, whose task is defined by the guiding annotations.\nThe main idea of this paper is using meta-learning to train a single neural network performing guidance segmentation.\nSpecifically, they encode S annotated support image into a task representation and use it to perform binary segmentation.\nBy performing episodic optimisation, the model's guidance to segmentation output is defined by the task distribution.\n\nStrength\nLearning a single segmentation algorithm to solve various segmentation problem is an interesting problem that worth exploring.\nThis paper tackles this problem and showed results on various segmentation problems.\n\nWeakness\nThe proposed method, including the architecture and training strategy, is relatively simple and very closely related to existing approach. Especially, the only differences with the referenced paper (Shaban et al., 2017) is how the support is fused and how multiple guidance could be handled, which can be done by averaging. These differences are relatively minor, so I question the novelty of this paper.\n\nThis paper performs experiments on diverse tasks but the method is compared with relatively weak baselines absolute performance looks bad compared to existing algorithms exploiting prior knowledge for each of the tasks.\nFor example, the oracle performance in semantic segmentation (fully supervised method) is 0.45 IOU in PASCAL VOC dataset, while many existing algorithms could achieve more than 0.8 mean IOU in this dataset. \nIn addition, I question whether foreground / background baseline is reasonable baseline for all these tasks, because a little domain knowledge might already give very strong result on various segmentation tasks.\nFor example, in terms of video segmentation, one trivial baseline might include propagating ground truth labels in the first frame with color and spatial location similarity, which might be already stronger than the foreground / background baseline.\n\nThere are some strong arguments that require further justification. \n- In 4.3, authors argue that the model is trained with S=1, but could operate with different (S, P).\nHowever, it's suspicious whether this would be really true, because it requires generalisation to out-of-distribution examples, which is very difficult machine learning problem. The performance in Figure 5 (right) might support the difficulty of this generalisation, because increasing S does not necessarily increase the performance.\n- In 5.3, this paper investigated whether the model trained with instances could be used for semantic segmentation. I think performing semantic segmentation with model trained for instance segmentation in the same dataset might show reasonable performance, but this might be just because there are many images with single instance in each image and because instance annotations in this dataset are based on semantic classes. So the argument that training with instance segmentation lead to semantic segmentation should be more carefully made.\n\nOverall comment\nI believe the method proposed in this paper is rather incremental and analysis is not supporting the main arguments of this paper and strength of the proposed method. \nEspecially, simple performance comparison with weak baselines give no clues about the property of the method and advantage of using this method compared to other existing approaches.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}