{"title": "Nice idea & paper, but needs to highlight at least one practical advantage ", "review": "This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian). The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).  \n  \nPlugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.  Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score). Some potential options include:\n\n1) Faster training times. It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).  However, the authors only presented per-batch processing times as opposed to overall training time for these models.   \n\n2) Stabler training. Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure. The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.\n\n3) Usefulness of the CW distance outside of the autoencoder context.\nSince the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE). Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?\n\nWithout demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.\n\nOther Comments:\n\n- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians. See for example: \n\nChwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf\n\nwhich like CW-distance is also a quadratic-time closed-form distance between samples and a target density.\n\nBesides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives. \n\n- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?\n\nAfter reading the revision: I have raised my score by 1 point and recommend acceptance.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}