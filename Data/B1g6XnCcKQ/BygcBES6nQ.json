{"title": "Technically not novel, experimentally weak, unsupported arguments.", "review": "In this paper, an unsupervised representation learning method for visual inputs is proposed. The proposed method incorporates a metric learning approach that pulls nearest neighbor pairs of image patches close to each other in the embedding space while pushing apart other pairs. The train and test scenarios are captured based on a table top scenario with multiple objects of different colors such as cups and bowls. Standard datasets are not used for benchmarking. \n\nPros:\n- Unsupervised feature learning is an active and interesting area of research.\n- The proposed method and network architecture is simple.\n\nCons:\n\n-The paper does not present technical novelty. The proposed network architecture is a ResNet50 that uses the object proposals obtained by Faster RCNN (Ren et al. 2015) and incorporates the n-pair loss function proposed by Sohn 2016. \n\n-Technical details are missing. \n\n>When using object proposals obtained by Faster RCNN (Ren et al. 2015) to generate pairs, do you use all of the object proposals or do you select a subset of them by some threshold? \n>How is the robotic control performed in the robot grasping and pointing tasks? It seems that grasping and pointing tasks are not learned and are based on conventional motion planning and state estimation. These details are not included in the manuscript.\n>Section 4.2 mentions that a mobile robot was needed for collecting the data. What kind of mobile robot was used? Why was a mobile robot needed? There is no details about the robot platform in the manuscript and no visualization is provided for the robot setup.\n>Why is a ResNet pretrained with ImageNet is used throughout the experiments (and not pretrained with COCO?) while object proposals are obtained by Faster RCNN which is pretrained by COCO. How would that affect the results? \n\n-The paper uses imprecise, confusing and sometimes incorrect phrases and wordings throughout the draft. Here are a few examples:\n\n> It is unclear what it is exactly meant by \u201cobject correspondences\u201d that naturally emerge (e.g. in the last four lines of abstract). Would a correct object correspondence refer to \u201csimilar instances\u201d, \u201csimilar colors\u201d, \u201csimilar object categories\u201d, \u201csimilar object poses\u201d, \u201csimilar functionality\u201d? For example, the first column of Fig. 1 shows an example of two cups with \u201csimilar green color\u201d, \u201csimilar tall shapes\u201d and \u201csimilar functionality\u201d and \u201csimilar background\u201d that are considered as \u201cnegative pairs (with red arrows)\u201d while in the same Fig 1. Two cups with drastically different appearances one of which is \u201cyellow with handle\u201d (second column) and the another is \u201cgreen without handle\u201d (third column) are considered to be positive pairs (blue arrows). Similar confusing examples and arguments can repeatedly be found in the experiments and embedding visualizations:  Fig. 4, Fig.5, Fig. 10- Fig. 15. \n\n> Throughout the draft and in Figures (e.g. Fig. 1) it is emphasized the the data collection is done by \u201crobot\u201d or it is \u201crobotic\u201d. Why was a robot needed to capture images or videos of around a table? A hand held moving camera could also be simply used.  Capturing images and videos around a space is a very well-known and simple practice done in many previous works in computer vision and machine learning. It does not need a robot and it is not robotic. \n\n> First sentence of the second paragraph of the introduction is not a technically correct statement based on the well-known computer vision literature. Specifically, \u201cclass\u201d is not considered as an \u201cattribute\u201d.  What does it mean to disentangle \u201cperceptual\u201d and \u201csemantic\u201d object attributes? This is a very vague statement. \u201ccolor\u201d is neither a \u201csemantic\u201d attribute nor a \u201cperceptual\u201d attribute. Also, in section 3.2 and Fig. 2, it is confusing to consider \u201cclass\u201d label as an attribute. Please refer to the well-known prior work of \u201cFarhadi A, Endres I, Hoiem D, Forsyth D. Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on 2009 Jun 20 (pp. 1778-1785). IEEE.\u201d for a clear and well-known explanation of object attributes vs categories. If your definition of \u201cattributes\u201d is technically different than that of this aforementioned prior work, it should be clarified to avoid confusion. \n\n-Unconvincing usefulness for robotics applications: The paper explains that the proposed method is effective and useful for robotic tasks however tis claim is not backed up with convincing robotic tasks or scenarios. Also, the performance in the robotic grasping and pointing tasks are not compared with any baselines.\n\n> While usefulness of the method for improving robotic grasping task is listed in the contributions (number 3 in the last paragraph of Section 1) there are only two qualitative grasping scenarios shown in the paper. It is not explained how the robot control is done. In both grasping scenarios the target object is the one in the middle. It seems that the grasping control policies are not learned and are motion planned or are a scripted policy and only the visual matching of the target object with the objects of the scene are done with the proposed method. For evaluating such scenario no robot is needed and only similarity score could be reported. It would had been interesting to see how representation learning can be seamlessly incorporated for robotic grasping which involves control as a tangible contribution however, the presented scenario does is not contributing to that problem and is only doing visual matching. No baseline comparison is provided here.\n\n> The details of the robot control for the robot pointing scenario is also not provided. The objects are places on two rows in all scenarios and it looks like the the actual pointing is done by motion planning after the visual matching step done by the proposed method. The presented method in this paper tries to find the closest visual match to the target object amongst the objects on the table and does not integrate it with any policy learning for improving the \u201crobotic pointing task\u201d so there is no robotic contribution here. No baseline is provided performance comparison in this robotic task as well.  \n\n- The experimental results are weak and unconvincing:\n\n> In the experiments corresponding to Table 6 and Table 1, what is the performance of ResNet 50 embedding (linear)? Can you provide these results?\n\n> Table 6 shows that the performance gain of the proposed method compared to the supervised and unsupervised baselines is marginal. \n\n> What is the performance of a baseline network with similar architecture that uses \u201ccontrastive loss\u201d (based on object pairs with similar attributes). This baseline is missed.\n\n> Qualitatively, the visualizations in Fig. 1, Fig. 4-5, Fig. 10-15 show that OCN embedding is noisy. In all these figures, there are many less similar instances that are retrieved on the top and many similar instances that are ranked down in the list.    \n\n> The paper refers to ad-hoc experimental protocols which makes the evaluations unclear and invalid: what is meant to report \u201cclass\u201d and \u201ccontainer\u201d attributes in Table 3 and section 5.3? Why are \u201cbottles and cans\u201d are considered in a same group while there were referred to as different objects in all previous explanations of the object types and categories used in the training and testing? What is the difference between \u201ccups and Mugs\u201d and \u201cglasses\u201d that are separated? How are \u201cBalls\u201d , \u201cBowls\u201d, \u201cPlates\u201d, etc listed as *attributes*?  Aren\u2019t these previously referred to as object categories? \n\n> The paper has some inconsistent experimental setups. Why in Fig 16, the same instances of were removed for the real objects and not for the synthetic objects?\n\n\n-The presentation of the paper can be improved. Here are a few examples:\n\n>Grammatical or unclear wordings and typos (e.g. Caption of Fig. 2 \u201cattracting embedding nearest neighbors\u201d; is not a readable. Repetitive words, Last line of section 3. , etc)\n>In Fig. 10-15, using t-SNE (Maaten LV, Hinton G. Visualizing data using t-SNE. Journal of machine learning research. 2008;9(Nov):2579-605.) instead of a sorted list provides much more expressive visualization of the effectiveness of the approach. It is highly recommended that t-SNE be used instead of sorted lists if image patches.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}