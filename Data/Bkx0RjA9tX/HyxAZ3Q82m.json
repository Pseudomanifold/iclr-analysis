{"title": "Great idea, well executed, well written", "review": "This paper introduces a generative model for question answering.  Instead of modeling p(a|q,c), the authors propose to model p(q,a|c), factorized as p(a|c) * p(q|a,c).  This is a great idea, it was executed very well, and the paper is very well written.  I'm glad to see this idea implemented and working.                                                       \n                                                                                                     \nReactions:                                                                                           \n- Section 2.1: Is there a bias problem here, where you're only ever training with the correct answer?  Oh, I see you covered that in section 2.6.  Great.\n- Section 2.4: what happens when there are multiple QA pairs per paragraph or image?  Are you just getting conflicting gradients at different batches, so you'll end up somewhere in the middle of the two answers?  Could you do better here?\n- Section 2.6: The equation you're optimizing there reduces to -log p(a|q,c), which is exactly the loss function used by typical models.  You should note that here.  It's a little surprising (and interesting) that training on this loss function does so poorly compared to the generative training.  This is because of how you've factorized the distributions, so the model isn't as strong a discriminator as it could be, yes?\n- Section 3.1 (and section 2.6): Can you back up your claim of \"modeling more complex dependencies\" in the generative case?  Is that really what's going on?  How can we know?  What does \"modeling more complex dependencies\" even mean?  I don't think these statements really add anything currently, as they are largely vacuous without some more description and analysis.\n- Section 3.3: Your goal here seems similar to the goal of Clark and Gardner (2018), trying to correctly calibrate confidence scores in the face of SQuAD-like data, and similar to the goals of adding unanswerable questions in SQuAD 2.0.  I know that what you're doing isn't directly comparable to either of those, but some discussion of the options here for addressing this bias, and whether your approach is better, could be interesting.\n                                                                                                     \nClarity issues:                                                                                      \n- Bottom of page 2, \"sum with a vector of size d\" - it's not clear to me what this means.            \n- Top of page 3, \"Answer Encoder\", something is off with the sentence \"For each word representation\" \n- Section 2.5, \"we first embed words independently of the question\" - did you mean \"of the _context_\"?\n- Section 2.5.2 - it's not clear to me how that particular bias mechanism \"allows the model to easily filter out parts of the context which are irrelevant to the question\".  The bias mechanism is independent of the question.\n- Section 2.7 - when you said \"beam search\", I was expecting a beam over the question words, or something.  I suppose a two-step beam search is still a beam search, it just conjured the wrong image for me, and I wonder if there's another way you can describe it that better evokes what you're actually doing.\n- Section 3.1 - \"and are results...\" - missing \"competitive with\"?                                   \n- Last sentence: \"we believe their is\" -> \"we believe there is\" ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}