{"title": "Good theoretical contribution but lack of motivation ", "review": "The paper proposes an algorithm that learns word embeddings in hyperbolic space. It adopts the Skip-Gram objective from Word2Vec on the hyperboloid model and derives the update equations for gradients accordingly. \nThe authors also propose to compute the word analogy by parallel transport along geodesics on the hyperboloid.\n\nStrength: The paper is well written, both the background geometry and the derived update method are clearly explained. The novelty and theoretical contribution are adequate. \n\nWeakness: My main concern is the lack of motivation for embedding words on the hyperboloid and the choice of evaluation metrics. For Poincare embeddings, the disc area and circle length grow exponentially with their radius, and the distances on the Poincare disk/ball reflect well the hierarchical structure of symbolic data, which make it natural to embed a graph in this space and lead to great evaluation results. The geometric property of the hyperboloid model, however, does not seem to in favor of encoding non-hierarchical semantics of words and the evaluation on word similarity/analogy tasks. The evaluation results in Table 1 and Table 2 show that the hyperbolic embeddings only performs better than the Euclidean embeddings in low dimensions but worse on higher dimensions (>50), while higher dimension embeddings generally encode more semantics and thus are used in downstream tasks. It will be great if the authors could elaborate on the advantages of learning word embeddings in hyperbolic space and evaluate accordingly. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}