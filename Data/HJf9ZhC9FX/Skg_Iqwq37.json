{"title": "Minimax optimality results are proven for SGD and SMD. These results demonstrate implicit regularization properties of these algorithms even when the models are trained without explicit regularization", "review": "The authors look at SGD, and SMD updates applied to various models and loss functions. They derive a fundamental identity lemma 2 for the case of linear model and squared loss + SGD and in general for non-linear models+ SMD + non squared loss functions. The main results shown are\n1. SGD is optimal in a certain sense for squared loss and linear model.\n2. SGD always converges to a solution closest to the starting point.\n3. SMD when it converges, converges to a point closest to the starting point in the bregman divergence. The convergence of SMD iterates is shown for certain learning scenarios.\n\nPros: Shows implicit regularization properties for models beyond linear case.\nCons: 1. The notion of optimality is w.r.t. a metric that is pretty non-standard and it was not clear to me as to why the metric is important to study (the ratio metric in eq 9).\n2. The result is not very surprising since SMD is pretty much a gradient descent w.r.t a different distance metric. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}