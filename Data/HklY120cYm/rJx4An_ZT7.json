{"title": "A strong result but unclear experiments and contribution", "review": "After reading other reviews and author comments, I have raised my rating to a 6. My main concerns remain (lack of significant contribution and lack of an ablation study with more comprehensive experiments). However, I'm not against the paper as an interesting finding in and of itself. It would be great if the authors (or interested members of the research community) may analyze how general-purpose their proposals are (e.g., of Gaussian base distribution) and how extensive the results are on TTS benchmarks.\n\n--\n\nOverall, I very much like the direction this paper pursues. However, the content doesn't substantiate their two claimed contributions. I highly recommend the authors either back up their claims in more detail, or center their work in terms of the result and less so about the ideas (which at the moment, are not convincing to use outside of this specific setup).\n\nThe authors propose two contributions:\n\n1. They build on parallel WaveNet which uses distillation by minimizing a KL divergence from a Logistic IAF as a student to a Mixture of Logistic AF as a teacher. Instead, they simply use Gaussians which has a closed-form KL divergence and makes training during distillation significantly simpler. Because of stability problems, they also add 1. a penalty term to discourage the original loss from dividing by a standard deviation close to zero; and 2. converting van den Oord et al. (2018)'s average power loss penalty to a frame-level loss penalty.\n\nTheir choice of Gaussians requires a restriction on the likelihood, and they show one result arguing the likelihood choice doesn't make much of a difference. This result comprises 4 human-evaluated numbers, with a fixed architecture and training hyperparameters of their choice. Unfortunately, I'm not convinced. Can the authors provide more compelling evidence? If the authors argue this is one of their main contributions, I find that lack of a more comprehensive empirical or theoretical study disconcerting.\n\nSimilarly, while I like that using Gaussian KLs makes the distillation objective in closed-form, there isn't evidence indicating the benefit. The one result (the 4 numbers above) are conflated by both the change in model as well as utilizing the closed-form loss. The same goes for their one result (2 numbers) comparing forward to reverse KL.\n\n2. They \"propose the first text-to-wave neural architecture for TTS, which can be trained from scratch in an end-to-end\nmanner.\" I'm not an expert on speech so I can't accurately assess the novelty here. However, it would be nice to show these results independent of the other proposed changes.\n\nWriting-wise, the paper was clear, although potentially too packed with background information. As a expert on generative models, most of Sections 1-3 are already well-known and could be made more concise by referencing past works for more details. They add various details (such as the architecture notes at the end of 3.1) which should be better placed elsewhere to tease out what the important changes are in this paper.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}