{"title": "Weak technical novelty and insight", "review": "**Summary of the paper**\n\nThe paper introduces a novel approach for unsupervised object detection based on motion cues from two separate videos: one with and one without the object of interest. The main novelty is the usage of the negative video example which enables unsupervised detection of objects by being robust to distractor objects in the scene as the algorithm is able to differentiate between interesting motion and distractor object motion. The method trains a convolutional network to output dense probability maps for the object location and optimizes for strong/weak detection in positive/negative video respectively along with smoothness and variation objectives. The authors evaluate the proposed method on self-collected video datasets and test detection on held-out videos from the same scene as well as generalization to detecting the same object in new scenes. They compare to template matching and tracking baselines and give suggestions for a possible application of their method as automated demonstration annotation tool for pick-and-place tasks.\n\n\n**Clarity**\n\nAbove average\n\n**Significance**\n\nBelow Average\n\n**Correctness**\n\nThe paper is technically correct.\n\n**Detailed comments**\n\n_Paper Strengths_\n\n- the idea to use a negative video example for unsupervised detection learning seems novel\n- the proposed method is simple and the needed data can be collected with widely available equipments\n- the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example\n- the authors collected real-world data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)\n- the authors compare to a number of non-learning approaches from open-source implementations (the reviewer cannot judge whether any relevant technique is missing)\n- the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes\n\n\n_Paper Weaknesses_\n\n- the authors clearly reduced the horizontal margins of the standard ICLR style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard ICLR style template upon update of their manuscript\n- the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. maker-based approaches could be employed for detecting the object\n- the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multi-object examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is self-occluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)\n- other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging non-training scenes with heavier clutter, non-seen lighting changes and occlusions to support their robustness claim\n- the proposed method cannot operate in-the-wild (e.g. Youtube data) as it makes very strong assumptions about the required input data\n- the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene\n- the authors make no comparison to other unsupervised detection approaches (e.g. to the self-cited Jonschkowski et al. (2017)) to prove short-comings of other methods on the newly generated dataset\n- as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for ego-motion estimation) makes applications to non-video data impossible, also none of the experiments exploit the non-temporal property of the approach to show single frame detection on a more varied set of scenes\n- the experiment showcasing the proposed application to \"learning from demonstration\" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task\n- the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)\n- the reviewer cannot follow the references to object-centric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an object-centric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion\n- the \"random search optimization\" discussed in section 3.3 is not a valid method as it \"solves\" this problem of instable training by picking the best of N runs with varying random seeds\n- Figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach\n\n\n_Reproducibility_\n\n- Given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work.\n\n_Conclusion_\n\n- Overall, the reviewer appreciates the effort that went into the work but sees considerable need for improvement concerning the motivation and possible applications given the strong assumptions that are made about the input data. To truly position the paper against other detection/tracking algorithms more experiments are needed that show zero-shot (without retraining) generalization to substantially different scenes, or even architectural changes to be able to handle novel objects too. If the authors want to motivate the method as an automated tool for demonstration annotation an actual example of policy learning from unsupervisedly annotated sequences would strengthen the argument. The proposed method does not provide considerable technical novelty or insight to compensate for the lack in motivation. In the current state the paper is not convincing enough to warrant acceptance.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}