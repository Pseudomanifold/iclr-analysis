{"title": "Nice algorithm but incremental ", "review": "This paper discusses a technique for continuous normalization flow in which the transformations are not required to be volume preserving (the transformation with unity Jacobian), and architecture of neural network does not need to be designed to hold such property. Instead authors proposed no restriction on architecture of neural network to design their reversible mapping.\nThe Paper has good background and literature review, and as authors mentioned  this paper is base on the idea of  Chen, Tian Qi, et al. \"Neural Ordinary Differential Equations.\"\u00a0arXiv preprint arXiv:1806.07366\u00a0(2018). Chapter two of this paper is summary of  \"Neural Ordinary Differential Equations.\"\u00a0and chapter Three is main contribution of this paper that can be summarized under two points:\n\n1- Authors borrowed the \"continuous normalizing flow \" in Chen et al. and they have designed unbiased log density estimator using Hutchinson trace estimator and evaluated the trace with complexity of O(D) (dimension of data) instead of O(D^2) that  is used in chen et al. Paper\n\n2- They proposed by reducing the hidden layer dimension of neural network, it is possible that variance of estimator to be reduced \n\nNovelty and Quality:\nthe main contribution of this paper is summarized above.\nThe paper do not contain any significant theorem or mathematical claims, it is more focused on design of linear algorithm that estimate continuous normalizing flow that they have borrowed from the Chen et al. paper.  This is a good achievement that can help continuous normalizing flow scale on data with higher dimensions, but in results and experiments section no comparison has been made to performance of chen et al. Also no guarantees or bound has been given about the variance reduction of  estimator and it is more based on the authors intuition.\n\nClarity:\nThe paper is well written and previous relevant methods have been reviewed well. There are a few issues that are listed below:\n1-in section 3 the reason that dimensionality of estimator can reduce to D from D^2 can be explained more clearly \n\n2- Figure 1 is located on first page of the paper but it has never been referred in main paper, just it is mentioned once in appendix , it can be moved to appendix.\n\n3- in section 3.1.1 the \u201cview view\u201d can be changed to \u201cview\u201d\n\nsignificance and experiments:\nThe experiments are very detailed and extensive and authors have compared their algorithm with many other competing algorithms and showed improvement in many of the cases. \nAs mentioned in Quality and Novelty part of the review, just one comparison is missing and that is the comparison to method that the paper is inspired by. It would be interesting to see how much trace estimator approach that has been used in this paper, would sacrifice the negative log-likelihood or ELBO specially in real data like MNIST and CIFAR 10.  it seems original paper has not reported the performance on those data-sets as well, is this difficult as chen et. al. paper algorithm for trace calculation has complexity of O(D^2)? \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}