{"title": "BETTER GENERALIZATION WITH ON-THE-FLY DATASET DENOISING", "review": "This paper presents ODD, a method that rejects incorrectly labeled / noisy examples from training on the fly. The motivation is sound, that with the capacity of modern neural networks, it's easy to memorize the mislabeled data and thus hurt generalization. If we could reject such mislabeled data, we may be able to get a more generalizable model. The authors made an observation that when training with large learning rate, examples with correct labeling and incorrect labeling exhibits different loss distributions. The authors further noticed that the loss distribution of incorrectly labeled examples can be simulated using eq.(1). Therefore, by setting a threshold that corresponds to a percentile of the incorrectly labeled loss distribution, the authors are able to reject incorrect examples.\n\nSome comments:\n1. Eq.(1) basically assumes all the noise is uniformly distributed among classes. What if only 2 classes are easily mislabeled while others are fine?\n2. Section 4.1.3 and Section 4.4 Sensitivity to Noise are confusing. Please clarify the importance and rationale for such analysis.\n3. Cosine schedule is used in the experiments. However, since the method does not work well with small learning rate, why not using a fixed large learning rate and decrease it after noise rejection? Also, in section 4.4 Sensitivity to E, the analysis of the sensitivity to the number of epochs is coupled with a changing learning rate. It would be better to see an experiment with the two decoupled.\n4. The loss of an example is averaged over h epochs. It will better to clarify how the simulated distribution generated in such case since the distribution is dependent on fc(.), which is changed between two epochs.\n5. Except for the first experiment, all other experiments are only compared with ERM, the vanilla algorithm. It would be better to show a comparison with other methods.\n6. Please show a precision/recall of the examples that are marked as \"noise\" by the method.\n7. I assume this method will remove a lot of hard examples. How does this affect training? Does this make the network more error-prone to harder instances?", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}