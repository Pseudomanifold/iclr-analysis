{"title": "Good paper", "review": "This paper analyzes that the Integral Probability Metric (IPM) can be a good approximation of Wasserstein distance under some mild assumptions. They first showed two theorems based on simple cases (Gaussian Distribution and Exponential Families). Then, they proved that, for an invertible generator, a special designed neural network can approximate Wasserstein distance with IPM. The main contribution is that, for a stable generator (i.e., invertible generator), a discriminator can reversely \u201cre-visit\u201d inner status of the generator, then use this information to make a decision. \n\nIn the appendix, several numerical examples are presented to support their theoretical bound. \n\nQ: Assumption 1, \\sigma(t) is twice differentiable. However, Leaky ReLU is not twice differentiable at t=0. Do I misunderstand some part?\n\nQ: The invertible generator assumption is not held in practice. Is that possible to extend the theorem to this case, even with a shallow network (e.g. 2 layers)?\n\nQ: The numerical examples are all based on synthetic data. Did you have any results based on the real dataset?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}