{"title": "Application of triplet loss to duplicate detection, lack of novelty.", "review": "The paper presents an application of deep convolutional networks for the task\nof duplicate image detection.\n\nIt is a pure application paper and the triplet loss very well known in the\ncommunity for retrieval and ranking.\nThe generation of the near-duplicate images using random transformations is\nalso quite standard. However, given the application domain the authors should\nexplain why some parameters have been chosen rather than others. I assume the\nlevel of variability here is not that large and the space can be constrained.\n\nThe parameter alpha in the triplet loss could be omitted as it can be\n\"absorbed\" by the network. Also why didn't the author comment on their choice\nof such loss? The original one has a margin for example.\n\nI am afraid that the paper lacks the necessary rigor in the experimental\nsection. There are no baselines and comparisons to other losses such as the\nsiamese loss, that would seem easier and more appropriate for this task given\nthe binary outcome.\n\nI have to admit that I don't quite understand the motivation of the work,\nto me it seems that standard techniques would be already quite successful here.\nFor example classic feature matching with SIFT, to which I encourage the authors\nto compare to.\n\nTo summarize I think the paper still requires lot of work before being considered\nfor publication.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}