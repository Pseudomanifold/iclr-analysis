{"title": "Interesting and valuable model extension, but with rather early results", "review": "This paper proposes an extension to VAE to model sequential datasets and extract disentangled representations of their evolution.\nIt consists of a straight extension of CCI-VAE (Burgess et al 2018) to accept sequential data, combining it with a Ladder VAE architecture (VLAE, Zhao et al 2017).\nThey show early results on fitting toy domains involving 2D points moving in space (reaching, reaching in sub-sequences with complex dynamics, gripper domain).\n\nOverall, I think this is an interesting piece of work, which proposes a good model extension and assessment of its characteristics. The model is well presented, the different components are sufficiently motivated and they perform just enough experiments to showcase the effectiveness of their method, although with some reservations.\n\nCritics:\n1.\tThe comparison to Beta-VAE is a straw man, and I\u2019m not sure it\u2019s a valid way to introduce your model. You are basically saying that treating sequential data as if it was non-sequential is bad, which is clearly not surprising? Hence any comparison with Beta-VAE that you show, Figure 1 and Figure 3, are not appropriate (the caption of Fig 1 is particularly bad in that aspect). A more correct comparison would be to directly feed x_1:T to a Beta-VAE and see what happens, maybe with a causal time-convolution as well if you want to avoid 3D filters.\n2.\tYou are also not comparing to the FHVAE model you present in your Introduction, which would have been nice to see, given that your model is simpler and requires less supervision. Does FAVAE perform better than these?\n3.\tSection 3 could use a citation to Esmaeili et al 2018, which breaks out the ELBO even further and compares multiple models in a really nice way (e.g. Table A.2). Overall Section 2 and 3 feel a bit long and pedantic, you could just point people to the original papers and move some of the justification to Appendix (e.g. the IB arguments are not that required for your model. ). \u2028The main point you want to put across is that you want to have your \u201cz\u201d compress a full trajectory x_1:T, under a single KL pressure (i.e. last sentence of Section 4).\n4.\tFigure 3 was hard to interpret at first, specifically for panels b and c. Maybe if you showed the \u201csampled trajectory\u201d only once in another plot it would make it clearer.\n5.\tTime-convolution seems to wrongly be using the opposite indexing? With z_tk = \\sum_{p=0}^{H} x_{t+p}, you have an anti-causal filter which looks at the future x_t\u2019 for a z_t? That does not sound right? Also, you should call these \u201ccausal convolutions\u201d, which is the more standard term.\n6.\tThe exact format of the observations was never clearly explained. From 7.1 I understood that you input 2D positions into the models, but what about the Gripper? \u2028As you\u2019re aware, Beta-VAE and others usually get RGB images as inputs, hence you should make that difference rather clear. This is a much simpler setting you\u2019re working in.\n7.\tDid you anneal the C as was originally proposed in Burgess et al 2018? With which schedule? This was not clear to me. The exact choices of C for the different Ladder levels lacked support as well. An overall section in the Appendix about the parameter ranges you tried, the architectures, the observation specifications, the optimisation schedule etc etc would be useful.\n8.\tI appreciate the introduction of the MIE metric, which seems to slightly improve over MIG in a meaningful way. However, it would be good to show situations where the two metrics disagree and why MIE is better, because in the current experiments this is unclear.\n9.\tOverall the Gripper experiments seem to merit a more complete assessment. Figure 7 was hard to understand, and I am not sure it shows clearly any disentangled factors. Its caption was strange too (what are the \u201c(1, 8)\u201d, \u201c(2, 1)\u201d things referring to?). \n10.\tI would have liked more interpretation and comments on why the Ladder is needed, and why FAVAE (without Ladder and C) does so badly in Table 1.\n11.\tIt would be good to know if you really find that the different levels of the Ladder end up extracting different time scales, as you originally claim it can. There are no results supporting this assumption in the current version.\n12.\tFigure 4B uses a bad scale, which makes it hard to assess what happens between the two C conditions for Beta \\approx 100, where they seem to differ the most in Fig 4A.\n13.\tFigure 5 could use titles/labels indicating which \u201cgenerative factors\u201d you think are being represented. Just compare them to your Figure 8 in Appendix.\n14.\tFigure 6 MIE scores look all within noise between models considered. How sensitive is the metric to actual differences in the disentanglement?\n\nOverall, I think this is an interesting improvement to disentangled representations learning, but suffers a bit from early experimental results. I would still like it to be shown at ICLR though as it really fits the venue.\nI'm happy to improve my score given some improvements on the points mentioned above.\n\nReferences:\n-\tBurgess et al., 2018: https://arxiv.org/abs/1804.03599\n-\tZhao et al., 2017: https://arxiv.org/abs/1702.08396 \n-\tEsmaeili et al., 2018: https://arxiv.org/abs/1804.02086 \n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}