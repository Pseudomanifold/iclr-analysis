{"title": "Effective knowledge distillation for multilingual NMT, at the cost of increased training time", "review": "The authors apply knowledge distillation for many-to-one multilingual\nneural machine translation, first training separate models for each language\npair. For most language pairs, performance matches or improves upon\nsingle-task baselines.\n\nStrengths:\n\nImprovements upon the baselines are fairly impressive, especially for the\n44-language model.\n\nThe approach is quite simple and could be easily implemented by other groups.\n\nThe paper is well-written and easy to understand.\n\nAt inference, only a single model needs to be retained, which is memory-efficient.\n\nWeaknesses:\n\nThe authors only test distillation in a many-to-one scenario. I believe that\nproviding results for many-to-many multilingual NMT would be valuable.\n\nOverall, this approach increases training time as all single-task models\nmust have converged before beginning distillation.\n\nThe authors provide no direct comparison to other work, which makes it hard to\nknow how strong the baselines are. At least for WMT, I would suggest reporting\nresults with mteval-v13a (or SACREBLEU), so that results can be compared against\nofficial results.\n\nQuestions:\n\nFor the top-K approach, do you normalize the top K probabilities so that they\nsum to 1 or not?\n\nDid you consider applying sequence knowledge distillation (Kim and Rush, 2016)\n(using the baseline beam search output as references) instead of word knowledge\ndistillation?\n\n***\nEDIT: In my opinion, the changes made after the review period clearly improve the quality of the paper. I am increasing my rating from 6 to 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}