{"title": "The proposed idea is to replace standard nonlinear activation function with an erosion/dilation operation. The authors report encouraging results but the baseline networks are not state-of-the-art. ", "review": "This paper proposes to replace the standard RELU/tanh units with a combination of dilation and erosion operations, arguing for the observation that the new operator creates more hyper-planes and therefore have more expressive power.\n\nThe paper is interesting and there are encouraging results which show a couple of percentage improvements over relu/tanh units.  This paper is also clearly written and easy to understand. However there are two issues:\n1. It is somewhat unclear from the paper what  is the main novelty here (compared to existing morpho neurons), is it the learning of the structuring element s? is it the combination of the dilation+erosion operations?\n2. The second issue is that presumably due to the fact that Conv layers are not used, the accuracy on cifar-10 and cifar-100 are significantly lower than state-of-the-art. It would make the paper extremely strong if the improvement translated to CNNs which are performing near the state-of-the-art. What happens if relu units in CNNs were swapped out for the proposed dilation/erosion operators?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}