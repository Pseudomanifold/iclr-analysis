{"title": "A nice idea but weak empirical results", "review": "* Update:\nThanks for you answer and clarification. While the Morph-net appears novel, the authors only report result for image classification task and don't achieve as good performance as standard convolutional baselines. Given the current empirical evaluation, I find hard to assess how significant is the contribution. I would encourage the authors to either compare on a task where dense networks achieve state-of-art performances or extend their approach to 2D inputs.\n\n\n* Review\n\nThis paper introduces Morph-Net, a new architecture that intertwine morphological operator such as dilation/erosion with linear layer. Authors first show than Morph-Net are universal approximator. Morph-Net can be expressed as a sum of multi-order hinge functions which can approximate any continuous function. They then validate empirically the Morph-Net on  MNIST, FashionMNIST,  CIFAR10 and CIFAR100 datasets. In particular, authors investigate a 3 layers  fully-connected Morph-Net and shows that it can outperform its Tanh/Relu/Maxout counterparts.\n\nThe paper is a nice read also some specific point could be clarify. For instance it is not clear how the structuring elements of the dilation/erosion are learned? Are the learned simply through backpropagation? Also, it is not clear to me how Morph-Net differs from the previously proposed morphological neurons? \n\nEmpirical evaluation of Morph-Net could be improved as well. In particular, authors focus on image classification task. While they show that Morph-Net can outperform other fully connected architecture, the results on CIFAR10/100 seems low compared to convolutional network. It raises the question of the advantages of Morph-Net over convolutional neural networks ?  Authors also limit their exploration to  3-layer networks. Why don\u2019t you explore deeper network for both baseline and Morph-Net?  Finally, if I am not mistaken, authors use the same set of hyperparameters for the baselines/Morph-Net? It is not clear to me if the hyperparameters are optimal for all the approach? They might give an unfair advantage to one of the baseline or Morph-Net?\n\nOverall, this paper present a nice idea. Showing the Morph-Net is an universal approximator is a nice result. However, the empirical evaluation could be improved. It is not clear to me at this point if Morph-Net brings a benefit compare to convolutional net for image classification task.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}