{"title": "Interesting idea for using morphological operators but too preliminary", "review": "The authors introduce Morph-Net, a single layer neural network where\nthe mapping is performed using morphological dilation and erosion.\nI was expecting something applied to convolutional networks as such operators\nare very popular in image processing, so the naming is a bit misleading.\n\nIt is shown that the proposed network can approximate any smooth function, \nassuming a sufficiently large number of hidden neurons, that is a nice result.\n\nClarity should be improved, for example it is mentioned that the structuring\nelement is learned but never clearly explained how and what difficulties it poses.\nIn the main text it is written that alpha is {-1, 1}, which would result in a\ncombinatorial search, but never explained how it is learned in practice.\nThis is shown only in the appendix but it is not clear to me that using a binarization\nwith the weights is not prone to degenerate solutions and/or to learn at all\nif proper initialization is not used.\nDid the authors experiment with smooth versions or other form of binarization with\nstraight-through estimator or sampling?\n\nIn the proof for theorem 1 it is not clear if the convergence of the proposed\nnetwork is faster or slower than that of a classic single layer network.\n\nThe main result of the paper is that the structuring element can be learned,\nbut there is no discussion on what it is learned. Also, there is no comparison\non related approaches that try to learn the structuring element in an end-to-end\nfashion such as [1].\n\nExperiments lack a more thorough comparison with state-of-the-art and at least\nan ablation study to show that the proposed approach is effective and has merit.\nFor example, what is the relative contribution of using dilation and erosion\njointly versus either one of them.\nWhat is the comparison with a winner-take-all unit over groups of neurons\nsuch as max-pooling?\n\nIt seems that extending the work to multiple layers should be trivial but it is\nnot reported and is left to future investigations. This hints at issues with\nthe optimization and should be discussed, is it related to the binarization\nmentioned above?\n\nOverall the idea is interesting but the way the structuring element is learned\nshould be discussed in more details and exemplified visually. Experiments need\nto be improved and overall applicability is uncertain at this stage.\n\n=======\n[1] Masci et al., A Learning Framework for Morphological Operators Using Counter--Harmonic Mean.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}