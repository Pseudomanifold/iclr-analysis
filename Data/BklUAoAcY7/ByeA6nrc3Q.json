{"title": "Simple method for learning sentence representations, with competitive results", "review": "\n== Clarity == \nThe primary strength of this paper is the simplicity of the approach.\n\nMain idea #1: corrupt sentences (via random insertions/deletions/permutations), and train a sentence encoder to determine whether a sentence has been corrupted or not.\n\nMain idea #2: split a sentence into two parts (two different ways to do this were proposed). Train a sequence encoder to encode each part such that we can tell whether the two parts came from the same sentence or not.\n\nI can see that this would be very easy for others to implement, perhaps encouraging its adoption.\n\n== Quality of results ==\nThe proposed approach is evaluated on the well-known SentEval benchmark.\n\nIt generally does not outperform supervised approaches such as InferSent and MultiTask. However, this is fine because the proposed approach uses no supervised data, and can be applied in domains/languages where supervised data is not available.\n\nThe approach is competitive with existing state-of-the-art sentence representations such as QuickThoughts. However, it is not definitively better:\n\nOut of the 9 tasks with results for QuickThoughts, this approach (ConsSent) performs better on 3 (MPQA +0.1%, TREC +0.4%, MRPC +0.4%). For the other 6 tasks, ConsSent performs worse (MR -1.8%, CR -1.7%, SUBJ -1%, SST -3.8%, SK-R, -2.4%). Taken together, the losses seem to be larger than the gains.\n\nFurthermore, the QuickThoughts results were obtained with a single model across all SentEval tasks. In contrast, the ConsSent approach requires a different hyperparameter setting for each task in order to achieve comparable results -- there is no single hyperparameter setting that would give state-of-the-art results across all tasks.\n\nThe authors also evaluate on the newly-released linguistic probing tasks in SentEval. They strongly outperform several existing methods on this benchmark. However, it is unclear why they did not compare against QuickThoughts, which was the strongest baseline on the original SentEval tasks.\n\n== Originality ==\nThe proposed approach is simple and straightforward. This is on the whole a great thing, but perhaps not especially surprising from an originality/novelty perspective.\n\nTherefore, the significance and impact of this approach really needs to be carried by the quality of the empirical results.\n\nThe sentence pair based approaches (ConsSent-N and C) are conceptually interesting, but don't seem to be responsible for the best results on the linguistic probing tasks.\n\n== Conclusion ==\n\nPros:\n- conceptual simplicity\n- competitive results (better than many previous unsup. sentence representation methods, excluding QuickThoughts)\n- strong results on SentEval's linguistic probing task\n\nCons:\n- no single hyperparameter value (perturbation method and value for k) gets great results across all tasks\n- some important baselines possibly missing for linguistic probing tasks", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}