{"title": "Interesting theoretical framework for predicting the necessary precision in deep networks, along with experimental evaluation confirming the theoretical results.", "review": "Quality and clarity:\nThe paper presents a theoretical framework and method to determine the necessary number of bits in a deep learning networks. The framework predicts the smallest number of bits necessary in the (multiply-add) calculations (forward propagation, backward propagation, and gradient calculation) in order to keep the precision at an acceptable level. \n\nThe statistical properties of the floating-point calculations form the basis for the approach, and expressions are derived to calculated the smallest number of bits based on, e.g., the length of the dot product and the number variance. \n\nThe paper seems theoretically correct, although I haven't studied the appendices in detail. The experimental part is good, using three networks of various sizes (CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet) as benchmarks. The experimental results support the theoretical predictions. \n\nOriginality and significance:\nThe paper seems original, at least the authors claim that no such work has been done before, despite the large amount work done on weight quantization, bit reduction techniques, etc. The paper may have some significance, since most earlier papers have not considered the statistical properties of the reduced precision calculations.\n\nPros:\n* Interesting topic\n* Theoretical predictions match the practical experiments\n\nCons:\n* Nothing particular\n\nMinor:\n* Fig 5a. The curve for m_acc = 13 does not seem to follow the same pattern as the other curves. Why?\n* Motivate why you have selected the networks that you have in the evaluation.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}