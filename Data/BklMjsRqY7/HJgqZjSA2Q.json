{"title": "Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks", "review": "The authors conduct a thorough analysis of the numeric precision required for the accumulation operations in neural network training. The analysis is based on Variance Retention Ratio (VRR), and authors show the theoretical impact of reducing the number of bits in the floating point accumulator. And through extensive benchmarks with popular vision models, the authors demonstrate the practical performance of their theoretical analysis.\n\nThere are several points that I am not particularly clear about this work:\n\n1) In section 4.4, the authors claim to use v(n) < 50 as the cutoff of suitability. This is somewhat arbitrary. As one can imagine, for an arbitrary model of VRR, we can find an empirical cutoff that seems to match benchmarks tightly. Or put it another way, this is a hyperparameter that the authors can tune to match their chosen benchmarks. It would be more interesting to see a detailed study on this cutoff on multiple datasets.\n\n2) Again the 0.5% accuracy cutoff from baseline in the experiment section is also another similar hyperparameter.\n\nIt would be more convincing if we can see a fuller picture of the training dynamics without these two hyperparameters clouding the big picture.\n\nHaving said this, I appreciate the authors' effort in formally studying this problem.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}