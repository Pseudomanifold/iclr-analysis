{"title": "A simple regularization to solve a representation degeneration problem", "review": "This work proposes a simple regularization term which penalize cosine similarity of word embedding parameters in the loss function. The motivation comes from empirical studies of word embedding parameters in three tasks, translation, word2vec and classification, and showed that the parameters for the translation task are not distributed when compared with other tasks. The problem is hypothesized by the rare word problem especially when parameters are tied for softmax and input embedding, and proposes a cosine similarity regularization. Experiments on English/German show consistent gains over non-regularized loss.\n\nPros:\n\n-  The proposed method is well motivated from empirical studies by visualizing parameters of three tasks, and the analysis on rare words are convincing.\n\n- Good performance in language modeling and translation tasks by incorporating the proposed regularization.\n\nCons:\n\n- The visualization might be slightly miss leading in that the size of classification, e.g., the vocabulary size, is different, e.g., BPE for translation, word for word2vec and categories of MNIST. I'd also like to see visualization for comparable experiments, e.g., language modeling with or without tied parameters.\n\n- Given that BPE is used in translation, the analysis might not hold since rare words would not occur very frequently, and thus, the gain might come from other factors, e.g., tied source/target embedding parameters in Transformer.\n\n- I'd like to see experiments under un-tided parameters with the proposed regularization.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}