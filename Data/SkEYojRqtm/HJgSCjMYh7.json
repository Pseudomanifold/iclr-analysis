{"title": "A new understanding of word embedding in LM and NMT", "review": "The authors propose a new understanding of word embedding in natural language generation tasks like language model and neural machine translation. \nThe paper is clear and original. The experiment results support their argument. \n\nThe problem they raised is quite interesting, however, it is not clear why the representation degeneration problem is important in language generation performance. In Figure 1, the classification is from MNIST, which is much different from words. The authors might want to explain more clearly why the uniformly distributed singular values are helpful in language generation tasks. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}