{"title": "Intriguing idea, strong performance, but missing empirical results to validate intuition", "review": "This paper proposes to feed the representations of various external \"teacher\" neural networks of a particular example as inputs to various layers of a student network. \nThe idea is quite intriguing and performs very well empirically, and the paper is also well written.  While I view the performance experiments as extremely thorough, I believe the paper could possibly use some additional ablation-style experiments just to verify the method actually operates as one intuitively thinks it should.   \n\nOther Comments:\n\n- Did you verify that in Table 3, the p_w values for the teachers trained on the more-relevant C10/C100 dataset are higher than the p_w value for the teacher trained on the SVHN data?  It would be interesting to see the plots of these p_w over the course of training (similar to Fig 1c) to verify this method actually operates as one intuitively believes it should.\n\n- Integrating the teacher-network representations into various hidden layers of the student network might also be considered some form of neural architecture search (NAS)  (by including parts of the teacher network into the student architecture). \nSee for example the DARTS paper: https://arxiv.org/abs/1806.09055\nwhich similarly employs mixtures of potential connections.  \nUnder this NAS perspective, the dependence loss subsequently distills the optimal architecture network back into the student network architecture.\n\nHave you verified that this method is not just doing NAS, by for example, providing a small student network with a few teacher networks that haven't been trained at all? (i.e. should not permit any knowledge flow)\n\n- Have the authors considered training the teacher networks jointly with the student? This could be viewed as teachers learning how to improve their knowledge flow (although might require large amounts of memory depending on the size of the teacher networks).\n\n- Suppose we have an L-layer student network and T M-layer teacher networks.\nDoes this imply we have to consider O(L*M*T) additional weight matrices Q?\nCan you comment on the memory requirements?\n\n- The teacher-student setup should be made more clear in Tables 1 and 2 captions (took me some time to comprehend).\n\n- The second and third paragraphs are redundant given the Related Work section that appears later on. I would like to see these redundancies minimized and the freed up  space used to include more results from the Appendix in the main text. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}