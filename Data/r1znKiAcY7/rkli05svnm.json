{"title": "Idea is reasonable; work is preliminary", "review": "Edited: I raised the score by 1 point after the authors revised the paper significantly.\n\n--------------------------------------------\n\nThis paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.\n\nThe approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.\n\n1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.\n\n2. In Section 2.2 the authors write \"... framework is shown in Figure X\" without even showing the figure.\n\n3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.\n\n4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: \"vertices which satisfies\", \"makes W be affected\", \"the some strong baseline methods\", \"a set research papers\", and \"in align with\". The authors are suggested to do a thorough proofreading.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}