{"title": "Interesting paper covering proofs as well as experiments on a newly defined unbiased risk estimator for unlabeled classification", "review": "This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method.\n\npros:\n\n- The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution.\n- The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years.\n- The large literature on the subject has been well covered in the introduction.\n- The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point.\n- The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. \n\nremarks:\n\n- part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that?\n- part 4.2 : the consistency part is too condensed and not clear enough.\n- experiments : what about computation time?\n- More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets?\n\nminor comments:\n- part 1: 'but also IN weakly-supervised learning'\n- part 2. related work : post- precessing --> post-processing\n- part 2. related work : it is proven THAT the minimal number of U sets...\n- part 2. related work : In fact, these two are fairly different --> not clear, did you mean 'Actually, ..' ?\n- part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing.\n- part 5.1 Analysis of moving ... closer: ... is exactly THE same as before.\n- part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}