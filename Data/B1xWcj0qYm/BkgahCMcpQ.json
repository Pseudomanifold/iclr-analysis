{"title": "thorough review of material and refinement that surpasses the state-of-the-art; needs some more development on real-world experiment or justified statement to defer issues until later paper", "review": "Summary: \nThe authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation.  \n\nFurther details:\nWhile the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed:\n- How does the natural extension of UU learning extend beyond the binary setting? \n- As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. \n- In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. \n- In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the \"arbitrary binary classifier\" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. \n\nMinor issues: \n-At the bottom of page 3 the authors state, \" In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen & Williamson (2018). \" It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. \n- In the first sentence of Section 3.1 \"imagining\" is mistyped as \"imaging.\"\n- What does \"classifier-calibrated\" mean in Section 3.1? \n- In Section 3.1, \"That is why by choosing a model G, g\u2217 = arg ming\u2208G R(g) is changed as the target to which\" was a bit unclear at first. The phrase \"is changed as the target to which\" was confusing because of the phrasing. Upon second read, the meaning was clear. \n- In the introduction it was stated \"impossibility is a proof by contradiction, and the possibility is a proof by construction.\" It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed.\n- In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. \n- Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}