{"title": "Interesting approach but questionable motivation and inadequate evaluation", "review": "\nSummary:\nThis paper proposes a deep clustering approach that learns a low-dimensional embedding of the data simultaneously while clustering data using a deep neural network. An autoencoder framework is used to learn embeddings and a joint loss term is used to couple the clustering with the AE loss. Their results on MNIST and newsgroups dataset compare with a couple of other deep clustering approaches and show modest improvements in clustering performance.\n\nClarity:\nThe write-up is reasonably clear, but in print the equations are rendered poorly. Some suggestions for improvements are described later.\n\nOriginality:\n- The idea of learning embeddings while learning a clustering is not novel and doing this using a deep NN is not novel either. But incorporating an AE component is novel. \n- The method builds on the overall idea behind an earlier approach, DEC (ICML, 2016) but has one key difference -- a group of autoencoders (AE) is trained simultaneously with the DNN and used to \"weigh\" the clustering loss\n- The other difference from DEC is in that their deep network is the \"encoder\" part of a SAE whereas in this paper, the deep network seems to be a general DNN. \n\nSignificance:\n- Clustering is an important problem and the authors show some results, but on only three datasets: MNIST, newsgroups, Reuters. They should have included the STL dataset from prior work to make it more comparable.\n\nMain concerns:\n- What is the state-of-the-art on MNIST and their other datasets? To be convincing, they should compare with that algorithm rather than just prior deep clustering approaches. (Note: k-means is not the state-of-the-art method for clustering many datasets).\n- The motivation behind learning the AEs is not clear as the learned embedding is not used anywhere in the clustering. Instead, the reconstruction loss is used to influence the clustering loss by acting as a \"weight\". Does this \"couple\" the two components in a desirable manner? It is not intuitive why a wrongly clustered point will always produce a change to h(xi), rather than to the AE part of the objective? That is, the corresp AE might \"accommodate\" the wrongly clustered point rather than the clustering modifying. This is a standard problem with non-convex functions involving products of functions/parameters.\n- Is the purpose of the AEs just as a means of regularization?\n- The authors should report standard error for experiments by trying different train/test splits. This standard ML practice of giving an average performance is being skipped by many papers these days and should be taken seriously for the results to be convincing.\n- There is no mention or citation of any of the Variational AE based clustering methods. There are methods using Gaussian mixtures of AEs. These should be cited and compared to (GMVAE, VaDE, DLGMM etc)\n\n\nOther comments:\n- For the architecture of the deep network used to train the non-linear representation, have they tried any other architectures or CNNs?\n- The DAE expertise section is not surprising at all. Since the initialization was based on an already clustered set of images, one would expect each DAE to have learned a representation of that digit.\n- How will their method perform with noisy data and outliers? Have they tried corrupting the images to see if the approach is robust?\n- Details of the DNN should be described early on and the choice of the particular size explained.\n- In addition to the shown clustering metrics, another useful metric is AUC-PR (AUC precision-recall curve). Here, precision and recall are computed by considering pairs of data-points and the binary classification task of whether the two belong to the same class. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}