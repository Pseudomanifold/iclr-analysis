{"title": "interesting but the performance not good enough. ", "review": "The paper presents a deep clustering method which represents each cluster with different auto-encoders. The idea is simple but seems interesting. Benefiting from the neural network, the proposed model works in an end-to-end manner. It also can be used to cluster new incoming data without redoing the whole clustering procedure. The paper is clearly written and some experiments are conducted. However, I have some concerns as below:\n\n1. The theoretical detail should be given on why no trivial solution will be given.\n2. In experiments, the proposed method outperforms the compared methods. However, I still feel that the experimental comparison may be the biggest disadvantage of this paper as the reported results are remarkably worse than the state-of-the-art results, e.g. VaDe, SpectralNet, etc. which has achieved >91% ACC/NMI on mnist.\n3. It is unclear why the proposed method makes sense. Comparing with recent developments such as VaDe, I think that the novelty of this work may be on the borderline of ICLR. \n3. Some writing mistakes: \n   - Table 1: The Deep** Autoencodr MIxture** Clustering (DAMIC) algorithm.\n   - It seems that the diagram (a) (b) in Fig.4 is incomplete.\n4. DAE#1 and DAE#9 achieve a similar result, could explain more and check your result? Does this indicates that the proposed model cannot cluster '1' and '9' correctly?\n\n[1] Variational deep embedding: A generative approach to clustering\n[2] SpectralNet: Spectral Clustering using Deep Neural Networks\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}