{"title": "Learning to model other static agents in the environment. Compelling idea but limited evaluation.", "review": "The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment. The method builds on imitation learning (behavioural cloning) to model the agent\u2019s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours. Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent\u2019s environment. While learning to model the target agent\u2019s inner state, the RL reward is generated based on the difference of the target agent\u2019s inner state between consecutive time steps.\n\nThe approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines. Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours. It would be highly beneficial to evaluate these aspects. Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness. \n\nOverall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming). One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4). \n\nMinor issues:\n- Reward formulations for the baselines as part of the appendix.\n- Same scale for the y-axes across figures\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}