{"title": "Nice work, more details and some references to previous work needed", "review": "The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator\u2019s policy by carrying out actions eliciting strong changes in the demonstrator\u2019s trajectory. The former is implemented as imitation learning, i.e. policy learning, the latter as curiosity driven RL.\n\nThe authors are encouraged to review some of the related literature on optimal teaching, which also has developed a rich set of approaches to agent modeling, e.g. the work by Patrick Shafto. It may also be relevant to think about the relationship to active learning in IRL. \n\nI am not sure whether I would be able to implement and reproduce the presented work on the basis of the current manuscript including the appendix. It would be very helpful for the community to be able to do so. E.g., details on the the training of the demonstrators, their reward functions, and the behavior tracker. Particularly the \"fusion\" module remains extremely unclear.\n\nOverall, this is a nice paper, despite the fact that the example domains and problems considered are engineered strongly to allow for the proposed algorithm to be useful. Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d. I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society, though. \n\nMinor points:\n\u201cdiffers from this in two folds\u201d\n\u201cby generate queries\u201d\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}