{"title": "Little novelty, experiments do not offer comparison with related work", "review": "The authors consider the problem of learning from positive and unlabeled data in which only a subset of the true positives is labeled. While the common assumption (eg Elkan & Noto, du Plessis et al.) prescribes that the labeled set is picked independently at random from the positive set, this paper assumes that a (positive) example x is more likely to be labeled the more it exhibits positive features: formally, the higher Pr(y=1 | x), the higher Pr(o=1 | x). For instance, in the case of anomaly detection, the more likely an example is anomalous, the more likely it would get manually flagged (labeled) as positive. The authors refer to this assumption as Invariance of Order.\n\nThe proposed method requires the knowledge of the positive class prior Pr(y=1), and can be summarized in the following three steps: (i) estimate r(x)=Pr(x | y=1, o=1`)/Pr(x); (ii) find the threshold \\theta such that the number of datapoints x with r(x) > \\theta is a fraction Pr(y=1); (iii) train a classifier on sign(r(x) - \\theta). Conceptually, the Invariance of Order assumption allows to use the order on r(x) as a proxy for an order on Pr(y=1|x), so then the knowledge of Pr(y=1) is enough to find \\theta, and to port the original problem to a vanilla binary classification problem.\n\nConcerns:\n- He et al. 2018 use a very similar assumption and no comparison with that work is provided. The authors briefly mention that work in the introduction but don't perform due diligence in assessing differences/novelties with respect to that work, neither as a discussion or in the experiments.\n- The requirement of knowing the fraction of positive examples is hard to justify in practice. Have you tried using the estimate obtained by Elkan et al, or other related work?\n- Experiments are confusing and not convincing: apart from the very last experiment, all datasets are synthetic. No comparison with previous work is presented, except for \"unbiased PU learning (PU)\", which I assume is Elkan et al ? If that is indeed the case, which one of their methods are you comparing against? Even more troublesome is the fact that in all experiments you're providing your algorithm with the correct class-prior Pr(y=1), but it's not clear if this is provided to PU as well. You may want to consider estimating Pr(y=1) using methods from related work to see how it affects the accuracy.\n- Related work discussion is completely missing apart from one paragraph in the introduction.\n\nMinor:\n- The acronym SCR is not very conventional; I would suggest IID which is often used as shorthand for independently identically distributed.\n- Invariance of Order: when introducing it, you may want to add a sentence providing the intuition behind the assumption.\n- Example 2 (Face recognition) is not very convincing and not very clear. Please rephrase.\n- Pseudo-classification risk: why was the log-loss used? Can other losses be used as well?\n- Theorem 3: add some intuition and explain tradeoff on \\epsilon\n- Experiments section: help the reader by adding a reminder on equations, as it's difficult to flip back and forth to their definitions. Eg, \"we trained a classifier minimizing (4) and (7) with the model (10)\" is difficult to digest and follow.\n- Experiments: confusing commas in {800,1,600,3,200} => {800, 1600, 3200}\n- Too many acronyms and abbreviations.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}