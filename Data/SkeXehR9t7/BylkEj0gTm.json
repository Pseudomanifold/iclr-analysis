{"title": "Interesting work but lacking some organization", "review": "This work proposes an end-to-end graph encoder to sequence decoder model with an attention mechanism in between.\nPros (+) :\n+ Overall, the paper provides a good first step towards flexible end-to-end graph-to-seq models.\n+ Experiments show promising results for the model to be tested in further domains.\nCons (-) :\n- The paper would benefit more motivation and organization.\n\nFurther details below (+ for pros / ~ for suggestions / - for cons):\n\nThe paper could benefit a little more motivation:\n- Mentioning a few tasks in the introduction may not be enough. Explaining why these tasks are important may help. What is the greater problem the authors are trying to solve?\n- Same thing in the experiments, not well motivated, why these three? What characteristics are the authors trying to analyze with each of these tasks?\n\nRephrase the novelty argument:\n- The authors argue to present a \u201cnovel attention mechanism\u201d but the attention mechanism used is not new (Bahdanau 2014 a & b). The fact that it is applied between a sequence decoder and graph node embeddings makes the paper interesting but maybe not novel.\n~ The novelty added by this paper is the \u201cbi-edge-direction\u201c aggregation technique with the exploration of various pooling techniques. This could be emphasized more.\n\nPrevious work:\n~ The Related Work section could mention Graph Attention Networks (https://arxiv.org/abs/1710.10903) as an alternative to the node aggregation strategy.\n\nAggregation variations:\n+ The exploration between the three aggregator architectures is well presented and well reported in experiments.\n~ The two Graph Embedding methods are also well presented, however, I didn\u2019t see them in experiments. Actually, it isn\u2019t clear at all if these are even used since the decoder is attending over node embeddings, not graph embedding\u2026 Could benefit a little more explanation\n\nExperiments:\n+ Experiments show some improvement on the proposed tasks compared to a few baselines.\n- The change of baselines between table 1 for the first two tasks and table 2 for the third task is not explained and thus confusing.\n~ There are multiple references to the advantage of using \u201cbi-directional\u201d node embeddings, but it is not clear from the description of each task where the edge direction comes from. A better explanation of each task could help.\n\nResults:\n- Page 9, the \u201cImpact of Attention Mechanism\u201d is discussed but no experimental result is shown to support these claims.\n\n\nSome editing notes:\n(1) Page 1, in the intro, when saying \u201cseq2seq are excellent for NMT, NLG, Speech Reco, and drug discovery\u201d: this last example breaks the logical structure of the sentence because it has nothing to do with NLP.\n(2) Page 1, in the intro, when saying that \u201c<...> a network can only be applied to sequential inputs\u201d: replace network by seq2seq models to be exact.\n(3) Typo on page 3, in paragraph \u201cNeural Networks on Graphs\u201d, on 8th line \u201cusig\u201d -> \u201cusing\u201d\n(4) Page 3, in paragraph \u201cNeural Networks on Graphs\u201d, the following sentence: \u201cAn extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs.\u201d is missing some information, like a reference, or a proof in Appendix, or something else\u2026\n(5) Page 9, the last section of the \u201cImpact of Hop Size\u201d paragraph talks about the impact of the attention strategy. This should be moved to the next paragraph which discusses attention.\n(6) Some references are duplicates:\n|_ Hamilton 2017 a & c\n|_ Bahdanau 2014 a & b\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}