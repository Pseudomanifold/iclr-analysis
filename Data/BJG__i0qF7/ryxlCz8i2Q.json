{"title": "Good paper but needs better positioning and presentation", "review": "This paper presents a system to map natural language descriptions of scenes containing spatial relations to 3D visualizations of the corresponsing scene. The authors collect a dataset of different scenes containing objects of varying shapes and colors, along with several descriptions from different viewpoints. They train a model based on the Generative Query Network, to generate scenes conditioned on multiple text descriptions as input, along with associated camera angles. Empirical results using human evaluators demonstrate better performance compared to baselines and the authors perform a good analysis of the model, showing that it learns to ground the meaning of spatial words robustly. \n\nPros:\n1. Well-executed paper, with convincing empirical results on the newly collected dataset. \n2. Nice analysis to demonstrate that the model indeed learns good semantic representations for spatial language. \n\nCons:\n1. The positioning of this paper with respect to recent work is disappointing. In both the Introduction and Related Work sections, the authors talk about dated models for spatial reasoning in language (pre-2012). There have been several pieces of work that have looked at learning multimodal representations for spatial reasoning. These are a few examples:\n   a) Misra, Dipendra, John Langford, and Yoav Artzi. \"Mapping Instructions and Visual Observations to Actions with Reinforcement Learning.\" Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017.\n   b) Michael Janner, Karthik Narasimhan, and Regina Barzilay. \"Representation Learning for Grounded Spatial Reasoning.\" Transactions of the Association of Computational Linguistics 6 (2018): 49-61.\n   c) Paul, Rohan, et al. \"Grounding abstract spatial concepts for language interaction with robots.\" Proceedings of the 26th International Joint Conference on Artificial Intelligence. AAAI Press, 2017.\n   d) Ankit Goyal, Jian Wang, Jia Deng. Think Visually: Question Answering through Virtual Imagery. Annual Meeting of the Association for Computational Linguistics (ACL), 2018 \n\nEven though Gershman & Tenenbaum (2015) demonstrate weaknesses of a specific model, some of the above papers demonstrate models that can understand things like \"A is in front of B\" = \"B is behind A\". A discussion of how this paper relates to some of this prior work, and an empirical comparison (if possible) would be good to have.\n\n2. The introduction reads a bit vague. It would help to clearly state the task considered in this paper upfront i.e. generating 3D scenes from text descriptions at various viewpoints. In the current form, it is hard to understand the task till one arrives at Section 3.\n\nOther comments:\n1. What is the difference between the bar graphs of Figures 5 and 6? Is the one on Figure 5 generated using the sentences (and their transforms) from Gershman & Tenenbaum? If so, how do you handle unseen words that are not present in your training data?\n2. Would be helpful to clearly explain what the red and black arrows represent in Figure 7.\n3. How does the model handle noisy input text i.e. if the object descriptions (shape/color) are off or if some of the input text is incorrect (say a small fraction of the different viewpoints)? \n\n------\nEdit:\nThank you for the author response. Even if you consider the story to be the same across literature (which in this case is not, since the more recent models handle spatial relations that the previous ones failed on), it's still worth doing due diligence to the recent work, especially so that the reader gets a better sense of how to position your work amongst these.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}