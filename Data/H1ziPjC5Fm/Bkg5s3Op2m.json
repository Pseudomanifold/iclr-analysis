{"title": "Some issues", "review": "In this paper, the authors proposed a novel scheme to interpret deep neural networks\u2019 prediction by identifying the most important neurons/activations for each category using a Lasso algorithm.\n\nFirstly, the authors produce a 1-dimensional descriptor for each filter in each convolutional layer for each image. Then these descriptors are concatenated as a new feature vector for this image. A feature selection algorithm (u-Lasso) is then trained to minimize the classification loss between the prediction from the new feature vector and the original prediction from DNN (formula (1)). Finally, the importance of each filter is identified by the weights of the lasso for each category.\n\nThe authors also improved the visual feedback quality over the deconvolution+guided back-propagation methods, and release a new synthetic dataset for benchmarking model explanation.\n\nThe paper is well-written, however, I have several concerns about this paper:\n\n1.      How to verify the importance of the identified relevant features is a problem. In the experiments, the authors removed features in the network by setting their corresponding layer/filter to zero. The authors only compared their method with randomly removing features. And in Fig 4, the differences seem small for ImageNet. The results are not convincing enough to me. It is a bit baffling randomly removing features did almost as well as the proposed approach.\n\n2.      I don't think one should get away with only showing some results from the synthetic dataset without showing any quantitative results on any real datasets. I like the idea of having a synthetic dataset where all the parameters are controllable. However in this case it is very simple and maybe lacking enough distracting features that can really test the capability of the algorithm. I would believe quantitative results on a realistic dataset are still necessary for the pubilcation of this paper.\n\n3.      Recently several papers pointed out some significant issues in Guided BP, \n\nXie et al. A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations. ICML 2018\nAdebayo et al. Sanity Checks for Saliency Maps. NIPS 2018\nKindermans et al. The (Un)reliability of saliency methods. NIPS workshop 2017\n\ncan the authors comment on that? Based on those papers I don't seem to think Guided BP is actually doing anything that is relevant to the classification, but is just finding prominent gradients. This, unfortunately would lead to reasonably good behavior on the synthetic dataset created by the authors. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}