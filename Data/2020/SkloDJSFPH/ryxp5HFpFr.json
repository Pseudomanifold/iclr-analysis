{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the sequential limitation of autoregressive model when doing sampling. Specifically, instead of sampling next observations in a sequential fashion, this paper generates future observations in parallel, with the help of i.i.d. future priors. With the help of learned confidence model, the model is able to get trade-off between speed and approximation accuracy. Experiments on synthetic data and image generation with PixelCNN++ show the comparable results while being significantly faster than baseline.\n\nOverall the paper is well motivated, with an interesting design of the variational distribution to approximate the true autoregressive distribution. The design of the confidence model looks a bit heuristic, but the trade-off ability between efficiency and quality it brings is also quite interesting. \n\nBelow are some minor comments:\n\n1. The theoretical analysis is basically comment about the objective which is less interesting. However more interesting guarantees would be: 1) with the additional correction term added, how would it help with reducing the variance; 2) As the q_{\\theta, \\phi} is always in a limited form due to the parallelism requirement, how bad the approximation could be in the worst case ---- I\u2019m not asking for these results, but any form of discussion would be helpful.\n\n2. The author only compared with the raw PixelCNN++. Would any of the existing AR-speedup method be applicable for a comparison? \n"}