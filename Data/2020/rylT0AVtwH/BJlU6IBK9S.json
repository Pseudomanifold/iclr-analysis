{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposed variational selective autoencoders (VSAE) to learn from partially-observed multimodal data.  Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws. See below for detailed comments.\n\n[Pros]\n1. The main idea of the paper is to propose a generative model that can handle partially-observed multimodal data during training. Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available. Especially in the field of multimodal learning, we often face the issue of imperfect sensors. This line of work should be encouraged. \n\n2. In my opinion, the idea is elegant. The way the author handles the missingness is by introducing an auxiliary binary random variable (the mask) for it. Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated.\n[Cons]\n\n1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data. Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors. It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))? One possible explanation is because we want to handle the partially-observable issues from multimodal data, and it would be easier to make the latent factors factorized (see Eq. (6)). The author should comment on this. \n\n2. [Phrasing.] There are too many unconcise or informal phrases in the paper. For example, I don't understand what does it mean in \"However, if training data is complete, ..... handle during missing data during test.\" Another example would be the last few paragraphs on page 4; they are very unclear. Also, the author should avoid using the word \"simply\" too often (see the last few paragraphs on page 5). \n\n3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper. I list some instances here. \n\t\ta. In Eq. (3), it surprises me to see the symbol \\epsilon without any explanation. \n\t\tb. In Eq. (6), it also surprises me to see no description of \\phi and \\psi. The author should also add more explanation here, since Eq. (6)  stands a crucial role in the author's method.\n\t\tc. Figure 1 is over-complicated.\n\t\td. What is the metric in Table 1 and 2?  The author never explains. E.g., link to NRMSE and PFC to the Table. \n\t\te. What are the two modalities in Table 2? The author should explain.\n\t\tf. The author completely moved the results of MNIST-SVHN to Supplementary. It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text. \n\t\tg. The author mentioned, in Table , the last two rows serve the upper bound for other methods. While some results are even better than the last two rows. The author should explain this.\n\t\th. Generally speaking, the paper does require a significant effort to polish Section 3 and 4. \n\n4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim. First, I consider the tabular features as multi-feature data and less to be the multimodal data. Second, the synthetic image pairs are not multimodal in nature. These synthetic setting can be used for sanity check, but cannot be the main part of the experiments. The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data. Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4. \n\n\nI do expect the paper be a strong submission after a significant effort in presentation and experimental designs. Therefore, I vote for weak rejection at this moment."}