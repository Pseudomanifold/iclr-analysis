{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, a problem of training DNNs having dead points and neurons was addressed. The problem was posed as a constraint optimization problem. The constraints were augmented to the loss functions of DNNs as a regularizer. The slack variables of the constrained loss functions were optimized together with parameters of DNNs to train DNNs avoiding dead points and neurons.\n\nThe paper was written well in general. However, the work is incomplete in terms of explanation of the details of the proposed method, and their experimental analyses. For instance:\n\n- In the code, you implemented the constraints in a ReLU.  In this case, could you please compare the proposed SeparatingReLU with the other variation of ReLU such as parametric ReLUs etc.\n\n- Could you please analyze convergence of parameters of DNNs together with slack variables during training? \n\n- Do you apply additional constraints on slack variables to control their scale?\n\n- How do the proposed methods scale with larger datasets and networks? Please provide more detailed analyses for training larger/wider DNNs with different structures (such as VGG and ResNets) on larger datasets, such as at least Cifar 100 and ImageNet.\n"}