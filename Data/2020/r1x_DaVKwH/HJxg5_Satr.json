{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an extension to the work of Machado et al. (2018) for standardizing training and evaluation procedures in the Arcade Learning Environment (ALE). It then introduces a collection of human world records for each Atari game to refute previous claims of superhuman performance, as well as recommend comparisons against these records. They proceed to evaluate Rainbow under their proposed evaluation procedures, as well as introduce a new algorithm, Rainbow-IQN, with similar evaluations made based on their proposal.\n\nI'm proposing a weak rejection as I feel some of the arguments made in the paper aren't very strong. In particular, I'd like the authors to comment on the following:\n\n1) The key difference between their evaluation benchmark and the recommendations in Machado et al. (2018) are that episodes should not have a time limit. The justification for this is that many algorithms might achieve practically optimal performance within this time limit, and so one wouldn't be able to compare algorithms on certain games within significance. They further emphasize that human high scores were achieved without limiting to 30 minutes of play. That said, several algorithms performing similarly within said limit can be instead interpreted as shifting emphasis toward comparing performance on the harder games. As the paper acknowledged, removing the maximum episode length ended up introducing more issues, such as the emulator never ending an episode (due to a supposed bug), as well as increasing the likelihood of the score overflowing. The paper suggested a trick of limiting how long an agent can go without receiving a reward, but it's unclear (1) if needing this fix is worth the proposed change, and (2) if the fix introduces additional game-specific nuances in evaluation; e.g., are there any situations where this can be detrimental to properly evaluating performance, or introduce biases based on a game's reward distribution?\n\n2) The paper gathered a list of human world records for the Atari games in the ALE. In my opinion, this is very valuable for the literature in terms of addressing prior work misrepresenting the competency of an algorithm relative to what humans are capable of; a professional game tester is supposed to be representative of the average game player, who is typically tasked with optimizing fun, whereas speedrunners and scorerunners of games are tasked with optimizing a comparable objective to an RL agent. Beyond this though, I think an alternative conclusion would be to use this information in support of not comparing results to human scores, and to focus on comparisons between algorithms. A considerable number of the human world records have reached the maximum allowable score, over drastically variable gameplay times to achieve these scores, that it might still not be that fair a comparison. Have the authors considered this possibility?\n\n3) Were any other algorithms evaluated on this benchmark, beyond Rainbow? While there are computational considerations, it seems odd for a benchmarking-focused paper to only evaluate one standard algorithm and slight modification of it.\n\nSuggestions\n\n1) The introduction of Rainbow-IQN in this paper feels a little random and out of place given the context created by the rest of the paper's contributions- I feel it might be more appropriate for a benchmarking paper to focus on a representative set of \"standard\" or relatively simple/trivial algorithms (Like Machado et al. (2018) did) to give a frame of reference for comparing novel ones."}