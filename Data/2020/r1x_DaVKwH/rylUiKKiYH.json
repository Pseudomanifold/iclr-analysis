{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThis paper proposes a new way to benchmark DRL algorithms using the Atari environment which is twofold, one part is a set of emulator recommendations, the other part is what quantity we should consider as a \"human reference\". The paper also compares Rainbow and Rainbow-IQN, where the IQN improvement matches the proposed human normalized score improvment.\n\nI'm not quite sure how to rate this paper, I have put weak-reject for now, as I don't strongly disagree with anything in the paper, but at the same time:\n- the difference to Machados et al. is marginal, but is a bit surprising\n- the Rainbow-IQN improvement is too incremental to be considered a significant contribution\n- there are some interesting remarks on why Atari is _not_ necessarily a good environment, e.g. most of Section 6, but this clashes with the paper's premise that we should be using Atari.\n\nIn a way, this paper reads like an interesting technical review of Atari, but I don't think it provides enough new knowledge to be a conference paper.\n\nDetailed comments:\n- I find it a bit weird that the many weaknesses of Atari as a platform are presented at DRL being bad at Atari. The line between environment design and algorithm design can be blurry, but in Atari's case, the weird peculiarities of each game are known to make it an inconvenient benchmark.\n- In the same vein, why is Atari+SABER better than other RL environments? This is rather crucial. We should only work on improving a benchmark if it is a useful benchmark, yet, we have many clues that Atari is not.\n- The link to TwinGalaxies should be a proper reference with the time of visit, especially if humans break new records in the future.\n- Why only compare Rainbow and a variant of Rainbow? I understand compute resources being a limitation, but at the same time, the reasoning behind having standardized testing is to be able to compare a wide variety of algorithms. This paper would be much stronger if it focused on a few representative games (e.g. one reflex game, one hard exploration game, etc.) and tested these games with a bunch of DRL algorithms. That ranking might reveal something very interesting.\n- The paper is easy to read, but there are a few grammar mistakes here and there."}