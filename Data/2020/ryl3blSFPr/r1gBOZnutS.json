{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presented a denoising adversarial autoencoder for sentence embeddings. The idea is that by introducing perturbations (word omissions, etc) the embeddings are more meaningful and less \"memorized\". Evaluations include measuring sentence perplexity in generation/reconstruction, tense changing via vector arithmetic, sentiment changes via negative/positive vector additions, and sentence interpolations. \n\nStrengths: I thought the idea is nice, and the results do seem to show improvements in a number of interesting tasks. \n\nWeaknesses: I don't really think the explanation, especially in Theorem 1, makes a lot of sense. Qualitatively speaking, it's true that \"memorization\" in autoencoders (where the latent space has a 1-1 mapping with the input space) is problematic when the autoencoders are too powerful, but it is not always the case, and it is too far to say that the probability in theorem 1 is ALWAYS agnostic to encoding. The fact is word2vec works just fine with no perturbations, and there is no mathematical reason why sentence embeddings are fundamentally different. What is more accurate to say is that there is a tradeoff between model complexity and latent space representation usefulness, which is also related to the regularization/overfitting tradeoff in supervised learning. Here, injecting noise in the exact same fashion proposed in this paper is a well-accepted practice. While I think it's interesting that it works well here, I wouldn't really frame it as such a novelty, in that case, and I believe the other works on denoising autoencoders should be compared against in the experiments. In general, I find the mathematical claims a bit dubious, as a main assumption seems to be that the autoencoder itself is so overparametrized that it isn't really functioning as a representation-learning tool anyway. \n\nI also feel that the last experiment (referenced in the appendix) needs to go in the main text if we're to see it as a contribution. There is some wording that can be tightened in the main text, to make more room. \n\nOverall, I would improve my rating if the paper refocused more on the experiments, included more baselines (like other denoising autoencoders) and tasks that measure something besides perplexity (such as actual sentiment prediction, or machine translation, or other somewhat unrelated downstream tasks), and decreased the emphasis on the theoretical analysis--unless there is something I am significantly misunderstanding, it does not seem to be a particularly powerful theoretical contribution. "}