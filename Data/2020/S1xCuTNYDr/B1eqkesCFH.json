{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach for local post-hoc explanation with introduction of a new regularization that helps regulate the \"fidelity\" and \"stability\" of the output explanation (in the style of LIME).  The fidelity regularizer is essentially the squared error of the explainer as compared to the given model in the local neighborhood, whereas the stability regularizer measures the total squared differences between the explanation outcomes between the sample in question and other samples in the local neighborhood. \nIn the experimental evaluation section, the authors evaluates the performance fo the proposed regularizers, used as part of both LIME and MAPLE, against three interpretability metrics: point fidelity, neighborhood fidelity and stability. The results verify that indeed the use of the regularizers improve the performance of both LIME and MAPLE over the unregularized versions, with respect to the corresponding metrics. This is in a way \"expected\", since the regularizer used in the method and that in the metric are closely related, and is an unsatisfying aspect of the work. \nUsing image data, they also demonstrate that qualitatively the use of stability regularizer seems to significantly improve the saliency of the output explanation. \nThe paper is well written, the proposal is reasonable, but the contribution is modest and experimental evaluation is not entirely convincing. \n"}