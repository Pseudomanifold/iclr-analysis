{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a model building off of the generative query network model that takes in as input multiple images, builds a model of the 3D scene, and renders it. This can be trained end to end. The insight of the method is that one can factor the underlying representation into different objects. The system is trained on scenes rendered in mujoco.\n\nSummary of positives:\u00a0\n+The factoring makes sense, and the use of voxels\u00a0+ the physical property to enforce that two objects can't be superimposed in z_pres is a good strategy.\n+There are a number of good qualitative results for understanding the learned object-oriented representation.\n+The approach of learning 3D representations by comparing projections to observations is a good direction.\n\nSummary of negatives:\n- The method is quite complex and explained, in my view poorly (although I'm open to the other reviewers' opinion on the matter).\n- The experiments are weak\n- The manuscript makes fairly broad claims that aren't substantiated and ignores a great deal of work in the vision community on this topic.\u00a0\n\nGiven the three largely orthogonal and fairly strong negatives, I lean heavily towards rejecting this paper. Independently each of these is an issue that would be push me to at least lean towards rejection. However, I would encourage a revision and resubmission with improved method explanation, stronger experiments, and a clearer picture with respect to existing work.\n\nIn more detail:\n\nMethod:\u00a0\n-I found the method section quite difficult to read, in part because the method is quite complex. This isn't intrinsically a bad thing, but complex methods with lots of steps should come with few surprises and descriptions that make the method accessible. In particular, the method section would benefit from a stronger figure that in part introduces the notation, as well as a little more thought in terms of the introduction of the method. A few instances:\u00a0\n1) \"This is done by an order-invariant object encoder r_n = f_{order-invar-obj}(...)\". One turns to the appendix, and tries to find this function. It's not explicitly there -- instead you need to match r_{n,C} = \\sum_{i} .... , then look up above at the note that \"ObjectRepNet is the module we use for object level order invariant encoder\", then remember that sum is order invariant.\u00a0\n2) I searched throughout the paper and couldn't find precisely what model f3d->2d was. The figure suggests a projective camera and the text says \"For more details on the coordinate projection, refer to Appendix.\", but there's none in the appendix as far as I can see.\u00a0\n3) STN-INV is nowhere defined -- inverse spatial transformer?\u00a0\n4) s^{what} doesn't appear to be anywhere in the appendix -- is s^{what} factored into y^att and alpha^{att}? By matching the RHS, this seems to be the only possibility, but in the main body it's called ConvDRAW aka the GQN decoder, but in the appendix it's called Renderer.\u00a0 \u00a0\n5) There are lots of other little things -- e.g., a figure that refers to a variable that doesn't exist (see small details section )\n\nI don't see why a paper this complex can't be accepted at ICLR, but I think at a minimum, the appendix should be more complete so that things are well-defined. I'm open to the possibility that I may just be slow so long as the other reviewers think the paper is crystal clear down to the details. However, I think even if I'm just the slow one, the authors should think about writing this more clearly and using consistent notation and function names.\u00a0\n\nExperiments:\n-As far as I can see, the difference between\u00a0ROOTS and GQN is that GQN is a little more blurry in its output (Figure 2) and ROOTS has a slightly better MSE for lots of objects (Table 1) but produces NLLs similar to GQN. There are a few issues with this:\n(a) It's surprising that the correlation between larger numbers of objects and better MSE isn't really investigated -- why not show that GQN breaks at some point?\u00a0The differences right now are fairly small, and I think the paper ought to delve into details to demonstrate that the differences are important.\n(b) There are so many changes between ROOTS and GQN that I don't know if this has to do with the object-bits of it, or something else. This is part of a larger problem where there are no ablations. A large complex system is trained, and lots of changes are made to GQN. But when there are no ablations, it's unclear what parts of the changes are important and which parts aren't.\n(c) It's not clear whether the GQN and ROOTS are being trained fairly -- do they have the same capacity? Why are they both trained for the same number of epochs? It seems entirely likely that ROOTS may train faster than GQN (or the reverse!). If there's only one experiment like this, why not train for a long enough time to ensure convergence and then take the checkpoint with best validation performance?\u00a0\n-The NLL results are very weak and probably not worth putting in, at least without some sort of explanation for why this gap is significant.\n-The object detection quality experiment is incomplete -- I just do not know how to parse the numbers that are presented without some sort of simple baseline in order to make sense of things. Why not also try something like this on GQN?\u00a0\n-The qualitative experiments are nice but would be substantially improved by showing that:(a) that GQN doesn't do any of these\u00a0(b) that ROOTs can train on 2-3 objects and test on 3-5 objects by simply changing the prior on K -- this is one of the primary advantages of object-centric representations of scenes (the ability to handle arbitrary numbers of objects).\u00a0\n\n\nRelated work:\n\nThe paper really needs to make its claims more specific and position itself better with respect to related work. \n\nTwo gratuitous\u00a0examples:\u00a0\n1) The title is \"Object-oriented representation of 3D scenes\", which covers decades of work in robotics and vision. This title should be changed. \n\n2) \"First unsupervised\u00a0model that can identify objects in a 3D scene\" is exceptionally broad: voxel segmentation is already a standard feature in point cloud libraries (e.g., \"Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds\" Papon et al CVPR 2013). Is the manuscript and the Papon paper the same at all? No. But are they both unsupervised models that can identify objects in scenes. I'm not demanding that the authors write out a claim of novelty that's like a patent, but claiming \"first unsupervised model that can identify objects in a 3D scene\" is, in my opinion, clearly incorrect and needs to be qualified.\n\n\nThe paper should also better position itself compared to the wide variety of work that's been done on unsupervised 3D shape estimation/feature learning using reprojection. For instance (among many): \n(1) Geometry-Aware Recurrent Neural Networks for Active Visual Recognition. Cheng et al. NeurIPS 2018\n(2)\u00a0Learning a multi-view stereo machine. Kar et al. NeurIPS 2017.\n(3)\u00a0Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency. Tulsiani et al. 2017.(4)\u00a0Unsupervised Learning of Depth and Ego-Motion from Video. Zhou et al. CVPR 2017 (not voxels, but 2.5D or a form of 3D)\n\nas well as the vast array of work on 3D reconstruction, including work that is object-oriented\n(1)\u00a0Learning to Exploit Stability for 3D Scene Parsing. Du et al. NeurIPS 2018.\n(2)\u00a0Cooperative Holistic Scene Understanding: Unifying3D Object, Layout, and Camera Pose Estimation. Huang et al. NeurIPS 2018\n(3)\u00a0Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene, Tulsiani et al. CVPR 2018\n(4) Potentially not out at ICLR submission deadline, but\u00a03D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers. Shin et al. ICCV 2019.\n\nI agree that there are differences between these works and the manuscript, but it's really peculiar to work on inferring a 3D volume of scenes from a 2D image or set of images, and only cite YOLO, faster RCNN, and FCNs from the world of CVPR/ICCV/ECCV etc where this work is done very frequently. These works do indeed sometimes rely on a bit more supervision (but not always). But they're tested on data that's far more complex than a set of spheres and cubes.\n\n\n\nSmall issues that do not affect my score.\n- The claim that the method is unsupervised when it has access to precise camera poses seems a bit like a stretch to me. It's common enough that I've given up quibbling about it. Peoples' sense of distance is not exact. This deserves some further thought.\n-The authors should go through and double check their use of GQN and CGQN -- it's said at the beginning that GQN just means CGQN, but then it's occasionally dropped (e.g., right before Table 1)\n- Fig 1 shows z^{where}, which I guess got renamed?\n- \"The Scene Representation Network is modified bases on Representation Network in GQN.\" -> this sentence is presumably a typo/cut off halfway through.\n- \" This helps remove the sequential object processing as dealing with an object does not need to consider other objects if the features are already from spatially distant areas.\" -> this is unclear\n- Eqn 11 appendix \"sacle\" -> \"scale\"\n- \"Object Representation Network\" in A.2 \"objcet\" -> \"object\"\n-Equation 15 -- the parentheses put i \\in \\mathcal{I}(D) inside the Renderer, which is presumably not true.\n-Table 1 -- table captions go on top"}