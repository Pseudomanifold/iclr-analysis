{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2400", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a framework for 3D representation learning from images of 2D scenes. The proposed architecture, which the authors call ROOTS (Representation of Object-Oriented Three-dimension Scenes), is based on the CGQN (Consistent Generative Query Networks) network. The paper provides 2 modifications. The representation is 1. factorized to differentiate objects and background and 2. hierarchical to first have a view point invariant 3D representation and then a view-point dependent 2D representation. Qualitative and qualitative experiments are performed using the MuJoCo physics simulator [1] (please add citation in the paper).  \n\n[1]Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based\ncontrol. In ICIRS, 2012.\n\n+Learning 3D representations from 2D images is an important problem. \n+The proposed methodology learns representations that are more interpretable, with higher compositionally. \n\nWhile the paper takes a step towards a potentially impactful work, I cannot recommend it for publication in its current form. \n\n1. There are claims in the paper that are not supported by the experiments. For example, \u201cAs seen in Figure 2, ROOTS has clearer generations than GQN. \u201d However, Figure 2 does not show this at all. It shows no difference between ROOTS and GQN. \n\n2. The paper can benefit from further clarity throughout\u2014in general it seems a bit rushed. For example, the caption on Figure 3 reads \u201cFor example, GoodNet segments a scene into foreground and background first and decompose foreground into each individual object further\u2026\u201d The text has not discussed what GoodNet is. Its also unclear what is depicted in each of the columns in Figure 3. This should be clearly explained. \n\n3. I suggest clarifying Figure 1 further and referring to it in section 3. Its currently not referred in the text although it is an overview of the proposed architecture. \n\n\nOther comments\n-Table 2: Why not have a precision-recall curve (as is standard) and report average precision numbers?\n-Table 2: Why not compare to CGQN?\n\nMinor\n-There are typos throughout the text. "}