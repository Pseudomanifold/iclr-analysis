{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper combines CapsuleNetworks and GCNNs with a novel formulation. First they modify the CapsNet formulation by replacing the linear transformation between two capsule layers with a group convolution. Second they share the group equivarient convolution filters per all capsules of the lower layer.  Third, they change the similarity metric from a lower-upper similarity into a pairwise lower similarity and aggregation which makes it keep the equivarience. Since the cij does not depend on upper capsule anymore they only perform 1 routing iteration (no modification of the routing factors).\n\nOne assumption in CapsNets is that each part belongs to one whole. Therefore, the normalization in Alg.2 usually is division by degree^k_i. The proposed normalization formula for c_ij seems to encourage that each upper capsule only receives one part. Is this a typo or is there a justification for this?\n\nThe discussion on ideal graph on page 5 is interesting. But the points made are not used later on. I expected the results to have an analysis or at least a show case that indeed if you transform the resultant graphs stay isomorphic. \n\nOne goal for CapsuleNetworks vs GCNNs is the hope for handling different transformations and not only rotations that one can grid with group convolutions. But, the experiments only report on rotation, translation as a transformation. Reporting results by training on MNIST, testing on AFFNIST could shed light on this aspect of SOVNETs.\n\nConditioned that the last two points will be addressed in the rebuttal I vote for accepting this paper since they suggest a novel formulation that brings some measures of rotation equivarience guarantee into CapsNets. Also their results suggest that there is no need for per Capsule filter bank and several refinements to get rotation robustness (it would be interesting to check the performance of a simple capsnet with shared parameters).  In the appendix there is a comparison with GCNNs on fashion MNIST which shows they have better performance than GCNNs. I would advise reporting GCNNs for all the experiments in the main paper. "}