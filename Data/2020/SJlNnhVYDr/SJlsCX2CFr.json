{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input. Furthermore, for training, PARCUS makes use of rationales. Those are indicators of input importance, and help to boost the loss for relevant tokens.\n\nThe main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case. This is due to it having relatively few parameters and to it having a strong inductive bias. However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising. Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.\n\nAnother selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have.\n\nOverall, the paper seems solid."}