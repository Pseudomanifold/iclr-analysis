{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission proposes NORML, a meta-learning method that 1) learns initial parameters for a base model that leads to good few-shot learning performance and 2) where a recurrent neural network (LSTM) is used to control the learning updates on a small support set for a given task. The method is derived specifically for full connected neural networks, where the meta-learner produces gating factors on the normal gradients (one for each neuron that the parameter is connecting). The method is compared with various published few-shot learning methods on miniImageNet, and an ablation study and detailed comparison with MAML is presented on Omniglot.\n\nUnfortunately, I believe this submission should be rejected, for the following reasons:\n\n1. Limited novelty: it is a fairly incremental variation compared to the Meta-Learner LSTM (Ravi & Larochelle). I actually kind of like the proposed variant, but then I would at least expect a direct comparison with Meta-Learner LSTM. And though the authors try to make a case for why NORML is better than Meta-Learner LSTM, unfortunately they don't actually provide a fair comparison. Indeed, the results for Meta-Learner LSTM on miniImageNet use a very different base learner (i.e. not a ResNet-12) which wasn't pretrained. Same in fact can be said about many of the results reported in Table 1.\n\n2. Missing baselines: though the comparison in Table 2 with MAML is a good step, I actually believe the 3 versions of MAML considered aren't appropriate. Instead, I believe that 1) at a minimum, a comparison with a version of MAML where a separate learning rate is learned \"per hidden unit\" AND \"per inner loop step\" is necessary, as it is closer to what NORML can achieve, and 2) a comparison with MAML++ (Antoniou et al.) would be ideal. Finally, I think this study should also be done on miniImageNet, not only Omniglot.\n\n3. Limited applicability: NORML assumes that the base learner is a fully connected network. That strongly limits the applicability of the method, given that most few-shot learners usually have convolutional layers.\n\n4. Inaccurate descriptions of some prior work: The related work includes certain statements that I believe aren't accurate. For example, it is stated that the Meta-Learner LSTM requires \"an enormous number of parameters\". I believe this is not true: there is a single, small LSTM that is used for all parameters of the base learner (however, the authors are right that running this LSTM requires a very large cell state and input size, since the batch size of the Meta-Learner LSTM is essentially the number of parameters of the base learner). Similarly, it is stated that Andrychowicz et al. proposes to \"individually [optimize] each parameter using a separate LSTM\". I believe this is also not true, and that a single LSTM is used for all parameters.  In fact, ironically, NORML on the contrary appears to be using different LSTMs for each layer of the base learner (at least based on Equations 14-19, where the LSTM parameters are indexed by layer ID l, thus implying different LSTMs), unlike what is claimed in the Relate Work section (\"this work uses a single meta-learner\").\n\nFor me to consider increasing my rating, I would expect all points above to be addressed. \n\nFinally, here are a few minor issues I've found:\n- In Equation 16, \"b_{i,\\tilde{c}}\" should be \"b_{l,\\tilde{c}}\" (i.e. i should be l)\n- The submission doesn't mention whether gradients are propagated into the base learner gradients (i.e. whether a first-order version of the method is used)\n- 4th paragraph of the Relate Work section has a missing reference (\"?\")\n- Table 2 is missing confidence intervals"}