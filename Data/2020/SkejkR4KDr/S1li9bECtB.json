{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes to use adaptive computation time (ACT) for the number of layers (instead of the number of timesteps in the original paper) for a RNN. The intuition is that some inputs are more complex and may need more processing compare to other inputs. \n\nComments on the paper,\n\n-  I am slightly confused over figure 2. For example, fig 2.c is the model performance over different steps. Does the number of steps here mean the number of time-steps per sequence? It seems that the performance of the proposed model and the RNN baseline matches at step 13. It seems to be natural to run RNNs over a sequence length of 13. it is not clear to me what is the advantage of using the proposed model in this case. \n\n- It is also unclear to me how many layers does the RNN baseline have and how the results changes wrt the number of the layers for the baselines, i would imagine that this would get better as the number of layers increases. Can the authors compare the proposed model to RNN baselines trained with different number of layers?\n\n\n- It is unclear to me what RNN-based models are, it seems that they are GRU models reading from section 4.1, but it seems to be implicit. Is there a reason to use GRUs compared to LSTMs? Do they achieve similar results?\n\n- I am not sure the proposed datasets have been picked. Is there a reason why each dataset is picked, what would the authors expect to see (hypothesis to test) from each dataset?\n\n- Some analytical experiments to help better understand the model would be nice. For example, some datasets would need more computational resources for some steps, but not others. Would the model be able to learn and pick that up? A really simple example if the copy task, would the model to learn to just have minimal layers for the 0's that does not contain information and would it use more layers for the steps that needs more processings (the non-zeros for example).\n\n\nMinor comments:\n\n1. some typos. P6, section 4.1, \"length\" was incorrectly spelled.\n\n"}