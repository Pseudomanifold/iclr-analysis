{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nThe authors propose Layer Flexible Adaptive Computation Time, an RNN-esque sequence model with varying depth at each time step. The idea is that the model can adaptively choose how much computational effort to spend on each example. The authors evaluate the model empirically on a financial dataset and Wikipedia language modeling, and find that it outperforms a vanilla RNN and the original adaptive computation time (ACT) model.\n\nUnfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking. These weaknesses lead me to vote for a weak reject.\n\nI address specific clarity points below.\n\nIn regards to the novelty claim, there have been several developments of depth-based (as opposed to time-based) adaptive computation time in the literature, for example:\n\n[1] McGill et al 2017 \"Deciding How to Decide: Dynamic Routing in Artificial Neural Networks\"\n[2] Bolukbasi et al 2017 \"Adaptive Neural Networks for Efficient Inference\"\n[3] Figurnov et al 2017 \"Spatially Adaptive Computation Time for Residual Networks\" (this paper is cited by the authors)\n\nThese papers do not present sequence models, but the ideas in them readily apply to sequence models. Thus, the main novelty in the authors' paper is handling the different number of hidden states at each timestep via their 'attention mechanism'. While this is definitely a contribution, the unclear presentation and lacking experimental evaluation combine to decrease the value of the paper.\n\nExperimentally, the authors evaluate on a financial time-series dataset and Wikipedia language modeling. They compare to Adaptive computation time and a standard RNN. While the experiments demonstrate a modest improvement over ACT and an RNN, they do not compare on larger, more standard datasets such as the WMT datasets, etc... Additionally, they do not compare with other, more powerful models. Both are required to thoroughly demonstrate their model's effectiveness.\n\nTo change my mind the authors would have to (in order of importance):\n1) Add experiments on WMT or other bigger datasets and compare with current SOTA models.\n2) Thoroughly edit their paper for clarity (specific points below).\n\nThe authors may want also add explorations of how much computation time can be saved using their model versus others, as this is very common in the ACT-esque literature.\n\nSpecific points:\n* While the original ACT paper does use 'mean-field' to denote the convex combination of states at a current timestep, that term has a specific technical meaning different from how it is used here. I would suggest using a different word.\n* The introduction is too long and repeats itself in several places.\n* There are repeated citations in the literature review.\n* In figure 1 sometimes nodes denote functions and sometimes nodes denote outputs. Sometimes the nodes are round and sometimes they are rectangular. Sometimes arrows denote inputs to a function, and sometimes they denote multiplication. These inconsistencies make the figure very hard to decipher.\n* The text description of your model is confusing. Specifically, distinguishing between the functions of u_t, \\hat{u}_t and \\overline{u}_t was difficult.\n* In your experiments section the plots are very difficult to interpret because they are phrase as 'improvement over x'. The standard presentation is a table of absolute results. Furthermore, bar charts can be misleading because the scale can make improvements seem bigger than they actually are. The figures should stand alone without having to read the text.\n\n"}