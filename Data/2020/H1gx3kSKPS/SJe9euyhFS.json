{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a training objective that combines three terms:\n* A Stein discrepancy for learning a energy model with intractable normalizing constant\n* A Wasserstein GAN objective for learning an implicit neural sampler.\n* A Stein discrepancy for minimizing the distance between distributions defined by the energy model and the GAN.\n\nThe third term is called \"Stein bridging\" by the authors. It seems pretty difficult to motivate such a bridging term because from the first glance this term does not add anything to learning the two models from data. So I'm wondering how the authors motivate themselves to study this modification. This is my main concern about the paper if this term appears simply because that energy model has an unnormalized density while from GANs we can sample, and Stein discrepancy is best applicable to such pair of distributions.\n\nApart from the concern on motivations, I tried to follow the arguments in Section 3, as the bridging term is justified as regularization to both models. However, I think the proof of Theorem 1 is incorrect:\n\n* I don't think it makes sense from \\nabla \\log (1 + h(x)) to (1 + h(x))\\nabla h(x), even if the taylor expansion suggested by the authors is applied.\n* In the next step, \"Consider a further approximation\", this approximation basically sets 1 + h(x) to 1, if h(x) is approximately 0, then P=P_E..\n\nMinor:\n* IN proof of Theorem 1, the expectation should be always over x,x'~P_E instead of x~P, right?\n\n\n"}