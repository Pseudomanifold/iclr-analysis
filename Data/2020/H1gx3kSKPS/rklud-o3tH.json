{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to train a GAN and an EBM jointly, and bridge them using a Stein discrepancy. The paper claims it leads to novel regularization effect on both models, and stablizes the optimization process. Experiments on MNIST and CIFAR-10 show improvement in sample quality and outlier detection.\n\nBoth the idea and the experiment results are interesting. However, the derivations contain too many typos and are in general confusing, and I cannot confirm their correctness. Therefore I cannot recommend acceptance.\n\nSpecifically the proof of Theorem 1 seems problematic:\n1. In the proof you claim (15) equals $\\frac{-1}{4\\lambda_2}\\lVert D-t\\rVert_{H^{-1}}$. But (15) could only simplify to \n$\\frac{1}{\\lambda_2} ( E[D\\cdot(\\lambda_2 h)] + E_{x,x'}[\\nabla(\\lambda_2 h(x))^T k(x,x') \\nabla(\\lambda_2h(x'))],$\nwhere h is unconstrained. Compare this with the definition of the $H^{-1}$ norm,\n$sup_h \\{E[D\\cdot h]: E_{x,x'}[\\nabla h(x)^T k(x,x') \\nabla h(x')] \\le 1\\},$\nhow did you drop the inequality constraint on h?\n2. The transformation from the original objective (14) to (15) is strange as well. In the proof you claim the minimization problem below \"invoking Lagrangian duality gives\" could only turn to (15) after \"applying the approximation log(1+a)=a+O(a^2)\" and \"a further approximation\". But you can turn it into\nE[(D-t)h]+\u03bb E_{x,x'~P_E}[\u2207h(x)^T k(x,x') \u2207h(x')]\nsimply by simplifying the gradient terms. Also, why did the $t$ disappear in (15)?\n\nThere are also typos and issues elsewhere. To list a few:\n3. Energy-based models are not generally referred to as \"explicit models\", since the normalization constant is intractable. I would suggest to replace the occurrences of (log) \"density\" with \"energy\" to avoid confusion.\n4. The GD update rule of (6) is incorrect; the optima should also be (0, 1), instead of (1, 0).\n5. On the second line on Page 8, the unnormalized log density cannot be x^2+\\phi x, as the normalization constant would then be infinity.\n\nFor these reasons, I believe this paper needs a thorough proofreading before it can be reviewed efficiently."}