{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper builds on recent developments of CNN-GP and CNTKs in multiple fronts obtaining significant performance boost on CIFAR-10 dataset (and some mild boost on Fashion-MNIST). One way is by usage of Local Average Pooling (LAP) layers which interpolates between Global Average Pooling (GAP) and no Pooling layer. The authors also introduce flip data augmentation by doubling the dataset. With the help of additional feature extractor, this paper obtained 89% classification accuracy on CIFAR-10 which is the best among methods not using trained neural networks. \n\nThe discussion on section 4 regarding augmented kernel and data augmentation is quite clear and revealing. It\u2019s unfortunate that the flip augmentation could not be introduced in kernel level. It would be interesting for future work to find kernel operation similar to GAP that encodes symmetries of the dataset. \n\nWhile the paper is clearly written and the results are strong, there are few criticisms I\u2019d like to address and hope the authors address. \n\nAFAIK both GAP and LAP for CNN-GP are already introduced and analyzed in [1]. It seems best results on CIFAR-10 all comes from CNN-GP (with without flip augmentation, with and without using extra feature extractor), and I think the authors should properly credit [1] for GAP/LAP in convolutional kernels. It\u2019s fair that this paper along with [2] was able to efficiently implement and scale up to  full CIFAR-10 dataset and demonstrated pooling layer\u2019s full potential for kernels corresponding to infinitely wide CNNs. Also in this regard the title could be misleading. It\u2019s strange to have paper\u2019s strongest result is based on CNN-GP while the title only mentions CNTK.\n\nAs the author\u2019s mention in the paper, Box Blur is just an average pooling operation. This is already widely use by practitioners(e.g. [3]) and I don\u2019t understand how author\u2019s claim: \u201cThis operation also suggests a new pooling layer for CNNs which we call BBlur\u201d \n\nFew question/comments:\n\nBest parameters for trained CNN\u2019s BBlur c is smaller than best c values for kernels, do authors understand the cause of discrepancy? \n\nIt would benefit the research community if authors could share code to generate the CNN-GP Kernels / CNTKs with LAP.  Also I would encourage authors to share actual numerical values of kernel matrix for other research groups to analyze and encourage reproducibility.\n\n\n[1] Novak et al., Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes, ICLR 2019\n[2] Arora et al., On Exact Computation with an Infinitely Wide Neural Net, NeurIPS 2019\n[3] Huang et al., Densely Connected Convolutional Networks, CVPR 2017\n"}