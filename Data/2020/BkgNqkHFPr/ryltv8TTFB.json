{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers architectures that do not involve learning (up to the classification layer) and tries to improve their accuracies. They're based on CNTK and CNN-GP works. This is purely a numerical paper and its contribution is to show that despite being not learned, the obtained representations are competitive with supervised neural networks.\n\nOverall, despite the fact if I find this numerical result interesting, I found too many flaws to justify its acceptance. (fine tuning on the test set, lack of comparison with the state of the art...) \n\nPros:\n- Good numerical performances.\n\nCons:\n- Given the claim in the abstract about accuracies, it should be pointed out that:\u2028\n* in the unsupervised setting, with a kernel engineering method, you can obtain ~86% on cifar10 (cf https://arxiv.org/abs/1605.06265 )\n* in the no-data(up to a linear model) setting, it is possible to get ~82% on cifar10 with the scattering networks (cf https://arxiv.org/abs/1412.8659 )\nThose two works are also mainly empirical, and thus some accuracies of this paper should be compared to them.\n- There is a significant amount of experiments (table 1/2/3/4). While this should have been a positive aspect of the paper, I noticed that the accuracies reported here are computed from the test set. A validation set should have been used with a careful cross-validation. I'm aware this is a standard practice in deep learning, yet here it seems obvious to me that some hyper parameters have been fine-tuned on the testing set.\n- Section 4: isn't it a rephrasing of (Dao et al, 2018)? (which is cited) I think this should be clearly stated.\n- Section 5: The paper cites the Local Average Pooling as a \"new operation\", but this is clearly standard in the literature. \"Boxblurring\" has always been named average pooling in deep learning, low-pass filtering in signal processing. It was used before researchers employ a stride of 2 in convolutions. A similar pooling is also present in https://arxiv.org/abs/1605.06265  \n- I'm nicely surprised that the authors didn't encounter any significant conditioning issues. Would it be possible to show the spectrum of the kernel? This could be commented.\n- Nothing about the future release of the code is indicated.\n\nMinor:\n- I find the Figure 1 is not informative to the reader."}