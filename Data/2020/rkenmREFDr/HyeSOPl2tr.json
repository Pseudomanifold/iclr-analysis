{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThe paper proposes a scheme to learn space partitions for improved\nnearest neighbor search by first converting the search problem into a\nsupervised classification problem by graph-partitioning the\nk-nearest-neighbor graph, and then using some machine learning model\nto learn space partitions corresponding to the supervised problem. The\nscheme is theoretically motivated and the empirical results\ndemonstrates the utility of the proposed scheme against various\nbaselines.  \n\n\nWhile the paper presents a promising new direction for\nnearest-neighbor search with space partitioning schemes, I am somewhat\non the border, leaning towards reject. This is mostly due to my lack\nof understanding why Cayton & Dasgupta (2007) and Li et al. (2011) are\nnot valid baselines and are dismissed as \"hyperplane\npartitions\". It would be very helpful to me as a reader to understand\nwhy this literature does not warrant to be a valid\nbaseline. Especially given the following connection points:\n\n- Firstly, the theoretical results presented by the authors also\n  limit to hyperplane splits. \n- Secondly, the authors present empirical evaluation of recursive\n  hyperplane partitions where the proposed scheme is the only learning\n  based scheme while the techniques presented in Cayton & Dasgupta\n  (2007) and Li et al. (2011) would have been the learning based\n  partitioning baseline to beat.\n- Thirdly, LSH space partitioning is considered as a baseline in\n  Section 4.2 with the Neural LSH and Neural Catalyzer + k-means being\n  the only learning based baseline; Cayton & Dasgupta (2007)'s learned\n  LSH scheme would have been a valid baseline to the best of my\n  understanding. \n- Finally, the authors mention the need for coupling the graph\n  partitioning and the supervised learning phase. Li et al., 2011\n  utilizes a loss function that essentially promotes a sparse graph\n  cut while preserving near-neighborhoods, providing the closely\n  coupled scheme. In the original paper, the authors utilize a\n  recursive hyperplane partitioning scheme. However, a neural network\n  with a softmax layer can be trained to minimize this closely coupled\n  loss directly, obviating the need for the separate graph\n  partitioning and supervised learning phases proposed in this\n  paper. But it is possible that I have misinterpreted the connection\n  and any clarification here would be very helpful.\n\nMinor:\n\n- It would be very helpful as a reader to understand how sensitive the\n  neural network based partioning scheme is to the network\n  architecture and hyperparameters. Some intuition behind the choices\n  would be very useful.\n- It is a little unclear how the plots in Figure 2 are generated --\n  for each query, we would have access to the accuracy vs. number of\n  candidates curve. In that case, it is unclear how the curves are\n  aggregated across queries to generate average number of candidates\n  (and 0.95 quantile) vs. accuracy curves. Given that not all queries\n  in all the methods (including the proposed ones) will have the same\n  sequences of accuracies, the quantile curve is especially unclear.\n- The authors mention that neural LSH is only useful when the bin\n  sizes are large, and otherwise k-means is more useful. Is there a\n  straightforward way to decide when to switch between the two? Is it\n  another hyperparameter we need to tune over while generating these\n  space partitions?\n- Theorem 3.1 considers only hyperplane partitions while neural\n  networks usually generate nonlinear partitions. What are the\n  conditions on the nonlinear splits that allows the guarantees to\n  improve (or at least not degrade)? Is it obviously always improved?\n"}