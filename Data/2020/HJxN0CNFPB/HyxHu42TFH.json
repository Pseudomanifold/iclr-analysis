{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes Ladder Polynomial Neural Networks (LPNNs) that use a new type of activation primitive -- a product activation -- in a feed-forward architecture. Unlike other polynomial architectures that grow in the order exponentially with network depth, the proposed approach gives explicit control over the order and smoothness of the network output and enables training with standard techniques.\t\n\t\t\t\t\nThe proposed architecture is closely related to a decomposition of a k\u2019th order multivariate polynomial function\n[T, x^{\\otimes k}] = \\lambda^\\top (A x \\odot A x \\odot \u2026  \\odot A x)\t=  \\lambda^\\top (A x)^{\\odot k}\nwhere T is a symmetric tensor of polynomial coefficients and [\\cdot,\\cdot] denotes contraction. This is a shallow (one layer architecture) and sometimes referred as a Waring decomposition.\n\nIn this paper, the authors propose a specific chain factorization of the polynomial (Eq 5 in the paper), where they write the factors recursively, that they name as a ladder polynomial neural network. \n\nh^\\ell = (W_\\ell h^{\\ell-1} \\odot V^{\\ell} x)\n\nThe ladder architecture is very closely related to tensor trains (https://epubs.siam.org/doi/10.1137/090752286). I found it surprising and somewhat alarming that this literature is not being cited as these methods are also quite well known in deep learning.\n\nI like the smoothness analysis of section 3.1 -- the proof is quite easy to follow and direct. I would be quite surprised if this result would not be known in the literature in some other form but I don\u2019t recall seeing it. On the other hand it seems to be inevitably very loose for a deep ladder network unless the network models the zero function. It would have been a valuable addition to the experimental section, if this bound would have been illustrated numerically on synthetic examples.\n\nIn 3.2, The authors say that the objective is multiconvex -- I would argue that it is multilinear (apart from the regularization term, that is later introduces). The observation in 3.3, that batch-normalization or dropout can be used for this model is perhaps tangential to the main argument. These is investigated in the experimental section but I don\u2019t see a clear conclusion. The section in 3.4 must include links to tensor decompositions beyond factorization machines.\n\t\t\nOverall, I think the paper has some merit and could be interesting for some readers, despite the fact that the contribution is not very original and the treatment could be improved in many ways. \n"}