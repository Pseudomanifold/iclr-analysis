{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "SUMMARY OF REVIEW\n\nThis paper motivates the need to \"contextualize\" responses based on the query to bring about stable training in NRG and consequently proposes localGAN to realize this. On the overall, I like the motivation and the proposed approach of this paper. The experimental results also look convincing.\n\nOn the flip side, the technical formulation and theoretical results are not presented rigorously and important technical details are missing, as discussed below. As a result, clarifications from the authors are needed to ensure the correctness of their formulation. The authors also need to improve the presentation and proof of the theoretical results; the correctness has to be checked again. In my opinion, these theoretical results do not improve my current assessment of the paper and can be removed to cut down to 8 pages. If the authors like to keep them, they need to revise them based on my concerns above.\n\nIt would be good to show some sample queries and corresponding \"meaningful\" responses produced by their proposed LocalGAN that are not considered safe responses which are produced by the other tested methods.\n\n\nDETAILED COMMENTS\n\nFor Lemmas 1 and 2 and Theorem 1, the authors need to present them rigorously by specifying the exact math expressions since they have not defined what it means by sufficient, approximates, small enough, and estimate properly. This will also eliminate any discrepancy in their interpretations. For example, the authors have used Taylor series expansion to approximate the expectation of F(q,r) in equation 19 (instead of bounding it). Hence, one can claim that Lemma 2 does not hold and hence Theorem 1 does not hold as well.\n\nIn Section 4.3, the described mechanism is confusing to me: \n\n(a) Are the authors saying that it is performed sequentially from foundation to phase-1, followed by phase-2? Or are the authors saying that these three phases are expected behaviors occurring during the optimization in equation 17? \n\n(b) For the foundation phase, is the DBM pre-trained, that is, prior to optimization in equation 17?\n\n(c) Are there multiple response clusters, that is, one for each q? If so, the second RELU term in the minimizing criterion in equation 17 does not seem to properly reflect this.\n\n(d) How are the response cluster centers r_c exactly determined? The authors vaguely say that they are modeled from training data. Is it one center per response cluster? Are the cluster centers also optimized in equation 17, besides the generator's weights? Can the authors provide the argument under the min operator in equation 17? It is confusing to leave out r_c from the subscript of alpha.\n\nI would have preferred that the authors specify the expression of the evaluation metrics to be self-contained.\n\nIn Fig. 2, how exactly do the authors measure stability? If the entropy rapidly increases like that of LocalGAN and Adver-REGS, are they considered stable?\n\n\nMinor issues\nPage 1: Despite of?\nPage 3: inequation?\nPage 3: Equation 4 and 5?\nEquation 6: The first summation should just be over q, unless there are multiple sets of R_q per q.\ntilde{R}_q is not used in equation 6.\nPage 4: a limited samples?\nPage 5: defined in 3.2?\nPage 5: To be consistent, mathbb should be applied to E.\nPage 9: valid this aspect?\nPage 9: from the the?"}