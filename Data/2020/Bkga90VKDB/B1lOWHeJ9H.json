{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\nThis paper proposes a method for compressing embedding matrices of both encoder/decoder embeddings.\nThe basic idea of the proposed method is to reconstruct the embedding matrix by what they called the \u201cfunneling decomposition\u201d method, whose parameter shape is identical to the SVD (low-rank matrix) decomposition with additional non-linear function.\nTherefore, the idea itself is not so novel and innovative.\nMoreover, their method requires the embedding matrix as the teacher signal for calculating the reconstruction loss.\nWe need to note that the memory requirement of the proposed method during training will increase.\n \nOne of the notable advantages of the proposed method is that their proposed method seems to successfully reduce the embedding matrix even if it shares the parameters with the output layer, which is a de-facto standard model architecture for NMT.\nAs pointed out by the authors, this seems to be the first success of reducing the embedding matrix with a tied embedding setting.\n\n\n1,\nThe authors claim that \u201cWe demonstrate that at the same compression rate our method outperforms existing state-of-the-art methods.\u201d at the end of the Introduction section.\nHowever, according to Tables 1, 2, and 3, it seems that the performance gain is marginal compared with similar methods.\nFor example, \n37.78 (proposed) <=> 37.78 (Shi & Yu (2018)) \n26.97 (proposed) <=> 26.75 (Chen et al. (2018)\nand\n 42.62 (proposed) <=> 42.37 (SVD with rank 64),\nwhich are the at most 0.25 BLEU gain.\nI believe that most of MT researchers hardly say that BLEU 0.25 difference is a significant improvement. Besides, the authors should perform a statistically significant test if they say \u201cour method outperforms existing state-of-the-art methods.\u201d\n \n \n2\nI am a bit confused about the following inconsistency;\nThe authors say that \u201cCompressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment\u201d in the abstract.\nHowever, according to Table 1, the number of parameters for embeddings is 16.3M, which is only 27% of the total number of parameters in Transformer base.\nBy this fact, compressing embedding matrices seems not essential for successful commercial edge deployment.\n \nIn Table 6, it is explicitly unclear what is the difference between \n\u201cFunneling with Emb. Distillation\u201d, \u201cFunneling (with non-linearity),\u201d and \u201cFunneling (with retraining all weights).\u201d\nPlease give us a more precise explanation.\n"}