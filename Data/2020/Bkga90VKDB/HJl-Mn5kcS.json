{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to use low-rank matrix decomposition for embedding compression, with relu in the reconstruction layer to gain non-linearity. Experiments on machine translation task shows improvement compared with state-of-the-art methods with different compression rates.\n\nDetailed comments:\n1)\tThe technical contribution seems to be a bit limited. Using relu in the reconstruction function looks straightforward and adding reconstruction loss in objective function is also common practice. Also, not much insight is provided on why such approach works better than other baselines. \n\n2)\tExperiments:\na.\tIt is good to see such simple approach outperforms several more sophisticated baseline methods. Also, ablation study is also performed to show the effect of different components.\n\nb.\tHow does the time complexity and running time of the proposed method compared to the baselines?\n\nc.\tThe paper only evaluates distilled embedding on one task (i.e., machine translation). The experiments would be more convincing if evaluated on more tasks as well.\n\nd.\tIt could be helpful to include some sensitivity analysis on the hyperparameters such as \\alpha which controls the weight of reconstruction loss. \n\nIn conclusion, this paper seems to be below the bar and I would recommend a \u2018weak reject\u2019 for the paper.\n"}