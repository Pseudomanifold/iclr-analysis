{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper tackles the problem of online learning for GMMs, in the context of high-dimensional data.  Specifically, the authors limit the scope to SGD-like approaches and EM-like optimization.  They also offer a TF implementation.\n\nI feel that this work is largely incremental, but more importantly indicating the authors' lack of understanding of the very long history of (online) EM.  While the authors do acknowledge some of the online EM work, they go on to develop what is a rather ad-hoc approach to online EM.\n\nThe max-component approximation in Sec. 3.1 is claimed to address the issue of numerical stability.  The authors do not appear to resort to the log-sum-exp \"trick\", which tackles such problems.  (In fact, their max approx is of this type.)\n\nSec. 3.2 uses a very standard representation of multinouli in terms of its natural parameters, which the authors again do not refer to.\n\nThe \"smoothing\" in Sec. 3.3 is hard to justify and difficult to understand, esp. the gridding approach.  Why not use hierarchical priors instead?\n\nIn Sec. 3.4, additional smoothing is accomplished using a subspace approach, which requires QR decomposition.  How will this affect computational efficiency, if the subspace needs to be recomputed?\n\nFinally, I have strong concerns about the experimental evaluation.  The authors choose datasets where the sample ambient space is at most 28x28, which is not exactly (very) high-dimensional.  \n\nI am mostly concerned about the evaluation.  "}