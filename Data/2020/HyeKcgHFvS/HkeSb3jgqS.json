{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new method based on Stochastic Gradient Descent to train Gaussian Mixture Models, and studies this method especially in the context of high dimension. \nThe method is based on optimizing max-lower bound to the log-likelihood, together with a regularization term, using stochastic gradient descent. \nThe method is applied to two image datasets, and seem to produce sensible results. \n\nHowever, in my opinion, the results and presentation do not seem at the level suitable for publication.\n\nThere are no guarantees for convergence/other performance measures for the new method, in contrast to recent methods based on moment matching (e.g Ge, Huand and Kakade, 2015). Therefore, the new method and paper should provide excellent empirical results to be worthy of publication. \n\nThe authors write in the 'related work' section that GMM with regularization was proposed by [Verbeek et al. 2005], but it is an older idea - for example [Ormoneit&Tresp 1998] \n\nIn Section 3.1, the motivation for the maximization in eq. (2) is unclear. \nWhy is it easier to compute the gradient this way rather than keep the original likelihood?\nMoreover, the max operation is non-smooth and can cause problems with the definition of the gradient at some points. \nThe authors point to a problem of underflow/overflow when evaluating the gradient of the \nfull log-likelihood because the densities p(x | k) can be very small - but it is standard practice in EM to keep all probabilities multiplied by say their maximum p_max and keep log(p_max) separately, to avoid underflow problems. \n\nSection 3.2: I don't understand the requirement \\Sigma_{i,j} >=0. Is it a requirement for each entry of the covariance matrix? (which covariance matrix? there are K such matrices). The requirement should be that each matrix is positive-definite, not the entries.\n\nSection 3.3: The first sentence is wrong - EM can also suffer from the problem of local minima. \nAlso, the single-component solution doesn't seem like a local minimum - but rather the log-likelihood is unbounded here\nsince you can put another component on one of the data points with infinitely small variance. \nThe degenerate solution where all Gaussians have equal weights, mean and variance does not seem like a local minimum\nof the log-likelihood. Say the data really comes from 2 distinct Gaussians - then separating the two Gaussians means a bit would increase the log-likelihood. is not a local minimum. I'm not even sure if the gradient is zero at this point - the authors should show this. Maybe the authors mean their modified loss L_MC - this should be stated clearly. \n\nThe change of regularization during the training process seems like a heuristic that worked well for the authors, but it is thus unclear what optimization problem is the optimization solving. The regularization is thus included for optimization reasons, and not in the usual sense of regularization. \n\n\nThe expression for \\tau at the end of Section 3.3 seems wrong to me. I don't see how plugging it into eq. (7) gives a continuous \\sigma(t) function. \n\nSection 3.4: What is \\mu^i? I didn't see a definition \n\nI don't understand the local principal directions covariance structure. The authors write 'a diagonal covariance matrix of S < D entries). But what about the other D-S coordinates? are they all zero? or can have any values? \nThe parameters count lists (S+1)*D+1 for each Gaussian so I'm assuming S*D parameters are used for the covariance matrix, but it is unclear how. Eq. (9) has the parameters vectors d_{ks} for the principal directions, together with the \\Sigma_ss scalar values - it would be good to relate them to the mean and variance of the Gaussians. \n\n\nSection 4: The paragraph that describes the experimental details at the beginning is repeated twice. \n\nThe experimental results are not very convincing. The images in Figure 2,4 were picked by an unknown (manual?) criteria. \n\nIn the comparison to EM in Figure 3 there are missing details - which log-likelihood is used? L, L_MC? or different ones for different methods? is this test set log-likelihood? what fraction of the data was used? \nThere is also no comparison of running time between the two methods. \n\n\n"}