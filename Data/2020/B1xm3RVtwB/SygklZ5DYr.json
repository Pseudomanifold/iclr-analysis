{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a novel exploitation of the centralized training for decentralized execution regime for Dec-POMDPs with publicly/commonly known actions.  In particular, the paper augments independent value-based reinforcement learning by allowing each agent to announce the action it would have taken, had it acted greedily. This relaxation is consistent with decentralized execution, at which time agents always act greedily and no counterfactual announcement is required. The paper demonstrates the utility of this trick in Hanabi, where it achieves strong performance when combined with distributed Q-learning and an auxiliary task. The paper is largely well-written.\n\nI think that the trick introduced in this paper is a valuable contribution to the community. As the paper discusses, SAD is simpler and easier to implement than BAD, the algorithm with the most comparable ambitions with respect to scalability. Also, unlike BAD, it is model-free. I think that investigating lightweight alternatives to common/public knowledge based approaches, such as SAD, is an important research direction.\n\nThat being said, I have some issues with the presentation of the content in the paper. \n\n1. Section 4 is problematic. The entirety of the section is based on the assumption that agent a\u2019 observes the Markov state. The paper claims that this assumption is made for simplicity: \u201cwhere for simplicity we have assumed that agent a\u2019 uses a feed-forward policy and observes the Markov state of the environment.\u201d \n    a. First, this assumption is unmet, with few exceptions. Other agents do \n        NOT in general observe the Markov state. This is a very significant \n        part of why Dec-POMDPs are so difficult. Justifying SAD in a setting \n        that so radically departs from its use case is not informative. \n    b. Second, the claim made by the paper (that the assumption is made \n        for simplicity) is misleading. The assumption is made out of \n        necessity. The cost of neglecting public/common knowledge, as SAD \n        does, is that principled Bayesian reasoning is not possible. It is \n        important that the paper acknowledges this to counterbalance its \n        (valid) criticisms that public/common knowledge based approaches \n        are difficult to scale.\n2. That the experiments are expensive is understandable and a fully complete ablation study is not expected. But the results of these experiments should be presented clearly. \n    a. In both Table 1 and Table 2, presenting the \u201cbest seed\u201d is \n        not appropriate. The paper should report the mean or the median. \n        Given that only three seeds could be afforded, it is understandable \n        that the results would have high variance. \n    b. In Table 2, (if I am understanding the paper correctly) it is claimed \n        that the s.e.m. is less than or equal to 0.01 over a set of three seeds \n        for all of the experiments. But looking at Figure 4, this is clearly not \n        the case, especially in the four and five player settings. The intended \n        meaning should be clarified.\n    c. In Table 1, the paper should report the results for the baseline, VDN, \n        VDN with greedy input, and SAD separately, as is done in Table 2. \n        These are the main experimental results and merit space in the main \n        body of the paper. Moreover, there are a number of results that \n        deserve written attention.\n        i. It looks the baseline is very competitive with BAD in two player \n           Hanabi. This in itself is a very interesting finding and is worth \n           discussing.\n        ii. Looking at Table 2, it appears that VDN with GreedyInput \n            outperforms SAD in three of the four settings and quite \n            significantly in the five player setting. If this is also the case when \n            results are aggregated over the median or mean, it should be \n            discussed.\n3. The greedy input approach is specific to Dec-POMDPs in which actions are publicly/commonly available. This should at least be mentioned somewhere.\n\nOverall, I think the ideas and experimental findings in the paper are very interesting. However, as outlined above, there are a number of issues. At a high level, I think that the paper is too concerned with 1) justifying greedy input with Bayesian reasoning and 2) promoting state of the art results. The ideas and experimental findings are more than sufficient for strong contribution without these things.\n\nIn its current state, I feel that this work is a rejection. However, its issues are relatively easily amendable:\n1. For each algorithm and each setting, separately report median or mean results over the seeds, with uncertainty.\n2. Reallocate space to section 6.2 for a scientific discussion of the experimental results.\n3. Remove or qualify the arguments made in section 4.\n4. Mention that the greedy input approach is specific to Dec-POMDPs with publicly/commonly available actions.\n\nWere these issues addressed, my opinion would change."}