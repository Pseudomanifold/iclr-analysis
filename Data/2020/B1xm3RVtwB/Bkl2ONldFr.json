{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper examines the problem of epsilon-greedy exploration in cooperative multi-agent reinforcement learning. Such exploration makes actions less informative to other agents because of the noise added to greedy optimal actions. The suggested solution is to consider two actions: one action is epsilon-greedy and it is passed to the environment, while another is fully greedy according to the agent\u2019s strategy, and it is shown to another agents. At test time these two actions are the same. This idea is applied to Hanabi game; the self-play regime is examined. The suggested method is claimed to show state-of-art results in 2-5 players Hanabi. \n\nThe Methods Section provides mathematical grounds of using Bayesian reasoning in ToM. It is shown how epsilon-greedy exploration leads to the blurring of the posterior and consequently making beliefs of other agents less informative. Thus, additional fully greedy input is motivated. However, there are still some vague parts in the description of methods used.\n1.\tIn Section 4.3 it is not clear what auxiliary supervised task is added to training. The information from Appendix should be moved to Section 4.3. Furthermore, some details on how training data for this task is collected and model is trained should be added. Also, how does this model affect the whole training procedure? \n2.\tThe description of the Q-learning in the first paragraph of Section 3.2 is too vague. In the first equation for Q(s,u), there is no u_t, u_{t\u2019}, so it\u2019s unclear why they pop out next. In the Bellman equation for Q(s, u) parentheses are omitted.\n\nThe advantage of the paper is that apart from the suggested additional greedy actions, several ingredients of the state-of-art approach (suggested in previous works) are tested separately, such as learning a joint Q-function and auxiliary supervised task. However, considering the results, it seems there is no clear winner for different number of players in Hanabi. From the Table 2, it is not clear whether the additional greedy input or the auxiliary task are beneficial for the best seed results. One of the three competing methods is the method named VDN in Tables 1 and 2. Is it the contribution of the paper or a previous work? If it is the contribution, the difference from the previous work should be clearly stated. If not, then it\u2019s not fully fair to say that the new approach shows state-of-art results.\n\nAlso, is it fair to say that s.e.m. of the best seed is 0.01? Comparing the results from Figure 3 and Table 2, one can see that the order of the winners is changing significantly, so 3 seeds are not enough to reliably estimate the performance of the best seed and s.e.m. should be higher.\n\nAnother comment on the Experiment 6.1: as far as BAD results are mentioned in the text, maybe they could be put on the plot? At least final training score as horizontal line, if the whole training curve is not available.\n\nMinor comments\n1.\tPage 1: Simply observing other agents are doing -> Simply observing what other agents are doing.\n2.\tPage 3: While our method are general, we restrict ourselves to turn based settings \u2013 odd phrase.\n3.\tPage 4: A combination these techniques -> A combination of these techniques.\n4.\t\\eps-greedy is written with a hyphen in some places of the paper and somewhere without.\n5.\tSection 5.3: Compute Requirements -> Computation Requirements.\n"}