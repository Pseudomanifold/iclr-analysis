{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to replace the generator in GANs by a single-layer polynomial generator. In other words, the usual deep network of the generator is replaced by linear tensor product. Although the tensor product are linear in each parameter of the noise vector, they involve multiplications between tuples of input parameters and are therefore more expressive than a single linear layer (single matrix multiplication).\n\nThe results are presented on synthetic low dimensional data, MNIST, YaleB and CIFAR10. Although it is always difficult to judge the quality of GAN generated samples, the samples presented are of poor quality compared to the state of the art.\n\nMy main objection to this paper is that it does not present a clear justification for their approach. It does not seem to work better empirically, and there is no strong theoretical justification. The authors explain well that polynomial can approximate any function, but that is also the case of many approximators, including deep neural networks. The main thing that this paper shows, in my opinion, is that the GAN framework is robust to different kinds of generators, but that is not the stated point of this paper.\nBesides, polynomial approximators tend to produce unstable approximations when their degree increases, which is not addressed in the paper. Why not use approximators that were proven to be stable, such as splines of wavelets, is not well explained. I am actually surprised that the polynomial generator is stable enough to generate samples. One possible reason is that GAN training (with a discriminator) makes the generated polynomials more stable, which could be an interesting result if investigated.\n\nThe overall structure of the paper is acceptable, although many of the notations that are introduced early in the paper are actually only useful in the appendix and should probably be moved there.\n\nI would suggest publishing this paper in a workshop."}