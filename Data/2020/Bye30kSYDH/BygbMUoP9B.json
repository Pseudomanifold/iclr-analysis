{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed a new member of the GAN zoo named PolyGAN. It differs from the other GANs in that the generator G(z) is a polynomial function of the input noise z. As the noise has dimension d, the number of free parameters grows in the order of O(d^k), where $k$ is the polynomial order. Therefore the authors proposed (1) to constrain k to be small; (2) to design a constrained parametrization of the polynomial coefficients, so as to reduce the total number of free parameters. The authors showed that PolyGAN can generate good-looking MNIST digits.\n\nThe reviewer is performing an urgent review by invitation.\n\nOverall, this contribution is a typical deep learning technical contribution, on the practical side rather than on the theoretical side. The paper is very well written in terms of clarify. However, the reviewer has some concerns about the overall significance and some other concerns on the experimental evaluations, and hence recommends rejection.\n\nFirst, the authors have to mention some literatures of the polynomial regression and have some in-depth discussions. The current written is too technical and lacks theoretical depth. The reviewer does not feel excited after reading the paper. Actually, Figure (4) shows a weakness of the PolyGAN, just as in polynomial regression: polynomial function is unbounded at infinity and therefore loss accuracy on the boundary of the data distribution. This kind of discussion will enhance the overall interestingness of this contribution.\n\nSecond, in the experiments, there has to be a baseline, such as the vanilla GAN (even PolyGAN performs worse than GAN, it will enhance the writing).  And, there has to be an experiment on real images such as ImageNet, just like the other GAN papers. And, it is recommended to show some cases where the order is (much) higher than 4, so as to show the usefulness of the proposed parametrization. The current evaluation is not complete. and is not convincing."}