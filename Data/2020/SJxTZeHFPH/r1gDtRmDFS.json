{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper studies the effect of the focal loss, proposed by Lin et al. in 2017 on network miscalibration, which appears when the network's confidence in its prediction does not match its correctness. The authors provide a theoretical explanation\u00a0to the superior results of the focal loss for calibration.\u00a0The temperature scaling technique of Guo et al. 2017 is applied (dividing the network's logits by a scalar learnt on a val set prior to softmax) to networks trained using the focal loss, with different options for the focal parameter, as well as the standard multi-class cross entropy and a few others.\u00a0The experiments\u00a0on CIFAR10/100 as well as two text dataset (20 Newsgroups, Stanford Sentiment Treebank) reach lower expected calibration error compared to the cross entropy (75% of relative improvement on cifar100 for instance).\n\nThe importance of the contribution will probably be discussed here. At first glance, it seems that the works build mainly on advances from Lin et al & Guo et al, but the authors do a promising job in combining the two.\u00a0\u00a0\n\nPositive aspects: \n- The paper is well written.\u00a0\n- Experiments\u00a0on both image and text dataset demonstrate the superiority of the focal loss on several calibration metrics.\u00a0\n- The theoretical explanation is convincing.\u00a0 \u00a0\n\nNegative points:\n- The importance of the problem is motivated by future assessments by downstream tasks but do not address this aspect in the experiments. In particular, as the images experiments are conducted on tiny images, an experiment on a real size image dataset would strengthen the paper.\u00a0\n- The policy that works best for defining the sample wise tuning of the focal parameter was hand-made but ultimately uses only 3 parameters so finally it is not so bad. \u00a0\n\nMinor:\u00a0\n- It'd be nice to illustrate the confidence improvements on a few qualitative examples, maybe in appendix.\n- 10 pages is too much (given that were were given instructions to be more severe with long paper) table 6 and 3 could be merged for instance.\u00a0\n- The focal loss column results of table 1 should be the same as Table 5 (sample wise)?\n- could specify what MMCE means\n- clean the bibliography"}