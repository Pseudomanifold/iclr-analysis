{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper leverages metric learning to learn graph structure jointly with the learning of graph embedding. Firstly, it defines the similarity between any pair of nodes as the cosine similarity of nodal representations learned from attributes from nodes. Some tricks such as multi-head and sparsification are applied to learned cosine similarity to enhance the performance. Secondly, the authors introduce several graph regularizations to make the learned graph smooth, connected, sparse and non-trivial. Finally, the learned graph is linearly combined with the existing graph, using a hyperparameter \\lambda. The convergence and time complexity are analyzed. The authors design experiments on five datasets. The paper also contains some issues:\n\n1. Actually, the proposed framework is learning an extra graph adjacency matrix from nodal features, and further train GNN jointly on those two graphs. Therefore, the analysis of \\lambda is necessary. However, this part is missing in the paper.\n\n2. The improvements comparing with LDS is not significant. Besides, LDS only uses the optimized graph structure to train GNN, while the proposed framework use both learning structure and the original one (or the kNN result). It raises the question if the proposed framework can still out-perform LDS if LDS also takes the original graph as input to train GNN.\n\n3. The learned graph should be better interpreted. For example, the cosine similarity on citation graphs with sparse features is very likely to be zero. As a result, the learned graph can be extremely sparse with very few non-zero entries. It would be interesting.\n"}