{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces an iterative method called IDGL for learning both the graph structure (more precisely adjacency matrix) and parameters of the graph neural network.\n\nThe idea of iteratively refining an adjacency matrix A to obtain sparsity and smoothness is interesting and the experimental results are quite supportive. The main issue here is that the regularization terms in Eqs. 4, 5, 6 (which should be considered to be the most important in the paper) are exactly similar to those in [1] (see Eq. 12 in [1]).  This reduces the novelty of the paper.\n\nOther parts in the paper such as using similarity between nodes (e.g. self-attention) to compute adjacency matrix or using the learned adjacency matrix with graph neural network is not new and have been done by many other works. There is also the rich related literature on graph generation (e.g., as in drug design), graph transformation (e.g., as in chemical reaction), structure learning in classical probabilistic graphical models, graph pooling (which is essentially building new latent graphs from an original graph), knowledge-graph completion, etc. This is not to say that the problem is solved (it isn't), but it is fair to place this work in a broader context.\n\nAbout the experiments, I have several concerns. First, I am not sure why the authors say that LDS [2] does not support inductive learning? LDS uses input node features to learn the unknown graph structure so I think it should be able to do inductive learning. DeepWalk or Node2Vec are examples of transductive methods because they do not use the node features. Second, for the running time comparison between IDGL and LDS, what are the size and number of parameters used in each model since they greatly affect the running time.\n\nMinor point: please clean up duplicates in the reference list.\n\n[1] How to learn a graph from smooth signals, Kalofolias et. al. 2016\n[2] Learning discrete structures for graph neural networks, Franceschi et. al. 2019.\n\n"}