{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper focuses on training neural networks using 8-bit floating-point numbers (FP8). The goal is highly motivated: training neural networks faster, with smaller memory footprint and energy consumption.\nThe proposed approach S2FP8 eliminates the need for loss scaling and does not require keeping some of the layers in FP32 precision as in Mellempudi et al. (2019).\nThe paper is well written, easy to follow, and provides a detailed background for readers who are not knowledgeable in this field.\n\nOn the downside, the first sections give the impression that FP32 is not used at all: \"S2FP8 does not require keeping the first and last layer in FP32 precision, which is needed for other approaches (Mellempudi et al., 2019).\". However, Section 3.2 says that \"Master weights are kept in FP32\" and that \"Accumulations inside the GEMM kernel are kept in full FP32 precision\". I think this should be stated earlier, because otherwise, the introduction is overclaiming.\n\nThe evaluation is very convincing - the approach is demonstrated for image classification, Transformer-based translation, and neural collaborative filtering. S2FP8 outperforms previous FP8 approaches and reaches the accuracy of FP32 out-of-the-box. \nIt would be interesting to see the Transformer results with S2FP8 on additional datasets rather than the English-Vietnamese dataset only, which is considered a low-resource dataset, and on Transformer-base rather than \"tiny\". If you have such results on additional NMT datasets, I would be interested to see them, even if the performance of S2FP8 is worse than FP32 (I will not reduce my rating because of lower results there).\n\nOverall, this is a good paper that presents an important contribution. Thus, I recommend accepting this paper.\n\nMinor:\n* Section 2, paragraph 3: \"(Mellempudi et al. 2019) also demonstrated ..\" the name of the authors should be outside of the parentheses (\\citet). Same in 4th paragraph: \"(Zhou et al., 2016) quantized ..\"\n* Equation (2) is a little difficult to read because of the sequence \"and max log\", in which each word has a different role. It might worth to break it into two lines, add brackets, or use logical \"and\" instead of the word \"and\"?\n* Equation (3) uses \"i-prime\" in the argument for \"max\", but \"i-prime\" is not used.\n* Figure 4 is referenced before Figure 3, this is a little confusing (the reader needs to scroll down for Figure 4, and scroll up for Figure 3).\n\n"}