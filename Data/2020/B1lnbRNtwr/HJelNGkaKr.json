{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Strength:\n-- Interesting problem\n--The paper is well written and easy to follow\n-- The proposed approach seems very effective\n\nWeakness:\n-- the novelty of the proposed is marginal\n-- Some of the claims are not right in the paper\n\nThis paper studied learning the representations of source codes by combining sequential-based approaches (RNN, Transformers) and graph neural network to model both the local and global dependency between the tokens. Experimental results on both synthetic and real-world datasets prove the effectiveness of the proposed approach.\n\nOverall, the paper is well written and easy to follow. However, the novelty of the proposed technique seems to be marginal to me. Some of the claims in the paper are not right. In the abstract, the authors said the graph neural network is more local-based while the transformer is more global-based. The essential difference between the two approaches lie in the way of constructing the graphs since transformer used the fully-connected graph (more local dependency) while graph neural networks usually capture the long-range dependency. \n\nAnd there are actually some existing work that have already explored this idea in the context of natural language understanding, e.g.,\nContextualized Non-local Neural Networks for Sequence Learning. https://arxiv.org/abs/1811.08600\nThe authors should clarify the difference between these approaches.\n\n"}