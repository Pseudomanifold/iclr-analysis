{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The main contribution of this paper are:\n\n1. The use of a heteroscedastic GP when performing Bayesian Optimization, this is in contrast to the more common practice of assuming homoscedastic noise, even when this does not quite fit the data. They use the existing algorithm called most likely heteroscedastic GP, and quote previous work that performed BO using different heteroscedastic GP implementations.\n2. They introduce two new acquisition functions that incorporate the predicted observation noise, either making candidates more likely or less likely to be chosen when predicted noise is higher, depending on the requirements. This are fairly minor extensions/heuristics if taken on their own, as they do not provide a very strong motivation indicating why these acquisition functions are useful or better than existing ones, other than that they take heteroscedasticity into account.\n3. They run a set of experiments on the above settings. Unfortunately the experiments are very limited, and their method does not improve on the baselines in a statistically significant way. Two of the experiments are on simple synthetic settings, the third approximates a real world setting, although the approximation is quite rough and they don't convincingly argue for it being realistic, neither do they give convincing motivation of their objective which uses g +- standard deviation.\n4. They provide source code of their implementation.\n\nThe paper is easy to understand, and covers an interesting topic, so while I don't think it meets the bar of ICLR (due to lack of convincing and non-trivial contributions) I think it could perhaps be made into a workshop submission with some of the following changes:\n* A wider set of experimental settings, and more replication such that any differences become statistically significant. It would also be worthwhile comparing to random search.\n* A better justification of the objective used in the experiments, using g +- the standard deviation appears fairly arbitrary, and there is no strong enough reason to believe this is a good approximation of what the cost is in the case of real world problems.\n* Better theoretical justification of the acquisition function; one option is to introduce more principled acquisition function like, say, expected upper/lower bound.\n\n\nOther notes/comments:\nabstract: as well as a real-world -> as well as *on* a real-world...\nsection 1 \"As a case study\" -> not very clear what this means\nincumbent best is not well defined, is it the empirical value of f used or the mean predicted f on the evaluated candidates?\nsection 6.2 \"outperforms\" -> this is not clear if one looks at the confidence intervals in the results\n"}