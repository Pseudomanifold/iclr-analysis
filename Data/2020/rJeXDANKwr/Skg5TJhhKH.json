{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the neural architecture search (NAS) problem under the out-of-distribution (OoD) environment. As the OoD problem is not visited in the current NAS literature, this paper proposes replacements for each of the three standard components in NAS, i.e., the proxy task, the search space, and the optimization algorithm; and each replacement is built upon an ensemble of existing techniques. Experiments are further verified on CelebA, CIFAR-10, SVHN, and MNIST.\n\nOverall, the novelty in this paper is limited and experiments are not very convincing. Please see the questions below:\n\nQ1. In the introduction, the authors write \"Machine learning systems often encounter OoD errors when dealing with testing data coming from a different distribution from the one used for training\".\n- What is the validation set used in this paper?\n- For the NAS problem, the architectural parameters must be guided by the validation set. So, does the validation set follows the same distribution as the training data set or the testing data set?\n- If the validation set and the testing set have the same distribution, is it still a meaningful OoD problem?\n\nQ2. \"For example, naively using data likelihood maximization as a proxy task would run into the issue pointed out by Nalisnick et al. (2019a), with models assigning higher likelihoods to OoD data\".\n- How can I see this point from the given experiments?\n- Is it better adding this into an ablation study?\n\nQ3. Except for WAIC, what other metrics can we consider? The authors should have a more comprehensive related work section, which includes discussion on this part.\n- Is it more meaningful to search a better metric than search architectures (which is just a standard applicaiton)?\n\nQ4. In Section 3.4:  \"CelebA (Liu et al.), CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), and MNIST (LeCun)\" are used.\n- Could the authors explain more tasks and details on these data sets? Specifically, why they are OoD problems.\n- Based on the descriptions from the authors, these data sets seem to be standard ones.\n\nQ5. Variation is already considered in (1), I mean the second term there. \n- Why should we still consider an ensemble of models?\n- Is it better adding an ablation study on M about \" ROC and PR curve\"? (not Figure 13, 14 in the appendix)\n\nQ6. Is it better to have a comparison with standard NAS in the experiments?\n- While authors argue they are not applicable here, it is still good to demonstrate how not applicable they are.\n- If the validation set follows the same distribution as the testing set, gradient signals on architectural parameters perhaps can still be helpful.\n\nQ7. Can natural gradient descent be applied to (4)?\n- \"First, optimizing p(\u03b1), a probability over ..., each network\u2019s optimal parameters would need to be individually\". The first problem is not a really challenging problem please have a check at \"Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search\".\n\nQ8. What is the searching time of the proposed method? How's it compared with recent NAS methods? e.g. DARTS (DARTS: Differentiable Architecture Search). "}