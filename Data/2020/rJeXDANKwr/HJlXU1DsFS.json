{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a neural architecture search (NAS) method to construct a Bayesian ensemble of deep learning models. This ensemble is then employed to detect out-of-distribution examples.\n\n\nThe authors propose to use a differentiable architecture search method which model the architectural parameters using a concrete distribution. This idea was originally proposed by Xie et al. (2019) but this work was not discussed. Similarly, the work by Chang et al. (2019) is not discussed. In my opinion the novelty with respect to NAS is the WAIC objective function and its application to out-of-distribution detection.\n\n\nThe idea of using ensemble to detect out-of-distribution examples is not new. The authors already refer to the works by Choi & Jang (2018) and Lakshminarayanan et al. (2017). I'd like to add MC-Dropout (Gal et al., 2016) to this list which was used e.g. to detect adversarial examples.\n\n\nThe experimental section is well-written and the proposed method is able to outperform the chosen baselines. Obvious baselines are missing. There is no experiment that proof that this way of searching architectures finds better suited ensembles. How about maximizing the cross-entropy and train the discovered architecture multiple times from scratch and use these models in an ensemble to detect out-of-distribution examples? How about any ensemble-based method mentioned in the previous paragraph?\n\n\nConcluding, the idea is nice but based on the current state of the paper it seems incremental. Experiments to back the usefulness of the described method are missing.\n\n\nSirui Xie, Hehui Zheng, Chunxiao Liu, Liang Lin: SNAS: stochastic neural architecture search. ICLR 2019\nJianlong Chang, Xinbang Zhang, Yiwen Guo, Gaofeng Meng, Shiming Xiang, Chunhong Pan: Differentiable Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019)\nYarin Gal, Zoubin Ghahramani: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. ICML 2016: 1050-1059"}