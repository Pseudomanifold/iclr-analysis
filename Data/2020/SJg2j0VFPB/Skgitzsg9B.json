{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work conducts a theoretical study of the generalization bounds for the number of wrongly predicted triples of KG embedding methods. Under a \"``missing completely at random'' assumption, the authors model the distributions of sampled KG and ground-truth KG, and derivate the bounds as a function of the KL-divergence by leveraging Pinsker's inequality. Preliminary discussions on how existing KG embedding methods fit into this theoretical study have been conducted.\n\nPros:\n- This work raises a novel problem, i.e., analyzing the theoretical generalization ability of existing KG embedding methods. \n- The paper is well motivated with clear writing and technically sound with rigorous formula derivation.\n\nCons:\n1) For KG construction, people usually collect/mine in a batch/incremental fashion in which each batch focuses on certain aspects. For example, one can collect some bio facts about Persons from Wikipedia, then collect some entertainment related facts from news. So in practice, \"``missing completely at random\" assumption may not hold for KG.\n\n2) The derivation is suitable for a class of KG models that maximize log-likelihood losses, while many KG embedding methods use margin-based loss functions. Although the authors mentioned \"``log-loss can in principle be also used and it was observed by Trouillon & Nickel (2017) that the margin-based loss functions, used by many knowledge graph embedding methods, are more prone to overfitting compared to log-likelihood'', more rigorous theoretical analysis is suggested to verify (or refine) the applicability of the proposed analysis \n\n3) It would be better to see more examples that applying the proposed analysis to different KG embedding methods, corresponding comparisons help to get a deeper understanding for existing KG embedding methods, and may shed further light on how to design a KG embedding method that achieves a good enough generalization with fewer model parameters.\n\n"}