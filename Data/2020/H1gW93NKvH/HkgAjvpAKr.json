{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a network architecture to attain temporally coherent video super-resolution using temporal connections across the residual block layers between the previous and current frames. It reuses the feature maps computed in the previous frame after passing it through a warping layer based on screen-space (that is, 2D image space) motion vectors ('velocities' in the paper). In other words, the second convolutional layer of the residual block gets not only the feature map of the first convolutional layer feature map but a concatenated feature map that is composed of the first convolutional layer feature map and the warped feature map of the first convolutional layer computed in the previous frame. This allows propagating feature maps in time. This is called as 'depth-recurrent residuals.' Other variants are also evaluated. \n\nThe network is trained with sequences of HR-LR image pairs that are rendered using Unity\u2019s HDRP. However, neither the original TecoGAN nor any of the other baselines have been trained with such rendered data. It is not clear whether the training of the entire network and testing with the Unity generated rendered data provides an unfair advantage to this method in comparison to the other methods that do not use such data in their training. The paper mentions that TecoGAN re-trained, yet no quantitative results are provided for that version in the paper. Also, it is not clear, why motion-compensated features in residual blocks are better than the existing architectures that use multiple motion-compensated frames, perhaps the paper could have provided a deeper discussion.\n\nTo make a case for a video SR method that is applicable not only to CGI rendered videos but also to a more general class of natural videos (e.g. camera acquired) additional results on the NTIRE 2019 video SR challenge dataset should have been provided. The paper fails to compare with any of the top methods in the video SR challenge in NTIRE 2019.  "}