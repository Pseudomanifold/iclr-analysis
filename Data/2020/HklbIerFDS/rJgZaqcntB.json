{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method, Slow Thinking to Learn (STL), that leverages the interaction between a collection of per-task \u2018fast learners\u2019 (FLs) and a cross-task 'slow predictor' (SP) in order to enable sequential few-shot learning without having to train on a large number of tasks and without catastrophic forgetting. Each fast learner consists of an embedding network and a memory module [1] that makes predictions by performing a KNN on the embedded inputs, while the slow learner is meta-trained using MAML [2] to make few-shot predictions based on the embedded nearest neighbours outputted by the fast learner. The key novelty of this paper is that each fast learner is trained with an auxiliary loss term that encourages an embedding that improves the inference of the slow predictor. The model is evaluated on standard sequential training settings of permuted MNIST and split CIFAR-100, demonstrating superior ability for mitigating catastrophic forgetting and sequential few-shot learning. While their method for synergising the two types of learner is interesting and seems to have potential for few-shot learning, I recommend that it be rejected for the following main reason: \n\n* The implementation of the model seems to overstep the resource constraints typically imposed in a lifelong learning setting [3,4] in that the external memory size and number of model parameters are allowed to increase indefinitely as more tasks come in. This makes it hard to compare it fairly to the baselines that do not benefit from these advantages to the same extent in the experiments that evaluate catastrophic forgetting and sequential few-shot learning, particularly because there is not a clear comparison of the amount of resources used by each model. The experiments would be much more informative if the method were compared, for example, to other lifelong learning methods that allow for growth in the number of network parameters over time, such as [5,6].\n\nMore detailed concerns / questions:\n* I\u2019m confused about the size of the external memory in the experiments. When the memory size is said to be fixed at 1000, is that per fast learner in the STL model or a total of 1000 across all FLs? If it is 1000 per FL, then is the memory size for the MbPA+ and the Memory Module baselines increased accordingly with each task for a fair comparison? \n* If the external memory grows indefinitely with each task, then the experimental setting becomes more akin to a multitask / meta-learning one than a lifelong learning one. Perhaps a fairer comparison would be to [7], where a meta-learning algorithm is learned in an online setting (i.e. with the tasks coming in sequentially) but where there is access to all previous data. \n* Since each FL has its own embedding network, the total number of parameters of the model grows significantly with each task, giving it an advantage over the baselines that have a fixed number of network parameters (EWC, MbPA+). Figure 4.(b) shows that MbPA+ catastrophically forgets because it uses only one embedding network for all tasks - does the Memory Module baseline use separate embedding networks for each task?\n* In Section 4.1, it is shown that STL is not sensitive to the size of the output network (the SP in the STL model), while EWC is. Firstly, it is stated that only the output network is changed but it also says that the number of convolutional layers is changed, which are previously stated to constitute the embedding network - is the embedding network also changed? In any case, the STL model benefits from multiple embedding networks, each consisting of 4 convolutional layers, resulting in a much higher parameter count than the baselines - in these circumstances, it does not seem fair to claim that STL is not sensitive to changes in the model size relative to the other models, since they have a different starting point. \n* Since the number of network parameters is increased with each task, it would be fairer to compare to other methods that do so, such as [5, 6], in a way that equalises the increase in number of parameters per task. It is claimed that STL has an advantage over these two methods because in those cases one has to decide how many neurons to add for each task - but is this not the case also for STL, since one must decide how large an embedding network to have in the FLs?\n* While the paper says that the method has better space efficiency in RAM than other methods since the fast learners can be kept on the hard disk and used one at at time for training, lifelong learning methods are typically concerned with limiting total memory usage, not just RAM.\n* The Separate-MAML baseline demonstrates the effect of removing the synergistic loss term in the FLs and its significantly worse performance in the small memory experiments (Figure 4.(c) and Figure 7) ostensibly emphasise the importance of this term, but perhaps this is not so surprising since its removal seems to mean that only a fraction of the training examples are ever passed through the SP. While in the full STL model, all the incoming data is passed through the SP in order to train the FL with the feedback loss, in the Separate-MAML model, the SP only benefits from training the output network on the examples stored in the FL memory. For this reason, it\u2019s hard to know whether the superior performance of the full STL model arises because of the interesting dual loss function for training the embedding network or because the gradient is only propagated through the full SP pathway (embedding + output network) on a subset of the training data. In order to understand the importance of the dual FL loss function, it might be revealing to train both the full STL and Separate-MAML models with full access to all previous data.\n* In the few-shot learning experiments,  how is the amount of training equalised between the different methods? Is each method trained to convergence on the available batches or are they each trained for a fixed number of iterations?\n* Are the accuracies plotted over just one run or averages over multiple runs? If they are averages, why are there no error bars?\n* It seems that the dotted and solid lines labelled the wrong way round in Figure 6. It currently shows EWC performing worse with a larger model, which is the opposite of what is stated in the main text.\n\nComments / questions not affecting review:\n* Do the authors have any intuition for why Separate-MAML performs worse than Separate-MbPA (Figure 4c. 5a, 5b) despite the SP having had the chance to learn better initial parameters with MAML?\n* Any intuition for why performances of FL and Memory Module dip as the number of shots increases to 32 in Figure 7?\n* Part 1, last paragraph, line 3: \u201cinterdependent\u201d rather than \u201cdependent\u201d.\n* Supplementary 3.1 line 1: \u201cFLs\u201d rather than \u201cFTs\u201d\n* The analogy of gradient steps in the few-shot prediction of the SP to \u201cthinking\u201d seems like a stretch and and a bit misleading.\n\n[1] Kaiser, \u0141ukasz, et al. \"Learning to remember rare events.\" arXiv preprint arXiv:1703.03129 (2017).\n[2] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n[3] Schaul, Tom, et al. \"The Barbados 2018 List of Open Issues in Continual Learning.\" arXiv preprint arXiv:1811.07004 (2018)\n[4] Continual Learning Workshop, NeurIPS 2018. Continual Learning desiderata: https://sites.google.com/view/continual2018/home\n[5] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n[6] Yoon, JaeHong, et al. \"Lifelong Learning with Dynamically Expandable Network.\" International Conference on Learning Representations. International Conference on Learning Representations, 2018.\n[7] Finn, Chelsea, et al. \"Online Meta-Learning.\" International Conference on Machine Learning. 2019."}