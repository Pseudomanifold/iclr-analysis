{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studied the problem of reinforcement learning under the sparse or dynamic rewarding environment. \nThe authors propose a promising automated curricula generation scheme, which considers goal validity, goal \nfeasibility, and goal coverage to construct useful curricula for the underlying agents. Rewards and loss functions \nare proposed individually for the solver, judge, and setter. Empirical studies demonstrate the capability of the \nproposed model in generating task curricula across several complex goals. In general, I believe the studied \nproblem is interesting, and the proposed model is promising.  However, I am not familiar with the curricula \ngeneration in the reinforcement learning setting. All I can say is the approach is intuitively appealing, the text is \nwell written and easy to follow, even for an outsider. A minor concern is about the experiments. Most of the \nexperiments are presented in the metric of Reward (% of best). It would be helpful if the authors can conduct \nsome illustrative case studies (better in different scenarios) to show some specific tasks, the generated task-oriented \ncurriculum, and provide some intuitive discussion to show the readers why the generated curriculum is beneficial \nfor the agent. "}