{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper tackles the task of automatically inducing a curriculum for agents learning through reinforcement. Specifically, they use two agents \u2014 a setter agent that sets goals, and a solver agent that solves the goals provided by the setter.  While this has been explored before, the difficulty lies in training both agents simultaneously in a robust fashion. If the goals are too difficult, the solver will be unable to solve them and if they are too easy, the solver will be unable to improve. The authors propose a combination of different losses to help the setter balance its goal predictions \u2014 validity, feasibility and coverage. In addition, they train a judge model predict the reward that the solver agent would achieve on a goal proposed by the setter. Empirical results demonstrate on two setups demonstrate the effectiveness of this approach in learning a good curriculum. \n\nPros:\n1. Clear writing, method is easy to understand. \n2. Novel objectives for a multi-agent training setup\n\nCons:\n1. Empirical results do not contain any baselines or prior work comparisons (only ablations of the proposed model)"}