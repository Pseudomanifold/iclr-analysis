{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use GMM as the latent prior distribution of GAN. The model adds the means, covariances and the discrete priors of the GMM as learnable parameters to the GAN, which are jointly optimized with other GAN parameters. The model also adds a discrete classifier to the training process. During training, the classifier predicts the probability of each image falling into each of the GMM clusters, and uses these probabilities to re-weight the GAN generated samples.\n\n# motivation\n\nThe motivation of this work is not clear to me. Even with an isotropic Gaussian prior, a fully connected neural network is already sufficient to (approximately) simulate the GMM sampling. Thus, explicitly modeling GMM doesn't seem to be necessary, and could make the learning more difficult. \n\nI also don't quite understand why the authors have to add a discrete classifier to the modeling. It appears that the discrete classifier is only used for controlling the relative weights of clusters in the GAN training. If that's the case, then what's truly needed is just the prior distribution of each cluster, which doesn't depend on the individual images. For concrete datasets, this prior is usually known. For example, in MNIST each cluster has an equal prior of 10%.\n\n# experiments\n\nThe model is evaluated on MNIST and Oxford-102. I'd like to see it tested on more realistic and higher resolution images, and compared with state-of-the-art GAN models. Since the motivation of the modeling design is unclear, the bar on the empirical results should be much higher.\n"}