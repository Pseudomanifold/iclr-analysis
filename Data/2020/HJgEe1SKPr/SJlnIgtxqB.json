{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers the Gaussian mixture model at the latent space to have a better GAN training result.  The proposed architecture consists of three networks, a classifier, a generator, and a discriminator. Every input image goes through the classifier and gets the softmax output. The softmax output is considered as the mixture weights of the GMM model and controls the loss function of the generator accordingly. \n\nAlthough this paper has some positive sides, I recommend \"weak reject\" because of the following reasons.\n\n1. This paper is hard to follow. Many concepts are not discussed enough. For instance, how to train mu_k and \\Sigma_k, why should we use Eq. (3) as the loss of the generator, where do we use L^I in Eq.(4),...\n\n2. The experiment section requires more works. It would be much better to do experiments with much higher dimensional data sets and compare with many other GAN algorithms e.g. InfoGAN.\n\n3. The pseudo-code has many errors. For instance,  what is \\alpha_i^0? It is not introduced. What is \\hat{\\alpha}?"}