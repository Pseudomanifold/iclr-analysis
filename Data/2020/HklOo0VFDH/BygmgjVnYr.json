{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "I think this is a good study, unless I miss something. It proposes a new solution to one of the fundamental problems in the field, which significantly improves previous solutions in terms of accuracy. I will recommend it for acceptance unless I miss something. (I\u2019m not an expert in the field and the problem seems to be so fundamental, making me cautious about judging the novelty of the proposed solution.)\n\nThere seems to be some unclarity about the optimization algorithm. In short, I suspect that the proposed optimization has some difficulty with its convergence. \n-\tIn Sec.4, the authors suggest \u201ca two-step block coordinate descent algorithm to alternate between (i) optimizing y\u2019s while g\u2019s are fixed, and (ii) optimizing g\u2019s while y\u2019s are fixed\u201d. They further suggest `a variant of the Viterbi algorithm\u2019 for (i) and gradient-based algorithms for (ii). These seem to make sense to me so far. \n-\tThen, in Sec.6, they state that \u201cWe use the Nesterov optimizer with a learning rate of 0.1. All \\mu\u2019s are initialised with 0.5 and are multiplied by 1.2 after 5 epochs\u201d. I think this needs some explanation. \n-\tFor instance, what is the thought behind the choice of the optimizer? Why was Nesterov\u2019s acceleration necessary instead of plain GD? Is it essential to use the scheduled adjustment of \\mu?\n-\tIt states \u201cAll \\mu\u2019s\u201d but there seems only a single \\mu in Eq.(5). Am I missing something?\n-\t Some explanation on what \u201cNesterov optimizer\u201d and \\mu would also be necessary for a wide range of readers. \n-\tThere is a statement on the initialization of the optimization in p.7: \u201ch corresponding to the beam search solution to initialize the g variables\u201d. How sensitive is the optimization to the initial values? For instance, will the results change if g\u2019s are initialized by the solution of the greedy method. \n-\tThere is a plot without a figure caption in p.8, which shows how the objective cost decreases with parameter updates. Why are their values at \u2018epoch\u2019=0 lower than those at the subsequent epochs? Does this mean the initial values give more optimal parameters?\n\nAdditional comments:\n-\tIt would be more friendly to the readers to show the definition of the notations like y^n_1. \n-\tThere is a statement like \u201cDecoding was run for 10 epochs.\u201d I suppose epochs here mean iteration count of the alternated parameter updates in the proposed algorithm. Why do the authors call it `epoch\u2019? \n-\tNo figure caption for the plot in p.8. It also uses `epochs\u2019 for the horizontal axis title. No axis title for the vertical axis. "}