{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Paper summary:\n\nThis paper proposes an algorithm to train a binary classifier without supervision, simply relying on (i) class prior, (ii) the hypothesis that class conditional classifier scores are Gaussian distributed. Experiments over sentiment classification and anomaly detection highlight the effectiveness of the approach.\n\nReview Summary\n\nThe paper reads well and is technically correct. It proposes a simple algorithm for unsupervised training of binary classifier. Experiments are appropriate but lack baseline comparison with generative models and a thorough description of the unsupervised validation procedure. Overall, the approach is simple and it would be a good paper with the addition of baselines (mixture) and the clarification of the validation procedure.\n\nDetailed review:\n\nThe paper is divided into two parts: a closed form solution to the problem introduced by Balasubramanian et al 2011 and an algorithm leveraging class prior for training. Both parts are clear. Since the algorithm only requires the derivative of 5 wrt model parameters, could you write this derivative (it should yield a simpler expression without erf, no?).\n\nThe experimental study could discuss the robustness wrt to the choice of p_0 and report a grid of experiments over a training set with varying true and assumed p_0. In unsupervised classification, the problem of parameter validation always occurs, e.g. for senteval, how did you select p_0 and the other parameter of the model? If you used a labeled validation set, could you report its size? Could you report the performance of a supervised system trained on a set of that size? Could you report the performance of your method when fine tuned with the label of that validation set?\n\nFor the senteval experiments, it would be interesting to report the number of labels for which the supervised and unsupervised accuracy are equal. It would be more informative than simply reporting accuracy with 100 labels. When reporting the number of labels, you need to report the size of the training and validation set combined.\n\nAs far as baseline are concerned, it would be necessary to consider generative models such as mixture models and possibly mixture model with constraints on the mixing weights (Chauveau 2013).  Adding one class SVM (RBF, polynomial kernel) and one class neural nets (Ruff, Chalapati) for SentEval would be necessary to show the advantage of your approach. A curve number of training points accuracy for all models is a must have.\n\nYou could also cite related work in adaptation to new class prior, class prior estimation, see below:\n\nDidier Chauveau, David Hunter. ECM and MM algorithms for normal mixtures with constrained parameters. 2013. ffhal-00625285v2f\n\nAdjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure\nM Saerens, P Latinne, C Decaestecker - Neural computation, 2002 - MIT Press\n\nSemi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching\nMarthinus Christoffel du Plessis, Masashi Sugiyama, ICML12\n\nClass proportion estimation with application to multiclass anomaly rejection\nT Sanderson, C Scott - Artificial Intelligence and Statistics, 2014\n\nFinally, I feel that the assumption that model scores are Gaussian distributed would be strongly justified if you could plot the distribution/run a statistical test on whether it is the case for a supervised model with the same architecture as yours."}