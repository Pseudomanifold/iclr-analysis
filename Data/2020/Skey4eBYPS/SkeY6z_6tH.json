{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "-- Summary \n\nThis paper considers the problem of developing neural processes which\nare translation-equivariant. The authors derive a necessary and sufficient\nfunctional form that the neural process \\Phi function must exhibit\nin order to be permutation invariant, continuous and translation\nequivariant.\n\nUsing the derived functional form, the authors construct a\ntranslation-equivariant neural process, the convolutional conditional\nneural process.\n\nResults in several experimental settings are given: 1d synthetic\nexperiments, an astronomical time-series modelling experiment, a\nsim2real experiment, and several image completion experiments.  All\nthe experiments show performance improvements over the AttnCNP, the\nmain baseline tested against. In the astronomy setting the authors\ntest against the winning kaggle entry, against which they get better\nlog likelihood. The authors give several qualitative experiments,\nincluding image completion tasks from a small number of pixels.\n\nProofs of all the theorems and full details of all the experiments\nare given in the appendix, along with ablations of the model.\n\n\n-- Review\n\nOverall I found this paper very impressive. It is clear how the theoretical\nresults motivate the choice of architecture. The fact that Theorem 1\ncompletely characterises the design of all translation-equivariant\nneural processes is a remarkable result which precisely specifies the\ndegrees of freedom available when constructing a convolutional NP.\n\nThe implementation gives state of the art results against\nthe AttnCNP while using fewer parameters on a variety of tasks. The image\ncompletion tasks are impressive.\n\nIt seems that the authors close an open question posed in (Zaheer 2017)\nregarding how to form embeddings of sets of varying size by embedding\nthe sets into an RKHS instead of a finite-dimensional space. This in itself\nis an interesting idea, and I am interested to see how this embedding method\nbe applied outside of the CNP framework.\n\nThe experimental results are comprehensive and diverse, showing good\nperformance on both toy examples and more real-world problems. The ablations\nand qualitative comparisons in the appendix are helpful in showing where\nthe ConvCNP outperforms the AttnCNP.\n\nMy main criticism of the work is that it's very dense, requiring a few\npasses to really grasp the theoretical contribution and the concrete\narchitecture used in the ConvCNP. I would recommend enlarging figure 1\n(b), which is illuminating but quite cluttered due to the small\nsize. Perhaps the section on multiplicity could be moved to the\nappendix to make space as it seems for all real-world datasets the\nmultiplicity would be equal to 1. \n\n\nMisc Comments\n\n- It would be good to have a brief discussion of why the ConvCNPPXL performs\nvery badly on the ZSMM task, while being the best performing method in all\nof the other tasks. I couldn't find such a discussion.\n- Did the authors try emitting a 36-dimensional joint covariance matrix over the\nsix-dimensional output in the plasticc experiment?\n- In the synthetic experiments, for the EQ and weak periodic kernels it would\nbe nice to see the `ground truth' log-likelihood given by the actual GP,\njust to have some idea of what the upper bound of LL could be.\n- In appendix C.2 Figure 6, what is the difference between the `true function' and the\n`Ground Truth GP'? I thought the true function was a gp..."}