{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThis paper proposes to introduce adversarial perturbations into intermediate layers of a neural network, to achieve more efficient adversarial training.\nThe idea has been proposed before. The paper is based on, and perpetrates, a number of fundamental misconceptions about adversarial examples. A lack of many relevant citations indicates that the authors are not familiar with the related work done in this field over the past three years. This paper is currently clearly below the bar for ICLR.\n\nWe recommend that the authors read, understand, and cite at least the list of papers below before revising their paper:\n\n- Madry et al., \"Towards deep learning models resistant to adversarial attacks\", 2017\n- Sabour et al., \"Adversarial Manipulation of Deep Representations\", 2016\n- Tsipras et al., \"Robustness May Be at Odds with Accuracy\", 2019\n- Carlini & Wagner., \"Towards Evaluating the Robustness of Neural Networks\", 2017\n- Carlini et al., \"On evaluating adversarial robustness\", 2019\n\nIn particular, this should help clarify the following misconceptions:\n- Adversarial training does not require training on both clean and adversarial examples. The best results are often obtained by only training on adversarial examples.\n- Adversarial training does not require extra memory\n- While expensive, adversarial training does not require days of computation on hundreds of GPUs (not even on ImageNet)\n- There is no evidence that adversarial training with PGD produces \"non-diverse\" adversarial examples (for the chosen perturbation set)\n- FGSM is not a good benchmark for training or evaluation"}