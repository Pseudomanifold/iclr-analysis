{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors try to tackle the problem of adversarial examples by introducing a special set of bias weights into the neural network. There are serious clarity issues with the writing of this paper. It cites Wen & Itti 2019 but provides few motivations and explanations of algorithmic choices. \n\nMy questions include: \n  - Is there a separate bias term for clean examples? When is the adversarial bias used? Is it only for adversarial examples? \n  - How is the adversarial bias term updated? In Algorithm 2, its gradient is a sum over samples, but over what samples? A mini-batch? \n  - What are the multi-modal weights? There is no forward equation on how they are used. We only have their updates in Algorithm 2. \n\nThese are all very confusing, given the central importance of Algorithm 2 to the paper. The authors should start with how the new bias terms affect inference and predictions, before going to their updates. \n\nThere are also limitations in the experiments. Only Fast Gradient Sign Method (FGSM) is used to generate adversarial examples. The more powerful projected gradient descent should be used for a better test against adversarial examples. Also, only MNIST and FashionMNIST are tested. The authors should consider including CIFAR10, CIFAR100, or other datasets. \n\nOverall I think the issues with clarity and experiments in the paper make it not ready for publication yet. \n\n\n"}