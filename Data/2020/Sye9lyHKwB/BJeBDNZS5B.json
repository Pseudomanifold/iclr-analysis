{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper addresses the problem of robustness for deep learning models in computer vision tasks. The authors proposed a new method to quantify uncertainty of output from a trained network under three ways of simple perturbations: infer-transformation, infer-noise, and infer-dropout. The perturbations were only operated during inference without influence the training process. The authors claimed that this method could achieve uncertainty estimation without to re-train, re-design, or fine-tune the model. The authors evaluated the effectiveness of proposed method in two regression tasks: super-resolution (via SRGAN) and depth estimation (via FCRN).\n\nThe paper is a pure experimental one. It is well written and easy to follow. The \"tolerable\" idea and corresponding methods are straight forward and easy to scale up. The reviewer hasn't found major flaws in the problem statements and experiments. The reviewer is not working on related topics and is likely to miss related work. The authors should compare the proposed evaluation methods with existing methods on quantifying uncertainty of neural nets."}