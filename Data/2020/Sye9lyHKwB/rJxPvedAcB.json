{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses the problem of deriving better predictive uncertainty estimates from pre-trained models. As the paper notes in the introduction, this is an important problem for machine learning models embedded in real-world decision-making systems, where the consequences of inaccurate predictions may be high, and better insight into model uncertainty can help avoid bad outcomes due to misplaced trust in model predictions. The paper focuses on training-free uncertainty estimation, in which uncertainty estimates are driven by input perturbations and modifications to \u2018grey-box\u2019 models (such as application of dropout to a trained model). These methods avoid the computational burden of applying uncertainty estimation during the training process. The paper evaluates the methods on two regression tasks: image super-resolution and monocular depth estimation. Evaluation metrics are based on the correlation between predictive variance and loss. The paper concludes that the training-free methods examined give comparable or better performance for uncertainty estimation, compared with state-of-the-art methods applied during training.\n\nThe paper addresses an important topic, and is generally well-executed and well-written. However, it focuses on a narrow set regression tasks (with little indication that results generalize further), and uses evaluation metrics that are nonstandard and not well-substantiated. For these reasons, I recommend that the paper be rejected.\n\nA systematic study of training-free uncertainty estimation, which is the goal of the paper, would be a significant contribution. The paper chooses to focus on regression tasks, which is a valid narrowing of scope (although the implication that uncertainty estimation for classification is a solved problem, via entropy methods, I believe is incorrect). Image super-resolution and monocular depth estimation, the two regression tasks that the paper examines, are important but do not represent a broad enough set of tasks to justify claims about the general performance of the methods (though they enable the observations in equations (1) and (2)). The performance of the methods on these two tasks alone I don\u2019t believe is a significant enough finding to warrant acceptance to ICLR.\n\nMy second major concern with the paper is in the choice of evaluation metrics, which are based on the correlation between variance of predictions (over a sample of perturbations) and the value of the loss. While I understand how these metrics provide intuition on the quality of uncertainty estimates (i.e. when a model is \u201cmore wrong\u201d, ideally it should be more uncertain), the paper does not give a good justification for the validity of the metrics beyond intuition (nor does the cited paper, Zhang et al. (2019)). The correlation metrics obscure a lot of information on the relationship between the predictive distribution and the true values, and without further justification, it is difficult to imagine trusting the resulting uncertainty estimates in a consequential downstream decision-making task. Proper scoring rules (Gneiting and Rafferty (2007), Lakshminarayanan et al. (2017)) could provide a more principled and useful basis for assessing the quality of uncertainty estimates.\n\nSmaller points:\n- The paper makes the implicit assumption that training/test sets are IID. While I understand that prediction on out-of-distribution (OOD) samples is beyond the paper\u2019s scope, high uncertainty on OOD samples is a desirable property for uncertainty estimation methods, and some discussion of this or indication of the methods\u2019 performance on OOD samples would strengthen the paper.\n- The paper implies that uncertainty estimation via input perturbation is a novel contribution; a similar method was used by Liang et al. (2018; https://arxiv.org/pdf/1706.02690.pdf) for the different but related task of out-of-distribution detection.\n- Is there a way to derive error bars for the results in Tables 1 and 2? Some of the correlation coefficients, reported to 4 decimal places, are very close and it\u2019s unclear whether the results are significantly different.\n"}