{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a very interesting viewpoint for understanding the generalization in deep learning, where the concept of generalization is defined as \u201cthe difference between training error and inference error\u201d, and covers the concept of adversarial robustness.  More specifically, \n1. The author treats the deep model as a source code, and provides some theoretic analysis based on the Kolmogorov complexity. The insight drawn from this theory is: finding a better source code is equivalent to minimize a Kolmogorov complexity term given the true source code; as a result, a learned classification function needs to be sufficiently complex to minimize the generalization gap. \n2. The author tries to approximate the normalized information distance and form an effectively computable optimization problem, which suggests that one can use channel codes on input features as additional inputs. \n3. The experiments show that using additional encodings improve robustness in several settings, including in the sense of adversarial robustness. \n\n\nPros: \n1. The writing is pretty clear. \n2. The complexity viewpoint provides more insights for understanding generalization, which is something not covered by learning theory. \n3. The empirical method that the authors proposed is simple, and computationally friendly. \n4. I personally like the way adversarial robustness is covered in this paper, especially when comparing with tons of other adversarial robustness papers. \n\n\nCons: \n1. Can you provide exact reference for the definition of normalized information distance? It is not directly mentioned in (Bennett 1988) through a quick scan. The normalization seems important for the theoretical analysis, so it would be better to explain where this definition comes from more clearly. \n"}