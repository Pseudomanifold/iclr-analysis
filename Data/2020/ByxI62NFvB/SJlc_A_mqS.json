{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "1. Summary: In this work, the authors proposed a coset-effective interactive machine learning framework that leverages the attention mechanism and neural process. The proposed framework can reduce the amount of time used by human annotation and avoid retraining of neural the neural network. They conduct experiments on three datasets from different domains to demonstrate the effectiveness of their model. Extensive analysis from different angles is also included in this work.\n2. Overall: It's an interesting paper to read. It's well written and easy to follow. However, the idea seems to be pretty incremental as it stacks multiple existing techniques together without many innovations. Meanwhile, more experiments over large and more complicated datasets should be added to strengthen the experiments of this work.\n3. Comments:\n3.1 Figure 4 needs some more explanations. Do you sum up the attention of each feature belonging to different types of instances(false positive, false negative, etc)?\n3.2 It seems to me that the Fitness data and Realstate sales transactions dataset do not naturally have very clear attention distribution as the prediction is affected by multiple factors in a complicated way. How do the annotators assign annotations to these datasets?\n3.3 More description of the human annotation part needs to be added. How many annotators do you use? How do you deal with disagreement? How consistent are human annotations?\n3.4 Why focus on the time series prediction problem? It seems to me that this framework can be applied to any kind of problems. Is there any specifically reason the authors only focus on the series prediction problem? Meanwhile, it looks these three datasets do not have very rich sequence information as they can be very sparse and data points can be far away from each other in terms of time. For the realstate sales transaction dataset, do many houses have transactions every year, which seems not so realistic?\n3.5 Only selective and randomness are compared in Figure 6. How about other strategies? One relevant problem is that not many other interactive/active learning strategies are compared in this work."}