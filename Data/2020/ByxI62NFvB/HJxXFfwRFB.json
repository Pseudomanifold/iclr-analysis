{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "# Summary\nThe paper presented an interesting workflow, where an attention-based model, once initially trained from a dataset, is to be successively iteratively updated based on explicit \"corrections\" on the attention map received from domain experts, and where such expert annotations (in regards to them being directed at which samples and which attention positions) are also to be selected interactively based on the current model. The overarching goal here is to 1) make efficient use of the (potentially) sparse expert annotations (on attention maps) to continuously improve the end model; 2) learn a meaningful attention map to help with increasing model interpretability; and 3) achieving 1) and 2) while minimizing annotation costs.\n\n# Conclusion\nOverall, this is an interesting setting and the technical solutions presented in the paper look reasonable in principle. However I find it a bit hard to advocate acceptance just yet, mainly because of two concerns that a) this paper is largely about stitching multiple existing techniques (e.g. Neural Processes, Influence functions, Monte Carlo dropout, etc.) together and may hence be a little bit lacking in its own originality, and b) both the presentation quality and the empirical studies still leave quite a bit to be desired (details to follow).\n\n# Detailed comments\n1. On using influence functions for annotation candidate selection (Eq.6) - why taking the \"absolute value\" of the influences rather than simply sticking with the raw values? I would have thought the sign of the influence to be meaningful and you would actually like to focus on those that have the most negative influences (as per the text)?\n\n2. I find it a bit strange that, across the entire paper including the supplementary materials, nowhere is it explained what exactly is the loss function being used in this paper. And to add to the confusion, the critical NAP process itself is also not given a proper introduction, e.g. what's the ELBO in this case and how is it related to the original loss function L? Questions like such left unanswered, it makes one further wonder why would samples selected using influence functions (based on the original loss L) even be helpful during the iterative refinement stage (which supposedly follows NAP and uses a different loss).\n\n3. What's the rationale behind selecting \"top P\" validation points rather than just using all of them?\n\n4. In the text it says \"- an embedding network ... embeds x into v\", but in Figure 2.A-1, it seems like it's mapping x to \"l\"?\n\n5. In the caption of Table 1, it says \"four electronic health records datasets\" but the number seems to be three everywhere else?\n\n6. \"s^{valid}\" looks like a single point, maybe use curly fonts for sets like this?\n\n7. I probably wouldn't call the performance difference in Table 1 and 2 \"significant improvement\".\n\n8. I also find the naming of the methods and baselines in the Experiments section a bit confusing. It's very hard for me not only to remember what each method is about from their names, but also to contrast them in a meaningful way for an implicit ablation study.\n\n# Typos (an incomplete list)\nretrainig, tihs, proces, native (should be naive), depeding, veriables"}