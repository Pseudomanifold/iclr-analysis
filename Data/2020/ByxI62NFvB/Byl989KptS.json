{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Cost effictive attention\n\nThe paper introduces an approach to effectively exploit human annotated attention maps using neural processes and select the most useful training instances to collect such annotations for using importance functions. Experiments show that the methods indeed lead to improved quantitative and qualitative results on 3 datasets.\n\nOverall, I like the ideas presented and the high level approach (importance functions and neural processes for active learning). However, I am a bit troubled by the datasets the methods are applied on. These seem overly simplistic to me. The input domain consists of a few variable classes (<100). I believe for the given tasks, the model has to mostly focus on a specific type of variable (e.g., smoking or blood pressure for classifying CVD) without really considering the other variables. A few expert annotations would be enough for the model to figure that out. E.g., one simple baseline would have been to use annotations of random examples apriori to select only the most annotated variables for each dataset and then train the model only on those. I think this willl lead to similar if not more improvements as it removes most confounders completely apriori. Finally, given the rather marginal improvements I am not sure the approach has a really big impact, other than eliminating some of the confounders.\n\nI am also a bit confused by the choice of architecture (attentional LSTMs) for non-time series data and the lack of explanation how and why these are used for the different datasets. I am also not sure about the practicality of the approach given the computation of the Hessians for the importance functions. I would like this to be discussed by the authors.\n\nAlthough the paper definitely has some interesting contributions, given the aforementioned considerations especially wrt. the experiments (datasets) I am leaning towards rejecting this paper in its current form.\n\n\nStrengths:\n- General methodology of using importance functions to select examples for labeling is promising.\n- Application of neural processes to avoid costly retraining.\n- Interesting results suggesting that importance weighting works reasonably well for the tested datasets, and that the suggested neural process indeed gives the model additional performance improvements.\n\nWeaknesses:\n- Datasets are very uncommon. The input domain is typically very simplistic (e few types of variables), as opposed to more general data types such as text, images, etc.\n- There are no baselines from the literature to ensure that the models used are somewhat competitive to state-of-the-art.\n- The results are not very convincing. Improvements are rather small, given the increased complexity of the overall approach.\n- Practicality: Computing Hessians for typical models  with millions of parameters is not practical. No solution or discussion about practicality is done.\n\nComments:\n- \\alpha_{var}  --> I would not call it var but state\n- Figure 1 could benefit from a description in the caption.\n- Use \\appendix before starting the appendix.\n- There should not be references after the appendix.\n\nQuestions:\n- Why do we do attention over the in RNN inputs (v) and not their outputs, as it is typically done because those contain contextual information?\n- What's g and h from the LSTM? The output and the internal state of the LSTM? If yes, typically only the output of the LSTM is used and not also the internal state.\n- Isn't the computation of the Hessian too expensive?\n- How is the LSTM applied to the different inputs of the very different types of datasets? The datasets are not time series, so why was a LSTM applied in the first place?"}