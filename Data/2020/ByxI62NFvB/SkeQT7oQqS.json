{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors introduce an interactive attention learning framework called Neural Attention Process (NAP) which produces weight predictions over the input variables in addition to the usual target predictions. The motivation behind this type of model is to obtain an interpretable explanation of the model\u2019s output which can be shaped by a small set of human expert annotations. At a higher level NAP seems to be an extension of a previously introduced model called RETAIN with an additional component inspired by Neural Processes (NPs) which incorporates the annotations of the experts. The authors test their model on five different tasks from 3 datasets (health records, fitness and real estate) and compare to a number of related baselines.\n\nWhile I think the idea is interesting, in my opinion this paper is not ready for publication and I would argue for it to be rejected, for the following reasons:\n\n* Presentation: The paper is over badly written, sloppy and not well structured.\n        * Badly written: there are lots of grammatical errors in the paper to the point that it severely affects its clarity (for example, the subject of a sentence often changes from plural to singular or vice versa within the same sentence and it\u2019s unclear what the authors are referring to). \n        * Sloppy: many variables are not defined in the paper, and some are called different names depending on the figure or part of the text (i.e. the use of \u2018s\u2019 or \u2018t\u2019 interchangeably to refer to the iterations). At some point even the name of the model (referred to as NAP for the majority of the paper) is just referred to as NP-Selective in the text and some figures (compare nomenclature in table 1 vs figure 5). This lack of attention to detail (although the name of the main model should hardly count as a detail) makes me worry about the scientific rigorousness of the work. \n        * Not well structured: The paper is not very easy to follow as there are jumps between different models (RETAIN, NPs, NAPs) which are not always evident. On that note, I would have liked to see an objective function to know what exactly is being used to optimise theta.\n\n* Significance: Overall the results don\u2019t seem incredibly significant (e.g. figure 6). If one of the main claims is that the number or annotated examples is smaller with NAP the authors should provide experimental evidence that this is the case (as this would be one of the most significant benefits of the model).\n\n* Motivation: a key motivation for this paper, as stated by the authors, is the desire to obtain interpretable representations, which in itself is an important research problem. However, I am not sure that the suggested solution is addressing this issue. If I understand correctly, what the model learns is, in essence, a weight mask over the inputs that determines how much each of the dimensions of the input contribute to the output. This could be achieved with any attention model in and is in itself not novel. I think a more interesting motivation for this model might be its ability to incorporate some human annotations that teach the model where to attend. However if this was the main story I think the authors should focus on making this a stronger argument and proving that narrowing down the attention with said annotations actually leads to better/faster/more robust learning, rather than focussing on the interpretability narrative.\n\nSome more specific comments and recommendations for the authors (I think there are some positive aspects of this paper and I hope that I can give a few points of advice to help improve its quality.) I will go through them in the order of the paper:\n\n1) The name Neural attention process might be a bit confusing given that there are already Attentive Neural Processes. I would strongly recommend changing the name.\n2) Check the language and make sure it\u2019s correct, the introduction is particularly full of errors and you don\u2019t want to lose the reader from the beginning.\n3) If you are talking about interpretability you would have to mention as well all the work on interpretable representations (e.g. disentangled representations etc) and if you talk about attention, it is probably worth mentioning transformers.\n4) Algorithm 1: this is not very clear. I would define the variables at the top that are not self-explanatory (such as S). Also, from the look of the algorithm theta is not updated in the else loop.\n5) The change in notation is very confusing. It\u2019s not always clear when you use \u2018v\u2019 vs \u2018l\u2019 or \u2018s\u2019 vs \u2018t\u2019. Sometimes it seems it depends on which of the different models you are talking about, but either way you should try to make it all uniform.\n6) I would strongly recommend to specify the loss function and the regularisation term as this will make the task much clearer to the reader.\n7) I would make the difference to RETAIN very clear and argue why you think your addition improves on the original (from the text at the moment it\u2019s not even clear when you are describing RETAIN and when you talk about your model, maybe making a subsection would help?)\n8) Figure 3 is too small.\n9) You claim to have 5 datasets, but the way I would interpret your setup, you have 5 tasks from 3 datasets.\n10) I would say a sentence or two about why you chose the baselines you chose and why you are hoping that they prove your different points.\n11) Figure 6 has no axis labels on the plot itself, which are definitely necessary. I also recommend not using red and green as the two colours as it will be hard for colour blind to distinguish between those two.\n"}