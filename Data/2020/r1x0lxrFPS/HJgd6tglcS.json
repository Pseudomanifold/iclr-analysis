{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new measure of gradient mismatch for training binary networks, and additionally proposes a method for getting better performance out of binary networks by initializing them to behave like a ternary network.\n\nI found the new measure of gradient deviation fairly underdeveloped, and I suspect the method of converting ternary activations into binary activations works for a different reason than that proposed by the authors.\n\nThere were English language issues that somewhat reduced clarity, though the intended meaning was always understandable.\n\nDetailed comments:\n\n\"Binary Neural Network (BNN) has been gaining interest thanks to its computing cost reduction and memory saving.\" --> \"Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings.\" (will stop making English language corrections from here on)\n\n\"Therefore, we argue that the sharp accuracy drop for the binary activation stems from the inefficient training method, not the capacity of the model.\"\nThis could also be due to poor initialization in the binary case. e.g., it might make sense to initialize the binary network with bias=-0.5, so that the nonlinearity has a kink at pre-activation=0, rather than pre-activation=0.5.\n\n\"Unfortunately, it is not possible to measure the amount of gradient mismatch directly because the\ntrue gradient of a quantized activation function is zero almost everywhere. \" It *is* possible to measure the mismatch to the true gradient exactly. One could even train using the true gradient. It's just that the true gradient is useless.\n\nFig 1b -- this is a nice baseline.\n\n\"the steepest descent direction, which is the direction toward the point with the smallest loss at given distance\"\nThis is not the usual definition of steepest descent direction. If you're going to redefine this, should do so mathematically and precisely (for instance, you are going to run into trouble with the word \"distance\", since your coordinate discrete gradient more closely resembles an L\\infty-ball perturbation, rather than an L2-ball perturbation.\n\neq. 3:\nNote that this equation is equivalent to taking the true gradient of a function which has been boxcar-smoothed along each parameter. This may more closely resemble existing measures of deviation than you like.\n\nYou should also consider the relationship to an evolutionary strategies style gradient estimate, which similarly provides an unbiased gradient estimate for a smoothed function, and which allows that estimate to be computed with fewer samples (at the cost of higher error).\n\nSec. 4.2 / Figure 3:\nThe results in this section will be *highly* sensitive to the choice of epsilon. You should discuss this, specify the epsilon used, and experimentally explore the dependence on epsilon.\n\n\"The results indicate that the cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better than previous approaches. \"\nDon't know that I followed this. Gradient mismatch is never formally defined, so it's hard to know what this says about its relationship. Additionally, CDG sounds more like something which is correlated with, rather than an explanation for, performance.\n\n\" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" --> \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \"\n\n\"we shift the bias of BN layer which comes right before the activation function layer. \"\nDid you try using these bias values without pre-training as a ternary network? I suspect it would work just as well!\n\n\"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage.\"\nDid not understand this.\n\n\"it is expected that the fine-tuning increases the accuracy even further\"\nDoes it improve the accuracy further? Should state this as result, not prediction, and should have an ablation experiment showing this.\n\n\"Table 2 shows the validation accuracy of BNN in various schemes.\"\nWhy not test accuracy?\n\nFigure 6:\nWhat are the filled circles?\nWhat was the sampling grid for the HP search? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function, rather than in the data."}