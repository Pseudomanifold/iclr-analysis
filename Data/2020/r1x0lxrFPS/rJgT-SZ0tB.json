{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies activation quantization in deep networks. The authors first compare the coordinate discrete gradient and those obtained by various kinds of straight-through estimators, and found 1-bit activation networks have much poorer gradient estimation than 2-bit ones. Thus they speculate that this explains the poorer performance of 1-bit activation networks than 2-bit ones. To utilize higher precision of activation, the authors then propose to decouple a ternary activation into two binary ones, and achieve competitive results on typical image classification data sets CIFAR-10 and ImageNet.\n\nThe paper is overall well-written and easy to follow. The decoupling method is simple and straightforward. The experiments are also well conducted. One main concern is that since the computation of the decoupled binary model and the coupled ternary model are the same, why does the decoupled binary model can finally to tuned to perform better than the original ternary model? Is there any intuition or theoretical explanation? Yet another concern is that ternary activation basically can be viewed as binary+sparse activations, can it be even more computationally cheaper than the decoupled binary activation?\n\nQuestion:\n1. One line below eq (2), does STE mean the estimated gradient? How can the difference be calculated based on different things (i.e., activations and gradients)?"}