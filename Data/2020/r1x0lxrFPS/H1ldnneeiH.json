{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "To be honest, I only read several papers on the neural network quantization. I am not familiar with this research topic, so I  provide my judgement based on my own limited knowledge rather than thorough comparison with other related works.\n\n1. The motivation is clear. The 1-bit activation networks usually deteriorates the performance greatly.\n2. The gradient mismatch for discrete variable did bring difficult for optimization. Do you mean 1-bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper?\n3. As to Eq(3), Appendix C.1 describes the way to choose step size. I understand the logic, but for the detailed method, is it cross-validation with grid search or some other tricks?\n4. Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}