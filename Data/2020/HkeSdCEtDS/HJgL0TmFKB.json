{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper is concerned with reducing the amount of manual dialog state or dialog act labeling in task-oriented dialog, following the lead of Eric and Manning (2017) who did not require any such explicit annotation. The authors note that Eric and Manning\u2019s approach didn\u2019t do well on more recent dialog datasets in comparison to e.g. Sequicity (Lei et al., 2018), which trains an explicit belief state. The first main contribution of the submission is to exploit GPT-2 to outperform Sequicitiy and other techniques, doing so without explicit dialog state/act supervision. The second contribution is an alternating parameterization of the model that distinguishes between agent and user utterances. Experimental results show that the method of the paper (ARDM) is either superior or on par with methods that exploit hand-labeled dialog states/acts.\n\nOriginality/impact:\n\nI think the approach of the paper is well motivated, though quite simple. The idea of improving on a generation task with GPT-2 is a tried one, and its effectiveness isn\u2019t really surprising. The alternating parameterization is interesting but has been done before in (e.g.) Zhao and Kawahara (2019) and (Zhang et al., 2019) [1]. Overall, I think the paper doesn\u2019t bring much novelty, but results are promising, and this work represents a step towards making task-oriented dialog system less reliant and hand coded linguistic information. It could have an impact for the dialog community.\n\nEmpirical contribution:\n\nMy main concern with the paper is a lack of ablation, which makes it hard to understand where the improvement comes from. More specifically:\n\n* It is not known how much the alternating nature of ARDM contributes the overall gains compared to (1) other details of the model; (2) pre-training with GPT-2. One way to ablate ARDM\u2019s alternating mechanism from other characteristics of the model would be for example to tie the parameters of the user and the agent. \n\n* Table 1 compares GPT-2 with ARMD, so we don\u2019t know if the improvement comes from either fine tuning on CamRest676, from ARDM\u2019s alternation between speakers, or from other characteristics of ARDM. Note: The text characterizes GPT-2 as \u201ca pre-trained large-scale language model GPT-2\u201d, so I presume \u201cGPT-2\u201d here means without fine-tuning.\n\n* Results of Table 3 are perplexing and more analyzes would be needed to understand what is really happening. Considering that both TransferTransfo and ARDM are based on transformer and GPT-2, and are apparently fine-tuned on the same data, I find it strange the gap between the two is so big on perplexity (10.1 vs TransferTransfo\u2019s 19.9) while TransferTransfo is actually superior on BLEU. I understand BLEU is problematic for dialog evaluation as it treats references (gold responses) as the only reasonable responses (Liu et al., 2016), but that makes perplexity as an evaluation metric problematic for exactly the same reason. In fact, (Schwenk et al., 2006)[2], (Luong et al., 2016)[3], and others have shown that BLEU and perplexity are highly correlated, which isn\u2019t surprising as both penalize generated responses to the extent that they lexically differ from the gold standard. So, these results look a bit strange/suspicious to me, and I think deserve further investigation. Sample outputs of both TransferTransfo and ARDM might help better understand the problem (Why showing only ARDM\u2019s? When showing generation examples, I think it is standard practice to show outputs of multiple systems, and not just for the system of the paper). \n\nMinor comments:\n\n- [Manning and Eric, 2017] should be [Eric and Manning, 2017]\n- ARDM with \u201crecurrent\u201d maybe isn\u2019t a good name as it may imply it is RNN instead of Transformer based.\n- Section 3.1: I suggest you explain the \u201cmemory mechanism\u201d more formally and in more details, e.g., M_{t-1} doesn\u2019t seem to be used anywhere.\n- \u201cassume there is only one layer in Transformer\u201d: Is it for the sake of the presentation only, I assume?\n- Tables 1-3: perhaps add confidence intervals as some of the differences are quite small, and so are some of the test sets.\n- Table 3 doesn\u2019t seem to be referenced anywhere.\n- Table 4: I would recommend showing ARDM and TransferTransfo side by side.\n\n[1]: https://arxiv.org/abs/1903.05759\n[2]: https://www.aclweb.org/anthology/P06-2093/ (e.g., Fig. 5 shows the high correlation between BLEU and perplexity)\n[3]: https://arxiv.org/abs/1410.8206\n\nQuestions:\n\n(1) In 4.3.1, are capacities of the two models the same? If ARDM has (roughly) double the number of parameters due to its modeling of two speakers, what about comparing TransferTransfo at same model capacity to make the comparison more meaningful?\n\n(2) I am confused how you (the authors) compute belief state success F1 for GPT-2, as the text says there is only pre-training with this model, which I think implies there is no fine-tuning on the belief state identification task. Could you provide more details?"}