{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a pre-trained language model architecture specifically used for task-oriented dialogue systems. The basic idea is to alternate between the likelihood of two parties in a dialogue. This simple yet effective approach yield significant improvements in baseline dialogue system datasets in terms of both BLEU score and accuracies. Experiments are done throughly by comparing to BERT and GPT-2, which reflected the cutting edge research in pre-trained language model. This paper opened up potential new domains that can inspire a wide range of work and it fits very well into the ICLR community. "}