{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper reports experimental results on global feature pooling for fine-grained visual categorization. The main experiments are (1) Comparison of features learned by average pooling and max pooling. (2) Comparison of nine pooling methods (3) Combination of two different pooling schemes. \n\nThe finding on the features learned by average pooling and max pooling is interesting. \nHowever, this paper is not suitable for publication because the experimental comparison of two different pooling schemes lacks details and does not support a significant contribution. \n\nThe hypothesis of the little improvement of a conventional combination of two different pooling is claimed that lower layers' filters become confused by gradient signals coming from the two different pooling schemes. However, such a high-level explanation is not convincing well. The hypothesis should be verified by equations and/or experiments. \n\nThe detail of \u201cchannel split\u201c is unclear. How is the feature map split into different splits? \n\nThe detail of the \u201cfreeze-and-train\u201d trick is unclear. Does \u201ca new linear layer with global max pooling\u201d means that a linear layer is added after or before the pooling? \n\np.10, the authors wrote that \u201cThis trick guarantees that \u2026\u201d. Why the training of a new linear layer before training whole network can guarantee that final performance becomes not worse? \n\nThis paper also proposed post-global batch normalization for achieving performance improvement and faster convergence. However, all experiments are conducted using this normalization; thus, the effects of this normalization cannot be confirmed.\n"}