{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper conducted thorough studies of the pooling layer types on fine-grained classification tasks. It validated that the maxpooling would encourage sparser final-conv layer feature maps than avgpooling by both qualitative results and quantitative results with two proposed reasonable metrics. Also, the experimental results show that models with the global maxpooling layer would outperform models with the global avgpooling layer for fine-trained classification tasks. Related work is listed in detail.\n\n\nHere I have several concerns:\n\nWill the global pooling encourages object-level features and thus bring benefits for coarse-grained classification tasks? These results will complement the argument proposed in the paper and should not include it in future work. We can also validate the effectiveness of post-global batch normalization in the coarse-grained classification setting.\n\nAlthough the author has discussed the differences between training from scratch and finetuning, I think it is still required to conduct the training from scratch experiments and compare the final scores for different methods, especially for the Table 2. Will the results align with the current version? If not, the proposed 'freeze-and-train' trick is not much solid.\n\nFor the discussions about learnable generalized pooling, we cannot directly draw the conclusion from the phenomena shown in Figure3. Because the learnable generalized pooling will dynamically change its type according to the specific input image which is quite different from the fixed type.\n"}