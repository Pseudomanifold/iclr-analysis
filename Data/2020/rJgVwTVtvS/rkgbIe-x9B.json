{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the problem of differentially private optimization in the (strongly) convex setting. The authors focus on the gradient perturbation methods, i.e., DP-GD and DP-SGD, and provide the utility guarantees of DP-GD and DP-SGD under the so called expected curvature assumption. However, it is very hard to verify the expected curvature assumption, and thus the theoretical results may be invalid. I summarize my main concerns as follows:\n\n1. All the theoretical results provided in this paper are based on the expected curvature assumption (Definition 3). However, it is unclear what kind of loss functions will satisfy this assumption. If the authors can prove that this assumption can hold for some specific loss functions, such as logistic loss or square loss, the contributions of the current paper will be much stronger.\n2. Again it is unclear how large $\\nu$ will be compared to $\\mu$, and thus the theoretical results can be useless.\n3. Since the authors use the approximation form of the gradient in equation (2), why the first inequality in equation (2) holds according to Definition 3?\n4. How do you get the average and minimum curvatures in Figure 1?\n5. I don\u2019t think the argument in the last paragraph in section 3.1 is sound. Because for the restricted strongly convex function, we will ensure the loss curvatures stay in certain directions during the training process.\n6. The contribution of the current paper is very incremental. All the proofs are just replacing the strongly convex condition with the expected curvature condition.\n7. There are several gradient perturbation based DP algorithms [1,2] for solving high-dimensional problems are missing in the related work.\n\nReference:\n[1].Talwar, Kunal, Abhradeep Guha Thakurta, and Li Zhang. \"Nearly optimal private lasso.\" Advances in Neural Information Processing Systems. 2015.\n[2].Wang, Lingxiao, and Quanquan Gu. \"Differentially Private Iterative Gradient Hard Thresholding for Sparse Learning.\" 28th International Joint Conference on Artificial Intelligence. 2019."}