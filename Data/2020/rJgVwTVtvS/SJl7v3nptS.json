{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Differential privacy (DP) can be achieved by perturbing the objective function, the output or the gradient. In this paper, the authors consider gradient perturbation and claim that it is more advantageous than other methods. To prove this claim, they present a novel utility analysis by taking the noise into account. The previous papers (like Bassily et. al) present utility guarantees in DP context, but their analysis follow the same steps used for non-noisy setting. In non-noisy setting, the analysis is based on strong convexity parameter \\mu which is the minimum curvature. However, in this study they present \u201dexpected curvature\u201d which is computed by considering the noise variance and based on averaging the curvatures along the number of iterations. The order of utility is given for both convex and strongly convex objectives and it has become smaller than the previous studies. Since other perturbation methods does not add noise at intermediate steps, expected curvature is the same with \\mu and the utility advantage is not valid.\n\nComments (Positive)\n\n- The paper is well-written and easy to follow. I didn\u2019t see typos or mistakes (I didn\u2019t check the last proof).\n\n- They claim that they are the first study showing the advantage of gradient perturbation theoretically (I haven\u2019t seen such a study either).\n\n- Since they remove the dependency to minimum curvature \\mu, they present utility order for both convex and strongly convex objective for DP-GD and DP-SGD.\n\n- With the help of privacy noise, they obtain a better utility which is an interesting contribution.\n\nComments (Negative)\n\n- In the numerical experiments, number of iterations are taken as 20, 200 and 800. It might be checked for more iterations. The chosen privacy levels are tight enough (0.1 - 1).\n\n- The learning rate of DP-SGD is divided by 2 at the middle of training. The reason and whether it is applied to other SGD method is not clear.\n\n\nOverall, this type of utility analysis exists in the DP literature, but their novelty comes from the idea of averaging the curvature."}