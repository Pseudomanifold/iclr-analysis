{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "~The authors propose an enhancement to the transformer architecture that takes molecule graph structure into account.~\n\nI applaud the authors work on making more physically plausible machine learning constraints. However, I feel like this work is incremental and does not vastly improve SoA.\n\nFor all tables, what are the error bars on the accuracy?\n\nThere are multiple possible SMILES strings for any unique molecule (which includes the canonical SMILES string.) Does bootstrapping your Transformer model with multiple non-canonical SMILES for the same input molecules improve performance?\n\nMany seq2seq molecules allow an attention mechanism on the input sequence while decoding, and that seems like this would be useful for this data. What would the impact of this be?\n\nDo you have any explanation why GET-LT1 is your top performing model?\n\nDoes your model generalize to unseen molecules or reactions better than previous methods?\n\nIn Table 5, why does the % invalid SMILES go up with more beams? I would figure that larger beam sizes would result in more valid generated molecules?\n\nIn Table 4, please report the number of parameters for each model.\n\nIn Table 2, what are the astrices (*) in the GET-LT1 row?\n\nIn Table 3, you should bold the Similarity model for the top-5 and top-10 accuracy.\n\nIn equation 9, do you mean \u201csigmoid\u201d?\n\nIn equation 8, should the k superscript be a subscript?\n\nHow would this model perform with SELFIES representation of small molecules, which are more robust representation [Krenn et al., 2019]?\n"}