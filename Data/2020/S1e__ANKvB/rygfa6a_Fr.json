{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThis paper focuses on the retrosynthesis prediction problem which to my understanding is the factorization of a target molecule into simpler structures. Previously, retrosynthesis prediction has been tackled as a language translation problem by using a Seq2Seq algorithm called \"Transformer\". This sequential approach was possible because molecules can be expressed as strings using the following format: Simplified Molecular-Input Line-Entry System (SMILES).\n\nDespite its good performance, language translation methods ignore the graphical structure of molecules. This paper proposes to add a graph neural network in front of the Seq2Seq/Transformer network to exploit the graphical structure, hence the name \u201cGraph Enhanced Transformer (GET)\u201d. The Graph Neural Network considered in this work also uses an attention mechanism similarly to Graph Attention Networks.\n\nThe main contribution is the addition of a Graph Neural Network to a Seq2Seq model for retrosynthesis prediction which provides state of the art results.\n\nFrom a machine learning / representation learning perspective, I do not consider the paper is innovative enough. Even so, the paper shows strong results for retrosynthesis prediction, I guess it would be a good fit in a chemistry conference.\n\n\nThings to improve:\n\nThere are plenty of works related to graph Autoencoders and Graph generative models that are not mentioned in the related work. It would be great to dedicate a section for them. Here some examples: \n\n- Li, Y., Vinyals, O., Dyer, C., Pascanu, R., & Battaglia, P. (2018). Learning deep generative models of graphs.\n\n-Simonovsky, M., & Komodakis, N. (2018, October). Graphvae: Towards generation of small graphs using variational autoencoders.\n\n-De Cao, N., & Kipf, T. (2018). MolGAN: An implicit generative model for small molecular graphs.\n\n-You, J., Ying, R., Ren, X., Hamilton, W. L., & Leskovec, J. (2018). Graphrnn: Generating realistic graphs with deep auto-regressive models.\n\nAll experiments are compared to author\u2019s re-implementations of other works. It would be interesting to directly report other work accuracies.\n"}