{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a new normalization scheme, cross normalization, that stabilizes the off-policy reinforcement learning algorithm. The results show that by simply performing batch-normalization, where the mean and variance statistics are computed with both behavior and target action samples, it can increase the performance of DDPG and TD3 algorithm consistently. The paper also shows that it prevents the algorithm from diverging even when the target network is removed, showing the source of stabilization.\n\nThe results are surprisingly good when the simplicity of the algorithm is considered. Nevertheless, I think the paper is not providing enough theoretical backups for the claimed algorithm, and it prevents me from being completely convinced. Also, the paper does not seem to be a complete draft - there are many points that seem to be incomplete. I think it would be much better if the paper develops some theory behind the normalization, referring some previous results as (Liu, Yao, et al. \"Representation balancing mdps for off-policy policy evaluation.\" Advances in Neural Information Processing Systems. 2018.). For now, I feel that the paper is not ready for publication.\n\nHere are some problems with the paper I found:\n\n1. In the introduction, the paper says that the paper investigates convergence: where is the convergence investigation?\n\n2. At the top of page 4, the sentence is not complete\n\n3. Below eq (1), it is written as \"the second order moments of the variance\". Is it the second moment, or the variance? How do you convex combinate variances? Is it OK to do so?\n\n4. While it is claimed in the paper that TD3 + CrossRenorm (alpha=0.99) performs well, it is not really justified why (alpha=0.99) is crucial. While it is written in the paper that \" As the distribution of the off-policy actions from\nthe experience replay changes considerably slower than the action distribution of the constantly\nchanging current policy,\" such point can also be applied to DDPG and it does not explain why alpha=0.99 is needed for the only TD3. The paper also lacks experiments about BatchRenorms on DDPG and TD3, which would be a fair comparison against CrossRenorm.\n\n5. Why do we need Figure 4? Is it only for the comparison against SAC?\n\n6. Below eq (2), the paper says about big \\Phi, but it is never defined and not used anymore. What is it about?\n\n7. The stability improvement analysis implies that the mean-only crossnorm is sufficient for stabilization. Why do we need variance normalization then?\n\n\n\n"}