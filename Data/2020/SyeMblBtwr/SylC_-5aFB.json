{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a novel normalization strategy for Off-Policy TD Reinforcement learning. Normally, Off-Policy TD RL is stabilized by usage of a target network, which has the disadvantage of slowing down the learning process. The paper first shows the effects of using existing normalization methods (batch and layer normalization) in the context of OPTD methods. Those approaches are shown to be inferior to target networks, because the data (actions in off-policy transitions and on-policy transitions) is coming from two different distributions. Experiments show that those normalization methods do not lead to consistent improvements over the benchmarks.\nTo tackle this problem, the authors introduce a cross-normalization scheme that works across the two datasets in this context. Cross-normalization is achieved by calculating a mixture of the mean values of both on- and off-policy state-action pairs. The weight of the contribution of those distributions is handled by a hyperparameter, which defaults to 0.5; thus using a balanced influence on both distributions (CrossNorm). Since the mean features of the off-policy data are more stationary, two strategies are applied: First, the hyperparameter is set to 0.99, thus giving more weight to the off-policy data, which is less volatile. Second, mean and variances are computed over several batches to increase stability (CrossRenorm). It finally is shown that the CrossRenorm approach is able to surpass state-of-the-art performance on the MuJoCo benchmark while having the benefit of not needing a target-network. In further experiments, it is shown that CrossNorm stabilizes learning in most contexts, but does not guarantee to converge in all settings.\n\nOverall, the paper manages in a very clear and structured manner, (1) to show the current approaches for stabilizing learning and their downsides, (2) to show why common normalization methods fail and (3) formulates a possible solution for this problem. Furthermore, empirical results are not only shown, but also analysed. \n"}