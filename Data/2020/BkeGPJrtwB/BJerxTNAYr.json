{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method for adding an approximate disparate impact loss to a classification objective, and show that optimizing a classifier for this loss leads to \"fairer\" predictions with little or no accuracy loss.\n\nThe authors first formulate two notions of fairness in terms of earth mover distance between the distribution of scores conditioned on either value of a protected variable. They then show that the dual formulation of the earth mover distance can be approximated (specifically, lower-bounded) by optimizing the parameters of a neural network under spectral norm constraints. This leads to a min-max global optimization scheme to learn a fair classifier.\n\nStrengths: The proposed method does better on the considered fairness metric than a GAN model with similar accuracy.\n\nWeaknesses: The paper is difficult to read, glosses over some important details, and contains some inaccuracies.\n\n-- Clarity: \n--- The authors need to better describe the assumptions (or lack thereof) made on the joint distribution of X, S, and Y. \n--- Measures such as the quantiles or probability laws need to be formally defined before they are used in definitions. \n--- The \\mathcal{L} notation is overloaded (it is used for probability laws, conditional and unconditional, as well as marginals, with \\mathcal{L}_1 referring to both!), leading to potential confusion.\n--- The domain of X and Y in equation (2) is not defined anywhere, neither is the distance.\n--- Similar lack of consistency with the use of F / \\mathcal{F} / \\hat{f}, without any explicit parameterization\n--- Figure 2 needs to be in Section 4, and Table 1 needs to be trimmed to size\n\n-- Overlooked problems:\n--- In the dual formulation, the optimization is done over a sub-set of Lipschitz function, hence approximation of \\mathcal{W} is a lower bound at every step. Minimizing a lower bound on a loss can be justified, but requires more discussion\n--- The trade-off inherent in the choice of n_w in algorithm 1 needs to be further discussed, especially in the case of large datasets where a full epoch of SGD in the inner loop of the optimization process is impractical\n\n-- Inaccuracies:\nThe graphical models in Figures 1 and 2 and conditional independences written in the text are not consistent:\n--- In Figure 1,  X is NOT independent of S given Y (neither is Y*) (see: V structures in a directed graphical model)\n--- In Figure 2, X* is independent of S regardless of conditioning on Y\n\nConsidering all of the above issues, the paper is not currently ready for publication"}