{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed a fairness-aware learning method.\nIn particular, the authors considered two kinds of fairness problem and designed two regularizers accordingly.\nEssentially, both of these two strategies learn classifiers and calibrate the distributions conditioned on protected variables jointly. \nThe calibration of the distribution is achieved in the framework of optimal transport.\n\nThis work is a natural extension of the optimal transport-based method shown in (Barrio et al, 2019a,b). The main differences include 1) instead of calibrating distributions after learning classifiers, the proposed method achieves calibration and learning jointly, replacing the primal Wasserstein barycenter problem with the dual form of Wasserstein distance (Arjovsky et al. 2017); 2) the proposed method considers two types of fairness problem. \n\nCompared with vanilla GAN, the potential advantage of WGAN on distribution matching is well-known. It seems unfair that the authors compared the vanilla GAN-based regularizer with the proposed WGAN-based regularizer just on EMD because EMD corresponds to the proposed regularizer directly. In Table 1, although the DI of vanilla GAN is higher than that of WGAN, its ACC is also higher than that of WGAN as well. In Figure 5 (a, b), if we set lambda=0.6 for WGAN and lambda=1 for vanilla GAN, both of them can achieve ~0.838 ACC and ~0.100 DI. In Figure 5(c), what do the points represent? Why not use DI as the x-axis? Because of the issues in experiments, it is hard to evaluate the improvements of the proposed method.\n\nAdditionally, the proposed method always causes the degradation of ACC when improving DI. However, the method in (Barrio et al, 2019a) just applies a Wasserstein barycenter-based post-processing but can suppress the degradation on ACC greatly. Could the authors discuss the differences and the advantages of the proposed method in detail?  Could the authors consider more recent work as their baselines?\n\nIn summary, the method makes sense, but its novelty is limited and the improvements are incremental.\n\nMinors:\nPage 6, Line 3: Figure 3 \u2014> Figure 2.\nI suggest swapping Figure 2 and Figure 3."}