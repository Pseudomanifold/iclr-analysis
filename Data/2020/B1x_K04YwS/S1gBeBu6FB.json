{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper describes a unified span-relation-based formulation that can be used to solve a range of structured prediction tasks, including coreference resolution, semantic role labeling, dependency/constituency parsing, relation extraction, open IE, etc. The authors consolidated 10 such NLP tasks into a benchmark dataset called GLAD. \n\nThe model described in this paper is a span-related-based model proposed in a series of previous works (Lee et al., 2017, 2018; He et al., 2018; Luan et al., 2019), which builds fixed-length span representations and classifies span-span relation labels or span labels. The model achieved near state-of-the-art results on the GLAD benchmark, and showed mixed results when trained with the multi-task setup.\n\nOverall, the paper pretty is well-written and well-organized. The technical contribution seems a bit thin, as this span-based multitask model has been introduced and well-explored in previous works. That being said, I have to give the authors credit for consolidating so many different NLP tasks and performing experiments on them. My other concern with this paper is that I'm not sure how to interpret the MTL results. Unsurprisingly, there is a mix of positive and negative results across the different task pairs. It would be nice to see a bit more analysis beyond sharing representations and comparing the final F1 numbers.\n\nOther questions/comments:\n- What is STL? Single-task learning?"}