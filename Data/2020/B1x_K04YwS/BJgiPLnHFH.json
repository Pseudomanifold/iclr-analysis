{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper generalizes a wide range of natural language processing tasks as a single span-based framework and proposes a general architecture to solve all these problems. Specifically, each task can be modeled as either span identification/prediction or relation prediction based on the spans. The authors adopt an existing approaches for span-based problem with minor modifications and show the final performances on various tasks. This paper is in general well-written and easy to follow. The problem and motivation are clearly stated. The contributions involve a unified NLP task modeling with extensive results, investigation on multi-task settings by jointly training the model on different tasks, and a collection of benchmark datasets annotated with span-based information to facilitate future researches. However, a few limitations are observed:\n\n1. Both span-oriented tasks and relation-oriented tasks involves heavy computation on all possible candidates. For example, the span prediction involves an MLP for each possible span with certain window threshold. This may lead to heavy computational cost and non-intuitive learning process. Many sequence labeling methods actually exists to provide more efficient learning strategy. Is there any reason on why this specific base model is chosen? What's the advantage of using this model compared with the others? Can you show the time complexity involved in the experiments?\n\n2. There is only minor adjustment made on the original model for span-based predictions, which limits the contribution of this paper. As shown in Table 4, the performance does not dominate most of the times.\n\n3. The authors claim that they investigate their general framework on multi-task settings. However, only statistical results are provided by using different combinations of tasks as source and target tasks without further analysis on how the model could benefit on this setting. In fact, it could make more contribution by providing a training strategy using a single framework on multiple tasks so that the performance for each task is increased compared to separate training, or a pretraining strategy might be involved (similar to BERT) for such span-based problems. This could bring much more attention to the NLP society."}