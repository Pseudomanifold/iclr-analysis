{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "- Overview: This work presents a unified formulation of various (phrase and token level) NLP tasks. They use this formulation to devise a single model architecture that, when trained individually on 10 diverse NLP tasks, can get near SOTA performance on all of them. They additionally conduct experiments to determine when multitask learning is useful for this set of tasks, which is made easy due to the unified framework. The stated main contributions are\n    - unified formulation of 10 diverse NLP tasks, including NER, constituency + dependency, parsing, POS tagging, sentiment, coref., relation extraction, SRL, information extraction, and more\n    - a benchmark of pre-existing datasets for each of these tasks\n    - experiments on their benchmark in pushing the state of the art \n    - experiments leveraging their task formulation and benchmark to analyze the effectiveness of multitask learning under a variety of settings.\n\n- Review: The paper as-is is not as novel as the authors claim, and I recommend a weak reject. That said, I think there is a lot of interesting work and analyses in the latter half of the work, and I think with some reframing, this could be a really nice work.\n- I'm not convinced of the novelty of the formulating various NLP tasks as span and and relation labelling. As the authors point out (somewhat concerningly buried on a footnote on page 4), Tenney et al.[1] use present a very similar framework for these tasks, also with the aim of having uniform task format and model architecture. The authors rightly point out a key difference between their work and [1]: The latter's work uses this unified framework in the context of analyzing pertained models (with no training), rather than training a model to perform as well as on these tasks as possible. As a result, [1] uses gold span information, whereas this paper also needs to predict those spans. While this difference in setting is fairly substantial, it doesn't change the fact that [1] previously unified a similar suite of tasks with more or less the same framework, which I believe significantly weakens this work's main stated claim of novelty.\n- I appreciate the effort to standardize analysis tasks in NLP. However, I think there's some tension in trying to group the proposed set of tasks into a sub-sentence-level language understanding benchmark and calling it an analysis tool. In providing a training set for each task and expecting users of the benchmark to train/fine-tune on each task, we lose the ability to say a pretrained model contains information for X task (e.g. \"My model has a strong sense of named entities\"). On the other hand, I suppose you gain some insight into how well your pertained model can be adapted for these NLP tasks, which is also useful but I think not how analysis tasks are usually implemented.\n- Matching previous (single-task) SOTA on each of the 10 tasks individually doesn't seem hugely interesting to me. It doesn't seem too surprising that pretained LMs perform well regardless of the task format / architecture. We see this in question answering where concatenating the question to the context is as effective as QA-specific architectures [2].\n- That being said, I think there is still a lot of interesting work and results here. For me, the most interesting part is the analysis for which tasks are mutually beneficial or harmful. Determining and characterizing mutually beneficial tasks is a big open question, and these experiments are useful evidence and should be leaned into more, especially over the unified task formulation aspect. There's a lot more that could be done here, e.g. flushing out the experiments testing the various tasks under different amounts of training data or deeper analysis into why certain task pairs are helpful/harmful (genre? both are semantic-y or syntactic? lots of word overlap between the datasets? etc.).\n\nThings to improve + Questions\n- The paper would strongly benefit from a more in-depth explanation of the covered tasks, perhaps in an appendix. The explanations on p3 feel redundant. The examples in Tables 2a,b do a lot of work, but some of the tasks and labels still could use more explanation. \n- I didn't really understand the phrase \"all pairs of remaining spans\". Are those the top-K / lowest NEG_SPAN probability spans?\n- What do $j$ and $k$ index ?\n- The conclusions from Tables 5 + 6 seem to contradict one another. Table 5, as you say, seems to suggest that \"as the contextualized representations become stronger, the performance of MTL+FT becomes more favorable\". On the other hand, Table 6 says that there seem to be many more mutually beneficial task pairs when using GloVe than when using BERT. Any hypotheses as to why this is ?\n- Figure 2 is hard to read because of the size and the legend formatting.\n\n[1] WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick.\n[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin,\u00a0Ming-Wei Chang,\u00a0Kenton Lee,\u00a0Kristina Toutanova"}