{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a learnable piece-wise linear activation unit whose hinges are placed symmetrically. It gives a proof on the universality of the proposed unit on a certain condition. The superiority of the method is empirically shown. The change of the activation during training is analyzed and insight on the behavior is provided. The robustness to adversarial attacks is also empirically examined.\n\nThis paper discusses a very basic component of neural network models: activation function. Thus, it should be of interest to many researchers. The proposed method is simple and seems easy to use in real settings. A number of experiments are conducted to validate the method and the results look promising. The experiments in Section 5 is particularly interesting. It might give some hints for the following studies.\n\nHowever, there are several things to be addressed for acceptance.\n\n1) What is actually proposed is not very clear. \n\nS-APL is formulated in Equation 2. However, there are some discussion after that which changes or restricts the equation. For example, it seems that b_i^s^+ = b_i^s^- is assumed throughout the paper. In that case, it should be just reflected in Equation 2. In the third paragraph of Section 3.2, it is mentioned that h_i^s(x) = h_i^s(-x) with b_i^s^+ = b_i^s^-. However, it should also assume that a^s^+ = a^s^-. From the experiments. apparently, a^s^+ = a^s^- is not assumed. It seems that the method has symmetry only for the hinge locations. \n\nIn the first paragraph of Section 3.2, it is implied that parameters are shared across layers. It is not very clear what is shared and what is not. Please make that part clear. It will make it easier to understand the experimental settings, too.\n\n2) Theorem 3.1 does not seem to prove the approximation ability of S-APL.\n\nIt is clear that g(x, S) can represent arbitrary h(x, S), but I am not sure if it is clear that h(x, S) can represent arbitrary g(x, S). It should also depend on the conditions on a^s^+, a^s^-, b_i^s^+, b_i^s^-. I think it needs to prove that h(x, S) can approximate arbitrary piecewise linear function (i.e., g(x, S)) if you want to prove the approximation ability of h(x, S).\n\nEquation 4 seems to assume that all intervals are the same (i.e., \u2200i, B_i - A_i = (B-A) / S). It should be stated explicitly. This relates to the problem 1).\n\nI may not understand some important aspect. I am happy to be corrected.\n\n3) Experimental conditions are not clear.\n\nPlease cite the papers which describe the architecture of the models used in the experiments. The effectiveness of the proposed method should depend on the network architecture and it is importable to be able to see the details of the models.\n\n4) On the sensitivity of optimization on the initial value.\n\nIt is interesting to see that \"fixed trained S-APL\" is not comparable with \"S-APL positive\". If the hypothesis in the paper is correct, it is natural to assume that \"fixed trained S-APL\" also has some issue on training. It would be interesting to see experimental results with \"initialized with trained-S-APL\" and \"S-APL positive with non-zero initial value\".  It is a bit weird to observe that \"S-APL positive\" never becomes non-zero for x < 0.\n\n5) Comparison results with other activation units in Section 6.\n\nThe proposed method is compared only with ReLU. It is important to see comparisons with other activations such as the plain APL.\n\n\nSome other minor comments:\n\nIt is quite interesting that objects are actually modified for adversarial attack for the proposed method in Figure 5. It would be interesting to have some consideration on it."}