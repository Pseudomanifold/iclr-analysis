{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an algorithm for adversarial imitation that uses off-policy data in a principled manner, unlike prior work. The core idea is to express the KL-divergence between the policy's state-action marginal and the expert's state action marginal using the Donsker-Varadhan representation and then applying the change of variable similar to DualDICE to avoid computing the marginal of the current policy, thus getting rid of the on-policy sampling requirement. The paper then shows how the auxiliary variable (critic) added to the optimization is a value function that maximizes the corresponding induced reward in AIL methods, thus unifying the objectives for policy optimization and reward learning. The authors then present practical considerations needed in getting this formulation to work, including sampling from a replay buffer, biased sampling for the exponentiated term and avoid the double-sampling issue. Finally, the paper presents some results, which show that valueDICE is comparable to most of the other imitation learning methods. \n\nI lean towards accepting this paper. The overall idea seems neat, however, Section 5, and the addition of a replay buffer distribution in the KL-divergence objective, though motivated enough seem to be somewhat not so principled. The experimental section is a bit weak, and I encourage the authors to strengthen this section. Also, in the case of HalfCheetah and Ant (Figure 2), valueDICE usually exhibits overfitting-like trends (the performance drops with more training), why does this happen? Can this be corrected? Overall the idea is neat, I would still say that the components very prominently exist in the literature (f-GANs, DualDICE, etc).   \n\nI would encourage the authors to add some more details experimentally and make the implementation available. For example, optimization of Bellman backup functions without target networks or delays can be unstable, solving saddle point problems can be unstable, etc. How should these factors be tuned? And overall, is the optimization of the ValueDICE objective easy?. DualDICE is known to be unstable (the DualDICE Github implementation mentions this), do similar problems arise with ValueDICE?"}