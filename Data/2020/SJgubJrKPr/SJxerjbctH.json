{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper describes a framework for macro-action design in deep reinforcement learning. Macro actions consist of open loop sequences of atomic actions. The macro actions are generated by a genetic algorithm that can extend or modify the sequences.  Fitness evaluation of the actions is achieved by training a reinforcement learning policy using an extended action set that includes the newly generated macros. The RL methods and the genetic algorithm do not seem to have been modified in any significant way for integrated application to reinforcement  learning problems. Training of macro actions happens offline before applying RL.\n\nI recommend rejection for this paper. The paper does not seem to introduce any major novelties. It mainly seems to be a straightforward application of a genetic algorithm. The experiments do not offer any major new insights.\n\nDetailed comments:\n\n*The paper claims a number of contributions which result in a framework for macro action construction. This framework seems to be a relatively straightforward application of GAs without large modifications needed for use in RL tasks. I have a hard time seeing how these contributions add to the state-of-the-art:\n\n-The proposed framework  is an SMDP with open-loop macro actions to which any deep RL algorithm can be applied. None of these elements are new.\n\n-Evaluation of macro actions consists of looking at the average reward of a policy  learnt with the macro actions added to the action set\n\n-Action set augmentation just adds macro-actions to the existing action set (just as is done in most temporal abstraction methods)\n\n-The macro action construction method seems to be a standard black, box GA using append-to-sequence and change-element-in-sequence as search operators.\n\n*The lack of proper discounting of macro actions seems rather hacky. One would expect that the next state is discounted by gamma^<macro action steps> rather than simple discounting.\n\n*Training multiple Deep RL algorithms to evaluate multiple generations of action sets seems incredibly expensive. It would be good to indicate the total amount of samples / time used to perform this optimisation.\n\n*The experimental section does not offer much insight beyond the fact that the macro actions can improve performance. Since these are simple open-loop action sequences it seems hardly surprising that they can be used with DRL methods or that they are not specific to a DRL method.\n\n\nMinor comments:\n-I don\u2019t see the need for specifically specifying \u2018deep\u2019 reinforcement learning as part of the framework. All of the -definitions seem to apply to general reinforcement learning and are not specific to deep approaches.\n\n-Is section 3 title intentionally \u2018generic algorithm\u2019 or was that meant to be \u2018genetic\u2019?\n"}