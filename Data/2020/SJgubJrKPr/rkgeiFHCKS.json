{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a generic algorithm for constructing macro actions for deep reinforcement learning. The main idea is to append a macro action to the primitive action space once a time and evaluates whether the augmented action spaces lead to promising performance. The algorithm is tested with two deep RL methods, PPO and A2C. The experiments were conducted in two environments, Atari 2600 and ViZDoom. The benefit of the algorithm is also demonstrated by the transferability of the constructed macro actions on a different DRL algorithm.\n\nThe overall idea of this paper is generally clear to me. But I have concerns about the technical details and writing of the paper.\n\n1. How the \u201cEvaluation\u201d is done is not clear to me (line 12 of the algorithm). Even though this paper focuses on the construction of macro actions, leaving the DRL algorithm as kind of a black box, I still think how the integration and evaluation are done plays a huge role in this kind of problem. Therefore, I think an important technical perspective is missing in this paper.\n\n2. The baselines are way too easy. I am not quite familiar with the literature but I sort of can tell the random, repeated, and naively handcrafted macro actions are way too easy as baselines. What this paper is doing is to have an automatic explore (augment) and evaluate the procedure for generating the macro actions. So the baselines should be at least somewhat similar flavor. \n\n3. The motivation mentioned in the introduction includes avoiding biased macros and also promoting more diverse macro actions. However, it is not clear to me how the proposed method is improving these two perspectives. I think part of the reason is that the notion of ``less biased\u201d or ``diverse\u201d actions are more something good to have or heuristics. Also, the append or alter operators are not exploring the whole combinatorial space.\n\n4. I think it is possible to have a more extended discussion of the related work in this paper. Also, the approach that is taken in this paper -- studying the construction of macro actions independently  -- could be better motivated. At this point, I am actually not sure whether it is appropriate, maybe due to limited discussion of the previous work (only one paragraph in the intro). \n"}