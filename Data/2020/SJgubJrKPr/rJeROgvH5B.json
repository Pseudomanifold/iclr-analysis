{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed to augment the existing action set with the \u2018macro\u2019 actions that consist of sequence of primitive actions. The framework is based on genetic algorithm (GA) to propose and modify the \u2018macro\u2019 actions, where the fitness function in GA is simply the performance of underlying RL algorithm on the newly constructed action set. Experiments on some benchmarks from Atari and VisDoom show some promising results. \n\nOverall the technique proposed is simple and straightforward, which should be easy to understand and implement for the community. In my opinion, the augmented action set forces some form of exploration, and thus would achieve better performance especially in the situations where reward is sparse. There are still some concerns I have:\n\n1. Some claims are too strong. For example, the paper criticizes the previous works with human supervision. However, the GA used here also requires a lot of human heuristics: the design of mutation/augmentation/selection/evaluation, etc. \n\n2. The current specific method only works with discrete actions. How would the method be adapted to continuous case? Actually I\u2019m also concerned about using such mutation based method for exploration, so I\u2019m not fully convinced that this could be generalized to continuous case.\n\n3. It seems that Q+ and Q* are always growing inside the while loop of Algorithm 1. Why there\u2019s no deletion of the actions? Also what would be the trade-off between the size of action space and the performance? I think there must be a limit where afterwards RL becomes harder with large action space.\n\n4. Regarding the experiment, I think the main competitors would be the other exploration based methods, like count based, intrinsic reward based, etc. I\u2019m not asking for comparing against everything, but something like \n1) separate the effort of curiosity with proposed method in Figure 4 (i.e., add a proposed method only baseline); \n2) In fig 2 and 3, add baselines that using A2C/PPO + exploration, e.g., curiosity, count, etc. \nI would expect a stronger statement with these ablation studies.  \n\nMinor:\nThe paper could be more compact. For example, the Algorithm A1 contains only one operation. For better readability, there\u2019s no need to put it into appendix. \n"}