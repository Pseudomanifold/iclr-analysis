{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper applies monotonic attention to the multiheaded (self-attention) mechanisms used in a Transformer. It also proposes a few new losses which encourage low-latency alignments. Experiments are carried out on WMT EnDe and IWSLT EnVi translation, with evaluation using BLEU and latency-related metrics. \n\nReview: Applying monotonic attention mechanisms to the Transformer architecture is an obvious and necessary idea, and as such this paper constitutes an important contribution. The application of monotonic attention to the Transformer is as I would have expected. The latency-reduction losses are novel, however. I think there are various things the authors could do to clarify their results as well as the presentation of their methods, which I discuss below. Overall, I think this is a solid accept, especially with some improvements to the presentation.\n\nSpecific comments & suggestions:\n- There is a typo in the abstract: \"multiple attentions heads\"\n- \"the denominator in Equation 5 is clamped into a range of (\\eps, 1]\" Technically this doesn't need to be an open range on the left.\n- \"which allows the decoder to apply softmax attention over a chunk (subsequence of encoder positions).\" It may be clearer if you just say \"which allows the decoder to apply to a fixed-length subsequence of encoder states preceding $t_i$\".\n- I think more could be done to distinguish the behavior of the encoder self-attention, the encoder-decoder attention, and the decoder self-attention. You write \"The model will write a new target token only after all the attentions have decided to write\" but also \"Some heads read new inputs, while the others can stay in the past to retain the source history information.\" If an attention head is \"staying in the past\", then the model will not be able to write a new target token. I think in one of these cases you are referring to (encoder) self-attention and in the other you are referring to decoder attention. Please clarify.\n- \"Although MMA-H is not quite yet streaming capable since both the encoder and decoder self-attention have an infinite lookback, that model represents a good step in that direction.\" I think you should distinguish between online and streaming translation. It sounds like when you say streaming you mean that the utterances may continue arbitrarily, so infinite lookback is impractical. However, one could truncate the source sequence whenever the decoder outputs an end-of-sentence token, or something. I'm not sure people usually make a strong distinction here.\n- For completeness it would be useful to include at least wait-k, and potentially a line corresponding to offline attention performance, in the plots in Figure 2.\n- Why don't you include MMA-IL in WMT'15 EnDe? (figure 2)\n- Are you copying the results from (Arivazhagan et al., 2019) or did you reimplement MILk in your codebase?\n- The results in Figure 2 could be made much more informative if there was some way of communicating the multiplier (the \"lambdas\") of the different latency losses for each of the different dots on the plot. This would make it much more obvious how important these losses are and how the effect the quality/latency trade-off.\n- There are some additional possible references for online seq2seq, like CTC, the Neural Transducer, \"Learning Hard Alignments with Variational Inference\", \"Learning online alignments with continuous rewards policy gradient\u201d, etc."}