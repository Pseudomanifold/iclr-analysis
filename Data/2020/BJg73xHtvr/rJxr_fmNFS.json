{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN. Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks). Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community. The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement.\n\nThis is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. This is interesting but no formal results are presented.\n\nI vote for rejection for four major weaknesses explained as follows.\n\n(1) The experimental results cannot show the usefulness of the proposed GCN. The results on real datasets are similar to the regular GCN. As the authors themselves remarked, \"it can be seen that our models sometimes outperform the two Euclidean GCN\". The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.\n\n(2) The method is not well motivated. The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. This is too general and not enough as a motivation. After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.\n\n(3) A large body of graph neural network literature is omitted. The authors start from a very high-level description of machine learning in constant curvature spaces. Such high-level introductions require more comprehensive literatures to support. In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN. This is misleading. For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned. The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks. This is a thriving area that requires a careful literature review.\n\n(4) The writing quality is not satisfactory. Here are a few examples: The ICLR citation style needs to use sometimes \\citep. The authors instead used \\cite everywhere, making the paper hard to read. The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold). The introduction can start at a lower level (such as flat/hyperbolic neural networks). Section 3.4, as the main technical innovation, can be extended and includes some demonstrations."}