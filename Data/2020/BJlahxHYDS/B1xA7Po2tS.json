{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview:\nThis paper introduces a new method for uncertainty estimation which utilizes randomly initialized networks. Essentially, instead of training a single predictor that outputs means and uncertainty estimates together, authors propose to have two separate models: one that outputs means, and one that outputs uncertainties. The later one consists of two networks: a randomly initialized \u201cprior\u201d which is fixed and is not trained, and a \u201cpredictor\u201d, which is then trained to predict the output of the randomly initialized \u201cprior\u201d applied to the training samples.  \nAuthors show that under some reasonable assumptions the resulting estimates are conservative and concentrated (i.e. bounded and converge to zero with more data).\n\nWriting quality: \nOverall, the paper is relatively well-written, although it might be at times hard to follow, especially for someone who is not familiar with the original work that used randomized prior functions (Burda\u201918, Osband \u201818, \u201819). \n\nEvaluation:\nThe method is experimentally evaluated on a task of out-of-distribution detection on CIFAR+SVHN, and seems to perform on-par or better than the baselines (including \u201cstandard\u201d deep ensembles and dropout networks). In addition, there are experiments that demonstrate that the model is performing relatively well in terms of calibration (whether the model predictive behaviour makes sense as the model confidence changes).\n\nDecision:\nI find the core idea behind the paper quite interesting, however, as indicated by authors themselves, it has already been studied in a slightly different context (RL, works by Burda et. al, Osband et. al). That said, authors do provide additional insides for the supervised settings, and also analyse theoretically the behaviour of uncertainty estimates. \nOverall, I cannot say I am fully convinced that the paper should be accepted as is (also see questions below), but generally I am positive about this work, and hence the final score: \u201cweak accept\u201d.\n\nAdditional comments / questions:\n(somewhat minor) p1: \u201cWhile deep ensembles \u2026, where the individual ensembles are trained on different data\u201c - here and related text, it should probably be \u201cindividual models\u201d / \u201cindividual networks\u201d. Generally, I am not convinced that these are strong arguments against deep ensembles.\n\n(minor) p2-p3: \u201c2. Preliminaries\u201d - I am not sure if this section adds much to the understanding, it would seem more natural to spend more time explaining the intuitions behind the net\n\n(kind of major) p3. \u201cprior\u201d - The explanation of why using a randomly initialized network makes sense is not very strict. I kind of get the general idea, but it is not clear to me why not use something less expensive, e.g. just random projections, and why do we actually need a full network. Intuitively it seems quite strange to waste a lot of capacity to fit to essentially fit a set of random weights: is it something that allows the network to avoid easily learning the \u201crandom prior\u201d? And, more generally, can this also be considered as a \u201ctrick\u201d to de-correlate individual predictors? I believe these points should be discussed in more detail.\n\n\n\n\n\n\n\n\n"}