{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work introduces a simple technique to obtain uncertainty estimates for deep neural networks. This is achieved by having a set of random networks (i.e. neural networks where their parameters are randomly initialized) and then computing an uncertainty value based on the difference in the predictions between those random networks and networks that are trained to mimic them on a finite collection of points. The authors further show that this method results into uncertainties that are conservative, meaning that they are higher than the uncertainty of a hypothetical posterior, and concentrate, i.e. they converge towards zero when we get more and more data. The authors further draw connections to ensemble methods and discuss how such a method can be effectively realized in practice. They then evaluate their approach on an out-of-distribution detection task, they measure the calibration of their uncertainty estimates and finally perform a small ablation study for their concentration result. \n\nThis work is interesting as it seems to provide a simple way to obtain reasonable uncertainty estimates. For this reason it can potentially serve as a strong baseline for this field. The theoretical considerations also help in providing some guarantees about such an approach. Having said that, in my opinion the writing could use some more work in order to make things more clear as some critical experimental details and baselines are missing and thus do not make the method as convincing. Furthermore, I also believe that some clarifications on the theoretical aspects of this work, will help in boosting its quality. More specifically:\n\n- How exactly do you apply your method on the classification scenario? Do you select an arbitrary hidden layer of the classification model for the prior and predictor network architectures or the output logits / softmax probabilities?  In appendix A you mention the architecture but not precisely how it is employed. I believe this can be an important piece of information in order to decipher the importance of e.g. the output dimensionality on the uncertainty quality, as higher dimensional outputs might be harder to approximate thus could induce a larger squared error and hence uncertainty.\n- What is the average training error of the predictor networks for the out-of-distribution task and subsampling ablation task, i.e. how far away from concentration were the priors? \n- An effect that I found weird is the following: what happens for the out-of-distribution examples when the predictor networks can perfectly predict the prior network outputs? Wouldn\u2019t that then imply that the uncertainty would be zero for any input (even an out-of-distribution one), as the prior network and predictor network always agree? One could imagine that for e.g. simple priors and with sufficiently dense sampling of the domain of the function this can happen in practice.\n- For the conservatism you show that your uncertainty estimate is higher, on average, than the posterior variance when you sample points from the model itself. In a sense this guarantee translates to the actual data when the prior is \u201ccorrect\u201d. How do those conservatism guarantees translate to the case when there is model misspecification, i.e. when the prior is not correct? Perhaps a small toy example would be informative.\n- For the predictor networks as described in figure 2; do you train both the green and red parts of the network or only the red parts and keep the green part fixed to the values you used for the prior f? (This helps in understanding how easy / difficult is the task of the predictor network).\n- What is the accuracy on the actual in-distribution prediction task for the RP and baselines? What did \u201cB\u201d correspond to for the dropout networks? Was it the number of dropout samples you averaged over to get the final predictive?\n- How sensitive are the results on the actual initialization strategy of the prior network? It would be good to see e.g. some form of performance / init variance curve in order to decipher the sensitivity. \n\nOther comments\n- It is worth pointing out that [1] showed that Monte-Carlo dropout performs approximate MAP inference, which seems more plausible than the approximate Bayesian inference perspective of [2].\n- In the introduction you argue that Bayesian neural networks rely on procedures different from standard supervised learning and thus most ML pipelines are not optimized for them in practice. Could you elaborate a bit about this statement? Variationally trained BNNs with e.g. the reparametrization trick [3, 4] are straightforward since you can just use backpropagation to update their (variational) parameters.\n- What is the x-axis for Figure 3 for the baselines? (I take it that for RP it is the \\hat{sigma}^2(x)).\n- I believe that a comparison against a simple variationally trained BNN would make the results more convincing.\n\nMisc\n- Second page, \u201cFigure 1, top two plota\u201d -> \u201cFigure 1, top two plots\u201d\n- Third page, \u201c[\u2026] introduced in equation 2 denotes the posterior covariance [\u2026.]\u201d -> \u201c[\u2026] introduced in equation 2 denotes the posterior variance [\u2026]\u201c\n- Fifth page, \u201c[\u2026] this makes it is reasonable for W large enough [\u2026]\u201d -> \u201c[\u2026] this makes it reasonable for W large enough [\u2026]\u201d\n- Sixth page, \u201cCorollary 1 and proposition 2\u201d; where is corollary 1? Do you mean Proposition 1?\n- Seventh page, \u201c[\u2026] inspired by, an builds on, [\u2026]\u201d -> \u201cinspired by, and builds on, [\u2026]\u201d\n- Ninth page \u201cmontonicity\u201d -> \u201cmonotonicity\u201d\n\nOverall, I tend to accept this work, although, depending on the author rebuttal and other discussions, I am willing to change my rating accordingly.\n\n[1] Eric Nalisnick, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Padhraic Smyth, Dropout as a Structured Shrinkage Prior, 2019\n[2] Yarin Gal, Zoubin Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, 2016\n[3] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014\n[4] Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropagation and Approximate Inference in Deep Generative Models, 2014"}