{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows that the MSE of a deep network trained to match fixed random network is a conservative estimate of uncertainty in expectation over many such network pairs. An experiment compares this previously proposed method to other approaches for uncertainty estimation on CIFAR 10. \n\nStrengths:\n- Obtaining uncertainty estimates for predictions of deep neural networks is an important and open research question.\n- Proposition 1 is an interesting result, although the paper does not seem to discuss its significance and implications enough.\n\nWeaknesses:\n- Proposition 1 is described as \"our uncertainty estimates are never too small\". However, as the name of the proposition suggests, it seems to only hold in expectation over models trained, which is a quite different statement.\n- Proposition 2 seems to simply state that a small network can be distilled into a large network. Maybe I missed part of the reasoning here? Otherwise, it should probably not be highlighted as a major contribution.\n- The experimental evaluation is very limited, training only on CIFAR 10. While the experiments add little value to the community, this may be acceptable for a mostly theoretical paper.\n\nClarity:\n- The paper was difficult to read and unclear in explanations. It could help to define notation upfront instead of introducing shorthands on the way.\n- In Figure 3, the Y axis limits should be fixed across seen and unseen histograms for the same method. The current presentation is a bit misleading here, as the presented method seems to have moved the most mass under this chart scaling.\n\nComments:\n- As found in prior work cited in the submission, the method tends to perform well with just one network pair. This raises the question whether the contribution of the paper that holds in expectation over many pairs and the empirical success of the approach are connected.\n- The marginal posterior variance \\sigma^2(x_\\star) appears in various forms with hat, tilde, and different subscripts. It maybe worth assigning different letters to these to avoid confusion."}