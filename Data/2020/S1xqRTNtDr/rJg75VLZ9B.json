{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents Behavioral Repertoire Imitation Learning (BRIL) which is a way to learn a policy via imitation learning that can be modulated with different behavior inputs that adjust the policy's behavior.\u00a0 Demonstrations used in training are labeled with differences in behavior across dimensions (which are then reduced to two dimensions using t-SNE), and then these behavior labels are provided as additional input when training a NN from demonstrations using behavior cloning.\u00a0 Experimental results are shown for learning a BRIL policy from 7000+ demonstrations of humans playing StarCraft, and are compared to that of learning a single behavior cloned policy trained on all demonstrations as well as behavior cloned policies trained on subsets of demonstrations clustered by their behavior. \n\nOne of the authors' claims is that BRIL is able to learn a policy that can have a wide variety of behaviors based on the behavior input given as input to the policy which is backed by their results.  They also claim that the BRIL model can be tuned to have higher performance than a policy learned by traditional IL which is shown by their results, although I think that may be somewhat of a function of the environment and set of demonstrations.\n\nAnother claim by the authors in section 4.3 is that behavior can be successfully controlled, and they say \"for both BRIL and IL on behavioral clusters, the average expressed behavior is closest to the cluster centroid that it was modulated to behave as, among the four clusters we selected.\"  I'm not sure this statement is true as IL (C30) is closer to C11 than C30 in table 1.\n\nThe biggest weakness in this paper -- and barrier to acceptance -- is in the really small sample size of the results where only 4 cluster behaviors out of 62 are evaluated.\u00a0 I would like to see information about the aggregate performance and relative behaviors of BRIL policies compared to the policies trained on the clustered subset of demonstrations across all clusters (or certainly a lot more than just 4 which is less than 10%!) in order to have a better evaluation of the BRIL approach.  The claims of BRIL doing better than clustered IL and being close to a behavior cluster centroid are not that convincing with so few sampled data points.  \n\nIn section 4.2 how do you define or quantify \"the most meaningful data separation\" when doing a grid search on parameters for clustering?\n\nOutside of making things easier to visualize, I'm not sure why it is necessary to reduce the dimensionality of the behavior input for BRIL. Reducing the dimensionality also reduces the set of different policy behaviors that can be expressed.  Additionally, if one might want to adjust a behavior such as having more or less of a certain unit type in StarCraft, the reduced behavior space makes it harder and less intuitive to do so. \n\nShowing that UCB can end up selecting the best BRIL policy (out of 4) and thereby resulting in a better cumulative win record than the traditional IL baseline is kind of trivial and doesn't add much to the paper.\u00a0 Using a Bayesian optimizer as suggested for future work to select the best behavior inputs for the BRIL policy would be more interesting and give more credence for BRIL being useful in discovering better policies."}