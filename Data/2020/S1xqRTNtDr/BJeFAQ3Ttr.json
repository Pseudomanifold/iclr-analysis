{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work examines the problem of using training a policy which can emulate a variety of different strategies based on a set of demonstrations representing this space of strategies.  The proposed method, BRIL, computes a feature vector for each demonstration, and then employs a dimensionality reduction technique to map the demonstrations to a latent space of strategies.  BRIL then preforms behavioral cloning on these demonstrations, with the reduced representation of the current strategy as an additional input to the policy model.  Empirical evaluation of BRIL is conducted in StarCraft II, where the agent is tasked with scheduling the construction of different units (other aspects of play are controlled by built-in AI).  Results show that when conditioned on good strategies, the BRIL model is superior to a base imitation learning model trained without strategy information.\n\nThe best way to view this work is as a method for learning goal conditioned polices from demonstrations.  While the raw data does not distinguish between different strategies, a postprocessing phase generates task description vectors with specific semantics (the ratios of different unit types built) which are given as input to the BRIL model in addition to the current state.  Therefore, rather than identifying the latent space of strategies present in the demonstration data, BIRL learns a policy which is parameterized by an externally defined space of target behaviors.  The dimensionality reduction step in the strategy space does recover some latent structure, but this is still with respect to a task-specific space of strategy annotations, rather than the demonstrated behaviors themselves.  \n\nWhile this is not an issue with BRIL itself, it should be made clear in the paper that BRIL is learning policies conditioned on explicit strategy descriptions, so that the work can be properly positioned in the literature.  Neither the theoretical discussion nor the empirical results compare BRIL against existing work on learning goal-conditioned policies.\n\nThe empirical results comparing BRIL against a base IL agent are interesting, however, in that BRIL, when conditioned on the best strategy (building mostly marines), actually outperforms IL trained solely on episodes which followed this strategy.  This suggest the possibility that BRIL was able to transfer knowledge from episodes demonstrating other strategies to make up for the limited information available in the demonstrations of the target behavior.\n\nIn section 4.4 the paper briefly discusses the idea of evaluating different strategies executed by the BRIL model, and selecting those that perform best, as a means of learning strong policies for the StarCraft task.  The use of human demonstrations to improve the performance of learning algorithms on difficult control tasks is a widely studied problem, and optimization.  This idea is not explored in any great detail however, and no comparisons with existing methods combining IL with RL are conducted, so the value of BRIL in that context is unclear.  I would, however, recommend this as direction for future work with BRIL."}