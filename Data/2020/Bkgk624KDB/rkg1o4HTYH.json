{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a meta-learning algorithm to solve the problem of exploration in a contextual bandit task using prior knowledge. This is analogous to how exploration strategies have been learned from past data in the meta-learning for RL (for example, [1]). Their algorithm simulates contextual bandit problems from fully labeled data and uses it to learn an exploration policy that works well on tasks with bandit feedback. The training step for the exploration policy builds upon the AggreVaTe algorithm, where policy optimization is performed on the history augmented data by using a separate roll-out policy to estimate the advantage of a particular action for a particular context, from the point of view of regret minimization.  In terms of theory, they show that by using specific algorithms (example, Banditron) as the inner policy optimization procedure, a no-regret algorithm can be obtained.   \n\nSome of my questions are:\n\n1. The method seems a bit hacky to me, for example, it requires calibration of f, requires access to test time examples, and it is unclear why this should be needed if an algorithm were provably good. Can this be elaborated upon?\n\n2. Section 3.2 and Algorithm 1 are very unclear, and it requires multiple passes to be able to understand in the current draft. I encourage the authors to revise these sections for increasing clarity. \n\n3. In Algorithm 1, which is the exploitation policy? The theory section says \" In particular, we first relate the\nregret of the learner in line 16 to the overall regret of $pi$\", but line 16 in the algorithm refers to \"end for\". Can the authors clearly point me to the location where the exploitation policy is mentioned in the algorithm? What is the precise definition of $\\pi_n$ (Line 15), and how does it relate to the average exploration policy and the policy $\\bar{\\pi}$ used in Theorem 1. These definitions have not been made clear, making this hard to follow. What is the optimal policy $\\pi^*$, in particular, what is the formal meaning of \"which effectively cheats\u201d at training time by looking at the true labels.\"\n\n4. The paper lacks discussion about how the roll-out and the roll-in policy affect regret bounds, and what assumption about these is used to derive no-regret guarantees. Can this be described? \n \nOverall, my recommendation currently is reject. I feel that the paper tries to theoretically analyse an interesting problem, however, at this point the paper is very hard to follow and not complete. I encourage the authors to revise the details and improve clarity. Also, it would be good if the authors can explain the significance of the results at the cost of added complexity.\n\nReferences:\n[1] Meta-exploration of structured exploration strategies, Gupta et.al. NeuRIPS 2018"}