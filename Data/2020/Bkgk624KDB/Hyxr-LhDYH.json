{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents an approach to learning exploration strategies for contextual bandits by meta-learning the exploration strategy across synthetic tasks. The approach is shown to outperform alternative exploration strategies on a learning to rank task as well as simulated bandit problems derived from classification tasks.\n\nApplying meta-learning to this setting is a novel and interesting approach and appears to have nice results.\n\nThe main issue with this paper is that the presentation of the algorithm is vague and it is difficult to determine what the algorithm actually does. Here are a few of my main questions:\n\n1. The algorithm is dependent on a function class F and PolOpt which finds an f in F with low expected regret. However, F is not really defined. In equation 1, it appears to directly output an action. Below that, it says \"For example, F may be... mapping user features x in X to predicted rewards for actoins a in [K].\" In Section 5.1, the policy is using a probability distribution over actions from f, the entropy of the predicted probability distribution, and a one hot encoding for the predicted action f(a). What is the class F? Is it predicting a probability distribution and an action? What's the one hot encoding of the predicted action, is that an action sampled from the probability distribution of F?\n\n2. Section 3.1 describes the test time behavior of MELEE. They state the goal is to learn an exploration policy pi, and specifically state that pi can \"not depend directly on the context x\" so it \"only learns to explore and not also to solve the underlying task-dependent classification problem\". However, in the equations, pi is a function of x_t. The algorithm is motivated by the exploration exploitation trade-off problem, however there's no policy doing any exploiting here.  When does exploitation happen? Is it a separate policy? How is it trained and when is it used? The decision of when to explore and when to exploit is the key question here and it's unclear how that is happening in this algorithm. It's unclear to me how the algorithm achieves good rewards on the task without any attempt at exploitation.\n\n3. What are the rollouts and expert cost-to-go in this setting? In section 3.2, you say the algorithm asks what would the \"long time reward look like if I were to take this action.\" So are you looking at how that action would affect future exploitation? You say there's an optimal reference policy* which effectively cheats by looking at the true labels. Is this policy taking the action with the highest reward at each state? Or is it looking at the effects of exploration on the learned policy? The start of page 4 says that the \"cost-to-go Q of the expert\" is observed, why would the expert in a bandit have a cost-to-go? In the section on roll-out values, you say that you evaluate the value of taking this action and then assuming all future actions were taken by the expert. But this then precludes there being any effect of this action on the policy and any exploration effect. Are you solely optimizing for the one step reward? In the algorithm, the rollouts computed on line 8 don't appear to be used anywhere.\n\n4. How are the synthetic tasks setup? There are almost no details other than that they are two dimensional and have uniformly distributed class conditional distributions. How are they related to the learning to rank task you perform at the end? How are they related to the other tasks you evaluate on? It seems like the choice of synthetic tasks would be important to what the algorithm learns, so it's important to be clear on what these tasks were and how closely they match the target tasks. It would be even better to show results with different sets of synthetic tasks that are more or less similar to the target tasks.\n\n5. In the results, you say you limit the number of labels to two extremes, 0 and 4. I assume you mean you limit it to just those two labels. What do you do with the other data? Do you drop it or do you map it to these two labels?"}