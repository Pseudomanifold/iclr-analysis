{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents to use \\eta-trick for log(.) in Donsker-Varadhan representation of the KL divergence. The paper discusses its dual form in RKHS and claims it is better than the dual form of f-divergence (while I don't think it's a correct claim). Nevertheless, in experiments, we see that it outperforms the original neural estimation of mutual information.\n\n[Overall]\nPros:\n1. The idea of avoiding the log-sum-exp computation in the DV representation of KL is good. One of the main reasons is to get rid of biased estimation. This idea may not be too novel, but definitely is useful.\n\nCons:\n1. I don't agree with some claims in the paper. Nevertheless, these claims are some of the main stories supporting the paper.\n2. The presentation of the paper should be improved. Including the presentation flow between sections, and also misleading part in experiments.\n3. There are TOO MANY typos in equations. \n\n[Cons In Details]\n<The role of discussing the dual formulation.>\nThe paper spends huge paragraphs discussing the role of the dual formulation. And it also introduces \\eta-DV-Fisher and \\eta-DV-Sobolev, which can be seen as extensions of the proposed method.\nNevertheless, the author doesn't present an evaluation using the dual form. It's a pity that this part is missing. Having Section 3 makes the paper contain several sporadic arguments unrelating to the research questions. Re-organize the paragraphs/ presentation is suggested.\nAnother example is introducing perturbed loss function into the proposed loss function to make it strictly convex. This paragraph is misleading and can be moved entirely to the Appendix.\n\n<Claim on the Proposed Method is Better than f-divergence>\nThe author emphasizes (in multiple places) that the f-divergence biases the estimate. And it exemplifies from eq. (15) to eq. (16). Nevertheless, in eq. (16), when r* is optimum, there should be no second term. The author's claim is based on the comparisons between eq. (13) and eq. (15) when assuming only the MMD term reaches zero. The statement may not be fair.\n<Section 4>\nSimilar to Section 3, this section is cluttered. I don't get the reason why the author specifically come up with a new section comparing only to one mutual information estimation approach (a-MINE). Another irrelevant part of the research question is point 4 under page 7. Why discussing the extension of the a-MINE to conditional MI estimation?\nSome Typos: In equation (21) and (22), \\eta is missing. In Algorithm 1, there are too many typos such as missing \\theta under f, entirely wrong equation for the Output.\n\n<MI Estimation Experiment>\nCan the author discuss the standard deviation for various MI estimation approaches? The large standard deviation for MINE seems unusual.\n\n<Self-Supervised Experiment>\nThe author mentioned that the network considering only the first two convolutional layers, followed by a linear classifier, leads to the best performance. Is there no other layer in between? Also, in Figure 4 (a) and (c), is the purple-color layer means the final classification layer? It is a bit confusing.\n\n<Appendix>\nI understand most of the people would not read the Appendix, but I do. Missing brackets, grammatic errors,  missing integrals, wrong notations, missing punctuation marks, ill-structured presentations, etc., are the problems in the Appendix. I would greatly appreciate the author also spend some time in the Appendix.\n\n[Summary]\nI would vote for a score with 4 or 5 to this paper.\nRegarding there're only 3/6, I'm proposing a score of 6 now. But I look forward to the authors' response and then addressing the problems that I identified. I feel the paper should be a strong paper after a good amount of revision."}