{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper develops variations on the Donsker-Varadhan (DV) lower bound on KL divergence which yields a lower bound on mutual information as a special case.  The paper is primarily theoretical with an emphasis on the case where the witness function f is drawn from an RKHS (a support vector machine).  They discuss, and present experimental results where the feature map of the RKHS is computed by a neural network, although the theoretical results largely do not apply to optimization of the neural network.\n\nI have two complaints.  First, the authors ignore fundamental limitations on the measurement of mutual information from finite samples.  See \"Fundamental Limitations on the Measurement of Mutual Information\" by McAllester and Stratos.  This paper makes the intuitively obvious observation that I(X,Y) <= H(X) and one cannot give meaningful lower bound on H(X) larger than about 2 log N when drawing only N samples --- the best test one can use is the birthday paradox and a non-repetitive sample only guarantees that the entropy is larger than about 2 log N.  This is obvious for discrete distributions but holds also for continuous distributions --- from a finite sample one cannot even tell if the distribution is continuous or discrete.   So meaningful lower bounds for \"high dimensional data\" are simply not possible for feasible samples.  Given this fact, the emphasis needs to be on experimental results.\n\nThe experimental results in this paper are extremely weak. They should be compared to those in \"Learning Representations by Maximizing Mutual Information Across Views\" by Philip Bachman, R Devon Hjelm, William Buchwalter\n\n"}