{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper propose a variational bound on the Mutual Information and showed how it can be used to improve the estimation of mutual information. \n\nI am afraid I cannot judge of the quality and correctness of the method introduced by the authors. I am not familiar with the subject and will be a poor judge of the quality of the paper. Nevertheless I find that the presentation of the paper could be improved. \n\nFor instance,  while I enjoyed Fig. 3 that showed the performances of different estimators of the entropy, i add the zoom out on a big screen to be able to see anything at all! Clearly, one the most important figure of the paper will be unreadable when printed! It is also not entirely clear how this figure supports the claim of superiority of the proposed method.\n\nThe only comment I may have is that it would be interesting --since the authors want to apply their bound to the case of neural networks-- to compare with, the rigorous estimation of the entropy of NN with random weights in Gabrie et al, NeurIPS 2018, Fig. 2 . It would be a much challenging task, albeit a synthetic one, than the Gaussian dataset one presented in Fig. 3. \n\n\n\n\n"}