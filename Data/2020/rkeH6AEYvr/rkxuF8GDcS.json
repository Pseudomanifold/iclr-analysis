{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary:\n\nPaper presents a progressive attention mechanism for multi-scale image classification. The network contains a module that prunes non-important regions progressively by a factor of 2, then extract the features through a CNN. After all level features are extracted, the network later aggregates them and produce a final prediction. Since Bernoulli sampling is used to binarize the region mask, REINFORCE algorithm is used to update the region network. The paper shows slight improvements on top of a simple CNN (with only 4 layers), on a synthetic MNIST and ImageNet, with some visualization examples of proposed region network.\n\nThis idea to use a region pruning seems interesting. However, as detailed later, the motivation and experiments are not sufficient to show that it brings new insight to the classification. Lacking proper comparisons with earlier methods also undercuts the effectiveness. It is unfortunate but I could not recommend this paper to be accepted in the current version.\n\nStrength\n\n+ Informative graphics to help understand the method\n+ Detailed methodology and discussion on experiments\n\nWeakness\n- Question about the motivation\nIn the second paragraph of the introduction, the paper claims \"problem is that computational resources are allocated uniformly to all image regions, no matter how important they are for the task at hand.\" Will this actually be a problem in training CNN nowadays? It is safe to assume the different regions could be described by some statistics, and the reason why CNN successes in the past years is its capability to capture such statistics during the training with gradient descent. Could the author provide more justification for this motivation?\n\n\n- Experiments are not sufficient\nAs the paper indicated in related work \"multi-scale representations\", this work belongs to the image pyramid method category. However, there is no empirical comparison shown.\nIn addition, the paper only tests its proposed method on top of a simple CNN, with 4 convolutional layers as in Table 4, and the top-1 accuracy on ImageNet is merely 40%. I suggest the author deploy their method to at least VGG-16 and ResNet-50 to further strengthen its impact. Otherwise, it is hard to convince people this method can generalize to CNN models.\nI suggest authors to conduct additional experiments in their future submission.\n\n- Localization metrics\nSince the key contribution of this method is the network learns to prune those not useful regions, should the author provide some numerical results, e.g. the mean intersection of union between the proposed method's region prediction and the ground-truth bounding box? This data should be available in your synthetic MNIST and the ImageNet one. Only showing some example in Figure 7 is not that convincing.\n\n\nMinor comments:\n\n1. What's the number in Figure 4, 6, next to the marks in Figure?\n2. While this KFLOPS per image is informative, should the author consider to put their results in a table, indicates by the level 1,2,3 for a better visualization? The current Figure 4 and Figure 6 are not straight-forward to grasp in the beginning."}