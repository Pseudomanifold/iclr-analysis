{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a multiscale CNN variant that processes images in a coarse-to-fine setting (low to high resolution).  At each resolution, the network both extracts features and computes an attention map to use in processing the next higher resolution.  Features extracted from all resolutions are merged together and used in the classification decision.  Experiments show results of the attention mechanism on an MNIST-based synthetic dataset, and show classification accuracy on ImageNet.\n\nUnfortunately, the paper has major shortcomings.\n\nExperimental results are simply not convincing as ImageNet classification performance is far below modern baselines.  Figure 6 reports Top-1 accuracy on ImageNet in the 50-55% range.  Modern CNN baselines such as VGG [Simonyan and Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition, 2014] or residual networks [He et al, Deep Residual Learning for Image Recognition, 2015] achieve Top-1 accuracy in the 70-80% range.  This is an enormous performance gap.\n\nThe paper should be using a modern network architecture as a baseline and modifying it to include the proposed multiscale feature extraction, location, aggregation, and merging modules.  Then test if those modifications improve ImageNet classification performance.  The baseline CNN architecture currently used is too impoverished to allow comparison to recent results in other work.\n\nFurthermore, this is not the first paper to propose multiscale methods or adaptive computation.  Though some work is cited, experimental comparison to competing methods in both of these areas is entirely lacking.  This alone is sufficient cause for rejection.\n\nFor example, how does the proposed attention mechanism compare to Autofocus [Najibi et al]?  How does the multiscale approach compare to Feature Pyramid Networks [Lin et al] or Multi-scale Dense Networks [Huang et al]?\n\nThere is also highly relevant work that is not cited by the paper, including:\n\n(1) SBNet: Sparse Blocks Network for Fast Inference\n    Mengye Ren, Andrei Pokrovsky, Bin Yang, Raquel Urtasun\n    CVPR 2018\n\nThis paper proposes an architecture with internal dynamic attention masks to reduce computation at inference.\n\n(2) Multigrid Neural Architectures\n    Tsung-Wei Ke, Michael Maire, Stella X. Yu\n    CVPR 2017\n\nThis paper present a multiscale network design that processes all scales simultaneously (an alternative to the proposed coarse-to-fine approach), yielding improved results on ImageNet.  It also conducts experiments on a synthetic MNIST dataset to demonstrate implicit attentional capabilities of its network architecture.\n\nBoth of these recent papers highly overlap the central themes (multiscale, attention) in the submitted work and should likely be included in citation, discussion, and experimental comparison.\n"}