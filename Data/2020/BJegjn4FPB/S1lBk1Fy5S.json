{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method for unsupervised video summarization. The problem formulation considered in this paper is to minimize the total distance between each shot in a video and its nearest shot in the selected summary. The authors propose to use iterated local search (ILS) to solve this problem. To escape local optima, the authors propose to add a perturbation heuristic, which randomly swaps in a number of random shots. The method is evaluated on 3 datasets. The authors choose to use color histograms as shot features. In ablation study, the results show that the proposed ILS-SUMM outperforms a vanilla Local-Search-SUMM and an improved variant of Restart-SUMM. It also outperforms existing methods on all three datasets.\n\nOverall I found this paper clearly written. The method is clearly described. The ablation is helpful for understanding the design choices. It's also nice that it outperforms existing methods in terms of total distance. \n\nMy main concerns are regarding the experiments. \n1. Does total distance reflect the quality of video summarization well? There exist human annotated video summarization datasets. It'll make the results more convincing if the authors can show that the selected metric with the selected features (color histograms) reflects the human-judged quality. \n2. How does it compare with supervised methods? I understand that being unsupervised is a useful feature (e.g., don't require expensive annotations), but it would be useful to assess the trade-off. For example, how many annotations do we need to outperform an unsupervised method? If the number is small, it suggests that unsupervised methods probably haven't reached to point being practically useful yet. Presenting these results and discussions would help readers build a more comprehensive view of the area of video summarization. In addition, if it works worse than supervised methods, knowing how big the gap is would also be useful.  (It's okay to perform worse than supervised methods, but having some comparisons would be useful. )\n3. In Sec 4.3, the authors discussed about the pros and cons of using deep features. While I agree with the authors on the observations, I think it's still useful to present quantitative results, so readers can compare them with color histogram results.\n4. Since the supporting of the proposed method is solely empirical (instead of providing new theoretical results), I believe making the experiments more thorough and extensive would greatly benefit this paper. \n5. I also think the proposed method is technically light-wight. This is not necessarily a weakness, but I'd have a higher standard on experiments if the experiments are among the main contributions. \n\nOverall I think the paper would benefit from extending the experiments to make results more convincing. In its current form I think it's not ready for publication yet. "}