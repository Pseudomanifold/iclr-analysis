{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper,  the authors pay attention on the bottleneck in the NAS of its large architecture space which cause low efficiency. They introduce the multi agent reinforcement learning method to take the neural architecture search as a multi agent reinforcement learning problem.\n\nMain contribution is :(1) Framing the MAS as a multi agent problem. (2)Purpose two lightweight implementation. (3) Presenting 3 new datasets for NAS evaluation to minimize algorithmic over-fitting.\n\nIt seems like that it is the first work to combine multi agent reinforcement learning with NAS, and you have make complete proof about the algorithm's efficiency both mathematically and empirically. But from the view of multi agent reinforcement learning, there are also some points which make me confused.\n\nThe main problem is coordination, and I understand it as the agents in your work aim to get a joint action and the training process of them are independent, but we all know that in multi agent problems, the changing of agent's policy will cause change of the environment, so it will bring the instability, so I want to know that how you deal with the instability or whether the instability influence a lot in your work? Another problem may be not a theoretically problem that I want to know that have you made the guarantee of the consistency of agents' policies when using parallel training (May be the framework in coding process guarantee it ?) or the consistency is unnecessary to talk because it doesn't influence the result?"}