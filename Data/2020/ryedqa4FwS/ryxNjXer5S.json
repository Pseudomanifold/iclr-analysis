{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work built on top of DARTS. In their setting, each edge on the DAG (same as the one proposed in DARTS) has one agent associated with it and every agent maintains weights to propose operations. The author introduced two ways to update these weights: 1) solving a least squares assuming the validation loss decomposes linearly on the operations (MANAS-LS); 2) only update the weights for the activated operations (MANAS). Due to the usage of bandit framework, theoretical guarantees on the regret can be derived. \n\nBecause the distributed nature of the agents, this work is memory efficient and it allows searching directly on large datasets. The empirical results showed competitive performance in less GPU days comparing to  DARTS and recent variants.\n\nThe paper is well written. Apart from the theoretical contributions, the empirical evaluations are well done: the author used 3 more datasets instead of the usual CIFAR-10 and IMAGENET. Also, the random search are brought into picture which I think every NAS paper should include.  \n\nIt's surprising to see MANAS-LS sometimes outperform MANAS. For me, MANAS is a more principle way. Do the authors have more explanations? Why the test error of MANAS-LS + AutoAugment is missing in Table 1?\n\nIt's nice that the authors apply bandit framework to derive theoretical guarantees, but how close are these guarantees to the practice (for example on the benchmarks used in the work)? Is there some study for that? As there are not so many NAS works with theories, I think it would be nice if the authors could also comment on that."}