{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes a new initialization method for recurrent neural networks. They first obtain the weight from a linear optimal autoencoder. And then they use the weight to initialize the Lieanr Memory Networks(LMN). Basically, this paper is a combination of [1] and [2].\n\nStrength:\nThe method of initializing LMN using a linear RNN is natural and simple. (section 3.2)\nThe proposed initialization outperforms the baselines on the MNIST dataset.\n\nWeakness:\nWhat do you mean by the \"optimal autoencoder\"?\nThe performance on TIMIT is worse than the baseline methods.\nThe scale of the experiments is too small. Do you have any experiment results on any large dataset? e.g. Penn Treebank.\n \n\nReference:\n[1] Pre-training of Recurrent Neural Networks via Linear Autoencoders\n[2] Linear Memory Networks"}