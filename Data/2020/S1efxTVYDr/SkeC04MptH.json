{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a new Gaussian prior objective, \"D2GPo\", that addresses the fact that in sequence generative models, all incorrect predictions are penalized equally by MLE, a phenomenon which the authors refer to as the negative diversity ignorance drawback. The proposed objective is simple to implement and can easily be added on top of a regular cross-entropy loss. The paper shows that the new objective shows consistent improvements across a wide range of tasks.\n\n- Something that perturbed me a lot when reading Section 4, is that the function f should take as input 2 arguments, and not just 1. Typically, in Equation 7, from what I understand the numerator should be exp(f(y'_i, y_i)/T), and the denominator the sum over the exp(f(y'_j, y_i)/T). In other words, the value of f assigns a score to a word y'_j which depends on the target word y_i. This is also what suggests Figure 1: f depends on the target word. I think this should really be clarified in the paper, and f needs to be defined formally. In Figure 1, what is the exact value of f when the model outputs \"armchair\" / when the model outputs \"armchairs\". If I did not understand correctly, please point me out.\n\n- The cross-entropy loss in a generative model is essentially the KL divergence loss KL(Q, P) where Q is the true distribution (i.e. a one-hot vector) and P is the model output. In Equation 7, when T -> 0, the distribution Q becomes this one-hot vector. In that case, from what I understand, the D2GPo objective is actually very similar to the initial MLE objective, except that you compute KL(P, Q) instead of KL(Q, P), and that Q is not exactly one-hot because you consider a T > 0. Can you confirm this? Also, any reason why you considered KL(P, Q) instead of KL(Q, P)?\n\n- What value did you use for the temperature T? Did you study its impact?\n\n- Same question for lambda in Equation 6, what is the value you considered, and did you study its impact?\n\n- As mentioned in the introduction of the paper, one drawback with traditional neural generative, is the generation diversity, i.e. the fact that generations are generic, and an easy way to see this is to observe that generations are mostly composed of the most frequent words in the vocabulary. Did you evaluate whether a model trained with D2GPo has more diversity, and is more likely to generate rare words compared to a regular model trained with MLE only?\n\n- Figure 1 in the appendix is helpful. It would be nice to move it to Section 4.\n\n- In the related work, you write \"the Transformer provides us with a more structured memory for handling long-term dependencies\", which sounds a bit odd. There is no explicit memory component in the transformer, the ability to handle long-term dependencies comes instead from the self-attention mechanism.\n\nOverall the paper tackles an interesting problem which I feel has received surprisingly little attention from the community. The paper is well written, and has a lot of experiments supporting the method. But I think Section 4 needs to be clarified, and some experimental details are missing. Also, more information about the impact of T and lambda (at least on one type of experiments) would be very useful to have."}