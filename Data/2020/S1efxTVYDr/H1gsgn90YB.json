{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces the use of data-dependent Gaussian prior, to overcome negative diversity ignorance problem that includes the exposure bias problem for sequence generation models. In addition to the usual MLE (teacher forcing) criteria, the authors add the KL divergence between the prediction and the Gaussian PDF on the word embedding space. Experimental results show that the proposed method consistently improves the performance of the state-of-the-art methods for neural machine translation, text summarization, storytelling, and image captioning.\n\nI lean to accept this paper. The proposed method is well motivated and shown to be effective in several tasks for language generation.\n\nI have some major comments about the evaluation function $f(\\cdot)$. The authors propose to define it as a Gaussian distribution.\n- While this choice seems to be reasonable, I would like to know how its standard deviation can be defined. If it is a hyperparameter, the sensitivity of different deviations for the performance should be experimentally reported. A small valued deviation would make the KL divergence close to zero, while a large one makes its convergence slow.\n- Another way to remedy the problem of KL divergence above is applying Wasserstein distance instead of KL divergence. I would like to know if the authors have investigated the use.\n\nMinor comments:\n- White space should be inserted between \"sequence-to-sequence\" and \"(seq2seq)\" on the third page.\n- If the authors define a sequence using a bold and italic font as $\\boldsymbol{y}$, each token can be represented using an italic font to distinguish each token and the entire sequence: $\\boldsymbol{y}=<y_1,...,y_l>$. Otherwise, the sequence can be defined as $\\mathcal{Y}$ if the authors like to represent each token as a vector.\n- There is a typo on the fifth page. The word \"dada-independent\" should be \"data-independent.\""}