{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a new step-size adaptation algorithm called AdaX. AdaX \nbuilds on the ideas of the Adam algorithm to address instability and non-convergence issues. \nConvergence of AdaX is proven in both convex and non-convex settings. The paper \nalso provides an empirical comparison of AdaX against its predecessors \n(SGD, RMSProp, Adam, AMSGrad) on a variety of tasks.\n\nI recommend the paper be rejected. I believe the convergence results could be a significant contribution, but the \nquality of the paper is hampered by its experimental design. The paper felt generally unpolished, containing\nfrequent grammatical errors, imprecise language, and uncited statements.\n\nMy main issue with the paper is the experimental design. I am not convinced that we can \ndraw valid conclusions from the experimental results for the following reasons:\n  - The experiments are lacking important details. How many independent runs of the experiment \n    were the experimental results averaged over? All of the experiments have random initial conditions \n    (e.g. initialization of the network), and should be ran multiple times, not just once. \n    There's no error bars in any of the plots, so it's unclear whether AdaX really \n    does provide a statistically significant improvement over the baselines. \n    Similarly, the data in all the tables is quite similar, so without indicating the \n    spread of these estimates its impossible to tell whether these results are significant or not.\n\n  - How were the hyperparameters and step-size schedules chosen? The performance of Adam, AMSGrad, and \n    RMSProp are quite sensitive to their hyperparameters, and the optimal hyperparameters are problem-dependent. \n    Some of the experiments just use the default hyperparameters; this is insufficient when trying to directly \n    compare the performance of these methods, as their performance can vary greatly with different values of\n    these parameters. I'm not convinced that we should be drawing conclusions about the relative \n    performance of these algorithms from any of the experiments for this reason.\n\nOf course, meaningful empirical results are not necessarily characterized by statistically outperforming the baselines. \nWell designed experiments can highlight important ways in which the performances differ, providing the community \nwith a deeper understanding of the methods investigated. I would argue that the experiments in the paper do not \nachieve this either; the experiments do not provide any new intuition or understanding of the methods, showing \nonly the relative performances in terms of learning curves on a somewhat random collection of supervised learning problems. Why were these specific problems chosen? What makes these problems ideal for showcasing the performance of AdaX? If AdaX is an improvement over Adam, why? What exactly is happening with it's effective step-sizes that leads \nto the better performance? Can you show how their step-sizes differ over time? \n\nStatements that need citation or revision:\n  - \"Adaptive optimization algorithms such as RMSProp and Adam... as well as weak performance \n     compared to the first order gradient methods such as SGD\" (Abstract). This needs a citation. \n     Similarly, \"AdaX outperforms various tasks of computer vision and natural language processing and can catch \n     up with SGD\"; as above, I'm unaware of work (other than theoretical) that shows that SGD significantly \n     outperforms Adam in deep neural networks.\n  -  \"In the era of deep learning, SGD ... remains the most effective algorithm in training deep neural \n      networks\" (Introduction). What are you referring to here? Vanilla SGD? Or are you including Adam etc here? \n      As above, this should have a citation. Adam's popularity is largely due to its effectiveness in training \n      deep neural networks.\n  - \"However, Adam has worse performance (i.e. generalization ability in testing stage) compared with SGD\" \n     (Introduction). Citation needed.\n  - In the last paragraph of the Introduction, you introduced AdaX twice: \"To address the above issues, we propose a \n    new adaptive optimization method, termed AdaX, which guarantees convergence...\", and, \"To address \n    the above problems, we introduce a novel AdaX algorithm and theoreetically prove that it converges...\"\n"}