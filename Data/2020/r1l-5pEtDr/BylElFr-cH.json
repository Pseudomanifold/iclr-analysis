{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper points out that existing adaptive methods (especially for the methods designing second-order momentum estimates in a exponentially moving average fashion) do not consider gradient decrease information and this might lead to\u00a0suboptimal convergences via simple non-cvx toy example. \nBased on this observation, the authors provide a novel optimization algorithm for long-term memory of past gradients by modifying second-order momentum design. Also, they provide aconvex regret analysis and convergence analysis for non-convex optimization. Finally, the authors evaluate their methods on various deep learning problems.\n\nSignificance/Novelty: While there have been many studies on non-convergence of Adam,\u00a0raising an issue on ignoring the gradient decrease information seems novel. \n\nPros:\n1. The motivating toy example in Section 3 is useful for readers to get intuitions.\n\n2. By introducing long-term memory on past-gradients, the authors fix the Adam's issues and they can also improve the convergence rate in a non-convex optimization (Corollary 4.2).\n\n3. Empirical studies show superiority to original Adam (Section 5).\n\nCons:\nWhile they provide a significant study on Adam's failure and a novel optimization algorithm, I have several concerns:\n1. What is default hyperparameters for AdaX in Algorithm 1? Is it the same as AdaX-W (Algorithm 3) in Appendix? The bias correction term in the line 7 of Algorithm 1 will be very large even with small $\\beta_2$ since it is expoential (For example, (1 + 0.0001)^(100000) ~ 20000 for $\\beta_2$ = 10^(-4)). So, it is not clear that the second momentum estimate of Ada-X is really stable. For this, it would be interesting to see how the trajectories of second-order momentum estimates of Adam, AMSGrad, Ada-X are different. I think this will help to understand Ada-X better.\n\n2. In terms of theory, I think the Lemma 4.1 is inevitable for convergence guarantees in Theorem 4.1 and Theorem 4.2. Although the authors effectively remove log T in the numerator in Corollary 3.2 of Chen et al. (2019) using their lemma 4.1 (I think this is the key point), the assumption that $\\beta_{2t} = \\beta_2 / t$ seems quite strong, and original Adam paper has no such assumptions. For a real deep learning problems such as training ResNet on CIFAR-10, the $\\beta_{2t}$ is almost zero after even one or two epochs where Ada-X behaves like vanilla SGD. Is there no room for relaxing this assumption such as $\\beta_{2t} = \\beta_2 / \\sqrt{t}$? Also, it is not clear how the authors derive Corollary 4.2 from Theorem 4.2 since Theorem 4.2 assumes $\\beta_{2t} = \\beta_2 / t$ while Corollary 4.2 does not.\n\n3. In the experiment, it is not clear that the authors use the same strategy for constructing first-order momentum for Adam with a newly introduced parameter $\\beta_3$. In other words, the authors should use the same policy on constructing the first-order momentum estimate for both Adam and Ada-X. Also, as the authors add an additional hyperparameter $\\beta_3$, the effect of $\\beta_3$ on performance should be discussed at least empirically.\n\n4. There are many studies on fixing poor generalization of adaptive methods (such as AdaBound which the authors cited). In this context, Zaheer et al. (2018, Adaptive methods for non-convex optimization) propose a large epsilon value (numerical stability parameter) such as $\\epsilon = 10^{-3}$ for better generalization. It will be more interesting to see the comparisons in this regime.\n\n5. In my experience with Adam-W (Decoupled weight decay regularization), Adam-W requires a relatively large weight decay parameter $\\lambda$.\nAs an example, DenseNet-BC-100-12 shows a similar validation accuracy with Adam-W $\\lambda = 0.05$ under the learning rate scheduling in (Huang et al. 2016, DenseNet)  as vanilla SGD.\nTherefore, the authors should consider more broader range of weight decay parameters for at least image classification tasks.\n\nMinor:\n1. In eq (2), the domain of x should be mentioned: according to Reddie et al, it is [-1,1].\n2. In both theorem 4.1 and corollary 4.1, $D_{\\infty^2}$ should be $D_{\\infty}^2$?"}