{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a new adaptive gradient descent algorithm with exponential long term memory. The authors analyzed the non-convergence issue in Adam into a simple non-convex case. The authors also presented the convergence of the proposed AdaX in both convex and non-convex settings.\n\n- The proposed algorithm revisited the non-convergence issue in Adam and proposed a new algorithm design to try to address this issue. However, the new algorithm design is a bit strange to me, especially in Line 6 of Algorithm 2, the authors proposed to update v_t by (1+ \\beta_2) v_{t-1} + \\beta_2 g_t^2, where normally people would use (1-\\beta2) and \\beta2 as the coefficients. I am not quite get the intuition of using such as strange design. I wonder if the authors could further explain that.\n\n- The authors also add change \\beta_1 in Line 5 of Algorithm 2 into \\beta_3. And then in theory, the authors again choose \\beta_3 as \\beta_1. It seems that \\beta_3 is not contributing to any theoretical result. It seems to me to just have another parameter to tune in order to get better performances. Can the authors gives more justification on why introducing such a term here? And also show how the different choice of \\beta_3 affects the final result? \n\n- Missing some important references closely related to this paper:\n\nChen, Jinghui, and Quanquan Gu. \"Closing the generalization gap of adaptive gradient methods in training deep neural networks.\" arXiv preprint arXiv:1806.06763 (2018).\nZaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018.\nZhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n\nI would suggest the authors to also compare with the above mentioned baselines to better demonstrate its performances and theoretical results.\n\n- The authors include theoretical analysis in both convex and non-convex settings, which is appreciated, however, the theoretical result seems to show similar convergence guarantees with AMSGrad. I wonder if the authors could provide theoretical justifications on why the proposed method is better than prior arts, probably some sharper convergences or some generalization guarantees?\n\n- In the experiments part, I wonder why the authors did not compare with AMSGrad, RMSProp in later parts such as ImageNet, IoU and RNN parts? I makes no sense to drop them for those experiments. Also, are the authors fully tuned the hyper-parameters for other baselines such as step size and weight decay on SGDM?\n"}