{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Recently, it has been shown that spiking neural networks (SNN) can be trained efficiently, in a supervised manner, using backpropagation through time. Indeed, the most commonly used spiking neuron model, the leaky integrate-and-fire neuron (LIF), obeys a differential equation which can be approximated using discrete time steps, leading to a recurrent relation for the potential. The firing threshold causes a non-differentiability issue, but it can be overcome using a surrogate gradient. In practice, it means that SNNs can be trained on GPUs using standard deep learning frameworks such as PyTorch or TensorFlow.\n\nHere the authors extend this approach by proposing two variations of the LIF model, called RLIF and LIF-LSTM. However, the presentation of these models is not clear at all.\nFor example:\n* what is U^t in Equation 4?\n* what is M^t in Equation 7?\n* what is the difference (if any) between u^t and u_d^t?\nEquation 8 is even more obscure. Why bothering defining a new variable Y if it is equal to F? What is index j, and why is it used only on the left hand side of the equation?\n\nThe description of the LIF-LSTM is even more obscure, nothing is defined.\n\nFigure 2 has an error. On the left, with the heavyside activation function, the gradient is actually defined everywhere (with a value of 0) but on the red segment!!!\n\nIn addition, the experiments are not convincing. I am not an expert in NLP, so I will focus on the vision experiments.\nTable 1 is incomplete. Wu et al 2019 (which they cite elsewhere!!!), reached 60.5% on DVS-CIFAR10, which is much better than this paper (56.93%)\n\nFor all these reasons, I recommend rejection.\n\n"}