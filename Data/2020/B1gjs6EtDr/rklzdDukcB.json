{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel way to increase efficiency for self-attention based sequence modeling neural networks. The proposed approach is incremental by combining content-based sparse attention with local/temporal sparse attention.\n\nWhile the extension is incremental, they are able to reduce the overall complexity and achieve new state-of-the-art on wiki-text 103 dataset.\n\nThe paper is also very well written and easy to follow and understand.\n\nOne negative of the paper is section 4.1. I find the discussions around NMF to be somewhat orthogonal, especially considering the paper does not use NMF techniques for their clustering algorithm in section 4.2.\nWould sparse coding in general be a good high level motivation for the proposed clustering?\n\nIt is also appreciated that the authors have released demo code for reproducibility."}