{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes content-based sparse attention to reduce the time/memory complexity of attention layers in Transformer networks. The method essentially boils down to keep a set of K mean vectors (which are learned/updated during training), which are used to provide clusters to be attended over. When combined with prior work on local attention (i.e. half of the heads are local attention, the other half are the newly proposed routing attention), but model is found to outperform, or be on par with, existing Transformer models despite generally being smaller.\n\nOverall the empirical performance is quite impressive, especially on the highly-competitive Wiktext-103 dataset. The fact\nHowever I had some detailed comments/questions:\n\n- In equation 3, isn't A_{ij} still the output from the full (lower triangular) attention? Or do you change the softmax normalization such that it is over C_i?\n\n- Some natural ablation studies are missing. How does the model do if it only uses routing attention? What about only local attention? Finally, what about full attention with O(n^2)? I understand some of the listed baselines are already working with local attention, but there are differences in setup that could contribute to differing performance. Therefore it would be good to ablate on these aspects, holding the other parts (layers/initialization/optimization algorithm etc.) constant.\n\n- What is the *actual* running time/memory for local/routing/full attention layers? My guess is that the actual, rather than theoretical, difference would not be that great. It seems like the routing layer requires additional operations (i.e. online k-means) which could increase running time. Also, sorting by distance to mu_i doesn't seem very GPU-friendly.\n\n- I found the connection to NMF somewhat tenuous, especially given the different objective (i.e. A is not fully instantiated). I feel that it would be more informative to have some of the ablation studies mentioned above instead.\n\n"}