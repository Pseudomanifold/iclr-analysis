{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "[Summary]\nThis paper performs an extensive empirical evaluation of Neural Tangent Kernel (NTK) classifiers---kernel methods that theoretically characterize infinitely wide neural nets---on small-data tasks. Experiments show that NTK classifiers (1) strongly resemble the performance of neural nets on small-data tasks, (2) can beat prior benchmark methods such as Random Forests (RF) on classification tasks in the UCI dataset, and (3) can also outperform standard linear SVM on a few-shot learning task.\n\n[Pros]\nThe question considered in this paper is well motivated, and a very natural extension of Lee et al. (2019) and Arora et al. (2019a). These papers show that NTK performs well on (relatively) large benchmark tasks such as CIFAR-10 but is still a bit inferior to fully trained neural nets. On the other hand, for small-data tasks, the relationship is reversed --- neural nets are slightly inferior to more traditional methods such as random forests (e.g. from Fernandez-Delgado et al. 2014) and Gaussian kernel SVMs. As the NTK gives a limiting characterization for wide neural nets, it is a sensible question to test the performance of NTK on these small datasets, and see if they can improve over neural nets and compare more favorably against the traditional methods.\n\nThe experimental results, in my perspective, is a reasonably convincing evidence that the resemblance between NTK and NN on small-data tasks is stronger than on larger tasks such as CIFAR-10, which agrees with the NTK theory. In addition to the UCI datasets, the paper also tries out NTK in a few-shot learning task and show that SVM with the convolutional NTK does better than the linear SVM as the few-shot learner. I am less familiar with few-shot learning though so am not entirely sure about the strength of this part.\n\nThe paper is well-written and delivers its messages clearly. The results and discussions are easy to follow.\n\n[Cons, and suggestions]\nThe message that \u201cNTK beats RF\u201d seems a bit delicate to me, specifically considering the fact that the average accuracies of (NTK, NN, RF) are all pretty close but the Friedman rank comparison says NTK > RF > NN (somewhat more significantly). This implies the difference between all these methods has to be small and it\u2019s only that NTK happens to win on more tasks. In addition, NTK tunes one more parameter (L\u2019) than NNs, so I guess perhaps NNs can also be tuned to outperform RF in the rank sense if we also tune L\u2019 (by fixing the bottom L\u2019 layers to be not trained) in NNs?\n\nAlso, it would be better if the authors could provide a bit more background on the metrics used in the UCI experiments -- for example, the Friedman rank is not defined in the paper."}