{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this work the authors address major deficiency of Neural Module Networks, aiming more effective compositional reasoning for VQA. NMN has a lack of automated layout construction and specificity of the modules. This is addressed by leveraging the new GQA dataset which provides scene graphs and whose questions are constructed by a Program Generator.\n\nI am familiar with the GQA dataset and related work, yet I had a few issues when reading the paper to understand precisely what is done. It would be good to have just one quick summary of what is done including where supervision is applied what the student is doing precisely, how the ground truth programs are used, etc. There are multiple diagrams which are hard to reconcile with each other and lots of invented language. Here is a few specific comments. \n\nFor example the penultimate paragraph of Intro the system is described and references Figure 1, however I found it challenging to associate figure 1 to the text description.\n\nSupervision/Teacher/Student: This is the part I struggled the most to understand and hope the reviewers can clarify this in the rebuttal. I failed to understand from Figure 4 and the associated description what is the form of the student. Note this diagram doesn\u2019t clearly distinguish where the student and teacher start and end. My current understanding is that the student (aka the final inference model) does not ever build an explicit scene graph. Does it build an explicit program? What is the final form of the student? \n\nA potential way the authors can improve clarity is by showing more explicitly in the diagrams how certain steps would be done in the NMN. \n\nExperiments: \nI believe Table 1 can be unclear/misleading to the casual reader in terms of what information is available at training time. E.g. most of the models listed aren\u2019t using the scene graphs, I believe the MMN is the only one using the programs as well.  I would like an additional column or table added that specifies what external information besides Q+Image are used. Note this confusion is also present in other works like the NSM paper.\n\nRegarding the NSM, I believe the authors attribute the poorer performance to using a weaker pretrained visual feature extractor (which in the NSM is used to output scene graphs). Is this the case? If so the text should be clarified to state this more explicitly. \n\n(minor) the Meta in the name gives the impression there is some meta learning involved. \n\nOverall, I think the direction the authors take is interesting, explicitly representing reasoning steps. I am currently giving the paper a weak reject due to clarity issues, but will strongly consider to raise my score after some clarifications.  Besides clarity my main concern about the paper is that it might be overengineering a solution using the exact information provided in the GQA dataset: scene graphs, programs etc. Is the model essentially being heavily supervised to infer backwards the data generating process of GQA, might this mean it will heavily overfit to the questions posed in this dataset. I would like the authors to address the generality of the method in the rebuttal. \n"}