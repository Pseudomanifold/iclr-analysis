{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors describe a method to improve the performance of generative adversarial networks in the task of generating structured objectives that have to satisfy complicated constraints. The proposed solution involves using an additional term in the GAN objective that penalizes the generation of invalid samples. This term, called the semantic loss, is given by a multiple of the log probability of the model generating valid samples.\n\nClarity:\n\nThe paper is not very well written and several parts need to be clarified. In particular, in equation 3. What is theta in this equation?  how is it obtained? The authors mention briefly how their method could be used to deal with intractable constraints, but they're almost no specific details or examples of how this is done in practice. The proposed approach relays on the knowledge compilation method, but they're very few details of it in the document. Is it used at all in the experiments?\n\nI am concern about the lack of reproducibility of the paper. I believe, from the paper as it is, it will be impossible to reproduce the results. There are no details about public code release, hyper-parameters settings, etc. For example,\nin section 4.3 the authors mention that they condition the constraint on 5 latent dimensions without giving details about which dimensions exactly.\n\nSignificance:\n\nIt is hard to quantify the significance of the contribution. The constrained images problem is very toy and simple and the experiments with molecules do not include any baseline (only the GAN model without the constraint). There have been\nmany recent contributions improving the validity of generative models for molecules and the authors do not compare with any of them.\n\nThe authors also fail to cite relevant work such as\n\nJaques, Natasha, et al. \"Sequence tutor: Conservative fine-tuning of sequence\ngeneration models with kl-control.\" Proceedings of the 34th International\nConference on Machine Learning-Volume 70. JMLR. org, 2017.\n\nSeff, Ari, et al. \"Discrete Object Generation with Reversible Inductive\nConstruction.\" arXiv preprint arXiv:1907.08268 (2019).\n\nNovelty:\n\nThe proposed approach is rather incremental and lacks novelty. It consists in just applying the semantic loss approach of Xu et al. 2018 to GAN training, with very limited new methodological or algorithm contributions.\n\nQuality:\n\nThe experiments performed are not strong enough to validate the proposed method. The authors do not consider strong baselines in their evaluations.\n\nSummary:\n\nI find that the problem addressed by the authors is highly relevant and the proposed approach has the potential to be useful in practice. However, the paper needs to be improved regarding its clarity, reproducibility and strength of experiments before it can be accepted for publication."}