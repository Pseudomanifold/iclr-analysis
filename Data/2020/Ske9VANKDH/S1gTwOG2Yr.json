{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a \"gamma principle\" for stochastic updates in deep neural network optimization. First, the authors propose if Eq. (2) is satisfied then convergence is guaranteed (Thm. 1). Second, they use experimental results of Alexnet and Resnet-18 on cifar10/100 to show that Eq. (2) is satisfied by SGD, SGD with momentum, and Adam, with different activations and tricks like batch normalization, skip connection.\n\nPros:\n1. This paper is well written and the presentation is clear.\n2. The experiments are extensive.\n\nCons:\n1. Before Eq. (2), it is assumed that over-parameterized NNs are used as \\theta. But there is no quantization how many parameters are enough? Are the Alexnet/Resnet-18 in experiments enough over-parameterized and how can we tell that? Some quantitative conditions should be provided to show what kind of models this principle hold.\n\n2. The connection between Eq. (2) and Thm. 1 is too obvious and the gamma is just to characterize the progress of each update. Of course, large progress corresponds to fast convergence. Eq. (2) is a strong assumption rather than a theoretical contribution.\n\n3. Experiments are used to show that Eq. (2) is a \"principle\". However, the experiments are problematic as follows.\nFirst, \"we set \\theta^* to be the network parameters produced in the last training iteration\", then how do we make sure \\theta in the last training iteration is \\theta^*, even if the loss is close to (but not exactly) zero? For this point, I suggest using a teacher-student setting, where the optimal \\theta^* is already known.\nSecond, using \\theta in the last training iteration makes the experiments show the following simple fact, that methods/tricks/activations with faster convergence to certain parameter will have larger every update progress to that parameter, which is, of course, true and does not reveal an optimization principle of deep learning.\nThird, it is difficult to claim that this is a principle for general deep learning by using experiments on two datasets.\n\nOverall, I found the theory not inspiring and experiments not convincing."}