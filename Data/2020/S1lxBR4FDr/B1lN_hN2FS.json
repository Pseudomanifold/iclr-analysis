{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper discussed adversarial training for robust model against strong evasion attack such as PGD. The main idea is to combine single-step adversarial example generation FGSM with dropout in multiple layers, and the authors find this strategy can train models as robust as PGD adversarial training.\n\nI have some comments\nFirst, the paper is unnecessarily long. The method is rather simple, and there is no rigorous theoretical analysis. For example, section 2 seems can be shortened and merged to other sections.\n\nSecond, the claim \u201c...learns to prevent the generalization of single-step adversaries...\u201d on page 2 looks rather confusing. What does \u201cgeneralization of single-step adversaries\u201d mean? In figure 1, is  the same method (FGSM) used to generate adversarial examples for training and validation? The FGSM adversarially trained robust model should be able to defend against FGSM attacks. How could you explain the big gap in Fig 1 last column?\n\nOn page 6, the hyper-parameters P_d and r_d seems to be well-tuned. What is the insights from these various parameters for different settings?\n\nIn almost all the experiments, the proposed method is actually worse than PGD adversarial training under the strong PGD attack.\n\nPlease discuss the following related work\nAdversarial Training for Free. https://arxiv.org/abs/1904.12843\nYou only propagate once. https://arxiv.org/abs/1905.00877\n\nDiscussing a concurrent submission will be a plus \nFast is better than free https://openreview.net/forum?id=BJx040EFvH"}