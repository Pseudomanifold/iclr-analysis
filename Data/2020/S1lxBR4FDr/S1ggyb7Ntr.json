{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary\n========\nThis paper investigates why single-step adversarial training leads to gradient-masking, as shown in \"Ensemble Adversarial Training\" by Tramer et al. The authors suggest that this is due to overfitting, and propose a new type of single-step adversarial training method coupled with dropout, which appears to achieve comparable robustness to more expensive multi-step training methods.\nOverall, I find the findings over prior work to be somewhat limited, and the experimental evaluation is also not the most convincing. I thus lean towards rejection.\n\nComments\n=========\nInvestigating why FGSM adversarial training leads to gradient masking is an interesting question.\nHowever, I'm not convinced that the experiments conducted by the authors give fundamentally new insights into this question. In particular, I'm not sure that \"overfitting\" is really the right characterization for what's happening.\n\nThe fact that the metric R_\\eps proposed by the authors satisfies R_\\eps < 1 was already hinted at in the work of Tramer et al. Specifically, Tramer et al. observe that for a model trained with single-step methods, the FGSM update step is less adversarial than a fully random step.\nThe interesting observation here (Figure 1) is that the FGSM adversarial examples are initially good, but then perform worse as the model trains. However, I don't think that characterizing this effect as overfitting is necessarily correct here, as the training data and validation data do not come from the same distribution anyways.\nThe authors' experiment with the validation loss shows that as training progresses, the model becomes more vulnerable to adversarial examples transferred from another model. This high transferability was also observed by Tramer et al., and seems to have a simpler explanation: after the model \"learns\" to mask gradients, the FGSM examples it produces are not too different from the clean examples. So once gradient-masking kicks in, the adversarially trained model is trained similarly to the model that is the source of the transferred examples, which explains the increase in transferability.\n\nThe combination of FGSM adversarial training with full-layer dropout is still interesting to consider.\nYet, I do find some of the experimental results a bit hard to believe:\n\nFor example, Figure 3 seems to suggest that FGSM + standard dropout (FAT-TS) achieves about 30% accuracy against PGD on MNIST. This seems clearly wrong. E.g., one of the models in the \"Ensemble Adversarial Training\" paper was trained in this way and did not achieve meaningful robustness against PGD.\nThe step-size and number of steps chosen for PGD seem problematic here. With 40 steps of size 0.01, you can at most make a per-pixel change of size 0.4. Yet, because of the random start, every pixel could potentially be 0.6 away from its optimal value.\n\nSome numbers in Tables 1,2,3 are also suspicious:\n- In Table 1, EAT models A and B achieve non-trivial robustness to PGD and IFGSM. This suggests that the attacks are too weak. These models are not robust to iterative attacks. The same holds for models B & C in Table 3.\n- In Table 2, FAT and EAT model D also achieve non-trivial PGD robustness which is surprising.\n\nWhile the evaluation is fairly thorough, many of the evaluated attacks are redundant. E.g., FGSM can be dropped altogether, while IFGSM, MI-FGSM, and PGD are essentially all variants of each other. Adding a gradient-free / hard-label attack would be good practice.\n\nFor the results in Table 6, were these obtained with models trained against l2 attacks?\n\nI don't understand what Figure 6 is supposed to demonstrate that can't already be inferred from Figure 5. Isn't Figure 6 just a zoomed-in and inverted portion of the graph in Figure 5?\n\nFinally, with regards to time complexity (Section 4.5), the authors should consider recent works that show that multi-step adversarial training can be accelerated significantly by recycling gradient information.\nE.g., \"Adversarial Training for Free!\" by Shafahi et al. or \" You Only Propagate Once\" by Zhang et al."}