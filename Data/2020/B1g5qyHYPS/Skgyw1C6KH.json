{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1889", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a pruning technique targeting Depthwise separable convolution (DWC), which is not yet importantly treated in this field.\n- The proposed method achieved meaningful improvements on CIFAR 10 and SVHN.\n\nMethods\n- Clustering the kernels having similar pruning patterns\n- Training and filter pruning are conducted simultaneously.\n- In the pruning step, the filters are pruned among the cluster.\n\nQuestions\n- The clustering of the kernel seems to be critical for the performance. Is the clustering derived by the constraint of DWC as explained in 3.2? \nIf so, it can be the strong point of the work, and it would be better to add a more detailed explanation in section 3.3.\n\nStrongpoints\n- As far as the reviewer knows, it is a novel attempt to prune lightweight DWC based networks, MobileNet v1 and v2, in this field.\n- They propose a pruning technique considering the constraints of widely used DWC structures.\n- The proposed method achieved meaningful improvements on CIFAR 10 and SVHN.\n\nConcerns\n- The main concern the reviewer has is that the experiment section should include more items.\n- Practically, many works stem from MobileNet v1 and v2 reports ImageNet (or at least Cifar 100) classification results. We concern that the results from CIFAR 10 and SVHN are not enough to verify the proposed method. Also, recent pruning methods (Mostafa.et.al, Dettmers.et.al, Han.et.al 2) report ImageNet result, or at least Mini-imageNet result (Lee.et.al).\nSince the proposed method targets MobileNet V1&V2, the verification on the larger dataset (ImageNet - at least Cifar 100 or Tiny-imageNet) is required.\n- The other concern is the lack of comparison to the other method. The author argues that the existing pruning methods are not adequate for DWC pruning, but we cannot find sufficient comparsion results regarding the argument. The reviewer thinks that it is possible to apply the existing pruning methods (Han.et.al 1),(Of course, we cannot guarantee the performance.) So the author must compare the proposed method to other pruning methods and show that the proposed method performs well in DWC pruning; MobileNet V1&V2, than the existing pruning method.\n\nConclusion\n- The author proposed a new pruning technique for DWC (MobileNet v1&V2) which would be novel, but the experiments should include more items to verify the method.\n\n\nInquiries\n- Testing on larger datasets (ImageNet, or at least CIFAR 100 and Tiny imageNet)\n- Comparison to the other pruning method\n- (supp) More explanation for the clustering part.\n\nReference\n[1] Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In ICML, 2019.\n[2] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.\n[3] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pp. 1135\u20131143, 2015.\n[4] Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Shijian Tang, Erich Elsen, Bryan Catanzaro, John Tran, and William J Dally. Dsd: regularizing deep neural networks with dense-sparse-dense training flow. In ICLR, 2017.\n[5] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. In ICLR, 2019."}