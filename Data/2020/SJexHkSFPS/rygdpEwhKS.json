{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper tackles the problem of making decisions for the next action while still engaged in doing the previous actions. Such a delay could either be part of the design (like a robot deciding the next action before its actors and motors have come to full rest after the current action) or an artefact of the delays inherent in the system (i.e. latency induced by calculations or latency of sensors). The paper shows how to model such delays within the Q-learning framework, show that their modelling preserves desirable contraction property of the Bellman update operator, and put their model into practice by an extensive set of experiments: learning policies for several simulated and a real-world setting.\n\nThe authors claim that that addition of this \"delay\" does not hinder the performance much of the RL method is given sufficient \"context\" about the delay, i.e., given extra features as input in order to learn to compensate for it. The writing of the paper is lucid and sufficient background is provided to make the paper self-sufficient in its explanations.\n\nHowever, there are some reasons which do not allow me to fully support the paper's acceptance.\n\nThe changes made to the basic Q-learning setup, albeit novel and with desirable properties, in my opinion, are (i) theoretically relatively straight forward, (ii) are not expressive enough to capture the problem in its full generality (explained later), and (iii) need more empirical justification with problems where their modification is indeed indispensable. The authors touch on several different research areas cursorily (viz. continuous reinforcement learning, Bellman contractions, feature engineering) while providing grounds for their idea, but in the end return to the familiar domain of discrete Q-learning with semi-hand-crafted (though theoretically motivated) features where the latency of actions can take a set of fixed values and the state is sampled at fixed intervals. \n\nIf the actions are continuous, then could method from Doya (2000) directly be used to solve these problems? Can the value-based models which he describes be augmented and extensions developed which build on Lemma 3.1 instead of the well-trodden ground of Lemma 3.2? Especially, if one of the objectives which the authors claim their policies are better is \"policy duration\", then the absence of purely continuous policies is particularly egregious. Further, reducing the policy duration seems like an independent objective which perhaps can be used for reward shaping for the traditional policy methods, which will also lead to different baselines.\n\nThe authors explicitly say that their method focuses on \"optimizing for a specific latency regime as opposed to being robust to all of them;\" and that they explicitly avoid learning forward models by including additional features. However, the advantages of placing such restrictions on the design space are unclear at best. Would it be the case that the high-dimensional methods will fail in this setting? Are there theoretical advantages to working on limiting the attention to known latency regimes? I suspect that the authors have concrete reasons for making these design decisions, but these do not come across in the paper in the writing, or by means of additional baselines.\n\nAs an example of a different approach towards the problem, which the authors overlook in their related work section, is that of learning with spiking neurons and point processes. These areas of research have also been interested in problems of the \"thinking while moving\" nature: that of reinforcement learning in the context of neurons where the neurons act by means of spikes in response to the environment and other \"spikes\" [1, 2]. More recently, with point processes, methods have been developed to attain truly asynchronous action and state updates [3, 4]. A differently motivated work which ends up dealing with similar problems is in the direction of adaptive skip intervals [5], where the network also chooses the \"latency\" in the discrete sense. Adding such related work would help better contextualize this paper.\n\nSome other ways the authors can improve the paper are (in no particular order):\n\n - The description of the Vector-to-go is insufficient; some concrete examples will help.\n - The results of the simulated experiments are given in the form of distributions and it is very difficult to discern the effect of individual features in Figure 1. Additionally, due to missing error bars, or other measures of uncertainty, the claim that the performance of models with and without the delayed-actions is comparable to the blocking setting seems tenuous at best, just looking at the rewards.\n - In particular, for the real experiments, we need more details about the experiment runs to determine why the performance of the policies in the real world is so vastly different. Could the authors describe why the gap can be completely covered through simulations but not in the real world?\n\n[1]: Vasilaki, Eleni, et al. \"Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail.\" PLoS computational biology 5.12 (2009): e1000586.\n[2]: Fr\u00e9maux, Nicolas, Henning Sprekeler, and Wulfram Gerstner. \"Reinforcement learning using a continuous time actor-critic framework with spiking neurons.\" PLoS computational biology 9.4 (2013): e1003024.\n[3]: Upadhyay, Utkarsh, Abir De, and Manuel Gomez Rodriguez. \"Deep reinforcement learning of marked temporal point processes.\" Advances in Neural Information Processing Systems. 2018.\n[4]: Li, Shuang, et al. \"Learning temporal point processes via reinforcement learning.\" Advances in Neural Information Processing Systems. 2018.\n[5]: Neitz, Alexander, et al. \"Adaptive skip intervals: Temporal abstraction for recurrent dynamical models.\" Advances in Neural Information Processing Systems. 2018."}