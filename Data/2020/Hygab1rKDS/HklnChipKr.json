{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors present a quantum algorithm for approximating the forward pass and gradient computation of a classical convolutional neural network layer with pooling and a bounded rectifier activation. This algorithm has complexity bounds that would open up (for instance) the possibility of exponentially large filter banks, and the authors show through a simple, classical simulation approach that the resulting network is also likely to be trainable.\n\nFeedback:\n\nA few typos/formatting issues:\n- The title accidentally includes \"Conference Submissions\"\n- The in-text citation format frequently has the parentheses in the wrong place; this is surprisingly distracting!\n\nPreliminaries:\n- Maybe explain what the ith vector in the standard basis is in terms of |0> and |1>? I assume the answer is along the lines of |000>, |001>, |010>, etc.?\n\nMain results:\n- The sentence \"a speedup compared to the classical CNN for both the forward pass and for training using backpropagation in certain cases\" is ambiguous; does \"in certain cases\" qualify only training speed or also forward pass speed?\n\n- There's a clear separation of background (which is concise and well explained) and contributions, but maybe it would be worth connecting the introduced algorithm more closely to existing work in non-convolutional quantum neural networks?\n\n- Can you briefly justify (or cite) the claim that \"most of the non linear functions in the machine learning literature can be implemented using small sized boolean circuits\"?\n\n- I'm a little confused about the discussion of quantum importance sampling on page 4. Could you give some intuition for the relationship between eta and the fraction of output values that are on average flushed to zero (is this 1 minus sigma?), and perhaps connect this to the literature about activation pruning and sparse NNs?\n\n- Maybe define what you mean by \"tomography\" for ML folks without the quantum background?\n\n- I'm convinced by the simulations, even though I shouldn't really be convinced by anything on MNIST... It just seems like the perturbations you're applying are all things that modern neural networks take in stride.\n\n- The discussion of using a sigma-based classical sampling rather than the eta-based quantum importance sampling mentions a \"Section C.1.15\" which does not exist (I think you mean the end of Section C.1.5).\n\n- Re: \"We will use this analogy in the numerical simulations (Section 6) to estimate, for a particular QCNN architecture and a particular dataset of images, which values of \u03c3 are enough to allow the neural network to learn.\" My understanding is that you're getting empirical estimates of which values of sigma are enough; it would be valuable to convert those to estimates of which values of eta would be enough (given quantum networks of the size used in the classical simulation experiment, or given larger networks).\n\n- The sampling procedure based on sigma might be inefficient in your PyTorch implementation, but it's certainly something that GPUs are fairly well suited to computing. There might be other PyTorch operators that would help here (perhaps Bernoulli sampling?) or if nothing else you could write a small custom CUDA kernel."}