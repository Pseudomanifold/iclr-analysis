{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a quantum version of the convolutional neural networks. They derive equivalent versions the computation of both the convolution operation and the back-propagation in the quantum computing framework. The authors claim a potential exponential speed up in the computation in the size of the kernel which would make possible to process much bigger inputs or just speed up current tasks involving images mainly. They exemplify the method on MNIST using quantum artifacts simulation using PyTorch and show their method is competitive with SoA.\n\nI think the paper is well written and provides a nice discussion about how quantum computing can be applied to CNNs. I appreciate that both the forward and backward passes are studied although most of the technical details are in appendices. So, I felt the paper  itself was a bit optimistic about the impact of quantum computing on CNNs.\n\nEspecially, I found the experimental section was missing details. I am not a quantum computing expert but I was a bit surprised that, in the context of CNNs, the introduction of quantum noise was just considered as introducing noise in the image and the gradient and then applying normal CNNs to the resulting image. Standard CNNs can probably deal with such noise and noisy gradient descent is not a big issue as such (it can even avoid local minima). But CNNs are notoriously known to reduce the number of weights in a network because of weight sharing. So, it is not all about making one convolution faster but also to compute invariant representations by sharing weights over different parts of the inputs. I would have been interested by a discussion about how quantum noise may impact this property and I didn't find this in the paper nor the appendices. \n\nAlso the author confess that the learning is not stable and results on MNIST to be the best they could get. I think it would be worth testing on large scale problems and see whether larger kernels with such noisy conditions would really improve the performance. "}