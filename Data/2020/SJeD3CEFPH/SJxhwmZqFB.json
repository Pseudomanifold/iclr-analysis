{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors investigate meta-learning in reinforcement learning with respect to sample efficiency and the necessity of meta-learning an adaptation scheme. Based on their findings, they propose a new algorithm 'MQL' (Meta-Q-Learning) that is off-policy and has a fixed adaptation scheme but is still competitive on meta-RL benchmarks (a distribution of environments that differ slightly in their reward functions).\n\nThey motivate the paper by data-inefficiency of current meta-learning approaches and empirical results suggesting that meta-learning the adaptation scheme is less important than feature reuse. \n\nOn the other hand, their introduction section would benefit from additional references to the kind of meta-learning they describe. In particular, their so-called \"definition of meta-learning\" is mostly about domain randomization (e.g. Tobin et al. 2017 https://arxiv.org/abs/1703.06907) and not about the broader 'learning to learn' RL methodology (in particular Schmidhuber 1994 \"On learning how to learn learning strategies\").\n\n**The authors make the following contributions:**\n\n1. They show that Q-Learning trained on multiple tasks with a context variable as an input (an RNN state summarizing previous transitions) is competitive to related work when evaluated on a test task even though no adaptation is performed\n\n2. Based on these observations, they introduce a new method for off-policy RL that does not directly optimize for adaptation but instead uses a fixed adaptation scheme\n\n3. The new method leverages data during meta-testing that was collected during meta-training using importance weights for increased sample efficiency\n\n**Overall, we believe the contributions are significant and sufficiently empirically justified.**\n\nThere are strong similarities, however, to parallel work on analyzing whether MAML relies on feature reuse or rapid learning (Raghu et al. 2019 https://arxiv.org/abs/1909.09157).\n\nThis work and the present submission conclude that feature reuse is much more significant than meta-learning an adaptation scheme when evaluated on current meta-RL benchmarks. This is a significant result and supports the new method developed in this paper.\n\nDuring meta-training, their proposed method maximizes only the average return across tasks, not the ability to adapt from the resulting parameters.\n\nTheir method introduces a fixed (non-learned) adaptation scheme that performs favorably compared to certain methods from the existing meta-learning literature and demonstrates that even dropping this adaptation still does well.\n\nThere are strong similarities to Nichol et al. 2018 (https://arxiv.org/abs/1803.02999). We encourage the authors to relate this work to Raghu et al. 2019 and Nichol et al. 2018.\n\n**Despite these interesting results, we strongly disagree with the meta-learning narrative of their new method.**\n\nBecause the adaptation scheme is no longer optimized directly, instead a fixed adaptation scheme is assumed, hence the approach in this paper is no longer a meta-learning algorithm.\n\nInstead, this method has strong similarities with transfer-learning and domain adaptation (first training on one distribution of tasks, then fine-tuning on another task).\n\nThe authors should discuss the links to these fields of research, and clarify what's really novel, already in the abstract.\n\nFor example, on page 2 the authors claim that optimizing the multi-task objective (the mean error across tasks) is the simplest form of meta-learning. This objective, however, is NOT meta-learning.\n\n**Decision.**\n\nThe submission contains strong empirical results emphasizing the significance of feature reuse and the insignificance of learned adaptation on the tested meta-RL benchmarks.\n\nThe reuse of experience from meta-training during meta-testing by employing importance weights is also an interesting contribution.\n\nIn contrast, we are not satisfied with the presentation of their new approach as a meta-learning approach. This method should be introduced along the lines of: 'Transfer Learning / Feature Reuse in RL is competitive to meta-learning across similar tasks'.\n\nIn its current form, we tend to reject the paper because it further obscures what the term meta-learning refers to. The authors are confusing it with more limited transfer learning. \n\nAdditionally, it was not clear to us whether the quadratic penalty they add to their adaptation scheme is only empirically valid or whether there is a theoretical reason.\n\nFor now, we'd lean towards rejecting this submission, but we might change our minds, provided the comments above were addressed in a satisfactory way - let us wait for the rebuttal."}