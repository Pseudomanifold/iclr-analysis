{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: This paper presents the V-MPO algorithm for on-policy reinforcement learning that can handle both continuous/discrete control, single/multi-task learning and use both low dimensional states and pixels. \n\nV-MPO adapts MPO, a recent off-policy deep reinforcement learning algorithm, to the on-policy setting with the following changes: (1) In policy evaluation step, instead of using state-action value Q estimated from off-policy replay data, the authors use on-policy data to estimate the state value V; (2) In E-step of policy learning, construct the target distribution q(a|s) with estimated advantages; and (3) in M-step, rewrite the KL divergence constraints with Lagrangian relaxation and alternate between optimising policy and the multiplier alpha. \n\nThe experiments are two-fold: (1) For discrete control, a multi-task problem setting is considered (DMLab-30 and Atari-57). V-MPO shows improved performance compared to IMPALA and R2D2, in terms of asymptotic scores. The result also suggest good stability for different hyper-parameters. (2) In continuous problems, the experiments follow the single-task problem setting, including Humanoids tasks and OpenAI Gym tasks. V-MPO achieves higher asymptotic returns compared with standard Deep RL algorithms (MPO, SAC, PPO). However, it has a lower sample-efficiency, especially compared to off-policy algorithms (MPO and SAC). \n\nPros:\n+ This paper demonstrates successful adaption of MPO to the on-policy problem setting and achieves better asymptotic score comparing to baseline methods. \n++ Results are achieved in a relatively hyper-parameter insensitive manner. And some commonly used tricks like entropy regularisation are not necessary.\n+ Along with the previous results of MPO, this demonstrates an alternative framework of policy gradient-based RL: first construct a nonparametric target behavioural distribution and then move the parametric policy towards this distribution. \n\nCons and Questions:\n1. The comparison between V-MPO with other single-task deep RL methods is non-standard because: (1) V-MPO is trained with far more samples, and (2) only asymptotic score is reported for baselines. It would be more informative to add the learning curve of baselines methods to show sample-efficiency and convergence properties. One may wonder if the baselines would provide better performance than reported if provided with a comparable number of steps in an appropriate way (E.g., seed search, hyper parameter sweeps). \n2. The proposed adaption of MPO from off-policy to on-policy applies to both discrete and continuous problem settings. However, the designed experiments include only the discrete multi-task setting and continuous single-task setting. How does V-MPO compare to other methods in single-task discrete problems? This may enable comparison to a wider set of prior work.\n3. Only top 50% advantages samples are used for generating target distribution in E-step, and all samples are used for policy updates in M-step. Does this mismatch between E-step and M-step sample pool make a difference in optimisation?\n4. Some small tricks we used like PopArt, Top-K advantage. It\u2019s not clear if these are things that (were/could-also-be used) in competitor methods, and how important these are to achieve the good results shown here. At least an ablation study on their impact for V-MPO would be nice. \n\nMinor: \n- The mixture use of subscripts \u201cold\u201d and \u201ctarget\u201d is rather confusing, for example, in equation (5) and (15)."}