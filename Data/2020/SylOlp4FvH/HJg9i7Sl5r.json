{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new approximate policy iteration algorithm that is based on the previous method, called MPO, which formulates policy optimization (PO) as a probabilistic inference problem. The adaptation makes MPO become an on-policy method. One key modification made to MPO is to use advantage functions instead of Q-value function. \n\nOverall, the paper follows an interesting topic in policy optimization as inference. It follows MPO to formulate PO as an inference problem using a similar principle. It proposes some extensions to MPO. The experiments show a lot of promising results. I have some concerns as follows.\n\n- As V-MPO is developed based on MPO, however, the background on MPO is missing. This makes the reader hard to judge the novelty and difference of V-MPO against MPO. The discussion on the difference is very little. \n\n- V-MPO extends MPO to on-policy setting, so the policy evaluation is key for this modification. Besides this modification, the E-step and M-step's derivations and objectives look quite similar to those of MPO. Can the authors comment on this?\n\n- In addition, V-MPO is said to work for both discrete and continuous domains? It would be clearer if the authors discuss which parts in their algorithm enable this ability?\n\n- It would also be great if the algorithmic description is included. The overall algorithm contains multiple optimization steps and hyperparameters, e.g. number of updates, how Lagrangian multiplier adapted etc..\n\n- Continuous tasks: The comparisons with low sample-efficient approaches look unfair. Are there ablations that show performance comparisons of all when set with an equal level of samples?\n\n- Can the author elaborate on the comment \"These must be consistent between the maximum likelihood weights in Eq. 3 and the temperature loss in Eq. 4\".\n\n"}