{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to learn an ensemble of policies that is hard to imitate from their rollout trajectories. I like the idea of introducing the problem of privacy in reinforcement learning, and it is quite essential. However, some concerns are raised after checking the draft, and I believe the paper could be improved if some of the questions are addressed:\n\n* The current experiment could be an interesting demonstrative part to show how the algorithm works. However, there are no robust empirical experiments that the proposed method could achieve comparable performance/accumulated return as the policy ensemble (PE). I think some experiments on popular benchmarks like Mujoco simulation environment, robotics learning tasks, and Atari games are needed to make the point. The paper will become more convincing if the argument is proved on those benchmarks. Also, more experts should be explored (n > 2) in the experiments. \n\n* It is better to have some mathematical/theoretical analysis of the learning behavior of APE. For instance, is there a theoretical guarantee that APE could achieve comparable performance as PE?\n\n* The paper should discuss more details/analysis of the algorithm, like the choice of $\\alpha$ and $\\beta$, etc., which I think will affect the algorithm a lot.  \n\n* Some related literature on privacy in machine learning could be discussed in the related work section. \n\n\n====Minor that leads to confusion:\n-No mention about J and M before Alg 1; It is assumed to be the objective function and environment \n-No mention of the hyper-parameter $\\alpha$ after equation 2. \n\n"}