{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Learning with noise labels is a hot topic now due to the reason that deep learning algorithms often require large-scale supervised training samples and labelling a large amount of data is costly. However, almost all of the existing methods assume that the label noise is instance-independent. It either depends on the clean classes or is completely random. This paper studies the instance-dependent label noise, which is more realistic and applicable, but difficulty to address. The authors target to solve this problem. A feasible solution would contribute to the community a lot.\n\nThe challenging part for dealing with instance-dependent label noise is to learn the instance-dependent label flip function, which is hard to learn by only exploiting noisy data without any assumption. Few existing papers proposed assumptions to make the flip function learnable. This paper also introduces an assumption that the confidence score for each data is given. To me, this is also a strong assumption. Although the authors have provided examples of how to collect confidence score, collecting confidence scores may not be easy for many specific problems. \n\nGiven the confidence score, the authors proposed to learn the flip function. The clean class posterior can be inferred by using the noisy class posterior and the label flip function. So, how to learn the instance-dependent flip function is an essential step. Some technical contribution has been made to learn the flip function. In this section, the authors further assume that the off-diagonal entries of the flip matrix are independent of instance. After seeing the explanation, I personally agree that the assumption is reasonable for some cases. However, I found that in the experiments, the authors synthesized label noise where the off-diagonal entries depend on the instance. The proposed method also shows its advantages on this case. Can you explain this?\n\nThe experiment parts show the effectiveness of the proposed method both on synthetic data and real-world data. Overall, the paper is well-motivated and well-written. \n\nI have another concern that the obtained confidence score may not accurate. Is the proposed method sensitive to the confidence scores?\n"}