{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "--- Overall ---\n\nI think the problem of incorporating information about labeling uncertainty (when such information is available) is an interesting and important problem; however, I think this paper contains a crucial misunderstanding of the definition of calibration (as defined in Guo et al. 2017), is missing some important parts of the literature, and does not provide adequate convergence guarantees for the proposed method.\n\n--- Major comments ---\n\n1. In section 3.1, the authors substitute the confidence score r_x for the conditional probability P(Y=y|\\bar{Y}=y,X=x); however, even if the confidence scores are perfectly calibrated, these quantities are not necessarily equal (or even particularly close). As defined in Guo et al. (2017), a confidence score is well calibrated if the probability that a prediction is correct given the confidence score is equal to the confidence score. Using their notation: P(Y=\\hat{Y}|\\hat{P}=p) = p. Importantly, this is not conditioned on \\hat{Y} or X. One way to see that these two quantities are not equal is to observe that there are many possible confidence score functions that satisfy the above definition whereas the conditional probability defined above is a unique function. In particular, if the confidence score function is a constant equal to the accuracy of the model, then the confidence score is well calibrated, but clearly not equal to the conditional probability above.\n\n2. The other scenario considered by the authors is when there are multiple annotators; however, there is substantial literature on learning from multiple noisy annotations which the authors do not review. I suggest the authors start with \"Modeling annotator expertise: Learning when everybody knows a bit of something\" by Yan et al. (2010)  (and the citations therein) which also considers the instance dependent noise case.\n\n3. Under what conditions does the proposed algorithm converge and to what does it converge to?\n\n4. The proposed method relies on several assumptions that are scattered throughout the description of the algorithm. I highly recommend making these assumptions clear near the beginning of the paper. Specifically, my understanding is that the main assumptions are:\n\ni. Confidence scores r_x are available for each instance and r_x \\approx P(Y=y|\\bar{Y}=y,X=x).\nii. Y _|_ X | Y\\neq\\bar{Y}, \\bar{Y} = y\niii. Anchor points are available on some portion of the data.\n\nBeyond the presentation, I find this to be a fairly strong set of assumptions, particularly the first assumption. \n\n--- Minor comments ---\n\n1. I really appreciated the synthetic example demonstrating the potential pitfalls of the small loss approach; however, I would spend a bit more time clearly explaining the small loss approach so that readers understand why it fails and how you are solving it's problems.\n\n2. Also in the synthetic example, the authors state that covariate shift leads poor accuracy, however, I think this point would be stronger if demonstrated instead of just asserted."}