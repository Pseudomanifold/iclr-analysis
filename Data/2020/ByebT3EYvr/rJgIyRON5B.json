{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary of the Paper\n====================\n\nThis paper addresses an important topic on using function approximation method to solve imperfect information games. In this paper, the author proposed a method called Single Deep CFR (SD-CFR), which is similar to Deep CFR but only needs to learn value networks. They compare SD-CFR against Deep CFR on small Leduc Hold'em and a medium-size 5-FHP. Their performance is slightly better than Deep CFR. \n\nHowever, the paper is not well written and many details in motivation, method, and experiments are too unclear to evaluate them. Theorem 2 is false as currently written. The experimental results are not convincing, e.g., the parameters and buffer size in neural networks are larger than the number of infosets. Some parts of this paper are very confusing and incorrect. I list some of them as follows. Many key issues have not been addressed. It requires *major* revisions before being resubmitted.\n\n\nThe main idea of the algorithm is very similar to Deep CFR. The major difference is that SD-CFR inferences average strategy based on a buffer of value networks rather than learning a separate network in Deep CFR. This idea is reasonable because the average strategy in CFR is the weighting average of current strategies on past iterations. \n\nOverall, I believe this idea is just a small incremental work based on Deep CFR and it's hard to say this work is novel enough to be accepted by ICLR.\n\n\n\nComments: \n========\n1. First abstraction in Sec.1, \"Counterfactual Regret Minimization (CFR) is the most successful algorithm for finding approximate Nash equilibria in imperfect information games\". \n\nCFR is only proven to converge on two-players zero-sum (or constant-sum) *perfect recall* imperfect information game(Martin Zinkevich et.al. 2017). \nIt's less rigorous to say CFR is the most successful algorithm for imperfect information games. Typically, CFR is slower than some first-order methods when solving some imperfect information games. However, the author didn't mention this kind of method.\n\n2. \"CFR\u2019s reliance on full game-tree traversals limits its scalability and generality.\" \nMCCFR and hybrid CFR continue resolving in DeepStack don't need to full game-tree traversals.\n\n3. The first sentence in Sec.1, \"In perfect information games, players usually seek to play an optimal deterministic strategy. In contrast, sound policy optimization algorithms for imperfect information games converge towards a Nash equilibrium, a distributional strategy characterized by minimizing the losses against a worst-case opponent\"\nMany reinforcement learning algorithms can learn optimal stochastic strategy profiles for perfect information games.\nDo you mean Nash equilibrium is a stochastic strategy profile? Typically, it can be a deterministic strategy in some games.\nIt could have more than one opponent in imperfect information games, such as six players Texas Hold'em in Pluribus.\n\n4. \"the scalability of such tabular CFR methods is limited since they need to visit a given state to update the policy played in it.\"\nTabular CFR methods are used in DeepStack, Libratus, and Pluribus. They are all successful applications of tabular CFR. why this causality should be true?\n\n5. \"While tabular CFR has to visit a state of the game to update its policy in it, a parameterized policy may be able to play an educated strategy in states it has never seen before.\"\nMCCFR also visits a subset of nodes in game tree and converges to strong approximate Nash Equilibrium, e.g., External sampling in Pluribus.\n\n6. \"Not aggregating into S removes the sampling- and approximation error that Bs and S introduce?\"\nUsing function approximation methods typically leads to approximation error. Do you mean Deep CFR has a sampling error? Why?\n\n7. The conclusion of Theorem 2 requires that the neural network converges to a global minimizer after every update. Finding the globally optimal parameters in a neural network is NP-hard in general, so the assumption in this theorem is false as stated. \n\n8. It's inconsistent in the description for Leduc Hold'em. In Sec.6.1, the authors use J, Q, K but in the Appendix, they use A, B, C. The figure.1 is on standard Leduc Hold'em. The standard Leduc only contains thousands of infosets. However, the buffer size is 1 million (much larger than #infosets). Collecting data over 1500 external sampling traversals will visit almost all the infosets in Leduc. So I think this work is failed to address the generalization of neural networks. Does 396MiB in Figure.1(b) refers to the capacity memory? That seems much larger than the tabular CFR. \n\n9. It's unclear how many infosets in Leduc and 5-FHP.  You have introduced Leduc in Appendix, why not introduce 5-FHP. Such a game is different from the one in Deep CFR. BTW, in FHP, there are many chance infosets, there is no need to store strategies for these nodes. All these issues are important and should be addressed. \n\n10. I notice that the author used similar parameters like Deep CFR. However, a self-consistent paper should contain such important hyperparameters and game settings. The one-on-one performance in Figure.2 is not clear enough. A better figure should contains a significant interval. It's quite confusing for the x-axis in Fig.3 like $10^{1.5}$, $10^{2.5}$.\n\n11. comparing different methods under different embedding size, sampling ratio/observation ratio in each iteration, running time of tabular against neural, neural network SGD update iterations are very necessary. However, this paper doesn't address these issues. It's difficult for me to judge whether SD-CFR is better than Deep CFR in particular settings.\n\n12. The authors only compare their methods against Deep CFR. Some related works, such as NFSP, Double Neural CFR, ED or at least tabular CFR should be added to the comparison.\n\n\n\n\nClarity:\n========\n1. \"This reduces the overall sampling- and approximation error\". What do you mean about sampling- and approximation error?\n2. $\\pi^{\\sigma^t}_{-i}(I)$ is not defined. Maybe, you can add it's defined. Also, expected utility $u^{\\sigma}_i(h)$ is important and I suggest you give a formal definition.\n3. In Section3.1, some latest CFR variants are worth mentioning, such as Lazy-CFR[1], ICFR[2], low-variance MCCFR[3].\n   In Section7, the latest function approximation for imperfect information games [4,5] should be discussed and compared.\n4. [Section 4]\"To mimic the behaviour of linear CFR, we need to ...\", in this section, it seems that you just introduce Noam Brown's work, why use we here? Do you introduce your work here? Confusing.\n5. [Section 4]Eq.3 and Eq.5 are very similar. Si(I, a) is not defined here. I suggest changing Eq.5 to the optimization function of the average strategy network S.\n6. [Section 4]\"Not aggregating into S removes the sampling- and approximation error that Bs and S introduce, respectively\". Very confusing. Do you mean the external sampling and reservoir sampling in Deep CFR will lead to sampling and approximation error? Why is this true?\n7. Sec.4, the regret r in $R^{T}_{i,linear}$ refers to the regret in external sampling MCCFR, it typically has a different definition with the one in Sec.3.\n\n\nTypos and minor errors:\n========\nThere are many confusing descriptions or typos. I list some of them as follows. I would suggest having the paper proofread and revised to correct these errors.\n\n1. \"improve the strategy played in each state.\"  -> use infoset is better.\n2. \"we need to weight the training losses between the predictions D makes and the sampled regret vectors in B with the iteration-number on which a given datapoint was added to the buffer.\" very confusing, could you give a more detailed explanation? BTW, the word \"makes\" here is a typo?\n3. \"In tabular methods, the gain of not needing to keep sigma in memory during training would come at the cost of storing t equally large tables (though potentially on disk) during training and during play.\" very confusing explanation, such as \"the gain of not needing\", \"would come at the cost of ...\", \"during training and during play\"\n4. \"Like in the tabular case, we do need to keep all iteration strategies, but this is much cheaper with Deep CFR as strategies are compressed within small neural networks.\" -> \"Like the tabular case\", \"chiper than Deep CFR because strategies [in whose method?] are ...\"\n5. The definition of I' in I in Eq.6 is confusing, such notation typically means I' is a member of I.\n6. \"While tabular CFR has to visit a state of the game to update its policy in it\" -> \"and update\"\n7. \"player t mod 2 updating his on iteration t\" -> updating his [what]\n8. \"most successful algorithms of the recent past\"\n9. $D^t$ above Eq.4 is $D^t_i$?\n10. \"the computational work needed to train\" -> worker?\n\nCitations\n=========\n\n[1] Lazy-CFR: fast and near optimal regret minimization for extensive games with imperfect information.\n[2] Efficient CFR for Imperfect Information Games with Instant Updates.\n[3] Low-Variance and Zero-Variance Baselines for Extensive-Form Games.\n[4] Computing Approximate Equilibria in Sequential Adversarial Games by Exploitability Descent.\n[5] Double Neural Counterfactual Regret Minimization.\n"}