{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper builds upon recently introduced Deep CFR, a method to solve large imperfect-information games. Deep CFR, which combines counterfactual regret minimization and deep learning, learns value networks and a strategy network. The paper suggests a modification to avoid learning a strategy network, proving that it is theoretically more attractive and showing the beneficial performance in experiments.\n\nCFR and its variants are state-of-art methods for large extensive form imperfect information games. Scalability of CFR is an actual problem, widely discussing in literature. The suggested modification to Deep CFR is a significant contribution. The paper proves theoretically that the modification indeed produces an average strategy. Also, it is proved that Deep CFR is not guaranteed to provide an average strategy because of the finite sample size and approximation error. Experiments show that SD-CFR produces less exploitable strategies than Deep CFR in Leduc Poker and wins one-on-one matches in 5-Flop Hold\u2019em Poker. Experiment in 5-FHF also shows significant decrease in the amount of necessary storage space, because SD-CFR stores all trained value networks, while Deep CFR needs additional space for strategy network dataset.\n\nThat being said, I follow up with some questions/criticism.\n1.\tI would like to see some discussion about scalability of the method. Can it be applied, for example, to HUNL? To 6-players holdem? If not, is it compatible with abstraction-based methods or with depth-limited solving?\n2.\tWhy were one-hot encodings of cards instead of embeddings chosen in experiment 6.1? How does it influence the performance of Deep CFR? In experiment 6.2 the paper reports \u201cThe neural architecture is as Brown et al. (2018a).\u201d Does it mean, that embeddings were used? \n3.\tBoth in the paper and in Deep CFR nns were trained from scratch each iteration. Could it be beneficial not to start from scratch? For example, pretrain good card embeddings on a variety of similar tasks in advance (using a lot of data and time, as it is offline computation), and then use these embeddings without changes during SD-CFR (or Deep CFR) training? \n\nMinor comments:\n1.\tIntroduction second paragraph: \u201cTo address these two problems, researchers started to augment CFR with neural network function approximation\u201d. Augmenting like in Deep CFR is one alternative to abstraction schemes for large games. Another alternative is depth-limited solving + resolving [1], [2] and DeepStack actually represents this branch of methods.\n2.\t\u201cNow, the iteration-strategy for player i can be derived by \u2026\u201d. It is probably better to say that choosing such strategy minimizes the overall regret (see regret matching).\n3.\tSection 5.1. Instead \u201cwe satisfy the linear averaging constraint of equation (5)\u201d, which I found a bit confusing, it is better to say that probability of sampling a particular action is the same as sampling the action from the average strategy (5).\n4.\tTo reduce variance in one-on-one test, AIVAT technique can be applied [3].\n\n[1] Noam Brown, Tuomas Sandholm, and Brandon Amos. Depth-limited solving for imperfect information games. arXiv preprint arXiv:1805.08195, 2018. \n[2] Matej Morav\u02c7c\u00b4\u0131k, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017.\n[3] Burch, Neil, Martin Schmid, Matej Moravcik, Dustin Morill, and Michael Bowling. \"Aivat: A new variance reduction technique for agent evaluation in imperfect information games.\" In Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"}