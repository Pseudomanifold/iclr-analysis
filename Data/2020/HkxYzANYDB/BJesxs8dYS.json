{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "A new benchmark is presented, which requires to reason on spatio-temporal data (videos) and incorporates physics (videos of physical dynamics), language (questions are formulated through language) and causality. The benchmark builds on the well-known CLEVR benchmark and adds several interesting contributions, in particular reasoning over time.\n\nCompared to other benchmarks proposed on reasoning on physical dynamics, this benchmark adds to the language component (which is presented in classical VQA benchmarks), and an interesting counterfactual component known from the causal inference literature. This is fairly new for the computer vision and statistical machine learning literature (a few papers on counterfactual reasoning exist), but in contrast to the causal inference literature, here the do-operator is observable during training: we do have access to do-interventions and even the outcome during training and therefore we can even learn counterfactuals using supervised learning. This statement is not meant as criticism, as learning counterfactuals without supervision from high-dimensional input like images seems to be currently out of a reach, or at least has not yet been demonstrated up to my knowledge.\n\nOn the downside, compared to other physical reasoning benchmarks, the answers here are multiple choice instead of regressions of the future motion, which requires more fine-grained reasoning. This will guide solutions to a certain type and will also favor solutions, requiring the network to detect certain binary concepts and combine them instead of regressing complex functions. It also favors solutions of the type presented in the paper.\n\nAs for other benchmarks, the dataset is quite large (20 000 videos) and, given its synthetic nature, is accompanied by functional programs. As for CLEVR, the simulations have been performed by a physics engine and then separately rendered with Blender to maximize visual quality. The result is a very interesting benchmark, and I have no doubt that it will be very useful for us (the learning reasoning community).\n\nAnother positive point is the number of baselines tested on the benchmark, among which we can find strong papers on VQA/VQA2\n\nI have a couple of questions:\n\nHow is the causal graph created? Are the experiments rendered and outcomes examined, creating the causal graph for the counterfactual answers, or are the experiments selected with a given outcome already decided?\n\nThe distribution of question types has been provided, but how about the biases? Distributions of the answers would have been helpful. How did the authors avoid biases during construction of the dataset, in particular in the counterfactual case (see remarks on the causal graph).\n\nThe paper also comes with a method for solving, which is very similar to existing methods on learning through functional programs. The method itself is unfortunately described only very briefly and the reader needs to look it up almost entirely in the appendix of the paper, which is surprising, as the paper is only 8 pages long instead of 10.\n\nAs for CLEVR, one of the downsides of this type of benchmark is the synthetic nature of the images and the limited range of different objects in the scene. Of course this comes with the advantage of being able to study compositional reasoning in detail, as a scene graph can be calculated easily (and is available during simulation). However, it also makes reasoning through functional programs much easier, as the proposed filters are limited in number and can strong respond to the small number of shapes and colors available in the data. I have strong doubts that this kind of approach extends to real life scenarios.\n\nFor VQA type of scenarios, GQA is a nice compromise between natural looking images and the availability of scene graphs and the restriction of questions to compositional reasoning. The optimal choice would be a similar compromise for spatio-temporal data, but of course this would be a huge effort and it would be up to impossible to have access to counterfactuals.\n\nLast point, and this question is not restricted to this paper, as the name came up elsewhere, why is the model called neuro-symbolic reasoning? While it could be argued that the questions require a sort of \u201csymbolic reasoning\u201d, I am not sure that the reasoning method itself is symbolic even partially. Other than selecting functional programs out of a discrete set, the reasoning itself is connectionist and performed with graph networks.\n"}