{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents UniLoss, a general framework for training a neural network model with respect to a non-differentiable evaluation metric as an objective function. The key idea of this paper is to decompose an evaluation metric into a sequence of functions (f, h, g), where: f maps scores computed by a neural network model to comparisons with the ground truth; h maps the comparisons to binary variables; and g computes the performance value from the binary variables. Even with the decomposition, the evaluation metric is still indifferentiable. In order to make the evaluation metric differentiable, this study approximates h with the sigmoid function and g with an interpolation method (inverse distance weighting). The experimental results show that the loss functions obtained by UniLoss can achieve roughly comparable performances to the conventional loss functions on MNIST (binary classification, average precision, cross-entropy loss), CIFAR-10/100 (multi-class classification, accuracy, cross-entropy loss), and MPII (pose estimation, PCKh, MSE with the manually-designed \"ground truth\" heatmaps).\n\nThis paper is well structured and easy to read. The idea of decomposing an evaluation metric into functions and considering the approximated versions the functions is interesting. It is great to see that the empirical results demonstrated that the proposed method could achieve the same level of performances with the conventional losses. The impact of this paper would be greater if the proposed method could discover a better approximation of an evaluation metric and achieve a better result.\n\nAlthough the procedure for obtaining a loss function looks systematic, I am wondering of the generality of this framework. For example, I'm not sure whether this method can be applied to other evaluation metrics such as F1 and BLEU scores. Even if this method can support these metrics, I would like to know how much effort we need to craft functions f, h, and g. A discussion about the applicability and limitation of the proposed framework would be useful.\n\nI'm also wondering the additional computational cost introduced by a surrogate loss. The binary classification on the MNIST, for example, requires $O(n^2)$ computations for binary variables whereas the original evaluation metric and cross-entropy loss require only $O(n)$.  For this reason, I'd like to read a discussion and empirical results about the computational cost.\n\nMinor comments:\n\nSection 1:\n\"For example, cross-entropy is a popular loss function for training a multi-class classifier, but when it comes to comparing models on a test set, classification error is used instead.\"\n\"For example, cross-entropy or hinge loss for classification, both of which have proven to work well in practice.\"\nThis discussion is a bit confusing because the former sentence claims that the cross-entropy loss is inappropriate, and the latter suddenly explains that the cross-entropy loss works well in practice.\n\nSection 1: \"Non-differentiability occurs when the output of the task is discrete (e.g. class labels)\"\nThis sentence is a bit misleading because the cross-entropy loss is differentiable and used for multi-class classification (even though labels are discrete).\n\nSection 3.1: \"Our approach seeks to automatically find a surrogate loss to minimize e\"\nWhen we call $e$ as 'performance', we want to 'maximize' (maximize the performance) instead of 'minimize' (because we do not want to minimize the performance). We may call $e$ as 'error' to be consistent."}