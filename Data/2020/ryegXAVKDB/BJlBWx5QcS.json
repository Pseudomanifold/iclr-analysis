{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThe paper proposes a method called UniLoss, which decomposes a task-specific loss into 4 components and essentially performs continuous relaxations on some discrete components. The paper reports some results on image classification and pose estimation.\n\nWriting\nThe content presented in this paper is well written; but as an ICLR paper, many technical details about the methods seem missing, and prevents me from having a holistic picture of the entire work. And the results presented in this paper do not suffice to show the effectiveness of the proposed method. See below.\n\n\nMotivation\n\nI am wondering, besides multi-class classification (which seems to have been abused hence less interested), what other loss metrics and problems have their loss function following your decomposition. It would be good to see an enumeration of such problems/loss metrics to show how general the proposed decomposition is and how many problems could benefit from your formulation (besides the very well-defined multi-class classification loss.)\n\nI do acknowledge there are some good innovations in this paper, but I think the introduction has some overclaims, as there exists several works (w/ code) published recently that consider similar problems. See below.\n\nAutoLoss: Learning Discrete Schedules for Alternate Optimization. Xu et al.\nAddressing the Loss-Metric Mismatch with Adaptive Loss Alignment. Huang et al.\n\n\nResults\n\nThis paper lacks convincing results to verify its claims. See detailed reasons below:\n- Mnist, and correspondingly the binary classification over MNIST, are too artificial a dataset and a task to show the effectiveness of your proposed method. I would expect to see more REAL experiments on more challenging tasks. Even for image classification, it is better to run the experiment on larger-scale data. \n- In terms of the results of MNIST, it seems UniLoss is not able to outperform cross-entropy loss. In terms of results on CIFAR, the improvement is very marginal.\n- Another major concern of mine is how much manual human effort and extra training are needed to deploy a training task using UniLoss. From the MNIST results I gained a sense that UniLoss involves a lot more design effort but still is not able to even perform better than a straightforward cross-entropy (even on this very artificial and abused dataset MNIST).\n- The paper claims for many times that UniLoss bypasses \u201ceffort on manual design\u201d, but does not have a quantitative metric on how much effort it can save or how much other costs it incurs. My understanding is that UniLoss itself needs to be designed based on the task of interest, and seems to introduce some cost of training or extra implementation.\n- For pose estimation, why don\u2019t perform experiments to report the results on the complete set of human joints, but choose head-only, even if you claim the method could be applied w/o any modification?\n"}