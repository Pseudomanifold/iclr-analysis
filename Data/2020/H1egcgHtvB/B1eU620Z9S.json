{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm to translate natural language questions into SQL queries. First, a question, columns, tables are encoded using bi-LSTMs. Then, relation-aware self-attention with a stack of layers is used. Scheme encoding and scheme linking is conducted to model the column/table references. Then, decoder by LSTM is used to output a sequence of actions. Experiments are executed using Spider dataset. The proposed algorithm is compared to several existing algorithms including ones with BERT, and shown to outperform ones without BERT.\nMy simple question is why the authors do not use BERT? If it brings much gain on performance, just try RAT-SQL+BERT, which will clarify the contribution of the paper more apparently. Otherwise, there should be some merit without BERT. Second question is that by looking at Table 2(c), a large portion of performance gain is brought by scheme linking. However, scheme linking part described in Sec. 3.6 seems somewhat heuristic. If this is the important part, more elaborate analysis is needed (including possibility to apply this part to other existing methods.)\nOverall, although I think this research is important, it can not be accepted as a strong ICLR paper yet."}