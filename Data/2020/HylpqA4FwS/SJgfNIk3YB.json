{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors present a novel work to address the problem of signal propagation in the recurrent neural networks. The idea is to build a attractor system for the signal transition from state h_{k-1} to h_k. If the attractor system converges to a equilibrium, then the hidden to hidden gradient is an identity matrix. This idea is elegant. The authors verify the performance of Increment RNN on long-term-dependency tasks and non-long-term-dependency tasks.\n\nThe work successfully constructs a negative identity hidden to hidden gradient matrix but I still have a few concerns about the theory and the experiments. If the authors can address my concerns in the rebuttal, I am willing to increase my score. \n\nTheoretical concerns:\nEven in the limit sense, the inner ordinary differential equation of g will converge to the equilibrium, it may not converge in the finite steps. Thus, the state-to-state gradient may be slightly away from the identity. And we know that (0.99)^T goes to infinity when T goes to infinity. In practice, the long-term-gradient problem may still exist in the incremental RNN.\n\nExperimental concerns:\nThe theorem requires the norm of U to be bounded. But I cannot see how the authors bound the norm of U in the experiment section.\n\nClarity of writing:\nThere are a bunch of typos in the papers, especially the ones in the proof of Theorem 1. The proof of theorem 1 can be polished. The author swap the use of phi and psi several times in the proof. And gradient calculation on equation (5) should be expanded into more details. The authors also missed one U in the equation in the second last line of the proof. \n\nOverall, I think the paper is an interesting contribution to the community.\n"}