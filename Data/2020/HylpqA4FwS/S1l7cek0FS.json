{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper on recurrent neural networks goes back to Rosenblatt's continuous time dynamics model and uses a discretised version of that equation (equation (1) in the paper) to build an incremental version of the RNN, called incremental RNN, where the transition from hidden state h_{k-1} at step k-1 to next step's h_k is done using small incremental updates until the system achieves equilibrium. It claims that it manages to solve the vanishing gradient problem by keeping all gradients \\frac{\\partial h_k}{\\partial h_{k-1}} equal to minus identity matrix. The algorithm is then extensively evaluated on a large number of tasks and compared with plain RNNs, LSTMs, and two recently published papers on antisymmetric RNN and FastRNN.\n\nI need to admit that after reading the paper twice, I am not sure I understand how the method works exactly (how does inserting intermediary steps and variables g_0, g_1, ... g_T enable the system to reach equilibrium: is there an iterative evaluation until convergence?) and more worringly, how the single-step SiRNN differs from a normal RNN with an extra residual connection?\n\nAccording to equation (5), for T=1 and g_0=0, we have:\ng_T = g_1 = \\eta_k^1 ( \\phi (U (h_{k-1}) + W x_k + b) - \\alpha h_{k-1} ).\nIf the gradients are vanishing in the normal RNN, why would they not vanish here for T=1?\n\nPropositions 1 and 2 are for the case K=\\infinity, and I could not understand the proof of theorem 1 that shows why \\frac{\\partial h_k}{\\partial h_{k-1}} = - I. This seems to be the major contribution of the paper and should be given prominence.\n\nWhat is missing is clear explanation, like (Bengio et al, 1994), of the identity gradient and of how the algorithm works. These questions could be solved by including code or pseudo-code explaining how to actually implement incremental RNNs.\n\nThere are also several important papers recently published that have approached the problems of continous-time dynamics and relaxation of hidden state to equilibria.\n* The paper does not mention at all Neural ODEs [2] [3] where the state flows in a continuously differentiable way thanks to the continuous-time residual network ODE formulation. Moreover, isn't the idea of inserting a relaxation to equilibrium using ODEs already implemented in the ODE-RNNs [3]?\n* How do incremental updates related to Adaptive Computation Time [4]?\n\nFor this reason, I am currently tending to reject the paper, but am open to change my score upon clarifications and links to other similar work.\n\nAdditional remarks:\nThe first paragraph of the paper explains the Elman RNN, not RNNs in general.\nPlease cite [1] alongside Bengio et al (1994) for the problem of the vanishing gradient.\nDefine alpha in equation (1)\nNotation k and K is very confusing\nBlue vs. green on Figure 2 is hard to read, and where is the new initialisation?\nWhy do you add h_k^K to h_{k-1} in equation (8)? I thought it was g_k^K?\nKeep the same colours for all experiments in figure 4.\n\n[1] Hochreiter (1991) \"Untersuchungen zu dynamischen neuronalen Netzen\"\n[2] Chen, Rubanova, Bettencourt & Duvenaud (2018) \"Neural Ordinary Differential Equations\"\n[3] Rubanova, Chen & Duvenaud (2019) \"Latent odes for irregularly-sampled time series\"\n[4] Graves (2016) \"Adaptive computation time for recurrent neural networks\"\n"}