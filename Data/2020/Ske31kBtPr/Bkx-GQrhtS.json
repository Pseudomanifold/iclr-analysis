{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a method to do math reasoning purely using formula embeddings. The proposed method employs a graph neural network to embed math formulas to a latent space. The formula embeddings are then combined with theorem embeddings (also formulas, computed in the same way as formula embeddings) to predict whether one can do one step of math reasoning using the corresponding theorem, and also to predict the embeddings of the resulting formula. Empirically the authors demonstrate that the method can be chained end-to-end to do multiple steps of reasoning purely in the latent space.\n\nI tend to accept this paper, (but also OK if it gets rejected), for the following reasons: (1) the idea is novel and interesting; (2) the writing of the paper is below conference standard and very hard to read, especially the method and the experiment sections.\n\n===========================================================================\n\nNovelty and significance\n\nI really like the idea of doing math reasoning in latent space. The idea is definitely novel and interesting. It is related to existing works such as neural logic induction[1] and planning in latent space[2]. It is amazing that one can do multiple steps of math reasoning after only training the model using data from one single step. It would be interesting to see how it can improve existing learning-based theorem provers.\n\nMy question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space? Also can it work with theorems that decompose the current goal into several sub-goals? I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers!\n\n===========================================================================\n\nWriting\n\nThe paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.\n\n1. Typo: Third paragraph in section 1, \"...which is makes use of ...\".\n2. It's very confusing when the authors introduce \\sigma and \\omega in the beginning of section 4: why would you need two networks predict the same thing?\n3. Mentioning \"merging \\sigma and \\omega, is left for future work\" is confusing before formally introducing \\sigma and \\omega.\n4. Even when the authors formally introduce \\sigma and \\omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.\n5. In fact, I don't know why \\omega needs to output p. It's never mentioned in the experiment section.\n6. The rationale of the two tower design (why not combine two) is not clearly explained.\n7. Typo: Page 5 last paragraph, \"... negative instances for for each ...\".\n8. The itemized part in 5.3, \"...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx\". However, both 3 and 4 are not baselines!\n9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.\n10. Reading the baselines before the experiments is very confusing. For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a \"random baseline\".\n11. Baseline 2 is actually referred to as \"usage baseline\" but this name is not introduced in the itemized part.\n\n\n\n[1] Rockt\u00e4schel, Tim, and Sebastian Riedel. \"End-to-end differentiable proving.\" Advances in Neural Information Processing Systems. 2017.\n[2] Srinivas, Aravind, et al. \"Universal planning networks.\" arXiv preprint arXiv:1804.00645 (2018).\n\n\n"}