{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "= Summary\nEmbeddings of mathematical theorems and rewrite rules are presented. An in-depth analysis of the resulting embeddings is presented, showing that a network can learn to \"apply\" embedded rewrite rules to embedded theorems, yielding results that are similar to the embedding of the rewritten theorem. [i.e., app'(emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result.\n\n= Strong/Weak Points\n+ Simply a cute result, showing that proof search can remain in embedding space for a limited time horizon without having to switch back into the theorem prover environment.\n+ Nicely designed experiments testing this (somewhat surprising) property empirically\n- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail\n- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)\n- Architecture choice unclear: Why are $\\sigma$ and $\\omega$ separate networks. This is discussed on p4, but it's unclear to me how keeping $\\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?\n\n= Recommendation\nOverall, this is a nice, somewhat surprising result. The writing and experiments could use some improvement, but I believe that the majority of the ICLR audience would enjoy seeing this result (even though it would have no impact on most people's research)\n\n= Detailed Comments\n- page 4, Sect. 4.4: Architecture of $\\alpha$ would be nice (more than a linear layer?)\n- page 5, paragraph 3: \"we from some\" -> \"we start from some\"\n- p6par1: \"much cheaper then computing\" -> than\n- p6par6: \"on formulas that with\" -> no that\n- p6par7: \"measure how rate\" -> \"measure the rate\"\n- p8par1: \"approximate embedding $\\alpha(e(\\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well. However, I don't understand the use of $\\alpha$ here. If Fig. 4 is following Fig. 3 in considering $p(c(\\gamma(T), \\pi(P)))$, then Fig. 4 should plot the performance of, e.g., $p(c(e'(c'(\\gamma'(T_{i-1}), \\pi'(P_{i-1}))), \\pi(P_i)))$ (i.e., $p$ applied to approximate embedding of $T_i$ and (\"true\") embedding of $P_i$). I believe that's what \"Pred (One Step)\" expresses, but it would maybe be generally helpful to be more precise about the notation in Sect. 6."}