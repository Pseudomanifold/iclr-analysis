{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a fine-tune-based few-shot classification baseline, which has been validated effectively on several datasets, including Mini-Imagenet, Tiered-Imagenet, CIFAR-FS, FC-100, and Imagenet-21k. In addition to the method, the authors also provide concrete experimental setting and new evaluation proposals.\n\n1. The authors propose to use the logits instead of embedding as the main bridge between the pre-trained model and the meta-learning model. Does it mean we represent novel classes based on the properties of the meta-train classes? If so, does this method requires more meta-train classes to enrich the representation ability? How will the method perform when working on few-shot learning problems with a large distribution shift?\n\n2. To make a fair comparison: instead of citing the values in the published papers directly and comparing different methods with different architectures, the authors should also apply the pre-trained model with the famous baselines, such as Matching Network, Prototypical Network, and MAML. Now there exists a very lap gap between the Matching Network values and the newly proposed one. For example, fine-tune the Matching Network on the pre-trained backbone in both train and train+val settings. Therefore, it is more clear to show the improvement of the proposed baseline models. Q/A 2-3 in appendix D do not fully solve this problem.\n\n3. Considering the randomness of the sampled few-shot tasks, the authors can consider evaluating over more episodes (e.g., 10,000 trials) than 1000 in the paper.\n\n4. It's better for the authors to emphasize and differentiate the transductive fine-tune and the inductive counterpart in the paper. "}