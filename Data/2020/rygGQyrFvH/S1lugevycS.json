{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In the domain of language models, the paper introduces a new heuristic sampling method called top-p sampling, or nucleus sampling (NS). It is a variant of top-k sampling where the smallest k is selected to ensure the combined likelihood is no less than p. The paper centers on claiming and showing that the generated samples are of higher quality and more diverse than common alternatives such as beam search, pure sampling, top-k sampling, and low-temperature sampling.\n\nWhile overall I think the proposed method is sound as an alternative to other heuristics such as beam search, I have reservations on the presentation and arguments made in the paper. \n\nPros:\n1. NS is sound as a heuristic sampling method.\n2. The paper contains many interesting experimental observations and I speculate that some of them will find future uses. For example, the selection of parameter values (not just for NS, also for top-k) and the nontrivial perplexity of generated text.\n\nCons:\n1. The ultimate performance measure (open-ended generation) of \u201chigh quality\u201d and \u201cdiversity\u201d is very vague. It seems that the authors end up doing is to evaluate by high self-BLEU, HUSE, few repetitions, and perplexity. Furthermore, it is unclear why one _should_ train with cross-entropy (trying to match the distributions) and then rely on the sampling procedure to fulfill these desiderata (See also Min1 and Min2).\n2. The comparison with beam search (BS) is not well motivated. BS is devised to find the maximal sentence and it is not stochastic. It seems out of place in the context of generating a \u201cdiverse\u201d set of samples.\n3. The arguments in the comparison with pure sampling is vague and sometimes misplaced. The key argument seems to hinge on the idea that the low likelihood tail is of \u201clow confidence.\u201d But this claim is problematic. If the estimate is wrong on the low probability tail, then so is the estimate on p(head) = 1-p(tail) by virtue of p being a probability measure. \n4. The arguments in the comparison with top-k is vague and sometimes misplaced. The main argument against top-k is the \u201c[d]ifficulty in choosing a suitable value of k\u201d but the same can be said for choosing p. After all, top-k and top-p (NS) can be thought of as a variant of each other (by dynamically choosing k or p respectively). Moreover, in Figure 5, a selection for k value is suggested. I agree that this value might not _appear_ as intuitive as p, and maybe other works have chosen a smaller k than they should have, but similarly, people might intuitively choose too high a value for p (Figure 5). \n\nPossible mistakes/typos:\n1. (2), \u201c>=\u201c -> \u2265.\n2. Figure 7, the human self-BLEU4 < human self-BLEU5 and that seems wrong, especially when all other bars show the opposite ordering.\n3. In References, \u201cAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In ACL, 2018a\u201d is duplicated.\n4. In References, the citation of \u201cUnifying human and statistical evaluation for natural language generation\u201d is from NAACL 2019, not \u201c2018.\u201d\n5. In References, the first names are shown as initials in \u201cSparse forward-backward using minimum divergence beams for fast training of conditional random fields.\u201d\n\nQuestions:\n1. In Table 1, how is Human perplexity estimated?\n\nMinor issues:\n1. Partly due to what the authors position NS to solve, i.e. open-ended generation, the core arguments is not as precise or rigorous as it could have been in my opinion. I feel that focusing on comparing NS to other heuristics as a heuristic might make the text appeal to a wider audience and the discussion more precise.  \n2. The distinction drawn between open-ended generation and directed generation is unpersuasive to me. In the context of language modeling, the former is to approximate a distribution (over an extended alphabet) whereas the latter is to approximate a conditional distribution (given the input). However, the most common formulation to solve the former is to decompose the distribution into a product of conditional distributions (1).\n3. The caption in Figure 1 draws a misleading comparison. The \u201cadmirable\u201d generation (presumably referring to the OpenAI blog post) was from the full GPT-2 model, not the initially released GPT-2-117M.\n\nPlease point out my misunderstanding directly. I am open to acknowledging them and revising my assessment."}