{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces two properties of adversarial training observed from abundant empirical results. Based on the discoveries, the authors propose plausible explanations as well as new methods to gain higher adversarial robustness. The two properties are as follows\n\n1. The batch normalization may negatively affect the adversarial robustness, where a training batch consists of a mixture of clean and adversarial examples. The authors observe that the parameters for batch normalization (BN) may be quite different between batches of clean and of adversarial examples, and conjecture the reason as that these two sets of examples are from two different domains. Therefore, the authors propose a few methods to boost the adversarial robustness: using different BN stats for clean and adversarial examples (but keeping the other parameters shared, which may seem useful only for fundamental study due to the requirement of clean/adversarial label for each example), using batch-unrelated normalization, and changing stat estimation for BN to reduce the difference between training and inference steps.\n\n2. The deep networks for general image classification may be too shallow for adversarial training. Since the mixture of clean and adversarial examples may form a two domain distribution that is challenging to model, typical neural networks (even for ResNet-152 that has high depth for general image classification) may have too low capacity. The authors show that increasing the depth of neural network (up to and possibly beyond ResNet-638) results in even higher accuracy.\n\nTo my best understanding, the results and analysis of this paper are valid, and the proposed methods have shown gains in abundant experiments. Due to the significance of adversarial training and the new discoveries of this paper, I think the contributions are sufficient and would lean towards the paper being published / weak accept. However, below are my questions and concerns that I would like the authors to address.\n\n1. For this paper, the adversarial examples are generated from a particular class of attacker, namely Projected Gradient Descent. I am curious how the conclusion could generalize and help the robustness if we have more than one adversarial attackers.\n\n2. The paper focuses on adversarial robustness and seems to deprioritize clean image accuracy, which could seem to limit the scope for application purposes. Frankly, I agree that a fundamental understanding of adversarial robustness would be significant, and the authors discuss the problem of relatively lower clean image accuracy in Section 4.4. However, I might feel that clean image accuracy should be as significant for practical purposes, and it would be great if we can balance the trade-off between clean image accuracy and the adversarial robustness. The authors should feel free to correct me if I do not understand correctly.\n\n3. There seems to be an interesting observation in Fig 1 for which I am curious. The accuracy for PGD-2000 in Fig 1 does not go monotonically with the ratio of clean images \u2014 the lowest accuracy is at 60 percent of clean images, which seems not to fully align with the argument that removing clean images will help robustness. Personally it would be great if the authors could share their insights of possible reasons.\n\n4. For the second discovery that deeper networks help adversarial robustness, the red line (for adversarial robustness) in Fig 7 seems not converged yet at ResNet-638. If the computation resources allow, I am curious on the depth of ResNet at which the red line becomes flat, and this could be useful for headroom analysis on how good accuracy we can reach.\n\n5. For the second discovery that deeper networks help adversarial robustness (Section 5), it seems Madry et al 2018 (Towards deep learning models resistant to adversarial attacks) also discusses model capacity vs the adversarial robustness. The mentioned paper does not seem to use deeper structure but uses other ways to increase capacity. The mentioned paper has been referred to in other sections of this work, however, it may be good to contrast in Section 5 on the conceptual novelty in this paper.\n\n6. Typos: The title of Table 1, \u201cMBN_{clean}/MBN_{clean}\u201d would be \u201cMBN_{clean}/MBN_{adv}\u201d. There are some \u201c, ,\u201d (double commas) in the appendices. Comma after \u2018(\u2019 in appendix B3. Overall this paper is well written and easy to follow."}