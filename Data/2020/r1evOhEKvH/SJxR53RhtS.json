{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to leverage the between-node-path information into the inference of conventional graph neural network methods. Specifically, the proposed method treats the nodes in training set as a reference corpus and, when infering the label of a specific node, makes this node \"attend\" to the reference corpus, where the \"attention\" weights are calculated based on the node representations and the between-node paths. (The paper used different terms about the \"attention\".)\n\nIn general, this paper shares similar insights like those in memory networks, which use a reference corpus to assist the inference on an individual sample. The major technical novelty of this paper seems to lie in the introduction of this idea on graphs and the use of between-node-path information for the \"attention mechanism\".\n\nThe paper is also well-organized and relatively easy to follow. \n\nOne of my major concerns is the unfair experimental comparison. The proposed method uses both the training set and validation set to train the model. And it's well-known that the offical split of the Cora, Citeseer, and Pubmed datasets has a validation set larger than the training set. As most of the baseline performances in Table 1 are obtained by using the training set only, the comparison of Table 1 is unfair and totally meaningless. A valid comparison is Table 2, where the baseline GCN is trained on both training set and validation set. But only the results on Cora is given and results on all other datasets are missing. Actually, I tried a quick run of GCN, without much tuning of hyper-parameters, on Citeseer with both training set and validation set as training data. And it can easily achieve a test accuracy of 0.75+, which is better than the performance of the proposed method reported in the paper. \n\nAnother concern is that, while the proposed method is intuitively sound, it is unclear how this method compares to naively increasing the number of layers of the GNN. Increasing the number of layers of the GNN can also eventually make the test node able to interact with all the training nodes, and the model should be able to implicitly distinguish the distances between nodes. Why incorporating between-node paths, and particularly in this way, helps?\n\nSome minor points:\n\t- Could you clarify how the figure 2 is generated? By grouping the test nodes with differen between-nodes steps? It is confusing why performance on the nodes far away from the training nodes is better?\n\t- Why figure 3 is on PubMed while all other ablation analyses are done on Cora?\n\nIn summary, this paper shows some technical novelty but the motivation behind the proposed method is not strong enough. The experiment design is questionable and especially the main experiment comparison Table 1 is meaningless. Therefore I think this work is not ready for publish yet."}