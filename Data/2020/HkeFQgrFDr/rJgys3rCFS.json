{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a unifying algorithm for multi-modal multi-domain image translation with either better performance or more functionality than its predecessors. It uses a GMM distribution to model the variations both within the same domain and cross-domain differences and an encoder to extract context information invariant across domains. The setup is able to degenerate into many of its predecessors (e.g. StarGAN, pix2pix, etc.) if we choose specific sets of hyperparameters. \n\nOverall, the paper did a good survey over the literature and comparisons across multiple data sets and baselines. There are a few caveats that made me hesitant to accept this paper. \na) There is a plethora of literature in image translation in recent years that works on improving image quality and diversity, avoiding model collapse, expanding to multi-domain translation, etc. Although it is worthwhile combining all those works into an overarching framework, I expect the sum to be more than the individuals all bundled together, which is unclear from this paper (see numbers in Table 2). MUNIT is a great example where the idea of AdaIN is not new, but its learned version applied to image translation yields great results and lots of future directions to explore.\nb) It would be better to have a clearer future direction of improvement for this paper. The authors mentioned this in the last sentence, but it would be great to have something to inspire future researchers to tackle the ill-defined unsupervised image-to-image translation problem. \n\nQuestions that are left unanswered:\n1. If multi-modal output is desirable, is there a tradeoff or could we get diversity for free?\n2. From eq.15, such a multi-term loss function may not scale to higher resolution output where model collapse occurs more often, or it could become extremely expensive to train, which hinders the speed to iterate through ideas. Has GMM-UNIT been trained at 256x256 or higher? How long did it take to train at 128x128?\n3. In image translation or style transfer domain, one of the often asked questions is \"what counts as a domain/style as opposed to the content?\" In this work, the content is defined as the encodable features that stays constant after translation to different domains. The style is defined as the Z modeled by the domain dependent GMM distribution. From fig. 6, it looks like local changes or simple changes like eye makeup and the overall color of the image are considered as parts of the style or noise that can vary within a domain. On the other hand, more complicated changes like hair style, hair length, jewelry, are treated as the content. What causes the network choosing to classify a feature as style versus as content? Does that change if we train from scratch again?\n"}