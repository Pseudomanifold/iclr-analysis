{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an unsupervised method for multi-domain image-to-image translation. Incorporating GMM as a latent representation, the proposed methods enjoys multi-modal generated images. \n\nI am voting for a paper to be accepted. \n\nThe main arguments for this decision:\nThe paper addresses an actively studied (i.e. interesting to the community) problem and the proposed method appears to be novel. The simple (in a good sense) idea of incorporating different domains as components of a GMM surprisingly provide great results. \n\nThe paper well written and easy to follow. The method is thoroughly studied from different angles. It is compared with relevant state of the art methods.\n\nSome major comments though where the paper has limitations and where it can be improved:\n1. Missing relevant work that challenges the claim that \u201cwe are the first ones being able to translate images in previously unseen domains\u201d: Chen et al. \"Homomorphic Latent Space Interpolation for Unpaired Image-to-image Translation.\" CVPR 2019. Although in the referred paper there is no direct study on that, it would be better to include at least discussion on comparison of the proposed and referred methods since they both exploit continuous latent representation\n2. Some kind of ablation study or at least discussion on importance of disentanglement is missing. In general, the idea of disentanglement being one of the main features in the proposed work is not well presented in the paper. It is not clear from the paper what is the role of disentanglement in the proposed work and in the image-to-image translation literature in general. How important disentanglement representation in the proposed method? \nI believe it may greatly improve the paper if disentanglement would be considered on the level as multi-domain and multi-modal properties. For example, in the Related work section, there is a review of work on multi-domain and multi-model translation, as well as a nice summary of that review in Table 1, it would be great to have that for the disentanglement property as well. \nIn terms of the review of disentanglement in image translation, I could suggest some works currently missing in the paper: \na. Gonzalez-Garcia et al. \"Image-to-image translation for cross-domain disentanglement.\" NeurIPS 2018 for disentanglement similar to the proposed method: content-domain attributes \nb. Wu, Wayne, et al. \"Transgaga: Geometry-aware unsupervised image-to-image translation.\"\u00a0CVPR 2019 for different type of disentanglement: appearance-geometry \n3. Regarding the claim that GMM-UNIT is a generalisation of the existing methods, do the special settings in GMM-UNIT described in the last paragraph on page 3 recover the exact versions of the existing methods? It seems it may depend on the implementation details such as specific losses, or architectures. It would be good to see empirical justification that GMM-UNIT with the special settings can recover the existing methods\n4. On the note of implementation details, for reproducibility, it is important to specify implementation details of the baseline methods in addition to the details of the proposed method. Are they coming from the original papers without any modifications? If the authors use public implementations of the baselines the references should be added \n5. Missing relevant work: Liu et al. \"Few-shot unsupervised image-to-image translation.\" arXiv preprint arXiv:1905.01723 (2019). Maybe of less importance because this work seems not to be published yet, but given the emphasis of the paper on ability of the proposed method to do few-shot translation, it seems that comparison with this work would be beneficial\nI am willing to increase the score if (some of) the above is addressed in the rebuttal\n\nOther lesser comments/suggestions:\n1.     Second paragraph in Introduction. Technically speaking, the text has not provided the information that image-to-image translation methods are usually based on GANs or VAEs. It would be helpful to include this introduction to GANs and VAEs explicitly, otherwise, from the text perspective it is unclear what GANs and VAEs have to do with image-to-image translation\n2.\t\u201cthus naming the method: GMM-UNIT\u201d \u2013 it explains only the GMM part of the name but not the UNIT part\n3.\t\u201cAltogether, the models in the literature are either multi-modal or multi-domain <\u2026>  DRIT++ (Lee et al. (2019)) also proposed a multi-modal and multi-domain model\u201d \u2013 These two sentence contradict each other\n4.\t\u201cExisting multi-domain models such as Choi et al. (2018) or Pumarola et al. (2018)\u201d \u2013 this would much better work if Pumarola et al work was introduced in the paper\n5.\t\u201cby separating the latent space from the domain code\u201d \u2013 what is domain code?\n6.\t\u201cits latent attribute z_n is assumed to follow the n-th Gaussian component of the GMM, z_n \\in N (\\mu^n,\\Sigma^n), thus K = N\u201d, given that N is never defined, the statement about its equality to K is a bit off. It is unclear what is the purpose to introduce another variable. It seems and indexing domains with k rather than n would be a more straightforward choice since it is earlier specified that components of a GMM would correspond to domains\n7.\tEq. (2) and (3) \u2013 it is unclear why exactly those two properties are required for ensuring disentanglement of content and attribute extractors. \n8.\t\u201cWhen traveling through the network\u201d \u2013 what does it mean?\n9.\tDifference between \u201cConsistency\u201d and \u201cRealism\u201d properties is unclear from the first paragraph in Section 3.2\n10.\tMotivation of the need to use the isometry loss (when it is introduced before the ablation study) in the paper would be much appreciated as well as a verbal description of this loss\n11.\tAny motivation of using l_1 norm in eq. (4) \u2013 (8) is encouraged\n12.\tNDB, JSD, and HED \u2013 full names before introducing acronyms are encouraged\n13.\t\u201cTo estimate the IS\u201d \u2013 IS was not defined\n14.\tLast paragraph on page 7 \u2013 is it supposed to be \\Sigma_k rather than \\sigma_k? If not, what is \\sigma_k? (the definition should be in the main text, not only in Appendix)\n15.\t\u201cD: the number of domains, N: the number of output channels, K: kernel size\u201d \u2013 mix in notation. N was used before for the number of domains and K was used for the number of components in GMM\n16.\t\u201cAppendix B.1 shows instead the quantitative results of paired image translation.\u201d: \u201cAppendix B.1\u201d -> \u201cTable 7\u201d?\n17.\tAppendix B.2, first sentence. It is better to add that these are results for the Digits experiment\n18.\tMore elaborated description in Appendix C would be appreciated\n\nMinor:\n1.\t\u201ca phenomenon known as model collapse.\u201d -> mode collapse\n2.\t\u201c(i.e. a day scene \u2194 night scene in different seasons)\u201d: \u201ci.e.\u201d -> \u201ce.g.\u201d?\n3.\t\u201cor map in the same model multiple domains\u201d: \u201cmodel\u201d -> \u201cmode\u201d\n4.\t\u201cWe willshow that our model\u201d \u2013 missing blank\n5.\t\u201cThus, our GMM-UNIT is a generalization of the existing state of the art. the In the\u201d \u2013 redundant \u201cthe\u201d before \u201cIn\u201d\n6.\t\u201cIn the textbfconsistency term\u201d -> \\textbf{consistency}\n7.\t\u201cThe value of most of these parameters\u201d -> The valueS\n8.\t\u201cthe quantitative results on the CelebA datset\u201d -> datAset\n9.\t\u201cGMM-UNIT will be applied\u201d \u2013 inconsistent tense. Better leave it present as in the other sentences\n\n\n\n"}