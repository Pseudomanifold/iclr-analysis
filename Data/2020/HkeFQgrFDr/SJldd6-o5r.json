{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work introduces a new method for stochastic multi-domain image translation called GMM-UNIT, which bridges a gap in the existing literature. Namely, the authors claim that existing approaches are limited to either deterministic translation between multiple domains, or stochastic multi-modal translation between only two domains.\n\nGMM-UNIT is an autoencoder-like model that allows stochastic multi-modal translation between multiple domains. To this end, images are described by two stochastic latent variables that represent the content and domain (or style) of the image. Importantly, the method uses a GMM prior for the style variables. By using the ground-truth domain knowledge, the authors train it such that each mode of the GMM represents a separate domain. Meanwhile, the encoders of GMM-UNIT take the role of inference networks that model the posterior distribution of (content, style) given an image. Having extracted a content variable, it is now possible to sample style from any of the GMM modes to translate that image to a different domain. One can also interpolate between modes to interpolate between domains. Additionally, one-shot domain transfer (style transfer) is also possible, since we can extract a style from a single image, possibly from a novel domain.\n\n The model is very complicated, and its training requires optimization of 8 (sic!) different loss terms (with corresponding weight hyperparameters), among which are two reconstruction errors, two GAN-style image generation losses, and a cycle-consistency loss. The model is compared against SOTA baselines on one-to-one domain translation of sketch->shoe, many-to-many domain translation of digits of different styles, and on translation between different attributes on CelebA. Additionally, style-transfer (as one-shot translation) results are provided, and authors investigate the effect of some of the loss terms in an ablation study.\n\nThe proposed model is very complicated, requires detailed analysis and clear descriptions if it is to be understood by the community. Meanwhile, this paper is poorly written, the approach is badly (and incorrectly) motivated, and there is not enough model analysis to warrant publication. For this reason, I recommend REJECTing this paper despite the impressive empirical results. I provide more detailed comments below and hope that this article can be improved and published at a future conference.\n\n1. The name GMM-UNIT is not explained beyond its first component, \"GMM\". What does UNIT mean/stand for?\n\n2. The authors claim that representing a domain with a continuous vector is novel, and contrary to discrete one-hot representations used in the literature, it 1) does not grow linearity with the number of domains, and 2) allows interpolation between different domains. I think this claim is wrong. The fact that a one-hot representation grows is an implementation detail; one could easily use integers and a learned constant-dimensional lookup table of domain embeddings. Similarly, even if one-hot representation is used, multiplying it with a matrix converts it into a vector. Since these one-hot vectors are typically fed into a neural network, one can easily interpolate between the results of matrix multiplication with such a vector. The fact that this has not been tried does not mean that it is impossible, or that it is a fundamental flow of prior art. Interestingly, the authors do say it themselves at the beginning of section 3, where they show how to derive prior art from the GMM-UNIT framework. Since these two claims are central to the paper and are both wrong, I think that the approach should be motivated differently.\n\n3. Figure 2 is very difficult to read, and the caption does not help. Please simplify the graphics and make the caption easier to follow. For example, what does \"train the network to fit the GMM\" mean?\n\n4. I don't understand why the \"domain classification\" losses are necessary, and there is no explanation given. Please give a detailed description and add an additional ablation study without those losses.\n\n5. I do not understand how non-exclusive binary attributes in CelebA can be modeled as different Gaussian components in a GMM other than having different modes enumerate the possible configurations of the attribute vector. I think this should be thoroughly explained.\n\n6. Some results (e.g., in table 2) are reported as number+-number, but it is unclear what those two numbers represent. Are they mean and standard deviation, respectively? Many runs were performed to get these numbers? Also, it seems that \"the best\" result is highlighted. However, in this case, all the best results (with no statistically significant difference) should be highlighted, which is not the case. Please fix that.\n\n7. The authors modify the StarGAN method to make it stochastic and say that this does not improve sample diversity. However, there are no details on this modification, which means that the readers cannot assess its sensibility by themselves. Please add the details.\n\n8. The appendix contains a figure with TSNE embeddings of inferred style vectors and samples from the GMM for the CelebA experiment, color-coded according to the domain. One can see that there is a substantial mismatch between the sampled values and the inferred ones, which indicates that the model did not do a good job at aligning those two distributions. Why is that? This should be discussed."}