{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a stylized text generation approach grounded on specific text domain, such as review or news article. They leverages the large-scale pretrained transformer (GPT-2) and employed a linear combination of 4 losses to train the model. While I do think that this task is important, challenging and interesting, several aspects of this paper failed to convince me that the proposed method is indeed effective:\n\n1. The paper does not define the goal of the proposed task clearly, e.g., what should the well-generated results look like? From my perspective, style example-guided text generation should first sustain related content as the input context (coherent semantic information). However, the paper neither demonstrates this nor provided a related metric to evaluate this. On the contrary, the paper proposes to use fluency score, style score, style diversity, content novelty to evaluate the generated results. Is a fluent paragraph with the correct style but totally off-topic content desirable? It's also not clear how to define the style of the reference. Since the reference is also a paragraph rather than a style label or a single sentence, the generated paragraph probably inherits characteristics more than just style information from the reference. How to tell whether the borrowed information is style or content?\n\n2. Although the paper provides a new text style generation task based on examples, the novelty of the proposed framework is relatively limited.  The proposed framework is based on the transformer generation with conditions on style example. Additional adversarial feature alignment is used to sustain correct style generations. However, these components are already widely used in previous text generation tasks. In other words, I didn't see that the proposed framework is specifically designed for example-guided text style generation. \n\n3. What are F_d and H_d in Figure 1? These are not defined in the paper. \n\n4. It seems that the generation ability is borrowed from GPT-2, rather than trained on the dataset. Without distillation from GPT-2, what would the performance become? \n\n5. Where is the F_f output layer illustrate in Figure 2? My concern is if style information z is incorporated into P_i in a very shallow way, then the input fake feature may have a very different distribution with the input real feature. Then the discriminator can easily be trained very strong and the generator couldn't get efficient updates. \n\n6. The experiment is not thorough. Many ablation studies are missing."}