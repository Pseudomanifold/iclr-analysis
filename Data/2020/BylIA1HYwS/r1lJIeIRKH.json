{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a model for generating text, conditionally on a context content and on a style information. Given a context sentence or paragraph, the task consists in generating plausible text with a content corresponding to this initial context and a style corresponding to another piece of text. The model is based on a transformer architecture inspired from GPT2. It is trained using a combination of loss functions: log likelihood, distillation w.r.t a pre-trained GPT2, style and adversarial loss. The model is evaluated on aggregations of multiple text corpora building a multi-style corpus.\n The proposed model combines several interesting features like making use of large pre-trained language models to guide the generation or the use of a latent GAN formulation for text decoding in order to avoid non differentiability. The authors propose different variants of the model for incorporating the style component into the transformer architecture. The role of the different loss components should however be better motivated. The _DIST and _GAN components are not directly linked to the objective and their importance for the model should be better stressed. The global model is probably quite heavy and a discussion on its complexity and on the training strategy would be useful in order to complete the description.\nThe evaluation is performed on two corpora assembled by the authors and representing a variety of styles. This contribution could be helpful to the community. The authors introduce a series of evaluation criteria:  perplexity of the generated text, style classifier, diversity and novelty. They also provide human evaluation. According to the quantitative criteria, one of the variants seems to perform reasonably well compared to the baselines. However, the evaluation is not convincing. It requires to be pushed further and the results and the system behavior should be better analyzed. The different examples of generated text are not consistent, and rather look like a random sampling over the vocabulary. This is in contradiction with the scores appearing in the tables and on the figures. One of the Baselines (SC) generates much more coherent texts \u2013 at least in the examples provided by the authors \u2013 while its perplexity score is similar to the authors\u2019 model. The ablation study does not show the importance of the different loss components. For example, the models without the GAN or the DIST loss terms behave as well as the complete model does. Concerning the human evaluation, I do not understand the numbers in table 3-right: it seems that the model performs better at generating among 21 styles (-category) than it does with 3 styles?\nOverall, the paper represents a large amount of experimental work, but deserves a more in depth discussion of the usefulness of the different components. The experimental evaluation is still preliminary.\n"}