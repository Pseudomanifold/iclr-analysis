{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a (to the best of my knowledge) new task and models to generate plausible completions of a given paragraph condition on a reference text that has a different \u201cstyle\u201d from the input. The proposed model is comprised of a transformer language model trained with multiple objectives including an LM completion objective where the reference text has the same style distribution as the input, a cross stream where the reference text has a different style and the generated LM completion is forced to have the style of the reference text as determined by a pre-trained classifier. Two additional objectives - a distillation objective to match the output distributions of the transformer LM and a pre-trained GPT model and a GAN objective to their feature distributions is used as well (since the pre-trained classifier operates in the space of pre-trained GPT hidden representations). I think the paper tackles the interesting problem of example guided text style transfer that hasn\u2019t been studied much and presents a complicated solution to the problem. However, I don\u2019t think the paper is ready for publication in its current state because 1) The writing needs to be significantly improved to include high level textual descriptions and intuitions for each training objective - this is particularly benefit readers new to this class of problems 2) The paper needs much simpler baselines especially since the tasks and datasets presented are new. Each presented model has several moving parts - a GAN, pre-trained GPT and the different style token conditioning strategies. 3) Samples from the model (presented in the Appendix) appear quite poor and I\u2019m not convinced that the model really works right now.\n\nI have a few comments and questions\n\n1) Distillation loss - you say this loss is trained by \u201cminimizing the mutual information between F and H\u201d don\u2019t you want to maximize the MI? Also the formulation written in L_{dist} is equivalent to minimizing cross-entropy between H and F.\n\n2) The paper would really benefit from a baseline where instead of an example from a target serves as reference, just it\u2019s style category is used. This would be turning the problem into the more common formulation of text style transfer. \n\n3) Did you consider having a back-translation objective instead of a complicated style-classifier and GAN loss? This has shown to be much easier to optimize and learn see Prabhumoye et al [1] and Subramanian et al [2].\n\n4) You report pre-trained accuracies of C when trained on H_{f}, what about during training? Does it correlate to progress in GAN training?\n\n5) If the goal of using a GAN objective is just to match the distributions of H_{f} and F_{f}, then why train it only on cross samples? Why not also on reconstruction samples?\n\n6) How does the style decoder F_{s} deal with variable lengths of p_{k}? Is the length of p_{k} truncated to a fixed length?\n\n7) Do human evaluators also know the target style type that the model is supposed to emulate?\n\nMinor\n\nAbstract - \u201ctexts decoder\u201d -> \u201ctext decoder\u201d\nIntroduction - \u201cmain mean\u201d -> \u201cmain means\u201d\n\nReferences\n\n[1] Style transfer through back-translation - https://arxiv.org/abs/1804.09000\n[2] Multiple-attribute text style transfer - https://arxiv.org/abs/1811.00552"}