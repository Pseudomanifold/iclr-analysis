{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The paper uses a Gaussian Processes framework previously introduced in [1] to identify the most important samples from the past for functional regularization. For evaluation authors report their average accuracy on Permuted MNIST, Split-MNIST, and CIFAR10-100 and achieve superior performance over EWC, DLP, SI, VCL-Coreset, and FRCL.\n\nPros:\n(+): The paper is well-written, addressed the prior work quite well despite missing a few important work from the past (more on this later)\n(+): The paper is well motivated\n\nCons that significantly affected my score and resulted in rejecting the paper are as follows:\n\n1- lack of support for \u201cscalability\u201d:\nAuthors claim their method is scalable in several parts of the paper (abstract in line 7, Section 3 in the 1st paragraph, and Section 5 in Discussion). However, this claim is not supported in the experimental setting as the benchmark used are only toy datasets (Permuted MNIST, Split MNIST, and CIFAR10 followed by CIFAR100) where the maximum # of task considered is 10 and the maximum size of the datasets is 60K which is not convincing for ability to scale. There is also no time complexity provided. \n\n2- Incremental novelty over the prior work (FRCL by Titsias et al 2019):\nThis baseline is the closest prior work to this work which according to the experiments shown in Table 2 are slightly outperformed by the proposed method. (for example for P-MNIST the gain is 0.6%+-0.1) where there is a lack of complete discussion on how the two methods are different. Particularly I suggest that the authors elaborate more on their claimed differences stated on page 4, paragraph 5 such as \u201ctractability of the objective function only when we assume independence across tasks\u201d. Do authors mean assuming clear task boundary between tasks? If so, have they considered a \u201cno-task\u201d or an \"overlapping\u201d task boundary in their experiment? Isn't it necessary to back up this if it is stated as a shortcoming of FRCL? Also, how are these methods differ in their computational expenses?\n\n3- Lack of measuring forgetting: \nThis is the most important drawback in the experimental setting. Authors indicate on page 3 \u201cOur goal in this paper is to design methods that can avoid such catastrophic forgetting.\u201d and reiterate on this on other parts of the paper yet there is no forgetting evaluation to support this claim. Authors can simply report the initial performance of the model on each task so that readers can compare it with the reported accuracy after being done with all tasks. Having a method with high average accuracy does not necessarily mean it has minimum forgetting. You can use forgetting measurements such as Backward Transfer (BWT) introduced in [1] or forgetting ratio defined in [4] for this assessment. \n\n4- Ambiguous claims about prior work:\n(a) On page 1, paragraph 3, when authors mention that methods such as GEM or iCaRL use random selection to pick previous samples, I think the line of follow-up work on these methods should be mentioned as well that have explored different techniques for sample selection and have provided benchmark comparisons (ex. [2,3]). In fact it would be beneficial if authors could compare the samples selected by their method versus other sampling techniques. \n(b) On page 1, paragraph 3, they mention some prior work such as GEM and iCaRL \u201cdo not take uncertainty of the output into account\u201d. While it is true, there have been methods proposed that use uncertainty of the output for parameter regularization [5]. It appears to be a parallel work to this but it\u2019s worth mentioning to prevent false claims.\n\n5- Claim on the state of the art should be double-checked:\n\tAlthough the results shown for the experiments are superior to the provided baselines, there is an important baseline missing which has achieved higher performance than the reported ones. Also missed to be cited in the prior work list. Serra et al [4] proposed a method at ICML 2018 called HAT, which is a regularization technique with no memory usage that learns an attention mask over parameters and was shown to be very effective on small and long sequence of significantly different tasks. They do not use samples from previous task but yet achieved good average ACC as well as minimum forgetting ratio. Note that 5-Split MNIST is not reported in [4], but a recent work has reported HAT\u2019s performance on this dataset (https://openreview.net/forum?id=HklUCCVKDB) that achieves 99.59%. I recommend authors provide comparison of their own on the given benchmarks with the original HAT\u2019s implementation (https://github.com/joansj/hat) before claiming to be SoTA. In my opinion, it is not an issue if a novel method achieves a slightly lower performance to the sota because I think it still adds value and proposes a new direction. However, a false claim should not be stated.\n\nLess major (only to help, and not necessarily part of my decision assessment):\n\n1- Providing upper bound?\nIt is common to show an upper bound for any continual learning algorithm by showing joint training performance which is considered to be the maximum achievable performance. I also recommend showing the naive baseline of fine-tuning for the proposed method  which often can give insight to maximum forgetting ratio.\n\n2- Forward transfer?\nRegularization techniques combined with memory might have an ability to perform zero-shot transfer or so called FWT. I recommend authors provide such metric to further support their method.\n\n3- Hyper parameter tuning?\nIt is also worth mentioning how the tuning process was performed. In continual learning we cannot assume that we have access to all tasks' data, hence authors might want to shed some light on this.\n\nReferences: \n[1] Khan, Mohammad Emtiyaz, et al. \"Approximate Inference Turns Deep Networks into Gaussian Processes.\" arXiv preprint arXiv:1906.01930 (2019).\n\n[2] Chaudhry, Arslan, et al. \"Continual Learning with Tiny Episodic Memories.\" arXiv preprint arXiv:1902.10486 (2019). (https://arxiv.org/abs/1902.10486)\n\n[3] Aljundi, Rahaf, et al. \"Gradient based sample selection for online continual learning.\" arXiv preprint arXiv:1903.08671 (2019). (https://arxiv.org/abs/1903.08671)\n\n[4] Serr\u00e0, J., Sur\u00eds, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557\n\n[5] Ebrahimi, Sayna, et al. \"Uncertainty-guided Continual Learning with Bayesian Neural Networks.\" arXiv preprint arXiv:1906.02425 (2019).\n\n"}