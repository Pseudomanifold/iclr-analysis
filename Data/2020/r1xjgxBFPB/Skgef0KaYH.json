{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a new functional regularization method with gaussian process which has similar direction with recent two works (khan et al, titsias et al).\nTo perform functional regularization, they introduce small coreset which are selected from previous dataset instances, called memorable past. They select most memorable samples depends on eigenvalue. The model FROMP outperforms baselines and their ablations. However, the experiments are only performed on shallow networks, it is required to apply on much deeper networks, such as ResNet. Also, in the experiment results, I feel the performance of the FROMP largely depends on the number of the coreset, while 'important' selection just shows marginal effects even on split CIFAR. \nFROMP show higher performance than FRORP with only a few of examples, but it isn't meaningful results that anyway the performances are too poor that are even worse than old baseline, EWC. \n\nI have several wonderings on the paper.\n\n- How about of training time on FROMP? I wonder if utilizing or selecting memorable pasts requires much time for training.\n\n- Is there an analysis like figure 1 on real dataset, such as MNIST or CIFAR?\n\n\n\n"}