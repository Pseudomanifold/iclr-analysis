{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper explores the interesting connections between action and sound, by building a sound-action-vision dataset with a tilt-bot. This is a good paper overall, I appreciate the efforts on the dataset, and this direction of research is worth pursuing. \n\nRegarding experiments, I like the way it is set up, especially the four microphones, and the action space of the robot. I\n\nA couple of questions:\n(1) In the inverse model learning, Fig 3(a) bottom, why are images used as input as well? Don't we want to predict action purely from sound?\n(2) In forward model prediction, how are the ground truth locations defined and labeled? Is it the center of mass, and annotated by humans? More details on this experiment will help.\n"}