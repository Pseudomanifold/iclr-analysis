{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work focuses on meta learning from both demonstrations and rewards. The proposed method has certain advantages over previous methods:\n(1) In comparison to meta-imitation, it enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. \n(2) In comparison to meta-reinforcement learning, it can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration.\n\nI am a bit confused by the writings and don't clearly understand the algorithm.\n1. In eq. 4, should \\theta be \\phi?\n\n2. What does \\pi_\\theta(a_t|s_t, {d_i,k}) mean? I'd like to see an example formulation of the policy. When the parameter \\theta is well trained, does the policy still take demonstrations {d_i,k} as inputs? If yes, why are demonstrations needed? \n\n3. In meta-testing (Algo. 2), will \\theta and \\phi be updated? Currently it looks like that the two parameters are fixed in meta testing. If so, this is a bit strange. Why not update the policies after sampling demonstrations (step 4) and collecting trials (step 5)?\n\n4. \"The state space S, reward ri, and dynamics Pi may vary across tasks.\" I think all the tasks should share the same input space; otherwise, the meta-trained two policies cannot be applied to a test task with different input space.  For example, if the states are 100x100 images in meta training, how to apply the policies to a meta-testing task with 100x200 images as states? \n"}