{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an exploration approach of Go-Explore together with imitation learning for playing text games. It is shown to outperform existing solutions in solving text-based games with better sample efficiency and stronger generalization ability to unseen games.\n\nPros:\nSeq2seq imitation learning + Go-Explore is applied to more challenging text games and achieves better performance, higher sample complexity and better generalization ability.\n\nCons: \n\u2022\tFrom modeling perspective, the policy network uses the standard sequence-to-sequence network with attention. And it is trained on the high-reward trajectories obtained with Go-Explore method using imitation learning. From this perspective, there is not much novelty in this paper.\n\nDetailed comments:\n\u2022\tMore details about the mapping function f(x) in Phase 1 should be given.\n\u2022\tIt is not clear why Phase 2 should be called \u201cRobustification\u201d. It seems to be just standard imitation learning of seq2seq model on the high-reward trajectories collected in Phase 1.\n\u2022\tIn the paragraph after eqn. (1), H is defined to be the hidden states of the decoder. Shouldn\u2019t it be the hidden states of the encoder?\n\u2022\tIt seems to be unfair to compare the proposed method with advanced exploration strategy to other model-free baselines that only have very simple exploration strategies (e.g., epsilon-greedy). It is not surprising at all that Go-Explore should outperform them on sparse reward problems. More baselines with better exploration strategies should be compared."}