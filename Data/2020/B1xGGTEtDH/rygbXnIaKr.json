{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proves universal approximation theorems for fully connected networks with fixed width and unbounded depth. Unlike recent results focusing on ReLU networks approximating scalar valued target functions, this paper provides universal approximation theorems for a wide range of activation functions and vector valued target functions. \n\nThis paper provides a number of universal approximation theorems on different activation functions, but the central result can be stated as follows: \nGiven input dimension n and output dimension m and activation \\rho, assume \\rho is a continuous function and there exists \\alpha \\in R such that \\rho is continuously differentiable at \\alpha and the derivative \\rho\u2019 is nonzero at \\alpha. Then, a fully connected network with layer width n+m+2 and unbounded depth can approximate any continuous function on a compact domain to arbitrary sup-norm accuracy.\n\nI would like to vote for acceptance of this paper because the authors develop nontrivial techniques that extend existing universal approximation results on width-bounded ReLU networks to essentially \u201call\u201d other activation functions. I think this paper is well-written and well-organized; it gives a good overview of the existing results that contextualizes this paper well, briefly summarizes the main results, and then reveals the details of construction. I haven\u2019t checked the full details of the proofs in the appendix, but as far as I can tell they look correct.\n\nWhile I think that Section 4 reveals the proof techniques reasonably well, I believe that proofs of Propositions 4.3 and 4.7 should be covered/sketched in greater detail in the main text. For example, Theorem 4.4 builds upon Proposition 4.3 by approximating identity function locally using \\rho. The main text only reveals approximation of identity function by \\rho and defers the whole proof of Proposition 4.3 to the appendix. I think adding proof sketches for the propositions will be more helpful to the readers, and to this end, cutting down some text in the recurring paragraph \u201cuniform continuity preserves uniform convergence\u2026\u201d should be helpful.\n\nAccording to Remark 4.9, it seems that the proof strategy for polynomials can also be applied to nonpolynomials. This motivates a natural question: why can\u2019t you apply the proof strategy for nonpolynomials to polynomials? I believe you can\u2019t because Proposition 4.3 relies on the existing universal approximation results which requires \\rho to be nonpolynomial (which is not mentioned in the main text). In my opinion, adding some comments on \u201cwhy nonpoly techniques can\u2019t be applied to poly activations\u201d would help readers better understand the proof techniques.\n\nAnother question just out of curiosity: can you prove tightness of your construction, e.g., show that a square model with width n+m is NOT a universal approximator?"}