{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The paper presents approximation power results for deep, but narrow networks with various activation functions. In particular, the authors target the minimum width possible, s.t. the class of networks considered remain universal approximators. The authors consider ReLU activation functions, polynomial activations, as well as some non-differentiable activations. \n\nEvaluation: I'm personally not very closely involved with some of the recent developments on representational power of narrow, deep networks, but it seems to me this paper follows mathematically similar intuitions to prior works (e.g. \"memorize\" the input / prior computations; another way to visually represent these proofs is to \"rotate 90 degrees\" the usual shallow network approximation -- the authors call this a \"register\" model. They cite the paper by Lu et al '17 which essentially uses this proof technique).  \n\nWhile there are some technically interesting parts (e.g. the polynomial activation part has some trickiness in maintaining the width at n+m+1), I don't think will be very interesting to the ICLR community at large, and I think it is fairly incremental. \n\nThe writing is by and large clear, though Appendix C is a little wordy and hand-wavy. \n\n"}