{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis paper proposes an algorithm to address bias in estimating the value function for actor-critic methods. The algorithm, balanced uses a convex combination of the minimum of two value estimates (as in clipped double Q-learning) and a single value estimate (as in DDPG) as a TD target. The weighting between the two is also learnt with stochastic gradient descent during training. Empirical results show that this algorithm produces more accurate value estimates and can improve performance on standard continuous control tasks. \n\nWhile value estimation in actor-critic methods is an important topic and this paper , I am not convinced that this paper makes a significant contribution to the area. The proposed algorithm is not well-motivated with no theoretical justification and the experiments do not show the clear benefits of the algorithm.\n\nThe main reason for my decision is that the proposed algorithm, BCDQ, does not seem to be motivated well enough. While a case is made for the importance of addressing bias in value estimates, the particular mechanism BCDQ uses is not well-motivated, theoretically or otherwise. While it has been observed that CDQ underestimates the true values and regular TD learning in DDPG overestimates the true Q-values, using a convex combination of both seems fairly arbitrary. The paper does not seem to provide any other justification for this choice. For example, a reasonable alternative could be to use \\beta * Q_CDQ(s,a), a scaled version of the CDQ estimate, with \\beta being tuned with SGD (this variant could serve as a baseline). \n\nFor the experiments, for Fig. 1, error bars are missing from the value estimates. This makes it difficult to judge if the results are significant or not and, at a glance, there does not seem to be a large difference with the baseline algorithms. Additionally, the performance of BTD3 on the benchmarks do not seem to produce (statistically) significantly better results than regular TD3. As such, empirical performance is not a strong motivating argument in favor of the algorithm.\n\nThe paper could also be strengthened by providing a deeper analysis of \\beta during training. Currently, in the paper, it is only mentioned that different environments lead to different values of \\beta. I think it would be interesting to further investigate how \\beta varies over training and how this relates to over/underestimation of the value function and training dynamics. For example, looking at Fig. 2, it seems like there is consistent trend of \\beta first dipping down then spiking upwards, before slowly decreasing over the rest of training. Is there a reasonable explanation for this? \n\nOther points:\n1) The objective for beta, eq. 5 seems to be missing a 2 in the exponent. Also, Q^\\pi is not defined.\n2) In Alg 1, similarly, the gradient of beta may need to be fixed.\n3) In table 1, using the maximum episode reward introduces higher variance and is not good practice. See the \"Reporting Evaluation Metrics\" section of \"Deep RL that matters\" by Hendersen et al.\n4) Some uncertainty measure would be helpful in Fig. 2, e.g. plotting the quartiles. \n\nMinor comments and typos:\n- The BTD3 algorithm figure could be placed on the next page\t\n- Fig. 1 Why is BCDQ on a different plot? If it's for clarity, the axes for both rows of plots should match.\n- p.4 \"over the time of the training process\" -> \"over the training process\"\n- p.4  backward quotes in \"min\"\n- p.6 \"suggests that - similary to ensemble methods - the ...\"  use --- instead of -\n- p.6 \"Differently\" \n- p.8 Fig. 4 caption: \"show\" -> \"shown\", \"OpenAi\" -> \"OpenAI\"\n- Many commas are missing. Here are a few: p.1 - \"In recent years\", \"For many important problems\", \"In an actor-critic setting\", \"Similarly to CDQ\", \"In CDQ\", p.2 - \"To guarantee reproducibility\", \"Later\", \"Similarly to our approach\", p.5 - \"For the Hopper task\", p.6 \"From the plots\", \"For example in Walker2d\", p.7 - \"In Table 1\", \"For all tasks\", ... \n\n"}