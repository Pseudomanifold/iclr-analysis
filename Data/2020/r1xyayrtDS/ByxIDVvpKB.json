{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a new way to eliminate bias in critic estimates, based on Clipped Double Q-learning (CDQ). It shows that CDQ could potentially underestimate the critic, and solves this problem by adaptively taking the mean of CDQ target and a normal critic target which might be an overestimate.\n\nMy main concern about this paper are: 1) The effectiveness of the way to learn beta adaptively is unclear to me. 2) the method proposed is an incremental change on top of CDQ, and introduce new (and maybe too much) complexity in an algorithm to remove the overestimation bias. I tend to reject this paper.\n\nDetailed comments:\n\n1) The algorithm introduce another parameter beta to balance the percentage of using an underestimation or overestimation of Q^\\pi. It\u2019s important that beta needed to be learned adaptively by the algorithm itself. I suspect the current empirical justification is not enough to convince me that using trajectory return as a sort of \u201cground truth\u201d to determine it is over- or under-estimation. Since Mujoco is deterministic and DDPG has a deterministic policy, the whole dynamics is deterministic which makes trajectory return becomes a very accurate estimate of Q^\\pi. I\u2019m not sure in a more stochastic environment if the algorithm of choosing beta still works or not. \nOn the other side, if the trajectory return is such good evidence of deciding  over-/under-estimation, we can just use it as Q^\\pi which gives REINFORCE instead of actor-critic. One reason we study actor-critic algorithm is that in some cases (such as stochastic environment) function approximator generalizes better than trajectory return. However the proposed BTD3 algorithm is an actor-critic that uses trajectory return to guide it, it seems to conflict with the motivation of using actor-critic.\n\n2) The method in this paper is a straightforward and incremental way to debias the underestimation in CDQ -- adding another overestimate on top of it. However, the original problem is critic with function approximator potentially overestimates true critic. Doing a minus then plus looks more than necessary as a straightforward solution to the original problem. Also, it increases the whole complexity of the algorithm. So it needed to be justified with more intuition/theory/diverse experiment domains.\n"}