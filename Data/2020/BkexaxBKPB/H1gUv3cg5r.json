{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary\n\nThe paper proposes extensions of Generative Adversarial Networks to modeling multiple text corpora. Concretely, the paper looks at two problems: 1) given independently pretrained word embeddings from K corpora, finding a common word embdding, 2) extracting document representations from a discriminator of a GAN trained to generate tf-idf vectors. Preliminary experiments show that the proposed approaches outperform baseline classifiers trained with word2vec.\n\n\nStrengths\n+ The paper clearly mentions all the experimental details\n+ The paper has a nice set of qualitative examples that probe what the proposed model is learning\n\n\nWeaknesses\n\n* It is not clear what problem the paper is trying to solve. Is the goal to get better document representations, in which case why do we think that using a GAN is a good idea. Learning a generative model to use representations for a downstream task seems like a pretty roundabout way of doing things (if we dont care about generating anything in the first place). Further, if better document representations are desired then the paper should compare to works like Bert (Devlin et.al. [1]).\n\n* Sec. 3.1: Calling the proposed weGAN model a GAN seems a bit inconsistent/ wrong. The proposed model is not a generative adversarial network, there is no sampling of a noise or a notion of a generative distribution. The real data, similar to the image case is a bunch of samples from the data distribution (where the stochasticity comes not from the word embeddings but from the tf-idf of thte document), but the generator also uses the tf-idf representation, so essentially both the generator as well as the real world data use the document as an input. Hence it feels like a somewhat odd model formulation, which would be nice to clarify / explain.\n\n* Further, it is not clear why one would want to train \u201ccross-corpus\u201d embeddings in the way describeed in Sec. 3. 1. Why not just train word2vec on the union of all the corpora (instead of per-corpus) and use it as the word representation? What are the conditions in which one would not want to do the common word representation?\n\n* Sec. 3.2: Why not formulate modeling multiple corpora in the spirit of CoGAN (Liu and Tuzel, 2016), by getting M discriminators (potentially with parameter sharing) to solve binary classification tasks as opposed to the 2M way classification problem? Atleast a comparison to an approach like this seems warranted. Further, it is not clear why one would want to generate tf-idf vectors. It seems like all the experiments are around representation learning and classification as opposed to generating or evaluating tf-idf vectors for documents. \n\n* As a side note, for the experiments concerning deGAN vs the baseline, it would be nice to check what happens when the last classification layer is not initialized with LDA parameters. Since that is an additional source of information. \n\nReferences\n[1]: Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. \u201cBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.\u201d arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1810.04805.\n"}