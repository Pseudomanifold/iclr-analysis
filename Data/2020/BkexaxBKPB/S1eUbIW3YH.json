{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes two models: (1) A model to learn word embeddings from different corpus. (2) A model to generate robust bag-of-words document embeddings. This topic may not be novel enough, considering the current development in GAN and domain adaptation. Some questions:\n-\tFor the word embedding model, why it is called GAN? It seems that nothing is generated. \n-\tFurther, for the word embedding model, the notation is over-complicated. This model is very similar to the domain adversarial network (DANN). This is my understanding and please correct me if I am inaccurate: A classifier is trained to distinguish the domains/corpus, while the word encoder tries to learn shared features across domains/corpus. The sub- and super-scripts are confusing. \n-\tFor the deGAN, only generating bag-of-words may not be that attracting. Further, the idea is also similar to many existing works in domain adaptation. Each domain/corpus has unique components, while all domains/corpus also share similar features. The final output is just a combination of the shared components and the unique features. This idea is straightforward. It would be much better if the author can generate natural language, instead of just bag-of-words. \n-\tPlease fix the formatting issues. For instance, Figure 3 and Figure 4 cover the text. \u2018To explain (10), first we consider\u2026\u2019. I think here should be (6).\n-\tMore baseline competitors are preferred. \n"}