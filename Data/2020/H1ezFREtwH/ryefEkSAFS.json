{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the specific question/problem tackled by the paper?\nThis paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, \u2026, pi_K) into policies for more complex tasks. Given a number of primitive skills and a new task within an environment, the paper aims to learn to pick and combine the primitives as needed to solve the new task. This problem statement is interesting and the method performs well on difficult tasks. \n\nHowever, I argue for rejecting this paper because it lacks meaningful contributions to the field. I do not see how the method presented in the paper is more than RL over  hand engineered action spaces that are better for the tasks. While this improves results, we already know that for any task, there is some best action space for performing that task. This is why most HRL work aims to also find the primitive policies in additional to composing them. Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives?\n\nSummary:\nThe method presented in the paper is as follows: at each state s_t, the K primitives are queried for their action a_k ~ pi_k(s_t). Then, a biRNN reads in the actions in order from 1 to k. In parallel, the state and a goal are encoded by a network named \u201cDecoder\u201d. The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. Finally, the output action is the weighted combination of all the actions. The encoders and attention weights are trained with RL. \n\nThis method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining \u201cforward\u201d and \u201cjump\u201d primitives. Each environment has predefined primitives such as \u201cforward\u201d \u201cleft\u201d \u201cright\u201d etc. \n\nThis method is compared against HIRO, which does not have access to the primitive policies. It is not surprising that hand engineering primitives helps performance. \n\nIs the approach well motivated?\nThe general idea behind the approach is well motivated: using primitive skills to learn complex skills is a useful goal. The details of the method are strange.\nI would like to see a better motivation and empirical justification for the biRNN. Why should the primitive\u2019s action be encode in order? The ordering of the primitives is arbitrary and constant: a fully connected network could be used, or the attentions could be output entirely independently per primitive.\nIn fact, I do see not why the primitives\u2019 actions need to be encoded at all. It would be much simpler for the encoder to look at (s_t, g_t) and output a discrete probability over the K primitives.\nThe ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive\u2019s actions.\n \n\nIs the method well placed in the literature? \nThe main idea of predicting weights over multiple experts is not novel (see \"Adaptive mixtures of local experts\u201d from 1991). In the context of RL literature, we can interpret the primitive skills as actions directly, and then the method is performing basic RL over a better action space (the better actions being \u201cgo left\u201d, \u201cjump\u201d etc. We can also interpret these as options, but unlike options a single primitive is not followed for multiple time steps with a termination condition. Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space. \n"}