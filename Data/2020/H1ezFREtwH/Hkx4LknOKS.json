{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overall, I think the method has some great merit but I am not overly confident in the reproducibility of the method. Some of the comparisons (HIRO) do not agree with the results in the HIRO paper. Also, more description is needed to describe how the baselines were used in the analysis. Were they also given the pre-trained sub-policies? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained.\n\n Here are some more detailed comments: \n- Figure 1 is not very clear and does not appear to add much to the explanation of the method. More detail should be included in the caption.\n - In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I find the comment about how HRL requires many more training steps than regular RL very odd. The purpose of HRL is to have better sample efficiency and learn strong polices faster. Has the author observed different? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way.\n- The assumption that the sub-policies solve the underlying MDP is rather strong. How are these policies going to be trained to guarantee this? \n- I like the idea of using an attention model to help pick learn a weighting for the combination of a number of sub-policies. I am not sure if using a bi-directional LSTM is the best or simplest method to accomplish this. The authors can look at \"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\" NeurIPS 2019 for a recent work that is similar to theirs. \n- For Figure 4: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? Also, how are these component policies trained? Over what tasks are they trained? This information is very important to make sure the method is not overly biased to the composition of them. \n- I find the results in Figure 5 very odd. The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well. Can this contradiction be explained? \n- For the HIRO comparison was the system also using the composite policies there were pretrained? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy. \n- I do not understand the visualization in Figure 7. How to the colored paths for the agent represent the weights for the compose policy?"}