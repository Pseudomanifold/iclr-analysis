{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper is introducing a scale-equivariant CNN architecture with joint convolutions over spatial and scale space. Moreover, the authors used decomposable convolutional filters to reduce the number of parameters. Based on Therom 1, it is shown that scale-equivariance is achieved if and only if joint convolutions are conducted over spatial and scale space. \n\nOverall, the contribution seems meaningful but incremental that the method is almost similar to the 'RotDCF: Decomposition of convolutional filters for rotation-equivariant deep network'.\n\n- to verify scale equivariance, visualizing features with tsne would be interesting\n- in table 1, what is the reason for doing experiments with and without batchnorm?\n- what if the baseline's size is close to the proposed method, how it would be improved?\n- it would be better to show how the decomposed filters reduce the total number of parameters.\n- also comparing flops would be informative.\n- the author's name of 'Locally scale-invariant convolutional neural networks' is wrongly written."}