{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper explores learning dilation-invariant sentence representations, with a goal of improving downstream task performance on rare events. A pre-trained embedding is encoded as a latent variable Z, which is constrained to be multi-variate heavy tailed. Separate classifiers are trained on the head and tail of the distribution. Similarly, separate sentence generators are trained on the head and tail of the distribution, in order to allow data augmentation (creating diversity in the outputs by scaling the representation). While the high level motivation and algorithm is interesting, I found the paper very hard to follow, and the experiments are weak.\n\nI have quite a few concerns:\n- The algorithm takes a sentence embedding from BERT as input. BERT produces contextualized word representations, not sentence embeddings, so I don't know what the authors did here (the intro also claims that ELMo and GPT learn sentence embeddings, which is also confusing).\n- The paper argues that with empirical risk minimization, \"nothing guarantees that such classifiers perform satisfactorily\non the tails of the explanatory variables\". However, I could not follow what such guarantees the proposed method offers, if any.\n- Experiment 4.1 is impossible to follow without reading the appendix. This section should be expanded, or completely moved to the appendix.\n- The authors claim without evidence that a baseline of a neural network trained on top of the \"BERT embedding\" is state-of-the-art for sentiment classification. While there isn't enough information to know what was done, most state-of-the-art approaches involve fine-tuning BERT.\n- No comparisons are made with any other work, despite the method attempting a very general and well studied problem of text classification.\n- The submission claims that \"Applying a dilation is equivalent to assess the generalization of classifiers outside\nthe envelope of both training and testing samples.\". It isn't obvious to me that dilation captures the variation in embeddings you'd get from out-of-domain training samples.\n- The authors compare their data augmentation results to \"backtranslation\". The citation for the method appears to be a class project, and in fact does round-trip translation for paraphrasing, and not back translation.\n- No attempt is made to show if the data augmentation approach actually improves end task performance."}