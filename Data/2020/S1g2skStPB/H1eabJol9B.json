{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose an RL-based structure searching method for causal discovery. The authors reformulate the score-based causal discovery problem into an RL-format, which includes the reward function re-design, hyper-parameter choose, and graph generation. To my knowledge, it\u2019s the first time that the RL algorithm is applied to causal discovery area for structure searching.\n \nThe authors\u2019 contributions are:\n(1) re-design the reword function which concludes the traditional score function and the acyclic constraint\n\n(2) Theoretically prove that the maximizing the reward function is equivalent to maximizing the original score function under some choices of the hyper-parameters.\n\n(3) Apply the reinforce gradient estimator to search the parameters related to adjacency matrix generation.  \n\n(4) In the experiment, the authors conduct experiment on datasets which includes both linear/non-linear model with Gaussian/Non-gaussian noise.\n\n(5) The authors public their code for reproducibility.\n \nOverall, the idea of this paper is novel, and the experiment is comprehensive. I have the following concerns, and if the authors can clarify my doubt, I would be willing to increase the score.\n \n(1) In page 4 Encoder paragraph, the authors mention that the self-attention scheme is capable of finding the causal relationships. Why? In my opinion, the attention scheme only reflects the correlation relationship. The authors should give more clarifications to convince me about their beliefs.\n \n(2) The authors first introduce the h(A) constraint in eqn. (4), and mentioned that only have that constraint would result in a large penalty weight. To solve this, the authors introduce the indicator function constraint. What if we only use the indicator function constraint? In this case, the equivalence is still satisfied, so I am confused about the motivation of imposing the h(A) constraint.\n \n(3) In the last paragraph of page 5, why the authors adjust the predefined scores to a certain range?\n \n(4) Whether the acyclic can be guaranteed after minimizing the negative reward function (the eqn.(6))? I.e., After the training process, whether the graph with the best reward can be theoretically guaranteed to be acyclic?\n \n(5) In section 5.3, the authors mention that the generated graph may contain spurious edges? Whether the edges that in the cyclic are spurious? Whether the last pruning step contains pruning the cyclic path?\n \n \n(6) In the experiment, the authors adopt three metrics. For better comparison, the author should clarify that: the smaller the FDR/SHD is, the better the performance, and the larger the TPR is, the better the performance.\n\n(7) From the experimental results, the proposed method seems more superiors under the non-linear model case. Why? Could the authors give a few sentences about the guidance of the model selection in the real-world? i.e., when to select the proposed RL-based method? And under which case to choose RL-BIC, and which case to selection RL-BIC2?\n \n(8) What\u2019s training time, and how many samples are needed in the training process?\n \n \nMinor:\n1. In the page 4 decoder section, the notation of enc_i and enc_j is not clarified.\n\n2. On page 5, the \\Delta_1 and \\Delta_2 are not explained.\n\n3. For better reading experience, in table 1,2,3,4, the authors should bold value that has the best performance.\n "}