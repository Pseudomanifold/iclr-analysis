{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors proposed a new reinforcement learning based algorithm to learn causal graphical models. Simulations on real and synthetic data also shows promise.\n\nPros\n\n1. It's great to see the authors has done a comprehensive comparison with the other methods, especially under different simulation scenarios.\n\n2. The novel idea of applying reinforcement learning to DAG search sounds intriguing. Reinforcement learning offers a powerful tool for policy evaluation and decision making. It\u2019s good to see that the author can successfully extend such toolbox to the field of causal structure learning. To the best of the author\u2019s knowledge, such idea has never been considered by previous work in causal graphical models.\n\nCons. \n\n1. In the introduction section, the authors claimed that \u201cGES is not guaranteed in the finite sample regime\u201d. This seems to be incorrect. For example, the Nandy et al. paper tackles exactly the finite sample problem. \n\nIn conclusion, overall this is a sensible idea, although some of the preliminaries still remain to be polished."}