{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper uses NTK and techniques from deriving NTK to study asymptotic spectrum of Hessian both at initialization and during training. For understanding neural network\u2019s optimization and generalization property, understanding Hessian spectrum is quite important. \nThis paper gives explicit formula for limiting moments of Hessian of wide neural networks throughout training. \n\nIn detail, Hessian of neural networks can be decomposed into two components, denoted by H = I + S. In the infinite width I is totally described by NTK, and authors show that I and S are asymptotically orthogonal (both at initialization and during training).  Residual contribution is described by S, which captures evolution of Hessian by its first moments Tr (S) since Tr (S^2) remains constant and Tr (S^k ) for k>=3 vanishes. \n\nCorollary 1 has analytic dynamics of moment of Hessian in the case of MSE loss demonstrating power of this paper\u2019s main Theorem.  This is also supported by experiments in Figure 1. \n\nfew comments:\nAuthors should follow format given by ICLR style file. The paper is more dense than typical submission and may have violated page limit (10 pages max) if the style guide line was followed. \nSimilar to the prior comment, the reference section should be cleaned and formatted better. The reference doesn\u2019t count towards page limit and I don\u2019t understand the reason for them to be formatted badly and become eligible. \nIt would be useful if the Figure axes are more legible. \nThere have been many variations of NTK beyond vanilla FC networks(Arora et al. 2019, Yang 2019). Is there a major block for the analysis given in the paper to extend beyond FC networks? \n"}