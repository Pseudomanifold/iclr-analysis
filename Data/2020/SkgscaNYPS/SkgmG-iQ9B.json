{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper uses the Neural Tangent Kernel (NTK) to presents an asymptotic analysis of the evolution of Hessian of the loss (w.r.t. model parameters) throughout training. The authors leverage the Neural Tangent Kernel to analyze the evolution of the Hessian of the loss w.r.t the model parameters. Specifically the authors show that as the width of the neural networks tend to infinity, the Hessian can be decomposed into two components: (1) one that reflects the initialization of the model parameters and reduces as training progresses; and (2) one that captures the principal directions of the data.\n\nTechnical Soundness: The paper appears to be technically sound, though I did not go through the 14 pages of proofs. \n\nPotential Impact:\nI found the focus of the paper to be quite narrow which I think will negatively impact the impact that this paper could have. The authors take the reader on a notational taxing voyage through the decomposition of the Hessian to finally arrive at a result that is rather intuitive if not entirely obvious. I learned something about the evolution of the Hessian (in the limit of the infinitely wide NN), but none of. I can not say that this paper will have significant impact on how I think about NN training and thus I fear its impact will be relatively small. \n\nTo address this issue of significance and impact: What novel conclusions about Neural Network learning dynamics can you draw from your analysis? What are the implications for generalization or for future training algorithms? \n\nClarity: Beyond the possibly unduly heavy notation, the paper is rather clear and well written. There is a minor typo in the first equation of Sec. 2.3 (i -> j). \n"}