{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nModels such as BERT are pretrained language models which provide significant improvement for different tasks, however they suffer from high huge size and complexity. This paper has proposed using proximal gradient descent to find sparse weights for BERT to reduce the number of parameters and make the model smaller. They concentrate on the drawbacks of the previous sparse-based approaches and claimed that they have convergence issues (they have provided some evidence in the appendix). therefore, they propose to use reweighed sparse method and optimise it using proximal gradient descent which provides a closed form solution for sparse constraint. \n\nALthough proposing a minor novelty (reweighted sparse optimization ), they have provided interesting results for both pretrained structure and fine-tuning for several different tasks. they have also provided some visualisation for the weight matrices after sparsification.\n\nTheir results are notably stronger than simply adding the L1 regularizer to the optimisation method. \n\nThe paper is well written and easy to follow with nearly comprehensive related work.\n\nHowever, there are some drawbacks:\n\n1. They have claimed that \u201c To the best of our knowledge, we are the first to apply reweighted l1 and proximal algorithm in the DNN weight pruning domain, and achieve effective weight pruning on BERT. \u201d, however proximal optimization has been used for DNN in works like \u201cCombined Group and Exclusive Sparsity for Deep Neural Networks, 2017\u201d.  \n2. It should be explained clearly about all the matrices included in the sparsification steps, despite only saying \u201cparameters of the model\u201d.\n3. More analysis is required on the results, specially the diagrams for fine-tuning over different datasets.\n4. It is essential to compare the method with other related works for Bert and transformer compression, including quantisation-based, factorisation-based, pruning, knowledge distillation papers such as:\n--Prato, Gabriele, Ella Charlaix, and Mehdi Rezagholizadeh. \"Fully Quantized Transformer for Improved Translation.\" arXiv preprint arXiv:1910.10485 (2019).\n--Tang, Raphael, et al. \"Distilling Task-Specific Knowledge from BERT into Simple Neural Networks.\" arXiv preprint arXiv:1903.12136 (2019).\n--Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019).\n--Ziheng Wang, et al. \"Structured Pruning of Large Language Models.\"\n"}