{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper describes a convolutional network (and later, conv-recurrent network) for automatic transcription of polyphonic piano music recordings.\nThe architecture borrows from prior work (Elowsson 2018) in automatic transcription to exploit regularities among harmonically related time-frequency bins, and the empirical evaluation appears to demonstrate that it performs comparably (in aggregate, on the MAPS dataset) to the onsets-and-frames method (Hawthorne et al., 2018) that was trained on a much larger corpus.\nWhile the work here seems to be well executed, the technical presentation is unclear throughout the article, and there is no clear statement of how exactly this work differs from prior work.\nMoreover, there is no attempt at any qualitative error analysis, nor is any attempt made to explain what the model is capturing specifically to improve over existing methods.\nThese two factors ultimately obscure the take-home message of this paper, and I do not recommend acceptance at this time.\n\nGoing by the arguments put forward by the authors, the crux of this model appears to be the \"sparse convolution\" layer, which is not novel to this article, nor does it receive a high-level explanation.\nMoreover, there is no ablation study conducted in this work to try to quantify its efficacy, so the reported gains may be due to other features of the architecture.\n\nThe authors refer several times to the \"HCQT\" model of (Bittner et al., 2017), though their description of the method seems to conflate the input representation (HCQT) with the convolutional network described in the paper (\"deep salience\").\nThis makes for a somewhat confusing presentation of the literature, which is not improved by the fact that no comparison to that method is included here.\nThis seems strange, as from what I can tell, the ideas in this paper are not incompatible with the HCQT idea (or HVQT, if you will).\n\nThe empirical results in table 2 do indeed seem promising, primarily due to the reduced size of the training set compared to (Hawthorne et al. 2018).\nHowever, it does lead me to wonder if part of the issue here is domain adaptation from MAESTRO to MAPS.\nIt seems that the proposed model performs comparably to Hawthorne's (within 1% on F-score), but strikes a different balance of precision vs. recall.\nThe comparison to the closely related (Elowsson 2018) does show a large improvement, but there is no concrete explanation given for why; the penultimate paragraph of section 3.3 is quite vague on this point.\nIn both cases, some qualitative error analysis would go a long way to help illustrate where these different models disagree and why.\n\nFinally, there are various \"magic numbers\" sprinkled throughout and used without justification.  For example:\n    - Why 14.1 Hz as a minimum bandwidth in eq1?\n    - Why downsample the spectrogram by 22?\n    - Where do the lines in Figure 2 come from?  It's fine to refer readers to prior work for a detailed explanation, but this figure conveys almost no information."}