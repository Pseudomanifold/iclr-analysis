{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "A Harmonic Structure-Based Neural Network Model for Musical Pitch Detection\n\n*Overview* \n\nThis paper presents a new architecture for performing frame-wise pitch detection for piano music. Compared to existing models for this task, this study proposes a so-called \u201csparse convolution layer\u201d, where the distribution of weights along the frequency axis is informed by prior knowledge about the distribution of partial frequencies that constitute a pitch. The primary shortcoming of this study is the lack of experimental rigour while testing the main hypothesis in the paper that the sparse convolution layer results in improved accuracies. \n\n*Shortcomings of the Evaluation Section*\n\nThe proposed architecture has 13 layers including the input. The input is in the form of the variable-Q transform (VQT) while the output layer is of the form Tx88, where T is the number of input frames and each frame has 88 outputs between [0,1] that indicate the probability of the MIDI notes between [21,108] being on/pitched. The main novelty in the paper is the sparse convolution in Layer 8, where the distribution of weights along the frequency axis is inspired by the design proposed in Elowsson (2018). Note that this layer also contains the most number of parameters (256x1x79 = 20,224). Table 2 presents comparisons with the most popular models in the literature. \n\nMy main criticism of Table 2 is the fact that all the models have been trained using different data which makes a direct comparison of accuracies unsatisfactory. For example the Hawthorne et. al. 2019 model is trained on the MAESTRO dataset while the proposed architecture is trained on the synthesised data from the MAPS dataset only. The authors claim that the fact that the proposed model yields better results than the Hawthorne et. al. 2019 model is impressive since the MAESTRO dataset is 15x larger. However, I do not agree with this claim since the repertoire of training data in the MAPS dataset might be more suited to the eval set than the repertoire of training data in the MAESTRO dataset. Furthermore, all the models compared in Table 2 have different numbers of trainable parameters which can also contribute to the difference in eval accuracies. There might be other confounding factors that might contribute to this result and therefore the only way to control for them is by designing a better experiment. \n\nA first step towards this would be to replace the sparse convolution layer in the proposed architecture with a standard convolution layer of shape (256x1x79) with varying dilation factors along the frequency axis. This would allow the main hypothesis in the paper to be tested i.e. sparse convolutions somewhere in the middle of the acoustic model are better than regular convolutions and that knowledge of the distribution of harmonics is a useful inductive prior. Secondly, it would be extremely informative to train the proposed model on the MAESTRO dataset with a similar number of parameters as Hawthorne et. al. 2019 and then compare results on both the MAESTRO and MAPS eval sets. The MAESTRO dataset is freely available for download and the evaluation section would be significantly better if the proposed model was trained on both MAPS and MAESTRO datasets. \n\n*Minor Comments*\n\n1. The description of pitch and harmonics in Section 1.2 is extremely informal. Although I agree there isn\u2019t enough space for a rigorous motivation of these concepts, a couple of references here would be extremely useful for the reader. \n2. \u201cTo tell if a specific note is active, intuitively we will first check if the pitch and harmonics are light\u201d. What does \u201clight\u201d in this context? Why not directly say if there is some energy distributed along the frequencies? \n3. \u201cSince the constituent frequencies of harmonic patters are sparsely distributed, it is inappropriate to capture harmonic patterns\u201d. I don\u2019t buy this argument, an acoustic model with many layers can in theory capture complex distribution and patterns along the frequency axis. As mentioned before, the experiments do not do a good job of testing this intuition. \n4. In Section 2.2, the training, validation and eval sets for the MAESTRO dataset are mentioned, but is this dataset used for training any of the proposed models? I got the impression that only the MAPS dataset was used for training. \n5. \u201cFor pitch detection, we do not like the hop size to be too small, because in this case the adjacent frames in the spectrogram will be highly correlated. So we down-sample the resulting spectrogram by a factor of 22\u201d. I do not follow this argument. Why is it is detrimental for neighbouring frames/windows to be correlated? This has been stated without any reasoning. Secondly, don\u2019t we lose training data by downsampling by a factor of 22? Where does this factor come from? Given that training data is already very sparse, doesn\u2019t it make sense to train on higher resolution inputs? There should be more discussion about this choice. \n6. In Equation 2, sr which I assume is Sampling Rate has not been defined. \n7. \u201c..solely measured by f-measure\u201d, the F should be capitalised. \n8. Given that the precision, recall and F-measure is computed frame-wise, I don\u2019t understand the difference between the average and ensemble results clearly. From Table 2, the differences seem to be very minor. It would be useful to explain this more clearly. \n"}