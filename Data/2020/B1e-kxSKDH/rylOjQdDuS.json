{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper introduces a structured deep generative model for video frame prediction, with an object recognition model based on the Attend, Infer, Repeat (AIR) model by Eslami et al. (2016) and a graph neural network as a latent dynamics model. The model is evaluated on two synthetic physics simulation datasets (N-body gravitational systems and bouncing billiard balls) for next frame prediction and on a control task in the billiard domain. The model can produce accurate predictions for several time steps into the future and beats a variational RNN and SQAIR (sequential AIR variant) baseline, and is more sample-efficient than a model-free PPO agent in the control task.\n\nOverall, the paper is well-structured, nicely written and addresses an interesting and challenging problem. The experiments use simple domains/problems, but give good insights into how the model performs.\n\nRelated work is covered to a satisfactory degree, but a discussion of some of the following closely related papers could improve the paper:\n* Chang et al., A Compositional Object-Based Approach To Learning Physical Dynamics, ICLR 2017\n* Greff et al., Neural Expectation Maximization, NeurIPS 2017\n* Kipf et al., Neural Relational Inference for Interacting Systems, ICML 2018\n* Greff et al., Multi-object representation learning with iterative variational inference, ICML 2019\n* Sun et al., Actor-centric relation network, ECCV 2018\n* Sun et al., Relational Action Forecasting, CVPR 2019\n* Wang et al., NerveNet: Learning structured policy with graph neural networks, ICLR 2018\n* Xu et al., Unsupervised discovery of parts, structure and dynamics, ICLR 2019\n* Erhardt et al., Unsupervised intuitive physics from visual observations, ACCV 2018\n\nIn terms of clarity, the paper could be improved by making the used model architecture more explicit, e.g., by adding a model figure, and by providing an introduction to the SuPAIR model (Stelzner et al., 2019) \u2014 the authors assume that the reader is more or less familiar with this particular model. It is further unclear how exactly the input data is provided to the model; Figure 2 makes it seem that inputs are colored frames, section 3.1 mentions that inputs are grayscale videos (do all objects have the same appearance or different shades of gray?), which is in conflict with the statement on page 5 that the model is provided with mean values of input color channels. Please clarify.\n\nIn terms of novelty, the proposed modification of SQAIR (separating object detection and latent dynamics prediction) is novel and likely leads to a speed-up in training and evaluation. Using a Graph Neural Network for modeling latent physics is reasonable and has been shown to work on related problems before (see referenced work above and related work mentioned in the paper). Similarly, using such a model for planning/control is interesting and adds to the value of the paper, but has in related settings been explored before (e.g. Wang et al. (ICLR 2018) and Sanchez-Gonzalez (ICML 2018)).\n\nExperimentally, it would be good to provide ablation studies (e.g. a different object detection module like AIR instead of SuPAIR, not splitting the latent variables into position, velocity, size etc.) and run-time comparisons (wall-clock time), as one of the main contributions of the paper is that the proposed model is claimed to be faster than SQAIR. The overall model predictions are (to my surprise) somewhat inaccurate, when looking at e.g. the billiard ball example in Figure 2. In Steenkiste et al. (ICLR 2018), roll-outs appear to be more accurate. Maybe a quantitative experimental comparison could help?\n\nWhy does the proposed model perform worse than a model-free PPO baseline when trained to convergence on the control task? What is missing to close this gap?\n\nDo all objects have the same appearance (color/greyscale values) or are they unique in appearance? In the second case, a simpler encoder architecture could be used such as in Jaques et al. (2019) or Xu et al. (ICLR 2019).\n\nOverall, I think that this paper addresses an important issue and is potentially of high interest to the community. Nonetheless I think that this paper needs a bit more work and at this point I recommend a weak reject.\n\nOther comments:\n* This sentence is unclear to me: \u201cAn additional benefit of this approach is that the information learned by the dynamics model is reused for inference \u2014 [\u2026]\u201d\n* What are the failure modes of the model? Where does it break down? \n* How does the model deal with partial occlusion?\n"}