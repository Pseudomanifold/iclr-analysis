{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper proposes an efficient method to perform Riemannian optimization over the Stiefel manifold using an efficient computation of the Cayley transform via fixed point iteration. While simple, the method is to my knowledge novel. Empirically, the authors validate that their method is able to tightly enforce the orthogonality constraint with a reasonable computational budget. Further experiments highlight the benefits of orthogonality in deep learning but the explanations feels lacking. The paper is missing references to some related work but is otherwise well written.\n\nOverall:\n\n1) In abstract, \"This amounts to Riemannian optimization on the Stiefel manifold\". Other approaches exist to enforce orthogonality throughout the network. In general, I felt that this paper was missing related work which enforces orthogonality constraints in deep learning. Some examples: [1,2,3,4] though this is by no means exhaustive.\n\n2) In order for the fixed-point iteration to have guaranteed convergence, the \"learning rate\" for the contraction mapping depends on the largest singular value of the weight matrix $W_k$. Could you please clarify how this is computed in practice?\n\n3) I am not convinced that the second change made to the Adam algorithm is reasonable (at least, not if we wish to continue calling the algorithm Adam). Adam preconditions the gradient by an estimate of the diagonal of the Fisher information matrix. Algorithm 2 presented is closer to a spherical approximation to the Fisher. The algorithm still looks sensible to me but the name is perhaps inaccurate.\n\n\n4) I am a little unsure of the motivation behind the experiments. For experiments validating the efficiency of the proposed method this is clear but for the classification experiments this is less obvious. Are the authors claiming that orthogonality is a good regularizer? Or is the only benefit in optimization? I did not understand the proposed explanation that that the orthogonal weights do not affect eachother during backpropagation --- could you please clarify? Even if this were true, why would this encourage exploration? There is existing literature which suggests that orthogonal networks will be easier to train (see e.g. [5] and others).\n\n5) The orthogonality of the convolutional layers is enforced by reshaping the kernel and imposing an orthogonality constraint there instead. Unfortunately, this does not guarantee that the actual convolution operator is orthogonal (see e.g. [2] for an example in 1D and [5] which characterizes orthogonal convolutions correctly).\n\nMinor:\n\n- Section 2, TYPO: \"non-suquare parameter matrices\".\n- The momentum $M_t$ is used below Equation 2 before it is defined.\n\nReferences:\n\n[1] Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group, Mario Lezcano-Casado and David Mart\u00ednez-Rubio\n[2] Parseval Networks: Improving Robustness to Adversarial Examples, Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier\n[3] Orthogonal Recurrent Neural Networks with Scaled Cayley Transform, Kyle Helfrich, Devin Willmott, and Qiang Ye\n[4] Variational Inference with Orthogonal Normalizing Flows, Leonard Hasenclever, Jakub M. Tomczak, Rianne van den Berg, and Max Welling\n[5] Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks, Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz,  and Jeffrey Pennington"}