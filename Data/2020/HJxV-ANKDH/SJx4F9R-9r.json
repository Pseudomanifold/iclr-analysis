{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a fast algorithm to train a NN under orthogonality\nconstraints on the weights for each layer. Using a new retraction\nthe paper proposes an adaptation of SGD or ADAM on the Stiefel\nmanifold.\n\nThe idea of the paper is to use a truncated fixed point iteration to\nobtain the Cayley transform. By doing so one has an approximation\nof a cheap retraction by just doing some matrix vector products\n(no matrix inversion or SVD needed). The idea is seducing but I\nsee some difficulties due to this approximation.\n\nTheorem 2 proves that the relative gradient tends to zero but\nthis does not control how \"orthogonal\" are the obtained weights.\nBasically I fear that the proposed iterative solver moves\nthe iterates away from the manifold although some empirical\nresults in Table 5 suggest otherwise.\n\nFigure 1 should be replaced or complemented by a convergence plot\nin running time. What matters is that test error decreases\nfaster as function of time (not epoch). Because of this,\nexperiments are not fully convincing.\n\ntypos:\n\n- non-suquare -> non-square"}