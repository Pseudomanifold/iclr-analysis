{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper claims to propose a novel framework to measure the trust score of a trained DL model and a solution approach to improve the trust score during training. It provides a new dataset. \n\nThe writing of the paper needs to be improved. The technical contribution is minimal.\n\nIt is unclear what is the goal of the paper.  Is it suppressing additional task learning or privacy preserving? The problem setting does not seem to make sense. For privacy preserving, it makes sense to process the data with privacy preserving techniques and then provide the processed data to the prediction/classification model. What is the point of providing the original data to the classification model and let the classification model perform privacy preserving?  In such case, who is the target you protect the information from?\n\nThe work also assumes the knowledge of suppressing tasks, which is not very practical.\n\nThe trust score is self-defined. It does not seem to impact the classification model training. What is the point of performing evaluation in terms of such a trust score?\n\nThe experimental study is insufficient and unconvincing without comparison to related works. \n"}