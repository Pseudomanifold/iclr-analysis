{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a metric (called trust score) that measures how well a model is at performing multiple tasks. Further, the authors propose training the classifier in a way that minimizes the loss on a desire task while simultaneously maximizing the loss on random tasks (randomly generated labels). \n\nThere are a number of issues with this paper:\n\n1. The authors claim that models that generate data representations (features) that are capable of capturing multiple learning tasks are bad for privacy. This claim is wrong for the following reasons: (a) if the model is used on-device, then the data & model decisions are hidden from service providers and there\u2019s no privacy violation; (b) if the model is used in the cloud as a machine learning service, then the users have to send their data to the service provide, and the problem here would be that the service provider has access to the data (and not that the models are capable of learning sensitive attributes). This is why researchers in this field have focused on one of two directions: (1) designing approaches for creating representations of the data that do not contain sensitive attributes (such representations could then be shared with a service provider or published), and (2) ensuring that a model is \u201cfair\u201d with respect to protected attributes (say race and/or gender) \u2014 i.e., the performance of the model does not vary across different populations/groups. (Note: privacy is usually about training models on user data such that an attacker observing the model cannot recover the training data or discover who participated in the training of the model. This is typically achieved via differential privacy.) \n2. The way the trust score is calculated is unclear and poorly presented. (a) What do the authors mean by |M - T|? The determinant of M - T? The entry-wise absolute value? (b) It\u2019s also unclear how W is computed. If \\mathbb{1}_5 is a 5x5 matrix of ones and \\mathcal{I}_5 is the 5x5 identity matrix, then W is simple 4x\\mathbb{1}_5, which clearly is not the case. So what is \\mathbb{1}_5? (c) What is \\Sigma in Equation (2)? A summation?  (d) More importantly, the authors claim that an ideal classifier that does not capture suppressed tasks has a trust score equal to one. However, in this case T ~= M, so M - T ~= 0, and therefore |M - T| ~= 0 (regardless of how |.| is defined) and the trust score must actually be small. The way the trust score is computed needs to be properly explained.\n3. The authors claim that their proposed approach (maximizing the loss of randomly generated labels) does not require knowing the tasks to be suppressed. This is true. However: (a) you cannot compute the trust score of a classifier on a dataset without knowing the tasks that need to be suppressed (so you cannot verify whether or not the proposed technique is actually helpful in practice), and (b) this technique (unfortunately) hurts the performance of the model on the desired task.\n4. The paper makes no attempt to properly survey the literature on learning representations under censorship and fairness constraints. For example, they have not referenced and compared against: (a) Censoring Representations with an Adversary (https://arxiv.org/abs/1511.05897), (b) Learning Adversarially Fair and Transferable Representations (https://arxiv.org/abs/1802.06309), (c) Context-Aware Generative Adversarial Privacy (https://arxiv.org/abs/1710.09549), (d) Learning Generative Adversarial RePresentations (GAP) under Fairness and Censoring Constraints (https://arxiv.org/abs/1910.00411), and many others.\n\nWithout a clear story and a compelling argument for why we need to design models that aren\u2019t capable of capturing multiple multiple downstream tasks, I cannot convince myself that suppressing unwanted tasks is valuable. Rather, I believe the authors should focus on training the models in a way (a) that makes their decisions \u201cuncorrelated\u201d or \u201cconditionally uncorrelated\u201d with respect to unwanted/undesired tasks (to suppress unfairness) , or (b) that ensures that the data of users who participated in the training model is not leaked to an attacker who has a white-or black-box access the model.\n"}