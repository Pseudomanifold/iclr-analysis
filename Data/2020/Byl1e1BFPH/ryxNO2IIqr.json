{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper generalizes the differentiable particle filtering (DPF) to POMDP setting. Moreover, it introduces an extra differentiable SMC component as planner into the model, which together with the DPF will be learned together. The authors demonstrated the effectiveness of the proposed algorithm on 2D floor positioning, 3D navigation with image input, and Reacher in MuJoCo.\n\nAlthough the learning through the planner in an end-to-end fashion has been investigated in RL community extensively, e.g., [1, 2], extending it to POMDP is relative novel. However, there are several issues need to be addressed:\n\n\n1, The optimization objective (1) seems come from nowhere. The motivation of such objective is very unclear. Why this objective is selected? Why not to adapt other alternative objectives, e.g., maximum log-likelihood? Without such justification, the learning target is not convincing. Meanwhile, whether $\\psi$ is learned or not. If it is learned, what is the learning objective. \n\n2, The authors claim that by twisting two SMCs, for planning and state prediction, respectively, the algorithm will lead to better performance. However, after twisting the planner into the learning, the proposed method is only applicable for on-policy setting, while the major advantage of model-based RL is for off-policy. This effect has not been discussed explicitly. I am not sure whether the benefit of the proposed algorithm comes from the coupling of two SMCs or on-policy learning.  \n\n3, The empirical experiments should be improved to be more convincing. One of the major contribution of this paper is using POMDP, rather than traditional MDP. To justify the benefits of the complicated model, a direct comparison between model-free (DQN) model-based (DeepMDP [4] and PlaNet [3]) algorithm upon MDP with same inputs as the inputs of the proposed model is necessary. Moreover, just one MuJoCo is not sufficient to demonstrate the advantages. Please evaluate the proposed algorithm on more environments in standard benchmarks, e.g., MuJoCo and Atari.\n\nMinor:\nThe terminology \"dual\" has rigorous definition in math. Please use the terminology following the common knowledge.\n\n\n1, Okada, Masashi, Luca Rigazio, and Takenobu Aoshima. \"Path integral networks: End-to-end differentiable optimal control.\" arXiv preprint arXiv:1706.09597 (2017).\n\n2, Pereira, Marcus, et al. \"Mpc-inspired neural network policies for sequential decision making.\" arXiv preprint arXiv:1802.05803 (2018).\n\n3, Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. (2018). Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551.\n\n4, Gelada, C., Kumar, S., Buckman, J., Nachum, O., & Bellemare, M. G. (2019). DeepMDP: Learning Continuous Latent Space Models for Representation Learning. arXiv preprint arXiv:1906.02736."}