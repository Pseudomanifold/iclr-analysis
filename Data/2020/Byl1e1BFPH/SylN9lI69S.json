{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThe authors propose Dual Sequential Monte Carlo, a method to jointly perform filtering and planning on continuous POMDPs by nesting two smc filters.\n\nThe authors use a variety of pieces in their model. \nFirst, filtering is used to represent the posterior over current state states (or: belief state).\nSecond, the authors use the control-as-inference formalism in order to perform planning using SMC by sampling from the posterior of optimal trajectories where optimality is defined via auxiliary binary latent variables proportional to the reward state action pairs yield (and as such trajectories).\nThird, the authors propose to use \"Adversarial Particle Filtering\" as a method to train their observation models.\nFourth, the crucial difference to the main baseline is the inclusion of the mean belief state into the planner by using it both as an auxiliary input for the policy network (as a conditional action prior) and for state propagation during planning. The rationale for this is the hope that this will represent uncertainty in the filter better during planning.\n\nIn the experiments, the Dual SMC method tends to outperform the baseline, though at times it is unclear if this is owed to the adversarial model or the actual planner.\n\nComments:\nMy main issue with the paper is the role of the mean belief state used for dual smc. This has two major appearances:\n1.  I understand the idea that including moments of the state distribution particles together with each individual particle could inform a learned policy network better as to whether states are uncertain or not. However, it is unclear if the first moment is sufficient to do this or if another method would have been more advisable? In the end, little proof is given that this actually achieves the stated goal.\n2. The second use of the mean belief state, however, is hard to justify in my opinion. In algorithm 2 line 6, the mean belief state is used to propagate the planner forward given the current action. While I understand that this may save computation compared to using sampled state particles to propagate the posterior forward, this choice ultimately is not principled and causes the planner to be planning for a different model than the one one may want it to plan from, as the state is systematically reset to the mean. This seems to be a particularly optimistic planner and it is unclear how it would respond to multi-modal or long tailed state-spaces during planning. Mainly, it simply is not guaranteed to be planning according to the Bellman equations anymore but generates \"some plan\" which is close to the model but also departs in a way that is hard to quantify or to have guarantees on. Why change the model during planning?\nWhy is this choice necessary and what would have been the alternative? What does it buy us?\n\n3. The authors are not doing a good job explaining model training rigorously. A nice way to explain how to train parameters in a state space model while performing SMC on it is via variational inference with adaptive SMC proposal distributions. But even if that were not the chosen way here, the paper is not doing a good job clarifying the objective function for model training.\n\n4. Adversarial Particle Filters are presented as a contribution by the authors. However, these objects have been proposed before and explained, for instance in \"Adversarial Sequential Monte Carlo\" by Kempinska and Shawe-Taylor. This reference is clearly missing here. In addition, the presentation of the section on adversarial particle filters is confusing. The adversarial training in this case has one role only as far as I can tell: building an implicit observation model in order to achieve more accurate representation of observations through latent states. It would be easy to consider this a particular modeling choice (as opposed to a Gaussian or a Bernoulli observation model), as implicit observation models are quite common in the current probabilistic literature. Presenting it as an adversarial filter is not conveying that accurately.\n\n5. Neural Adaptive Sequential Monte Carlo by Shane Gu et al is also a missing reference here.\n\n6. In experiment 2, the model with a density-based observation model (presumably Normal) and Dual SMC wins. Why is the adversarial aspect so important to this paper if it is not even consistently the winning model?\n\n7. I am quite uncomfortable with the missing comparison to a planner that would keep the mean belief state for the policy network but actually plan from the model during the transition T_psi. I would have found that to be the most natural baseline here to also address my comment #2. As it stands, the two main contributions of the paper (adding mean belief state to policy network and planning with mean belief state transitions) are not individually well explained and individually evaluated.\n\nDecision:\nThe paper proposes an interesting and technically challenging attempt to jointly perform filtering and planning. However, some of the choices in the paper are not well motivated or technically corroborated, as outlined in my comments. I hope the authors can clarify these weaknesses of their paper, as particle-based methods are promising avenues to rigorous model-based planning in POMDPs and the authors are tackling an interesting problem. As the paper stands, I lean towards rejection due to the technical questions about the method."}