{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to solve multi-label problems via learning a share embedding space for instances and its label sets. Specifically, the author considers deep neural networks F(x) as an encoder for the instance (either raw input or features) and a shallow MLPs G(y) as an encoder for the label outputs. In the training stage, the instance embedding and its label embedding are forced to be close. An additional constraint is instances with different labels should be far from each other. After training, the inference can be done in the embedding space by looking up the labels of the query\u2019s kNN instances. \n\nMetric learning for multi-label problems is not new, many works have been proposed such as [1,2]. Using deep neural networks for multi-label problems is also not new, see [3,4]. Thus, the novelty of the proposed method is rather limited. The interesting part is the constraint of Eq(11), where kNN need to be updated whenever the instance encoder model F(X) is changing during the optimization, which is very expensive for large-scale application.\n \n[1] Sparse Local Embeddings for Extreme Multi-label Classification, NIPS 2015.\n[2] Learning Deep Latent Space for Multi-label Classification, AAAI 2017.\n[3] Deep Learning for Extreme Multi-label Text Classification, SIGIR 2017.\n[4] X-BERT: eXtreme Multi-label Text Classification with BERT, ArXiv 2019. \n"}