{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a metric learning approach for the multi-label classification problem. It basically maps the input features and the output labels into the same space and then uses k-NN to find the closest labels for each inputs. During training, it minimizes the squared Euclidean distance between the input embedding and label embedding. In the experiments, some image datasets and text datasets are used to compare with several multi-label learning algorithms. \n\nThe proposed method is clearly presented in the paper. However, I have several concerns.\n\n1. The proposed method is straightforward and lacks of significant novelty.\n2. The datasets are very small scale in terms of both sample size and label size. \n3. The comparing methods are a little bit out-dated. I would also suggest to compare directly with the naive one-versus-all method using deep learning extraction models. \n4. There are a few typos. For example, on Page 1, \"because it wide application\", \"but cannon deal\", \"may lost key information\"; on Page 2, \"an bidirectional\", \"mapping of outputs to metric space are\"; on Page 3, \"a scalable models\", etc.. Besides, Eq (1) should be written in a more formal way. \n\nOverall, I vote for rejection and the main reason is the lack of novelty. "}