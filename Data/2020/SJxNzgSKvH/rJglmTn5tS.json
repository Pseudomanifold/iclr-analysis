{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "### Summary of contributions\n\nThis paper aims to accelerate the training of deep networks using a selective sampling. \nThey adapt ideas from active learning (which use some form of uncertainty estimation about the class of the label) to selectively choose samples on which to perform the backward pass. Specifically, they use the minimal margin score (MMS). \nTheir algorithm works by computing the forward pass over a batch of size B (which is much larger than the regular batch of size b), compute the uncertainty measure for each sample, and only perform the backward pass over the b samples with the highest uncertainty. The motivation is that the backward pass is more expensive than the forward pass, and that by only performing this pass on a subset of samples, computations are saved. \n\n\n### Recommendation\n\nReject. The central premise of the paper is unclear, the writing/presentation needs improvement, and the experiments are not convincing. \n\n\n### Detailed comments/improvements: \n\n\nThere is a central premise of the paper that I don't understand: that the forward pass is much cheaper than the backward pass. \nThis is claimed in the intro by referring to charts that hardware manufacturers publish (but there are no references included), but I don't see why this should be the case. \nFor a linear network with weights W, the forward pass is given by the matrix-matrix product (rows of X are minibatch samples):\nY = XW^T\n\nand the backward pass is given by the two matrix-matrix products:\ndL/dX = dL/dY*dY/dX = dL/dY*W\ndL/dW = dL/dY*dY/dW = dL/dY*X^T\n\nSimilarly the two operations in the backward pass for convolutional layers are given by a convolution of the output gradients with the transposed weigtht kernels and the input image respectively. \n\nPoint being, I don't see why the backward pass should be more than 3x more expensive than the forward pass. A simple experiment in PyTorch confirms this: the code snippet pasted at the bottom shows that the backward pass takes only around 2.6x longer than the forward pass.  \n\nfprop: 0.009286s\nbprop: 0.0240s\nbprop/fprop: 2.5893x\n\nIn algorithm 1, it is assumed that b << B. For this to be effective the forward pass would have to be *much* faster than the backward pass for this method to yield an improvement in computation. Can the authors comment on where this justification comes from?\n\nI am unclear on what the purpose of Section 4.1 is. This shows that the MMS of the proposed method is lower than the other two, but this should be completely expected since that is exactly the quantity being minimized. \nThere are also several unsubstantiated claims: \"Lower MMS scores resemble a better...batch of samples\", \"the batches selected by our method provide a higher value for the training procedure vs. the HNM samples.\", \"Evidently, the mean MMS provides a clearer perspective...and usefulness of the selected samples\". What does higher value, usefulness, clearer perspective mean?\n\nMore generally, it is unclear if there is really any improvement in the final performance from using the proposed method.\nIn Figure 2, all methods seem to have similar final performance. \nIn Figure 5, is there a reason why the curve for MMS is cut off? How does its final performance compare to that of the baseline method in red? It looks like the baseline might be better, but it's hard to tell from the figure. \n\nWhy are the experiments with the entropy measure in a seperate section? Please include them along with the other methods in the same plot, i.e merge Figure 2 and Figure 4. \n\nMy suggestions for improving the experimental section are as follows:\n- include all methods together in all the plots/tables\n- repeat experiments multiple times with different seeds to get error bars. Include these both in the learning curves and in the tables. \n- It's hard to see small differences in the learning curves, so including tables as well is important. Include best performance for all the methods in the tables. \n\nFinally, in 2019 CIFAR alone is not longer a sufficient dataset to report experiments on. Please report results on ImageNet as well. \n\nOne of the central premises of the paper is acceleration in terms of compute/time. To make this point, there should also be results in terms of walltime and floating-point operations. Please include these results in the paper.  \n    \n\n\n\n### Code snippet timing forward/backward passes\n\n\nimport torch, torch.nn as nn, time\n\nmodel =\tnn.Sequential(nn.Linear(784, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 10),\n                      nn.LogSoftmax())\n\ndata = torch.randn(128, 784)\nlabels = torch.ones(128).long()\nt = time.time()\npred = model.forward(data)\nloss = nn.functional.nll_loss(pred, labels)\nfprop_time = time.time() - t\nt = time.time()\nloss.backward()\nbprop_time = time.time() - t\nprint('fprop: {:.4}s'.format(fprop_time))\nprint('bprop: {:.4f}s'.format(bprop_time))\nprint('bprop/fprop: {:.4f}x'.format(bprop_time / fprop_time))\n"}