{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes the h-potential, which is a solution to an objective that measures state-transition asymmetry in an MDP. Roughly speaking, in many situations some state transitions (s-->s\u2019) are more probable than their converse (s\u2019-->s), and if we have a function that assigns a higher value to a more probable transition (compared to its converse), then we can use it as a measure of the \u201creversibility\u201d of that transition. This function can then be used, for example, as an intrinsic reward signal; indeed, there may be cases where state transitions should be avoided if they are not reversible. \n\nThe authors tie these ideas into the notion of the arrow of time. They go on to explore the various nuances and subtleties of this measure, and demonstrate its behaviour empirically on a number of environments.\n\nThis paper was an absolute pleasure to read. The prose was clear, interesting, and nuanced. The authors anticipate many questions and do well to explain the various subtleties of their method. Overall the experiments are a nice demonstration of the presented ideas.\n\nI am inclined to give this paper a high rating, as there was very little that I felt was \u201cwrong\u201d or inaccurate. But I must also admit that some of the theoretical components are beyond my expertise, and so I can only be moderately confident. I will defer to other reviewers and any online discussion on these more technical matters.\n\nI have a few questions that I hope the authors can address.\n\n1) The method depends on a random policy (or, more accurately, was empirically validated mainly using a random policy, aside from some very simple environments as far as I can see). Can the authors comment on the usefulness of this method for a non-random policy in more complicated environments? Do they have any experimental results showing the effect of incorporating this into an agent also receiving (and learning from) exogenous rewards in an environment such as Sokoban (as it\u2019s used here already) or Atari? \n\n2) Related to the first point, how does this method scale to environments whose state-space can only be sparsely covered with a random policy? It seems in this case a task-relevant policy would be needed to explore more of the state-space, which would place pressure on the h-potential function approximator as it has to learn with sequentially correlated inputs. You can imagine something like an experience replay buffer being needed, but in any case, there are definitely unique challenges here not explored in the paper.\n\n3) Can the authors comment on the notion of a function being statistically monotonic vs. deterministically so, and whether the arrow of time is classically considered the former or the latter? My reasoning for this question comes from a place where I\u2019m questioning whether the motivation of the work can be simplified. The appeal to the arrow of time is nice and reads well, but it\u2019s also the case that this work can simply be interpreted as \u201clearned state transition reversibility\u201d, with the links to the arrow of time being more of a point of discussion.\n"}