{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "TLDR: The paper proposes architectural variations/engineering to speed up the PointNet and reduce the memory footprint, but the paper is riddled with biased coverage of related works, no comparisons against any of the recent works, and a serious misunderstanding of the basic concepts resulting in misleading names.\n\nThe paper proposes 1) grouping operation they named \"point convolution block\" 2) multi-scale hierarchy, residual link and hypercolumn like links.\n\n1) Point convolution block is not actually a convolution. The authors claim that MLP + MaxPooling is the counterpart of the 2D convolution. Mathematically, \n\nConvolution\n\n$\\sum K_{ij} v_{x+i, y+j}$ note that weight is different for all spatial locations\n\nMLP + MaxPooling over N-nearest neighbors\n\n$\\max_{i} Kv_{i}$ note that weight is common\n\n\nThe MLP + MaxPooling cannot approximate the convolution. Also, the authors proposed to reconstruct the activation while backpropagation, but ReLU and Dropout are non-reversible operations. How do you recompute the activation?\n\n\n2) The second contribution is multi-resolution hierarchy, residual, and cross-connections. However, these have been already proposed / used widely in 3D perception. There has been a large body of works that discussed how to increase the receptive field size by downsampling / strided convolution / strided pooling / dilated convolution. This is no exception in 3D. Strided 3D convolutions, poooling have been widely used. One example is the Monte Carlo Convolution [1] Fig.6 and all 3D sparse convnet [2] Fig.4 and [3] Fig.4 variants use the strided convolution for multi-resolution. Similarly, these networks have used the residual connections in 3D semantic segmentation which the paper heavily relies on. Finally, the cross-link was first proposed in [4] for semantic segmentation.\n\nHowever, in p.5, the authors claim that they observed the problems with no multi-resolution networks and claim that they came up with this idea inspired by the mechanism in biological vision.........\n\nAlso, the paper claims that they compared their method with SOTA methods on Table 2 caption, but they do not actually cite any SOTA methods. For example, [3] outperformed their method by a significantly large margin on the S3DIS dataset. Similarly, for the ScanNet, which has an online leaderboard, the authors do not report any of the SOTA methods that exceed their performance. Instead, they introduced new metrics that are not used in official benchmarks for ScanNet.\n\nOverall, the paper has a lot of technical issues and recommend rejecting this paper.\n\n\n- [1] Monte Carlo Convolution for Learning on Non-Uniformly Sampled Point Clouds, Siggraph'18\n- [2] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, CVPR'18\n- [3] 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR'19\n- [4] Hypercolumns for Object Segmentation, CVPR'15"}