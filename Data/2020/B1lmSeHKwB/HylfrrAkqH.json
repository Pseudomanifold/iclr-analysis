{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper proposes an interesting generalized zero-shot learning (GZSL) method called soft-labeled ZSL (SZSL) which trains the network with seen data over both seen and unseen class labels via soft-labeling. The network should output probabilities of samples on both seen and unseen classes during training when only seen class samples are available: for seen classes, the target probabilities are easy to get based on the ground-truth labels; for unseen classes, their desired outputs can be estimated based on the semantic similarities between seen and unseen classes (by means of attributes). They rescale the probabilities on seen and unseen classes to make them sum to 1. Overall, the idea is simple and easy to implement. Experiments on conventional ZSL benchmark datasets are conducted and the results demonstrate that SZSL can obtain on-par or better results. Further analysis on the choice of some hyperparameters is also performed.\n\n+Strengths:\n1. The paper is well organized and easy to understand with very few grammar or spelling mistakes. The method is very clearly explained with the companion of the illustration figure. Besides, the paper provides a very thorough review of the existing works in the introduction part (both classic and latest methods).\n2. The proposed method is simple but effective, which performs better or on-par with SOTAs on ZSL benchmark datasets under the evaluation of the harmonic average of accuracies.\n\n-Weaknesses:\n1. While the framework is relatively simple, there are too many hyperparameters and the selection of gamma and lambda are not mentioned in the implementation detail. How about the sensitivity of the framework to the hyperparameters? Which combination produces the best results in Table 2 since so many combinations exists.\n2. The hyperparameter q, which denotes the sum of probabilities attributed to unseen classes, is an important factor in the proposed method. The author showed the accuracy curves under different q in Figure 2, and the optimal performance is obtained with much different q on the 4 datasets. First, did the author choose the optimal q for different datasets when reporting the results in Table 2? I was wondering for a new GZSL task in the real world (we know the labels of unseen classes), we cannot do a grid search over q beforehand. In this scenario, how to choose the q based on experience from the authors?\n3. The proposed approach has some limitations in terms of novelty. It seems that the proposed approach can only perform GZSL task. Can it apply to ZSL? If yes, how to adapt to the ZSL task? The novelty of the paper mainly lies in assigning soft labels to the training samples by class similarities. The similar idea has been studied in some previous approaches [r1][r2]. Such relevant works should be discussed and compared with the proposed method.\n[r1] Sung et al. Learning to compare: Relation network for few-shot learning. In IEEE CVPR 2018.\n[r2] Jiang et al. Transferable Contrastive Network for Generalized Zero-Shot Learning. In IEEE ICCV 2019.\n4. In Eq. (3), there are 2 regularization terms which is not very common. Is it a typo or for some special consideration? Please clarify.\n5. On AwA1, CUB and SUN, the performance of SZSL via the harmonic average of accuracies is lower than COSMO and CRnet. It is not a problem since COSMO and CRnet may be complex compared to SZSL as illustrated by the paper. I was wondering if quantitative comparisons of the complexity (such as the number of weights) can be given.\n6. In Sec.4.3, the paper offers some explanation why the accuracy curves show different properties on different datasets. The blue dotted curves (accuracy on seen classes) seems to drop monotonically on AwA1 and aPY, but first increase and then decrease on CUB and SUN. Further analysis should be done.\n7. In Eq. (2), is the similarity score calculated via cosine similarity?\n\nMinor issues:\n1. Typo: pairwsie -> pairwise (2nd paragraph in 4.3).\n2. The spacing between items in the references should be larger for better view.\n3. In A.1 Line 3, ZSL/GZSl -> ZSL/GZSL\n\n\n"}