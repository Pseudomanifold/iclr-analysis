{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\n========\nThis paper proposes a simple method for generalized zero-shot learning where the objective at test time is to classify between seen and unseen classes, where for the seen ones there were both images and attributes available at training time whereas for the unseen ones there were only attributes available at training time. Their proposal is to first embed image features x into a \u201csemantic space\u201d g(x). There, multiplication with a fixed \u201cattribute matrix\u201d A yields \u201clogits\u201d for classifying x into each of C classes, where C is the union of seen and unseen ones. Their main contribution is a mechanism to mitigate the fact that the training images x will never belong to unseen classes, and therefore the learned semantic space may \u201coverfit\u201d to capturing features that are useful for classifying seen classes but not necessarily unseen ones. \n\nSpecifically, they propose a soft labeling method where each seen image is assumed to belong (to some small extent) to unseen classes whose attributes are similar to its semantic representation, instead of belonging (in a hard manner) only to its ground-truth seen class. The soft label of each example is a vector whose length is the total number of seen and unseen classes as usual, but with more than one non-zero entry. Specifically, it will have exactly one non-zero entry corresponding to a seen class (the ground truth class) and at least one non-zero entry corresponding to unseen class(es). The non-zero unseen entries sum up to q, which is a hyperparameter, and the non-zero seen entry therefore has value 1-q. Their overall objective constitutes minimizing two losses in a weighted fashion (governed by another hyperparameter). The first loss is the standard (hard) cross-entropy, and the second is another cross entropy using the soft labels instead.\n\nExperimentally, they obtain strong performance on standard benchmarks, especially in terms of accuracy on unseen classes when compared to previous methods. However, they observe a trade-off in performance on seen and unseen classes.\n\nReview\n======\nA) Limitation: requires knowing the unseen classes in advance\nThe authors discuss that the proposed soft labels approach is a supervised regularizer that is more effective than the previously-used unsupervised ones. However, it also has an important limitation that is not discussed: the proposed method is transductive in that it requires knowing in advance (i.e. during training time) the attributes of each unseen class. It therefore is not able to handle truly new classes at test time, e.g. in an online fashion.\nNotice that other methods don\u2019t have the same limitation. For example, consider having trained a generative model that given a set of attributes characterizing a new class can generate images of that class. Then, no matter what classes we are given at test time (as long as they are expressible in terms of the same set of known attributes), we will be able to generate a training set for it. As far as I understand, CRnet does not suffer from that limitation either.\nIt might be that in the current benchmarks the unseen class attributes are indeed available in advance, but this in general should not be expected. It should be noted that the proposal is a solution to a restricted version of the problem.\n\nB) Trading off performance on seen classes for performance on unseen classes\nThe ideal objective would use soft labels to obtain some training signal for related unseen classes of each training example, but without sacrificing the ability to classify that training example into its true ground truth (seen) class. In other words, soft labels should allow to improve on unseen classes in addition to (not instead of) improving on seen classes. However, the formulation in Equation 6 says that an example belongs to some extent to unseen classes *at the expense of* belonging to its ground truth seen class. This naturally degrades the performance on seen classes (indeed their results are competitive in terms of accuracy on unseen classes but not the best in terms of accuracy on seen classes), and gives rise to trade-offs regulated by the value of q.\nA more natural formulation would be to treat this as a multi-label classification problem where there is more than one \u201ccorrect\u201d class for each output: one is the ground-truth (seen) class, and the other(s) is/are the similar unseen class(es), in a soft manner. Perhaps having a formulation that captures this desideratum would also alleviate the need for having both L_hard and L_soft as weighted terms of the overall loss.\n\nC) Insufficient description of related work. \nA lot of works are cited but they are not described sufficiently. High-level statements like \u201csome methods utilize embedding techniques\u201d and \u201cothers use semantic similarity between seen and unseen classes\u201d aren\u2019t sufficient in my opinion to get a sense of what those methods did and how the proposed one relates to them.\n\nD) At the end of page 3, just above section 3.3, in the definition of q, should i also be marginalized out? Otherwise q is dependent on a particular seen class i?\n\nE) Aside from being non-trainable, there is no description of the matrix A. Is each attribute vector a_k associated with a particular class a binary vector, i.e. each entry is a 1 iff that attribute is present for the particular class? Are attributes instead numerical values, e.g. number of legs in animals? It would be good to state any assumptions. \n\nF) In Equations 6 and 7, L^{soft}(x) is a function of x, but x does not appear on the right hand side of those equations.\n\nG) The analysis of the effect of \\tau is not too insightful - this behavior is what is expected of a temperature parameter. It would have been more useful in my opinion to use that space to discuss the effect of \\alpha, which balanced L_soft and L_hard in the overall objective.\n"}