{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: In this paper, a diffusion-based VAE is proposed. The authors introduced a non-linear dimension reduction method, diffusion map to standard VAE to encode neighborhood manifold information into the latent space before encoding. They proposed a lower-bound objective, similar to original VAE for content generation.\u00a0\n\nPros: 1) A new VAE method is proposed to incorporate local manifold information into the latent space. 2) A sufficient condition to measure the consensus between latent and input data distributions. 3) Three empirical studies on the visualization of generated images.\u00a0\n\nCons:\u00a0\n1) The writing could be significantly improved. There are a bunch of grammar errors and confusing notations. For example, \u201cwith many default priors the posterior/likelihood pair q(z|x)/p(x|z) can be viewed as an approximate homeomorphism\u201d -> \u00a0\u201cwith many default priors,\u00a0 the posterior/likelihood pair q(z|x)/p(x|z) that can be viewed as an approximate homeomorphism\u201d. \u201cIn this paper address issues in variational inference and manifold learning\u201d -> \u201cIn this paper, we address issues in variational inference \u2026.\u201d. \u201cfeedfoward pass\u201d -> \u201cfeedforward\u201d \u2026. In algorithm 1, \u201cX is a random batch from X\u201d. Conditional probability are mixed with joint probabilities, \u201cp(y|x) = p(x,y)\u201d. I suggest the authors do careful proofreading.\n\n2) The novelty in this paper is limited.\u00a0 The diffusion VAE was proposed in Rey\u201919 https://arxiv.org/abs/1901.08991 with a similar random walk procedure with transition kernels on the manifold of input. However, the authors neither did any comparison with it nor provided convincing advantages over them.\u00a0\n\n3) The authors claimed the standard VAE has too many assumptions on the priors, likelihood, and posteriors. However, their framework also assumed Gaussian distribution on posteriors and likelihood, only eliminating the prior distribution, but at the expense of introducing an assumed kernel and eigendecomposition approximation.\u00a0\n\n3) The experiments are very limited, containing only 3 visualization results of 3 image generation tasks. The Fig 2 is difficult to read and interpret. How about the log-likelihood estimates from your approach compared with others? "}