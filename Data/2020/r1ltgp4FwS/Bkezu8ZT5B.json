{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary:\nThis paper proposes a training objective for higher quality video generation for the tasks of Video Super Resolution (VSR) and Unpaired Video Translation (UVT) and also two evaluation metrics tOF and tLP. They provide a comprehensive ablative study of the proposed method and also show comparisons against baselines for the tasks of VSR and UVT.\n\n\nPros:\n+ Novel video generation method for VSR and UVT.\n+ Novel metrics\n+ Well written paper\n\nWeaknesses / comments:\n\n- Confused about inputs to discriminator (I_{w,g} and I_{w,b})\nI am not sure I understand the inputs I_{w,g} and I_{w,b} to the discriminator network. Based on the description, I_{w,g} = {W(g_{t-1}, v_t), g_t, W(g_{t+1}, v_t^\u2019)} = {g_t, g_t, g_t}, assuming v_t^\u201d means reverse flow. A similar process seems to be happening with I_{w,b}. If this is the case, will the discriminator optimally get the same frame concatenated together (i.e. {g_t, g_t, g_t})? Now, if that\u2019s the case, how is the discriminator modeling temporal consistency other than making the generator learn to put pixels forward and back in place since the \u201coriginal triplets\u201d (real data) are {g_{t-1}, g_t, g_{t+1}} and {b_{t-1}, g_t, g_{t+1}}, respectively. This is very confusing to me. It would be good if the authors can clarify this in the rebuttal.\n\n\n- How do you prevent the zero output in PP loss?\nThe PP-loss compares forward and backward prediction by || g_t - g_t^\u2019 ||. If not careful, the neural network can just learn to output zeros or the same frame for the full video. Did the authors see any behavior like this? Or did the proposed formulations prevent this? Or was there a very small weight applied to this loss?\n\n\n\n- TecoGAN vs baselines generator parameters (rather than TecoGAN^{-}).\nBased on the experimental section, it looks like TecoGAN is the main network being compared to the baseline methods. TecoGAN has more parameters in the generator compared to TecoGAN^{-}. Did the authors make sure that the generator had the same number of parameters as the other methods? The authors mention a performance difference between TecoGAN and DUF due to DUF having more parameters, but what about the other baselines?\n\n- Evaluation metric contributions in the supplementary material?\nThe description of the two proposed metrics tOF and tLP have been placed in Appendix B. These are mentioned as contributions of the paper so their description should be in the main text. Please make sure that Appendix B is in the main text in future versions of this paper.\n\n\n- UVT task evaluation in the supplementary material?\nThe UVT task is mentioned in the abstract and as a target task in this work, however, the evaluations for this are in the Appendix. Please move them to the main text in future versions of this paper. Secondary experiments should be in the Appendix but I feel this is primary.\n\n\n- UVT task only evaluated with the proposed evaluations?\nThe UVT evaluation in the Appendix only has the proposed metrics as evaluation. I understand that, since it\u2019s an unpaired video translation task, there is no ground truth to compare against. However, a human based study could be done where human raters would judge for the more realistic of N videos.\n\n\n- The same qualitative comparisons in the paper are not in the provided website.\nThe provided website does not have the same qualitative evaluations as the paper does. For example: There are 7 methods being compared for the same video in Figure 8 in the supplementary material, but I don\u2019t see anything like that in the website.\n\n\n- For the UVT task, we omit the DsDtPP model because \u2026\u2026.\nIn the paper, the authors mention that they don\u2019t provide the DsDtPP baselines because it requires a lot of computation. However, I feel these missing baselines make the experiments incomplete since it\u2019s a different task.\n\n\n- Figure 5 has no point of reference with which to compare the frames.\nFigure 5 points out that the video b) is better than video a). While it is true that one seems more noisy than the other, there should be a point of reference for readers to make sure of it.\n\n\nConclusion:\nIn conclusion, the paper seems to present a novel method and evaluation metrics but has many issues as stated above. It would make the submission better if the authors can address them in the rebuttal."}