{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper presents a novel method for training video-to-video translation (vid2vid) models. The authors introduce a spatio-temporal adversarial discriminator for GAN training, that shows significant benefits over prior methods, in particular, parallel (as opposed to joint) spatial and temporal discriminators. In addition the authors introduce a self-supervised objective based on cycle dependency that is crucial for producing temporally consistent videos. A new set of metrics is introduced to validate the claims of the authors.\n\nI really like this paper. Although the method is a rather complex mix of multiple losses, these are justified in detail, both intuitively and empirically. The appendix is filled with much more detail about implementation, architecture and more results. Finally the results show that the proposed method is superior across the board compared to previous approaches from the literature.\n\n\nStrenghts:\n- The approach works on two distinct applications of vid2vid.\n- Detailed ablations justifying the introduction of every single part of the overall objective.\n- Strong results.\n- Clear writing and presentation.\n- A lot of additional details and results in the appendix for the more interested reader.\n\nWeaknesses:\n- Rather complex overall objective\n- Seems to need a lot of tweaking\n\nQuestions:\n- Isn't the PP loss just an incarnation of cycle consistency loss? If so, maybe there is no need for the introduction of a new name for it.\n"}