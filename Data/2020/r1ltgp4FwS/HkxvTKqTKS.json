{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Contribution\nAugments the loss of video generation systems with a discriminator that considers multiple frames (as opposed to single frames independently) and a new objective termed ping-pong loss which is introduced in order to deal with \u201cartifacts\u201d that appear in video generation. The paper also proposes a few automatic metrics with which to compare systems. Although the performance does not convincingly exceed its competitors, the contribution seems to be getting the spatio-temporal adversarial loss to work at all.\n \nOverall\nI found the video generations impressive, and the addition of the discriminator seems to improve sharpness of the individual frames over methods trained with non-adversarial losses, in particular DUF. Although TecoGAN does not beat its competitors (for example DUF) on most proposed automatic metrics, it appears to be ranked better in expectation in user studies (Table 2). However, I am concerned about generalizability of the method.\n \nDecision\nAs someone not in this field, I find it hard to judge the performance of the system from so few samples. I also found the differences in the losses between tasks to be worrisome, as it indicates this method does not generalize without heavy tweaking of the loss. My current decision is weak reject, since the generations look quite good but I have concerns about generalization of the approach to other video generation tasks as well as the justification of the ping-pong loss.\n\nQuestions\n- Is RecycleGAN unable to be applied to video super-resolution? Why is it not compared to in Table 2?\n- Is L_Phi the perceptual loss? I don't think it was mentioned in the body of the text.\n- I found the ping-pong (PP) loss to be unjustified. The loss is motivated by the issue of \"artifacts\", which I assume are poor generations due to lacking a good model of the world. The paper says this issue could be alleviated by training with longer video sequences, but that would prevent the generator from working with sequences of arbitrary length. I do not believe the ping-pong loss allows the generator to work with arbitrary sequences, as training only on short sequences and their reverse should not allow the model to generalize to longer sequences. Additionally, this approach would likely not scale to long sequences as well, since it requires doubling the sequence length. Would you be able to train on longer sequences?"}