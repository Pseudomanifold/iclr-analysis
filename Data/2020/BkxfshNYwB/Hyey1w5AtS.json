{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a solution to the important problem of pooling in graph neural networks. The method relies on minimizing a surrogate function inside the standard SGD loop and in conjunction with the optimization of the model parameters - such loss function aiming at optimizing the minCut on the graph. By that it aims to effective achieve a soft clustering of nodes that are both well connected and that have similar embeddings. This in an elegant choice, somewhat resembling the DiffPool method since it's also end-to-end trainable. However it adds the local graph connectivity information due to the minCut loss (and related orthogonality penalty to achieve non trivial solutions on the relaxed minCut continuous problem). Such local graph connectivity is indeed important information to consider when carrying out pooling.\nResults show good performance improvement on different tasks of graph clustering, node and whole graph classification. The paper is well written and clear to read. The math is solid and the concept is well substantiated by results.\nI found no mention about code release and I would solicit the authors to release the code to reproduce the experiments."}