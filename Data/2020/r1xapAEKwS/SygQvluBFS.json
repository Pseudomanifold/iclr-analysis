{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a classifier, called SDGM, based on discriminative Gaussian mixture and its sparse parameter estimation. \nThe method aims to have a flexible decision boundary due to the mixture model as well as to generalize well owing to the sparse learning strategy. \nWhile the method incorporates distinct ideas in the literature such as increased model complexity and use of a sparse prior, the paper needs more clearer explanation of the method design and careful empirical investigation. \n\nMy major concerns are as follows. \n1) Explanation of the learning algorithm of Section 2.2 is unsatisfactory. \nSpecifically, the objective function and the principle of algorithm are unclear. \nMy guess is:\n* c, m and T, z are equivalent. (T, z are one-hot representation of c, m.) \n* The objective is to maximize the conditional probability of equation (1) given labeled data (x, t). \n* This maximization is carried out in a similar way to the EM algorithm where m (or z) is regarded as a latent variable. This gives equation (9). \n* A MAP estimate of parameter w is calculated using the prior (8). \nAssuming the points above, I am still unsure about the following points: \n1-a) How is the sparsity induced? \nThe prior of w seems to have the l2 regularizer, but how are \\pi and r pulled toward zero?\n1-b) How is the update of \\alpha (17) derived?\nDoes this strengthen the sparsity by increasing \\alpha given small \\hat{w}?\nWhat is the \"orthogonal component\" of \\Lambda?\n1-c) Is structure in w ignored?\nEquation (7) indicates that parameter w has a certain structure such as nonnegative s_{cmii} or the determinant |\\Sigma_{cm}| interacting with s_{cmij}. \nIn other words, the degree of freedom in w assuming the Gaussian likelihood of x is smaller than H. \nThese structure would be violated if the gradient descent or Newton's method is applied. \nDo you mean by *discriminative* that we can freely set the parameter w?\nThen, this point should be emphasized. \n\n2) Which parameter is learned when combined into NN?\nParameters w and \\pi? \nHow are these parameters made sparse in the end-to-end learning?\n\n3) Ablation test to investigate which aspect impacts the performance. \nSection 4 describes SDGM incorporates disciminative model, mixture model, and sparse Bayesian parameter estimation. \nIt is more informative to provide empirical results to see the impact of each property by comparing SDGM with, for example, RVM, discriminative GMM and sparse GMM. \n\nSome other commets follow.\n* \\sqrt{\\alpha_{cmh}} may be missing from the numerator of (8). \n* For what distribution the expectation with regard to z is taken in e.g. (9, 11, 12, 13, 16)?\n* What is D for CIFAR-10 experiment? DenseNet seems to use D=1000 units for the fully connected layer. Did the authors adopt this value?\n\n"}