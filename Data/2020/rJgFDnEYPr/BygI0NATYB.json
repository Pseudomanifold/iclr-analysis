{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a method to train a network for counting/localizing objects in a weakly supervised framework.\nThe network is optimized based on weak supervision about target object counts; training images have only the ground truth of the total number of object counts without their positions in images.\nThrough analyzing the feature maps of the network trained on toy bubble images, the authors propose two regularization techniques for rotation robustness and sparseness of the map in order to improve  performance of object localization in the feature map.\nThe experimental results on object counting tasks using the Mall dataset show that the proposed method produces favorable performance.\n\nThis paper is leaning toward rejection due to the following two reasons.\n(1) The presented techniques are limited to the specific task of object counting.\n(2) They are derived in an ad-hoc way based on less theoretical background and thus lack novelty.\n\nThe detailed comments are as follows.\n\n* The technique to embed rotation invariance by Eq.(4) is presented in an ad-hoc way. This seems to be limited to the tasks on toy data such as bubble images shown in the paper. In the real-world tasks/images to count the more complicated objects, the spatial correlation among object parts is an important clue to provide discriminative features for detecting objects such as torso below the head in human figures. Such spatial dependency could be missed by imposing the rotation invariance on the network.\n\n* The regularization by Gini impurity is a well-known technique to induce sparsity, lacking novelty. There is a large body of researches for enhancing sparsity, and thus to validate the regularization, the authors should compare the Gini impurity with the other sparsity-inducing regularization such as entropy. For example, it could be possible to argue the regularizations in terms of their derivatives (gradients) to be used for back-propagation. This paper lacks detailed analysis and discussion about the regularization in this counting framework.\n\n* It is unclear how the perspective from the (beta-)VAE contributes to the analysis of the proposed method. There seems to be less connection between Eq.(8) and the object counting task/framework. The authors just show the similarity between the presented method and VAE in terms of formulation. Since the proposed method belongs to just a simple optimization with sparsity-inducing regularization, the reviewer cannot find any convincing reason to discuss the connection to VAE.\n\n* The method is not fully validated in the experiments. The authors provide only one experimental result on the Mall dataset which is insufficient to validate the effectiveness of the proposed method. Considering that the method is limited to the specific task of object counting, it is necessary to qualitatively and thoroughly evaluate the performance on various datasets that exhibit various environmental conditions regarding such as lighting and occlusion.\n\nMinor comments:\n- Show the index for summation in Eqs.(1,4,6,7,8).\n- It is confusing to show q_\\Theta(D|I) in Eq.(3), even though the authors aim to discuss the connection to VAE. Show the mathematical definition of q_\\Theta(D|I) before that."}