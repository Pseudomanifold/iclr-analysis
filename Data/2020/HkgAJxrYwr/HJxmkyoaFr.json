{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes an aggregation algorithm, based on repeated median regression and residual-based weighting to defend federated learning from adversarial attacks. Experiments are shown to demonstrate the method robustness against label-flipping, backdoor and Gaussian noise attacks.\n\nThe paper is interesting, the topic recent and the methodology quite new, but a number of comments arise: \n\n1) the methodology seems to rely on a number of ad-hoc steps, hyper parameter-dependent, that might hinder reproducibility and generalization: i) residual normalization via IRLS; ii) confidence assignment; iii) extreme value correction. Could be interesting to show analysis how varying such hyper-parameters affects the results, or otherwise to add further explanation at the comments in Appendix A.3 as to why the model seems to be insensitive to \\lambda, or why \\delta is significantly affected by data distribution.\n\n2) could the repeated median estimator still be affected by the \u201cfederated size\u201d i.e. the number of models involved in the federated learning? Is there any bound on the number of participants, below which the estimator would perform poorly?\n\n3) proof of eq (14) could be more readable if\n\t\u2022\tfull passages were shown (for instance for the reviewer the first passage was not immediate and took adding and subtracting \\sum_i z^(i)\\mu/\\sum_i z^(i), and further simplification to be addressed), and\n\t\u2022\tii) reference to the exact point in which previous results are used were made explicit (i.e. where in (Yin et al 2018) the bounds are proven).\n\n4) proof of eq (14), when the attacker a\\in B is fixed, then |\\hat{y}^(i) - \\tilde{y}| should be replaced by  |\\hat{y}^(a) - \\tilde{y}|\n\n5) A last question concerns the aspect of \"fairness\" of this learning strategy. By removing aberrant updates there is still a chance of excluding from the learning process nodes that are intrinsically different form the average ones. In this sense, it is not clear from the paper how the reweighing strategy can mitigate this aspect, as there is no certainty that underrepresented data samples would not be rejected with the proposed scheme. Still aspect could have been better investigated in controlled scenarios.\n"}