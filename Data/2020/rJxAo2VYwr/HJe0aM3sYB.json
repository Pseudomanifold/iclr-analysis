{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "\nThe paper proposes a new transfer attack method (on undefended models), called FDA. The method uses auxiliary class-wise binary classifiers attached on intermediate layers, and optimizes the targeted probability of the binary model. \n\nThe proposed method is simple and shown effective according to the results in Table 1. But I have some serious concerns about the method and results.\n\nConcerns\n1. One weakness is the paper didn't seem to justify why using binary classification models on intermediate features is better than just using the final classification result in the original model. The authors should have an apple-to-apple comparison with optimizing the probability using the original model, including the \"ms\" variant, and also explain why using the auxiliary model is better than that. Even if you use the original model it is still utilizing the distribution of intermediate features, just through the main branch, so this reason alone does not seem convincing to me.\n\n2. The baseline results seem too low (tpgd and tmim in Table 1, <10% target matching success rate). The FDA's result is ~20%. I'm not too familiar with the targeted adversarial attack literature, but one of the very early paper on targeted attack [1] seems to report much higher target matching rate (e.g., see their Table 3, ~70,80%). Why is this the case? What am I missing here? Is this just because they used an ensemble approach? If so you should also use ensemble to compare. Also the authors are welcome to want to point me to more recent papers' results and show their paper's result is better, given [1] is back in ICLR 2017.\n\n3. One drawback of the method is it needs to train a custom model by oneself first, with one binary model for each class we want to attack or target. This can be very cumbersome in practice. For 1000 classes in ImageNet it is not possible to cover all of them. But other conventional methods don't need to worry about this, they just operate on a pre-trained model and supports any source and target class. I doubt whether the method is practically useful. This point should be acknowledged in the paper.\n\nOverall, due to the concerns raised above (especially 2), I tend to vote a reject to the paper. I'm happy to reconsider my rating after the discussion period.\n\n[1] Delving into transferable adversarial examples and black-box attacks.\n"}