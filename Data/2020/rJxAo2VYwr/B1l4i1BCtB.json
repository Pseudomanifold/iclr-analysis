{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an adversarial attack based on the feature representations at different layers given the classes. Instead of only looking at the final layers, class samples in intermediate feature space information is used to attack and increase transferability at the same time. Then the noise is optimized to perturb the input so that the a specific wrong output will be more likely to chosen. The paper is clear and well-written and different interesting experiments support the claims.\n\n1-\tIntermediate feature space is not fully independent of the architecture of  the model. However, the results in Figure 2 shows schematically the same behavior for tSuc for different models. I am wondering how different the results would be if adversarial attack was trained on the black box model. In other words, as a simple testing for example, how much higher the success rate would be in the case of DN121->RN50 if you trained the noise on RN50 itself?\n2-\tFor different scenarios explained in figure 2, the optimal layer is usually one of the intermediate ones, and it might change from case to case. To find the optimal layer, you needed to have access to the black box model. How possible do you think it is to make it black-box model agnostic? Maybe at the cost of less success?\n3-\tIn the Figure captions you have \u201cFigure\u201d and in the text it is Fig. please make it consistent.\n"}