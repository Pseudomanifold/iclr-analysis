{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use a \u2018slot-based\u2019 (factored) representation of a \u2018scene\u2019 s.t. a forward model learned over some observed transitions only requires sparse updates to the current representation. The results show that jointly learning the forward model and the scene representation encourages meaningful \u2018entities\u2019 to emerge in each slot. Additionally, the paper argues that this representation allows for better generalization and can also guide exploration by rewarding actions that change multiple entities\n\nI really like the overall idea i.e. jointly learning the scene representation and the transition model, while enforcing sparse transitions. The experiments in Sec 4.2 and the visual results in Fig 3 do clearly highlight the benefits of joint learning, and show that the emergent representations are more meaningful compared to learning representations independently.\n\nHowever, while the overall approach is intuitive and seems to yield desirable results, I have concerns regarding the experiments, comparisons to prior work, and the exact contributions of this work. Specifically:\n\n1) It is unclear what this paper is claiming to improve over prior work: is the goal to a) learn a good forward model, or b) show that emergent entities allow better downstream tasks. If it is \u2018a\u2019 , then there are several other ways of learning good forward models e.g. convolution flow-based [1], and simply showing comparisons to a naive baseline is not sufficient. Similarly, the only downstream task examined is exploration, and again the only comparison is against a variant of the method. Therefore, while the results regarding the emergent representation are good compared to a variant without joint training, they are not shown to be useful in context of any task when compared to the approaches in literature.\n\n2) Despite the motivation in the introduction regarding applications to RL, the paper essentially learns a specific form of factored forward model. There have been several prior works which also pursue a similar approach (though mainly in context of video prediction) e.g. [2], and I don\u2019t think this paper\u2019s approach is novel in context of these. In any case, some comparisons should be made to these form of models which also show emergent entities with a graph-structured transition model.\n\n3) While the experiments in Sec 4.2 clearly demonstrate the benefits of the approach, the ones in Sec 4.1 and 4.3 are less convincing:\n3a) Sec 4.1 shows that the slot based transition model generalizes better, but this is only in comparison to a naive fully-connected baseline. I feel any prediction model with some structure e.g. graph-based forward model, convolutional forward model etc. would also similarly generalize.\n3b) Sec 4.2 argues that this slot-based representation can help exploration, but this is in fact a chicken-and-egg problem, as one needs to have collected interesting transition samples for a good representation to emerge. In fact, the results shown in Fig 6b) show that actions affecting 3 blocks were not explored, perhaps because the random transitions did not sample these to begin with.\n\nOverall, the approach proposed is desirable, but there are closely related alternatives that exist in literature, and there need to be more concrete comparisons against these for either prediction quality or success in downstream task.\n\n---\nReferences:\n[1] Self-Supervised Visual Planning with Temporal Skip Connections, Ebert et. al, CORL 17\n[2] Learning to decompose and disentangle representations for video prediction, Hsieh et. al., NeurIPS 18"}