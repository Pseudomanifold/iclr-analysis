{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new algorithm - Projection based Constrained Policy Optimization, that is able to learn policies with constraints, i.e., for CMDPs. The algorithm consists of two stages: first an unconstrained update for maximizing reward, and the second step for projecting the policy back to the constraint set.  The authors provide analysis in terms of bounds for reward improvement and constraint violation.  The authors characterize the convergence with two projection metrics: KL divergence and L2 norm.  The new algorithm is tested on four control tasks: two mujoco environments with safety constraints, and two traffic management tasks, where it outperforms the CPO and lagrangian based approaches.\n\n\nThis is an interesting work with impressive results.  However, this work still has a few components that need to be addressed and further clarification on novelty.  Given these clarifications in an author's response, I would be willing to increase the score.\n\n\n1) Incremental work\nThe work extends the CPO [1] with a different update rule. Instead of having the update rule of CPO that does reward maximization and constraint satisfaction in the same step, the proposed update does that in two steps.  The theory and the algorithm stem directly from the original CPO work, including appendix A-C. The authors claim that another benefit of PCPO is that it requires no hyper-tuning, but same is true for CPO (in the sense that they both don\u2019t need Lagrange multiplier) . \n\n\n2) The utility of the performance bounds and fixed point\nThe performance bounds depend on the variable $\\delta$, which is never explained. I\u2019m assuming it is the same $\\delta$ that is used in Lemma A.1. In that case, Theorem 4.1 tells about the existence of the fixed point of the algorithm under the assumptions specified Sec 4 (smooth objective function, twice differentiable, Hessian is positive definite).  There is no discussion regarding the comparison of the fixed-point of the algorithm with the optimal value function/policies. Also, all the analysis is with Hessian, whereas in the algorithm the Hessian is approximated via conjugate descent. \n\n\n3) How is line-search eliminated? \nOne of the benefits of the proposed algorithm is that it doesn\u2019t require line search (Sec 1). The underlying algorithm is still based on monotonic policy improvement theory in general, and more specifically on TRPO, so it should still have line-search as part of the optimization procedure.\n\n\n\n\nReferences: \n\n[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of International Conference on Machine Learning, pp. 22\u201331, 2017.\n"}