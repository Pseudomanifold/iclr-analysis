{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a technique to handle a certain type of constraints involved in Markov Decision Processes (MDP). The problem is well-motivated, and according to the authors, there is not much relevant work. The authors compare with the competing methods that they think are most appropriate. The numerical experiments seem to show superiority of their method in most of the cases. The proposed method has 4 main variants: (1) define projection in terms of Euclidean distance or (2) KL-divergence, and (a) solve the projection problem exactly (usually intractable) or (b) solve a Taylor-expanded variant (so there are variants 1a,1b,2a,2b).\n\nUnfortunately, I do not feel well-qualified enough in the MDP literature to comment on the novelty, and appropriateness of comparisons. For now, I will take the authors' word, and rely on other reviewers.  The motivation of necessity of including constraints did seem persuasive to me.\n\nOverall, this seems like a nice contribution based on the importance of the problem and the good experimental results, hence I lean toward accepting.  I do have some concerns that I mention below (chiefly that the theory presented is a bit of a red herring), but it may be that the overall novelty/contribution outweight these concerns:\n\n(1) Concern 1: the theorems (Thm 3.1, 3.2) apply to the intractable version, and so are not relevant to the actual tractable version of the algorithm. These are nice motivations, but ultimately we're left with a heuristic method. Perhaps you can borrow ideas from the SQP literature?\n\n(2) Concern 2: Fig 3(e), \"Grid\" data, your algo with KL projection does worse in Reward than TRPO, which is not unexpected since TRPO ignores constraints. But the lower plot shows that TRPO actually outperforms your KL projection-based algorithm even in terms of constraint!  By trying to respect the constraint, your algorithm has made things worse.  Can you explain this phenomenon?\n\n(3) Concern 3: Thm 4.1 and Thm D.2, I don't know what you're proving because I don't know what f is. Please relate it to your problem and to your updates (e.g., Algo 1). If you are talking about just minimizing f(x) with convex quadratic constraints, then I think you are re-inventing the wheel (overall, your proof looks like you are re-inventing the wheel -- doesn't everything follow from the fact that projections operators are non-expansive?  If you scale with H (and assume it is positive definite) then you're still working in a Hilbert space, just with a non-Euclidean norm, and so you can re-use existing standard results on the convergence of projected gradient methods in Hilbert space to stationary points.\n\n\nSmaller issues:\n\n- The appendix was never referenced in the main paper. At the appropriate places in the main text, please mention that the proofs are in the appendix, and mention appendix C when discussing the PCPO update.\n\n- For the PCPO update, the theorem needs to mention that H is positive definite. Using H as the Fisher Information matrix automatically guarantees it is positive semi-definite (please mention that), so the problem is at least convex, and then you assume it is invertible to get a unique solution.\n\n- It wasn't obvious to me when the constraint set is closed and convex. Please discuss.\n\n- When H is ill-conditioned, why not regularize with the identity? You switch between H and the identity, but why not make this more continuous, and look at H + mu I for small values of mu?\n\n- Lemma D.1 is trivial if you use the definition of normal cones and subgradients. You also don't need to exclude theta from the set C, since if it is in the set C, then the quadratic term will be zero, hence less than/equal to zero."}