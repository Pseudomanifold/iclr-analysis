{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a new problem formulation in the broader area of lifelong learning.\nSpecifically, the Unsupervised Progressive Learning (UPL) problem, requires a learner to consume a stream of data, where each data point is associated with a class, but only very few labels are provided. Similar to continuous learning the learner can only access current and not access previous data in the stream. The paper clearly describes how this setup is different to commonly other learning setups studied, including continual learning. \n\nTo solve this problem the paper proposes a deep-learning-free approach based on clustering and novelty detection.\n\nStrength:\n-\tI think the problem formulation is very realistic and interesting and I am not aware of it being explored previously\n-\tThe newly proposed architecture STAM is also interesting.\n-\tThe paper evaluates STAM on UPL on three datasets, MNIST, EMNIST, SVHN\n-\tThe paper ablates several aspects of the model evaluates the effect of hyper parameters.\n\nWeaknesses:\n1.\tThe paper misses to include baselines, both, w.r.t. the proposed method, as well as for the learning problem. \n1.1.\tThe paper argues that deep learning cannot work, I am wondering, why don\u2019t include standard baselines, e.g. auto-encoder based with a prototype stored whenever a labeled example is seen. (For single pass continual learning, see e.g. [B], although that work is supervised)\n1.2.\tHebbian learning inspired algorithms might similarly form a valuable baseline\n2.\tThe UPL problem makes sense to me, however, the experimental setup could be improved. Specifically, the paper uses the entire dataset to find hyperparameters. This is highly concerning as this basically means the entire dataset has been seen multiple times before the final pass over the dataset (the one with the best hyperparameters) is performed. This is basically a contradiction to the setup of UPL, which assumes all data is seen only once.\n3.\tRelated Work:\n3.1.\t[A] seems to study a similar setup and proposes an approach which should also be applicable in this work.\n\n\n\n\n\n\n\nConclusion:\nThe paper\u2019s contribution is a new learning setup (UPL) and an approach (STAM). While it remains unclear if STAM can generalize to realistic images, the idea of UPL is very interesting and speaks for accepting the paper, although there are several weaknesses the authors should address.\nIt would also be great if the authors plan to release the exact experimental setup (i.e. which images are sampled in which order and which ones are labeled) so future work can compare to this work.\n\n\n\nReference:\n[A] Continuous Online Sequence Learning with an Unsupervised Neural Network Model\nYuwei Cui, Subutai Ahmad and Jeff Hawkins; Neural Computation, 2016\n[B] Gradient episodic memory for continual learning, D Lopez-Paz, MA Ranzato ; NeurIPS, 2017\n\n"}