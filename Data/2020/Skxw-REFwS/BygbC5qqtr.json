{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method called Self-Taught Associative Memory (STAM) for Unsupervised Progressive Learning (UPL) , i.e., learning salient representation from streams of mostly unlabeled data with occasional class labels, where the number of class increases over time. The motivation of this paper is quite interesting in that the authors try to mimic how animals learn. The surrounding environments of animals are considered to be unlabeled, and animals gradually learn to distinguish between objects without explicit information. The model shed light on the problem of catastrophic forgetting by introducing dual-memory organization. To be specific, Short-Term Memory contains a set of centroids associated with the unlabeled data, whereas Long-Term Memory stores the prototypical centroids, which are frequently seen patterns. In addition, the model utilizes novelty detection technique to introduce new centroids to each layer of the model, and it prepares the newly created centroids to be associated with new classes. \nOverall, the paper reads well and it is self-explanatory with clear notations and hyperparameters. Each step of the architecture is well-formulated mathematically and the necessity of the step in the model is explained clearly. The problem proposed, Unsupervised Progressive Learning (UPL) problem, is novel.\nOn the other hand, there are shortcomings of the paper, one being the model evaluation on dataset such as MNIST, SVHN, and EMNIST. The dimensional size of the dataset and the number of classes are small which is not convincing to evaluate the performance of the model. In addition, each layer of the model is independent from other layers (no connections between units at different layers), keeping the updated representation of patches to itself. This fact would hinder the model from understanding complex representation. Furthermore, there is a concern on the use of L2 distance metric for the similarity between the patch and patterns (centroids). L2 is not meaningful in high dimensional spaces.\nThis paper is a good start in tackling the Unsupervised Progressive Learning problem, but some weaknesses are present in the nature of the model architecture as mentioned above. However, the approach taken appears to be ad hoc. It is difficult to imagine the method can work for more complex situations. Even for the small datasets used in the paper, the learning require 10\u2019s of thousands of training example. This does not mimic animal learning at all. Also, the accuracy of the model on EMNIST with 47 classes is around 65% which makes me doubt the application of the model on real world.\n-\tShed some light on the problem of catastrophic forgetting and continual learning without supervision\n-\tThe model architecture is well defined, but has some weaknesses in the methodology\n-\tInteresting motivation of trying to mimic how animals learn in an environment\n"}