{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an algorithm to find linear trajectories in the latent space of a generative model that correspond to a user-specified transformation T in image space. Roughly, the latent trajectory is obtained by inverting the generator at the transformed image and a clever recursive estimation strategy is proposed to overcome difficulties in this nonconvex optimization. Qualitative results of the method, applied to a (pretrained) BigGAN model are shown, where the transformations are chosen as translation, zoom or brightness. A quantitative evaluation is performed on the dSprites and ILSVRC dataset. \n\nMy take:\nThere seem to be a lot of errors and typos in the manuscript, which made the paper unfortunately a bit frustrating to review. In particular, I had trouble following and understanding the details of the main procedure used to obtain the linear latent trajectories. Considering the recent works (Goetschalckx et al, 2019) and (Jahanian et al, 2019), I also don't see too much novelty in this approach. Therefore, I cannot recommend acceptance of this paper at this point. \n\nDetails:\n\n1) In algorithm 1, there seem to be some typos which makes it difficult understand the method in detail. z_{\\delta t} is initialized as z_0 and then never changed but always appended into the data set. Should it maybe be z_{\\delta t} <- argmin ... instead of z_t <- argmin ... ? But then why initialize z_{\\delta t} at all? Why store tuples of three values in D, especially store z_0 multiple times?\n\nWhile it is clear, formally the method always discards D and one might add a D_i <- D at the end.\n\n2) I could not follow the reasoning in Section 2.2 and the clarity should be improved, as it is one of the main contributions of the work. In particular, I would like to see the intuition behind the model t = g(<u, z>) better described. \n\nWhy does the projection of z follow a normal distribution? Is it because the latent distribution in the GAN is chosen as a normal distribution?  \n\nWhat is the loss for training f_{\\theta,u}? How is the dataset D used here? \n\nMinor comments / typos / suggestions (no influence on my rating):\n- InfoGAN (Chen et al., 2016) does not require a labeled dataset, the corresponding sentence in related work should be reformulated a bit. \n- Please use \\operatorname or \\text in math mode for operators such as Var or text.  \n- 'Encodes a the parameter t' --> 'Encodes the parameter t'; Many other typos, please run a spell checker.\n- For image translation, what boundary conditions are used? A sensible way would be to impose the reconstruction loss not on the full image but only on the smaller part. \n"}