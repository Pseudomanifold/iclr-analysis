{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\nThe authors proposed to disentangle syntactic information and semantic information from pre-trained contextualized word representations.\n\nThey use BERT to generate groups of sentences that are structurally similar (have the same POS tag for each word) but semantically different. Then they use a metric-learning approach to learn a linear transformation that encourages sentences from the same group to have closer distance. Specifically, they defined a triplet loss (Eq4) and uses negative sampling.\n\nThey use 150,000 sentences from Wikipedia to train the transformation. POS tags are obtained from spaCy. To evaluate the learned representations, they provided a tSNE visualization of the original and transformed representations (groups by dependency label); evaluate whether the nearest neighbor shares the same syntactic role; low-resource parsing.\n\nReasons of rejection:\n1. I don't agree with the authors' argument, \"we aim to extract the structural information encoded in the network in an unsupervised manner, without pre-supposing an existing syntactic annotation scheme\".  First, what do you mean by structural information without a clear definition? Also, in the method, the authors construct a dataset where each group of the sentence share similar syntactic structures (having the same POS tag). It seems there that the structural information just means POS tags.\n\n2. The author failed to convince me that the learned representation is more powerful than just combining POS tags with the original representations. Since POS tags are assumed to be available during training. I think a reasonable baseline in all experiments would be the performance based on POS tags. For example, in Figure 3, although the original EMLo representation does not correlates with the dependency label very much, the POS tags may do. In Figure 4, the authors should compare with delexicalized dependency parsing, which performs pretty well in los-resource setting.\n"}