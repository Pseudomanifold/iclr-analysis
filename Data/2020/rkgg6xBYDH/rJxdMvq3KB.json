{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new generalization bound for vanilla RNN with ReLU activation in terms of matrix-1 norm and Fisher-Rao norm. This bound has no explicit dependence on the size of networks.\n\nI am actually not familiar with the generalization theorem on RNN. Nevertheless, according to the demonstration of the authors, I can understand the results of the theorems in this paper. I think the analysis on the property of the covariance matrix of the input data are valuable. However, I still have some concerns as follows.\n\n1.\tIt is interesting if the bound can be independent on the size of networks. However, according to the bounds, I find that the bounds still depend on the size of the network but implicitly. Thus, I understand why the authors claim that the bound they provide has no \u201cexplicit\u201d dependence on the size of networks. Then what is the value of this contribution?\n2.\tIn Section 3.4.1, the authors state that \u201cSince the smaller eigenvalues usually contribute to high frequency components of the input signal, our bound suggests that high frequency information is often more difficult to generalize, which is consistent with intuition.\u201d. Can the authors provide more explanations on this since I do not understand why smaller eigenvalues usually contribute to high frequency components of the input signal and what the frequency components of the input signal is. What is the formulation of the high frequency components of the input signal?\n3.\tIt seems that ReLU activation is not widely used in RNN. Instead, tangent and sigmoid function are more prevalent. The authors mention in Conclusion that the extending the results to other variants of RNNs like LSTM and MGU might be an interesting topic for future research. I think the first extending work is to study the generalization bound of vanilla RNN with tangent or sigmoid activation. \n\n"}