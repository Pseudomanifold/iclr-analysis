{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a Multi-Scale attention model. The results are strong/competitive but the overall idea is highly incremental. In the author\u2019s own words, the key contribution of this paper is \u201ca concatenation of a self-attention network, a feed-forward network and a dynamic convolution network together\u201d.  \n\nThe multi-scale here refers to global context and local context modeled by self-attention and convolution respectively.\n\nWhile I appreciate the engineering efforts in this paper to glue several recent neural components together, I really think there is a serious lacking of novelty in this work, e.g., gluing several known components together. The performance gains are also not particularly interesting since more parameters are introduced. Notably, this model performances on par with other models (vanilla Dynamic Conv) on WMT En-Fr and no results are reported on En-De.\n\nI think this work would make a good technical report and/or workshop paper. But the concatenating of several papers for a marginal performance gain probably will not warrant acceptance in a top DL conference. Overall, I am voting for a weak reject largely because of the lack of novelty.\n\nThat said, it is nice to see how dynamic convolutions and self-attention can work hand in hand for better performance. I am also curious about why the need for \"token-level\" features with position-wise FFNs, since Transformers are already composed of these.\n\nThe claim of trying to answer the question if \"is attention all you need\" is a bit of a stretch. I think opening with such a line and not really fulfilling the question can be seen as risky. I am not a fan of how the authors opened the paper.\n\nI appreciate the author\u2019s honesty and actually not over-selling the work but at this juncture, I think the authors may want to investigate the possibility of both conv and attention modules working together with better synergy.  This might help nudge this paper over the current hurdle of lacking novelty."}