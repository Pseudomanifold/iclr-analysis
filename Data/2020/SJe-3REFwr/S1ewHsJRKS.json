{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The proposes a transformer layer that performs self-attention, feed-forward and dynamic-convolution all in parallel. The dynamic convolution consists of several different kernel sizes, which are then combined by learned weights. Experiments are done on three NMT tasks. \n\nIn terms of novelty, there is very little. The model essentially combines DynamicConv from Wu et.al with self-attention. More importantly, it's not clear why that's better. The only direct comparison to Wu et.al. is the EN-FR translation task, but the proposed model doesn't not outperform DynamicConv, even though having more parameters. \n\nAnother novelty in the paper is placing feedforward in parallel to self-attention, which seems to improve the performance. This is probably why the model is more stable compared to Transformer because it has essentially 2x less depth, while maintaining the same capacity. \n\nAs I read the paper, it felt bit rushed. For example, page 2 top says \"language modeling\", but I can't find any language modeling experiment. Figure 3 is never mentioned in the text. Also, the equation numbers in \"weight tying\" paragraph doesn't match, and Eq8 is missing an operator (probably ReLU).  In page6, the authors say \"DynamicConv diverges\", but I can't find that result. Also it said \"The small version of MUSE still beats Transformer by a large margin.\", but where is that result? From table 3, it looks like they match in performance.\n\nOther comments:\n- The authors claim their architecture allow the feed-forward module better access to token features, but I think that's only true for the first layer. Besides, Transformer has skip-connection that can directly feed token features to all layers.\n- Why W_in is tied to W^V? Why not W^K for example? \n- Why \\alpha is initialized to 1/n as if they are probabilities? When you pass \\alpha through softmax, any constant offset will have no effect on the output. \n- What are the kernel sizes used? \n- Type: \"... dataset, We ...\"\n- Figure 4 caption says \"dynamically\", but my understanding is that the kernel weights are just learned (adaptive) and doesn't dynamically change with inputs."}