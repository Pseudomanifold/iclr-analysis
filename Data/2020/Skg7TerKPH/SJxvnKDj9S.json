{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper addresses an interesting case of text generation, interpolating between the requirements of content fidelity and style transfer. This is carried out using a hybrid \"attention-copy\" mechanism, which is intended to attend to style in a reference sentence and copy content over from a structured data representation. This along with data augmentation and a combined unsupervised objective comprise the learning framework. An ablation study and human evaluation are carried out, confirming that the model presented has advantages over foregoing style transfer and generation models. \n\nI think this paper should be accepted as it addresses an interesting and widely applicable scenario, and the combined unsupervised objective is a creative and well-motivated solution. It consitutes a contribution both to neural generation and style transfer methods. It is not a strong accept as I think that when introducing a new task, more in-depth evaluation and error analysis is appropriate.\n\nOne challenge with this task is evaluation -- some of the scores are close enough to 50% that it would be useful to have significance of some kind calculated, more information about the annotators and inter annotator agreement, and some amount of error analysis to help give context for Table 3. Particularly, since this is a new task, the foregoing models compared would be expected to perform worse than the new model. To learn more about ways in which the new model is well adapted to the task, it could be useful to look closer into the tradeoff between fidelity and style transfer, e.g. comapring the BLEU score between y and y_hat in addition to y' and y_hat for all models.\n\nRecommendations:\n1. If possible, please use clearer notation to explain the data augmentation step. It is really interesting and creative, but very hard to follow x, x', y, y' -- please consider using subscripts like y_side or something.\n2. In section 6.2, a binary transformer classifier is mentioned. Please clarify how 'manipulating the content successfully' is measured, and describe the classification task in further detail."}