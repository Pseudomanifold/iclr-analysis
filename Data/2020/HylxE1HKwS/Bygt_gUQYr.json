{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this papers, the authors learn a Once-for-all net. This starts as a big neural network which is trained normally (albeit with input images of different resolutions). It is then fine-tuned while sampling sub-networks with progressively smaller kernels, then lower depth, then width (while still sampling larger networks occasionally, as it reads). This results in a network from which one can extract sub-networks for various resource constraints (latency, memory etc.) that perform well without a need for retraining.\n\nThis paper is well written, and the results are very good. However there are serious problems that need addressing.\n\nThe method as described *is not reproducible*. The scheduling of sampling subnetworks is alluded to on page 4, and that's it. It is essential that the authors include their exact subnet sampling schedule e.g. as pseudocode with hyperparameters. There is no point doing good work if other researchers cannot build off it. \n\nOn another reproducibility note, as far as I can tell, the original model isn't given. There would be no harm in adding this to the appendix. \n\nFigure 1 is misleading, as we don't find out until later in the paper that Once For All #25 means that each of these points was finetuned for a further 25 epochs (which on ImageNet is non-trivial). This defeats the narrative of the paper (once-for-all plus some fine-tuning isn't exactly once-for-all).\n\nIs there a reason why the progressive shrinking goes resolution->kernel->depth->width? Was this just the permutation that worked best? I would be curious as to why this is.\n\nFor elastic width, I wasn't sure why the \"channel sorting operation preserves the accuracy of larger sub-networks\". Could you please elaborate?\n\nKudos on adding CO2 emissions in Table 2, I hope this gets reported more often.\n\nIn the introduction, the authors talk about iPhones and then the hardware considered is Samsung and Google. A minor note, but it seems inconsistent.\n\nAnother minor note, in Table 2, (Strubell et al) should be out of the brackets, as it is part of the sentence.\n\nGiven that there are 10^19 subnetworks that can be sampled, it would be nice to see more than 3-4 appear on a plot. This makes it seem like they might have been cherry-picked. Sampling a few 100/1000 subnets and producing some Pareto curves would be both interesting and insightful.\n\nPros\n-------\n- Good results\n- Well written\n- Neat idea\n\nCons\n-------\n- Training details are obfuscated. This paper should not be accepted without them.\n- Very few subnetworks of the vast quantity that exist are observed.\n\nIn conclusion, I am giving this paper a weak reject, as it is currently impossible to reproduce, and as such, is of no use to the community. If the authors remedy this I will gladly raise my score."}