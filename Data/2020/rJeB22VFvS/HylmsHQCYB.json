{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the important open question of how to ensure that uncertainty estimates of neural network predictions actually reflect real world error distributions. The paper introduces an uncertainty quality metric along with a statistical test based off this metric that enables a binary decision of whether to accept a model\u2019s uncertainties as realistic. The paper also introduces a new model for supervised image-to-image tasks that combines two existing uncertainty mechanisms, and achieves reasonable uncertainty estimates, in particular, demonstrating robustness to out-of-sample data.\n\nI am tending to reject, because although each of the two distinct contributions are good starts on interesting approaches, neither provides a convincing solutions for the main question, and the two contributions are quite distinct, so that the paper lacks a consistent thread.\n\nFirst, the Mahalanobis-based uncertainty evaluation makes sense, but it is not clear what it adds beyond the standard average negative log-likelihood (NLL) metric. Mahalanobis distance increases monotonically with negative log-likelihood, so is there any reason to expect it is a better way to compare models based on their uncertainty? Is there some experiment that could show that the new metric is better than NLL at evaluating uncertainty realism?\n\nThe statistical test could potentially be a nice real world tool for deciding whether to trust neural network uncertainties. However, the paper only applies the test in one case, where the MC dropout model is shown to have unrealistic uncertainties. To show that this test is useful, there should also be experiments where uncertainty estimates are shown to be realistic. Is there some alternative to MC dropout for regression or some improvements to the algorithm that could yield realistic uncertainties under this statistical test? E.g., would the U-Net-based model in the paper pass the test if it were adapted to regression problems?\n\nSecond, the new U-Net-based classification model looks like a reasonable approach, but it is disjoint from the new statistical test, and it is not clear that the method yields improvements to uncertainty realism, since there are no comparisons to external results. Since only a new architecture is used, it is not clear that the deficiencies in uncertainty realism are not architecture-specific. Is it that there are no existing architectures that can be applied to this problem? Also, is there some more realistic setting where the CVAE approach would improve in-sample scores, i.e., where out-of-sample data is not generated synthetically? Can the statistical test be adapted to the classification setting? Similar, to the case of regression, the uncertainty realism metrics used for classification are tightly coupled with the prediction accuracy; is there a way to decouple these, would one want to? The conclusion that CVAE is better than MC for this problem is solid, but is there a more general conclusion to be drawn? E.g., could a CVAE model yield realistic estimates in the regression setting?\n"}