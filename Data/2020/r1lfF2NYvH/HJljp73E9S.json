{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents an unsupervised method for graph embedding. The authors seek to obtain graph representations by maximizing the mutual information between graph-level and patch-level\nrepresentations. They also consider a semi-supervised task when the Mutual Information-based criterion has an additional term which quantifies a classification error, obtained when constructing a classifier based on the obtained graph representations. \n\nDespite having good experimental results, the proposed approach is rather a mix of previous works and hence not novel. \n\nIn particular, the main building block of the embedding algorithm, the target functional based on mutual information, was borrowed from Deep Graph Informax paper. The differences, listed by the authors, are only of technical nature. Advantage of using it for unlabeled data is poorly motivated: why we can learn smth useful when maximizing the mutual information between graph-level and patch-level representations obtained via GNN? What if patch-level representations are not sufficiently characteristic to have anything in common with the graph?\n\nThere is no discussion of [1], which uses CBOW framework, has theoretical properties, and produces good results in experiments. There is no comparison with GNN models such as [2]. \n\nMinor comments: please, correct fonts - they are different in formulas 6,7 and 5\n\nI would be more interested to see explanation of the obtained results for each particular dataset (e.g. why MUTAG has 89% accuracy and PTC 61%); what so different about dataset and whether we reached a limit on most commonly used datasets. \n\n[1] Anonymous Walk Embeddings? ICML 2018, Ivanov et. al. \n[2] How Powerful are Graph Neural Networks? ICLR 2019, Xu et. al."}