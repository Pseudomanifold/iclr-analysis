{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a divide-and-conquer style adversarial learning algorithm for weakly supervised learning of high resolution image enhancement. Their work is primarily motivated by the limitations of modern mobile cameras in producing images with adequate dynamic range, texture sharpness, and exposure. They also point out limitations of recent work in image enhancement; e.g. supervised approaches suffer from difficulties in data collection (must have paired low quality, high quality images) and previous applications of GANs have suffered from their inability to simultaneously enhance multiple \"perceptual\" qualities of the input images (i.e. texture, color, light, etc). Furthermore, many recent approaches rely on downsampling or patching strategies in order to deal with the computational demands of high resolution images. The authors aim to address these limitations by 1) introducing a \"divide and conquer\" approach in the generator/discriminator networks, thereby jointly optimizing a range of image qualities, 2) adopting the CycleGAN objective for weakly-supervised learning of low/high image sets, and 3) proposing a multi-scale architecture that is able to transfer learned features on downsampled low-resolution images to higher resolution images and improve the overall quality of results. They perform an empirical study using several datasets comparing PSNR and SSIM scores with previously proposed methods. They also provide Mechanical Turk human evaluation results for a small sample of images from two of the datasets.\n\nThe fundamental idea of divide-and-conquer for image enhancement, as well as the multi-scale architecture, are both interesting contributions to the field. The authors also demonstrate fairly promising results on several datasets. However, due to the fact that this work ambitiously introduces a wide range of architectural changes, it suffers from a general lack of thoroughness in empirical support for all of the proposed ideas. For example, while the authors provide reasonable and convincing support for the divide and conquer approach through an ablation study, they fail to provide adequate justification for other proposals, such as the \"adaptive\" sliced Wasserstein GAN (dubbed AdaSWGAN). The only empirical evidence given for this change is an unconvincing qualitative comparison on the 25-Gaussians dataset, in which the alleged improvement over previous methods is unclear. Additionally, despite the fact that the proposed multi-scale architecture is solving a problem suspiciously similar to super-resolution, the authors do not compare their method to similar multi-scale, super-resolution methods such as ESR-GAN. The authors also do not provide any runtime analysis (theoretical or empirical), which seems fairly critical given the apparent complexity of their method.\n\nGiven these concerns, as well as some others which I will detail per-section below, I am giving this paper an overall score of \"weak reject\" for lack of clarity and rigor. If the authors are able to address some of my questions and concerns, I am willing to increase this score, as I do think that some of their proposed ideas are valuable contributions.\n\nSection 1: Introduction\n\n1. \"Besides, due to the computational complexity and unstable adversarial training, the existing image enhancement methods treat high-resolution images using either the downscaled version or patch-wise processing strategies, neither of which are optimal\"\n\nUnless I have misunderstood something about the authors' multi-scale architecture, it seems that their method (as applied in the experiment) also suffers from this limitation. In the appendix, they say that the high-resolution enhancer uses images of size 1024x512 resolution, which appears to be smaller (and oddly has a different aspect ratio) than the resolution of their training data. They note that it is \"still feasible\" to train their enhancer on higher-resolution images \"if [they] have more computational resources.\" But the same would also apply to other methods, since the whole reason for downscaling/patching is to lessen these computational constraints.\n\n2. It is not clear how figure 1 and figure 2 relate to one another. The generator does not perform frequency-based or dimension-based division, so it seems that this \"recombining\" step is sort of implicit in the loss function learned by the discriminator. This should be clarified.\n\nSection 2: Proposed Method\n\n1. Please clarify the x*s_x^-1 term in equation (1); specifically, what is the meaning of the -1 exponent? Is it simply 1/s_x? If so, why is the reciprocal necessary?\n\n2. The mathematical meaning of global concatenation is not clear from the figure or the text. Please clarify.\n\n3. There are no citations or theoretical justication for the section on conv + average pooling with regards to capturing meaningful global features. Specifically the claim \"this design enables our model to enhance full resolution images while it is trained on downscaled images\" needs empirical and/or theoretical support. This seems more like conjecture than fact.\n\n4. Organization of section 2.2 is weird. It's supposed to be about \"frequency based division\" but most of the section is dedicated to discussing CycleGANs. Perhaps these topics should be split into separate sections.\n\n5. For freqency separation, is a Fourier transform used? Or just the application of Gaussian kernels and grayscale components as described in the last paragraph? Additionally, how are the Gaussian kernels parameterized?\n\n6. The integral in equation (5) is missing a differential, presmuably d-theta. The theta \\in S^{n-1} notation doesn't really make sense for integrals, since the space is (I think) continuous. If not continuous, this should be a summation.\n\n7. As mentioned before, the motivation for an adaptive lambda parameter in the Wasserstein objective is not clear. Does this architecture not converge or perform worse without it?\n\n8. The adaptive lambda also trades one hyperparameter for another, i.e. the bound tau. How is this term chosen? And what is the effect on performance?\n\n9. The 25-Gaussians experiment is does not provide adequate support for the necessity of AdaSWGAN. The source paper for SWGAN provides a far more comprehensive comparison as well as quantitative results, rather than a single visualization. Furthermore, there is no evidence given that AdaSWGAN has better performance/convergence on real data.\n\n10. Typo in second to last paragraph: Stifel -> Stiefel\n\nSection 3: Multiscale Network and Training\n\n1. Please clarify whether or not the given low/high resolution image sizes in the appendix are used by the DACAL architecture in all experiments.\n\n2. This section should be condensed to the main points. Some of the details and tangential discussions can be moved to the appendix.\n\nSection 4: Experiment\n\n1. Please clarify the low/high image resolutions for the MIT-Adobe FiveK datasets and whether or not the images used for the high resolution DACAL are downscaled.\n\n2. How are SSIM_f and PSNR_f calculatd on full resolution images if DACAL (and the other methods) operate on downscaled version of the images? Is there an upsampling operation applied in post processing?\n\n3. Why is DPED in the weakly supervised section if it contains paired images? Is that not a supervised task?\n\n4. Space permitting, please provide a qualitative example from one of the supervised tasks that shows the enhanced output of DACAL vs. the ground truth high resolution image.\n\n5. Is the CycleGAN architecture still used for supervised learning tasks? Or just the two-stream U-Net?\n\n6. Style suggestion: replace \"our DACAL\" with just \"DACAL\" throughout this section. This is better technical writing practice.\n\n7. As mentioned before, please provide some assessments of computation costs and runtime (theoretical or empirical) for DACAL vs. the other methods. If DACAL requires significantly more compute time or memory, this is an important trade-off to consider.\n\nUpdate 1: Fixed typos"}