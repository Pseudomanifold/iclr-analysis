{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a variation on the generator of GANs. The authors modify the generator by adding a concept of \"blocks\" which are randomly activated based on part of the random input vector. It is similar to adding random dropout in the generator, except that the dropout would apply to larger sets of activations instead of single component.\n\nThe authors also add a diversity term to force the different blocks to have different blocks. This is a term based on the L2 distance between the weights of different blocks. Although this term is ad-hoc and could probably be refined into something more grounded in theory, it should indeed provide some diversity.\n\nThis block structure allows for more understanding of what each layer of the generator does, since it is easy to change the discrete variables that switch blocks. The paper presents an empirical evaluation of what each switch does, and show that concepts are well disentangled between layers (for instance, one layer changes the background whereas another one changes the color of the foreground).\n\nThey also show that blocks can be added after training is done which is a nice property for incremental training.\n\nThey also show that this framework can train a generator without non-linearities (except for the block switching), which could potentially simplify the analysis of such networks.\n\nGenerated samples are presented up to 128x128 pixels which, although far from state of the art, proves that the concept works."}