{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper addresses the issue of interpretability of GAN generation through an alternative approach to the introduction of variability. To seed the generation, instead of providing a random input vector (typically sampled from a standard Gaussian distribution), the authors instead modify the generator architecture so as to allow for randomization in the routing: each layer is replaced by a bucket consisting of several blocks, and in forward propagation only through randomly chosen blocks. In this case, the input vector is chosen to be a constant - the only source of randomization\nprovided to the generator is in the choice of blocks through which to propagate. The explanability derives from the tendency of blocks to associate with a common interpretation after training. Their use of blocks necessitates the introduction of a block diversity loss, to discourage mode collapse. The scheme is referred to as RPGAN, for \"Random Path GAN\".\n\nThe main strengths of the paper:\n\n(1) Their proposed approach is highly flexible. In principle, any underlying GAN architecture can be adapted by assigning each layer to a distinct bucket, and then replicating the layer across the blocks. \n\n(2) Experimental results do show that different block sequences are associated with common image characteristics after training, especially for the initial and final layers.\n\n(3) The use of non-standard ways of introducing stochasticity to GAN generation is an interesting idea in itself.\n\nThe main weaknesses:\n\n(1) Although the authors provide experimental examples showing images associated with various paths through the architecture, is not clear how interpretations can be associated with these paths. In the examples presented, there seems to be a tendency for greater interpretability at the initial and final layers, with the explanation given for the intermediate layers being less convincing.\n\n(2) The number of experimental examples is low, yet the authors draw rather firm conclusions (end of Section 4.1) regarding interpretability across layers. I am not sure that their conclusions adequately capture what is going on here, nor am I convinced that they generalize to other situations.\n\n(3) The number of buckets limits the numbers of explanations. Essentially, the method has the same difficulties as in clustering, where specifying too many or too few groups can profoundly influence the nature and quality of the result. Although the authors do discuss an approach by which the number of buckets can be incrementally increased (thereby allowing for variation in the number of explanations generated), the experimental evidence is insufficient.\n\n(4) Presumably, the replication of a layer across the blocks assigned to its bucket would require more training data and/or greater training times. What is the relationship in both time and quality between the original GAN network and its RPGAN versions?\n\n(5) There are many presentational problems with this paper, in grammar, vocabulary and terminology, sentence structure, etc.\n\nOverall, in its current state (not least due to presentational issues) the paper appears to be below the acceptance threshold.\n"}