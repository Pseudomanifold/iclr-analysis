{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a new method for active learning, which picks the samples to be labeled by sampling the elements of the dataset with highest gradient norm, under some constraint of diversity. The aforementioned gradient is computed w.r.t. the predicted label (rather than the true label, that is unknown) and diversity is achieved by sampling via the k-MEANS++ algorithm.\nThe paper is well written and while the experiments look thorough, the motivation to support the proposed method seem too weak and unconvincing as does the discussion of the results, which is why I am leaning toward rejection. \nI am willing to amend my vote if the authors provide stronger (not empirical) motivations on why using the gradient norm w.r.t. the predicted label is a better metric than those in the literature, and  More comments below.\n \nDetailed feedback:\n\n1) The paper lacks a proper motivation as to why using the norm of the gradient is a better metric than the many others already present in the literature. In particular, I cannot think of any case where it would be best to use that than the entropy of the network\u2019s output distribution, even though the empirical results seem to suggest otherwise. Specifically, while I believe that in many cases it will be similarly good, if we consider the case when the network is able to rule out most of the classes but is unsure on a small fraction of them, the entropy will better reflect this uncertainty than the norm of the gradient of the predicted class. \n\nGenerally speaking, I believe that the use of the norm of the gradient of the predicted class should be much better motivated, being the core idea of the paper. Stating that it is cheap to compute and empirically performs as well as k-DPP in two experiments is not convincing enough in my opinion.\n2) I wonder how much of the performance of BADGE is due to k-MEANS++ and how much to the choice of using the gradient norm. Please perform an ablation study where you can e.g., replace the gradient norm with the entropy, or replace k-MEANS++ with random sampling, and discuss the results. \n3) How is the embedding \u201cground set\u201d space determined for k-MEANS++? How are the centroids determined? In which space? It is unclear to me how k-MEANS++ is used in the context of the norm of the gradients. Please improve the explanation in the main text.\n4) Please add a curve for k-DPP to the plots in the main text, rather than having separate plots for it in the appendix. Also, it would be interesting to compare against Derezinski, 2018 as well, if that\u2019s the current state of the art (which is what I infer from your text, but I might be wrong).\n5) The paper builds on the claim that the gradient norm w.r.t. the prediction is a lower bound for the gradient norm induced by any other label, yet Proposition 1 that proves it is in Appendix B. This prove is central to the proposed idea and should be in the main text.\n6) The authors claim that to capture diversity they collect a batch of examples where the gradients span a diverse set of directions, but it\u2019s unclear to me that k-means++ actually accomplishes that. Where is the *direction* of the gradient taken into account in the algorithm?\n7) The \u201cdiscussion\u201d section is really a \u201cconclusion\u201d one, and indeed a proper in-depth discussion of the experiments is missing. Please expand the comments on the experimental results. \n8) The metric to compute the \u201cpairwise comparison\u201d looks quite convoluted. Is it common in the literature? If so, please add a reference. If not, can you motivate the use of this specific formula?\n9) The random baseline seems to be very competitive. Why is that? Please provide your intuition. Could this be indicative that the baselines have not been tuned properly?\n10) Introduction: the sentence \u201c[deep neural networks] successes have been limited to domains where large amounts of labeled data are available\u201d is incorrect. Indeed, neural networks have been used successfully in many domains where labelled data is scarce, such as the medical images domain for example. Please remove the sentence.\n11) Introduction: please add a sentence to explain what a version-space-based approach is.\n\n12) Is Figure 2 the average over multiple runs or a single run?\n\n13) Notation: please do not use g for the gradient (g^y_x) and for the intermediate activations (g(x; V)).\n\n14) The lower margin seem too wide. Please make sure you respect the formatting style of the conference.\n\n\nMinor:\n- Notation: if you must shorten g^{\\hat{y}}_{x} please do so with \\hat{g}_{x} and equivalently shorten g^{y}_{x} as g_{x}\n- Notation: in the pairwise comparison, please don\u2019t reuse i to denote an algorithm (it is used a few lines before to compute the labeling budget)\n- Please add reference to Appendix A when k-MEANS++ is first referred to in page 2.\n- Page 3,  when Proposition 1 is mentioned add reference to the location where it\u2019s defined.\n\n\nTypos:\n- Page 2: expenive -> expensive\n- Page 5: Learning curves. \u201cHere we show ..\u201d -> Remove \u201chere\u201d\n- Figure 3: pariwise -> pairwise\n- Page 7: Apppendx E"}