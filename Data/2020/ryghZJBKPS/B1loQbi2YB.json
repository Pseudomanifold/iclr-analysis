{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces an algorithm for active learning in deep neural networks named  BADGE. It consists basically of two steps: (1) computing how uncertain the model is about the examples in the dataset (by looking at the gradients of the loss with respect to the parameters of the last layer of the network), and (2) sampling the examples that would maximize the diversity through k-means++. The empirical results show that BADGE is able to get the best of two worlds (sampling to maximize diversity/to minimize uncertainty), consistently outperforming other approaches in a wide-rage of classification tasks.\n\nThis is a very well-written paper that seems to make a meaningful contribution to the field with a very good justification for the proposed method and with convincing empirical results. Active learning is not my main area of expertise so I can\u2019t judge how novel the proposed idea is, but from an outsider\u2019s perspective, this is a great paper. It is clear, it does a good job explaining the problem, the different approaches people have used to tackle the problem, and how it fits in this literature. Below I have a couple of (minor) comments and questions:\n\n1. Out of curiosity, it seems that it is standard in the literature, but isn\u2019t the assumption that one can go over the whole dataset, U, at each iteration of the active learning algorithm, limiting? It is not that cheap to go over large datasets (e.g., ImageNet).\n2. MARG seems to often outperform the other baselines but it doesn\u2019t have a reference attached to it (bullet points on page 5). Is this a case that a \u201ctrivial\u201d baseline outperforms existing methods or is there a reference missing?\n3. In some figures, such as Figure 2, there are shaded regions in the plots. It is not clear what they are though. Are they representing confidence intervals? Standard deviation? They are quite tight for a sample size of 5.\n4. In the section \u201cPairwise comparisons\u201d it reads \u201cAlgorithm i is said to beat algorithm j in this setting if z > 1.96, and similarly \u2026 z < -1.96\u201d.  It seems to me that the number 1.96 comes from the z-score table for 95% confidence. However, if that\u2019s the case, it seems z should be much bigger in this context. With a sample-size of 5 (if this is still the sample size, maybe I missed something here), the normal assumptions do not hold and the t-score should\u2019ve been used here. What did I miss?\n\nIn terms of presentation,  Proposition 1 seems to be a very interesting result. I would move it to the main paper instead of leaving it in the Appendix. I also think the paper would read better if it didn\u2019t use references as nouns (e.g., \u201calgorithm of (Derezinski, 2018)\u201d). Finally, there\u2019s also a typo on page 7 (Apppendx)."}