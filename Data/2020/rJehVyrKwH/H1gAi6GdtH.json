{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The authors aim to reduce the distortion of each layer rather than the weight distortion. The proposed algorithm first selects the candidate codeword vectors using k-means clustering and fine-tune them via knowledge distillation. The authors verify the proposed algorithm by comparing it with existing algorithms for ResNet-18 and ResNet-50.\n\nOverall, I think that the proposed algorithm is easy to apply and the draft is relatively well written. Some questions and doubts are listed below.\n\n-In k-means clustering (E-step and M-step), is it correct to multiply \\tilde x to (c-v)? I think that the error arising from quantizing v into c is only affected by a subset of rows of \\tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, \u2026 rows of \\tilde x affect to the error.\n\n-Does minimizing reconstruction error minimizes the training loss (before any further fine-tuning) compared to na\u00efve PQ? If not, \n\n-Is there any guideline for choosing the optimal number of centroids and the optimal block size given a target compression rate?\n\n-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation) \n"}