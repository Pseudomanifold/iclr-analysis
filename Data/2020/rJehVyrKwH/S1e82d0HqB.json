{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use codes and codebooks to compress the weights. The authors also try minimizing the layer reconstruction error instead of weight approximation error for better quantization results.\nDistillation loss is also used for fine-tuning the quantized weight. Empirical results on resnets show that the proposed method has a good compression ratio while maintaining competitive accuracy.\n\nThis paper is overall easy to follow. My main concern comes from the novelty of this paper. The two main contributions of the paper: \n(1) using codes and codebooks to compress weights; and \n(2) minimizing layer reconstruction error instead of weight approximation error\nare both not new. For instance, using codes and codebooks to compress the weights has already been used in [1,2].  A weighted k-means solver is also used in [2], though the \"weighted\" in [2] comes from second-order information instead of minimizing reconstruction error. In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4]. \nClarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.\n\nIt is not clear how the compression ratio in table 1 is obtained. Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).\nCan the authors provide an example to explain how to compute the compression ratio? \n\n[1]. Model compression as constrained optimization, with application to neural nets. part ii: quantization. \n[2]. Towards the limit of network quantization.\n[3]. Efficient and Accurate Approximations of Nonlinear Convolutional Networks.\n[4]. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. \n\n"}