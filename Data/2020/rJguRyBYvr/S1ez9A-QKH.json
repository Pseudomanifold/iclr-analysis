{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: \nThis paper proposes new regularization techniques to train DNNs, which after training, make the crafted adversarial examples more detectable. The general idea is to minimize the inter-class variance and maximize the intra-class distance, at some feature layer. This involves regularization terms: 1) SiameseLoss, an existing idea of contrastive learning known can increase inter-class margin; 2) reduce variance loss (RVL), a variance term on deep features, and 3) reverse cross entropy (RCE), a previously proposed term for detection purpose. The motivation behind seems intuitive and the empirical results demonstrate moderate improve in detection AUC, compared to one existing technique (e.g RCE).\n\nMy concerns:\n1. The proposed technique requires retraining the networks to get a few percents of detection improvement. This is a disadvantage compared to standard detection approaches such as [1] and [2] which do not need to retain the network. I am surprised that these standard detection methods were not even mentioned at all. Retraining with fixed loss becomes problematic when the networks have to be trained using their own loss functions due to application-specific reasons. Moreover, the detection performance reported in this paper is not better than the one reported in [2] (ResNet, CIFAR-10, 95.84%) which do not need retraining.\n\n2. There are already well-known margin-based loss functions, such as triplet loss [4], center loss [5], large-Margin softmax loss [6], and many others, which are not mentioned at all.\n\n3. In terms of retraining-based detection, higher AUCs have been reported in [3] for a neural fingerprinting method.\n\n4. Incorrect references to existing works. The second sentence in Intro paragraph 2: Metzen, et al, .... these are not adversarial training. Xu, et al. (feature squeezing) is not a randomization technique.\n\n5. The \"baseline\" method reported in Table 2, is confusing. RCE is also a baseline? You mean conventional cross entropy (CE) training?\n\n6. Some of the norms are not properly defined, which can be confusing in adversarial research. For example, from Equation (1) to (4). The \"Frobenius norm used here\" statement in Equation (3), don't know this F norm comes from.\n\n\n[1] Characterizing adversarial subspaces using local intrinsic dimensionality. ICLR, 2018\n[2] A simple unified framework for detecting out-of-distribution samples and adversarial attacks. NeurIPS, 2018\n[3] Detecting Adversarial Examples via Neural Fingerprinting. arXiv preprint arXiv:1803.03870, 2018\n[4] Facenet: A unified embedding for face recognition and clustering. CVPR, 2015.\n[5] A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV, 2016.\n[6] Large-Margin Softmax Loss for Convolutional Neural Networks. ICML 2016."}