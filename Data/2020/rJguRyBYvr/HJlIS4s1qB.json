{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a general defense method against adversarial attacks by maximizing an approximate bound on the magnitude of distortion needed to force a misclassification. The authors note that this maximization can be achieved by increasing the margin between class clusters and by reducing the norm of the Jacobian of intermediate layers. Subsequently, they either directly adopt or introduce simple modifications to existing techniques to affect these two factors, showing the robustness of the combined method to several adversarial attacks on MNIST and CIFAR-10 datasets. \n\nAs neural networks get deployed for increasingly critical applications, the issue of defense against adversarial attacks becomes progressively relevant. The paper does a good job of motivating a relatively simple approach to the problem based on an approximate bound, and pulls in from different existing methods to build a robust system. The strong points of the paper:\n1. The paper is clearly written, and the approach is sensible. \n2. Fairly thorough empirical investigation under different threat models.\n3. The proposed method performs consistently above the baselines for different experiments.\n\nHere are some of my concerns:\n1. The work is somewhat incremental and the novelty mostly lies in pulling a few different methods together that seem to work well in unison.\n2. The two methods used for increasing the margin don\u2019t actually optimize that objective directly. The Siamese Loss uses cosine distance as proxy and the variance reduction doesn\u2019t guarantee increase in margin which is sensitive to outliers. Any improvement achieved thus appears to be an ill-understood side-effect. \n3. The definition of cluster distance (page 3) looks erroneous. \n4. The authors note that the proposal doesn\u2019t work very well for a specific kind of attack (BIM) but don\u2019t have clear recommendations for improvement. The tentative explanation of why this happens is also somewhat loose. \n\nIn summary, I think the paper addresses an interesting problem even though the development is arguably incremental. However, since the unified approach is simple yet novel, and the results fairly promising, I am somewhat inclined to accept this paper."}