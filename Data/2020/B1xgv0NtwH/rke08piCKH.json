{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a strong baseline defense, the K-nearest neighbor (KNN) based data filtering against clean-label poisoning attack, which aims to add small noises to (a subset of) training data but maintain the original training labels such that some targeted images at inference will be misclassified. The authors provide geometric interpretations of why K-NN is a good defense and also compare the defense performance against several baselines, including L2 Defense, Robust Feature Extractor, One-Class SVM, and Random Defense. The proposed defense outperforms the compared defense methods by a large factor.\n\nIn general, this paper is well-motivated. The defense is simple yet quite effective. However, based on the current presentation, I have the following concerns.\n\n1. The scope of this paper can be limited since it only focuses on defending against one type of attack (clean-label poisoning attack) in the family of data poisoning attacks. It will be great if the authors can further motivate how the KNN defense can be used in a broader context.\n\n2. It is good to see simple methods like KNN turns out to be a strong defense in this context. However, I am wondering how easy it is to bypass this defense. For example, when crafting clean-label poisoning attack, it should be possible to consider a more insidious setting that the added perturbations will make the K-nearest neighbors of the perturbed training samples belong to the target class label (similar to how targeted adversarial examples work). In this case, this defense may be in vain. There are some prior works that study how robust KNN algorithms are when used as a defense, such as \"On the Robustness of Deep K-Nearest Neighbors\" by Sitawarin and Wagner. Since the proposed defense is \"simple\" enough so that the attacker may use it as a baseline defense to propose stronger attacks, I believe it's necessary to verify how \"easy\" it is to be bypassed. Otherwise, technical insights can be limited.\n\n3. How does the proposed defense compare to the data filtering approach used in\"Spectral Signatures in Backdoor Attacks\" by Tran et al?  Can it be also used as a baseline defense?\n "}