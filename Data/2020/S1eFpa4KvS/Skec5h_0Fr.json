{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new regularization technique for neural networks which the authors call \u201cBiologically Enhanced Artificial Neuronal Assembly Regularization\u201d (or just BEAN). The authors call it this because they think this technique bears relation to how real brains work. BEAN is really best summarized as the addition of a regularization term in the loss function that encourages neurons in a given layer to form functional clusters that are highly correlated with one another. The authors demonstrate that this leads to clusters in neural activity. They also show that it leads to apparently better performance on MNIST and Fashion MNIST than other regularization techniques after only a few training examples.\n\nUltimately, the idea of this new regularizer is interesting, and the final data suggests that it may be a very good regularizer for classification tasks. But, unfortunately, this paper is a mess, and should not be accepted to ICLR. Here are the major issues:\n\n1) The claims of biological plausibility and relation to how real brains work are risible. Hebbian assemblies are ultimately still a supposition in the literature, and there is no evidence for the strict clustering that the BEAN regularizer induces. Indeed, there is actually good evidence that when the neocortex is actively processing sensory stimuli neurons *decorrelate*, presumably for better information transmission (see e.g. Harris, Kenneth D., and Alexander Thiele. \"Cortical state and attention.\" Nature reviews neuroscience 12, no. 9 (2011): 509). As someone who has actually done many recordings in animal\u2019s brains, I can tell you that the activity looks nothing like the BEAN profiles shown in Figs. 2 & 3, and actually looks a lot more like the MLP and ResNet data. Also, the best evidence that does exist for Hebbian assemblies lies in the mnemonic circuits of the brain, like the hippocampus and amygdala (see e.g. Josselyn, Sheena A., and Paul W. Frankland. \"Memory allocation: mechanisms and function.\" Annual review of neuroscience 41 (2018): 389-413.). These Hebbian assemblies are likely the results of recurrent connections between neurons and Hebbian plasticity on those connections, not feedforward connectivity with a regularizer that enforces stimulus driven correlations. Thus, the BEAN regularizer is both biologically implausible, and it results in representations that do not look like the representations found in real brains. The claim that this has anything to do with biology is just plain misguided.\n\n2) What BEAN really does is force clusters on the network. That is explicit in the regularization term. As such, all of the figures showing clusters in the BEAN networks is really just confirmation that the regularizer does what it is designed to. Now, given that this is not biologically relevant/plausible, is this at least maybe a good thing to do for machine learning purposes? The few-shot data is interesting, and suggests it might be. But, I have a sneaking suspicion that BEAN regularization will only work well this way on categorization tasks where the enforcement of clusters is likely to be beneficial. How would this do with RL, or more standard regression? Not so well, I bet. Moreover, I wonder what the performance of the BEAN networks is after more training. Is it better or worse than the other regularizers? That no data is shown that way is suspicious, and leads me to suspect that BEAN sacrifices end of training performance for rapid learning in categorization tasks, due to the enforcement of hard category clusters early in training.\n\n3) The interpretability claims in this paper are hard to swallow. Let\u2019s clarify what has actually been demonstrated. The authors show that: i) BEAN forms clusters in the last layers that correspond to specific categories, ii) lesioning these clusters has a major impact on the associated category performance. In other words, enforcing clusters with a regularization term leads to clusters. We already knew that. That has nothing to do with interpretability, per se. Moreover, to return to the question of biological connections, the authors seem completely unaware that real neural activity is in large part uninterpretable and untuned!!! (See for example: Olshausen, Bruno A., and David J. Field. \"What is the other 85 percent of V1 doing.\" L. van Hemmen, & T. Sejnowski (Eds.) 23 (2006): 182-211. or Zylberberg, Joel. \"Untuned But Not Irrelevant: The Role of Untuned Neurons In Sensory Information Coding.\" BioRxiv (2017): 134379.). Thus, far from being a demonstration that BEAN has used a biological principle to inform the design of more interpretable neural networks, this data essentially shows that BEAN is clearly nothing like what occurs in the brain, and is really all about clusters (which again, we already knew).\n\n4) The inclusion of the graph theory and BIG-ADO rule commentary is completely out of nowhere and has no bearing on the rest of the paper. I do not see why it was included.\n\nAltogether, this paper is not suitable for publication. I recommend strongly to the authors that for future submissions they drop the pretense of any biological realism, and instead just focus on the question of whether the cluster enforcing regularizer actually improves few shot learning in a manner that has practical implications for ML."}