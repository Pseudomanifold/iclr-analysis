{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors investigate a novel form of regularizer for feedforward neural networks, that adds a penalty to the cost function for pairs of neurons with: a) similar connection weights to the next (downstream) layer, and b) dissimilar activations. This regularizer pushes the networks to form cell assemblies: groups of neurons that receive similar inputs (hence, having similar activations), and make similar outputs. Experiments indicate that the regularizer works as intended: such assemblies do in fact form.\n\nMoreover, on few shot learning from scratch tasks, networks with the BEAN regularizer(s) outperform ones with other regularizers.\n\nI think this is a potentially useful idea, although there are several factors that make me skeptical, and reduce my enthusiasm. Those are outlined below.\n\n1) I need more information before I can evaluate the performance comparison and clustering comparison (Table 1 and Fig. 2). Specifically, what were the regularizer weights (vs. the cross-entropy weight) in the loss functions? (How) were these hyper-parameters optimized for each of the regularizers? Is it possible, for example, that the authors are comparing networks with the optimal BEAN regularizer weights, to networks with sub-optimal dropout keep rates, in Table 1?\n\n2) It's not clear to me how BEAN-2 is meaningfully different from BEAN-1. BEAN-2 appears to be an element-wise squaring of BEAN-1: in the extreme case of binary variables (0 or 1), then, BEAN-1 and BEAN-2 should be identical. Perhaps I've missed something, but these really look the same.\n\n3) It seems like the most important thing the BEAN regularizer does is to reduce entropy in the unit activations, by making groups of units do more-or-less the same thing.  While that does seem to be a benefit for underconstrained tasks, like few show learning from scratch, I wonder if the same (or similar) benefits could be derived from simply having fewer hidden units? A more careful comparison across network sizes would help resolve this question."}