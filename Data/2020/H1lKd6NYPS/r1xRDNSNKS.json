{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a meta reinforcement learning approach where a meta-critic is trained in addition to a conventional actor and critic, so that the meta-critic boosts the training of the actor over a single task. The approach is combined to DDPG, TD3 and SAC and is claimed to convey superior performance (or learning speed) over these state-of-the-art actor-critic algorithms.\n\nAt first glance, the paper looks convincing, but closer inspection reveals a potential issue that I would like the authors to discuss.\n\nThe first cue is that in Fig. 6, the meta-loss does not seem to converge to anything, as the authors say it just fluctuates. Shouldn't it converge once the actor converges to close to optimal performance? \n\nThe second cue is that appart from the rllab tasks (ant and half-cheetah), it is not clear that the meta-critic approach brings some gain in the end of training. Particularly in Reacher (Fig 7), the performance seems to collapse faster with DDPG-MC than with DDPG, and on the rest of curves of Figs 7 and 8 it is had to determine whether the MC approach brings something significant or not. And in Fig. 3, in Walker2D, TD3-MC looks rather unstable.\n\nThe third cue is that in Section 3.2, the paragraphs \"Updating MC parameters\" and \"Designing MC\" are rather unclear (I'll come back to that) and lack a theoretical justification.\n\nSo I'm wondering what exactly MC is doing and I would like to see a more detailled analysis. Couldn't a similar performance improvement be obtained by just increasing the actor learning rate and addiing some noise to the actor learning gradient? Isn't this more or less what the meta-loss does, when looking at Fig. 6?\n\nTo me, unless the authors give a clear answer to the points above, the paper should be rejected as it does not provide clear enough evidence and justification in favor of the proposed meta-learning approach.\n\nAs stated above, I found the paragraph \"Updating MC parameters\" lacking a principled justification.\n\nAbout \"Designing MC\", it could be much clearer. You first express two requirements (i) and (ii). Fine.\nThen you explain how to meet requirement (i), but you say this is not what you are doing (!), and then you try to explain what you are doing but without going back to the requirements. In particular, it is not clear at all why your design is \"permutation invariant\". You should be much more direct. Besides, the way to extract features and the relationship to batch-wise set-embedding should be made more explicit. Well, it is complicated and should be explained more clearly...\n\n\"TD3 borrows the Double Q learning idea...\": no, TD3 does more than this, as it uses the min between both critics, which double Q-learning and DDQN do not.\n\nYou use HalfCheetah-v2 from gym-mujoco and HalfCheetah from rllab. Why? One might suspect this is because the performance of the MC appraoch is only better with rllab...\n\nYou say you are using 10 million steps, but this is not always the case.\n\nIt is interesting to see that the performance of TD3 on Half-Cheetah collapses at around 4 million steps. Any idea why? It seems that in many papers experiments are stopped before performance collapses, and a strong study about this phenomenon is missing as the authors don't want to show this. This is an open call to readers: a paper on that would be great! :)\n\n\"SAC-MC\" gives a clear-boost for several tasks\": well, looking at the figures it is not so clear. Do you mean faster learning, higher final performance or both? I would like to see this claim backed-up by some proper statistical significance test and a clear specification of the number of seeds, etc. Fig.5 conveys the adequate data for such test if your claim is that this is the final performance that matters (but beware that curves are crossing eachother, so the final performance depends a lot on where you stop...).\n\ntypos:\n\np1 \"For example, ... (Zheng et al., 2018).\" is not a sentence (no main verb).\np2 \"the on-policy (approach?) needs to interact with (the) environment...\"\np2 \"is less effective than off-policy transitions.\" => unclear statement\np3 to assist(s)\n\nEquations (2), (6) and (8) should finish with a dot as they close a sentence.\n\nEq X => Eq (X) : use \\eqref{label} instead of just \\ref{label}\n\np5 It's input => Its\np5 two key technologies: I would not call this a technology. Ingredients?\np5 \"computational cost is ? by using\": missong word\n\np6: asmyptotic\n\np9: we removing\n"}