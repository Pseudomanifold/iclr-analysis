{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In actor-critic algorithms, the policy tries to optimize cumulative discounted rewards by a loss formed with components from the critic. In this paper, the authors propose a novel way, namely, meta-critic, which utilizes meta-learning to learn an additional loss for the policy to accelerate the learning process of the agent. There are several advantages of the proposed method, it's learned online for a single task, it's trained with off-policy data, it provides sample efficiency and it's generally applicable to existing off-policy actor-critic methods. \n\nOverall the proposed method is novel and the research direction is a very interesting one to explore. Eqn (3) and Eqn (4) explains the key idea of the proposed method. Eqn (3) describes the meta-learning problem as a bi-level optimization problem where the agent is updated with the main loss L^main with data d_train, in addition, it's updated with L^aux where the loss is learned and parameterized by \\omega. After the agent being updated, it uses L^meta and data d_val to validate the performance of the updated agent. Eqn (4) describes an explicit way to formalize the usage of the auxiliary loss, which is to accelerate the learning process. Thus the meta loss is whether the L^aux helps the learning process or not. \n\nHope that the authors could address the following issues in the rebuttal:\n1) Investigate why DDPG with meta-critic gets much more improvements than TD3/SAC;\n2) Show SAC (or TD3) could get better performance on harder task. (I can understand that they are strong baselines and hard to improve on the current environments, however, for harder environments, there might be rooms for improvements.);\n3) Investigate different ways to parameterize the meta-critic other than simple MLP;\n\nA few minor points:\n1) Page 2 first paragraph, \"This is in stark contrast to the\nmainstream meta-learning research paradigm \u2013 where entire task families are required to provide\nenough data for meta-learning, and to provide new tasks to amortize the huge cost of meta-learning.\". Try to revise it and avoid the words like \"mainstream\". Think about the paper being read 5 years later or even more;\n2) Consider introducing a weighting hyperparam between L^main and L^aux in Eqn (3), these two losses might have different scales and it might be better to weigh them differently;\n3) Minor literature detail: Page 7 \"Comparison vs PPO-LIRPG\" mentioned that \"Intrinsic Reward Learning for PPO (Zheng et al., 2018) is the only existing online meta-learning method that we are aware of.\", however, AFAIK, \"Meta-Gradient Reinforcement Learning\" in NeurIPS 2018 and \"Discovery of Useful Questions as Auxiliary Tasks\" NeurIPS 2019 are methods where meta-learning is applied online and for a single task;"}