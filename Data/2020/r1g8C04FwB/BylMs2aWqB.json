{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a model for handwriting generation. In specific, it argues that the publicly available datasets for supervised learning are limited in volume and breadth and as such they hinder usability in real-life scenarios. For this reason it proposes an approach to generate hand-written text from freely available handwriting-like fonts.\n\nThe hypothesis that the paper makes is rather reasonable, as is the proposed model, which is also for the most part well-exposed, making the paper mostly easy to follow. Paper writeup is at times unclear and convoluted, and would benefit from a round of revisions. \n\nThe main concern that I have with the paper is the experiment depth: In specific, the paper does span a number of related works, but it does not empirically expose the systems comparison in sufficient detail. While the Results section (and beyond) does in detail analyze the performance of the proposed model, the comparison to related work does not extend beyond the results tables. This is in stark contrast to the strong claim (already in the abstract) that supervised learning falls short in the task. Why does it fall short, in terms of error analysis?\n\nI vote weak accept because the method is neatly exposed and the results are convincing.\n\nMinor remarks:\n- Does the title contain a typo? Shouldn't it be \"spline template-based\"?"}