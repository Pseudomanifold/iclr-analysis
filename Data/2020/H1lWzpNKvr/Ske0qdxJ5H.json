{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper approaches the problem of exploding arms in multivariate multi-armed bandits. To solve this problem, the authors suggest an approach that uses Thompson sampling for arm selection and decision trees and graphs (inspired from Monte-Carlo tree search) to model reward success rates. They propose to versions of this path planning procedure and 2 version inspired from Hill-climbing methods. They validate their claims on a simulation showing better performance and faster convergence, and providing an extensive analysis of the results. \n\nThe idea seems novel. The paper is well structured, but the writing can be improved, and some parts are hard to read and follow (See minor comments below). \n\nHere are few questions:\n- The authors claim that the approach can be extended to categorical/numeric rewards. Can they give more details on how? \n- The experiments are done only with D=3 and N=10. How easy would it be to scale to higher dimensions?\n- In the curves of Figure 3, N^D-MAB and DS seem not converged yet contrarily to the other methods. Have the authors tried to let them run for longer to see to which values they converge? \n\nMinor comments (non-exhaustive examples): \n- Punctuation issues: \n    *Multi-Armed Bandit (MAB) problem, is widely ...\n    * Generally, RT is on the order O( T) under linear payoff settings Dani et al. (2008)Chu et al. (2011)Agrawal & Goyal (2013). Although the optimal regret of non-contextual Multivariate MAB is on the order O(logT ) ...\n\n- Imprecise statements:  ... for each combination of C (assuming not too many), ...\n\n- Sentences to rewrite: \n     * The posterior sampling distribution of reward is its likelihood integrates with some fixed prior distribution of weights \u03bc.\n     * ..., and would call joint distribution of (A, C) and (B, C) are independent. \n     * ..., which means the algorithm converges selection to single arm (convergence) as well as best arm. \n\n- Typos: Jointly distribution/relationship -> joint ,It worth to note -> it is worth to note, extensively exam -> extensively examine, a serial processes -> process, would been -> be ...\n "}