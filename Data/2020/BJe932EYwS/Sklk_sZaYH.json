{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work proposes a non-autoregressive model for conditioned text generation. The non-autoregressive decoder conditions on a sequence of discrete latent variables, which represent the generation order and can be autoregressively calculated. Instead of doing marginal inference, the paper takes the top 1 generation order that best match inputs. Experiments on machine translation and paraphrase generation show strong result in comparison to other non-autoregressive models.\n\nThe idea of delegating generation order to the latent variables seems interesting, and the paper takes a reasonable approach when fleshing it out. Most of the experiments seem solid to me. The presentation does a good job in giving a very brief description of the model to readers familiar with non-autoregressive generation; but for those who are not (like myself), much content needs to be clarified.\n\nDetails:\n\n- How is the model trained? Eq.13--14 present the loss, does it ever include the length predictor? Is the computation graph fully-diiferentiable? If it is, how do the authors backprop through the argmax in the length? If not, is reinforcement learning used?\n\n- Can the authors specify B around Eq.5? Also \\Delta M above section 4.3.\n\n- Is the length predictor basically a classifier? Have the authors considered doing regression, which preserves the order relation (e.g., 3 is less than 5)?\n\n- HSP. Can the authors describe the matching algorithm in detail? If it is some well-known algo like the Hungarian algorithm, please specify. If the authors come up with their own algo, please typeset it and analyze the complexity.\n\n- Adding onto the above: is the algorithm exact or approximate? If the later, Eq.11 should not be \\argmax.\n\n- Table 1 is really confusing: what do NPD, WT, i_dec stand for?\n\n- 3.2 discusses two position predictors. Which of them is actually used?\n\n- Tabl 2: what does `Searched-Position` stand for? I thought the position search is only used to derived the reference position sequence.\n\n- It is a bit sad that the paper started with marginal inference but ends up taking the top 1. Have the authors considered taking top K of the predicted orders to have a better approximation to the likelihood?\n\n\nMinor:\n\n- The second paragraph in intro. I'm not sure why non-autoregressive model has a larger search space. Both of them search over \\Sigma^\\ast, with \\Sigma being the vocabulary.\n\n- Typo above section 4.4: ... not powerful enough...."}