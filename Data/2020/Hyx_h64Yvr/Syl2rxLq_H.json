{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose to reduce the memory and computation complexity of attention networks applied to 2D data (images) by using, in the attention operator, the mean over the rows and columns of images instead of its vectorized version.\nThe paper is relatively well written, based on intuitive ideas and including clear figures. Several experimental results are reported but, in my opinion, they are not convincing enough. \nBelow, a list of issues is presented:\n-\tI think the term \u201cKronecker\u201d in the name of the model is not well justified or explained. I don\u2019t see a Kronecker product of matrices in the attention operator. Maybe, only in the case of KAO_{QKV} the outer sum could be written as a Kronecker product of some matrices justifying the name. I think this is not a big issue but it is a little bit annoying to use a term that is not naturally introduced to the reader.\n-\tIn my opinion, the main issue is that taking the mean over columns and rows as the only information to construct the attention operator seems not able to capture all relevant information about the underlying dataset. I think there is not a theory that justifies this approach. For example, it could be possible that other linear or nonlinear combination of rows and columns provides better attention operators. \n-\tIt is not clear how the theoretical results (Theorem 1) helps to support the proposed method. Maybe the authors could better explain the implications of their theoretical results. I found the introduction of the method clear but rather disconnected from the theoretical sections 3.3-3.5.\n-\tExperimental results are not convincing. I cannot agree with the author\u2019s claim that \u201cKANets significantly outperform the previous state-of-the-art compact models\u201d. Below I make comments to each of the presented experimental results.\no\tThe experimental results about memory and time used by regular attention operator compared with KAOs is obvious and not relevant because their theoretical complexities are well known.\no\tIn Table 2, it is shown that the proposed method is better than other non-attentional networks. I think this result is not relevant. On the other side, in Table 3, they compare the proposed method against two other attentional networks and show that the proposed method is not better than previous methods in terms of performance. The only advantage seems to be a slight reduction in number of operations which is marginal. \no\tIn the Ablation Studies (Table 4), the method is not compared with other classical attention network. They only compared against MobileNet. I wonder how the performance results would be if classical attention networks were used having the same level of number of parameters.\no\tThe results on Image Segmentation are not relevant because the proposed method is not the best one.\n"}