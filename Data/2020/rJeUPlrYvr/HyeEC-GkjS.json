{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper studies the Neural Network Pruning problem. It proposes an FNNP method, which incorporates an Adaptive Batch Normalization (ABN) based evaluation module to connect the accuracy of pruning candidates and their final converged accuracy. The experiments show that the ABN-based sub-net evaluation module facilitates fast and accurate neural network pruning.\n \nMy major concern of this paper is that it fails to state the motivation to adopt adaptive BN clearly. Based on the existing statements, ABN would be fast since it recalculates the statistics for BN based on a small part of training data. And It may be accurate since this evaluation module establishes a strong correlation between the sub-nets accuracies and their converged accuracy. However, the key reason why the global BN can cause week correlations and why adaptive BN can establish a strong connection is never pointed out. It is not convincing to simply experimentally evaluate the baselines achieve lower correlations to support the claim.  \n \nWith this fatal deficiency, although the results seem empirical, it is hard to evaluate the methodical contribution of this work. Other questions are listed as follows. \n \n1). The right panel of Figure 5 is ambiguous. Is the horizontal axis represents the number of Iters? Can you illustrate the distance results more clearly?\n"}