{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed an adversarial generative model of a formal grammar, called \u201cadversarial grammars\u201d, whose generated grammars have multiple possibilities. Thanks to multiple options of generated grammars, the proposed method can produce a long-term future prediction. The proposed method achieved state-of-the-art performance on the future 3D human pose prediction task and the future activity prediction task.\n\nThe paper tackles an exciting topic to attendees of ICLR, but the paper contains several issues that I will try to cover in the following.\n\n(1) (No clarity about contributions)\nFirst: The related works section is not well described. Several relevant papers are shown in the related works section, but the relationship is ambiguous. How are grouped existing works which tackle the same problem? Where is placed the proposed method in that grouped literature?\nSecond: Contributions are not clear. At the beginning of the introduction section, the authors noted that they addressed the problem of modeling sequential dependencies and multiple possible long-term future predictions. However, those challenges are well studied in the machine learning literature, and treated by a lot of works such as RNN. For example, a simple 1-layer RNN can model the sequential dependencies as its latent representations and can generate multiple possible predictions with multiple evaluations. Please explain your contributions more clearly.\n\n(2) (Incremental proposed method)\nThe proposed method can be regarded as a simple extension of GAN to produce formal grammars. This type of extension is already well-studied in other application areas such as CV or NLP. I\u2019m afraid of the low impact of this paper in its current form.\n\n(3) (Presentation)\nThe presentation is slightly confusing.\n- In equation (5), what is \u201cmin max =\u201c? Should we remove the \u201c=\u201c?\n- The type of each variable or function is ambiguous. For example, what is the type of the function \u201cG\u201d? Does \u201cN \\rightarrow {(N, \\Sigma)}\u201d mean \u201cN \\rightarrow 2^{N \\times \\Sigma}\u201d?\n- From the introduction section, it seems that the proposed method can be applied to so many future prediction problems. However, it verified only on the pose prediction task and the activity prediction task. I\u2019m afraid of the statement by authors too strong.\n- The architecture of the proposed method is explained in detail in the approach section. The overall architecture figure helps readers understand the proposed method."}