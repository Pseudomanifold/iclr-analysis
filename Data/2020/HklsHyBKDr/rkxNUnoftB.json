{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThe paper investigate how optimal recurrent neural networks (RNNs) are at storing past information such that it is useful for predicting the future. The authors estimated optimality in terms of mutual information between the past and the future. If the RNN was able to retain MI between the past and the future, it then has kept optimal information from the past for predicting the future. The experiments suggest that RNNs are not optimal in terms of prediction of the future. It also suggest that this is due to the maximum likelihood training objective.\n\n\nComments for the paper:\n\n1. Overall, the paper is a very interesting read and it explores and analyzes RNN under a different light. It answers a fundamental question about RNN training.\n\n2. There are a few things that would be nice to clarify. At the end of P3, the authors mentioned that the stochastic RNNs are trained either by a). deterministically during training and noise added during test or b) noise added during training and test. It is not very clear to me how the authors trained stochastic RNNs deterministically during training. It would be nice if this can be clarified. \n\n3. I am also curious how this compares to the training methods for example used in https://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf. It seems that this would also help with retaining RNN optimality in terms of predicting the future. it would be interesting to include a comparison to this method for example.\n\nOverall an interesting paper. However, I think a few things could be improved and I would be willing to rise the score if the authors could addressed the above points.\n"}