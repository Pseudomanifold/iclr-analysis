{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed to evaluate model performance when the ground truth labels were not available and noisy labels provided by multiple uncertain experts were provided instead. The proposed evaluation metric, called discrepancy ratio, is defined as the ratio between the average model-annotator discrepancy and the average annotator-annotator discrepancy. It can be applied to compare 1) the relative performance of different models; and 2) the relative performance of average annotators and the model. Experimental results on the MNIST classification task showed the proposed metric can compare model performance under different noise levels and is robust w.r.t. the number of annotators and a single annotator's strictness. Experimental results on a real-world medical image classification was also presented. \n\nThis work is well motivated. In real-world applications, such as medical image classifications, obtaining ground-truth labels is difficult or even infeasible, only multiple noisy labels are available to evaluate the model performance. Therefore, the setup of this problem seems novel (besides Rajpurkar et al. 2018).\n\nIn the experimental results on MNIST classification, for Naive Evaluation and Majority Vote, as the author pointed out, the gap between three models (model 1.0, model 0.9, model 0.8) becomes narrower when the label swap probability increases, indicating it's becoming more difficult to differentiate model improvements. However, this is also true for the proposed discrepancy ratio, as is shown in Figure 2(b). Therefore, the discrepancy ratio didn't overcome those \"drawbacks\" associated with these two baselines, correct? In other words, in the high-noise regime, it would be natural to observe narrower gaps between those models. \n\nIn MNIST classification, the performances gap of those three models evaluated by Naive Evaluation and Majority Vote didn't decrease rapidly as the noisy level increases. This is consistent with the constant gap between their ground truth accuracies (gap=10% between 100%, 90%, 80%). However, for the proposed discrepancy ratio, the gap between these three models decreases rapidly when the noise level increases from 0 to 0.4. This is an undesirable behavior since it over-estimate the performance gap in the low-noise regime. Since most real-world datasets could belong to this low-noise regime, this rapid change of performance gaps seems problematic.\n\nIn section 4.1.4, the claim that \"It is in this regime that the model performance can be said to be better than the average human performance\" seems questionable. When the ratio is less than 1, it simply means the annotator-model agreement is larger than the annotator-annotator agreement. What's the definition of the \"average human performance\" in this context?\n\nIn the work, multiple assumptions were made regarding different annotators. First, multiple annotators are assumed to be conditionally independent given the ground-truth label. Second, each annotator should provide high-quality labels. Third, different experts should have similar expertise. However, in medical applications, any of these assumptions could be violated. For example,1) multiple experts could provide similar labels given their similar knowledge background. It's more likely to observe multiple groups of experts, where experts in the same group are similar. 2), experts could provide random labels when they are extremely uncertain 3) different experts could have varying levels of expertise. Could the author provide some ideas on how to relax some of these assumptions?"}