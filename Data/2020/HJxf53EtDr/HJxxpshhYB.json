{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper derives a matrix-factorization approach for node classification. The approach is closely related to GCN. The authors show that the proposed approach outperforms GCN and Planetoid empirically.\n\nThough empirically appealing, this paper has a few pitfalls that need be addressed.\n\n1. The wording \"unifying\" is a misnomer. The title \"unifying graph convolutional networks\" hallucinates a framework that unifies several neural network architectures, which is not precise. In reality, the authors propose a learning objective that consists of two loss terms, the classification loss and the structure loss. The classification loss is nothing but the usual GCN. The structure loss is the contribution of the paper. The derivation of this term starts from GCN and a Laplacian smoothing argument, and arrives at a matrix factorization form through a series of modeling modifications. By and large, the title is misleading.\n\n2. The wording \"correctness of our theoretical analysis\" is dubious. The paper does not present a theoretical analysis. The derivation of the matrix factorization is only a modeling process. In no mathematical sense the factorization is equivalent to GCN.\n\n3. The alternating training is questionable. The authors propose alternately optimizing the structure loss and the classification loss. Since taking the gradient of the whole loss function is straightforward in all graph neural network approaches, it is unclear why the authors prefer the alternating optimization approach. Supplementing a convergence plot and comparing the two approaches may help, if the alternating approach is indeed better.\n\n4. The \"distributed computing\" component needs more substantiation. It is unclear whether this phrase actually means the concept familiar by the parallel computing community. Therein, computation is done by using several machines communicated through networked protocols. Machine setting, parallel implementation details, and speedup are the primary interests in distributed computing. All information should be reported.\n\nQuestions:\n\n1. First sentence of section 5. What does \"all-round\" mean?\n\n2. Stability Analysis. What is b? The reader does not find a definition elsewhere. A probably related concept is alpha (see eqn (8)).\n\n3. Figure 3(b) shows that larger b leads to poorer performance. The authors state that a larger b means a stronger emphasis on the structure loss. Consequently, it appears that putting more emphasis on the structure term leads to poorer performance. Then, does it mean that the structure term is a useless contribution?\n"}