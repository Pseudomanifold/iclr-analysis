{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors propose a new method for semi-supervised node classification by drawing connection between GCN and MF. The authors borrow the idea of convergence of GCN as Laplacian Smoothing. With this observation, the authors propose a joint loss with two components: classification loss and structure loss for the similarity between embedding of neighboring nodes. The authors train the parameters via optimizing the two losses alternatively. Experiments are carried out on seven networks with comparison to baselines.\n\nStrength:\n1. It is an interesting and innovative idea to draw connection between GCN and MF.\n2. The propose method is more suitable for distributed setting. With negative sampling for structure loss, both structure batch and classification batch can be constructed locally with only one-hop information.\n3. The authors carry out experiments on seven real-world networks with ablation study for components in the model. Moreover, the authors carry out comparison to baselines in distributed setting.\n\nWeakness:\n1. The connection of GCN to MF is very indirect. It holds only when the GCN converges to the Laplacian smoothing. It is not clear whether this holds empirically. Moreover, there are too much intermediate steps and approximation between the Laplacian smoothing to the matrix factorization. As far as I am concerned, the connection is closer to node embedding versus matrix factorization.\n2. Given that GCN serves as Laplacian smoothing, it would be great if the authors can simply add additional regularization on dis(h_i, h_i) for (v_i, v_j)\\in E. Moreover, there is no reference and description to the Planetoid* algorithm.\n3. The authors use alternative batches between structure and classification loss. It would be interesting to see if joint training the two loss in mini-batch among a node and its neighbors can leads to any difference.\n3. The authors report only accuracy as evaluation metrics. It would be better If the authors could report recall@K and F1 score as well.\n"}