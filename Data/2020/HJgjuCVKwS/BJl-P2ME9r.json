{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a method to solve, what they call, the \u201cGeneralized Few-Shot Learning\u201d problem. This problem involves learning a set of novel classes from few labeled examples, while still being able to classify examples from the base classes for which we have a lot of data. \n\nThe method is the following: they first learn an embedding network and fully-connected classification weights for the base classes for which we have a lot of data. Then, there is a meta-learning stage, in which we treat some classes from the base classes as novel classes, and learn how to generate new fully-connected classification weights for these novel classes so that these new weights combined with the previous weights are able to classify a held-out set of examples from both base and novel classes. The generated weights are generated using a learned dictionary, where we learn a set of keys and values. The prototype from each novel class is used as a query in an attention mechanism scheme. A weighted mean of the values is computed, where the weight is the similarity of the corresponding key and original query. The prototype and this weighed mean are added together to serve as the new classification weight for the novel class. Thus, during the meta-learning stage, we are learning keys and values so that this weight generation process works for novel classes. The method is then tested on a new set of novel classes to see how well it is able to learn each new few-shot learning task, while still being able to generalize to the base classes. Experiments are performed on mini-ImageNet and Tiered-ImageNet.\n\nThis paper is very similar to the work by Gidaris & Komodakis. To me, the only difference between that work and this submission is: in the attention mechanism, rather than directly using the fully-connected layer weights of the base classes as the values during weight generation, here a separate set of bases are learned to serve as values.\n\nI believe that the pre-training step, the loss used during meta-learning, sampling base classes to use as novel classes during meta-learning, and even using multiple episodes in a batch (where different base classes serve as novel classes) are all borrowed from Gidaris & Komodakis; However, in the paper, it seemed to me that many of these techniques are presented as being novel. In the related work, it is mentioned that \u201c[compared to Gidaris & Komodakis]\u2026 we differ in how we compose classifiers and the unified learning objective.\u201d As mentioned, the generation scheme is a bit different, but I don\u2019t see a difference in the learning objective between the two papers?\n\nThus, if it is true that the only difference between the two methods is a minor difference in the attention mechanism of the weight generation scheme, I feel that should be better reflected in the writing of the paper and I don\u2019t think this is enough of a contribution to merit acceptance of the submission.\n\nReferences\nGidaris & Komodakis. Dynamic Few-Shot Visual Learning without Forgetting. CVPR 2018.\n"}