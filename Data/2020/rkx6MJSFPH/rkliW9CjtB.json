{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper uses GANs to generate unrestricted adversarial examples for the task of semantic segmentation, which is traditionally harder to attack via traditional methods (like norm-bounded perturbations). They add an extra term to the GAN loss function for the SPADE generative model. The authors show through experiments on two datasets that their newly generated examples preserve most of the visual realism while also being adversarial for semantic segmentation models.\n\nI vote to weakly accept this paper. The key reason is that the authors present impressive experimental results in terms of both visual realism and fooling classifiers more consistently.\n\nThe authors show through experiments on two datasets that their method, called AdvSPADE, generates images that are almost as realistic as the original SPADE. The authors achieve this by adding the Unrestricted Adversarial Loss defined in equation (1), which seems like a reasonable way to encourage generated examples to fool semantic segmentation models. While the difference from SPADE is fairly small, the authors do perform extensive experiments and provide good baselines to show the effectiveness of their method.\n\nIn particular, I am happy that the authors compared AdvSPADE with SPADE + FGSM/PGD, which seems like a reasonable baseline. They show that, if we are encouraging realism of the image to be maintained, AdvSPADE is more effective than SPADE + FGSM/PGD. For various different segmentation models, the authors show improvements in terms of the mIoU metric, and they also show in Table 2 that their generated AdvSPADE examples are still fairly realistic (at least by the FID metric).\n\nI would like to see certain things addressed in a revision though.\n\nThe text in Table 1 is quite small even though the results are important. I would want bigger text. Additionally, it is a bit eye-opening that the method degrades accuracy for DRN-105 on Cityscapes by so much, but it does not degrade accuracy for similar models like DRN-38 and DRN-22 by much. If this can be explained, that would be helpful.\n\nFor the PGD attacks described on page 7, how can you set the number of attack iterations if eps is not an integer (wouldn\u2019t eps+4 not be an integer either?)?\n\nFor the Semantic Consistency Test, I think the human task (choosing \u201cmatching\u201d or \u201cnot matching\u201d) is a lot easier than the machine task (of segmenting the whole image), so it\u2019s unclear what those results should indicate. Perhaps a better task would be to show the real annotation and the annotation from the segmentation network on AdvSPADE, and ask a human to classify them as \u201csimilar\u201d or \u201cnot similar.\u201d\n\nFinally, I am also confused by the Fidelity AB Test, where Amazon Mechanical Turkers are asked to compare the realism of images generated from SPADE with AdvSPADE. In these, AdvSPADE is actually favored by a fairly large margin over the original SPADE. Do you have any insight for why this happens? It seems like AdvSPADE is unintentionally improving image quality by a lot.\n\nAdditional Feedback:\n\n- \u201csynthesis images\u201d -> \u201csynthetic images\u201d everywhere\n- The last sentence of \u201cRelated Work\u201d has quite a few errors and should be corrected.\n- In Section 4, for the very last paragraph, first sentence, it should be \u201cdoes not lead an image to fall\u201d instead of \u201cdoes not lead an image fall\u201d"}