{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the problem of unrestricted adversarial attack in the image domain. Building upon the idea of previous work (Song et al. 2018), this paper leverages the state-of-the-art image-to-image translation model SPADE (Park et al. 2019) to synthesize high-resolution adversarial images from semantic segmentation maps. Compared to Song et al. 2018 which mainly experimented with MNIST and face images, the proposed AdvSPADE conducted experiments on both real images captured indoor (ADE20K) and outdoor (Cityscapes). Experiments demonstrated that the proposed AdvSPADE is able to generate white-box and transfer-based black-box attacks with reasonable performance (see Table 1). Furthermore, the paper compared against traditional Lp-bounded attacks in terms of attack success rate (see Table 4), mIOU (see Table 5) and robustness.\n\nOverall, reviewer feels the paper in the current form is not ready to be published at ICLR for the following reasons.\n\n(1) Main motivation of the paper is not very clear. First, images generated by the AdvSPADE have noticeable artifacts (e.g., distortion, textures) and look quite different from the corresponding real images. Reviewer is not pretty sure if such \u201cunrestricted\u201d image manipulation (every pixel has been re-synthesized) is really meaningful or not for semantic segmentation model in the real world. In contrast, \u201crestricted\u201d image manipulation or local image editing seems like more natural settings for studying the unrestricted adversarial examples.\n\n-- 3D-Aware Scene Manipulation via Inverse Graphics. Yao et al. In NeurIPS 2018.\n-- Context-aware Synthesis and Placement of Object Instances, Lee et al. In NeurIPS 2018.\n-- Learning Hierarchical Semantic Image Manipulation through Structured Representations. Hong et al. In NeurIPS 2018.\n-- Free-Form Image Inpainting with Gated Convolution. Yu et al. In ICCV 2019.\n\n(2) Technical novelty of the paper is limited. As far as reviewer can understand, this paper looks like a straight-forward extension of Song et al. 2018 by leveraging the state-of-the-art image-to-image translation model. There is not much technical novelty in the paper. Also, the paper writing (especially Section 3) requires significant improvement. For example, the second term \\mathcal{L}_{FM}, the third term \\mathcal{L}_{VGG}, and fourth term \\mathcal{L}_{KLD} are never formally defined (as they are not clearly defined in SPADE paper).\n\n(3) Ablation studies on hyperparameters \\lambda_{0,1,2,3} are missing. First, it is not crystal clear how these hyper-parameters were decided for the proposed AdvSPADE. Second, in the comparison with vanilla SPADE + perturbations, it is not clear whether the hyper-parameters are optimal for the baseline vanilla SPADE + perturbations in terms of adversarial attacks.\n\n(4) The proposed pipeline (see Figure 2) is largely adapted from Song et al. 2018 and Xiao et al. 2018 (advGAN). But the second paper is never mentioned in this submission. The difference is that Song et al. 2018 and AdvSPADE used a conditional GAN framework to synthesize images from class label, while Xiao et al. proposed to synthesize perturbation using an encoder-decoder GAN framework.\n\n-- Generating Adversarial Examples with Adversarial Networks, Xiao et al. IJCAI 2018.\n\nFor a fair comparison to Lp-bounded attack using a generative model, this paper could consider a much stronger AdvGAN baseline (generate perturbation instead of image) using the SPADE architecture.\n\n(5) Reviewer is surprised to see the ineffectiveness of pixel-perturbation attack on real images or synthesized images (see Figure 1), as the Houdini loss proves to be very effective in attacking the semantic segmentation model detailed in the following paper.\n\n-- Houdini: Fooling Deep Structured Prediction Models. Cisse et al. In NIPS 2017.\n"}