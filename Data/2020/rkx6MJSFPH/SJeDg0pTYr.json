{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper mainly studies the robustness of semantic segmentation network by designing novel unrestricted adversarial examples. The authors propose to obtain adversarial examples through generative adversarial networks with SPADE structure. Through experiments, the authors try to prove that the adversarial examples are indistinguishable\nto natural images for humans, while can cause great attack for the current state-of-the-art models. Furthermore, the authors argue that these adversarial examples can help improve the robustness of existing models through adversarial training.\n\nClarity:\nI think this paper has complete experiments to support their opinions. However, there should be some additional experiments to make the claims more credible. Thus, I think this paper could be weakly accepted with some supplementary experiments detailed in the following.\n\nNovelty:\n1. In practice, users will often notice that there are adversarial perturbations in images if we adopt unrestricted adversarial attacks. How can such unrestricted adversarial attacks be employed in reality? Or just to improve the robustness of networks?\n2. The SPADE structure is built upon existing ideas and concepts. Are there some more effective generation structures for this task besides SPADE?\n\nExperiments Results:\n1) The authors execute experiments on two popular semantic segmentation datasets, to illustrate the attack ability of their methods by comparing with traditional norm-bounded attacks. \nLimits: \na. In Table 3 and Table 4, the authors have made a comparison under the white-box setting while not consider the setting of the black-box setting which is a more practical situation. \n\n2) The authors use FID and human evaluation to indicate that adversarial examples are indistinguishable to natural images for humans, and the semantic meanings of adversarial examples are consistent with corresponding ground truth.\nLimits: \na. For the human evaluation, the authors should indicate the total number of persons in evaluation, rather than only quantitative result in Section 5.3 and Table 5.\nb. Although authors have proved that AdvSPADE has better generation performance, this is already pointed out by the paper \u201cSemantic Image Synthesis with Spatially-Adaptive Normalization\u201d. \nIn fact, the authors should also compare with these generation methods under the attack setting: whether the adversarial examples are generated from other methods, such as Pix2PixHD, have higher attack ability if the SPADE generator is replaced by other structures.\n\n3) The experiments in Section 5.4 are designed to show that AdvSPADE can help improve the robustness of the network. \nLimits:\na. The authors have not indicated the training epoch for AdvSPADE in adversarial training.\nb. In adversarial training, the adversarial examples for training are generated according to the state of the network. Thus, the training adversarial examples are different even for the same training setting. The authors should indicate whether the adversarial training results are stable or not."}