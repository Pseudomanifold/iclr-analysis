{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper provides new understandings on adversarial robustness from the perspective of input gradient regularization. Input gradient regularization hasn't been able to achieve comparable robustness to  adversarial training. Built upon existing works, this paper derives two minimum perturbation bounds (L-bound and w-bound) to explain this, as well as other defenses such as Lipschitz regularization and defensive distillation. Taking a step further, this paper proposes to use the finite difference to estimate the input gradients, which not only gives a nice property for reduced modulus of continuity (eg. the w-bound), but also makes the regulation scalable to large networks and datasets. I quite like the theoretical connections derived in this paper. Empirical evidences support their claims, and demonstrate indeed comparable robustness of input gradient regularization to adversarial training.\n\nThe empirical results can be strengthened by including the normal input gradient regularization baseline (using double backpropagation), at least on cifar-10. This is less likely to change the conclusions, but  would be interesting to see the comparisons.\n\nNote that, there are already new progresses in adversarial training:\n[1] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" ICML, 2019.\n[2] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML, 2019.\n[3] Carmon, Yair, et al. \"Unlabeled data improves adversarial robustness.\" NeurIPS, 2019.\n[4] Uesato, et al. \"Are Labels Required for Improving Adversarial Robustness?\" NeurIPS, 2019."}