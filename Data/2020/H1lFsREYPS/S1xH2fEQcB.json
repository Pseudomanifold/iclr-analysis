{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper in the field of machine reading comprehension. The authors address the issue of generating labeled data of question-answer tuples, without the need of manual annotation. Specifically, the authors propose a method that dynamically generates K answers given a paragraph  in order to generate diverse questions and, secondly, pre-training the question generator on answers in a sentence generation task. The authors then show that this method is superior to existing baseline methods.\n\nOverall the paper is written rather well, however,  at times  it is tough  to understand because of the technical writing and heavy use of abbreviations.  The experiments make sense given the research question. \n\nI have a couple of questions:\n\n1.  Section 4&5: How often where these experiments repeated? What are the standard errors?\n2. How sensitive is the architecture w.r.t. its hyper-parameters?\n3. What was the training time/training cost when running the method? How does the number of parameters compare to previous state of the art?\n4. In Figure 5a the F1 scores of Bert+NS and Bert+AS seem to converge with increasing  data size. Why does the difference between those two methods seem to vanish when data set size increases?\n\n\nAdditional comment:  \"[..] researchers have proposed MRC models, often surpassing human performance.\"  This is a bold statement.  Machines definitely do not surpass humans in reading comprehensions."}