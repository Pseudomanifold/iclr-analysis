{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. The intuition of this method is that question generation requires to generate a sentence which contains the information about the answer while being conditioned on the given paragraph. In particular, the paper compares its approach to Devlin\u2019s presentation (https://nlp.stanford.edu/seminar/details/jdevlin.pdf according to the references; is it not a published work?) which uses next sentence generation for pretraining, that is less related to the downstream question generation task.\nThe proposed pretrained model is then used to finetune on a standard question generation task, which is then used to generate synthetic QA pairs for data augmentation in QA. The proposed method is evaluated on four datasets (SQuAD v1, SQuAD v2, KorQuAD, QUASAR-T) and have shown substantial performance gain.\n\nThe strength of this paper is clear to me: the idea in the paper is new and interesting, and they provide a good intuition of why the proposed learning objective is helpful compared to the previous methods.\n\nHowever, I have several concerns as follows.\n\nFirst, the performance improvements are marginal (less than 1.0 on three datasets, except QUASAR-T, which is actually interesting given it is under transfer learning setting).\n\nSecond, there is a very related work, \u201cDong et al. Unified Language Model Pre-training for Natural Language Understanding and Generation NeurIPS 2019\u201d which also proposes a new pretraining approach for question generation and data augmentation for question answering. Not only this submission did not discuss this paper (it was posted in May 2019, which is substantially ahead of ICLR deadline), the result is substantially worse in both BLEU score of question generation and EM/F1 of end QA performance after data augmentation on SQuAD v2.\n\nThird, the paper does not discuss any work in question generation overall, although the task of question generation was studied for decades, including question generation as data augmentation for question answering. I believe this paper should discuss previous work in question generation and compare the performance with them.\n\nSome of the work on question generation.\n- Heilman & Smith. Good Question! Statistical Ranking for Question Generation. NAACL 2010.\n- Wang et al. Learning to ask questions in open-domain conversational systems with typed decoders. ACL 2018.\n- Zhao et al. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Network. EMNLP 2018.\n- Kim et al. Improving Neural Question Generation Using Answer Separation. AAAI 2019.\n\nSome of the work on question generation for question answering.\n- Du et al. Learning to ask: Neural question generation for reading comprehension. ACL 2017.\n- Song et al. A Unified Query-based Generative Model for Question Generation and Question Answering. AAAI 2018.\n- Tang et al. Learning to Collaborate for Question Answering and Asking. NAACL 2018.\n- Sachan & Xing, Self-training for jointly learning to ask and answer questions. NAACL 2018.\n- Tang et al. Question Answering and Question Generation as Dual Tasks. AAAI 2018.\n- Lewis et al. Unsupervised Question Answering by Cloze Translation. ACL 2019.\n\nFourth, it looks like the paper uses the development set for both development and the final evaluation. The paper should either use a portion of the train set for development and treat the original development set as the test set, or just show the final model\u2019s performance on the hidden test set by submitting to the leaderboard. (Especially, for Table 3, did authors reimplement BERT+NS and evaluate on the dev set? Because Devlin\u2019s presentation only shows the result on the test set from the official leaderboard.)\n\n\n(This is not a weakness of the paper, but more a subjective opinion about the paper's claim) The paper claims answer-containing sentence generation is close to question generation. However, unlike question generation where core information about the answer is given as input text, answer containing sentence generation should generate the information about the answer itself with no given information. Consider the first example in Table 5. Question generation requires to read \u201cThe racial makeup of Fresno was 245,306 (49.6%)\u201d and generate question \u201cWhat percentage of the Fresno population is White?\u201d However, in the proposed pretraining technique, the generation model is supposed to generate \u201cThe racial makeup of Fresno was 245,306 (49.6%)\u201d with no information. In fact, I think the fact that answer candidate is given makes it closer to question generation task, rather than \u201cgenerating answer-containing sentence\u201d is the key.\n\n(By the way, I am giving 3 as a rating although my actual rating is closer to 4 or 5, because 4 or 5 is blocked in the review system. I am happy to increase the score after rebuttals.)\n\n--------------------------------------------------------------------------------------------------------------------------------------------\n\nNow, here are clarification questions.\n\nRegarding Section 2.1\n1) Did you attempt to predict \u2018number of answer candidates\u2019 by regression or classification? Which objective function did you use? Based on the current description, it looks like it\u2019s neither regression nor classification which makes me confused.\n2) I believe the second last formula in this section should say \u201c#{...}=k\u201d instead of \u201c#{...}\u201d?\n3) Have you compared with thresholding instead of predicting the number of answer candidates?\n4) What is the objective of training the span selection model? There are multiple candidate spans given a single sentence, and hopefully you do not want to discourage candidates except only one.\n5) How was this model trained (both predicting the number and predicting the span)? Was it trained on QA datasets such as SQuAD? Then, did you train this model & go through preprocessing separately for each dataset? (Except QUASAR-I which uses synthetic data from SQuAD v1.)\n6) I believe that the most popular approach in question generation literature is selecting named entities & noun phrases using off-the-shelf tools, and wonder if authors have compared their method with this baseline.\n\nRegarding Section 2.2\n1) Although this section is the most important section in the paper, the description is a bit confusing to me. My understanding is that, if the initial paragraph has three sentences, <Sentence 1>, \u201cThe racial makeup of Fresno was 245,306 (49.6%).\u201d, and <Sentence 3>, and the answer prediction model (described in Section 2.1) chooses \u201c49.6%\u201d, the question generation model is given \u201c<Sentence 1> 49.6% <Sentence 3>\u201d, and the model is supposed to generate  \u201cThe racial makeup of Fresno was 245,306 (49.6%).\u201d. Is it correct?\n2) Did you insert any special token to indicate whether it is the sentence or the target answer candidate, or which sentence is before or after the target sentence?\n3) Have you tried the formulation of the masked language model, instead of the proposed model?\n\nRegarding Section 3\nIs preprocessing the same for all Golub et al 2017, NS and AS?\n\nRegarding Section 4\n1) I am curious why the improvement in BLEU-4 score (in Table 2) is significant but the improvement in the end task (in Table 3) is marginal. Do you have any intuition on it?\n2) For Figure 5(a), do you have results with BERT without any data augmentation as well? I wonder if using too small number of SQuAD annotations makes question generation inaccurate, which makes synthetic data be bad quality and hurts the performance compared to not using any synthetic data.\n3) How many synthetic QA pairs are used for Table 3? Curious since the model gets around 92.5 with the largest amount of data shown in Figure 5(b) but the number reported in Table 3 is 92.8.\n"}