{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a modification to momentum based optimization algorithms motivated from theory of geodesic optimization algorithms on manifolds. The modification suggested by the paper is to use the normalized inner product between the previous update and the present gradient in the momentum multiplier in standard momentum algorithms like Polyak's Heavy Ball or Nesterov's momentum. They propose two DIFFERENT algorithms for strongly convex vs mini-batch non-convex optimization. In essence for STRONGLY CONVEX, the momentum multiplier is INCREASED proportionately when the present negative gradient aligns with the previous update. On the other hand for NON-CONVEX mini batch optimization the update is the exact opposite, the momentum multiplier is INCREASED proportionately when the present negative gradient DOES NOT align with the previous update (essentially treating the newly seen update as an outlier). Experiments are performed with strongly convex objectives and Deep auto encoders. \n\nI propose a rating of Weak Reject for the paper. The primary reason for this rating is that while the idea looks interesting and potentially helpful, the treatment of the idea presented in the paper is unfortunately quite PRELIMINARY and leaves much to be desired. The following points detail what I find missing from the paper - \n\n1. More discussion about the intuition wrt Geodesic Optimization - Even though the paper presents itself as a close relative of the Geodesic Optimization, the connection between the presented algorithm and the geodesic optimization seems a little superficial. It would have been great to see a derivation for the algorithm as a special case of the Geodesic optimization on R^d or something to that effect. In absence of such scientific connection - the present motivation seems merely visual. \n\n2. Theoretically grounding at least the convex case - I would have loved to see some analysis of the convex case of the algorithm. Of course one cannot beat the Nesterov acceleration rate in general. In that light, some analysis/discussion as to in which kind of functions one might expect to see a benefit of such a term even on quadratic functions would help to build intuition. At the very least establishing a convergence rate of the same order as Nesterov momentum would be extremely helpful. The purpose of such analysis is no to merely provide a proof but rather to build intuition as well as understand that there are no failure modes for the algorithm. \n\n3. Proposing completely opposite updates for strongly convex and non-convex belie the lack of intuition even more. The authors seem to point to the fact that momentum seems to play different roles for mini batch potentially non-convex optimization and full strongly convex optimization. This is a good understanding to have, one that is still in the process of being better understood in theory. Nevertheless, proposing opposite updates with little discussion seems very preliminary. Maybe a good question to ask here is suppose I wish to do mini-bath convex optimization, which of the two opposite versions would the authors recommend in that scenario?  \n\n4. The experiments also are preliminary. They are complete in the full batch strongly convex case, but even in convex cases mini-batch versions seem to be missing and these are important considerations. For non-convex deep learning based experiments - the authors claim that the autoencoders considered in the paper are benchmarks for deep learning optimization but that is unfortunately an outdated viewpoint. While they are definitely the first important cases to verify, to seriously verify the efficacy of the method (especially in the light of no theoretical treatment) it would be good to experiments on larger vision/language models. \n\nOverall while I believe that the idea is promising, the treatment of the idea seems preliminary and hence my rating. The authors can certainly improve the paper by adding some analysis/discussion of the algorithm even in simple cases or performing large scale modern experiments to establish practical efficacy. \n\nAlso I would like to note that the idea of including the inner product of the previous gradient and the present gradient in training dynamics is not completely new even in deep learning. Consider this paper for instance - https://arxiv.org/pdf/1703.04782.pdf-  which i think should be cited/compared to. "}