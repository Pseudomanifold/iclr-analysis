{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors proposed a new optimization algorithm called Stochastic Geodesic Optimization based on a geodesic optimization algorithm. It can also be viewed as an adaptive momentum methods. It may be some kinds of useful tools, but I feel the intuition behind the algorithms a little strange.\n\nSpecifically, I cannot relate the geodesic optimization with the newly proposed algorithms. Please see the detailed comments. Also, the authors do not provide some theoretical analysis of their algorithms, which let me doubt the generality of their algorithms. I know the analysis of adaptive momentum methods can be very hard, even with strongly convex and smoothness assumption. But the current formulation cannot convince me well.\n\nDetailed Comments:\n1. The notation is somewhat confusing. It maybe better to use g_t as the gradient and d_t as the previous update throughout the paper.\n2. What is the adaptive coefficient in Eq.(2) mentioned before Eq.(10)? If it is the inner product of v_t and gradient, the authors should mention is in Sec 2.1.\n3. Still, how to derive the algorithm in Sec 3.1? The quadratic approximate geodesic equation is theta_{t+1} = theta_t + d_t delta_t + 0.5 g_t delta_t^2 - <g_t, d_t>d_t delta_t^2, and the update of the proposed Geodesic Optimization is theta_{t+1} = theta_t + alpha (1-<\\bar{g}_t, \\bar{d}_t>) d_t - \\delta_t g_t. There are so many differences between this two formulas. For example, the approximate geodesic have the quadratic term delta_t^2, while the proposed update do not have. Also, there is no need for the approximate geodesic equation to normalize the d_t and g_t. The authors additionally introduce a tunable alpha, which makes me confusing. If it directly follows from geodesic approximation, why should the authors introduce this alpha? If there is only little connection with geodesic approximation, why should the authors mention it?\n4. As far as I can say, there are some differences between momentum and geodesic approximation. For example, in momentum methods, the coefficient of momentum can be set close to 1, but for geodesic approximation, [1] says that the step size (similar to the coefficient of momentum) should scale with inverse of the norm of gradient. I am not sure how the authors deal with this issue and connect these two methods.\n5. I think the transformation between (11) and (13) is so intuitive without any further theoretical support. And using the normalized gradient and previous update also violates the claim of geodesic optimization. I can hardly say what does the proposed SGeO really do. It may be useful, but I feel the intuition of it really weird.\n6. There are also several other second-order methods as well as optimization on Riemannian manifold, e.g. [2][3]. Although they may use higher-order information, I think the authors should still have some discussion over that, if the authors want to claim they are geodesic optimization.\n7. The experiment results are strong. However, I prefer much more tasks with much more dataset and much more network structures, at least classification with CIFAR10 and CIFAR100 is doable.\n\nTo sum up, I feel the proposed methods have little correlation with geodesic optimization. It is more likely to be some adaptive momentum methods, but with no theoretical analysis. The experiment results are somewhat strong, but I wonder how it can generalize to different tasks, different network structures and different dataset. Both of the theoretical analysis and more elaborated experiment results are necessary for this paper to meet the standard of publication.\n\n[1] Ricky Fok, Aijun An, and Xiaogong Wang. Geodesic and contour optimization using conformal mapping. Journal of Global Optimization, 69(1):23\u201344, 2017.\n[2] Zhang, H. & Sra, S.. (2016). First-order Methods for Geodesically Convex Optimization. 29th Annual Conference on Learning Theory, in PMLR 49:1617-1638\n[3] Song, Yang, Jiaming Song, and Stefano Ermon. \"Accelerating Natural Gradient with Higher-Order Invariance.\" International Conference on Machine Learning. 2018."}