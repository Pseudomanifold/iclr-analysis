{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== SUMMARY ===\n\nThe paper...\n- suggests an estimation method of mutual information that is less susceptible to numerical problems when many neurons are involved\n- suggests an estimation method tailored to convolutional neural networks. \n- analyses whether H(X) \u2248 I(T;X) resp. H(Y) \u2248 I(T;Y) is true in high dimensions for their suggested estimator.\n- analyses how common regularization techniques affect the learning dynamics. They show that early stopping prevents the learning process from entering the compression phase\n\n\n=== RELATED WORK ===\n\nPrior work is clearly presented and well-treated. The introduction and related work give a good overview of prior work on information plane theory, touching on the important contributions from Tishby & Zaslavsky [2015], Schwartz Ziv & Tishby [2017], Saxe et. al [2018], and Noshad et. al [2019], among others. In Section 3, Renyi\u2019s \\alpha-order entropy is outlined, and the multi-variate extension proposed by Yu et al. [2019] is described.\n\n\n=== APPROACH === \nMultiple papers are put into perspective, and the findings of these papers serve as a foundation for the suggested method\n- Renyi\u2019s Entropy is presented as a generalization of different entropy measures (among them Shannon Entropy). Computing Renyi\u2019s Entropy however necessitates to get a high-dimensional density estimation over the data distribution\n- Giraldo et. al. suggested a kernel-based method to estimate Renyi\u2019s Entropy solely on data without estimating the (intractable) density. An estimation for the mutual information can also be found in said paper\n- Yu et al. suggested a generalization of b) that can handle C variables instead of only two. As this computation includes taking the product over C values that lie within (0,1/N) the result tends to 0.\n\nAn RBF-based kernel is defined for a fixed conv. layer by computing the Frobenius norm w.r.t two 3D-output tensors of the conv. layer X_i, X_j. A derived matrix A is defined similarly to Giraldo et al.\n\nThis way, C individual kernels are defined, which are combined according to Yu et al.\n\n\n=== EXPERIMENTS ===\n\nComparison to Previous Approaches\n- Very detailed and clear structure of experiment. \n\nIncreasing DNN size\n- The experiment shows that there is both a fitting and a compression phase\n- H(X) \u2248 I(T;X) resp. H(Y) \u2248 I(T;Y) was disproven\n\nEffect of Early Stopping\n- Early Stopping prevents the learning process from entering the compression phase. The authors conjecture that the compression phase is related to overfitting -- it would be interesting to see some evidence to support this.\n\nData Processing Inequality\n- Enough evidence is provided to sustain that the data processing equality also holds for the suggested estimator\n\n\n=== OTHER COMMENTS ===\n\nThe paper would benefit from providing more rationale on why the suggested estimation method is numerically stable (as opposed to Yu et al.), maybe by supplying another proof.\n\nTechnical mistakes\nSection 4.1. Introduces X_i as a 3D tensor of dimensions C x H x W. Eq (7) mentions the Frobenius norm which is defined over matrices. We assume that X_i should be rather defined as a 2D tensor of dimensions H x W for a fixed layer index after reading Section 4.2.\n"}