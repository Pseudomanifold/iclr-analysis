{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This works considers the task of Natural Language Inference (NLI).\nThe question addressed is that SOTA NLI models tend to lead to\nhigher confidence when some parts are deleted from the \"premise\".\nIt is a problem known as under-sensitivity.\nA method based on IBP is proposed to address this issue.\nThe idea of Interval Bound Propagation (IBP) is to use interval arithmetic to propagate\nintervals and bound the variation of the target based\non variation of the input. In other words, one propagates\nupper and lower interval bounds through the network.\nThe DAM model from (Parikh et al., 2016)\nis studied in particular.\n\nThe paper is well written and easy to follow.\n\nMy only concern is about the relevance of approach based on DAM when\nthere are now more accurate models for this task. The paper is however\ninteresting and addressed a relevant topic.\n\nMisc:\n- transpose should be written with $^\\top$ (not $^T$).\n\n"}