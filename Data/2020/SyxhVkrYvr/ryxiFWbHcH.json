{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "-- Overall --\nThis submission tackles to verify the \u201cunder-sensitivity\u201d problem of neural network models in the natural language inference by ensuring modes do not become more confident in the predictions when arbitrary subsets of words from the input text are deleted. The authors developed new verification approaches based on decomposable attention mechanism with interval bound propagation (IBP), which can prove the under-sensitivity issue given a model and a particular sample. The experimental results on SNLI and MNLI show that the proposed approach leads to a much improved verified accuracy.\n\n-- In general, \u201cunder-sensitivity\u201d is a very critical problem for applying neural models in natural language understanding where powerful neural networks tend to capture spurious correlations from the biased datasets. This submission formulates \u201cunder-sensitivity\u201d as a mathematical specification and then try to verify it with IBP verification. Although the used technique IBP is not new, it would interesting to have the verification in NLI models.\n\n-- Section 5 is a bit unclear how to compute the IBP for deleting several words, and what is the output. It would be better to have a clear example for how this was computed.\n\n-- As the author mentioned, the verification of under-sensitivity can also be done by using beam-search, although it is costly and not accurate. IBP is another more efficient option, but not the optimal neigher. Maybe consider to change the title as \u201cefficient verification\u201d?\n\n-- Specific Questions -- \nThe entire paper builds on decomposable attention. Is the same approach also applicable to other model types, or only single layer attention-based models? \nAlso, how this methods work for other NLI or NLU tasks?\nIn experiments, how the data augumentation penalize the model with a loss for specification violation? What does the equation look like?\nCan you explain a bit more for IBP-training? How that hinge loss applies to the objective function? Is the IBP training differentiable?\n"}