{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper works out estimators for the gradient of Mutual Information (MI). The focus is on its recent popular use for representation learning. The insight the authors provide is to see encoding the representation as a \u2018reparametrization\u2019 of the data. This insight enables mathematical tools from the literature on \u2018pathwise derivatives\u2019. With gradients on the MI, one can estimate models that aim to maximize this quantity. For example in unsupervised learning one can learn representations for downstream tasks. This is shown in Table 1. Another application in supervised learning is the Information Bottleneck. This shown in Table 2.\n\nThree points for review:\n1)\nThe estimator for the gradient is shown at first on a toy task. Take a correlated Gaussian distribution and estimate gradients of the MI. The correlated Gaussian has an analytical form of MI, which makes this a useful experiment. The paper claims that this estimator \u2018provides a tighter and smoother\u2019 gradient estimate. I don\u2019t see how this experiment and this claim tie together. Could the tightness or smoothness be quantified? It seems the MIGE has a lower variance, could empirical results or bounds on the variance be obtained? \n\nMoreover, this plot concerns random quantities, whereas we see only one realization. Both the MIGE and the MINE hold under expectation of samples from the data distribution. This is a toy example where we can sample infinitely from the data distribution. That means we can either a) plot more samples or b) obtain (empirical) error bounds on the gradient under these sampling distributions.\n\n2)\nThe major experimental result in the paper shows advantage of the gradient estimator in Transfer learning. Specifically, the authors compare against the recent DIM of Hjelm et al 2019. The authors train a quote \u2018small fully connected neural network classifier\u2019. However, the work of Hjelm trains a linear SVM on the representations. It is not clear where the increase in performance originates. Is it the improved representations (as obtained by using MIGE) or is it the change classifier? \n\n3)\nOne contribution of the paper is to make the gradient estimator work in high dimensions. To this end, the authors propose Random Projections. It is not clear how this approximation influences the results. An experiment regarding this topic would make the point clearer. Is RP used in the current experiments? Then how does this influence the results? Is the RP used for computational purposes? Then can we quantify the gain in computation?\n\nMinor comments: \n  *\u2019In practice, we do not care about MI estimation\u2019. Please explain further or refer to previous work.\n  *\u2019In optimization, it should be achieved by maximizing the information between z and z.\u2019 (Section 2). Two points\n    1. The information \u2018between z and z\u2019 is probably a typo?\n    2. How does sufficiency relate to an optimization problem? Doesn\u2019t sufficiency mean in this context I(X;Y)=I(Z;Y)?\n  * Equation 12: In the part $\\nabla_psi (x, E_\\psi(x))$. Why do we take gradient w.r.t. x? It seems to me that the reparametrization is a function of x only via $E_\\psi(x)$. If not, then please explain what this tuple means.\n  * Table 1 has no units. How to interpret the numbers in this table?\n  * Section 4.3, authors note their experiment is \u2018a little bit different\u2019 from other related research. How and what exactly is different?\n\nTypographic comments\n  *Just below eqn17, \u2018minibatche\u2019 => \u2018mini-batch\u2019 or \u2018mini batch\u2019\n  *Section 4.2 \u2018images classification\u2019 => \u2018image classification\u2019\n  *\u2019However A tractable density is\u2019 => \u2018However, a tractable density is\u2019\n  *\u2019Estimating gradients of MI than\u2019 -> \u2018Estimating gradients of MI rather than\u2019\n  *Section 3, circumstance 1 \u2018representation\u2019 => \u2018represent\u2019\n"}