{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes the Mutual Information Gradient Estimator (MIGE) for estimating the gradient of the mutual information (MI) instead of calculating it directly in learning representation. They are using Stein's estimator following by a random projection to build a tractable approximation to the gradient of the MI. \n The MIGE is evaluated on several of unsupervised and supervised tasks, and shown improvement over prior MI estimation approaches in maximize the MI and learning features for classification.\n\nIn general, I think that the idea of estimating the gradient of the MI instead of directly calculating it  is an exciting research direction, and this paper combines a few pieces together (As mentioned in the paper, there was a work of Li & Turner, 2017 that applied Stein's estimator for implicit models).\nHowever, the experimental part of this paper is lacking. My main concern is regarding the performance on downstream tasks. Although the experiments demonstrate wins over different models in maximizing MI for CIFAR10 and CIFAR100, the only comparison for downstream tasks is for Permutation-invariant MNIST. One more concern is regarding the random projection. It is not clear what is the effect of it on the representation, and how it impacts on the gradine's estimation.  \n\nStrengths:\n+ Interesting new model for representation learning based on an estimation of the MI gradients'.\n+ Good set experiments looking at MI maximization performance.\n+A well-written and well-organized paper.\n\nWeaknesses:\n No comparison on downstream tasks for more datasets except MNIST. In the end, a key question is a final accuracy on different datasets and how to maximize the information effect on it. \nThere is no discussion about the effect of the random projection on the representation. For example, how it affects performance? How much the algorithm sensitive to this projection? What is the performance of the MINE if it combined with random projection...\n\nMinor comments:\n\n-Typos and English mistakes - there are many typos. For example -\n    In the introduction - \"Another closely related work is the the Information\u2026\"\n    In section 2 - \u201cIn order to overcome this disadvantages\"  \n    In section  2.20  - \"In optimization, it should be achieved by maximizing the information between z and z.\"\n- There should be more detailed explanations of the experiments. For example - what is the projected dimension (for all the experiments). \n"}