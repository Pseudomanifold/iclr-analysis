{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes MIGE---a novel estimator of the mutual information (MI) gradient, based on estimating the score function of an implicit distribution. To this end, the authors employ the spectral Stein gradient estimator (SSGE) and propose its scalable version based on random projections of the original input. The theoretical advantages of the method are presented using a toy experiment with correlated Gaussian random variables, where both the mutual information and its gradient can be computed analytically. In this setting, MIGE provides gradient estimates that are less biased and smoother than baselines. The method is also evaluated on two more complicated tasks: unsupervised representation learning on Cifar-10 and CIfar-100 via DeepInfoMax (DIM) and classification on MNIST with Information Bottleneck (IB), where MIGE outperforms all baselines by a significant margin.\n\nI recommend ACCEPTing this paper. This work discusses a vital problem and proposes a novel, well-motivated, principled and very performant solution; additionally, it demonstrates the broad applicability of the introduced method. While the proposed technique consists of previously known building blocks (spectral Stein gradient estimator and random projections), it is cleverly applied in a novel context of estimating MI gradients.\n\nWhile the paper is solid, I believe that it could be improved in the following ways. Firstly, I would like to see 1) more extensive and 2) larger-scale evaluations. In the DIM experiment, 1) would correspond to trying the DIM(L) approach, which maximises patch-wise MI. In fact, I strongly recommend including this experiment as it corresponds to and could improve current state-of-the-art. If it turns out that MIGE does not work well on DIM(L), then this would correspond to a serious issue with the method. In this experiment, 1) would also include providing other metrics for learned representations. It would be much more convincing to include estimates of true mutual information (e.g. InfoNCE bound evaluated with a large number of samples [1]) and showing that MIGE can attain higher values than baselines. 2) would correspond to evaluation on bigger datasets: (tiny) ImageNet and STL-10 dataset. Also, the toy experiment would benefit from a higher-dimensional setting (e.g. d=256 to d=1024), since these are often used in practice. \nSecondly, the paper is sloppily-written, which quite a few grammar and stylistic mistakes (e.g. sentence in sec 3, paragraph 2: \u201cwe assume obtain to\u2026\u201d, which starts with a lower-case letter and doesn\u2019t make sense). Finally, the paper would benefit from the following clarifications: 1) explain what is the Nystr\\:om method, 2) provide either a proof or a citation for eq (19); also the error bound for SSGE should be provided for the paper to be self-contained, 3) explain the difference between q_\\psi and p_\\psi, which seem to be used interchangeably.\n\nAdditional remarks:\nSec 2.2, 2), \u201cstreamlining\u201d is unclear\nCircumstances 2 and 3 can be quite easily derived from circumstance 1; also they are not evaluated empirically; it would be nice to have experiments for them, and they can be moved to the appendix in case of lack of space\nEq (19) while nice, seem to bear no significance for the proposed method and the rest of the paper; consider removing it\nSection 4.2 paragraph 2: \u201cshrinking\u201d for different layers wasn\u2019t mentioned before, and is not immediately clear what it means; the reader needs to be intimately familiar with the DIM paper to understand.\nSection 4.3 mentions \u201cthreshold\u201d for stein gradient estimator, which was not mentioned before. Please explain what it is.\nEquations (8-10) are just simple derivations and are not necessary; it would be enough to provide Eq (10).\nThe authors talk about MINE, which optimizes the InfoNCE bound [1], which is also used in DIM and CPC [2]. I strongly encourage the authors to cite [1] and [2] and mention them in the related works. Additionally, it would be clear if Figure 1 and related references and description used \u201cInfoNCE\u201d instead of \u201cMINE\u201d as the name of the method since InfoNCE is an estimator and MINE is just a particular implementation of the method.\n \n[1] Poole et. al., \u201cOn variational bounds of mutual information\u201d, ICML 2019.\n[2] van den Oord et. al, \u201cRepresentation Learning with Contrastive Predictive Coding\u201c, arXiv 2018."}