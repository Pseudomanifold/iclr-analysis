{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper argues that directly estimating the intractable mutual information (MI) for representation learning is challenging in high dimensions. Instead the authors propose to estimate the needed MI gradients directly using a score function based approach. Using some identities of MI the authors arrive at an expression for the gradient of the mutual information between input and latent representation (eq 10) and proposes to use a generalization of the reparameterization trick and spectral stein gradient descent to approximate this gradient. In Toy experiment and MNIST/CIFAR10 experiments the authors demonstrate that their method produces latent representations that are more informative than competing MI methods for downstream classification tasks. I found the approach and content of the paper interesting and the results seems encouraging.  My main concern is that I did not find the that I did not find the method and experimental section to be fully comprehensive and further lacking many details which makes it hard to compare the results with prior work. \n\n\nPros:\n1) I find the approach taken by the authors interesting and different from current MI estimation approaches. The paper convincingly motivates their approach by describing the deficiencies of current MI estimators and why targeting the gradients directly might have merits.\n2) The authors propose to use SSGD and 'generalized' reparameterization in a (well motivated) new setting.\n3) The cifar10 experiments in table 1 are encouraging and the toy experiment in 2D is illustrates nicely the deficiencies of the current MI estimators \n\nCons\n1) The experimental section is lacking many details to fully understand how and what experiments were performed and how comparable they are to prior work\n 2) The paper would benefit greatly from a thorough editing to clarify the presentation - there are many missing concepts and definitions that makes it hard to follow without intimate knowledge of related literature. \n \n \nFurther suggestions / questions \n\n1. In section 3. Please define q(z)_psi(z),  q(x,z)_psi and describe how they relate to  E_psi.\n\n2) What exactly are the contributions by the authors wrt to spectral stein gradient descent (sec 2.1) e.g. is it the scalable approach based on random projections described in sec 3 ? Further i would like some discussion on the quality of this approximation?\n\n3) Please provide some more details on the DeepInfoMax and Information bottleneck experiments e.g. How exactly did you estimate the MI gradients in these settings? how is the downstream task setup and is it identical to prior work?\n\n\n4) About writing style:\nI think it would benefit the paper if you let the reader decide for them self what adjectives should be used to describe a result. A few concrete suggestions:\n - Use remarkable/y about your own findings a bit more sparingly (used 4x). \n - Consider deleting \u201cmuch\u201d and \u201cvast\u201d in a sentence like: \u201cour approach MIGE gives much more favorable gradient direction, and demonstrates more power in controlling information flows without vast loss\u201d."}