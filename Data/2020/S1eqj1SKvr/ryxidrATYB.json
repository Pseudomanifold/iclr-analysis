{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Authors have introduced a new type of adversarial attacks that perturb abstract features of the image. They have shown that pixel space adversarial attack detection and defense techniques are ineffective in guarding against feature space attacks.\n\nI have some concerns about the novelty of the attack and the appropriateness of defenses that have been tested.\n\n- Since the attack is done in the feature space, the defense should also be done in the feature space. For example, adversarial training or smoothing can be done in the feature space. See: https://arxiv.org/abs/1802.03471\n\n- There are attacks that perturb colors or other interpretable features of the image that have not been mentioned in the paper. For example, see https://arxiv.org/abs/1804.00499 and https://arxiv.org/pdf/1906.00001\n\n- If the decoder has a high Lipschitz constant, a small perturbation in the feature scape 'can' lead to a large and visible perturbation in the pixel space. It was not clear to me how this is being controlled in the current method. \n "}