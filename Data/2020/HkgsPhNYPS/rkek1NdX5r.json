{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe paper proposed a self-ensemble label filtering (SELF) method to deal with the noisy label learning problem. They progressively filter out the wrong labels during training, i.e.,  filtered samples are removed entirely from the supervised\ntraining loss, and are leveraged via semi-supervised learning in the unsupervised loss. The filtering is based on identification of inconsistent predictions throughout training. \n\nStrengths:\n1. The motivation of the paper is very clear. \n2. Experiments are conducted on various dataset CIFAR10, CIFAR-100 and ImageNet. \n\nWeakness:\n1. The contribution of SELF is not clear. Just a combining of several previously proposed components?   \n2. For the experimental comparisons, the authors at least should report the acc on clean test set, which is useful for understanding the ideal case performance. \n3. The organization of the tables and figures are somehow hard to read. \n3. The comparisons are not fair. SELF incorporate semi-supervised techniques while baselines are not. \n4. The author missed some important baselines here. \n     1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019 \n     2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018 \n     3) Dimensionality-driven learning with noisy labels, ICML2018"}