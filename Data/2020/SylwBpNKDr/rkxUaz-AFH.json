{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary:\nThis paper focuses on topic of searching for the optimal architecture for the deep network. Building on the split linearized bregman iteration strategy, the authors propose two practical algorithms to boost network, namely GT-filters Alg and GT-layers Alg. The proposed algorithms can simultaneously grow and train a network by progressively adding both convolutional filters and layers. The experiments conducted on VGG and ResNets display the comparable accuracies between the BoN and the standard big models, but with much more compact representations and balanced computational cost.\n\nStrengths:\n1 The authors introduce two simple but practical algorithms for augmenting the architectures of deep network. The quite promising results are achieved on baselines, w.r.t. the balance of prediction performance and model complexity with the budgeted computational resources.\n2 The paper is clearly written and easy to follow. \n\nWeaknesses:\n1 The proposed BoN is built upon the existing SplitLBI algorithm that can identify the sparse approximation of the weight structure. The contributions are mainly from incorporating the adaptive criteria for adding filters and layers. Thus contribution is incremental and novelty is limited. \n2 The projection operator in equation (4) is a key step of the boosting procedure, but is not clearly defined and explained.\n3 For experiments, results shows good results on simple baselines, more complicated or large-scale datasets should be included for evaluations. Also, only Autogrow is compared to the proposed method, to make the results more convincing, other architecture searching approaches (e.g. NAS) should be added to comparisons.\n\nOther questions:\n1 For the GT-layers Alg, would it more efficient to boost layer first before boosting the filters? \n2 Will GT-layers be robust to the case when the number of filters is overly specified?"}