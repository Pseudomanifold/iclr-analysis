{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies a very interesting topic: automatically grow filters and layers in neural networks and find an \"optimal\" width and depth for neural networks. The method is motivated by SPLITLBI, and its effectiveness is verified by experiments and comparison with AutoGrow. I tend to accept this paper, before the following questions can be answered:\n1. I guess the major issue in this paper is that the method is not clearly explained and rigorously formulated, although it's an extension of SPLITLBI.\n-- 1.1. what's \u0393? Is it a copy of W or not? What's the exact mathematical function of loss L() w.r.t. W and \u0393? How does the neural architecture change after adding \u0393?\n-- 1.2. why \u0393 can be approximated by gradients in Eq. (3)? What's the intuition behind?\n-- 1.3. why \u0393 is necessary? How does it compare with enforcing group Lasso on W directly, like what was done in Nonparametric Neural Networks [1]?\nWithout clarifying those, people can hardly learn from and use this paper.\n\n2. Experiments\n-- 2.1. Include the learned width in Table 1.\n-- 2.2. In comparison with AutoGrow, the pairs of ResNet is fair, but the pairs of PlainNet is hard to judge because different neural architectures are used. AutoGrow uses 4 blocks while this paper uses 5 blocks. It's unclear if the benefit comes from the method or just from an additional block.\n-- 2.3. In the experiments of layer growing, please clarify if filter growth is also applied or not.\n-- 2.4 clarify \"their growing process is not efficient.\" If I read the AutoGrow paper correctly, efficiency is one of the their claims and they showed that the growing process is as fast as \"training a single DNN\", and they scaled to ImageNet, which is not covered in this paper.\n\nMinors:\n1. networks with \"20 filters\" and \"100 neurons\" are used as the seeds. How critical are they?\n2. \"To the best of our knowledge, this is the first algorithm for BoN that can simultaneously learn the network structures and parameters from training data.\" is over-claimed. Lots of pruning methods can do it, although they start from a large one and prune it down.\n\n[1] Philipp, George, and Jaime G. Carbonell. \"Nonparametric neural networks.\" arXiv preprint arXiv:1712.05440 (2017)."}