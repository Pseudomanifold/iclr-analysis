{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to solve the inverse reinforcement learning (IRL) problem via supervised learning. This is done by formulating the IRL problem as one of variational inference using a Conditional-VAE. The proposed approach seeks to learn an encoder that takes (s,a,s') tuples and encodes them into a reward signal. The decoder then seeks to recover s'. \n\nThis is a very interesting idea, but it is not ready for publication. The paper is lacking experiments involving RL. When evaluating IRL algorithms you need to run RL on the learned reward to really verify if it works. Just getting a reward that is \"close\" to the true reward doesn't mean that the learned policy will be anything like the optimal policy. \n\nThe results in Figure 2 seem to show that the method is not working as well as the paper claims. As mentioned above, the real test of reward learning is what policy results from RL. Similarly, in Figure 3 the mean looks good, but the variance appears very high and it is unclear if the network has really learned the right reward function without comparing policy performances on the ground truth reward. \n\nThis paper also overlooks two recent papers that also convert IRL into a supervised learning problem and do not require knowledge of MDP dynamics or data collection for reward inference:\nWang, et al. \"Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation.\" ICML, 2019.\nBrown, et al. \"Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations.\" ICML, 2019.\n\nIn general, I would recommend spending less time on deriving the preliminaries and use the extra space for experiments involving policy optimization on the learned reward function. Given successful experiments this would become a nice solid paper."}