{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a feature partitioning scheme for efficient multi-task neural architecture search.  The proposed scheme automatically determines the network capacity that should be shared across tasks and kept exclusive for each task.\n\nOverall, this is a well written paper. \n\nThe problem of neural architecture search is important and beneficial to deep learning community to be able to extract the best out of these methods. Multi-task learning is no exception. However, I am not sure why the problem of NAS is so different for multiple outs than a single output. Given multiple outputs, we need to either look at a weighted combination where weights could be provided by user to reflect the priority over the tasks, or individual outputs in a multi-objective approach such as Pareto optimality manner. I find the approach of this paper a heuristic.\n\nUsually, due to negative transfer learning, too much sharing of the weights is detrimental for the performance of an individual task. I could not see this anywhere in the results. Especially, I missed any baselines where such problems were present, which would then be improved due to using this method. How would the results be if all tasks are given equal weight and NAS is performed with respect to their average output (of course normalised appropriately)?\n"}