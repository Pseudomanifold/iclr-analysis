{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a framework for learning multi-task convolutional neural networks. For each layer of the network, the proposed algorithm assigns a subset of the layer's channels to each of the tasks. This is in contrast to existing methods that assign whole layers to tasks. There are two key ideas here: (1) instead of searching in the space of binary assignments of layers to tasks, search in the continuous space of fractions of channels assigned to each layer, subject to some consistency constraints; this allows for using finite differences for gradient estimation which can be fed into a black-box optimization procedure; (2) the use of distillation to estimate the performance of a given assignment, rather than retraining many models. Experimentally, the proposed framework performs relatively well on the Visual Decathlon benchmark.\n\nOverall, I do like the paper's key ideas. However, I am not convinced by the presentation of the paper and by the lack of multi-task learning baselines in the experiments. I will consider changing my score if the following comments are addressed.\n\n- Testing your main premise: \"we instead partition out individual feature channels within a layer. This offers a greater degree of control over both the computation required by each task and the sharing that takes place between tasks.\" \nActually, based on the paper, it is not clear that assigning channels rather than layers brings about additional gains. To prove that hypothesis, you should carry out experiments comparing your method to existing multi-task learning methods that you surveyed in the paper.\n\n- \"share all\" performance: this baseline seems to do quite well in Table 2, beating es in 5/10 w.r.t. the average accuracy. Why does this happen?\n\n- Too many hacks hidden in the appendix and it is not clear what works why. Have these values been found by cross-validation? The reader should be able to understand how your method works *exactly*. Examples of these magical values:\n-- Number of samples for the ES\n-- Number of parameter directions\n-- \"Distillation training is done for a brief 3000 iterations with a batch size of 4 and a learning rate of 1 which is dropped by a factor of 10 at iteration 2000.\"\n\nClarification questions:\n- I don't understand what \"strategy\" means here: \"Fixed vs Learned Strategies: Is a uniform strategy applied across tasks or is a task-specific solution learned?\"\n\n- Section 4.2 needs to be rewritten to give a more structured exposition of the distillation process. Please add pseudocode if needed. The Figure is not very informative.\n\n- MIP: Generally, this is a very important component of your method, yet it is lacking in detail and left to the appendix. Some questions:\n\n-- how long does it take to solve? Which commercial solver do you use? Please provide more details.\n-- constraint (8) is for which k? Is it for all k? Please fix. Also for (10), it should be for all i and j as well as k.\n-- what is the \"non-linear constraint\" that (8-9) are linearizing? Please provide the original MIP formulation (without slack or linearization), and explain the final formulation accordingly.\n-- MIP: are you adding slack variables to allow for slightly infeasible solutions in case no M can be derived from \\tilde{P}?\n\nMinor:\n- \"cut down average feature dramatically\": unclear, rephrase as needed.\n- Figure 6 is low-resolution."}