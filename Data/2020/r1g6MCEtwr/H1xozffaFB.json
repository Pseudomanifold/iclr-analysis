{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper uses Gram matrices for OOD detection. This enables the reliable detection of far-from-distribution examples, which is a long-standing and surprisingly difficult problem in OOD detection.\nThis is the best paper in my batch due to the strength of the results. However, this paper should more accurately reflect the contribution: this helps with far-from-distribution examples, not near-distribution yet OOD examples.\nFor instance, I used their code and found that their technique leads to an AUROC of 79.01% when using CIFAR-10 as the in-distribution and CIFAR-100 as the OOD set. Likewise, with their code I found having CIFAR-100 as in and CIFAR-10 as out gives an AUROC of 67.95%. This is much worse than currently existing techniques. Hence the paper should re-frame or qualify their results as helping with far-from-distribution detection or the detection of obvious anomalies. This paper makes a solid stride in improving the detection of garbage inputs, but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks.\n\nSmall comments:\n\n> Lee et al. (2018b)\u2014to the best of our knowledge, the current SOTA technique by a significant margin\nThis should be again qualified and expanded. If you assume access to knowledge of the test distribution, then Mahalanobis is best. If not, the Outlier Exposure is best. If you assume access to no extra data during training, then the maximum softmax probability + rotation prediction is best.\n\n> However, while the OE method is able to generalize across different non-training distributions, it does not achieve the SOTA rates of Lee et al. (2018b) on most cases.\nThis is like saying it's not SOTA because it does not cheat. There are different senses of state-of-the-art and these should be qualified.\n\nDoes this technique do better if you do 5th and 9th percentile instead of min and max? Is it important to do the min and max with training examples instead of validation examples? (Not a pressing question.)\n\n> Can work without access to OOD validation examples?\nTable 1 is deceptive. OE does not need the \"validation\" examples.\nI suggest two columns instead of one. \"Can this work without knowledge of OOD test examples? Does this use OOD training examples?\"\n\nShow AUROC and AUPR. Detection accuracy is an unusual metric relative to AUPR (OOD as positive).\n\nShow CIFAR-10 vs CIFAR-100 in the tables or I'll downgrade my rating, since otherwise the paper is not leaving an accurate impression.\n\nSince OE is complementary, perhaps this technique can be combined to tackle these near-distribution cases?\n\nThe title is confusing. \"Zero-Shot\" could be applied to various techniques in this space. Perhaps emphasize Gram matrices?\n\nIn their code:\n        validation_indices = random.sample(range(len(all_test_deviations)),int(0.1*len(all_test_deviations)))\n        test_indices = sorted(list(set(range(len(all_test_deviations)))-set(validation_indices)))\nThese indices change with every power, which is unrealistic. Please fix the sets beforehand.\n\nSince neural style transfer, which uses Gram matrices, works much better with VGG architectures than ResNets, does this technique work better with VGG architectures?"}