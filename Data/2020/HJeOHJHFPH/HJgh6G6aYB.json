{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors introduce a super-resolution method especially tailored for facial images. The main contribution of this work lies in: (a) utilizing a 3D face reconstruction branch which is used to extract 3D face priors, and (b) using the Spatial Attention Mechanism to adaptively integrate the 3D face prior in the network. \n\nI am inclined to reject this paper as I deem that:\n\n- there is a lack in novelty: (a) Spatial Attention Mechanism is based on already existing works in the literature, i.e., Spatial Feature Transform (Wang et al, 2018) and Attention Mechanism (Hu et al., 2017). In particular, for the super-resolution task, Residual Channel Attention Blocks are employed in the recent work of Zhang et al., 2018b (RCAN), (b) the introduction of the priors from the 3D face rendering process is incremental. I understand that this addition makes the difference from the compared methods as this method is tailored for faces and incorporating relevant information such as identity will definitely help. However, the idea per se is not novel enough.\n\n- the baseline is weak. This paper is focused on face super-resolution (also known as face hallucination).  The authors should have compared with SOTA methods in face hallucination such as Super-Identity Convolutional Neural Network for Face Hallucination (Zhang et al., ECCV 2018), which is especially tailored for recovering the face identity during the generation process. \n\nAdditional comments:\n\nFaces are a significant area of Machine learning research, but can your method generalize in other domains and if so, how? \nIn Fig. 2, how many times do you apply the Residual Channel Attention Blocks? Have you seen any increase in the performance depending on the number of times you apply them?\nPlease cite RCAN (Zhang et al., 2018b) in Section 4.2, where you write about the Residual Channel Attention Blocks.\nBe consistent in the notation. For example, in Section 3, you use bold letters for the matrices/vectors in Eq (1) but you do not do so in Eq (3) (e.g., image I is a matrix, etc.).  Also, put the vectors in bold in Eq (4), etc."}