{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper builds upon the CROWN framework (Zhang et al 2018) to provide robustness verification for transformers. The  CROWN framework is based upon the idea of propagating linear bounds and has been applied to architectures like MLP, CNNs and RNNs. However, in Transformers, the presence of cross-nonlinearities and cross-position dependencies makes the backward propagation of bounds in CROWN computationally intensive. A major contribution of this paper is to use forward propagation of bounds in self attention layers along with the usual back-propagation of bounds in all other layers. The proposed method provides overall reduction in computational complexity by a factor of O(n). Although the fully forward propagation leads to loose bounds, the mixed approach (forward-backward) presented in this work provides bounds which are as tight as fully backward method. \n\n\nStrength:\nUse of forward propagation to reduce computational complexity is non-trivial\nStrong results on two text classification datasets:\nLower bounds obtained are significantly tighter than IBP\nThe proposed method is an order of magnitude faster than fully backward propagation, while still maintaining the bounds tight.\n\nWeakness:\nExperiments only cover the task of text classification. Experiments on other tasks utilizing transformers would have made the results stronger.\nThe paper makes the simplifying assumption that only a single position of an input sentence will be perturbed.  They claim that generalizing to multiple positions is easy in their setup but that is not supported.   The paper needs to declare this assumption early on in the paper (abstract and intro). As far as I could tell, even during the experiments they perturb a single word at a time.\n\nThe paper is more technical than insightful.  I am not at all convinced from a practitioners viewpoint that such bounds are useful.  However, given the hotness of this topic, someone or the other got to work out the details.   If the math is correct, then this paper can be it.  \n\nThe presentation requires improvement.  Some parts, example, the Discussion section cannot be understood.\n\n\nQuestions:\nThe set-up in the paper assumes only one position in the input sequence is perturbed for simplicity. Does the analysis remain the same when multiple positions are perturbed? \n\nSuggestions:\nA diagram to describe the forward and backward process would significantly improve the understanding of the reader.\n\nIn Table 3, I am surprised that none of the sentiment bearing words were selected as the top-word by any of the methods.  Among the \u2018best\u2019 words,  the words chosen by their method does not seem better than those selected by the grad method.\nSeveral typos in the paper: spelling mistakes in \u201cobtain a safty guarantee\u201d, poor sentence construction in \u201cand independent on embedding distributions\u201d, subject verb disagreement in \u201cUpper bounds are discrete and rely on the distribution of words in the embedding space and thus it cannot well verify\u201d.\n\nI have not verified the math to see if they indeed compute a lower bound.  \n"}