{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose Dual-GD-DDAN as an extension of DDAN for the task of domain adaptation in software vulnerability detection (SVD), a problem where domain adaptation is important given the scarcity of labels. The authors report an experimental comparison across multiple datasets against several baselines and demonstrate clear performance improvements in terms of the F1-score. The paper is well-written and provides sufficient context to the readers.\n\nComments:\n\n- Result tables report the average predictive performance over 5 experiment runs, but they do not include a measure of variance of the results (+-std or an estimate of confidence intervals). This is relevant both for understanding the size of the effect coming from the proposed improvements, as well as the stability of the method - whether the improvements are consistent or occasional.\n\n- Similar to the above, despite the reader being reasonably convinced of the performance improvements in terms of average precision, the authors should consider using a formal statistical test if possible - if it is computationally prohibitive to run many repetitions for each domain adaptation direction, perhaps this can be derived jointly from the entire set of runs across all adaptation tasks?\n\n- The paper might benefit from the authors providing some examples of engineered features for capturing vulnerabilities. Even though the focus of the paper is on developing an automated approach, this particular task is not one that most readers will be familiar with. Providing additional domain-related information would be beneficial for the readers to understand what the model is ultimately supposed to be able to achieve - so far, this is only given in the appendix in the \"Motivating example\" section.\n\n- Can vulnerabilities be categorized? Are there multiple types of vulnerabilities? If so, what is the model performance (similarly, for the baselines) across different types of vulnerabilities rather than jointly - are there some that are much harder to get right via domain adaptation than others? Either way, something to touch on in the discussion at least briefly\n\n- The proposed approach seems to always achieve the highest F1-score, but not necessarily the highest precision or the highest recall. At the moment, the paper does not discuss the metrics in sufficient detail. Are precision and recall to be traded off equally? Should false negatives not be penalized significantly more than false positives? If so, would it not be beneficial to focus on a different measure? This is especially important considering Table 3, where SCDAN achieves a higher recall in 3 out of 4 experiments, though at the price of lower precision and a somewhat lower F1-score.\n\n- In line with the above, assuming that the classifier outputs a soft label (probability), would it be possible to select a different operating point, based on either the training performance or one particular domain adaptation (to inform the operating point for the others, presuming there is some generalization). In particular, if one was to aim for close to 100% recall, what would be the resulting precision? Would it be possible to derive a precision-recall curve and compute PR AUC in such a setting?\n"}