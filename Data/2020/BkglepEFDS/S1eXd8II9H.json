{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a method to perform domain adaptation in the space of\nsoftware vulnerability detection using generative adversarial networks. A\ndual discriminator GAN projects labeled and unlabeled data to the same\nrepresentation space in order to assign it labels and address the domain\nadaptation problem.\n\nThere are two broad classes of concerns I have with this paper.\n\nFirst, the paper does not clearly describe why domain adaptation and GANs\nare especially important for this domain. What is different about this\ndomain that requires developing a completely new approach to domain\nadaptation instead of borrowing from prior work? Some comparisons to\nbaseline domain adaptation in the evaluation (see the second concern)\nwould help justify this decision.\n\nIt would help to justify why domain adaptation is even important to\nbegin with. What would happen if the classifier were trained on the\nprogram text of N-1 different programs and then evaluated on the Nth,\ndisregarding the fact that it's a different program?\n\nAfter showing that domain adaptation is a problem, it would then help\nto show why GANs are the right way to solve this. It is not obvious\nthat GANs---which often come with a significant increase in complexity\nand have problems of their own---are the right way to improve on the\ndomain adaption.\n\nSecond, the evaluation is not thorough. As the authors state, obtaining\na good dataset for evaluating vulnerability detection is exceptionally\ndifficult. Using the dataset from Li et al. is about as good as could\nbe asked for.\n\nHowever, given the limits of the dataset, it is important to be very\ncareful in performing the evaluation. The dataset consists of roughly\n20,000 labeled examples, of which fewer than 400 are in the positive\nclass. When splitting out only 20% as the test set, this gives under\n100 positive examples in the test set.\n\nAs a result, the statistical power of Table 1 is questionable. It would\nhelp to include margins of error over the five runs the authors performed\n(a good thing!) in order to show what results are meaningful and what\nresults are not statistically significant.\n\nFurther, such a small dataset has implications for overfitting: how\nmany parameters are in the model? Does the model completely overfit\nthe dataset within a few epochs?\n\nThe proposed approach has *many* technical details. This calls for an\nextensive evaluation to demonstrate which components of the proposed\napproach contribute most strongly to the accuracy of the classifier.\nHowever, this is not performed. Only two tables evaluate the proposed\nclassifier after developing it in the prior four pages.\n\nThus, many of the core assertions, such as the fact that covering more\nmodes makes the classifier more accurate, can not be experimentally\nvalidated to be true.\n\nLess important for a ML conference than the prior two concerns, I have\nsome serious reservations regarding the utility of such a classifier\n(or even ability of a classifier to work at all). Almost all C code is possibly\nvulnerable in isolation. It is only by knowing the bounds of arrays,\npreconditions on pointers, and other underlying assumptions that\nit is possible to say code is not vulnerable to some attack. (Worryingly,\nthis is the exact information that is *removed* in the preprocessing of\nthe data.) What is the intended use case for this line of work?\n\n\nMinor comments:\n- I have never heard of SVD used for software vulnerability detection---\n  it's always meant singular value decomposition to me. Looking online this\n  doesn't seem to be common practice.\n\n- \"We implement eight mentioned methods in Python using Tensorflow which\n  is an open-source software library for Machine Intelligence developed\n  by the Google Brain Team\" -> better to cite Abadi et al.\n\n\n(As this paper is ten pages long, and CFP asks for reviewers to \"apply a\nhigher standard to papers in excess of 8 pages\", I have done so.)\n"}