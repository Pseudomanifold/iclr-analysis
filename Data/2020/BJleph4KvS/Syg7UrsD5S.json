{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper investigates the problem of graph classification using neural networks and suggests a hierarchical approach for constructing a feature vector describing a whole graph via the use of the compressed Haar transform. The general method utilizes a hierarchical chain of coarsened versions of the graph (group multiple nodes into a parent 'node')  where the coarsening is achieved via spectral clustering. After obtaining the graph chain, at each level the authors apply a GCN followed by a HaarPooling, namely applying a lossy Haar Transform compression to get a representation for each cluster. \n\nOverall graph classification and regression tasks are quite important and this work provides a new way of transitioning from node based learning methods to full graph representations via hierarchical compression. Nonetheless, I find that the organization of the paper needs additional work and the experimental investigation is not sufficient and compelling enough. So I do not think the current paper is ready for publication in a top-tier venue like ICLR yet.\n\nIn particular I find the discussion of the HaarPooling step's computational performance not particularly appealing since other parts of the hierarchical approach are computationally expensive for example, the spectral clustering or GCN steps are costly. It also seems that the discussion of the paper is more on the computation and viability of the compressive Haar transform than it is about using it as a compressor as part of a larger hierarchical system. \n"}