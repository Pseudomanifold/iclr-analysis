{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces an approach to network compression by encouraging the weight matrix in each layer to have low rank. Instead of relying on SVD during training, so as to minimize the nuclear norm of these matrices, the authors propose to explicitly factorize the weight matrices into an SVD-like factorization U*diag(s)*V, and treat the resulting matrices as new parameters. Then, penalizing the rank can be achieved via a sparsity-inducing regularizer on the elements of s and by encouraging U and V to be orthonormal.\n\nThe idea is simple, yet very natural, to the point that I am surprised it hasn't been done before. However, I could not find any other paper doing so.\n\nRelated work:\n- Note that additional methods than those cited in the paper have proposed to decompose the weights post-training, e.g., Denton et al., NIPS 2014; Lebedev et al., ICLR 2015. It would be worth discussing these methods.\n- More importantly, while the authors discuss the work of Xu et al., 2018, which aims to minimize the nuclear norm during training, they fail to discuss that of Alvarez & Salzmann, NIPS 2017, which also proposed to do so. In that work, nuclear norm minimization was achieved via proximal gradient descent. Since the proximal operator for the nuclear norm regularizer has a closed-form solution, this prevents the instabilities mentioned in this submission when discussing the work of Xu et al., 2018.\n\nExperiments:\n- While the experiments show the good behavior of the proposed method, I feel that a comparison with Alvarez & Salzmann, NIPS 2017, would give a more complete picture, due to the aforementioned close relationship.\n- It is also disappointing that the results of TRP (Xu et al., 2018), which constitutes the other closest baseline, are only reported on CIFAR-10, not on ImageNet (based on Table 5 in the appendix).\n- As a matter of fact, the results on ImageNet are less impressive than those on CIFAR-10. As such, I believe that a complete comparison with all the baselines is important.\n- While the results in Table 1 show the impact of the orthogonality regularizer, the reasons why this is the case are not clear to me and are not explained by the authors.\n\nSummary:\nI have mixed feelings about this paper. While I like the general idea, I feel that some experiments are currently lacking to fully support the authors' claims.\n"}