{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to parametrize each layer of a deep neural network, before training, with a low-rank matrix decomposition, namely SVD. The convolutions are accordingly replaced by two consecutive convolutions (channel-wise or spatial-wise depending to the shape of the matrix the weights are reshaped to). The decomposed method is then trained with an additional l2 orthogonality regularization on the factors of the decomposition, as well as an l1 (Hoyer) regularization on the eigenvalues of the SVD to control the rank of each decomposition.                                                                  \n\nThe idea is simple and appealing but not novel. Decomposing neural networks is already widely studied, both before training and post-hoc, with various regularizations. In addition, the proposed methodology is suboptimal due to the (arbitrary) reshaping of the convolutional kernels to matrices. By contrast, tensor based methods can apply decomposition directly to the tensor (e.g. Higher-order SVD), resulting in much less parameters.\n\n* The reviewer claim in 2 that batch-normalization breaks the theoretical guarantee that the decomposed layers can approximate the operation of the original layer and will largely hurt the inference efficiency: i) most methods actually train the decomposition end-to-end. ii) even for post-hoc decompositions, fine-tuning is typically used to (successfully) restore performance.\n* The experiment on the effect of the l2 regularization is interesting. The same one should be carried for the sparsity inducing regularization.\n* Figure 2 and 3 are hard to read.\n* There should be a table of comparison indicating number of parameters and performance of the proposed methods, compared with previous work.\n* The authors do not mention tensor decomposition based approaches, e.g. \"Speeding-up convolutional neural networks using fine-tuned cp-decomposition\", ICLR 2015, \"T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor\", CVPR 2019. These methods (which validate the rank as an additional parameters) should be compared with the proposed approach, in terms of compression and performance (these methods obtain better performance for small compression rates for instance).\n"}