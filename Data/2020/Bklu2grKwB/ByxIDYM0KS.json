{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThe premisse of the work is very interesting: RNNs that are permutation-invariant. Unfortunately, the paper seems rushed and needs a better justification for not having a RNN memory that is associative. It also should cast the contributions in light of other existing work (not cited). The paper says \"In this section and the remainder of the paper, we focus on the latter [commutative RNN memory operator], namely introducing a constraint (or equivalently, regularizer) that is commutative\", but it never talks about the impact of a RNN memory using a non-associative operator. Being commutative is easy, isn't Equation (2.4) commutative if \\Theta = W? Being associative is hard, since non-linear activations are not easily amenable to associativity.\n\nSection 4: \"The above example demonstrates that RNNs can in some cases be a natural computational model for permutation invariant functions.\" => Janossy pooling (Murphy et al., 2019) gives an alternative way to use RNNs, with a way to make their method tractable. Actually, my guess to why the RNNs experiments work well, even without an associative memory, is because the training examples come in multiple permuted forms, which is the data-augmentation version of the pi-SGD optimization described in Janossy pooling. \n\nOn page 1, \"consider the problem of computing the permutation invariant function f(x_1, . . . , x_n) = max_i x_i\", what follows is not a proof of necessity. It is an informal argument that either should be made formal or should be described as informal.\n\nThere is a lot of missing related work for sets:\nMurphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. \"Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs.\" ICLR 2019.\nWagstaff, Edward, Fabian B. Fuchs, Martin Engelcke, Ingmar Posner, and Michael Osborne. \"On the limitations of representing functions on sets.\" ICML 2019.\nLee, Juho, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. \"Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks.\" ICML 2019.\n\nAlso missing related work for graphs:\nBloem-Reddy, Benjamin, and Yee Whye Teh. \"Probabilistic symmetry and invariant neural networks.\" arXiv:1901.06082 (2019).\nMurphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. \"Relational Pooling for Graph Representations.\" ICML 2019.\n\nThe paper has an interesting question but needs to build on prior work. As of now, I am unconvinced that not having an associative operator for the RNN memory will lead to a good nearly permutation invariance function (unless there is data augmentation, per Janossy pooling).\n"}