{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper starts with presenting an RNN formulation and essentially writing out the sequence of RNN applications. Not surprisingly, if these applications were associative and commutative the RNN would be permutation invariant.  Then a condition for commutativity is formulated in terms of an expectation of a difference.  Based on a prior result, it is shown that the expectation can be computed in closed form.  Although it is not shown if an RNN regularized that way is permutation invariant, since the associativity is not demonstrated, empirically it is shown that it may be already of use.\n\n   Contributions:\n   1. A regularizer for RNNs that enforces commutativity\n   2. A closed form for computing it\n   3. A fully learnable permutation invariant \"deep\" network, per an empirical demonstration\n\n   The main contribution is the empirical demonstration of the learnable nature of the obtained function unlike the prior art (e.g.  DeepSets) where a choice of the aggregation function severily affects the results.\n\n    The theoretical component of the paper is unclear:\n      1. Section 3 is rather trivial.\n      2. Theorem 3.6 is hard to connect to an RNN and the rest of the paper. Unclear why bother learning the RNN at all if it needs to converge to addition of the input and hidden state to be universal anyway.\n      3. In essence, the result of the paper is a way to encourage commutativity in an RNN and a demonstration that it works in practice for encouraging permutation invariance. The other explanations make things confusing and do not seem to contribute to the rest of the paper.\n\nCould it be that the network indeed learns addition operator?  RNNs usually are only able to operate on very small sequences because of the vanishing gradient problem, yet the proposed approach will not directly work on the more robust LSTM.\n\nSignificance or lack of the difference between the proposed method and DeepSets is unclear as the plots are missing the error bars.\n\n The table and the accuracies reported in Section 8 are impossible to interpret. It is unclear whether the authors done cross validation.  If so, it would be helpful to see standard deviations of the reported numbers\n"}