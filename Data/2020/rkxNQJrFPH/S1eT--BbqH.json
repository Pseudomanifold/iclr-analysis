{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new approach to zero-shot learning using natural language descriptions of class. In particular, the paper introduces a method, N^3, that regress model parameters from textual descriptions of a seen class. The regressed parameters are combined with parameters of a  pre-trained classifier and fine-tuned on seen classes. The text description is first embedded using BERT method and then used as an input to a novel module, called parameter adaptation module, PEM. The proposed method shows good classification improvement on zero-shot class of four major datasets. \n\n+ves:\n+ The idea of providing a newer approach to zero-shot classification using natural language descriptions is interesting. Such approaches have been used in generative models (GANs, for e.g.), but it is novel in the zero-shot setting.\n+ The introduction of the PEM module seems interesting.\n+ The paper is well-written, and the results show good promise.\n\nConcerns:\n- The primary concern is that the paper does not evaluate the proposed method against state-of-the-art zero-shot classification methods (e.g. CADA-VAE: Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders, CVPR 2019), but only against other methods that use natural language descriptions (or adaptations of related methods). This seems limiting in the usefulness of this work (and the key reason for my decision on this paper). It would have been nice to at least see results of what happens if the proposed method is induced into an existing SOTA method, and if its performance improves.\n\n- It is not clear why we need an additional parameter adaptation module (PAM) module. It seems that a combined version of Semantic Encoding Module (SEM) and the PAM module is possible to regress parameters.\n\n- Table 3 shows different values of \\Gamma and \\Mu. From that study, the dominance of the pre-trained network, i.e. \\Gamma = 0.99, is evident. When we ignore the pre-trained network, i.e. \\Gamma = 0.01, the accuracy is bad. So, are the PAM model parameters actually contributing much? What if we use some other regularization on weight space of the pre-trained network and ignore the PAM module? (by doing so, we can mitigate the extra overhead of training PAM network and then fine-tuning the pre-trained network). Please mention the end values of \\Mu.\n\n- Can we replace the pre-trained model parameters with the generated PAM model parameters? What if we do not use the pre-trained model parameters and use only the regressed/generated parameters from PAM to classify and make an end-to-end network? (In this case, the PAM generated parameter space must match the pre-trained network parameters space). In this way, we can understand the exact contribution of the PAM module.\n\n- Tables 3 and 4:  The Accuracy @ 1 on which dataset? Table 4: using BERT, the Accuracy @ 1 is 17.6%; however, in Table 3 we can see the Accuracy @ 1 is 18.5% (almost 1% improvement). Are we using the same BERT model, \\Gamma = 0.99 and initial \\Mu = 0.01 for these tables?\n\n- Please clarify in the paper if the term \u201cTask description\u201d  actually implies \u201cClass description\u201d, and not referring to the \u201cTask\u201d definition given in work such as Taskonomy, Zamir et al., CVPR 2018. It would be nice to associate task as a more general concept than just a class. "}