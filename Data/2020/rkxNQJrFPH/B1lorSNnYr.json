{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary: \nThis paper proposes a meta-model called Neural Networks from Natural language (N^3) for zero-shot learning. This model maps a set of class object descriptions to neural network parameters to create zero-shot classifiers, which contains two modules: (1) SEM encodes class descriptions into corresponding embeddings; (2) PAM fine-tunes the generated parameters in the SEM. The authors conduct experiments on 4 popular benchmark datasets and achieve best results compared with baselines.\n\nStrengths:\n(1) This paper proposes a good Parameter Adaptation Module (PAM) to generate parameter adaptations to fine-tune all layers in the pretrained network based on the class embeddings, to classify unseen classes.\n(2) This paper achieves good results compared with baselines.\nWeaknesses:\n(1) I am not sure the effectiveness of the N^3 according to the results in Table 3, 4, because the BERT module may play a major role.\n(2) Although PDCNN (2015) and GAZSL (2017) are very strong baselines, this paper does not compare the work published in 2018 or 2019.\n(3) There are many grammatical mistakes in this paper.\n\nI have some following major concerns about the paper:\n(1) This paper conducts extensive experiments to demonstrate the performance of N^3 in Table 2, but why not compare N^3 with the work published in 2018 or 2019? \n(2) In Table 3, this paper studies what extent does the superior performance of N^3 depend on \\gamma and \\mu, but why not show the results with gamma =1, mu=0 to demonstrate the effectiveness of PAM or fine-tuning?\n(3) In Table 4, I can come to a conclusion that BERT plays a great role in your model. To better demonstrate the effectiveness of N^3, I think you should show the accuracy of the baselines and N^3 with ELMo and Glove embeddings separately in Table 2.\n(4) This paper does not explain the results clearly such as Figure 5 (a) and (b).\n(5) There are many grammatical mistakes in this paper, such as the sentence \u201cwe also perform ...... and demonstrated ......\u201d in Section 1.\n"}