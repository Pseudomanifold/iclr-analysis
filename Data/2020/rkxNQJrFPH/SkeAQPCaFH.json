{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper approaches zero-shot learning (ZSL) by directly generating neural network weights from natural language descriptions. The main motivation is to resolve the limitation of existing methods which rely on meta-data to get feature embeddings for text descriptions.  To this end, it employs a semantic encoding module to obtain text feature embeddings directly from descriptions by incorporating BERT and Transformer. From the text feature embeddings, it generates delta weights for every deep neural network layers, which is different from existing approaches that generate weights only for the last classification layer.  The delta weights are added to the pretrained weights in the base model (i.e., ResNet101) to adapt it for zero-shot classification. Experimental evaluations on four text-based zero-shot learning datasets demonstrates its superior to previous SOTA methods.\n\nPros\n\nThe paper is clearly written overall with helpful schematic illustrations\n\nGenerating the weights for the whole classification neural network rather than only the last classification layer is interesting.\n\nThe semantic encoding module is technically sound. Leveraging the powerful BERT model for class embedding is  better than TFIDF feature as used in previous work.\n\n\nCons\n\n1) For the parameter adaptation module, the approach to finetuning the base model guided by class embedding seems interesting but the motivation is lacking. The delta weights are produced and added to the weights of the base model. A more straightforward approach is to directly update the weights of the base model based on class embedding as some previous works did in Ref. [1, 2].  The motivations of choosing this way of updating weights should be clearly provided and the performance comparisons of these two approaches are expected.\n\n\n2) The performance comparison with MEGAZSL is unfair. In the proposed model, the feature extracted network (i.e. ResNet101) is finetuned with zero-shot classification objective while MEGAZSL uses the pretrained networked without finetuning. I believe that  finetuning plasy a very significant role since it pushes the model to distinguish the particular classes in each dataset, specially for the fine-grained datasets, such as CUB, NAB, and Flowers, in which subtle differences among classes can hardly be captured without finetuning. For a more fair comparison,  the base model should be first fine-tuned with a conventional classification objective on the training set, and the features are extracted by the fine-tuned model for zero-shot learning.\n\n3) Missing training details: It is unclear which are the learnable variables of the proposed model, and how they are learned -- which parameters need load weights from pretrained models and which do not; the information about the learning rate, optimizer, training epochs etc. are also missing.\n\n4) The compared methods seem to be dated. More comparisons with recent ZSL methods will strengthen the experiments.\n\n\n[1]  Morgado et al. Semantically consistent regularization for zero-shot recognition. In CVPR, 2017\n\n[2] Li et al. Discriminative learning of latent features for zero-shot recognition. CVPR 2018\n"}