{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper examines the utility of a sinusoidal representation of timestamps for predicting patient outcomes from EHR data. It uses a representation inspired by the Transformer's Positional Embedding, in this case applied to time and called the Time Embedding. The same idea was recently used in a pre-print called \"TAPER: Time-Aware Patient EHR Representation\u201d on the same task (and even the same dataset). It\u2019s possible that this paper is a followup by those authors or maybe it\u2019s convergent thinking in the field of ML4Health. \n\nIn any case, the idea is relatively simple and intuitively could be useful for certain kinds of temporal events. This paper does a cursory evaluation of its utility on the MIMIC-III EHR dataset and shows that the addition of time embeddings is typically slightly worse than using binary masking of missing time steps. It\u2019s totally fine to arrive at a negative result! However, we should learn more from this evaluation than simply \u201cwe tried an idea and it lost to an alternative\u201d. To make this paper successful the authors should evaluate on multiple datasets, evaluate different granularities of time (hours vs. days vs. months), and imitate Lipton 2016 in examining baselines such as forward filling imputation of missing data. As it stands the paper doesn\u2019t add enough to our understanding of when a sinusoidal embedding of time would or wouldn\u2019t help. \n\nUnclear phrasing and/or grammatical nits:\n* \"This makes it harder to learn time dependencies found in time series problems.\u201d \u2014 do you mean e.g. time of day of an event or the degree of delay between two events, or some other kind of \u201ctime dependencies\u201d? \n* \"The tests were made with two tasks\u201d \u2014 maybe better to say that you evaluate on two tasks?\n* \"The problem in focus of this work is how can a machine learning method learn representations from irregularly sampled data.\u201d Repeats introduction, or \u201cThe problem in focus of this work\u201d isn\u2019t quite right grammatically. \n* \"even with the sparsity of binary masks.\u201d \u2014 is this talking about the low frequency of observed events? \n* \"Despite the improvement an issue about these methods is missing the potential of how the observation time can be informative\u201d \u2014 the improvement of what over what? Should this sentence be part of the previous paragraph? \n* \"also proposed a method to improve discretization\u201d \u2014 discretization of what?\n* \"Another approach is to make complex models capable of dealing with irregularities\u201d \u2014 does this mean models that are somehow more complicated or those that somehow represent sparse time with complex numbers?\n* \"To make the dataset even more irregular we removed randomly part of observed test data\u201d \u2014 missing \u201cthe\u201d? \n* \"In the length of stay task TEs achieved better results, especially with bigger gaps at the reduced data test\u201d \u2014 this should come after some description of figures 3 & 4, since thus far the reduced data results weren\u2019t described.\n\nTypos:\n* \u201casses\u201d -> \u201cassess\u201d \n* \"were binary masking had a better performance.\u201d -> \u201cwhere binary masking\u2026\"\n* \"Specially to very irregular time series and high dimensional data\u201d -> \u201cEspecially\"\n* \"were TEs can be applied by addition without increasing the input dimensionality\u201d -> \u201cwhere\"\n\n\n"}