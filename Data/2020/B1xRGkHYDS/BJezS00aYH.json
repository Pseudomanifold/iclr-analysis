{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper was an interesting read. The idea of this paper is to challenge the use of Laplacian matrix in GCN. Indeed, typical GCNs use the same adjacency matrix across different layers. In particular, this typically leads in Euclidean case to learning isotropic filters (because the euclidean Laplacian is isotropic). Consequently, such filters have no selectivity at all.(in the Euclidean case, that could correspond to the selectivity to orientations - no selectivity would lead to a difference of Gaussians) Furthermore, for non-sparse graphs, computing the iterations of the Laplacian matrix can require a significant computational power.\n\nIn order to tackle this problem, the authors introduced a diffusion factor to sample a set of nodes to build some GCN filters with finite support. At a given layer, the diffusion factors is based on the interaction with other layers of the GCN. Then, a layer-wise attention mechanism that will allow to weight the graph connectivity of the sampled nodes is used, which is supervisedly learned. Each numerical experiments lead to a significantly better accuracy, while the method trains in reasonable time. This is thus numerically convincing. Furthermore, this method is, to my knowledge, new.\n\nThe paper is clearly written, the numerical experiments are convincing and the authors address a difficult problem with a simple method: I'm leaning toward an \"Accept\".\n\nMinor: \n- Tables 2/3 are hard to read.\n- The paper is 10 pages long, yet this was an interesting read."}