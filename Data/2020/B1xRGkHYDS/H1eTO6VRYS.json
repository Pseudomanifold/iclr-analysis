{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a sampling method for graph neural networks which is applicable to very large graphs (where not all nodes can be kept in memory at the same time). The method uses the transition probabilities of a random walk to construct a sampling probability of the nodes in the lower layer given the nodes in the upper layer. Since this samples nodes which can be one or multiple hops away, an attention mechanism is used to weight updates from connected nodes. Experiments show that this method is promising.\n\nIn its current state I would be inclined to reject this paper, but I could be convinced otherwise. The idea, although relatively straightforward, seems powerful and the experiments seem to support it. My main concern is that paper is not well written. It contains long meandering paragraphs (e.g., all of section 2 is a single paragraph) with high-level intuitions and ill-defined terms, making it hard to read. Similarly, results are badly presented. For example, table 3 should probably be given as relative speedups with the best results bold-faced rather than a long table with numbers. Illustrations would also be very helpful to provide an intuition about formulas 4, 5, and 6. Moreover, a simple ablation study is necessary (e.g., using bi-diffusion based sampling, but using constant weights instead of the attention mechanism, or vice-versa, using the attention mechanism with other types of sampling). It is currently impossible to disentangle the effects of the two parts of BLS-GAN."}