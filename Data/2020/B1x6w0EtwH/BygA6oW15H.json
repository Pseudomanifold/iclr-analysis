{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper tackles the problem of developing agents to solve interactive fiction (IF) games. The authors propose an agent that builds a dynamic knowledge graph of each state from the textual observation provided by the games, while choosing actions from a template-based action space. While both these directions have been explored in prior work as pointed out, this paper combines them effectively to produce an agent that outperfoms existing methods on a benchmark of different IF games. \n\nPros:\n1. Writing is clear, method is easy to understand and design choices are clearly specified. \n2. Empirical results are good and presented on real IF games.\n\nCons:\n1. (minor) While the authors test their method on a suite of human-made IF games, it would also be great to have a study on synthetic cases like the Microsoft TextWorld environments, if only to see which aspects of the method are crucial to making the jump from synthetic to real IF games. \n\n\nOther comments:\n1. It looks like the knowledge graph is constructed for every state separately. The DRRN on the other hand incorporates more of the state history. From Figure 3(b), both DRRN and KG-A2C seem to do well - have you analyzed whether these methods are complementary? In general, it would be nice to have some more analysis on all the models across the different games rather than just Zork1."}