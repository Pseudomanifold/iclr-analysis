{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose an unsupervised knowledge graph alignment framework, KAGAN, using adversarial training. Specifically, they utilize a triple discriminator to discriminate the aligned source triples and target triples, and a reward function to minimize the divergence between source triples and target triples. Also, they propose to leverage the lower bound of the mean KL divergence (mutual information) to resolve the mode collapse problem. The proposed method can be incorporated with a supervised method to be a weakly-supervised approach. Even though there are a family of unsupervised approaches for domain alignment, this paper is the first to solve the knowledge graph alignment problem in an unsupervised/weakly supervised way.\n\nStrength:\n1.\tThe paper addresses a critical knowledge graph alignment problem using GAN based on triplets, not usually entity alignment in literature, but also considers the relation alignment in knowledge graph.\n2.\tThe paper tries to solve the problem in an unsupervised way, and shows the on-par performance with weakly supervised methods in FB15k dataset.\n3.\tThe paper considers mode collapse problem and tries to solve the problem via mutual information rather than mean KL divergence, also gives the theoretical proof.\n4.\tDetailed experimental analysis and ablation studies show the effectiveness of the proposed method on small datasets.\n5.\tThe paper is well-written and easy to follow.\n\n\nI have two concerns as follows:\n1.\tThe authors conduct experiments on multiple small KG datasets such as FB15K and WK15K. But the reviewer finds that the baseline papers authors mentioned also have experiments on larger datasets like WK120K (Chen et al., 2017a), WK60k, DBP-WD or DBP YG (Sun et al., 2018a). It is essential to conduct the experiment on larger datasets to verify the effectiveness of the proposed method.\n2.\tIs it necessary to construct a reward function to update the alignment function using REINFORCE algorithm? For instance, current distribution matching methods can define the discrepancy between the two distributions (such as target distribution and aligned source distribution). It can directly optimize the loss in an end-to-end differentiable way instead of a reinforcement learning way. It can avoid the sampling and provide a more stable optimization process.\n"}