{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper touches the signal processing/long term propagation problem in gated recurrent neural networks from the mean field theory. The paper starts from a dynamic system view of the recurrent neural networks and calculates the time scale of converging to the fixed point. In order to avoid the system to converge to the fixed point, the authors utilize some initialization strategy to keep the time scale to infinity. The authors also relate the time scale to state-to-state Jacobians. \n\nThe paper is interesting but more details could be added to both theories and experiments. Given all those extra details, I can increase my scores.\n\nFor the theory:\n1. It is unclear how you go from equation (7) to (8). References or more explanations need to be added.\n2. It is unclear how the initializations in E.4 satisfies the criteria you define in the paper. More explanations can be added.\n3. This mean field approximation is still far away from practice. It is hard to believe the input in real life is Gaussian distributed vectors (sec 4.2). What will happen if the input distributions are not Gaussian? This should be discussed.\n4. This initialization only helps at the beginning of the training. What will happen if we do one backpropagation? This should be addressed.\n\nFor the experiments:\n1. The authors did not state the some of the experiment details in the papers, like what optimizer, regularization, learning rate.... To make better assessment of the experiments, those details should be added. Do you train/tune all the different initializations in the same way?\n2. I cannot find the description of Figure 1 anywhere in the paper. It is hard to believe LSTM did poorly on sequential MNIST unless giving more details since LSTM has been proved to perform okay on sequential MNIST in a bunch of papers[1]. \n3. What is the meaning of 112 dimension in 5.3? Does that mean you only choose the first 4 rows of MNIST images?\n4. Comparison over random seeds should be honestly justified for all the experiments.\n\nMinors:\n1. You have one missing \\Sigma_z in equation (4a).\n2. \\mu_s^2 in equation (4b) should be (\\mu_s^t)^2\n\nReferences:\n[1] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \"Unitary evolution recurrent neural networks.\" International Conference on Machine Learning. 2016.\n"}