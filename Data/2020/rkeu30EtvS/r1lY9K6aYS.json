{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an operation for removing the pixel-wise and channel-wise correlations of input features. This operation can be considered as a generalization form of the former proposed decorrelated batch normalization. The approach has a well-sounded neurological inspired motivation and a solid explanation of the relationship with deconvolution. In the experimental analysis, the authors demonstrate a good performance of the proposed method compared with batch normalization. Also, the authors provide the CPU time of the proposed method, which is very appreciated. \n\nOverall, this paper has reasonable contributions to the learning algorithm of the deep neural network, so I recommend the AC to accept this paper.\n\nHowever, I have 2 negative points on this paper.\nFirst, the computation cost for the im2col based convolution operation in a large kernel (eg. 7x7) is insanely large, and the authors only show us the results using VGG, which only has small 3x3 kernels.\nSecond, the arguments made on the sparse representations is somehow not convincing to me, it is really difficult to say the sparse representations has mad regularizations more effective with only 2 learning curves"}