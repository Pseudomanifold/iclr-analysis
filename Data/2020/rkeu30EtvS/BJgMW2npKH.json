{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes \"network deconvolution\", a neural network primitive aimed at whitening the activations of each layer of the network. The method is a generalization of batch normalization that not only whitens per channel, but also removes correlations between channels and across spatial locations. Experiments show that the proposed methods improves training speed and predictive accuracy on a number of image classification models.\n\nThe method is novel and the proposed implementation details constitute a significant technical contribution. The experiments are not exhaustive and leave some open questions, but the first results are highly promising. The paper is clearly written, and embeds the proposed method in the literature.\n\nI'd like to see more discussion and experiments on:\n- What's the dependence on batch size? Does the method work better for larger batches? Or are small batches better for the added regularization effect, like what occurs with batch norm?\n- Are results sensitive to the epsilon in algo 1? How is this chosen in practice and how does it interact with the approximate inversion of the covariance matrix?\n- Under what conditions do you observe a speed-up in convergence? The only training curves shown in the main paper are on Fashion-MNIST which I don't think is very interesting. The appendix does offer a bit more information but I feel this deserves more attention.\n- How does this method interact with regularization methods? For batch normalization much of the benefit seems to come from the noise it adds to network activations; is the same true for your method? Does network deconvolution still improve final accuracy if the baseline uses more extensive data augmentation or other regularization?"}