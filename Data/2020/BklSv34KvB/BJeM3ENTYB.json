{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty\u201d. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. \n\nI like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. The thought is similar as active learning. It is interesting to see how similar thought can be used to improve the performance of the algorithm in the general batch gradient descent setting.\n\nOn the other hand I am not quite convinced the proposed way is truly better. The main concern is the experiments do not quite show the state-of-the-art result at all. It is not even close on MNIST, CIFAR-10 and CIFAR-100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net?\n\nBesides this main concern I also have some worries about the design of the algorithm. I listed them below:\n1. The vanilla stochastic gradient descent can be roughly justified since the expectation of the stochastic gradient is the true gradient of the loss. Now with the proposed heuristic will this still be true?\n2. Is there any guarantee the algorithm can converge? It is not clear to me as the optimization proceeds the ``uncertainty\u201d may oscillate. Is there any condition when the convergence is guaranteed?\n3. As the number of classes grows the estimation of the entropy itself is a tough problem. Is there any way to mitigate this issue other than increase the window size?\n\nAnother minor comment:\nCould the authors add more explanation on equation (4)? For example, $\\delta$ is related to the maximum entropy led by a uniform distribution, and the summation term in (4) is related to the empirical entropy.\n \n"}