{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes an approach to learn word embedding functions more efficiently and with fewer parameters. This is done by replacing the embedding lookup function which is typical in NLP tasks such as language modeling and machine translation with a hierarchical embedding model. This allows for a low dimensional embedding layer, reducing total parameters and training time. A novel skip-connections architecture is introduced as a part of the \"embedding generation model\". Experiments are conducted for language modeling and machine translation tasks and performance improvements are observed with a reduction in parameters and lesser training time.\n\nThe direction of this work is nice, the problem that is being tackled is indeed important.\nThe obtained results are nice (though this can be improved) and there is indeed some potential value in this work.\nHowever, I have the following concern. The paper completely ignores a lot of previous and concurrent work in reducing the size of the embedding layer. These works are in most cases not even cited and no empirical comparisons are provided. For example, please see below works in matrix factorization approaches, sparse word representation learning, codebook learning and other quantization approaches for compressing word embeddings:\n\nhttps://www.aclweb.org/anthology/P16-1022/\nhttps://aaai.org/ojs/index.php/AAAI/article/download/4578/4456\nhttp://web.cs.ucla.edu/~chohsieh/papers/Mulcode_Compressor.pdf\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17042/16071\nhttps://storage.googleapis.com/pub-tools-public-publication-data/pdf/f158f7c81ed8e985fd51a20d193103ce427cad51.pdf\nhttps://arxiv.org/pdf/1711.01068.pdf\nhttps://arxiv.org/abs/1510.00149\n\nI would appreciate if comparisons with some of these approaches is provided in the next iteration of this work.\n\nOther suggestions:\n\n1. I think the paper would benefit from some analysis of the differences in the word embeddings learnt by a general lookup table learning model in comparison with the word embeddings learnt by this model. How are the embeddings compressed? How do the decompressed embeddings compare to the embeddings learnt by the lookup approach? More insights in the machinery via some visualizations would help.\n\n2. How do the gains of this method change as more or less training data is provided. For example, are the gains lesser on Gigaword? This would be interesting to know.\n\n3. GLT is mentioned twice in this paper. Perhaps a slightly more detailed explanation of the same would help improve the readability of this paper."}