{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper describes a new method for learning deep word-level representations efficiently. The architecture uses a hierarchical structure with skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods.\n\n1. From table 1a or table 2, the training time of the proposed method is not reduced compared with existing methods. \n\n2. It seems the number of parameters in DeFINE still depends directly on vocabulary size. Methods proposed in [1] and [2] do not depend directly on the vocabulary size.  For dataset that has very large vocabulary size, [1] and [2] could potentially have larger compression rate.\n\n3. The experiments are detailed, and includes ABLATION studies.  \n\n[1] Variani, Ehsan, Ananda Theertha Suresh, and Mitchel Weintraub. \"WEST: Word Encoded Sequence Transducers.\" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019.\n[2] Li, Z., Kulhanek, R., Wang, S., Zhao, Y., & Wu, S. (2018, April). Slim embedding layers for recurrent neural language models. In Thirty-Second AAAI Conference on Artificial Intelligence."}