{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a novel sparse network architecture to learn word embeddings more effectively. \n\nI am not an expert in the area of machine translation, so I am able to sanity-check the results and the reasoning and motivation given in the paper. \n\nGenerally I failed to find motivation as to why this specific architecture was chosen out of many others. I also do not understand the purpose of doing aggressive embedding expansion before another contraction. Why would this allows to learn a more efficient low dimensional embedding than the original one? This may happen to be the case, but why?\n\nOverall, the results seem to be a bit inconclusive.\nTable 1: b) DeFINE uses less parameters but also gives worse results. This does not allow me to conclude anything.\nTable 1: c) DeFINE seems to give better perplexity results, while using less parameters. This is good.\nTable 2: DeFINE uses more parameters and gives better perplexity results. I do not know what to conclude, as ideally I would like to see how would DeFINE do with the same number of parameters.\nTable 3: \"our implementation\" seems to provide much lower scores than the ones found in the literature and thus can not be used as an fair baseline. Once this baseline is discarded, DeFINE seems to be producing worse results while using less parameters. Is this good? I do no know. But certainly this is inconclusive. I do not see how this table allows to conclude the following: \"DeFINE improves the performance by 2% while simultaneously reducing the total number of parameters by 26%, suggesting that DeFINE is effective\".\n\nA few other comments:\n\nFigure 1: if m >> n, why is the bottom (green) of DeDINE network wider than the top (tellow)?\n"}