{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary:\nThis paper builds on a recently proposed algorithm (\"splitting steepest descent\", Wu et al 2019) for guiding the growth of a smaller network into a larger one in architecture search. The algorithm in Wu, et al. alternates between two steps, (i) optimization of parameters for a fixed model and (ii) modification of the architecture by identifying a subset of neurons to split into more neurons, based on the \"splitting index\" of each neuron (amounting to evaluating the smallest eigenvalue of a matrix). This work builds on that in two ways: (i) it incorporates an energy budget into the optimization procedure for choosing which subset of neurons to split, which it approximately solves by a continuous relaxation, and (ii) avoids doing exact eigendecomposition to extract the minimum eigenvalue (splitting index) but instead replaces it with a more efficient SGD on the Rayleigh quotient. \n\nEvaluation:\n--Evaluations are done on variants of MobileNet on CIFAR-100 and ImageNet (the latter would be infeasible without the approximation scheme). The proposed approach appears to get better tradeoff between accuracy and FLOPs in these cases. In practice the non-energy aware \"vanilla\" networks do tend towards models that are small in size (fewer parameters) but are not necessarily low in energy consumption.\n--There is new material here, although I find the novelty a bit limited (e.g. only an additional constraint compared to the original approach of Wu et al and addressing a clear scalability issue with the original work, i.e. eigendecomposition of a matrix, with what seem straightforward approximations, ). The empirical results in Table 1 and 2 seem solid, but I'm not familiar enough with past results in this area  to evaluate their significance. "}