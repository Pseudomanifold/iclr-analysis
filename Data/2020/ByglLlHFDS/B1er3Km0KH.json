{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents an algorithm to match two distributions with latent variables, named expected information maximization (EIM). Specifically, EIM is based on the I-Projection, which basically is equivalent to minimizing the reverse KL divergence (i.e. min KL[p_model || p_data]); to handle latent variables, an upper-bound is derived, which is the corresponding reverse KL divergence in the joint space. To minimize that joint reverse KL, a specific procedure is developed, leading to the presented EIM. EIM variants for different applications are discussed. Fancy robot-related experiments are used to evaluate the presented algorithm.\n\nOverall, the paper is in good shape wrt the logic and the writing. My main concerns focus on the novelty (compared to existing methods that are similar but not discussed) and the experiments. For the former, reverse KL has been exploited before, both in the marginal space [1] and the joint one [2]. Other detailed comments are listed below.\n\nAs Eq 4 is for matching two joint distributions, discussions/comparisons should be made to reveal the novelty of the presented EIM over existing methods such as [2], etc.\n\nIn Figure 2 (b), the experimental settings for adversarial learning are not fair, as the discriminator is not fixed there. \n \nIn Sec 4.4, it seems EIM is highly overlapped with VIPS. So what're the advantages of EIM here?\n\nIn Figure 3, how many steps for Generator and Discriminator are used for f-GAN? Does f-GAN finally converge? It would be helpful if some results are given to demonstrate the final state of each method.\n\nIn Eq. 9, adding the denominator q(z_i) will change the optimal solution. Why only add it to the first term?\n\nIn Section 5.3 and Figure 5, \u201cSSD\u201d might be a typo. \n\n[1] Adversarial Learning of a Sampler Based on an Unnormalized Distribution. AISTATS 2018.\n[2] Symmetric Variational Autoencoder and Connections to Adversarial Learning. AISTATS 2019.\n"}