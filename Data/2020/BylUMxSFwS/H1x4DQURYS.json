{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper addresses the problem of policy transfer in reinforcement learning, which is an extremely relevant open problem in RL, and is being actively studied by the community. \u2028The authors propose a framework for discovering a set of policies without external supervision which can then be used to produce reasonable performance on extrinsic tasks. \u2028The work exhibits originality in that it shows that disentangled representations, learned by intrinsic rewards,  can lead to learn behaviours that are transferable to novel situations. \u2028\n\nAlthough the problem talked here is of high relevance and the approach proposed is original and supported by theoretical results, I am leaning to reject for the following reasons:\n\n- Missing connection to some existing works in the literature. In particular, it seems that there is a link to previous works that focus on discovering reward agnostic options (such as [1]).\n- Clarity. The method description is somehow difficult to read, mainly because some variables are introduced without explanation/definition. On page 2, please define n and explain the choices of m and k. The font of the axes and legends in Figure 4 is too small, not readable when printed.\n- Experiments: My first concern is that I do not think I would be able to reproduce the results solely given the paper and supplementary material. It would be necessary to either have access to the code, or a very detailed implementation report.\n- I would have loved to see either another experiment or at least an intuition on how the framework extends to a very different domain. If not, it should be made clearer that this framework works on 2d domains, where the tasks are navigation tasks. (I am specifically referring to the representation learning phase).\n\nMinor comments:\nPage 1, first paragraph \u201c(controlling the position of fruits and nuts)\u201d\npage 5, below (9): shouldn\u2019t it be \u201cWhile \\pi_w is not \u2026\u201d?\nPage 8, last sentence of Sec 5, \u201ccould only learn to perform\u201d.\n\n1. Machado et al, EIGENOPTION DISCOVERY THROUGH THE DEEP SUCCESSOR REPRESENTATION, ICLR 2018 \n\n"}