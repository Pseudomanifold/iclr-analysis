{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to pre-train policies on some goal-reaching tasks, and then leverage the associated successor features to improve the learning of a new task. The method heavily draws from the Generalized Policy Evaluation/Improvement framework without adding much to it. The only relevant point would be showing (as the title indicates) how to obtain disentangled cumulants, and whether they help transfer to new tasks. Nevertheless, both the definition, the full method, and the claimed benefits are quite ambiguous.\n\nAmong other concerns showing that the theory needs more formal treatment, the pillar definition of \u201cOptimal independent controllability\u201d is very confusing because it seems to depend on \u201ca trajectory generated by following \\pi_i^*\u201d. But what If the environment is stochastic? Then following that policy might give different trajectories! This definition needs to be revisited. More concerning examples are given at the end of this review.\n\nOn the experimental side, Fig. 4 is the only reported result, and it has an x-axis that is not clearly explained. What are the \u201csteps (min)\u201d?\nIt is also not clear what they mean by the \u201coff-diagonal trick\u201d, which seems so important for the good performance.\nFurthermore, it seems that their method doesn\u2019t really learn anything new in most of the tasks, it just stays at the same performance that is started with after the whole pre-training steps. It is not clearly stated how much computation effort is required to obtain the desired cumulants, and this invalidates quite strongly any result they report. Even if there\u2019s no \u201creward\u201d needed during the pre-training, which arguably is not even true because you do need the rewards related to whether you have achieved a specific change in a feature!\nIn fact, it would be greatly appreciated if the \u201cfinal\u201d tasks could be expressed in a similar notation than the rest of the pre-training tasks, or vice-versa. As far as I understand, the pre-training tasks consist of making a certain feature fall into a certain subset of its possible values. Can\u2019t the final tasks, like \u201cmove the agent to the top right\u201d be also expressed in that form. The link between the two kinds of tasks needs to be much more explicit to be able to assess the relevance of this work.\n\nFinally, they only test their algorithm on Spritworld, which is a small discrete state-action space environment. Even if they try different kinds of tasks in this environment, more detailed analysis or more extensive experiments are needed to assess the benefits of the proposed approach.\nThis is particularly timely because their method relies on a discretization of some given features that represent the state, which will probably not be very practical in higher dimensional environments.\nFinally, I would like a comment on how this method interacts with discrete versus continuous action-state spaces.\n\nMisc comments:\n- Why do the authors introduce the terminology \u201cEndogenous RL\u201d, and then say it\u2019s the same as doing RL with intrinsic motivation? This seems like introducing a new name for the same concept, which seems pointless and confusing.\n- The connection with \u201clatent learning\u201d of Tolman 1984 is very unclear.\n- There\u2019s a \u201cRepresentation Learning\u201d section, but it\u2019s not clear at all whether any features are actually learned, or whether the features are actually hand-defined. Is the number of features n also hand-defined?\n- There might be a typo in the first sentence after equation (9): \u201cWhile \\phi_w is not guaranteed to be optimal with respect to \\phi_w\u201d.\n\nBecause of all these concerns, I suggest the paper to be weakly rejected. "}