{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors tried several metrics, from which they pinpoint one as the indicator to select a pretrained model without practically fine-tuning. I appreciate that the authors addressed an important problem, while the experimental setup and results are less convincing. Therefore, the proposed metric(s) and settings in the current shape are not that practical. \n\nPros:\n-\tIn this work, the authors focused on an important problem \u2013 selecting the best pretrained model from a zoo of models without finetuning all of them in a brute-force manner. \n-\tThe idea of applying the metric(s) to select a layer from which the consecutive layers are truncated is intriguing, for the sake of model compression.\n\nCons:\n-\tThe work is more alike a course project than a novel scientific contribution, with all the metrics mainly inherited from the evaluation of clustering and other existing literatures. \n-\tThe experiments are not comprehensive, so that the conclusions drawn are weak and untenable.  \n-\tThe writing with many grammatical errors and typos definitely needs polishing.\n\nDetailed comments:\n1.\tThe authors calculated all the metrics in terms of feature maps in the last layer. Therefore, it is not valid to conclude that other metrics are less effective than sparsity. Some metrics, like fisher discriminator, are likely effective only in the low-level features. It is better for the authors to investigate all metrics in terms of all layers.\n2.\tSince all the pretrained models are pretrained from ImageNet, it is possible that they lose some diversity, which tends to deactivate metrics other than S5 and S6. Unless the authors trained several models from scratch on different datasets and achieved similar results, the conclusion that S5 and S6 are strong is not that convincing.\n3.\tWhy during fine-tuning, do you use SGD instead of Adam?\n4.\tWhat kind of correlation metrics do you use in Eqn. (4)? And will the correlation metric influence the effectiveness of S4?\n5.\tThe most confusing part lies in that the valid metric suggested by the author, i.e., C5,6(0.574), is only an empirical observation on a very limited number of datasets, without any principled methodology. Actually, it is highly possible that given a highly different dataset, the best metric changes. Should the practitioners determine which metric to use first, and even the coefficient to combine metrics?  This is paradox, in my opinion. \n6.\tGrammatical errors and missing details:\no\tAbstract: a variety of network structure -> a variety of network structures\no\tSection 3: The sentences for the three hypotheses, H1/2/3, do not even have verbs in the if part. \no\tSection 3.4: to estimates -> to estimate\no\tSection 3.6: which quantify -> which quantifies\no\t\u2026"}