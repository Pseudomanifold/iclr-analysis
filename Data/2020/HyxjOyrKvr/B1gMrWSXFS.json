{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors learn epitomes, which are small weight tensors which can be used with a learnt transform to produce tensors of an appropriate size (e.g. the sizes used in MobileNet v2).  This gives a reduction in the number of parameters required, and the number of MAdds in theory.\n\nThis paper is badly written, and could do with a rewrite:\n\n- Citations are used incorrectly (\\cite should be used when the citation is meant to be read as part of the sentence). \n- \"less elements\" --> \"fewer elements\"\n- \"misuse the notion\" --> \"abuse the notation\"? \n- \"for fair comparisons\" --> \"for a fair comparison\"\n\nThe method is poorly explained; I have read Section 3.2 several times, and I'm still not entirely certain of what's going on. Figure 1 is helpful, but Figure 2 is not, and could be redesigned. 2(b) makes it look like you are going from a 3x3 epitome to a 2x2 kernel, which is clearly not what is happening. I think it would be helpful to give a detailed pictoral example of an epitome mapping to a weight tensor, with arrows between relevant indices changing. \n\nOn initial reading, I thought the method allowed dynamic allocation of your epitomes to lots of different tensor sizes. From what I can tell, the network has to be trained from scratch for each possible size, so it isn't flexible in that respect.  From what I can gather, the paper is presenting an alternate approach to *downscale* networks, as opposed to say, reducing width or depth. The comparisons to different widths of MobileNet v2 make more sense under this scenario.\n\nThe main sell of the methods appears to be on the basis of MAdd reduction. This makes me nervous, as it doesn't necessarily correspond to actual speed-up or a reduction in energy (see https://arxiv.org/abs/1801.04326). EfficientNet was mainly about Madds too, but they provided some inference times. Would it be possible to add these? The method used with the integral image sounds expensive.\n\nThe results look good, but error bars,on the CIFAR experiments at the very least, would be appreciated.\n\nPros:\n-------\n- Good results\n- Method appears largely novel (although bears some resemblance to https://arxiv.org/abs/1906.04309)\n\nCons:\n--------\n- Badly written\n- The method is poorly explained\n- Uncertainty re: MAdds as a primary comparator\n\nI propose a weak reject for this paper for two primary reasons:\n1) The standard of writing, and the explanation of the all-important method are not up to scratch for a top tier conference\n2) I have concerns regarding the MAdd calculations. Perhaps you could provide some pseudo-code in the author response?\n\nI am happy to upgrade my score if the authors deal with these issues sufficiently."}