{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this work, the authors describe a technique for compressing neural networks by learning a so-called Epitome (E), and a transformation function (\\theta) such that the weights for each layer can be constructed using \\theta(E). The epitome and the transformation function can be learnt jointly while optimizing the network for the task specific loss. \n\nThe main idea of this paper is really interesting, and the experimental results which compare against other recent techniques also validate the proposed technique. However, while I think the main idea is relatively clear, I personally found the description of the proposed techniques -- particularly 3.2 and 3.4 -- to be somewhat hard to follow, particularly given that some details only appear in the Appendix. I would suggest that the authors try to revise this section by trying to move Figures 4 and 5 from the appendix into the main text. Some additional suggestions also appear below.\n\nOverall, I would while I like the ideas in this paper, based on the current presentation I am inclined to rate the paper as a \u201cweak accept\u201d, though I would raise my rating if the paper was revised to improve the presentation.\n\nMain comments:\n1. The authors mention that \u201cDuring inference, (the) routing map enables the model to reuse computations when the expanded weight tensors are formed based on the same set of elements in the epitomes and therefore effectively reduces the computation cost.\u201d It would be nice to include some results which indicate what the savings are with the proposed routing map.\n\n2. Section 3.2: There were a few aspects of section 3.2 that I think could be improved for clarity.\nA.) Personally, I found Figure 2 somewhat tricky to follow. I would suggest removing the 3x2 \u201cEpitome\u201d and \u201cGenerated Kernel\u201d figures on the extreme right of the image, since I\u2019m assuming the only goal of these is to indicate that \u201corange\u201d and \u201cblue\u201d correspond to \u201cEpitome\u201d and \u201cKernel\u201d respectively. I would also suggest mentioning the correspondence of the colors as the first sentence in the caption. Finally, if possible, I would suggest adding a small description alongside the (a), (b), (c) subcaptions: e.g., (a) straightforward but non-differentiable mapping, \u2026 , (c) Generated Weigh Kernel.\n\nB.) I believe that the authors use a separate E for each layer, and that Epitomes are not shared across layers. I may have missed this in the text, but it would be useful to clarify this explicitly again in the section.\n\nC.) The \u201cparameterized transformation layer\u201d is mentioned before Equation 3. I think it would be useful to mention that this is implemented using neural networks in your work for clarity. E.g.: \u201cTo handle the above two obstacles, ... three\nparts: (1) a parameterized transformation learner \u03b7 (implemented using a neural network in this work) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d\nor\n\u201cTo handle the above two obstacles, ... three\nparts: (1) a parameterized transformation learner \u03b7 (See Section 3.3) used to learn a set of starting indices for patches in epitome (i.e. all elements in the same epitome patch share identical starting indices); \u2026 and an interpolation based generator (Eqn. 3).\u201d\n\nD.) The exact structure of the \u201cparameterized transformation layer\u201d wasn\u2019t exactly clear to me. In 3.3, the authors mention that it consists of \u201cof two convolutional layer, followed by a sigmoid function \u2026 takes the feature map of the convolutional layer as input\u201d. Please clarify exactly what is fed in as the input to this network e.g., (input feature map: F, and the indices i,j).\n\n3. I personally also found Section 3.4 which discusses the Computation reduction was also somewhat hard to follow. Some clarification questions: Is the memory/computation cost of the storing/creating the routing map included in the compression calculations? I think it is important to factor these costs when computing the savings achieved by the model. Also, I was unclear on what R_{cin} and R_{cout} are, and why they appear in Equation 5. Could the authors please clarify. \n\nMinor Comments:\n1. Abstract: \u201cTraditional compression methods \u2026 all assume that network architectures\nand parameters should be hardwired.\u201d What does it mean for them to be \u201chardwired\u201d in this context?\n2. Abstract: \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction and AutoML for Model Compression (AMC) by 2.5% with nearly the same compression ratio.\u201d --> \u201cExperiments demonstrate that, \u2026 with 25% MAdd reduction, and a 2.5% Madd reduction for AutoML for Model Compression (AMC) with nearly the same compression ratio.\u201d\n"}