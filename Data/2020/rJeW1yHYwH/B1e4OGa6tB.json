{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary: This paper addresses the problem of representation learning for temporal graphs. That is, graphs where the topology can evolve over time. The contribution is a temporal graph attention (TGAT) layer aims to exploit learned temporal dynamics of graph evolution in tasks such as node classification and link prediction. This TGAT layer can work in an inductive manner unlike much prior work which is restricted to the transduction setting. Specifically, a temporal-kernel is introduced to generate time-related features, and incorporated into the self-attention mechanism. The results on some standard and new graph-structured benchmarks show improved performance vs a variety of baselines in both transduction and inductive settings. \n\nPros: \n+ Dynamic graphs are an important but challenging data structure for many problems. Improved methods in this area are welcome. \n+ Dealing with the inductive setting is an important advantage. \n+ Clear performance improvements on prior state of the art is visible in both transductive+inductive settings and node+edge related tasks.\n\nCons+Questions:\n1. Technical significance: Some theory is presented to underpin the approach, but in practice it seems to involve concatenating or adding temporal kernels element-wise to the features already used by GAT. In terms of implementation the concatenation in Eq 6 seems to be the only major change to GAT. I\u2019m not sure if this is a major advance.  \n2. Insight. The presented method apparently improves on prior work by learning something about temporal evolution and exploiting it in graph-prediction tasks. But it's currently rather black-box. It would be better if some insight could be extracted about *what* this actually learns. What kind of temporal trends exist in the data that this method has learned? And how are they exploited in by the prediction tasks?\n3. Writing. The English is rather flaky throughout. One particular recurring frustration is the use of the term \u201carchitect\u201d which seems wrong. Probably \u201carchitecture\u201d is the correct alternative. \n4. Clarity of explanation. The paper is rather hard to follow and ambiguous. A few specific things that are not explained so well: \n4.1. Eq 1+2 is not a sufficiently clear and self-contained recap of prior work. \n4.2. Symbol d_T used at the start of Sec 3.1 seems to be used without prior definition making it hard to connect to previous Eq1+2. \n4.3 The claim made about alternative approaches (Pg4) \u201cReparameterization is only applicable to local-scale distribution family, which is not rich enough\u201d. Seems both too vague and unjustified. \n4.4 The relationship between $t_i$ and the neighbours of the target node in Eq. 6 is not very clear.\n"}