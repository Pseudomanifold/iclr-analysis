{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper suggest to use the previously proposed ScatterNet Hybrid Deep Learning  (SHDL) network in a continual learning setting. This is motivated by the fact that the SHDL needs less supervised data, so keeping a small replay buffer can be enough to maintain performance while avoiding catastrophic forgetting.\n\nMy main doubt is the benchmark and evaluation of the proposed method. The metrics reported are all relative to a baseline value (which I could not find reported), and make it difficult to understand how the model is performing in absolute term. This is particularly a problem when comparing with existing state of the art method (Fig. 3, Table 4), since this does not exclude that they may have an overall much better accuracy in absolute terms.\n\nAlso concerning the comparison with the previous literature, I could find no details about the architecture and the training algorithm used. Notice that this may in particular affect some the reported metrics, since they depend on the shape of the training curve (reporting the training curves for all methods may also be useful). Also, since SHDL uses a small replay buffer, are EWC and the other method modified to use the replay buffer and make the comparison fair?\n\nWhile several standard tests for continual learning exists (for example the split CIFAR10/100 in Zenke et al., 2017), those are not used, and rather a simpler test is used which only attempt to learn continually two datasets.  It would be helpful to also report a direct comparison on those tests.\n\nRegarding the line: \"The autoencoder is jointly trained from scratch for classes of both phases to learn mid-level features\", does this mean that the auto-encoder is trained using data of the two distributions at the same time rather than one after the other? If it is the former case, while it is unsupervised training, it would be a deviation from the standard continual learning framework and should clearly be stated.\n\n"}