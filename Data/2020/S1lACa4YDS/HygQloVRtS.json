{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "SUMMARY OF THE PAPER:\nThis paper proposes a way to automatically select a divergence to use in variational inference (VI) given a set of datasets (tasks). They consider searching within the alpha-\ndivergence and f-divergence families. The proposed algorithm works as follows: do a few gradient descent steps on the variational parameters given a fixed divergence parameter, then update the divergence parameter by taking the gradient with respect to a meta loss which is a task-specific measure of goodness of the variational distribution (like test log marginal likelihood). The latter gradient is computed through the gradient descent computation in the inner loop.\nThe second proposed algorithm does the same, but also learns a good initialization for the variational parameters. This initialization is good in the sense that taking one (or a few) gradient descent step on the variational parameters should give us good variational parameters (MAML style). This is done by taking a gradient with respect to this good initialization parameter through the inner loop gradient descent.\n\nSTRUCTURE:\nThe paper is well-written and easy to understand.\n\nNOVELTY:\nAs far as I know, learning a divergence for VI using meta-learning is new. The related work is discussed well in section 5.\n\nEXPERIMENTS:\nThere are experiments on three tasks of increasing complexity. In the simpler experiments, careful ablation studies are done. The results generally show that the proposed method is preferable to alternatives.\n\nCONCLUSION:\nI recommend acceptance."}