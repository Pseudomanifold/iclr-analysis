{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "* Summary *\nThe paper proposes a new unsupervised anomaly detection method using the discrepancy between the activations of a deep convolutional autoencoder (AE) for clean (in-distribution) samples and noisy (out-distribution) samples. The authors propose a Subset scanning method which searches for a subset of node activations that are higher than expected, in order to classify new inputs are anomalous or clean. Furthermore, the authors propose the ability to visualize the set of anomalous nodes in the AE output error space to provide an explanation for the decision of the anomaly detector. They also extend the idea of AE reconstruction error based anomaly detection to incorporate hidden layers of the AE. The focus is on detecting anomalies generated via adversarial attacks, rather than on out-of-distribution samples coming from different classes.\n\nPros:\n1. The paper is well written, and provides sufficient background on the topic.\n2. The idea of using a subset scanning approach for activations in the hidden layer is interesting.\n3. The motivation and background for the subset scanning approach are well explained.\n\nCons: \n1. The experimental section lacks in depth, and tests only a very restricted scenario of anomaly detection. The method is sold as working on \"any pre-trained, off-the-shelf autoencoder network\", yet the evaluation is only on very simple datasets MNIST and Fashion-MNIST, which have no background, centered, single-scale images. Evaluations on harder datasets which require more sophisticated autoencoder architectures would have been interesting to showcase the strength of the method. If e.g. geometrical transformations are applied to images (translations, rotations, etc.), I strongly assume that the subset scanning method would have difficulty, because the set of highly activated neurons would most likely differ, especially in early layers.\n2. The comparison is only done to two not exactly state-of-the- art methods. Again, if the method can be applied to pre-trained models, it would have been interesting to apply the method to existing state-of-the-art models, or at least multiple variations of autoencoder architectures.\n3. The authors focus entirely on adversarially generated samples as anomalies. Although this is an interesting and difficult scenario, this is not properly motivated, and there should also be an evaluation of other anomaly cases, e.g. detection of out-of-distribution data coming from different classes or different datasets.\n4. The focus is on detecting adversarial examples from a single attack model, then the performance of the proposed unsupervised method should be compared to supervised detectors of adversarial attacks, e.g. as described in Metzen et al. (2017) \"On detecting adversarial attacks\" and follow-up papers.\n\nMinor issues:\n1. Many of the referenced figures are in the appendix, and figures are not presented in the order in which they are cited.\n2. There are quite a few typos in the manuscript.\n3. Fig. 5 is confusing to understand. Why should we expect the clean images to have anomalous nodes along the contours of the digit? \n\nOverall I think the authors propose an interesting idea but not enough convincing evidence that their method is actually improving unsupervised anomaly detection in its general setting. In its current form the paper is not fit for publication at ICLR, but after addressing the weak points and a more thorough experimental evaluation this could become a good paper."}