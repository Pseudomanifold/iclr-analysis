{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper explores the idea of using subset scanning [7] over the hidden activations of an autoencoder (AE) for the task of unsupervised anomaly detection (AD). The proposed method combines the standard reconstruction error with subset scanning scores and does not require any modification/retraining of the standard AE. The paper further inspects how units of the AE that detect anomalous patterns from such subset scanning scores might be used for anomaly visualization. Finally, the paper concludes with experiments on MNIST and Fashion-MNIST that indicate an improvement in detection performance over the standard AE reconstruction error and OC-SVM baselines.\n\nThis paper should be rejected in my opinion due to the following four main reasons: \n(i) The technical quality of the paper is poor and the main idea not well explained;\n(ii) The experimental evaluation considers rather simple datasets (MNIST, Fashion-MNIST) and only includes two baselines (vanilla AE, OC-SVM), but not any major competitors ;\n(iii) The work is not well placed in the literature and major related work is missing;\n(iv) The overall presentation is poor;\n\n(i) I find that subset scanning [7], seemingly the main component the approach, is not well defined and explained in the paper. Section 3 introduces the method with an example that to me reads like an explanation of p-hacking (Section 3.1, second paragraph). Inconsistent notation and the use of undefined expressions (see examples in comments below) make it hard to follow and understand what exactly is being done. After reading, I would somewhat summarize the idea as \u201cconducting multiple hypothesis testing over the hidden network activation statistics of an autoencoder for unsupervised anomaly detection\u201d. This idea might be valid and indeed lead to significant improvements over the baseline of just using the AE reconstruction error. However, the specific method proposed is not rigorously defined and clearly explained in this manuscript. The explanation of the method further leaves important questions unanswered: (ia) Why use the Berk-Jones over the Kolmogorov-Smirnov test? (ib) How is the Linear Time Subset Scanning (LTSS) property exactly defined and why is it satisfied for AEs?\n\n(ii) I find the experimental evaluation in the paper not convincing. Only two baselines (vanilla AE using reconstruction error and OC-SVM) are considered, but there are many deep competitors (VAEs, GAN-based AD, Deep SVDD, etc.) [1, 10, 4, 9, 5, 6] that have significantly improved detection performance over the AE baseline. MNIST and Fashion-MNIST are also rather simple datasets. Adding more complex datasets like CIFAR-10 or MVTec [2] would be insightful (also see experimental evaluations in [9, 5, 6]).\n\n(iii) The paper does not properly place the work into context with existing literature. Major works on deep anomaly detection and out-of-distribution detection are not included [1, 10, 4, 9, 5, 6, 8, 3].\n\n(iv) The overall presentation of the paper is poor. The writing is wordy, terms are imprecisely and inconsistently used, and the text contains many grammatical errors. Figures are unpolished and references are scattered over the paper. See comments below for examples. Going over the 8 page limit is not justified in my opinion.\n\nI listed some suggestions and ideas to improve this work below.\n\n\n####################\n*Additional Feedback*\n\n*Ideas for Improvement*\n1. Explain your method rigorously and clearly. I believe that using the activation statistics of AEs (via testing or otherwise) should lead to an improvement over the reconstruction error baseline.\n2. Plots as in Figure 9 for the activation distributions of normal vs. anomalous samples would be insightful. In which layers may anomalies be detected?\n3. It might be interesting to specifically look into different activation functions and choose/design specific suitable tests. Might using some form of normalization (e.g. batch normalization) be beneficial?\n4. Going further into the direction of deep AD interpretability and improving over the reconstruction heatmap baseline might be worthwhile since this is an important problem with only little prior work.\n5. Include major related works as outlined above and clearly state the contribution of your work in context of the existing literature.\n\n*Specific comments*\n6. Imprecise use of terms and language:\n\u2022 Abstract: \u201clarge outliers\u201d What should large outliers be exactly? Be specific.\n\u2022 Section 1: \u201c... improve the anomaly score of current autoencoders ...\u201d You improve the anomaly detection performance. A score is just a function.\n\u2022 Section 1: \u201c... reconstruction error space ...\u201d The reconstruction error finally is a one-dimensional loss. Be specific what you mean.\n\u2022 Figure 1, caption: \u201c... the highest mutual information exchange ...\u201d Exchange?\n\u2022 Section 3 on subset scanning jumps between subsets of data samples and subsets of network activations.\n7. Inconsistent notation:\n\u2022 Section 1, second paragraph: Jumping between x and w for samples.\n\u2022 Function $\\phi$ in Eq. 4 not defined.\n\u2022 Section 2.2, definition of BIM: Clip function not explained. Readers might guess, but rather explain your notation and definitions.\n8. Writing is repetitive and redundant:\n\u2022 Section 1, first paragraph: Repetitions of applications with the same references already (\u201c... human annotation errors ...\u201d).\n\u2022 The autoencoder is introduced and explained three times: Section 1, Section 2.1, Section 4.\n9. Abstract: \u201cIn this paper, we proposed ...\u201d \u00bb \u201cIn this paper, we propose ...\u201d Simple present is the primary writing tense. Many more examples like this in the main text.\n10. Closely place Figures where they are referenced in the text and vice versa. The overall first reference in the Introduction, is to Figure 4 (b) on page 9.\n11. Section 1, second paragraph: \u201cFurther detail on the autoencoder architecture and training setup for the experiments can be found in the Section A.4\u201d A reference like this only should first appear in the experimental section.\n12. Section 1: \u201cThis is formally quantified as the subset with the highest score according to a non-parametric scan statistic.\u201d Passive voice is bad writing style. Write in active voice. Many more examples like this in the main text. Section 2.2: \u201cSeveral attack models have been used to target classifiers in this study, ...\u201d etc.\n13. Section 2.1: \u201c... to address the shortcoming of conventional autoencoders in the presence of anomalies samples during training.\u201d Name them. Be specific!\n14. Section 3: \u201cTreating the detection problem as a subset scan has desirable statistical properties.\u201d No reference or explanation. Be specific!\n\n\n####################\n*References*\n[1] J. An and S. Cho. Variational autoencoder based anomaly detection using reconstruction probability. Technical report, SNU Data Mining Center, Seoul, South Korea, 2015.\n[2] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592\u20139600, 2019.\n[3] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[4] H. Choi, E. Jang, and A. A. Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n[5] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[6] D. Hendrycks, M. Mazeika, and T. G. Dietterich. Deep anomaly detection with outlier exposure. In ICLR, 2019.\n[7] E. McFowland, S. Speakman, and D. B. Neill. Fast generalized subset scan for anomalous pattern detection. The Journal of Machine Learning Research, 14(1):1533\u20131561, 2013.\n[8] E. Nalisnick, A. Matsukawa, Y. W. Teh, D. Gorur, and B. Lakshminarayanan. Do deep generative models know what they don\u2019t know? In ICLR, 2018.\n[9] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Mu\u0308ller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393\u20134402, 2018.\n[10] T. Schlegl, P. Seebo\u0308ck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146\u2013157. Springer, 2017."}