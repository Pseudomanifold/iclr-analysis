{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary\n---\n\n(motivation)\nThis paper proposes a new approach to pruning activations from neural networks,\nbut uses it to understand neural nets better rather than trying to make them\nmore efficient. It prunes filters per-class 1) to measure how sensitive image\nclasses are to pruning and 2) to measure how similar classes are by comparing\nthe filters they prune.\n\n(approach)\nUsing only images from class c, CNN pre-activations are aggregated across spatial\ndimensions and examples. This gives an average feature vector for that class.\nT percent of neurons are pruned. Features with lower pre-activations\n(possibly negative with large magnitude) are pruned first with successively\nhigher activations pruned later.\nNo re-training is performed.\n\n(experiments)\nExperiments use AlexNet and 50 of the 1000 ImageNet challenge classes to show:\n1. Pruning filters results in decreased accuracy with smaller decreases in accuracy as the first and last few filters are pruned.\n2. More filters from middle layers are pruned than are those from early and late layers.\n3. After some filters have been pruned, sometimes pruning can increase accuracy.\n4. I could not understand section 3.2.\n\n\nStrengths\n---\n\nThe proposed pruning method is simple and efficient.\n\nI like the broad goal of understanding CNNs. We could use more papers that just focus on analysis like this one.\n\nI think the idea of class conditional pruning is novel.\n\n\nWeaknesses\n---\n\n# Major Weaknesses\n\nI don't see why these results are significant. I do not find these results very surprising (see next comment) and I do not see why the community will find them useful.\n\n* In particular, consider the conclusion. Sentences 2, 3, 5, and 6 seem to all be observations about what happened in the experiments. The conclusion should re-iterate the results, but it should also say why they are important. How does the work relate to the goals of the community at large? Which goals? Will this enable important new capabilities? What general concepts did we learn from this that we didn't know before?\n\nSome of the positions in the paper could be more carefully considered. There are alternate explanations for many of these phenomena.\n\n* Pruning smallest activations doesn't make sense to me. Typically redundant or un\"important\" activations are pruned because doing so has negligible impact on accuracy. The smallest activations are pruned here. Should the smallest activations be redundant or unimportant in some way? Does this rely on ReLU activations following the pre-activations? I think these values are not necessarily redundant and could be quite important (e.g., as measured by some saliency explanation like Integrated Gradients), but I could be wrong. It would be useful to provide a baseline which removes highest instead of lowest activations.\n\n* Much of the surprise about figure 5 seems to be because it is not monotonic but it was expected to be monotonic. I agree that these curves should generally go down as theta increases, but I don't see why that relationship should be strict. I would be surprised if activations were not in some way dependent on one another. Furthermore, why does that dependence have to be interference? Couldn't it also be that some activations are complementary (and thus ineffective when only one is present)?\n\n* Does Network Wise Pruning (NWP) favor more layers than others? It may be that some filters have higher Accumulated Responses per filter simply because there are more feature maps in the previous layer (thus more things summed up) and not because of what information they capture or their relationships with other filters. Does this happen? This could be an alternative to the following conclusion: \"This means that the encoding provided by the first and last layer seems to be the most crucial and the densest.\"\n\n\n# Other General Weaknesses:\n\n* AlexNet is a rather old architecture to use for this analysis, so I can't be confident these methods or behaviors will generalize to other architectures. Does it hold for more modern architectures?\n\n\n# Missing details / Points of confusion:\n\n* The paper says the correlation from Figure 6 should be expected to decrease as theta increases. Why should this be expected?\n\n* What exactly does Figure 6 measure? I think it's the ratio of the size of the intersection rho_sigma to the size of the union of the same two sets. However, the text calls the metric \"correlation.\" This should be made clearer.\n\n\n# Minor presentation weaknesses\n\nSome parts of the notation/explanation don't make sense to me:\n* \"Let Lambda=... be the number of object classes in the dataset.\" But Lambda is defined as a set, not a numeral.\n* The number p needs more context/subscripts as it depends on class c and filter i.\n* \"\u03b8 = [0, . . . , 1]\" This defines theta as a finite set, but I think it's meant to be a number in the interval range (0, 1).\n* Why is lambda_c needed instead of just using the index c to specify a class? The variable lambda doesn't seem to be any different than a class index.\n* \"let F\u03c3 and F\u03c3 \u0304 be the set of unpruned and pruned filters for a given \u03b3...\" I don't see how this works. According to eq. 3 gamma depends on a particular filter i and class c, so the only filter F could contain is the ith one. (Later it became clear that F was only mean to class conditional, not filter conditional.)\n\nFigure 5: These plots shoul all have the same y axis range. This would make them comparable and allow readers to much more easily compare trends across classes. Similar steps should be taken so the same range is used any time theta is plotted on the x axis.\n\nFigure 3: I find it hard to get an overall ordering of the approaches in this figure because there is so much variance from class to class. It effectively conveys the variance, but I'd also like to know what the means across classes are for each method so I can compare the proposed approaches more effectively.\n\n\"The results indicate that classification classes are asymmetrically represented by filters resulting in the fact that some object classes have their classification accuracy increased when pruned for.\"\n* I'm not sure what it means to be asymmetrically represented by filters.\n\n\nSuggestions\n---\n\nThis analysis would have been more interesting with an existing pruning approach because we would already know that such an approach is good a removing unnecessary filters.\n\nFinal Evaluation\n---\n\nQuality: Experiments could have been cleaner, but they basically demonstrate the patterns the paper intended to show.\nClarity: I could understand most experiments at a high level, but I found it hard to understand the motivation and the rest of the experiments.\nSignificance: As explained above, I do not see why the paper is significant.\nOriginality: The experiments and the proposed class conditional pruning approach are somewhat novel.\n\nThe paper is somewhat novel, but do not find it very clear and I do not see why it is important, so I cannot reccomend it for acceptance.\n"}