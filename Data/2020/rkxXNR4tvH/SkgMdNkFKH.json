{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes to prune filters in CNN model to interpret the correlation among different filters or classes. Though interpreting CNN filter is a well-studied topic, learning the interpretability through pruning is new and interesting. The proposed method is simple, by just using the averaging the value of the output of each filter as the indicator. The author claims that object classes represented in high feature density area usually share similar filters, which is in accordance with the common sense. Also, filters at lower layer are usually important. \n\nSome questions: \n1.\tSince each filter is still like a black box, is it possible to visualize some result of the discovered interpretability?\n2.\tI\u2019m confused with the implementation of pruning. If the filter at layer j-1 is pruned, then the dimensions of the filters at layer j should also change. How this issue is dealt with? Further, will this dimension reduction, instead of the pruned filter itself, influence the model performance?\n3.\tWhy not use the absolute value of r_i? Any justification for this?\n4.\tThe author mentioned that no normalization across categories are applied. However, are r_i from different layers comparable under NWP? Also, How can you guarantee that the Eq.(2) is comparable for different filters? More discussion on the normalization is desired.\n"}