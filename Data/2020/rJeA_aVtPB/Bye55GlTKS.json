{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose Demon schedule that automatically decays momentum. The paper is very well written, and the experiments are performed on an, extensive, compared to typical papers in the domain, suite of tasks. Unfortunately, while the paper is technically well executed, I have fundamental issues with novelty. Based on the current state-of-the-art understanding of optimization in deep learning, it is quite expected that decaying momentum has an analogous effect to decaying learning rate or increasing batch size (see [1,2]). Given this \"Similarly, applying DEMON to momentum SGD rivals momentum SGD with learning rate decay\" is not surprising, and \"and in many cases leads to improved performance\" warrarnts a bit of scepticism.\n\nMore precisely, a very similar analysis as in \"Motivation and interpretation.\" can be already found in [1]. Similarly, [2] already suggests decreasing momentum. Based on this, experiments in Table. 2 (and the other similar Tables) should compare to decaying learning rate or batch size in Adam and other adaptive methods using an analogous schedule. It might be surprisng, but adaptive methods do benefit from learning rate schedules. Analogously, could you please compare performance of Demon to the recommendation in [2] to decay momentum together with increasing learning rate initially? \n\nUnless a more detailed experiments demonstrate that tuning momentum brings additional benefit on top of its effect on the effective learning rate, there isn't unfortunately in my onion enough practical value in the proposed method.\n\nReferences:\n\n[1] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\n[2] Fast.ai documentation on one cycle method, https://docs.fast.ai/callbacks.one_cycle.html"}