{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the author propose a decaying momentum rule to improve algorithm. Furthermore, he apply this rule in momentum SGD and Adam, then use experiment to prove the algorithm.\n\nIn the experiment, it training on many different dataset and compare with many baseline. The algorithm with decaying momentum rule get much better result than all other algorithm. Furthermore, the paper is well written and east to follow.\n\nHowever, I still have a question about this paper. \n\nEven the algorithm get a good performance, the intuition of the algorithm is not clear. In fact, it is reasonable to decay the total contribution of a gradient to all future updates; it is still unclear why you choose such $\\beta$. As you say, the contribution of previous item is $ \\sum \\beta^i=\\frac \\beta {(1-\\beta)} $, however, this is only correct why $\\beta$ is constant, and when the $\\beta_t$ is change like your definition, what will the contribution become? Furthermore, even the contribution of previous item is $ \\frac \\beta {(1-\\beta)} $, why you want the contribution is equal to $ (1-\\frac t T)\\beta_{init}/(1-\\beta_{init})$, it may need more comment about why you choose the value.\n"}