{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers the model-based reinforcement learning (MBRL)\nproblem in which one of the main parts is to learn the transition\nprobability matrix of the underlying MDP (called Transition Learning -\nTL). The motivation is that if the transition model can be learnt from\nsome real-world trajectories, the agent can improve by interacting with\nsimulated environment built from the learnt transition model; hence,\nthe overall number of real-world interactions is reduced.\n\n\nGiven a policy $\\pi(a|s)$, and a transition $T(s\u2019|s,a)$, the occupancy\nmeasure $\\rho(s,a)$, defined in Eq (2) in the paper, can be\ninterpreted as the discounted distribution of state-action pairs in\nthe rollouts. The main idea is to use Wasserstein-GAN (WGAN) to match\nthe distribution of $p(s,a,s\u2019) = \\rho(s,a) T(s\u2019|s,a)$. In particular,\ngiven a policy, the idea is to get some real-world trajectories and\nfeed into the WGAN framework in which the generator tries to generate\nsynthesized trajectories and the discriminator tries to discriminate\nbetween them. These trajectories are then used in policy optimization\nto obtain a new policy.\n\n\n\nThe authors presented two theoretical results: 1) Consistency for\nWGAN: if WGAN is trained optimally, then the synthesized transition is\nthe same as the real transition; and 2) Error bound for WGAN: the\ndifference between cumulative reward under synthesized transition and\nthat under real transition is upper bounded by a linear function of\nWGAN training error. On the experimental side, the authors used the\nmodel-based benchmark library (MBBL, Wang et al. 2019) to compare the\nproposed algorithm with several existing algorithms on four MuJoCo\ntasks: Hopper, HalfCheetah, Ant, and Reacher.\n\n\n\nOverall, I think this work is positive. The theoretical part is\ntechnically sound (there might be an error in the proof of Proposition\n1 but it is fixable). The experimental part is also sensible, although\nit would be good to include the performance comparisons for other\ntasks in MBBL. Details in the comments below.\n\n\n\nMain   comments/suggestions:\n\n- MBBL (Wang et al. 2019) has many other tasks (18 in total, the\n  authors only include 4 in this work). It would be good if the\n  authors can also include the performance of the proposed algorithm\n  with respect to those tasks.\n  \n\n- The claim at the end of Proposition 1\u2019s proof (in side the proof), in my assessment, is not\n  established. It is clear from the proof that $p(s,a,s\u2019) = p\u2019(s,a,s\u2019)$ is\n  a sufficient condition for $\\rho_{T}(s,a) = \\rho_{T\u2019}(s,a)$, but I\u2019m\n  not convinced that it is also a necessary condition. Note that\n  $\\rho_{T}(s,a)$ and $\\rho_{T\u2019}(s,a)$ are two unique solutions of two\n  *different* Bellman equations. It would be good if the authors can\n  provide the detailed proof for the necessary condition if it is\n  true. Nevertheless, the statement of Proposition 1 only claims the\n  sufficient condition so this is fixable.\n  \n\n\n\n\nOther minor comments:\n\n- End of Section 1, page 2: the notations $T$, $T\u2019$, $R(T)$, $R(T\u2019)$ are used but not introduced until Section 3.\n\n- Typo, page 2, last paragraph: \u201cIfD method\u201d ==> \u201cLfD\u201d method?\n\n- Algorithm 1, page 6: what are the $\\phi$ and $w$ parameters? It looks to me that they are taken from WGAN paper but with no explanation.\n\n- Two references \u201cSyed et al. (2008a)\u201d and \u201cSyed et al. (2008b)\u201d are actually the same.\n\n- Reference \u201cLanglois et al. (2019)\u201d should be corrected as \u201cWang et al. (2019)\u201d -- see https://arxiv.org/abs/1907.02057\n\n\n"}