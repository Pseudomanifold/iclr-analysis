{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In model-based reinforcement learning methods, in order to alleviate the compounding error induced in rollout-based planning, the paper proposes a distribution matching method, which should be better than a regular supervised learning approach (i.e. minimizing mean square error). Experiments on continuous control domains are presented to show the algorithm\u2019s advantages. The author compares the proposed method with several well-known baselines. The proposed approach is interesting, however, there are some issues.\n\n1. Missing citations. The author should include discussions regarding model-correction methods. For example, Self-Correcting Models for Model-Based Reinforcement Learning, Combating the Compounding-Error Problem with a Multi-step Model. Also, there is a highly relevant work \u201cLearning Latent Dynamics for Planning from Pixels\u201d, in section 4, methods regarding multi-step prediction issue are introduced. \n\n2. The experimental results do not reflect the key issue the author attempts to resolve. MBRL algorithms are very diverse, there are different ways of learning a model and of using a model. Even though the proposed algorithm seems to be better, it is unclear whether it is due to better model accuracy. I think the author should show model accuracy in a rollout of samples comparing against traditional strategies. "}