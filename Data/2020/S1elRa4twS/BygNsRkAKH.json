{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper focuses on a very interesting problem, that of pre-training deep RL solutions from observational data.   The specific angle selected for tackling this problem is through meta-learning, where a set of Q-functions / policies are learned during pre-training, and during testing the network identifies the training set MDP matching the data to extract a transferable solution.\n\nThe main strength of the paper is to draw attention to the issue of pre-training in RL, which is much less studied than in supervised learning, where it has been shown to have tremendous impact.   The paper also provides reasonable coverage of a large amount of related work.\n\nUnfortunately I really struggled (despite careful reading) to understand several aspects of the proposed methods.   The training of function f() is not clearly explained; is this done as per Sec.2.4?  What is the loss function for this?  Is it done end-to-end as per the pipeline in Fig.1 (right), so using a gradient propagated back from Q?   What is the purpose of Proposition 1?  The more interesting point seems to be that solutions can \u201cconverge to a degenerate solution\u201d, but this is not formally defined (i.e. how do you assess degeneracy, and how is that information used?)  Furthermore Proposition 1 seems limited to discrete state/action spaces. Is this the case for TIME in general?  The results on Mujoco suggest not.  Regarding the second phase of the pipeline, it is briefly mentioned that \u201cP has low capacity\u201d (bottom of p.4), but this is not explained further.  Is this due to a generalization issue, or a computational issue?  Why would P be low capacity but not E?  How does this actually impact the implementation?\n\nAs a higher-level comment: is it really necessary (preferable) to infer the identity of a specific train MDP (using the function f)?  This is used as a premise in this work, but I am not convinced this is desirable (for good generalization) or scalable (in the case of several observed meta-train MDPs).   What is the advantage of proceeding in this way?  Much of the work on pre-training in supervised learning just exposes the learner to large amounts of observational data to pre-condition the solution.\n\nFinally, I have some concerns with the results as presented in the paper.  There are some details lacking, for example how specifically are the meta-test MDPs chosen for the Mujoco experiments?  How similar/different from the meta-train MDPs?  This is an issue because in Sec.5.1 the meta-test MDPs are chosen to coincide with meta-train MDPs.  So I am definitely interested in seeing how well the method actually generalizes to unseen MDPs, so need more detail on this part of the experiment.   I would also like to see a few additional na\u00efve baselines.  First, what is the result if you do the pre-training as specified, and then at test time you randomly sample one of the pre-trained MDPs (rather than use the identification function).  Second, what is the result if you put all the meta-train data into a single batch, train a solution with SAC, then use this as a pre-trained solution (rather than the current \u201cSAC trained from scratch\u201d), allowing more training at test time.   Both these are useful sanity checks to verify the effectiveness of the proposed approach.\n\n"}