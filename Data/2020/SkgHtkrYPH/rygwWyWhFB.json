{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Training energy-efficient deep neural networks is of critical importance to improve inference efficiency, especially on mobile and edge devices where the resource budget is highly limited. This paper presents an efficient implementation of sparse 1x1 conv kernels that leads to significant improvements in various real-world hardware settings. I believe the proposed tool would definitely encourage more research in developing more fine-grained network pruning approaches.\n \nFirst of all, I feel like the merits of this paper would be better appreciated for a computer architecture related venue. \nIt\u2019s kind of intuitive to me that fine-grained weight-matrices sparsity would lead to energy-efficiency boost with dedicated accelerators. As an outsider from a more machine-learning like community, I personally would be most interested in techniques for training block sparsity-induced networks, which, however, are largely based on previously published work.\n \nAdditional related references you might find interesting:\nYao et al., Balanced Sparsity for Efficient DNN Inference on GPU. AAAI. 2019.\nCao et al., Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity. FPGA. 2019.\n \nAgain, I feel like the results are super exciting. However, I am leaning to vote rejection as the paper lacks of ML-related contribution. \n"}