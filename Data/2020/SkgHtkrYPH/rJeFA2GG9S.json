{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper aims to make inference of convNets on mobile (and other such lower power) devices a) faster,  b) use less flops, and c) with reduced model size, d) and yet with higher overall classification accuracy. It focuses on improving improving three existing models namely MobileNet V1, MobileNet V2 and EfficientNet although the same approach can be used more generally on other models. It sparsifies the networks using the approach of (Zhu & Gupta, ICLR 2018) with a few additional details, tricks (which layers to sparsify, etc). The main new contribution is the implementation of new kernels for sparse matrix (times) dense matrix multiplication and libraries for running sparse models training with their model pruning library in TensorFlow. \n\nUnfortunately while they briefly allude to important ideas for the sparse matrix*dense matrix multiplication, much of the detail necessary to understand and reproduce this is missing from the main paper (they promise code). I suggest reducing the introduction and related work sections in order to expand on the description of their main contributions in much more details. In effect I am currently unable to comment in detail on that contribution, its novelty etc. To me, this is the main area where the paper needs improvement, but it is so critical that unfortunately it significantly impacts my scores\n\nExperimental results seem quite interesting. They show significant reduction in flops (3x) and model parameter counts (2x). Inference time also improves but to a much lesser extent (1.1-2x). While this work focuses on mobile phone or similar processing environment it would also be useful to understand and explicitly highlight impact when processing on a GPU (if nothing else, to highlight that the work does not aim to address this use case). \n\nThe experimental set up itself is quite sufficient for ICLR and shows a good amount of data and analysis to convince at least this reader about the value in their specific use setting on mobile phones. "}