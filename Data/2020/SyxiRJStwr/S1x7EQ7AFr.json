{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method to dynamically adapt some structural features of a semantic image segmentation model at inference time based on the entropy of the predictions.\n\nUsing a model that explicitly controls the size of the filters at each layer, they show that running a small number of SGD steps on the scale and final prediction parameters in the last layer to minimize the entropy of least confident predictions for a specific example leads to better performance overall, and especially better generalization when there is a size discrepancy between training and test set.\n\nStrengths: The method is inspired, and leads to significant improvements. The dynamic inference setup is clearly explained, and well motivated for the case of the scale parameters. Extensive ablation experiments and the inclusion of an oracle system help understand the contributions of each component of the setup, and the potential of inference-time optimization of the considered parameters.\n\nWeaknesses: Some information is missing from the description of the experimental setting. A quick review of the DLA model would be welcome, to get a better sense of the roles of \\theta_{scale} and \\theta_{score}. The authors should include published numbers for a relevant baseline and the current or recent state-of-the-art for the considered dataset (Table 1should also report 1x numbers in both settings). Finally, while the authors make a strong case for dynamic adaptation of the scale parameters, the prior reason for adapting \\theta_{score} is less obvious and would require further explanation.\n\nQuestions and miscellaneous remarks:\n\u201cAs reported in Table 1, our method consistently improves on the baseline by \u223c2 points for all scales\u201d > this statement is a little misleading, since the improvement is ~2 points on average, not ~2 points for each scale.\n\nWouldn\u2019t simply multiplying \\theta_{score} by a large number decrease the entropy of the predictions? Do you do anything to prevent that from happening?\n\nSimilarly, couldn\u2019t the adversary simply rotate \\theta_{score} to reduce IoU? Is the adversary optimized for long enough?\n"}