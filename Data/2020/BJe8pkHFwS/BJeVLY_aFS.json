{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a training method for graph convolution networks on large graphs. The idea is to train a full GCN on partial samples of the graph. The graph samples are computed based on the graph connectivity, and the authors propose methods for reducing the bias and variance in the training procedure. \n\nThe idea is elegant and intuitive, and the fact that the approach can work with various graph sampling methods adds to its generality. The paper is well-written and the fact that code is published is valuable.\n\nThe results on bias and variance are under the assumption that each layer independently learns an embedding. This would be clearer if added explicitly in the theorem statements (and not as part of the main text). It would be interesting to discuss how realistic this assumption is, and how large the actual bias is. Perhaps this can be measured empirically? \nNevertheless, the empirical result indeed support the claim that this simplifying assumption is enough to derive useful learning rules.\n\nOverall, I believe this is a solid contribution, and I can foresee future extensions that improve the results with more complex graph sampling methods.\n\nQuestion to the authors: I did not understand the second equality in Eq. 3. Could there be a typo?\n"}