{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the context of neural machine translation, limitations of some reinforcement learning methods, in particular REINFORCE and contrastive minimum risk training (MRT), are analyzed. The authors argue that MRT doesn't optimize the expected reward. Moreover, they show that using REINFORCE, with either realistic or dummy constant rewards, may lead to a peakier distribution. Similar BLEU scores are obtained with either type of rewards, which is an interesting and perplexing result (in my opinion). For both REINFORCE and MRT, the paper shows that unless the gold token was already amongst the most probable after pre-training, it takes many samples for it to become the most likely output, which limits the usefulness of on-policy RL approaches.\n\nI lean slightly towards rejection because the scope of the paper is somewhat too limited. The experimental section mostly covers REINFORCE without a baseline (except for validation in section 4.2, but no results are shown varying the baseline), as well as MRT in a restricted scenario. However, the analysis may still be beneficial to the community.\n\nEstablishing that minimum risk training doesn't optimize the expected reward is a valuable observation. Can the optimized and expected rewards be arbitrarily far?\n\nThe experiments demonstrating the sample inefficiency of REINFORCE (without baseline) and MRT when the best tokens have low initial probability (relative to other tokens) raise important questions about the effectiveness of these approaches.\n\nThe analysis of REINFORCE assumes non-negative rewards, which likely contributes to the peakiness effect (PKE). I would assume the peakiness effect to be mostly neutral with normalized rewards, or diminish with negative rewards (on average). It is unclear how different baselines would affect the results.\n\nFor MRT, only synthetic experiments are run. Given that the peakiness effect is much weaker than for REINFORCE, it would be useful to know results (BLEU) on the same MT task.\n\nOther questions and remarks:\n\nIn appendix A.1, the parameter value is bounded (to maintain a valid probability distribution), which is generally not the case within a neural network. Does this distinction matter?\n\nDo you have the RL learning curves? What could we learn from them?\n\nIt is unclear how much the claims generalize to other methods such as actor-critic (Bahdanau et al. An Actor-Critic Algorithm for Sequence Prediction). \n\nIn appendix A.1, some values are wrong. Do these affect the final result?\nFor example, P({a,c}) should be $2 \\theta (1-\\theta-2 \\theta^2)$.\nFor S={a,c}, \\nabla \\tilde{R} should be $\\frac{1+2 \\theta^2}{2(1-2 \\theta^2)^2}$.\nFor S={b,c} \\nabla \\tilde{R} should be $\\frac{\\theta (\\theta-2)}{(1-\\theta)^2}$.\n"}