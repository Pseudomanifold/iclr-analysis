{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to use deep RL to learn a policy for communication in the parameter-server setup of distributed training. From the perspective, the problem formulation is a nice contribution. \n\nWhile it is a reasonable idea and the initial results are promising, the lack of an evaluation on a real cluster, or for training more computationally-demanding models, is limiting. I fully appreciate the need to perform experiments in a controlled environment, such as the ones reported in the paper. These are useful to validate the idea and explore its potential limitations. However, to truly validate such an idea completely it is also necessary to implement it and run it \"in the wild\" on an actual distributed system. From my experience, although performing such experiments is certainly more involved and challenging, there can also be significant differences in the outcomes when one goes to such an implementation. Normally these are due to discrepancies between the assumed/simulated model, and real system behavior.\n\nIs it clear that deep RL is needed for this application, as opposed to more traditional RL approaches (either tabular, with suitably quantized actions, or a simpler form of function approximation? And to ask in the other direction, did you consider using a more complex policy architecture, e.g., involving an LSTM or other recurrent unit?\n\n"}