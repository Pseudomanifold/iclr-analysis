{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Background disclaimer: I work in RL research for quite an amount of time, but I do not know much about the domain of distributed systems. For this reason, I may not know the details of technical terms, and I might not be the best person to review this work (when compared with the literature in this field). Nevertheless, below I try to give my evaluation based on reading the paper. \n\n====================\nIn this work, the authors applied value-based reinforcement learning to learn an optimal policy for global parameter tuning in the parameter server (PS) that trains machine learning models in a distributed way using  stochastic gradient descent. Example parameters include SGD hyper-parameters (such as learning rate) and system-level parameters. Immediate cost is to minimize the training time of the SGD algorithm, and i believe the states are the server/worker parameters. From the RL perspective, the algorithm used here is a standard DQN with discrete actions (choices of parameters). But in general I am puzzled why the action space is discrete instead of continuous, if the actions are the hyper-parameters. State transition wise, I am not sure if the states follow an action-dependent MDP transition, and therefore at this point I am not sure if DQN is the best algorithm for this applications (versus bandits/combinatorial bandits).  While it is impressive to see that RL beats many of the SOTA baselines for parameter tuning, I also find that instead of using data in the real system to do RL training, the paper proposes generating \"simulation\" data by training a separate DNN. I wonder how the performance would differ if the RL policy is trained on the batch real data.\n\n"}