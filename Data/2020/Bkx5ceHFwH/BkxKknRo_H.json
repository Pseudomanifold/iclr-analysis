{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nPaper Summary: The paper addresses the problem of Room Navigation, where the goal is to navigate to a room that is provided as target using language. The paper tries to incorporate common-sense knowledge and semantic understanding (room detection) via a set of auxiliary tasks. The model has been trained using three techniques: (1) imitation learning, (2) imitation learning + reinforcement learning and (3) self-supervised imitation learning.\n\n* Originality:\n\nThe idea of using detectors and prior object-room or room-room knowledge for navigation has been around for a while (for example, Mousavian et al., ECCVW 2018, Yang et al. ICLR 2019, etc). There is not much novelty in the model or the training strategies either, and standard techniques have been used.\n\n* Quality:\n\n- There is no comparison with any other method in the paper. The authors mention \"we do not know of any other work related to RoomNav on MatterPort3D environment and therefore we could not directly compare against previous work\". There are several navigation papers with released code. They could have applied those methods on this dataset.\n\n- There are some claims that cannot be justified. For example, it is mentioned that \"These approximate maps obtained from embeddings can be an alternative to SLAM\". The structure of the environment such as connectivity of the rooms is obtained by SLAM, while these embeddings cannot provide such information. They are just locations of some rooms in the training set.\n\n* Clarity:\n\n- Some details are missing. For example, it is not clear how the questions is generated in SU_PN task or what the parameters are for RL training.\n\n- It is hard to read the information from Figure 3. Also, standard deviation should be provided for RL experiments.\n\n- In Table 1, what is the difference between the bottom two rows and SIL baselines above it?\n\n- There is no additional information provided by SU_RD during self-supervision as room detection is already included in the model. Why should it improve the performance?\n\n* Significance:\n\n- A number of auxiliary tasks have been introduced, but for each experiment a different subset of them is being used due to inconsistency of the results. This shows that proposed auxiliary tasks are not really helpful in most scenarios. It is mentioned that \"when several auxiliary tasks are introduced to the agent, the performance generally degrades as the agent tends to focus more on the auxiliary tasks\". What is the point of the paper then? In some cases even adding one auxiliary task makes the performance worse. For example, it is strange that CS_nxt is much better than CS_nxt+CS_RS.\n\n- When the method is trained using imitation learning+RL, the performance drops significantly (from > 0.15 to less than 0.07). What is the advantage of using RL?"}