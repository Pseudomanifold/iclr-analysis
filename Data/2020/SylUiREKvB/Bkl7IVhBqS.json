{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper the authors propose an architecture based on variational autoencoders and hyper-networks. The basic idea is that the weights of the underlying RNN/autoencoder are not fixed, but are coming from another RNN/feed-forward network which captures the underlying dynamics and adjusts the weights accordingly. The experimental results show the benefit of the model compared to a similar method without hypernets.\n \nIn terms of novelty, the combination of auto-encoder RNNs and hyper-networks is not entirely novel and it has previously been developed (https://www.biorxiv.org/content/10.1101/658252v1). However, while I think these previous works should be discussed in the paper (they are not currently), the two architectures are sufficiently different and the current work is novel enough in my opinion. On the other hand, in terms of presentation, I think the paper can be improved. The architecture is not entirely clear from the text. I think a graph showing the architecture of the model would be very helpful here. The notations also seem loosely defined (what is the dimensionality of x_t, z_t, etc.) and sometimes undefined (e.g., x_t in equation 1 is not defined).  \n \nIn terms of model architecture, it wasn\u2019t clear for me why the hyper-network for \\phi is feedforward but the one for \\theta is RNN?\n \nThe experiments seem promising, but I have the following questions before being able to assess the results:\n \n- How many LSTM units are in the model in each experiment? Are they similar in both VHRNN and VRNN?\n \n- What is the structure of encoder/decoder layers? How many units in each layer? Are they the same for VHRNN and VRNN?\n \n-There are four sets of weights in the primary model: weights of RNN, dec, enc, prior. How are these weights generated by the two hyper-networks \\theta and w?\n \n- How is the number of parameters in the experiment calculated. Do they refer to the number of parameters in the hypernetworks only?\n \n \nMinor:\nI suspect the spaces between the equations and captions are also manually changed which has made the paper physically dense and a bit unreadable (see equation 4 for example). Similarly, the space between the caption of Figure 1 and the text after seems too small.\n \n\n"}