{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes the variational hyper RNN (VHRNN), which extends the previous variational RNN (VRNN) by learning the parameters of RNN using a hyper RNN. VRHNN is tested and compared with VRNN on synthetic and real datasets. The authors report superior performance parameter efficiency over VRNN.\n\nThe performance of VHRNN is promising and certainly better than the previous VRNN for some applications. However, the VHRNN is constructed by a straight-forward combination of existing techniques and hence the technical contribution of this paper is marginal.\n\nAlthough Section 4 is entitled as systematic generalization analysis of VHRNN, the reported results are only for the specific structures of VHRNN and VRNN. Isn\u2019t it useless to present results for the VRNN with a latent dimension of 4, at least as a sanity check? \n\nFig. 2 and the texts referring to it discuss the KL divergence between the prior and the variational posterior. While the FIBO is mainly used as the objective in this paper, is the ELBO enough if the authors care the simultaneous low reconstruction error and low KL divergence?\n\nIt is unclear and explained little if the comparison using parameter count is fair for VHRNN and VRNN since they have different structures.\n\nIt would be nicer to discuss for which kind of time-series VRNN is enough.\n\nMinor comments:\nThe caption of Figure 1 is too close to the main texts.\nEq. (4) is overlapping with texts.\nCan the equations at the bottom of p.3 be explained with an illustration?\n"}