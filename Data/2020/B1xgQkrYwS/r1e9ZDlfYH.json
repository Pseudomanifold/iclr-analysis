{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "There are major problems with this paper. It is concerned with the examination of pruning experiments for a LeNet on the MNIST dataset.  I fail to see how anything useful can be derived from this, as MNIST is a completely trivial dataset and LeNet is a very old, small architecture which does not at all resemble the massive overparameterised models that we care about.\n\nFrom a narrative perspective, I am not sure what the key point is, what should the reader take home? What should they take account of when performing network pruning?\n\nIn terms of presentation, some of the figures are unreadable (figure 4). Figure 15 looks like noise. The writing is good however, if a bit grandiloquent.\n\nI dislike writing short reviews, but I fear this paper falls too far short of ICLR standard.\n\nPros:\n- Well written\n\nCons:\n- Experiments are weak\n- Unclear narrative; what's the one key message?\n\nI have to give this paper a reject as the experiments conducted are far too weak, and there is little evidence anything found here will, say, generalise to a ResNet/DenseNet on ImageNet.\n\n"}