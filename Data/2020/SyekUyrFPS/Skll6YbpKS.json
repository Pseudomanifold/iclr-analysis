{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a mechanism to construct an automatic text generator to produce biased text. The biased generator has such an effect that it would incorrectly associate a particular label-agnostic feature with the biased target label. The author also conducted experiments on existing SOTA models (BERT etc): namely training these models with the biased dataset to see the impact of learning from biased data.\n\nThis paper requires some clarifications and hopefully additional experiment results. I am leaning towards rejecting the paper for below reasons.\n\nThere are apparently two parts of this paper. The first part introduced a text generator (CARA). The second part consumes the produced text from CARA. Before consuming the output from this newly introduced text generator, should we evaluate the text generator itself by some metrics? We know that text generator evaluation is a challenging topic, but I expect some common criteria like \"fluency or readability\" and \"accuracy or relevance\" are measured somehow either by human judgement or automatic evaluation metrics. I understand that evaluation of the machine generated sentences is a difficult task , but without any evaluation the quality and correctness of the generated text cannot be justified. Furthermore, since this text generator is supposed to generate a biased dataset, should we also define \"bias\" and measure quantitively how well it accomplish this task (the amount of biasedness in the generated text)? \n\nIf the author could show that the text generator as a stand-alone module could approve its own value using some evaluation on the above two criteria i) basic text generation quality 2) biasedness evaluation , the proposed method would be well justified.\n\ntypos, nitpick\n* last paragraph in section 1 \"Our approach paves the wave for ...\". I guess you mean \"way\" instead of \"wave\".\n"}