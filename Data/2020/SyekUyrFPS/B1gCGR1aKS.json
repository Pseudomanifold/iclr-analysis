{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the timely and important problem of 'bias' in datasets, and how these affect neural NLP models. The authors point out that at present it is difficult to undertake systematic study of biases and the degree to which models exploit these. To address this, they propose a method for generating (synthetic) \"biased\" datasets in a controllable fashion, via a generative model. As far as I know, this --- the idea of learning to generate 'biased' data to benchmark models resistance to them --- is a novel idea, and one with a fair amount of potential if the aim can be realized. However, I am not convinced that the specific method the authors have proposed here encodes or instantiates 'bias' in a meaningful way. \n\nIn particular, the authors do not actually define 'bias' in the first place. A formal definition of this seems necessary to advance the agenda of generating biased datasets, but the nearest thing provided is the Eq in Section 3; but this relies strongly on the assumption that $\\delta$ is a label-agnostic feature. Perhaps I am missing something here, but as I understand the method entails manipulating a subset of hidden representations for instances $z$ via $T$ (yielding $z'$) and associating these with the bias target class. Consequently, the decoder will generate a reconstruction $x'_b$. \n\nThe authors report that if we do not add the reconstructed (synthetic) train instances to the train set, the model classifies them `correctly', i.e., as per the original designation. However, if we do train on these, the model will learn to associate the $\\delta$ signature (however it is instantiated by the decoder) with the label samples have been changed to. Should we find this surprising? It seems to me both obvious and even desirable that a model would pick up on such a strong cue, if present (and it is indeed strong by construction, as per 4.1). The other result --- that models \"correctly\" classify inscribed examples if they are not seen during training --- likely reflects the smoothness objective imposed by CARA, so is also not terribly surprising. \n\nThese issues aside: Do we have any reason to believe that the $\\delta$-inscribed texts resemble the kinds of biases present in real data? Looking at Tables 1 and 2 do not obviously suggest so. What I would guess is that the $T$ manipulation in the latent space yields systematic patterns that the authors then explicitly and strongly associate with labels; again I am left wondering what it is the models \"should\" do in such cases? It seems that it would only be reasonable to hope the model would avoid using such signatures if it \"knew\" that they reflect sensitive attributes; otherwise, it is simply doing what the objective says to do --- minimize error. I do think it is interesting that model size does not seem to directly affect the propensity for finding these cues. \n\nMore generally, my feeling is that this is a truly interesting direction, but I think for it to be a compelling approach the authors need to formally define bias (the sort they are trying to simulate) and then convincingly show that generated samples reflect the sort of biases that plague real-world datasets. I realize this is no small ask, but with the current setup I am just not sure what I can take away from this other than: If you systematically introduce input patterns that are strongly correlated with target labels, neural networks will pick up on these. Without further assumptions or evidence, I don't know that this tells us much about 'bias' specifically.\n\n"}