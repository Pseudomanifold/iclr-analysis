{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary\n========\nThis paper proposes a method to generated biased datasets for NLP. The method relies on a conditional adversarially regularized autoencoder (CARA), which is similar to the original adversarially-regularized auto-encoder (ARAE), but in addition is conditioned either on a label (in text classification) or on a label and another sentence (in natural language inference). The latent variable in this CARA model is also transformed by adding to it a label-specific value that represents the bias in the latent space. Then the generated sentences are presumably biased. \nThe generated texts are then used for training classifiers based on several pre-trained model, and it is shown that the learned classifiers are highly biased, while classifiers trained on the original datasets are much less biased. \n\n\nThe paper presents an interesting approach for studying the problem of bias in NLP datasets by generating biased datasets. However, the empirical evaluation is lacking in terms of text generation quality and classification quality. There are also unclear points in the methodology. Finally, the motivation may be more clearly explained. Please see the main comments below. \n\n\nMain comments:\n==============\n1. Motivation: the motivation to the work is not entirely clear to me. I agree that studying model robustness against bias is importnat. However, I'm not convinced by the desideratum for generating \"objectively biased samples that explicitly associate features to labels, regardless of label semantics.\" This may be far from the actual bias scenarios that occur in practice, and thus less practical. On the other hand, there may be an ethical concern with generating datasets that are biased w.r.t sensitive attributes. So there's a bit of a conundrum here, which is worth thinking more about. \n2. Generation quality: in work on language generation, the quality of the generated language needs to be properly evaluated. This typically means human evaluation for criteria like coherence, diversity, or faithfullness/consistency, depending on the scenario. At the very least, some proxy measures of quality should be reported, for instance perplexity for coherence and some similarity measure for consistency. This limitation is especially concerning given that the ARAE often produce text that is not of high quality. \n3. Bias triggers: I was not able to understand the bias trigger synthesis in sec 4 and 4.1. It's reasonable to transform z to contain label-specific information, so I can understand if z were to change to be far from P_base and closer to P_target. But then, 4.1 maximizes the distance from examples x ~ P_target, which means that delta is actually farther away from P_target. \n4. In general, I found it hard to follow the methodology in sections 3+4. On the one hand, there's redundancy between themse sections and subsections. On the other hand, the references to the algorithms are incomplete and misplaced, only referring to certain lines and after having described other parts. \n5. Classification quality: the quantitative results show how biased classifiers are, but not how good they perform. This is partly related to point (2) from above, but also can be assessed separately. What is the accuracy of classifiers trained on the generated/clean, when evaluated on generated/clean data? \n\n\nOther comments: \n===============\n- Related work: there is in fact at least one other work on a conditional ARAE:\nGu et al., 2019. DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\n- As a matter of presentation, it might be better to present the model with the simpler text classification case, and later explain how to extend to the sentence-pair case. \n- The various text examples should be described in more detail, to explain what they mean to convey.  \n- Do you have any explanation for the differences between the behavior of different models noted in the end of 5.1. \n\n\nPhrasing, grammar, etc.\n=====================\n- The abstract is very vague and generic, and can be made more concrete. \n- Abstract: myriad ways -> a myriad ways \n- 1: consider (Sap et al., 2019) -> consider Sap et al. (2019)\n- 3: may leads -> may lead; holdout -> heldout; is controllable -> is a controllable\n- 4: Algorithm 2 show -> shows\n- When referring to tables, figures, etc. in the appendix from the main text, indicate that they are in the appendix. \n"}