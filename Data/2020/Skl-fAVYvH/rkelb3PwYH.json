{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The high level aim of this work is to produce a model capable of one-shot exploration, without using many (or even any) demonstrations to train this model.  One shot imitation systems often consist of an encoder which observes and embeds the demonstration and a decoder that can produce the relevant movement, when conditioned on the encoded demonstration.  The authors propose that to train a model of this general form.  And instead of many demonstrations, they propose that it should be possible to learn from self-generated exploration data.  Then, as a test, a single demonstration, not used in training, can be presented to the system and it can imitate.\n\nMy most significant concerns have to do with the fact that this approach can essentially only work well if the self-supervised training of the one-shot system is performed using data that resembles what the demonstration will look like.  This means that the exploration data used to train the one-shot model must be structured and generally sensible.  If the exploration data is jittery movement that does not cover the space of behaviors one might encounter in demonstrations, it is unlikely to work.  By validating primarily on inherently stable domains where the controlled element is attached (i.e., a robot arm rather than a freely moving robot), this concern is likely masked to some extent, but it still seems fundamental is an issue in scaling this approach to other systems and possibly to more complex behaviors.\n\nUltimately, the very high-level motivation for undertaking this work is fairly reasonable, and the narrow proposed algorithm (train a one-shot imitation system using exploration data) comes through, but it is not presented very well, and I think what precisely the authors are aiming for relative to other approaches is actually a bit confused.  The opening remarks are not entirely accurate.  The comparison of imitation learning results versus RL blurs over many distinctions.  For example, approaches such as GAIL (cited after the first sentence) are efficient with respect to number of demonstrations, but require similar amount of exploration experience as RL approaches.  More broadly, this kind of clumsiness in describing what the aim is, especially relative to other work suggests a somewhat unclear focus on the part of the authors.  I will elaborate on a few points of confusion further:\n\n1) Though written as a single integrated training setting, it is unclear why this algorithm isn't presented as two separate processes.  The part of the system that learns to produce diverse trajectories is, as far as I understand, separate from the part of the system that is used for imitation.  Couldn't one entirely train the exploration policy to produce diverse samples, then log data from that policy, and then separately train the one-shot imitation system?  Or are trajectories that are accumulated in the buffer during training important for successfully training the one-shot imitation network?  \n\n2) The baselines are a bit confusing.  BC is a different kind of baseline since for one-shot imitation, one sees a demonstration, usually without actions and simply wants to reproduce that movement, whereas for BC, a policy is produced that should do exactly what the actions provided in the single demonstration indicate.  The BC baseline is not clearly described and it is unclear if the proposed architecture is being used or if a single policy trained on just that one demonstration is being produced?  Anyway, it should be made clear what the setting is and what the baselines are aiming to compare.  For example, comparisons might change the way that exploration data for training the one-shot imitator is generated, while keeping the architecture the same.  Alternatively, comparisons might consider very different approaches that use only a single trajectory (in different training settings).  There are other possibilities.  The current comparisons are a bit poorly motivated and reductive because they seem to be claimed to be generic imitation learning comparisons, but they confound the training setting, the exploration approach, and other factors.  Minimally I would be happier if it were clear what elements of the authors method the baselines are supposed to highlight.  \n\n3) In terms of writing style, section 5 (and the supplement) is seemingly written as an arbitrary set of questions that a pre-submission reader might have had.  While these questions are reasonable, I think the fact that the authors needed to address so many questions one-by-one reflects the fact that the earlier presentation was not very clear.  Enumerating a bunch of specific questions that readers have brought up and attempting to rebut them one-by-one in the text is not a very effective way of addressing the core source of confusion.  The underlying problem is that the authors should do a better job at making a positive case earlier in the paper for what they are doing and how it relates to other work. \n\n4) Other comparisons are also a bit confusing.  For example, in one of the ablations, if there is no encoding provided to the imitator network, how does it know what to imitate at all?\n\nOverall, I feel that the authors took a stab at developing an algorithm without a clear sense of what they were aiming to do, besides the very high-level idea of only using a single demonstration (they state: \"Can we develop an effective one shot imitation learning algorithm that assumes access to only a single expert demonstration?\").  I believe this problem should be a bit more precisely thought through, because there are distinct settings in which one demonstration could be used.  For example, if the initial state is always the same, then a planner can solve the problem of approximately tracking that demonstration.  If the initial condition is always the same, but the aim is for there to be some robustness to noise, then build a robust tracking controller.  If initial conditions vary, only a single demonstration is available, and the aim is for the resulting policy to generalize a little bit from that, GAIL or IRL+RL may work in certain of these settings (but not in a way that is data efficient with respect to exploration experience).  For other precise settings, there is other work that attempts to address similar issues.  \n\nInstead, I would urge the authors to frame their problem more precisely, comparing only among approaches that leverage exploration data as a proxy distribution in the hope training a one-shot imitation system that can generalize to unseen examples (including a single demonstration).  This really seems to be the more narrow problem they are interested in.  Furthermore, the authors should be upfront and address the distribution shift issue (between the exploration data used to train the imitator and the demonstration data) and how to deal with it, or explain why it isn't an issue.\n"}