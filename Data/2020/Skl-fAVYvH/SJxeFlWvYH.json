{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\n**Summary: \nThe paper proposes a one-shot IL method that aims to perform quick inference from one expert demonstration without training with the expert demonstration. The main idea of the paper is to learn an encoder that encodes exploration demonstrations into contexts and learn a context-dependent policy that imitates the exploration demonstrations. To obtain the exploration demonstrations, the proposed method trains an RL agent where the reward function encourages diversity of the collected demonstrations. At inference time, a given expert demonstration is encoded into a context and the context-dependent policy is executed to imitate the given expert demonstration. The proposed method is experimentally evaluated on continuous control tasks. Extensive ablation studies are presented.  \n\n**Score:\nThe paper proposes an interesting idea for one-shot IL. However, there are several issues, including the experimental setting and clarity. These issues should be addressed to meet the acceptance bar of ICLR. I vote for rejection. \n\n**Detailed comments:\n- The experimental setting is not suitable for evaluating one-shot IL. \nThe paper uses the standard IL experimental setting with a demonstration from an expert who maximizes the ground-truth reward. This experimental setting does not show that the proposed method can imitate different kinds of expert demonstrations, which is the main advantage of one-shot IL methods. In my opinion, a more suitable experimental setting is to evaluate the proposed method using different experts who achieve different goals (e.g., a forward hopping demonstration and a backward hopping demonstration in the Hopper task).\n\nIn addition, the experiments should evaluate the sample complexity of the proposed method (i.e., the total number of unsupervised trajectories generated by the exploration policy). I think this is very important, since they proposed method might require a very large number of unsupervised trajectories, which is not desirable in practice. \n\n- The mathematical details of the proposed method are not properly written and explained. \nIn Eq. (1), the expectation is taken over the distribution of action a_t, but a_t does not appear in the equation. In addition, the policy pi is deterministic, and thus the expectation is not really required. Also, the definition of expected sum of discounted reward in Eq. (1) is different from that in Section 3.1 which is an expectation over a distribution of state sequences.\n\nThe paper claims that Eq. (2) is an alternative form of Eq. (1). Does this mean they are equivalent? I could not find a derivation from Eq. (1) to Eq. (2) in the paper or appendix. \n\nThe paper motivates using the Gaussian factor model based on the global task prior, but the paper does not explain what is the task prior and why it is useful in one-shot IL. \n\nIn Eq. (6), what is the index j indicating? Does it indicate subsets of trajectories in the replay buffer or other states in the same trajectory? Is this the same j in Eq. (7) and Eq. (8)?\n\nThere is a loss L in line 9 of Algorithm 1. Is this a typo or there is another loss function? \n\n- Clarity of experimental results and analysis. \nThe paper does not provide any explanation or summary of the primary results in Section 5.2. This lack of explanation can lead to different interpretations depending on readers. For example, readers may take the primary results as negative since the proposed method successfully imitates experts only in 2 out of 6 tasks. I suggest the authors to provide a summary of the experimental results that supports the main claim of the paper. \n\nThe performance of the proposed method highly depends on diversity of unsupervised demonstrations. The paper should evaluate diversity of these demonstrations in the experiments.\n\n**Minor comments and suggestions:\n-- The paper uses many inconsistent wordings. For example, \u201cEq\u201d, \u201cequation\u201d, and \u201cEquation\u201d. \u201ceuclidean\u201d and \u201cEuclidean\u201d.  Also, there is an obvious typo \u201cequation X\u201d.\n\n-- The paper can be improved by moving parts of Section 5.3 into the primary results. For instance, results of a policy without an encoder (Section 5.3.1) and GAIL (Section 5.3.3) could be presented as the primary results. This would make the contents more compact and easier to follow. \n\n-- Section 5.3.1 mentions about validation loss. How do you obtain the validation dataset? Also, Section 5.3 should present results on all environments (possibly in appendix) instead of just one selected environment. \n\n"}