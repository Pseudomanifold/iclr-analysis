{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the problem of one-shot imitation learning. The aim is to learn a policy which, given a single expert demonstration, can mimic the expert's behavior. Making progress on this problem is important for decreasing the amount of human supervision required to teach intelligent agents. This paper proposes to solve this problem by generating a bunch of trajectories, and then performing trajectory-conditioned behavior cloning on these trajectories. Internally, the policy encodes each trajectory as a (stochastic) random variable. The self-generated trajectories for this procedure are created by solving an auxiliary RL problem with some intrinsic motivation reward (a few variants are considered). The paper compares the proposed method to a few baselines on three robotic control tasks. The proposed method outperforms baselines on 3/6 tasks. The ablation experiments are quite extensive and answer a number of questions about the method.\n\nI am leaning towards rejecting this paper. My main reservation is that the proposed method is quite similar to two classes of prior work. First, methods for unsupervised skill learning [Variational Intrinsic Control, Self-Consistent Autoencoder, Diversity is All You Need, Variational Option Discovery Algorithms] learn to encode trajectories (or some function of a trajectory) and learn an encoding-conditioned policy that attempts to replicate the trajectory (see, e.g., Section 4.2.3 in Diversity is All You Need). Importantly, these methods are not just techniques for collecting diverse data, but provide a single algorithm for learning the encoder and policy. A second class of related work is goal-conditioned RL [e.g., Hindsight Experience Replay, Temporal Difference Models, Visual RL with Imagined Goals]. Why these methods only reach a particular state and do not do trajectory imitation, I suspect they would perform quite well on most of the tasks considered.\n\nOverall, I think the paper is on the right track to solving an important problem. I would consider increasing my review if comparisons with the aforementioned prior work were added, and if the experiments were clarified.\n\n\nMinor Feedback\n* What is the difference between the baselines and the proposed method?\n* \"wherein agents routinely require millions or even billions of data points.\" This critique seems to conflate data points of expert supervision with data points of environment interaction. I think the implicit assumption here is that expert supervision is expensive, but environment interaction is cheap. I'd recommend clarifying this.\n* \"simply kicking the can down the road.\" I didn't understand this sentence.\n* \"expected sum of discounted reward\" I think the subscript s_2 should be s_1\n* \"If a second expert demonstration exists\u2026\" I didn't understand this approach. The experiment in Fig 8 seems to use a different method for combining expert demonstrations.\n* Section 5.3: Why does each ablation use a different environment?\n* \"[Ghasemipour 20]\" I think this paper is actually a NeurIPS 2019 paper, not a CORAL 2020 paper."}