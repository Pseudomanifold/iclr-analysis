{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "(1) Contributions:\nThe paper proposes an algorithm for training quantized Bayesian neural networks (BQN). The method optimizes a lower bound to the ELBO that can be analytically computed for BQNs.\n\nThe paper shows that propagation in BQNs is a sequence of tensor contractions, for which there exist fast and efficient approximate algorithms.\n\nBQNs show good performance on set of image classification benchmarks (MNIST, KMNIST, Fashion MNIST).\n\n(2) Originality:\nI am not aware of any prior works that attempts to do inference in quantized networks. The idea of BQNs and propagation as tensor contractions is interesting and novel. It is a cool result that in quantized networks, one can exactly calculate the distribution of the activations (h^(l)) and therefore get an accurate estimate of the ELBO.\n\nIt is also interesting that this algorithm can be implemented as a series of tensor contractions. This leads to a fast and efficient algorithm for training.\n\nRegarding the novelty in the lower bound to the ELBO, variational Bayes makes the same approximation by sampling the layer activations (h^(1) .. h^(L-1)). In VB, when we sample the layer activations, we approximate the logarithm of the expectation by the logarithm of a Monte Carlo sample, which is a lower bound due to Jensen\u2019s inequality. The only difference here is that this approximation is only done in the final hidden layer instead of every layer.\n\n(2) Clarity:\nI want to point out some issues with the writing.\n\nOverall, I believe that the number of equations and theorems hinders the readability of the paper. It is very confusing to try and navigate the various notations and assess the significance of each result. I would recommend including a few key equations and theorems in the paper and leaving the rest to the appendix. The ideas and concepts in the paper could be easily communicated in writing.\n\nI do not think this paper should be 9.5 pages. The work could be comfortably explained in the standard 8 pages. Namely, in Section 3.1 and 3.2, it is unnecessary to provide analytic gradients for the expressions (in the age of automatic differentiation). Section 4.3 should be left to the appendix. I found this section very confusing and it does not seem to be important in the narrative.\n\n(4) Significance:\nI think this work is significant, although I felt that the experiments section could have included more regression benchmarks as well as more difficult datasets such as CIFAR10.\n\n(5) Impact:\nThis work would certainly be interesting to researchers and anyone working on quantized neural networks.\n\nOverall assessment:\nI think this paper presents interesting and novel ideas, but it has shortcomings in the presentation."}