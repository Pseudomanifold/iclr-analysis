{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a set of efficient algorithms for learning and prediction in Bayesian quantized networks, which allows for differentiable learning without the need to sampling. From this point of view, this work targets at solving a real and interesting problem in BNN. I am not an expert in this area, and I can only comment on broad picture.\n\nSome questions:\n-\tThe proposed method is claimed to work on large-scale datasets. However, all datasets selected in experiment are black and white images with low resolution (28 x 28 ?). A \u2018real\u2019 large-scale dataset is expected. The current experiment results are far from sufficient.\n-\tThe competitive methods are limited. If the author claims BNN in general can have well-calibrated uncertainties, non-BNN methods should also be considered. Further, when the data has heavy tail or is not Gaussian-distributed, will the model still give well calibrated results?\n-\tDoes this proposed sampling free BQN have faster computational speed than standard BNN? If yes, it is also desired to have a comparison.\n-\tI am totally lost at section 4.3. It seems to give a fast way of computing Eq.(12). A general introduction at the beginning or the end of section 4.3 could be really helpful. Further, I\u2019m not clear why the approximation made in the model is good enough. A pseudo-algorithm is desired for clarification.  \n"}