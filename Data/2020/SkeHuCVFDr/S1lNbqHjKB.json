{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes BERTScore, a method for automatic evaluation of text. Their method uses BERT to produce contextualized word representations for the words in the reference and hypothesis. Then they compute the precision, recall, and F1 by greedily matching up words between the hypothesis and reference. To be more specific, for say recall they take each word in the reference and compute the cosine with all words in the hypothesis. Then they add up the largest cosine similarity for each word and average them together. Precision is defined similarly but with the roles of hypothesis and reference switched. F1 is then the harmonic mean of these two scores. They also experiment with using idf to weight importance.\n\nTheir method is simple, but achieves very strong results and there are a ton of experiments in this paper (it is 41 pages). The focus is largely on metrics for MT, but they also evaluate on image captioning. The paper is also very thorough and many of the questions I had when reading it are answered (like effect of optimal matching, running time, etc.). The latter (running time) being one of the downsides of the method if it was to be used for fine-tuning MT systems. 40 times slower than BLEU, but I think this increased cost would be worth it and could be engineered around.\n\nOverall, I like the paper - it is simple and effective on its goal task of automatic evaluation for text generation. I think we are moving that way as a field and this paper proposes a useful method and is additionally a good study on the subject.\n\nA question I have is why the method doesn't perform well in certain cases. For instance, in Table 2 and 3 - some of the evaluations with tr and fi fall well below relative performance for other language pairs. Does this have to do with the quality of the representations in multilingual BERT? What is YiSi-1 doing, for instance for model selection of en-fi and en-tr that makes it have so much better performance?\n\nEdit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus. I think it would make the most sense to compute these from the training data for the underlying BERT models. Since BERT itself is a function of this training data, it seems appropriate that these values would be as well (or perhaps at least a subset of this data).\n\nMissing citations:\nA citation to \"Beyond BLEU:Training Neural Machine Translation with Semantic Similarity\" from ACL 2019 should be incorporated into the related work. They use semantic similarity to fine-tune NMT systems with their own embedding-based (semantic similarity) metric and they found some nice properties from training in this way. Have you tried BERTScore on sentence similarity tasks? It's possible BERTScore could have strong performance and some readers may wonder this. There are evaluations on PAWS for paraphrase detection which I appreciated, but that is a little different.\n\nA citation to \"Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization\" from EMNLP 2019 should also be incorporated. This paper is a big boon to BERT score showing that it is a very helpful metric for fine-tuning summarization systems. They don't even need a cross-entropy term since BERTScore captures fluency so well. I'd like to see it for MT as well, but perhaps that is the next paper.\n\nTypos:\nThe word \"language\" is misspelled twice in Appendix E."}