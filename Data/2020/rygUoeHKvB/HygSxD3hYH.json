{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "*Summary*\nThe paper addresses the challenge of intrinsically-driven exploration in tasks with sparse or delayed rewards. First, the authors try to bridge the gap between the objectives of intrinsically-motivated goal generation and maximum state entropy exploration. Then, they propose a new exploration method, called novelty-pursuit, that prescribes the following receipt: first, reach the exploration boundary through a goal-conditioned policy, then take random actions to explore novel states. Finally, the authors compare their approach to a curiosity-driven method based on Random Network Distillation in a wide range of experiments: from toy domains to continuous control, to hard-exploration video games.\n\nI think that the paper displays some appealing empirical and methodological contributions, but it is not sufficiently theoretically grounded. For this reason, I would vote for rejection. I would advise the authors to rephrase their work as a primarily empirical contribution, in order to emphasize the merits of their method over a lacking theoretical analysis.\n\n*Detailed Comments*\n\n*Major Concern*\nMy major concern is about the claim that goal-conditioned exploration towards the least visited state would, at the same time, maximize the entropy of the state distribution. The derivations seem technically sound, but I think that the underlying assumption is unreasonable in this context: it neglects the influence of the trajectory to reach the target state, which is rather crucial in reinforcement learning instead. It is quite easy to design a counter-example in which the (optimal) goal-conditioned policy towards the least visited state actually decreases the overall entropy of the state distribution. One could avoid the issue by assuming to have access to a generative model over the states, but that would fairly limit the applicability of the approach.\n\n*Other Concerns and Typos*\n- I think that the authors minimize the relation between their methodology and the one proposed in (Ecoffet et al., 2019). It is true that the applicability of Go-Explore is quite limited. However, the idea behind their approach, which is based on first reaching an already visited state and then exploring randomly from that state, is not all dissimilar from the two-phase exploration scheme of novelty-pursuit.\n- It is not completely clear to me how the disentanglement between exploration and exploitation works in the novelty-pursuit algorithm.\n- What is the vanilla policy considered in the experiments?\n- Section 4.2, after equation 3: rewarding shaping -> reward shaping\n- section 5.4: we consider the SuperMarioBros environments, which is very hard ecc. -> we consider the SuperMarioBros environments, in which it is very hard ecc.\n"}