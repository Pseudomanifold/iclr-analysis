{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors study the problem of exploration in deep reinforcement learning. The authors borrow the ideas developed in the intrinsically motivated goal exploration process, and entropy maximization and propose a method on Noverly-Pursuit. The authors then empirically study the proposed method. \n\n1) The authors investigate an important problem and I would appreciate the authors if they could motivate its importance more in their work.\n\n2) In the second paragraph, the author mentioned that goal-conditioned exploration behaviours can maximize entropy. Later in the same paragraph, they claim that \"The exploration policy leads\nto maximize the state entropy on the whole state distribution considering tabular MDP\". I guess the authors' point was this approach might increase the entropy rather than maximizing it. If the claim is, in fact, maximization, a reference would be helpful. If the authors prove it in this paper, implying it in this paragraph is also helpful. \n\n3) In the background section, the authors did not specify whether they provide background on tabular MDP or beyond that. By calling the transition kernel the state transition probabilities, it seems they introduced a tabular MDP, but a more concrete introduction and preliminaries would help to follow the paper.\n\n3) The first paragraph of section 2, the author mentioned that\n\"The target of reinforcement learning is to maximize the expected discounted return\". I hope the authors mean\"one of the targets in the study of reinforcement learning ...\"\n\n4) in the same paragraph, why the \\gamma = 0 is excluded? is there any specific reason? Also, when the authors include the gamma = 1, do they make sure the maximization in line 9 of the same paragraph is well defined in regular cases?\n\n5) Regarding the experiment in figure 2. It would be useful to the readers if the authors provide more details about this experimental study.\n\n6) In the few paragraphs below Figure 2, it would be nicer if the authors provide a clear definition of each term. In order to follow the paper, I relied on my imperfect inference to infer the definitions. Also, I find it probably useful to distinguish the random variables and their realizations in the notation.\n\n6) Regarding the theorem1. I would recommend making the statement more transparent and more clear. I also recommend to even not calling it a theorem since it, as mentioned, is as clear as the definitions. Also, arent x_t(i)s non-negative by definition? \n\n7) In this sentence:\n\"However, we don\u2019t know what non-visited states are and\nwhere non-visited states locate in practice since we can\u2019t access ...\"\nI think the authors' point was that \"we might not have access to it in general\".\n\n8) It would be helpful to me to evaluate this paper if the authors explain more how the following statements go through:\n\"To deal with this problem, we assume that state density over the whole state space is continuous, thus visited states and non-visited states are close\". I am not sure how \"thus visited states and non-visited states are close\" follows from continuity of density and what is the notion of closeness. \n\n9-1) Theorem 2: while H seems to be a function of d_\\pi1:t(s), I am not sure how to interpret the argmax_{e_t}. A bit of help from the authors would be appreciated. \n\n9-2) Theorem 2: In the proof, I was not able to justify to my self the transition form g(xi; xj) = Hxi [d1:t+1] \udbc0\udc00 Hxj [d1:t+1] to the second line. Also, what is the definition of H_x_i?\n\n10) In equation 2, the authors use a notation d, I guess as distance. It would not only be helpful to define it but also would be helpful to use a different notation for distance and the d used on page 3, presumably for \"empirical state distribution\". \n\n11) At the beginning of the paper, the authors motivated the maximum entropy but the final algorithm is based on other approaches. \n\n12) Despite the fact that I could not find this paper ready enough and well-posed, I also have a concern about the novelty of the approach. I think it is not novel enough for publication at ICLR, but I am open to reading other reviewers', as well as commenters', and more especially the authors' rebuttal response.\n\n13) I also encourage the authors to provide a discussion on the cases where the novelty (whatever that could mean) does not matter, rather the  novelty of state-action pair matters. \n"}