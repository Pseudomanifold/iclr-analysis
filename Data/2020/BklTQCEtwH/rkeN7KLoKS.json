{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: This well written paper presents an effective way to remove outliers for deep generative models, provided examples are ranked along their centrality. One question I have is how exactly this centrality is measured\n\nThe word \u2018curriculum\u2019 has become a terminology used to describe a wide variety of totally different algorithms. While the authors provide an excellent introduction to this diversity and clearly differentiate their own flavor of \u2018cluster curriculum\u2019, I am wondering if they would not have been better off by describing the proposed approach in terms of outlier removal, especially has it has very little in common with the original idea of curriculum, which is a learning progression designed by the teacher.\n\nNevertheless, the paper is very clearly written and reads well in the present form. I am especially impressed about the clarity of the rather technical part on percolation.\n\nIn terms of outlier removal, this could be interpreted as the following constructive algorithm:\n\u2022\tExamples are ranked along their centrality measure \n\u2022\tOne identify the critical point in percolation for a starting point where too many examples are removed\n\u2022\tMore examples are added until a minimum is found on validation data\nThis process can be made faster as:\n\u2022\tThe optimum can happen very soon after the critical point\n\u2022\tOtherwise, an active set algorithm can be used to control the size of the training set.\n\nWhile my recent expertise has been more NLP than Vision, I think this algorithm is original, and could have a significant impact as it can be extended beyond GANs. The technical presentation is excellent.\n\nOne puzzling issue is the computation of the centrality measure: all I could find in Appendix A1. and A.3 is that it is directly measured on the raw image (RGB pixels?) by taking some distance, which I assume is Euclidean? My experience in image classification suggests such a distance is meaningless, so I may be missing something. I looked for a Github pointer, but none was provided.\n\nThe English is OK, but there are missing words and strange constructions:\n\u2022\tPage 1:  \u201cDeep generative models HAVE piqueD researchers\u2019 interest in the past decade\u201d\n\u2022\tPage 4: \u201cThe training of many loops will lead to time-consuming (MISSING WORD)\u201d\n\nIn particular, the usage of \u201cthe\u201d, for instance in pages:\n\u2022\t7 \u201cTherefore, a fast learning strategy can be derived from THE percolation process.\u201d\n\u2022\t7 \u201cTraining may begin from the curriculum\u201d\n\u2022\t8 \u201cCluster curriculum is proposed for robust training of generative models.\u201d\n"}