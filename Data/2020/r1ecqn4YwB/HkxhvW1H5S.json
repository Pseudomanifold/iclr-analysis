{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a DL architecture that achieves better performance on time series prediction. The proposed architecture is relatively straightforward and composes residual blocks. While the paper does achieve superior results, a lot of the text is devoted to comparing to prior work and arguing that DL approaches can do better than hand-crafted approaches, instead of focussing on the importance of specific technical contributions made in the paper. \n\nMy main concerns are: \n(a) The main technical idea in this paper is the use of back-casting and forecasting (i.e. doubly residual connections). However, no ablations are provided to show how important doubly residual connections are. What is the performance, if the DL architecture is kept exactly the same, except for:\n    (i) No residual connections in backcasting (simply feed in the overall input to every block) \n    (ii) In addition to (i), make no backcasting predictions\n\n(b) Given no architectural details of the previous ML methods are provided, its unclear if the current architecture is better because it has more parameters or it is indeed the doubly residual idea that is important. \n\n(c) Authors make a point about interpretability \u2014 but interpretability is only achieved using domain specific knowledge. I am not really sure, what is so novel about this. \n\nOverall, this seems to be a good applications paper which has been optimized for performance. As a research paper, the contributions are less clear. Answers to my concerns above will help clarify. Given my current concerns, I cannot recommend this paper to be accepted. \n"}