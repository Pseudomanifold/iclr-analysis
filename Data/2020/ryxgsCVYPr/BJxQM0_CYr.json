{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper incorporates an answer inspection encoder verifying whether the answers selected by the Machine Reading Comprehension (MRC) component is valid, into the MRC reader. For training, the model includes two additional losses and trained jointly. Evaluation is on SQuAD V2.0 which is constructed from Wikipedia and contains unanswerable questions generated by crowd sources. The approaches are verified in the settings with/without including BERT to show the generalization of the answer inspector. \n\nThis is an interesting paper which focuses on the answer verification and validation, and shows the effectiveness of the proposed model. It also gives a good ablation study showing the contributions of each component, and provides examples to illustrate why it works. \n\nHowever, there are a few concerns detailed as follows:\n\n1. The model is only evaluated on SQuAD 2.0, which is over explored by many works. I\u2019m wondering if this could be generalized to other MRC tasks, e.g. MSMARCO or DuReader. It would be nice to see some experiments on them or other datasets.\n\n2. It seems that the performance of state-of-the-art SOTA system on SQuAD is much higher than the proposed approaches. I would like to see some discussion on what are the pros and cons between them.\n\n3. How sensitive are the gammas in Eq 1? \n"}