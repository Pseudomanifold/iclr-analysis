{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper tries to study the sign gradient descent by the squared maximum norm.  The authors clarify the meaning of a certain separable smoothness assumption using previous studies for signed gradient descent methods.\n\n1. The authors change the problem. They study the sign gradient times its norm, not the classical sign gradient. \nIn fact, this change dramatically changes the flow in continuous time. \n\n2. The results are too general, which does not focus on machine learning problems. \n\n3. The paper is clearly not written well with many typos. The organization of the paper also needs to be improved a lot for publication. \n\nFor these reasons, I clearly reject this paper for publications.  "}