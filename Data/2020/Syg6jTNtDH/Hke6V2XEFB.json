{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nSummary:\nThe paper talks about a recently highlighted problem in word embeddings which is their incapability to represent numerals, especially the out-of-vocabulary numerals. For addressing the problem, they propose a method that induces a finite set of prototype numerals using either self-organizing map or Gaussian Mixture model. Then, each numeral is represented as a weighted average of prototype numeral embeddings. The method also involves squashing large quantities using log function. Finally, the training is performed similar to Skip-gram in word2vec but with the embedding of numerals computed using prototype numerals. \n\nQuestions:\n1. There are two basic motivations of the paper: (1) Most of the word embedding methods do not embed numerals correctly. (2) There is no mechanism of handing OOV numerals. The second one is well addressed but for the former one, it has been shown in recent work [1] that most of the embedding methods do have numerical reasoning capabilities. As stated in [1], the results of [2] demonstrate the opposite conclusion because their analysis is based on cosine distance and nearest neighbor which are not capable of capturing non-linear dependencies between embeddings. \n\nSo, it would be great if, for the results in Section 4.3 instead of using cosine distance, some neural models could be utilized for evaluation (3 layer MLP similar to [1]).\n\n2. In Section 4.4, the task is to predict the target numeral given its context words (similar to CBOW) while the embeddings are trained with a modified skip-gram model. Can one expect superior results if one uses modified CBOW for training embeddings rather than skip-gram?\n\n3. In Table 5, with 100% training data, the performance of all the methods is very close. It would be better if mean and variance across multiple runs are reported. "}