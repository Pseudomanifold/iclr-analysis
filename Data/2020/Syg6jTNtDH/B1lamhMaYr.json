{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a word embedding approach for numbers.  The method is based on finding prototype numbers, and then representing numbers as a weighted average of the prototype embeddings, where the weights are based on numeric proximity.  The approach provides some gains over baselines on number similarity, number prediction, and sequence tagging tasks.  While modeling numbers is an interesting task, several aspects of the paper needed more clarification, and the paper\u2019s focus may be somewhat narrow for the ICLR audience.\n\nI think the paper could use a better motivation for the prototype-based approach.  In particular, the fact that the approach uses only quantity (and not the form of numbers) to represent similarity is contrary to my intuition.  For example, I would expect 1960 and 1960.1 to behave very differently in text, because one is a year and the other isn\u2019t.  But the proposed method, if I understand it correctly, would give them similar embeddings because they are very close numerically.\n\nI was not able to understand the SOM portion of the method, it is not self-contained within this paper.\n\nOn the Numeracy-600K data set, Chen et al. (2019) shows much higher F1 results than those shown here.  What explains this difference?\n\nThe proposed method seems relatively similar to that of (Spithourakis & Riedel, 2018), in that it exposes numeric quantity to the language/embedding model, and uses GMMs to represent numeral distributions.  More clarity about how this approach compares to that one (and others) would be helpful.  Also, how does the (Spithourakis & Riedel, 2018) approach fare on e.g. the number prediction tasks in the submission?  It is true that the submission's approach produces general-purpose embeddings that can be re-used, unlike (Spithourakis & Riedel, 2018).  But we would like to know whether that generality comes at the cost of performance on tasks, and if so how much of a cost.\n\nMinor:\nThe Lund and Burgess reference seems incorrect.  I don\u2019t see that those authors published a paper by that title in Brain and Cognition in 1996.\n"}