{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a multi-agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi-agent settings (e.g. avoid collisions, collaboration, chase and escape) in a physically simulated environment. The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots. This is the main reason of using the hierarchical RL. \n\nIn general, I like this paper. It is an important step towards multi-agent learning in complex physical environments. The results look appealing, too. However, I voted for \"Weak Reject\" for two reasons. First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel. The combination of these two methods seems straightforward. Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works. I do not understand the \"deep integration of MARL and HRL\" that is claimed in the Introduction. I also do not agree with another claim that \"We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2\". In most of the simulators that I am familiar with, such as Mujoco, Bullet, DART, it is straightforward to add multiple simulated robots.\n\nSecond, the writing can be greatly improved. Almost half of the technical details are buried in \"8. Supplementary material\". Since it is not fair to use \"Supplementary material\" as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7. In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting. I think that these are important details and may also be the contributions of this paper. Most of these should be moved to the main text. \n\nHere are some more suggestions on writing:\n1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2. \n2) It would be great if the paper can clearly define the experiments: \"waypoint\", \"oncoming\", \"mall\", and \"bottleneck\".\n3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,\npromiss->promise\nweek signal->weak signal\nmissing citation [?] in page 3\nreuse the same symbol v_{com} for agent's velocity and desired speed in eq(3)\n"}