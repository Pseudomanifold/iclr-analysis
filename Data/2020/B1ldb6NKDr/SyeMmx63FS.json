{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission proposes a method for hierarchical RL in multiagent settings. In particular it proposes to explicitly decouple training of a high-level and low-level controller with grounded the controller interface as goals in the environment to reach for the low-level controller. The model is trained via PPO with GAE and evaluated on a small set of multi agent locomotion tasks.\n\nThe paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case. Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG. To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission. \n\nA large part of making the MA system work well is based on reward shaping which nearly fills all of page 5. This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms. \n\nThe experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).\nRegarding the challenges (and focus on learning simple tasks), reference [3] might be of interest to the authors.\n\nMinor\n- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.\n- Self-referential sentences in the supplementary materials (i.e. referral to itself)\n- Missing references on page 3\n- The egocentric velocity field is not described (section 5)\n- Section 3.1: maximize\n- The wording new paradigm in MARL might be unsuited given existing work on complex domains.\n\u2018Our proposed approach represents the first physics-based simulation of its kind that supports MARL.\u2019 This sentence remains unclear as the authors do not propose a simulation engine.\n- Text on experiment figures is much too small.\n\n[1] Andrew Levy, Robert Platt, and Kate Saenko. Learning Multi-Level Hierarchies with Hindsight. In International Conference on Learning Representations, 2019.\n\n[2] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient Hierarchical Reinforcement Learning. In Advances in Neural Information Processing Systems, pp. 3303\u20133313, 2018.\n\n[3] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning Tom Schaul, Diana Borsa, Joseph Modayil and Razvan Pascanu\n"}