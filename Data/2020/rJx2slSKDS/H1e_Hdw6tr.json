{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper considers the L2 normalization of samples \u201cz\u201d from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE.\n\nMain comments. \n\nThis paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below.\n\n- An important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications.\n- In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because \u201cthe posterior q(z|x) cannot match  the prior p(z) perfectly\u201d. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero.\n- Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\n- Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\n- Please consider revising the following statement in the introduction: \u201cThe encoder f in VAE approximates the posterior q(z|x)\u201d. The encoder \u201cf\u201d in VAE parametrizes the variational posterior.\n- Some typos,\n\t- Abstract, \u201c\u2026 by sampling and inference tasks\u201d  -- \u201con sampling \u2026\u201d\n\t- Introduction second paragraph after eq 2. \u201c\u2026 it also causes the new problems\u201d \u2013 \u201c \u2026 causes new problems\u201d\n\t- Section 2.1, \u201cFor convenient analysis \u2026\u201d \u2013 \u201cFor a convenient \u2026\u201d \n\t- Second paragraph after Theorem 1. \u201c\u2026 perform probabilistic optimizations \u2026 \u201d \u2013 \u201c\u2026 optimization \u2026\u201d \n\t- Section 5.2, second paragraph. Is it Figure 9?\n\nThe main recommendations I would make are as follows.\n- Consider revising the paper to improve its writing.\n- Provide rigorous theoretical analysis and discussions to support the main claims. \n- Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \n\n[1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018.\n"}