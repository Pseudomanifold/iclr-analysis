{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a framework for semantic parsing, which includes a neural generator that synthesizes the logical forms from natural language utterances, and a neural reranker that re-ranks the top predictions generated by beam search decoding using the neural generator. While the neural generator is the same as prior work, the main novelty is the reranker design, which is a binary classifier that takes a pair of natural language utterance/logical form, and predicts the similarity between them. This reranker could also be pre-trained using auxiliary data sources, e.g., Quora question pairs benchmark for paraphrasing. They evaluate their approach on 3 semantic parsing datasets (GEO, ATIS, and OVERNIGHT), and show that their reranker can further improve the performance of the base generator.\n\nI think the general motivation of the framework is sound. Although the idea of reranking is not new in the semantic parsing community, with the most recent work [1] already shows the promise of this direction, the concrete approach described in this paper is different, seems simple yet effective. The most interesting part is to transform the generated logical form into a pseudo-natural language text, so that it becomes a paraphrase of the input natural language utterance in some sense, which enables the re-ranker to be pre-trained with auxiliary data sources, and to use the wordpiece tokenizer that is effective in understanding natural language. In their evaluation, they indeed show that this transformation helps improve the performance of the reranker.\n\nMy main concern of  this paper is about evaluation. First, although they already evaluate on 3 datasets, all of them are not among the most challenging benchmarks in semantic parsing. In [1], they also evaluate on Django and Conala, which are 2 benchmarks to translate natural language to Python, and also are more complicated than the benchmarks in this paper. It would be helpful for the authors to show results on such datasets that the results of baseline neural generators are less satisfactory, which may also make more room for the possible improvement using a re-ranker.\n\nOn the other hand, they also lack a comparison with existing re-ranking approaches. For example, it will be helpful to compare with [1], given that they also evaluate on GEO and ATIS. Right now the results are not directly comparable because: (1) the base generators are different; and (2) the beam size used in this paper (10) is larger than the beam size (5) in [1]. It will be helpful if the authors can at least provide results with a smaller beam size, and would be better if they can provide results that are directly comparable to [1].\n\n[1] Yin and Neubig, Reranking for Neural Semantic Parsing, ACL 2019."}