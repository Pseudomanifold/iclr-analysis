{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents the Re-Net model which sequentially generates a temporal knowledge graph (TKG) in an autoregressive fashion by taking both global and local information into account.\nThe generation of TKG is motivated via a joint distribution problem which is then parametrized by the usage of a recurrent event encoder.\nIn addition to past information, the encoder aggregates local as well as global information for which the authors propose three different aggregation schemes build upon the works of, e.g., attentive pooling and the RGCN model.\nIn an in-depth-evaluation study, the performance of the proposed model is evaluated on five different datasets on which it consistently improves upon the state-of-the-art.\nAn ablation study shows the benefits of all proposed features of Re-Net, e.g., the usage of more sophisticated aggregation schemes, the impact of using global information, and the number of RGCN layers.\n\nAs far as I know, the proposed method is a novel and clever (though not ground-breaking) contribution to the field of performing global structure inference over TGKs.\nThe paper is well-written but is partially becoming a little hard to comprehend due to its overloaded notation, e.g., $h_t(s)$ vs. $h_s^l$ and $N(s)_t$ vs. $N_t^{(s)}$ vs. $N_t^{(s,r)}$, and could be improved by a more rigorous formulation, e.g., for $N(s)_t$ or $c_s$ (which should also depend on r).\n\n1. Re-Net evolves the embeddings of entities and performs predictions via negative log likelihood. Hence, the model seems to be limited to predict events between entities which have been already seen during training and does not generalize to unseen entities. In addition, by applying a softmax classifier your model does not seem to be able to scale to large knowledge graphs? Are those observations correct and how could they be resolved?\n\n2. As far as I understood, the formulation of $N^{(s,r)}$ is not needed for defining the mean and attentive pooling aggregators since you are aggregating information independent of the relation type. However, the current formulation could confuse readers (including me).\n\n3. Algorithm 1 could be made more clear since the sampled number of M subjects does not get mentioned again. I guess the top-k triples are picked across all M samples and not individually? In addition, the sampling of subjects should relate to Equation 5 instead of Equation 4.\n\n4. In Figure 3 it is not clear why the accuracy does not decrease with temporal distance from the training examples. It would be helpful to interpret and clarify the results in more detail.\n\n5. I was not able to fully comprehend your complexity analysis. For example, it is not clear what $|E|$ means (I guess the maximum number of triples in a time step?). In addition, it seems that you are still dependent on computing node embeddings for all entities in your graph, even if you only report runtimes for computing a single example. In my opinion, there is a $L \\cdot |E|$ term missing in your complexity analysis for computing RGCN across the whole graph. Please clarify!\n\n6. The results of using the attentive aggregation scheme should be included into Tables 1 and 2.\n\n7. Since Figure 5c signalizes that Re-Net can effectively leverage larger receptive field sizes, how does it perform when increasing the number of layers further?"}