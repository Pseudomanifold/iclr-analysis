{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the adversarial robustness of the Bayes-optimal classifier (i.e., optimal for the standard \"benign\" risk). To do so, the authors construct various synthetic distributions and show that in some cases the Bayes-optimal classifier is also adversarially robust, while in other cases it is not. In the main experiment, the authors construct two high-dimensional synthetic distributions of human faces via a generative model. In one of the distributions, even the Bayes-optimal classifier is vulnerable to adversarial examples. In the other distribution (where the Bayes-optimal classifier is robust), CNNs do not achieve high robustness while other approaches such as an RBF SVM are more robust.\n\nOverall I find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data. There are natural follow-up experiments that would shed some light on this question and could substantially strengthen the paper. Hence I unfortunately recommend to reject the paper at this point and encourage the authors to deepen their experimental investigation. For instance, the following points would be relevant:\n\n- Does adversarial training / robust optimization result in a robust neural network on the synthetic data distribution where the Bayes-optimal classifier is robust (and the RBF SVM is more robust than a CNN)?\n\n- Do RBF SVMs also exhibit higher adversarial robustness than CNNs on comparable real datasets? This would indicate to what extent the synthetic distributions are representative of real data w.r.t. adversarial robustness.\n\n\nAdditional comments:\n\n- Briefly defining the MFA model in the main text would provide helpful context.\n\n- A few more details about the experiments could be informative in the main text, e.g., the CNN architecture and the accuracies the various methods achieve.\n\n- An end-of-proof symbol at the end of proofs would be helpful to the reader.\n\n- Is there an index i missing in \\pi in Equation (8)?\n\n- Is the probability given in (9) exact? Gaussians are supported on all of R^d, so even a Gaussian component far away will contribute to the probability of a point under p_1, at least a (very) small amount.\n\n- The proof of Observation 2 is more a sketch. It would be good to include a more formal proof in the appendix."}