{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new decoding mechanism that allows  span-copying which can be viewed as a generalisation of\npointer networks.\n\nBecause action sequences of the same length can lead to sequences of different lengths decoding becomes tricky therefore authors propose a variation of standard beam search that calculates a lower bound of sequence probabilities rather than the true probability of generation this is achieved in practice by merging probabilities of sampled rays of actions generating the same sequence during the search.\n\none advantage of this proposed model is that it doesn't need to copy word by word to update sequences which need minor changes, rather than the seq2seq model with copy actions which due to the way we train those models using NLL loss will likely assign high probabilities to the non-modified input. \n\nAuthors evaluate their model against traditional seq2seq models with copy actions over a set of tasks:\n* code correction tasks: two bug-fix pair (BFP) datasets of Tufano et al. (2019)\n* grammar error correction (Bryant et al., 2017)\n* learning edit representations \n\nPros: \nOverall I am in favour of this work acceptance it represents a neat modelling for copying sequences that integrated simply with seq2seq models, especially the transformer model. \n\nCons: \n- One of the drawbacks of this method is the decoding strategy although authors present a motivated solution for that. The proposed variation of beam search by calculating the lower bound solution seems adhoc and some corner cases are not explained in the paper (see the question below). \n- Experiments could have been more thorough, especially in terms of architectures. I was disappointed not to see authors only comparing between GRU based seq2seq with copy actions, one baseline and their model implementation over a biGRU seq2seq. \n\nQuestions to authors: \n- During inference using the proposed variation of beam search (e.g. k=5), What will happen for example if one ray of actions was dropped because not of the top 5 this ray of actions if continued using future actions would map to one existing top-scoring rays? do you do a way to control that? \n- What is the reason behind choosing the bi-gru architecture? \n\nMissing references: \nThere are a couple of similar work that authors might want to add: \n* Latent Predictor Networks for Code Generation https://arxiv.org/pdf/1603.06744.pdf\n* An Operation Network for Abstractive Sentence Compression https://www.aclweb.org/anthology/C18-1091.pdf\n* Levenshtein Transformer https://arxiv.org/pdf/1905.11006.pdf"}