{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work, the authors tackle the problem of span-based copying in sequence-based neural models. In particular, they extend the standard copying techniques of  (Vinyals et. al., Gulcehre et. al., etc.) which only allow for single-token copy actions. Their span-based copy mechanism allows for multiple tokens to be copied at a time during decoding via a recursive formulation that defines the output sequence distribution as a marginal over the complete set of action combinations that result in the sequence being produced. The authors also propose a span-based beam decoding algorithm that scores output sequences via a sum over the probabilities of action sequences that produce the same output. \n\nThe authors evaluate their model on four tasks: code repair, grammar error correction, editing wikipedia, and editing code. They find that their proposed technique consistently outperforms single-token copy-based seq2seq baselines. They also show that the efficacy of their proposed beam decoding mechanism and do some simple quantitative analysis that the model learns to copy spans longer than a single token.\n\n\nIn general, I found this paper to be very clearly written with very good motivation for the proposed solution. In addition, I thought the authors did a good job of testing their model against a wide range of benchmark problems. It seems that their copy extension is a meaningful contribution. \n\nI do, however, some questions regarding the evaluation, in particular the complexity of the baselines that were compared against. For example, the model consistently outperforms simple copy seq2seq baselines as well as the baselines in which the benchmark datasets were proposed (Tufano et. al, Yin et. al.) However, it does not seem the span-based copying method is state-of-the-art. If it is not state-of-the-art, how far off the SOTA is this proposed architecture? Did the authors do any analysis whereby the span-copy mechanism was added to an existing SOTA model, and if so, did this still produce gains? It's a bit difficult to situate the exact power of this new mechanism, given that it is often only compared to a simplistic copy-seq2seq method. \n\nOther questions/feedback I have for the authors:\n1) How efficient/scalable is the proposed mechanism? I would like to see a more formal treatment of the run-time of the training marginalization operation.\n2) It would be nice to see a quantitative analysis for distribution of sequence lengths copied over (like some sort of histogram) for the datasets.\n3) It would also be helpful to add some short descriptions of the benchmark datasets.\n\n"}