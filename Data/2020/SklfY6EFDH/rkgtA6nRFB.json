{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThis paper aims at revealing the relationship between the quality of deep representations and the attack susceptibility of deep classification models. To this end, they propose the zero-shot test to investigate the \"quality\" of learned representations for unknown classes. Specifically, they leverage two kinds of quality metrics on data of unknown classes. The first one is based on clustering named Davies-Bouldin Index which measures the compactness of intra-cluster. The second one is based on the difference of soft-label histogram distributions with/without unknown classes during training, which may describe the generalization for unknown classes or bias towards known classes of learned features. Finally, with these two metrics, they rank the quality of different models and compare such ranking results with the attack robustness obtained by different attack techniques on CIFAR-10 dataset.\n\n+Strengths:\n1. This paper shows satisfactory related works about recent advances in adversarial attacks and defenses. \n\n-Weaknesses:\n1. The writing of this paper contains many mistakes, especially for the issue of using singular and plural. I strongly recommend the authors to polish the paper carefully. Take the first two pages as an example, \u201cresults suggests\u201d in Abstract, \u201cNetworks\u2019s\u201d in Introduction, \u201ca DNNs\u201d, \u201cperturbations causes\u201d in Sec1.1, \u201cmethods\u2026 which uses\u201d in Sec.1.2, etc.\n2. The idea of representations quality on unknown classes determining attack robustness is not reasonable. Since the adversarial attack is defined on the known classes, the reasons for that are in two aspects. The first is the training dataset. If the collected training data cannot represent/fulfil the whole continuous distribution/manifold of the categories or even biased, the models are of course easily fooled by unseen modes during test. The second is the mapping for classification is from high to low, which is naturally many to one. In a word, I think the quality of models on unknown classes is not directly related to the problem of adversarial attack.\n3. This paper only conducts experiments on CIFAR-10 dataset, which is not convincing enough. It would be better to evaluate their method on more challenging benchmarks and also give more validation about their idea. E.g. on ImageNet, if enough number of categories has been seen for models, whether they would become more robust to adversarial attack. Evaluating features of more layers rather than the softmax outputs is also needed. \n4. What is amalgam proportion? Please explain it in detail or give a reference paper. Otherwise the readers cannot understand the motivation of the second metric in Sec3.2. Besides, Fig.2 contains little information and few captions for readers to understand their method. \n5. In Tab.1 and 2, the meanings of Attack Accuracy and Average Amount of Perturbations (typo in caption for Tab.2) are not introduced. Tab.5 shows 7 methods but its caption says \u201cfive\u201d. Fig.4 is also confused, for AM, which histogram is with unknown classes and which one is without?\n6. The title says \"representation quality explain adversarial attacks\". After reading this paper, I haven\u2019t found the mechanism leading to the adversarial attacks of DNNs."}