{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n-\tThis paper modifies and extends the recent \u201cfree\u201d training strategies in adversarial training for representation learning for natural language.  The proposed \u201cFree\u201d Large-Batch Adversarial Training is well motived, in comparison with plain PGD-based adversarial training and the existing methods like FreeAT and YOPO, which virtually enlarges the batch size and minimize maximum risk at every ascent step. The contributions are solid. \n\n-\tThe proposed methods are empirically shown to be effective, in addition to being aligned with some recent theoretic analysis.  The models achieve SOTA on GLUE (by time the paper was submitted; it is not the best model now but that does not affect the contributions), ARC, and the commonsenseQA dataset.\n\n-\tThe paper conducted good analysis demonstrating the effectiveness of the proposed components, including detailed ablation analysis. \n\n-\tThe paper is well written. It is well structured and easy to follow.  A minor suggestion (just a personal view) is that the author(s) may consider using \u201cnatural  language\u201d instead of just \u201clanguage\u201d in the title and may consider using more specific words like \u201crepresentation\u201d instead of \u201cunderstanding\u201d. But this is minor. \n\nI recommend an accept. \n"}