{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required.\n\nPlease see my detailed comments below.\n\nPositive points\uff1a\n1. The proposed inference example weighing method yields promising results and does not require any re-training.\n2. The combination of batch and group normalization makes it possible to train deep models with very small batch size.\n3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes.\n\nNegative points:\n\n1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, \u2026, x_{B-1}?\n\n2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods?\n\n3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2.\n\n4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions.\n\nReference: \"Bag of tricks for image classification with convolutional neural networks.\" CVPR, 2019.\n\n5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. \n\n6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case?\n\n7. Some closely related work should be discussed in the paper, such as\n\n[1] \"Decorrelated Batch Normalization.\" CVPR, 2018.\n[2] \"Double Forward Propagation for Memorized Batch Normalization.\" AAAI, 2018.\n[3] \"Differentiable Dynamic Normalization for Learning Deep Representation.\" ICML, 2019.\n[4] \"Iterative Normalization: Beyond Standardization towards Efficient Whitening.\" CVPR, 2019.\n\nMinor issues:\n1. In Section 1, the third contribution is not a complete sentence.\n\n2. There are many typos in the paper.\n(1) In Section 2, \u201cLayer Normalization, which has found use in many natural language processing tasks.\u201d Should \u201cwhich has found use\u201d be \u201cwhich has been used\u201d?\n(2) In Section 3.1, \u201cBatch Normalization has a disparity in function between training inference\u201d. \u201cbetween training inference\u201d should be \u201cbetween training and inference\u201d.\n(3) In Section 3.1, \u201cwe need only figure out \u2026\u201d should be \u201cwe only need to figure out \u2026\u201d\n\n"}