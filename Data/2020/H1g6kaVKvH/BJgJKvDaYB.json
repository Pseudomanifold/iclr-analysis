{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "========================== Summary of paper ==========================\nThe paper proposes a new loss balancing approach for lifelong learning. The proposed method dynamically balances the old task loss (from an episodic memory) and the new task loss based on their magnitude. The paper outperforms state-of-the-art method (that don't use extra attribute info), and is straightforward to understand.\n\n========================== Decision ==========================\nI vote for rejecting the paper. The paper adds a very simple dynamic loss balancing to a joint loss, which has limited novelty, yet does not discuss its relationship with loss balancing in multitask learning. Although the method outperforms state-of-the-art, a very simple baseline of their method also outperforms state-of-the-art, making the contribution ineffective. The writing of the paper may also benefit from further edits.\n\n========================== Pros/Cons ==========================\n+ The paper outperforms state-of-the-art.\n+ The method is simple to explain and straightforward to implement.\n+ Proposes a weighted variant of forgetting measure (although the purpose is not justified -- need to discuss what use case would make one prefer this weighted variant better than the original).\n- The paper is not well-placed in the literature. Not until the bottom of page 5 can readers see very closely related methods, and despite the similarity, little is discussed about the difference. To improve, relationships with existing multitask learning weight balancing methods (e.g. https://arxiv.org/abs/1810.04650, https://arxiv.org/abs/1705.07115, https://arxiv.org/abs/1711.02257) should also be discussed, and maybe compared to.\n- Motivation to use the proposed loss balancing rather than that of very similar methods (e.g. GEM/A-GEM) is lacking (\"These approaches cannot capture the dynamics in the lifelong learning process\", but GEM's balancing is also dynamic)\n- Solving for beta using optimization (eq. 10), while (eq. 9) should have an explicit, close-form solution.\n- Writing sometimes is confusing, uses absolute language, or using claims unsupported by evidence.\n    - (1) page 3 \"In this case, the weights are only optimized for the current task while ignoring previous tasks which leads to catastrophic forgetting.\" ignores the existence of regularization-based methods such as EWC\n    - (2) bottom of page 3 \"alpha1 and alpha2 should be adjusted adaptively\" does not have experiment results supporting it (especially considering GEM is also adaptive)\n    - (3) \"l_ref(w; \u03b6) = 0 implies that there is almost no catastrophic forgetting\" claim is problematic. Overfitting to the episodic memory is a common problem.\n    - (3) page 3 xi and zeta not clearly defined. \n    - (4) brings up NP-hardness while it is seldom of interest in this field.\n    - (5) missing \"\u2207\" on the denominator in (eq. 7).\n- Experiment:\n    - The only ablation (the direct approach) is statistically indistinguishable from the proposed method. This also outperforms state-of-the-art, while it should not be. One can only assume that the experiment is problematic.\n    - Completely uses hyperparameter from an unpublished paper.\n    - Claims state-of-the-art, yet omits a state-of-the-art variant in cited A-GEM paper (with joint-embedding). This should be discussed even if the comparison is unfair.\n\n========================== Improving the paper ==========================\n- Rewrite the introduction so it is clear the paper is an improvement over A-GEM with a better loss balancing, and focus the motivation and side experiments on why this is important for lifelong learning.\n- Clean up writing.\n- Explain why the direct approach, while being a basic loss balancing method, outperform state-of-the-art GEM greatly. Perhaps a set of ablation studies can help.\n- Replace optimizing for beta as solving for beta.\n- Comparison with other loss balancing papers.\n- Clarify state-of-the-art comparison.\n"}