{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n\n\nThe paper presents an new unsupervised model for graph classification. It borrows the idea from universal self-attention network and applies it to graph learning. It achieves surprisingly good results on benchmark datasets. Despite the good results, I do not think the technical quality is good enough to make it accepted. My concerns contain the following aspects:\n\n1.\tIf we compare the proposed model with the graph attention networks (GAT), it just adds the recurrent transition and the layer normalizer, which are also from the universal self-attention. This makes the paper not novel enough.  Furthermore, adding these components are not so related to unsupervised learning, it does not add any value to the unsupervised learning strategy.\n2.\tThe description of the unsupervised learning objective is not clear. From Algorithm 1, it seems $o_v$ is equal to $h_v^T$, I cannot understand the meaning of Eq. (7) at all.\n3.\tThe results are too good to be true. Although we cannot judge it based on this belief, the authors have to convince the readers and explain how the huge performance gain is obtained (on some datasets U2GAN is even 27% higher than all of other methods).  I understand the experimental setting is transductive, but even that cannot explain everything. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it.\n\n\n"}