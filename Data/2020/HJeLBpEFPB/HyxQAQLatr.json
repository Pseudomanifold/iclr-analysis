{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors developed a graph embedding method called U2GAN based on self-attention mechanism. Similar to many existing graph neural network, U2GAN samples and aggregate neighboring features for each node in a graph. The aggregation function is similar to GAT, i.e., a query based attention layer. The difference is an incorporation of a transition function followed the attention layer. By minimizing Eq. 7, node embeddings can be inferred, which are summed up to obtain a graph embedding, for the downstream graph classification task.\n\nThere are several points that are unclear in the paper.\n1.  The major argument of the advantage of using self-attention for neighborhood aggregation is to facilitate memorizing the dependencies between nodes and explore the graph structure similarities locally and globally. This argument, however, was not clearly discussed in the paper. First, it is not clear on why existing GNN methods, such as GCN, GraphSage, and GAT, cannot do so. Second, it is not clear on how does the proposed U2GAN achieve it. The current paper only provides some high-level descriptions. A more specific or theoretical discussion is desired.\n2. Since the attention based aggregation is similar to GAT, a discussion on the difference is important.\n3. Several model designs are not well justified. In Eq. 2, 3, the reason to employ Layernorm is missing. In Eq. 7, the intuition on how does the loss function help learn effective embeddings remains to be clarified. Also, it may be better to evaluate different pooling method to obtain graph embedding to justify the choice of sum in Eq. 1.\n4. Since the proposed method aims to learn node embeddings in an unsupervised manner, it is better to see some descriptions on why graph classification was selected as the task in evaluation, instead of node classification.\n5. In the experiments, some methods such as deepwalk, node2vec, graphsage and GAT are missing in comparison. In particular, due to the similarity between the proposed method GAT, it is interesting to evaluate GAT by replacing its supervised loss by Eq. 7 as a compared method. Moreover, in fig. 4, other visualizations of other methods can be compared to demonstrate the difference between the proposed method and others.\n"}