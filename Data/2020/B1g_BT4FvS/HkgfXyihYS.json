{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a simple modification to policy gradient methods that relies on the variance explained to filter samples. The paper contains experiments showing empirical gains to the method, as well as some evidence that filtering rather than simply predicting more quantities is making a difference. \n\nI put a \"weak accept\" score. This method is novel, afaict, and is based on interesting statistical hypotheses. I think showing why this method is relevant could be better executed. There are some gains compared to a PPO baseline, but the gains are somewhat incremental and may only apply to PPO, as other PG methods aren't tested.\n\nDetailed comments:\n- is this method always relevant? Are there environments where it makes more sense to use? (in terms of reward density/variance, exploration difficulty, etc.) Policy gradient algorithms for which it makes more sense to add?\n- The conclusion in particular claims much more than what is in the paper:\n>> \"applicable to any policy gradient method\", technically yes, but it remains untested\n>> \"SAUNA removes noise\", also remains to be seen. A graph showing this would add a lot to this paper\n>> \"We [..] studied the impact [..] on both the exploitation [..] and on the exploration\", Figure 5 is a single data point, where one run for one environment got out of a \"well-known\" local minima. To really convince readers of this you would need to test your method on multiple environments with actual exploration and sparse rewards (Mujoco locomotion tasks do not satisfy those requirements, even though they has local minima, this is far from exploration as commonly understood in RL)\n- This method is very much related to prioritized experience replay and others, as observed in 2.2, yet no comparison is made (PER can be implemented online using two exponential moving averages to estimate the distribution of TD error, used to do rejection sampling). Simpler baselines could also have been tested against, e.g. simply rejecting samples with high TD error.\n- It's not clear how the threshold of 0.3 was chosen, nor what its effect is empirically. \n- Estimating V^ex vs using empirical V^ex seems to make a big difference. It's not obvious why we should be estimating V^ex at all, and I think this deserves more analysis.\n- I've seen Roboschool cited, you can use @misc{klimov2017roboschool, title={Roboschool}, author={Klimov, Oleg and Schulman, J}, year={2017} }"}