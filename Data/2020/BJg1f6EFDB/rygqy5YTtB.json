{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work studies the identifiability of attention, i.e., to what extent does the attention weight uniquely determine the output. The paper (1) formally establishes, using a rank argument, that at least for the cases where the attention head dimensions are smaller than the sources, infinite different attention weights can yield the same attention output. Based on such an observation, (2) a principled tool to extract the `effective` attention is introduced, with which several previous observations are challenged. This tool can certainly inspire future research. The paper further (3) explores how much token and context information is mixed at different layers of a transformer model, using both a carefully designed probing task and a gradient-based method.\n\nOverall I like the paper a lot. Its hypothesis and arguments are very clearly presented, which are then fleshed out by carefully designed experiments. I vote for an acceptance.\n\nI don't have any major complaint. Below are some questions and comments.\n\n- Since the paper only experimented with BERT, I think the authors mean \"BERT\" instead of the transformer models in general, whenever an empirical argument is drawn.\n\n- Section 4.1, can the authors justify the use of MRPC for this analysis? Adding onto this, do the authors think the conclusion here holds for other input text domain? What about other transformer models, e.g., GPT, or a transformer trained using MT objective?\n\n- Section 4.2, 2nd paragraph. I'm not sure why that g^{MLP} is better than g^{lin} can be interpreted as \"non-linearities play an important role in maintaining token identifiability.\" Could it be the case that the non-linearities make the token information more opaque, such that it can only be extracted with a more expressive model (MLP)? A better way to support this claim, in my opinion, is to show a better token identifiability in a BERT model trained without non-linearities.\n\n- In section 4.2, many conclusions are drawn that the last layer behaves differently than the rest. To eliminate the potential effect of depth, have the authors considered conducting a controlling experiment, where the probing task is to recover e_{i}^{l-1} (i.e., the representation from layer l-1, instead of the input token embedding) from e_{i}^{l}?\n"}