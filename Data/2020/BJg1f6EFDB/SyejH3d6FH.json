{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This very interesting paper proposes new perspectives for investigating the self-attention distribution and contextual embedding and challenges many existing findings in the literature. The conclusion presented in this paper partially aligned with some concurrent works (Danish Pruthi et al. 2019, Serrano et al. 2019 and etc.)\nMany concrete findings in the paper make sense very much (not surprising), like 1) Input tokens retain their identity in the first a few hidden layers and then progressively become less identifiable, 2) Non-linear activations are crucial in preserving token identity and 3) Strong mixing of input information in the generation of contextual embedding.\n\nThe neat proof in the paper, though is based on some assumptions, formally shows that the multi-head self-attention is not always identifiable. The conclusion is that when the sequence length is larger than the size of attention head, there is redundant parameterization space for the attention.\nThe authors propose \u201ceffective attention\u201d which is orthogonal to the null space as a diagnostic tool. In Figure one, the fact that the Pearson correlation between raw and effective attention weight decreases as the sequence length increases experimentally support the theorem. \n\nHowever, I am not sure what benefit identifiable attention would bring us. It seems that \"redundancy\" sometimes have some \"benefits\". As shown in the original Transformer paper, very large d_v or very small d_v both lead to worse performance. I am not sure how \u201ceffective attention\u201d would ease the training, though I agree it is a good \"diagnostic tool\".\n\nAnother concern is about how the \u201cnon-identifiability\u201d really hurts the model? A pre-trained transformer-based model can achieve very good performance for discriminative tasks after fine-tuning. This shows that a deep model, though there are many uncertain choices in the earlier and middle layers, they finally give us correct discriminative labels.\nAs the authors claim that contextual embeddings are strong mixing of input (which I agree), my question is \"will the non-identifiable attention weights, dramatically affect the mixing component of the contextual embeddings\"? Or, will the uncertain choices in earlier layers finally come to close contextual representations in layers closer to output?\n\nTowards the claim that \u201cnon-linear activations in preserving token identity\u201d, the authors provide evidence in Figure 2.\nCan we also do experiments that using the embedding e^{l}_j (Layer l and position j), through non-linear activations to recover neighboring tokens of position j? I suspect that neighboring tokens can probably also being covered very well by MLP g^{MLP}_l.\n\n"}