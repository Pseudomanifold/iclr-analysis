{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work tried to address the covariate shift problem in imitation learning, which is due to the mismatch between training and test state distribution and may cause compounding errors. \n\nThe authors proposed the algorithm called value iteration with negative sampling (VINS) of which the main ideas can be summarized as follows. First, value iteration is used on expert trajectories with negative sampling. Specifically, the states that are randomly perturbed from expert\u2019s states were used to enforce (4.1) and (4.2) in the submission (called *conservative extrapolation* requirements in the submission). By doing so, the state values outside the support of expert state visitation distribution become less than those inside the support. In the meantime, temporal-difference (TD) error was minimized to satisfy Bellman consistency among state values. The second main idea of this work is to use *self-correctable policy*, where the approximate dynamics and behavioral-cloning (BC) policy were used to select the action which is expected to give higher value at the successor states. \n\nTo consolidate their ideas, the authors proved Theorem 4.4 showing that state visitation distribution of resultant policy is approximately close to that of an expert under a few assumptions. Empirically, they considered two experiments. In the first experiment, assuming that the environment simulation is not allowed, the performance of VINS was compared with that of BC over 5 tasks, and VINS achieved higher success rates. In the second experiment, assuming the simulation is allowed, VINS was compared with existing methods such as HER+BC, GAIL and Nair et al \u201818 and shown to be much more sample-efficient compared to the selected baselines. \n\nAlthough the theoretical and empirical contributions of this work are clear to me when the environment simulation is not allowed (as shown in the first experiment in Table 1), I think the second experiment, which allows the environment simulation (as shown in Figure 3), is misleading, and this is why I vote weak reject for this work. For instance, we can simply think about GAIL with BC initialization, but it seems to me that GAIL with random initialization was used in the second experiment (since authors mentioned GAIL in OpenAI baselines was used without hyperparameter tuning (https://github.com/openai/baselines/blob/master/baselines/gail/run_mujoco.py#L53)). In addition to it, there have been some recent works on the sample efficiency of imitation learning with environment simulation which are not included as baselines in this work:\n\n[1] GMMIL (Kim and Park, 2018, \u201cImitation Learning via Kernel Mean Embedding\u201d) - cost learning with maximum mean discrepancy minimization leads to sample-efficient training\n\n[2] BGAIL (Jeon et al, 2019, \u201cBayesian Approach to Generative Adversarial Imitation Learning\u201d) - Bayesian cost is helpful for sample-efficient imitation learning\n\n[3] DAC (Kostrikov et al, 2019, \u201cDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\u201d) - Solving reward bias in imitation learning with simulation and using off-policy RL algorithm to enhance the sample efficiency\n\n[4] Sasaki et al, 2019, \u201cSample Efficient Imitation Learning for Continuous Control\u201d - Bypassing cost learning and introducing off-policy RL to enhance the sample efficiency. \n\nEspecially, off-policy imitation learning methods [3], [4] are shown to be extremely sample-efficient compared to GAIL. I think the authors should have compared the performance of VINS + RL with those baselines in the second experiment if they tried to emphasize the sample efficiency of VINS + RL. Otherwise, they should have focused more on the initialization effect of VINS and BC. For example, one can consider the convergence speed of GAIL to the expert performance when policies were initialized with either VINS or BC."}