{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nLearning Key Steps to Attack Deep Reinforcement Learning Agents\n\nThis paper proposes an extension of existing discrete action image space adversarial attack algorithms.\nInstead of choosing the steps by heuristic, the authors propose to choose the key steps by augmenting the reward function with a penalty to decrease the ratio of attacks.\n\nI tend to vote rejection for this paper, given that the proposed algorithms seem incremental compared to the existing algorithms, and the experiments seem not sufficient enough to support the core claim proposed in the paper.\n\nPros:\n- The paper is well written, with sufficient background and related work section for the paper to be self-contained.\n- The proposed framework is an interesting and practical framework for attacking RL agents.\n\nCons:\n- The experiment section is insufficient.\nMore specifically:\n1) Results from Figure 5 and Figure 6 seem to disagree with what the authors claim in the paper. In many (the majority) of the environments, the proposed algorithm has only trivial improvement and even worse performance under the same attack rate.\n\n2) It is not very convincing when only one result sample is plotted in Figure 5 and Figure 6.\nI think it is necessary to show the performance of the proposed algorithm under different attack rate.\nA wide range of candidate penalty parameter lambda should be tested, so that a curve can be fitted for the proposed algorithm similar to the baselines (similar to the one shown in Table 1, but with much more test values).\n\n3) Related to 2), it seems the Lagrange relaxation makes it hard to control the attack rate in the proposed algorithms. How sensitive it is to control the attack rate?\n\n3) Can the authors elaborate on why the algorithms is not too sensitive to the value of penalty in section 4.5?\nTable 1, where the performance is almost the same for different penalty parameter, does not necessarily show that the algorithms is not too sensitive to the choice of the penalty parameter.\nAs mentioned by the author, -21 is the minimum reward (or random reward) an agent can get from Pong.\n\n\nIn general, given the current status of the paper, where there is a lot of room for improvement of experiment section, I will vote for a rejection.\n"}