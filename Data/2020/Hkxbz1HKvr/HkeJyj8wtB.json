{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Contribution:\n\nThis work presents a method for performing budgeted attacks on a RL agent where an attacker can apply a perturbation to a limited subset of the observations of the said agent.\nTo select the \"key steps\" that needs to be corrupted, they train an opponent RL agent, with a reward crafted such that it should minimize the number of attacks as well as the cumulated reward of the target agent.\nTheir attack works either in a white-box setting, using FGSM, or in a black-box setting, using a substitute model.\n\nThey then proceed to compare to to prior work that use heuristics based on the policy values to select the steps to attack.\n\nThe paper is overall well written and easy to follow.\n\nReview:\n\n\nOne major limitation of the work is that the attack rate is not readily modifiable. It uses a penalty term in the reward function with a tunable weight $\\lambda$, but changing this weight seemingly requires retraining the opponent from scratch, which is unpractical. By constrast, note that the previous attack methods allow to change the attack rate at will.\nMore importantly, there is no clear relationship between the value of lambda and the attack rate. With the same lambda, depending on the game, the opponent settles on attack rates varying between 10% (on pong) and 70% (on Riverraid).\n\n\nThe results themselves show only marginal improvement over the baselines, and in the absence of clear error bars / confidence intervals, it is difficult to state the significance of the method. In the particular case of Space Invaders (figure 5b), the proposed method seems tied to the best heuristic (at attack-rate 70%), but the said heuristic reaches the same performance level for much lower attack-rate (as low as 40-50%), implying that the presented method did not do a good job at minimizing the number of attacks.\nOverall, a rigorous way of comparing the methods need to be devised. Maybe something like an AUC with proper confidence bounds could do the trick?\n\n\nIn equation (2), you present a target objective function designed using Lagrange relaxation. However, the RL algorithm uses decay (\\gamma = 0.99), which means that the resulting function that is effectively minimized is different. Could you clarify the impact of the decay on the lagrange-relaxed objective function?\n\n\nCould you clarify a bit the section 4.3 on black-box attacks? It seems that you are using a substitute model attack, but it's not clear to me how the substitute is obtained. Is it the same model? How is it trained? Is the attack robust to differences in the algorithm?\n\nFinally, I'm a bit surprised by the choice of DQN as the base algorithm, especially since the chosen framework (Dopamine), offers significantly stronger algorithms (Rainbow or IQN). DQN doesn't even reach perfect score on Pong, which means that the raw policy itself is a bit brittle, since it looses 5 points. Did you try to apply this method on more robust policies?"}