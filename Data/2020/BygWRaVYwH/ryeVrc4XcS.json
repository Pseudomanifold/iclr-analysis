{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors present a PyTorch based framework for performing second-order\nreverse mode autodiff for meta-learning.\n\nFirst, the authors present a formalization of a general prototypical\nmeta-learning setting.\nThey then provide an algorithm that solves this problem via gradient based\noptimization.\nFinally, perhaps the main contribution is a specific PyTorch implementation\nof said algorithm.\n\nThe type of meta-learning setting the authors consider is one where a gradient\nbased inner loop optimizer finds $\\theta^\\star$ by performing a finite number of steps.\nThe inner loop optimizer is parameterized through $\\varphi$ that consists of\ntwo parts $\\varphi^\\text{loss}$ and $\\varphi^\\text{opt}$.\nThe parameters $\\varphi^\\text{loss}$ are somehow part of the loss used for\ntraining in the inner loop. Example: Regularization paramter.\nThe parameters $\\varphi^\\text{opt}$ do not occur in the loss but in the\noptimizer step. Example: Learning rate.\n\nExample of an inner loop step:\n$\\theta^{k+1} := \\theta^k - \\alpha (\\nabla L(\\theta^k) + \\lambda \\nabla R(\\theta^k))$\nwhere $\\theta$ are the parameters of a neural network, $L$ is the training loss,\n$R$ is the regularizer, $\\alpha$ is the learning rate, $\\lambda$ is the regularization parameter.\nIn this example we would have $\\varphi = (\\alpha \\lambda)^T$.\n\nThe authos assume $\\theta^K$, the output of the inner loop after $K$ steps to\nbe differentiable wrt $\\varphi$.\nFurthermore, the meta-learning loss is assumed to be differentiable wrt $\\theta$\nso that a gradient of the meta-learning loss wrt to $\\varphi$ can be computed.\nThe authors also assume the meta-learning loss to be sufficient smooth in $\\varphi$\nsuch that a gradient based optimization can even be used for meta learning to\na local optimum.\n\nThe authors explicitly write down the reverse mode auto differentiation of\nthe inner loop and show how to, in that way, compute the gradient of the\nmeta-learning loss wrt to $\\varphi$.\n\nThe reverse (adjoint) mode auto differentiation of the above example inner loop step\nis the following step (iterated over in reverse down from $k = K$ to $k = 1$:\n$\\bar \\theta^k := (I - \\alpha (\\nabla^2 L(\\theta^k) + \\lambda \\nabla^2 R(\\theta^k)))^T \\bar \\theta^{k+1}$\n$\\bar \\alpha := \\bar \\alpha - (\\nabla l(\\theta^k) + \\lambda \\nabla R(\\theta^k))^T \\bar \\theta^{k+1}$\n$\\bar \\lambda := \\bar \\lambda - \\alpha \\nabla R(\\theta^k)^T \\bar \\theta^{k+1}$\nwhere $\\bar \\alpha$ accumulates the gradient of $\\theta$ wrt $\\alpha$ and\n$\\bar \\lambda$ accumulates the gradient of $\\theta$ wrt $\\lambda$.\n\nThe authors give some implementation details specific to some frameworks necessary\nfor implementing such \"gradient of an inner loop\".\n\nThe authors present experiments where they show how to meta-learn learning rates\nwith their framework.\nThey also how their framework can be used to quickly implement a MAML type\nmeta-learning optimizer ablation study comparing various combinations of\narchitecture, optimizer and inner loop steps etc..\n\nRecommendation:\nI propose to reject the paper.\nIn my eyes the only contribution is the implementation of a meta-learner in\nPyTorch based on well known methods.\nThe provided unifying formalization is theoretically inaccurate (see below)\nand to me come across as merely a motivation for their framework\n(but no value added compared to existing literature).\nThere is no new insight provided on the software engineering level either as\nfar as I can see.\n\nDetailed comments:\n- Page 1: ...provides tooling for analysing the provable requirements...\n\tseems like a complicated way of saying something that could be said simple\n\n- Page 2: Without loss of generality, ...\n\tYou are assuming a parametric model. Not sure what generality this phrase\n\trefers to.\n\n- Page 2: A formalization as $\\theta^\\star = argmin(\\theta, L(\\theta, \\varphi))$\n\tis inaccurate in the sense that it does not acknowledge the existing of\n\tmultiple optima.\n\tI would recommend not to use the $argmin$ operator here, since $argmin$ for\n\tsomething like a neural network would for example either return a global\n\toptimum (which no optimizer used in practice finds, and is not ment here)\n\tor would take on a set value with multiple local minima for example.\n\n\tIn the same context, the authors should mention the issues about uniqueness\n\tof optima (we are not even really finding optima when training neural networks),\n\timplicit functions / implicit differentiation\n\n\tIn the context of using stochastic optimizers one should also at least\n\tmention something about the differentiability of outputs of such optimizers\n\tand how they potentially depend on randomness of mini-batches\n\t(what if different randomness is used with the same or a perturbed\n\thyper parameter?)\n\n- Page 3: You mention the potential statefulness of the optimizer.\n\tWhy not explicitly carry it in the math notation?\n\tProbably things would get cluttered but saying it should be covered within\n\t$\\varphi$ does not seem reasonable to me.\n\n- Page 3: While this may seem like a fairly trivial formalization...\n\tYes, but also nesting this in an outer loop is fairly trivial in the sense\n\tthat it is a well known approach.\n\n- Page 4: there exist continuous hyperparam...\n\tIf they are not continuous then they should not even occur in this\n\tformalization so saying there exist... does not make much sense to me here\n\n\t$\\alpha \\subseteq \\varphi^\\text{opt}$ implies that $\\varphi^\\text{opt}$ is\n\ta set from notation although we are treating it as a vector everywhere else\n\n- Page 4: All of section 2.4 seems somewhat trivial to me, but I guess that is\n\thighly subjective.\n\n- Page 5: in the definition of stop operator perhaps use $:\\Leftrightarrow$\n\n- Page 5: Perhaps explicitly mention how your approach differs from a reverse\n\tmode differentiation of training or if it does not differ, say this.\n\n- Page 14: When talking about _S_GD (instead of just GD) perhaps mention\n\tsomething about non-existence of mini-batch randomness / being deterministic\n"}