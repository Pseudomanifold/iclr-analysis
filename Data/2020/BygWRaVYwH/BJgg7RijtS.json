{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose the general formulation of recent meta-learning methods and propose a good library to use.\n\nPros:\n1. The general formulation of recent meta-learning methods is reasonable.\n2. The proposed library is easy to use.\n\nCons:\n\nThe paper lacks technical novelty. I understand the goal of this paper is to build a library. However, the paper only describes a general formulation for recent meta-learning methods (e.g., MAML) and implement the formulation. It is better to clarify and some key engineering challenges and do the corresponding experiments.\n\nIn addition, in the experiment parts, the authors only compare the results with MAML++. It will be more convincing if the authors can analyze other popular meta-learning methods (e.g.. Prototypical network [1], meta-LSTM [2]). \n\nAnother suggestion is that the authors can give some examples to connect current meta-learning models with the proposed general formulation. For example, the meaning of \\phi_i^opt, \\phi_i^loss in MAML, Prototype, Reptile, etc.\n\nIt is better to explain the meaning of different colors in Figure 3.\n\n[1] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" Advances in Neural Information Processing Systems. 2017.\n[2] Ravi, Sachin, and Hugo Larochelle. \"Optimization as a model for few-shot learning.\" ICLR (2016)."}