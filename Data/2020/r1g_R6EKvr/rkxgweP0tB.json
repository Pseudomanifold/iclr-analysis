{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel (self)-attention mechanism dubbed \"doubly normalized attention\". When taking the attention between keys and query, standard attention proceeds by computing o_i = 1/Z \\sum_j exp(q_i * k_j) * v_j. This corresponds to normalizing the logits exp(q_i * k_j) over the keys (j). The authors point out that this can lead to an \"explaining away\" phenomenon: basically, the score for a particular j, exp(q_i * k_j), could be 0 for all i, which means that information coming from position \"j\" is basically discarded, and the authors argue this is bad. Moreover, they argue that standard attention suffers from \"mode collapse\" (i didn't quite get / found clear what this effect corresponds to in standard architectures). To palliate the two problems, they propose to re-normalize the attention weights along the columns. Basically, first computing \"responsibilities\" (in a EM flavor) p_j = 1/Z \\sum_i exp(q_i * k_j) , and then computing the attention by renormalizing p_j, o_i = (1 / \\sum p) \\sum_j pi_j * v_j. The author show that this corresponds to one step of EM (or basically kmeans if q_i are cluster centers and k_i = v_i are the data points). The authors test their model on headline generation, a visual qa setting and some natural language understanding task, reporting modest gains (0.2-0.4 on SQUAD/MNLI) w.r.t. standard attention.\n\nThis work has some quite interesting aspects due to the fact that it tries to interpret and generalize attention computations under a more general framework. I appreciated the theoretical work done by the authors, even if sometimes it appeared to me more complex than needed to be. However, this paper also has some weaknesses which prevent me for putting it above the acceptance bar, namely (i) lack novelty of proposed method and (ii) somewhat weak motivation. I'd be happy to increase my score if the author shed light on the problems below.\n\n1) About the lack of novelty:\n1.1) The doubly normalized attention appears to me identical to EM routing proposed in the capsule network (ICLR '18, https://openreview.net/pdf?id=HJWLfGWRb). I think the author mention this in the appendix. If it is, what's the main contribution of the first part of the paper ? I agree that the analogue to Gaussian Mixture Models appears novel but solving GMMs are just a special applications of EM-type algorithms.\n\n2) About weak motivation:\n2.1) The authors mention the \u201cexplaining away effect\u201d as a problem to solve. It is however unclear as to whether/how the performance of the architecture tested suffered in virtue of this phenomena. Can you justify or quantify this ? Can you attempt at actually computing how much information is discarded by looking at the attention distribution ? You could maybe link this to works analyzing \"pruning heads\" https://arxiv.org/abs/1905.09418 ? This could give more depth to the paper.\n2.2) I didn't quite get the section on \"mode collapse\" and why adding residual layers would help in solving mode collapse in standard transformers architectures (as written in a note in the paper). Could you clarify this point ?\n2.3) The analogue to optimal transport felt less well-motivated and it was unclear what new understanding was gained from the reframing of self-attention as constrained optimization. I don't know what to get out of it. "}