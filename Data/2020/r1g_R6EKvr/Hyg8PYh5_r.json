{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes \"doubly normalized attention\". Practically, it is a very simple modification to existing attention mechanisms (which is a good thing!), and involves normalizing over both rows and columns. The authors motivate this modification based on interpreting the output of an attention layer as being the mode of a mixture of Gaussians, where the mean parameters are given by the key vector. This motivates the doubly-normalized approach given that for encoding, information should flow from the other direction (section 3).\n\nThe approach is tested on three tasks: VQA, summarization, and BERT-pretraining, where it is found to slightly improve upon existing \"singlely\" normalized attention layers.\n\nOverall I found the exposition to be quite clear. And I appreciated the authors' experiments on a diverse range of tasks/benchmarks. Having said that, I must admit that I do not find the motivation very compelling. If the argument is that doubly-normalized attention is more suitable for encoding than decoding, it should be possible to test this. Namely, in a seq2seq task, the best performing model should be one that uses DNAS in the encoder and LNAS in the decoder. Is this borne out in practice? In my opinion the paper could be better motivated/presented as simply a modification to the existing attention mechanism (nothing wrong with that!).\n\nI had some other minor comments/questions:\n\n- Do you achieve similar performance gains on translation? While summarization experiments are appreciated, the real test-bed for whether an architectural modification to the Transformer results in empirical gains is translation. (However I very much appreciated the experiments on pretraining BERT in Table 3, so thank you for this).\n\n- Did you try analyzing the attention layers, and seeing how DNAS qualitatively differs from LNAS?\n\n- What if you tune over \"u\" as a fixed hyperparameter? What if it is predicted by an auxiliary network as a function of the input x?\n\n\n\n "}