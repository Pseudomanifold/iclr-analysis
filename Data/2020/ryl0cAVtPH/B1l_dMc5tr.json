{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThis paper conducted an empirical study on why training with warm starting has worse generalization ability than learning from scratch. The paper is interesting, however, it has something unclear to me, as explained below.\n\n\n1)\tThe motivation of the paper is not very strong. Even the authors themselves acknowledged that this topic has not received much direct attention from the research community. There are only a very small number of related works, and most of the motivation is based on the authors\u2019 own experiments. I am not criticizing this \u2013 but if the problem is general and important in practice, it is hard to believe that other people have not found it and have not conducted studies on it.\n\n2)\tThe scale and diversity of the study can be improved. Only three models and three datasets were examined, which might not be representative enough. For example, the popular Transformer model, the large-scale datasets like ImageNet, the language understanding and machine translation tasks, etc. were not included in the study. This may make the study less relevant to many important tasks and domains.\n\n3)\tThe interesting and highlight part of the paper is that it studies many different factors and aspects, including the influence of batch size, learning rate, regularization, moment, denoising, etc. However, I kind of feel that the experimental setting has some fatal problems, which makes the experimental results not convincing. The authors partitioned the dataset into two halves, using the first half for pre-training, and then use the whole datasets for continued training. Although the partitioning is random, given the limited size of the datasets, such a treatment will change the underlying distribution (frequency of the samples during training). The first half of the dataset plays a more important role in training: it was used during pre-training, and also used in training. So somehow the first half was used twice, or at least used more than once,  depending on the numbers of epochs in pre-training and training. This distribution change may make the training a little biased, and at least it is not a fair comparison with learning from scratch (the latter will not have such bias in data). So for a fair comparison, one needs to add some baselines to understand the influence of data frequency change. Without the understanding from this angle, the study may be mis-leading.\n"}