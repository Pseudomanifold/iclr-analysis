{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The author proposes different approaches to the problem of  \"warm-started\" neural networks. Models trained from scratch on the whole dataset have better performance than \"warm-started\" models, which are trained with weights initialized from training using part of the available data.  The authors change hyperparameters like batch size and learning rate and demonstrate a tradeoff between generalization performance of the model and time, required for its training. We can also see that the choice of hyperparameters, necessary for the best performance, levels benefit in time from \"warm starting.\"\n\nThe core idea of the paper is the investigation of various possible causes of difficulty of \"warm start\" to reduce training time without damaging a generalization performance. The authors investigate the dependence of this effect with gradient values, regularization, part of \"warm started\" layers, noising weights, catastrophic forgetting, and so on. \n\nThe authors describe different problems where this research can be useful and tries to shed light on the causes of this problem, but the solution is not found. This article can be helpful for future researchers as they continue research in this direction from a warm start. However, it is hard to judge how valuable this contribution is.\n\nAlso, see a few minor comments:\n1. Maybe it should be useful to include in Table 1 results for models trained using only 50% of data.\n2. Typo: NVIDA -> NVIDIA (p. 6)\n3. It seems that the problem lies in the area of the complex learning landscape for optimization of Neural Networks as we end up in the worse local optimum if we use a warm start. Maybe one should attack the problem with this direction, as there are several papers that investigate how the loss function landscape looks like e.g. [1] \n4. It seems that the behavior becomes worse if we increase the complexity of the model. Investigation of the dependence of severity of warm start effect on e.g. number of layers in the network can be useful. Also, it can be possible to gain some theoretical insights and provable results while dealing with simpler models. [1.] H. Li, Zh. Xu et. al. Visua\n5. It might be useful to research the dependence of \"warm start\" effect from the portion of data on which the model was pre-trained. As I understand situations, where 80-90% of data are used in the first round of training, are more common.\n6. The authors provide us with graphics of accuracy on training dataset for \"warm started\" and trained from the scratch models. We can see that both models reach 100% train accuracy, and the authors state that it is impossible to spot the \"warm start\" problem on the training dataset because the metrics are equal. Maybe it could be useful to show graphics of loss function. Despite of similar 100% train accuracy, final losses might be different(\"warm start\" loss > from scratch loss). It may mean that in case of warm start we reach a local minimum, but not the best one. \n7. It might be a good idea to add to Table 2 results for not-regularized model in order to compare results with and without regularization and figure out what effect on \"warm started\" models regularization have.\n\n"}