{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper theoretically investigates whether  generative models can universally approximate any compact smooth manifold.\n\nAlthough I think this paper has some interesting observations, I think it has very little significance for generative models as it ignores the distribution over the image completely.  \n\nI must note that I am not familiar enough with the related work to judge the novelty of this work.\n\nDetailed remarks:\n- The paper claims to talk about generative models, but it actually only talks about GANs. The paper needs to make this clear as this is misleading.\n- One severe limitation of this work is that it only looks at the output set and says nothing about the distribution induced on it by the mapping (or the original). \n- The paper doesn't connect (nor do I think it is possible to connect) the metric used here, Hausdorff distance, to any metric between distributions. Therefore it is not clear what value this results have for *generative* models instead of general neural networks.\n- The question \"In particular, what does it even mean for a GAN to model a distribution?\" is trivial. Any mapping on a distribution induces a distribution on its image. While this distribution might be hard/impossible to compute and evaluate in practice, it is clear what it means for a GAN to model a distribution.\n- I think the analysis is interesting and should be published but under some revision. If the focus is generative models or GANs, then some thought should be made to the distribution, connecting to metrics between distributions and/or saying something even on a uniform distribution over the manifold embedded in R^n. Otherwise, these results might be worth considering in the more general framework of DNN and maybe see GANs as a single instance. \n\n\nMinor details:\n- You assume oriantability, but unless I missed something you do not use this assumption anywhere\n- While the claim that a compact and connected manifold is geodesically complete is a well known, a reference might be nice as this is a different community. \n- Typo in sec. 7 \" is two train two neural networks\" -> \"is to train two neural networks\"."}