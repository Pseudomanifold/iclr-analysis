{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents DynE, a self-supervised approach for learning dynamics-aware state and action representations. DynE learns an encoding of individual states and action sequences, assigning nearby embeddings to those that have similar outcomes. This is achieved via a reconstruction objective that predicts the outcome of a sequence of \"k\" actions from a given state, along with losses that encourage compression in the learned latent representations. Additionally, a learned decoder allows for reconstruction of minimum-norm action sequences from the high-level latent action embedding. Combining DynE state and action embeddings with an actor-critic agent operating directly in the learned high-level action space leads to significant speedups in both from-scratch and transfer learning (on 2D and 3D OpenAI Gym tasks), leading to better data efficiency compared to model-free baselines. Additionally, the learned action and state embeddings lend themselves to better exploration and consistent value prediction, respectively.\n\nThe paper is very well written and the approach looks quite promising. A few comments:\n1. The approach is well validated but additional ablation results can help quantify the effect of different components. For example, it would be useful to see the effect of varying \"k\", the number of actions to be encoded for generating the action embedding.  \n2. A related paper that learns state representations that are physically consistent and dynamics-aware is this work:\nJonschkowski, Rico, et al. \"Pves: Position-velocity encoders for unsupervised learning of structured state representations.\" arXiv preprint arXiv:1705.09805 (2017).\nHere the state representation is learned to implicitly encode physical consistency via self-supervised losses that mimic constraints such as controlability, inertia, conservation of mass etc. Combining such additional self-supervised losses can help structure the state embedding learning further, albeit at the cost of introducing additional hyperparameters during optimization.\n3. It would be useful to know what the actions are (and their dimensions) for the tasks considered in the paper.\n4. The paper would benefit from a short discussion on the limitations of the proposed approach and potential to scale to more complicated tasks.\n5. Fig. 5, bottom right: It is not clear why PPO (blue) performs significantly better on this task compared to the other 7DoF tasks considering that the thrower should be more complex than the pusher and striker. PPO also seems to match the data efficiency of DynE-TD3. Is this correct? \n\nOverall, I find the approach quite interesting and promising. I would suggest an accept.\n\nTypos:\n1. Intro, 2nd para, 2nd line, many samples to learn than a better one\n2. Fig. 1, the pixel representation is very unintuitive"}