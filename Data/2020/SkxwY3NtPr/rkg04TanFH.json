{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes four metrics to evaluate explanation methods, namely 1) bias of the attribution map based on Shapley value, 2) unexplainable feature components, 3) robustness of explanations, and 4) mutual verification (whether two explanation methods can verify each other). These metrics focus on explanation methods that give attribution maps as outputs (such as a pixel-level attention or saliency map). Besides, it also evaluates nine widely used explanation approaches with these metrics.\n\nI would recommend for acceptance. Several explanation methods have been proposed to generate attribution maps. However, it is shown that it is hard to inspect the quality of these explanation methods visually (e.g., as mentioned in http://arxiv.org/abs/1810.03292). I believe the proposed metrics provide valuable insights on quantifying the performance of explanation methods. The evaluation results on nine existing approaches are also helpful to future work on explainable AI.\n\nHowever, I would also like to note that while the four metrics proposed in the paper reflect the desired properties of explanation methods, they should not be the only criteria. The desired criteria depend on the application, and methods without these properties could also be potentially useful or even better in some cases. For example, instead of giving an unbiased attribution map, it would be sometimes more helpful to provide a discrete binary mask of regions of interest (e.g., for downstream image processing)."}