{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe manuscript proposes an evaluation methodology to assess objectively methods for model explanation without the need of ground truth. This methodology is based on four metrics evaluating different aspects of the generated explanation. These four metrics evaluated the bias of the explanation map at the pixel level, quantify the unexplainable features components, robustness of the explanation to spatial masking, and mutual verification of different explanation methods.\n\nExperiments on the CIFAR10 an Pascal VOC 2012 considering a good set of representative explanation methods show the strenghts and weaknesses of the proposed method. \n\nGiven the recent proposal of a significant number of works aiming a model explanation, the manuscript touches the critical point of assessing objectively the performance of these methods. \nOverall the manuscript has a good presentation, and its content, to great extent, is clear and easy to follow and good formal preentation of the proposed method is given. I appreciate that the experiments section covers two datasets and various landmark methods for model explanation.\n\nMy main concerns with the manuscript are as follows:\n\nIn several parts of the manuscript they were sentences refering to some explanation methods as ables to reflect/diagnose the \"logic of a CNN/model/network\". I could not help but all the time that I encountered the expression \"logic\" ideas related to reasoning or internal decision-making process came to my mind. This really affected the flow of the content, and with all due respect, none of the explanations generated by existent methods is able to faithfully reflect the decision-making process of the models being explained. The majority are oriented towards generating approximations and indicating relevant regions of the input data.\n\nAlong the direction of objective evaluation of explanation methods, Oramas et al. ICLR'19 and more recently \nYang and Kim, arXiv:1907.09701 proposed methodologies aimed at quantitative evaluations of explanation methods. \nGiven the common objectives that these works have with the manuscript, it would be strenghten the manuscript to positition itself wrt. these methods in the related work section.\n\nIn Sec.3.3 the proposed method quantifies the amount of unexplainable regions, i.e. regions with low absolute attribution, of different methods. However, unless I missed something, it seems that these unexplainable regions are native of the explanation methods but native by the masking step introduced by the proposed metric.\nIn addition, this metric relies on a pre-defined threshold which determines the number of pixels to be considered. Could you motivate how this threshold value was selected? what would be the effect of selecting different values?\n\nThe metric proposed in Sec. 3.4 works under the assumption that a explanation method which is robust to spatial masking, can be considered more convincing. More specifically, it assumes that explanations from occluded versions of the same input should be similar. Do this metric takes into account the prediction $y=F(\\hat{I})$ produced by the model to be explained? if these predictions differ, assumming that the explanations should be similar might not be correct. Could you comment on this?\n\nThe multual verification metric proposed in Sec. 3.5: assumes that methods for explanation are more reliable if they produce similar heatmaps. I think this assumption might be incorrect. For instance, Adebayo et al., NIPS'18 showed that several methods were able to produce very similar explanations. Yet, all these explanations were wrong since they were all biases towards edge-like structures of the input data and were not faithful to the decision-making process of the model being explained. Under the proposed mutual verification metric, these biased methods would have been flagged as objectively reliable.\n\nIn the evaluation, when conducting experiments on PascalVOC 2012, could you motivate why cropping the images is necessary? Isn't this removing some background clutter that could pose an interesting scenario on which the performance of explanation methods could be analyzed?\n\nThe manuscript had a good detailed and motivated beginning. In comparison to its beginning its conclusion was somewhat abrupt. Discussions presented in the evaluation section related to the four proposed metrics were very shallow and most of the time pointed towards the supplementary material. In this regard, it is not clear how much of an insight we got from the evaluated explanation methods. Moreover, having this relevant part in the supplementary, gives the impression that the manuscript is not self-contained. Perhaps some re-organization of the content might be needed.\n\nFinally, the manuscript claims the proposed method to be sufficiently general to be applied to different method. However, as was evidenced in the experiments, there were some methods, e.g. CAM, grad-CAM, Pert, where some of the proposed metrics were not applicable. Therefore the claims related to the generality of the proposed method should be relaxed."}