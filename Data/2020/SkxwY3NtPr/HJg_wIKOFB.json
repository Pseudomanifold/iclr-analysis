{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper considers the important problem of evaluating explanation methods for neural models without recourse to `ground-truth' explanations. \n\nThe authors rightly highlight that plausibility (to humans) does not imply faithfulness to the underlying models. They then define a suite of metrics, largely drawing upon known techniques, to evaluate explanations. For instance, the authors propose evaluating the agreement between model attributions and Shapley values. The latter are approximated in two ways: Only pixels with high (or low) attribution values are considered; and sampling is used to make estimation tractable.\n\nOverall, this is a nice paper that investigates an important problem --- I particularly appreciated the comparison of multiple attribution methods across these suite of metrics. However, the results are something of a mixed bag, and I was left without a clear takeaway. I am not sure the ICLR audience will get much out of this submission. It would be nice if the authors could provide additional guidance concerning the interpretation of the various metrics they propose, and for which scenarios they view these as appropriate. \n\nStrengths\n---\n+ The topic of explainability is of increasing importance, and the authors focus on an important question regarding this: How can we evaluate attribution methods? \n+ The approach of defining a suite of metrics, rather than only one, is welcome given that different applications will call for different types of attribution. \n+ The comparison of existing methods for attribution across a range of metrics is a nice contribution. That said, the results are sort of a mixed bag and it is not obvious what one should take away from this. \n\nWeaknesses\n---\n- Much of this work (and the proposed metrics) seems to derive from prior efforts. I'm not sure how much novelty there is here, although there is a value in bringing these metrics together.\n- To the latter point, however, the authors do not really speak to the pros, cons, and applicability or relevance of the different metrics considered. A user of this suite of metrics would therefore be left with four separate measures, and it is not immediately clear how to interpret these, or what the relative performance of explainability methods with respect to these metrics tells us. \n- I am not sure I followed the unexplainable feature component metric (3.3). I found the notation and terminology here difficult to parse; can the authors please clarify what they mean by \"unexplainable feature components\", and also clarify what $f_I$ represent (I think induced feature representations of the image I but again am not certain). From what I gather this is somehow measuring the peakiness (or lack thereof) of attribution values, but it is not obvious to me that this is an inherently desirable property.\n- The robustness metric seems to make the assumption that the model does not capture interactions, i.e., a pixel j may be important only when seen together with some other pixel i. I'm not sure this assumption is warranted; perhaps the authors can elaborate to provide further intuition.\n\n"}