{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "The paper proposes an energy based model for text. Since energy-based model requires two kinds of examples (positive and negative) with negative examples as being harder to mine, the paper proposes to use existing language models as a source of negative examples. \n\nI like the empirical analysis for generalization in the paper. This is novel, rigorous, and interesting. But I have two main concerns:\n\n1. The general idea of the paper is however not new. There have been at least 4 works for post-hoc training of classifiers to correct for mistakes of generative models using resampling based methods. These have been applied to image models, but the methods presented in these works are general purpose. Unfortunately, none of these works have been cited.\n- [1] proposes a boosting procedure based on classifier training over an existing generative model (Section 3.2)\n- [2, 3, 4] propose different resampling methods (rejection sampling, MCMC, importance weighting) for improving the base generative model via post-hoc training of a binary classifier.\n\n2. Casting this as an energy-based model is not very interesting if we only care about classification accuracy. A generative model permits many more inference tasks; likelihood evaluation or sampling. These are intractable for the current EBM I believe and I wish there was a more rigorous discussion around this. [4] for example formalizes the induced EBM and presents a particle based approximation for sampling from such an EBM (Section 4). Similarly, EBMs can be used for tasks requiring any task requiring relative likelihoods e.g., inpainting, denoising etc. Would have been great if some experiment along those lines was included to truly stress on the generative modeling capabilities of current EBM.\n\n- I am also curious of the training metrics for the classifier. What happens if you use a non-probabilistic classification loss (e.g., hinge loss)? \n- Also, what happens when the base language model doesn't have full support? Would the distribution of negatives of the EBM be biased?\n\nAuthors are recommended to clarify the above questions, clearly discuss relationship with existing work and analyze the EBM formulation in greater detail during the rebuttal period. \n\nReferences:\n1. Boosted Generative Models.  AAAI 2018.\n2. Discriminator rejection sampling. ICLR 2019.\n3. Metropolis-hastings generative adversarial networks. ICML 2019.\n4. Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting. NeurIPS 2019.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}