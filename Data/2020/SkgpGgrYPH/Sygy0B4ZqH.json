{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper discusses training an energy-based discriminator to discriminates the generated examples from a language model from human-written sentences.  To do this, it assigns lower energy value to the real text and higher energy values to the generated text, by optimizing binary cross-entropy.\n\nThe motivation of the paper claims that the authors solve (or at least move toward solving) the problem of sampling from energy-based models for high-dimensional sequence problems; however, I do not see any contribution towards that. \n\nThe importance of having a well-trained energy-based model is its modeling capability, which also enables us to sample from it (that is done using gradient descent and Langevin dynamic for modeling images). But here, the energy-based model has no effect on the quality of samples from the auto-regressive model, so it does not give us a better model for text. It would be similar to EBGAN (Zhao et al., 2017), but only training the energy discriminator and keeping the generator as is (here a pretrained LM).\n\nThis work would become an interesting and influential work if you can update the generator parameters from the feedback of the energy-based model, so the generator works as a sampling process for the energy-based model (which was what I expected while I was reading the first part of the paper). "}