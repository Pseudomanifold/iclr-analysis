{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to generating negative samples for EBMs using pre-trained auto-regressive language models, and the trained EBMs (called the residual EBMs) are used for real/fake text discrimination.\nThe generalization ability of residual EBMs is thoroughly evaluated.\n\nIn generally, the paper is well motivated and well written. But I have some concerns.\n\n1. Important relevant references are missed.\n\nEBMs (a.k.a. un-normalized models, random fields) have been successfully applied to model text sequences in recent years. Connecting and comparing to these previous works are needed.\n\n[1] R. Rosenfeld, S. F. Chen, and X. Zhu, \u201cWhole-sentence exponential language models: a vehicle for linguistic-statistical integration,\u201d Computer Speech & Language,  2001.\n[2] B. Wang, Z. Ou, and Z. Tan, \u201cTrans-dimensional random fields for language modeling,\u201d ACL, 2015.\n[3] B. Wang, Z. Ou, and Z. Tan, \u201cLearning transdimensional random fields with applications to language modeling,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.\n[4] B. Wang and Z. Ou, \u201cLanguage modeling with neural trans-dimensional random fields,\u201d IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.\n[5] B. Wang and Z. Ou, \u201cLearning neural trans-dimensional random field language models with noise-contrastive estimation,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n[6] B. Wang and Z. Ou, \u201cImproved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation,\u201d IEEE Spoken Language Technology Workshop (SLT), 2018.\n\n\n2. Some unclear issues:\n\nIt is not very clear how the trained EBMs are used to do real/fake text discrimination specifically.\n\nThe energies used in Eq.(1) are in the form of conditional energies. It is not very clear how the conditional input is fed into the EBM architectures in Section 4.3.\n\n3. I am a little bit concerned that the theoretical contribution seems weak, though the application of applying EBMs in real/fake text discrimination is novel.\n\nI'm happy to adjust the score if the paper can be better placed in the literature and the authors take efforts to improve the paper."}