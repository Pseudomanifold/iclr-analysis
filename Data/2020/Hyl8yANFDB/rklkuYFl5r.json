{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper studies the generalization property of DRL.  Fundamentally, this is a very interesting problem. The authors experimentally analyze this issue through the lens of memorization, and showed that it can be observed directly during training. This paper presents the measure of gradient update generalization, best understood as a side-effect of neural networks sharing parameters over the entire input space. This paper is very written, and well organized.  The experiments are quite solid. However  I may be not capable in judging the novelties and contributions of this paper, since I did not conduct research on this topic.\n"}