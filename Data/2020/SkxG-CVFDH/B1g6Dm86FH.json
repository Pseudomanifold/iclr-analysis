{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors developed GNN by integrating an interpolation based regularization. In particular, the proposed GraphMix model has two components, one GNN and one MLP. For the MLP, the manifold mixup loss is used for training. The two components share parameters so that the GCN training can be regularized by the manifold mixup loss. Moreover, the predicted samples of GCN are used to augment the data of MLP for training in a self-supervised manner. In the experiments, GraphMix was compared with several recent GNN models on several benchmark datasets and demonstrates effectiveness for node classification and link classification.\n\nThere are several concerns on this paper.\n1. The motivation to integrate interpolation based regularization for GNN is not intuitively clear. The authors may want to further clarify on how can Eq. (2) help improve the classification power of GNN in an intuitive manner.\n2. Some algorithm design choices, such as sharpening for entropy minimization, are not well justified. It is not clear why sharpening is selected over psudolabels.\n3. The overall technical contribution is somewhat incremental based on previous work on GNN and manifold mixup based regularizations. The model design is heuristic, and lack theoretic analysis, for example, on the parameter sharing based regularization over other choices of regularizations, if any.\n4. From the experimental results, the proposed model improves GCN and GAT, but the improvements on other baseline methods are a little bit subtle. The authors may want to further evaluate their framework on those GNNs such as GMNN and Graph U-Net to make the experiments more comprehensive, or justify on why they cannot be implemented using the proposed framework.\n5. In Table 2, 3, and 4, it is not clear on why GAT and GraphMix (GAT) are missing. This inconsistency with regard to Table 1 should be justified.\n6. In the ablation analysis, if the reason for the suboptimal performance when using EMA is the lack of hyperparameter search, then it is suggested to perform comprehensive hyperparameter search so that the results are more solid. Otherwise, the design choice cannot be well justified.\n"}