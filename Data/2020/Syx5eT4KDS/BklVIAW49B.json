{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to learn classifier outputs for meta learning in the form of factorized discrete codes by maximizing the mutual information between the model's outputs and the ground truth labels. The authors further present an information theoretic generalization bound for meta learning in terms of the number of tasks, number of training samples per task and the expressively of the model. They further show empirically that their approach does not need a separate query set during meta-training and can generalize better than a few of the other metric-learning based meta-learning approaches specifically at lower shot values. They show that their method requires less memory than the N-pair meta-learning method.\n\nThe paper addresses an important problem of generalization with very low shot values and proposes an interesting theoretical treatment of the problem in terms of deriving an information theoretic lower bound for it. I liked the authors' theoretical treatment of relating various metrics to the mutual information between the models' outputs and their labels. \n\nHowever, there have been many recent works attempting to address the problem of better generalization of meta learning models. Most notable among them is the work of Kim et al. \"Bayesian Model-Agnostic Meta-Learning\" (https://arxiv.org/pdf/1806.03836.pdf), which is not referenced or compared against in this paper at all.\n\nThe proposed method is also limited in its scope to classification tasks only and the authors make no attempt to address regression or reinforcement learning problem, which limits is widespread applicability.\n\nWhile the authors address the generalization of meta-learning methods for small values of M, they do not address how their model behaves viz-a-viz others when the number of tasks N available for training is small. Theoretically having more compact representations should also help in situations where the number of tasks available for training are small. I would like to see an empirical analysis of that as well.\n\n"}