{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes to maximize improve generalization in meta-learning by learning discrete codes via a mutual information maximization objective. I liked the motivation and presentation of the paper but see some critical shortcomings:\n\n- In Theorem 1, shouldn't there be a term that accounts for the complexity of the hypothesis class (VC-dim, Rademacher complexity etc.). Can we actually verify Theorem 1 in practice on synthetic distributions to get a sense of the constant terms?\n- The experiments do not compare with any mutual information related baselines. Eg, VIB [Alemi et al.]. This comparison is critical to stress the importance of discrete codes as opposed to continuous. \n- Can the authors shed more light on the tradeoffs between p and d? Empirical insights would lend more intuition and understanding.\n- Discreteness in activations is one form of regularization to reduce |tilde(x)|. Would regularizing the weights of the stochastic encoder (say by variational inference or minimizing say l2 norm) have the same/better/worse regularization effect (as done in Bayes by Backprop)?\n- There are a few missing references on stochastic encoders trained based on variational information maximization [1, 2]. \n\nReferences:\n[1] Uncertainty Autoencoders: Learning Compressed Representations via Variational Information Maximization. AISTATS 2019.\n[2] Neural Joint Source-Channel Coding. ICML 2019.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}