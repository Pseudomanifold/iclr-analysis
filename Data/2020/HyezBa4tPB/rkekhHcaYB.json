{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for learning a \u201cwrapper\u201d model which endows a multiclass predictor with an estimate of model uncertainty. The base model is treated as a black box which emits a categorical distribution, while the wrapper model estimates the parameters of a Dirichlet distribution. The wrapper is trained against silver labels from the base model, and the sampled predictive entropy is used to threshold predictions so as to withhold a decision on uncertain examples.\n\nI believe this paper should not be accepted, as the main contribution is not particularly novel and some experimental details (Section 3) are not well-motivated. \n\nIn particular, the idea of learning a Dirichlet prior is very similar to that proposed in (Malinin and Gales, NeurIPS 2018). The main contribution in this work seems to be the application to an existing black-box model, but this seems like a straightforward application of knowledge distillation, another well-established technique (e.g. Hinton et al. 2015, though oddly, this paper doesn\u2019t mention this).\n\nThe details of the modeling set-up (section 2.2 and 3) are also not entirely clear. It seems from the preceding discussion that the main goal of the wrapper model is to estimate \\beta, but it is not clear how the sampling procedure (2.2) allows for this. It seems that the value of the \u201csampled predictive entropy\u201d (no equation number, but see end of Section 3) is just that of the mean predicted distribution, and it\u2019s not clear why this should be different (assuming the wrapper model converges) than the predictive entropy of the base model.\n\nThis paper could be improved by a more thorough comparison to the literature, and by a clearer motivation for the training procedure used. Additionally, it would help to present a summary table of results, and to frame the metrics in terms of standard classification metrics such as precision and recall where applicable.\n"}