{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Notes: \n\n  -Goal is to study the \"active subnetwork\" of Relu based networks for interpretability.  \n\n  -The question of interpretation seems rather thorny.  \n\n  -In Figure 4, the result for Insens seem alright, although it's weird that the data is just mnist digit / noise.  I feel like something with multiple objects would make it much clearer if there is an actual improvement?  For example on Figure 5 I'm not really sure if Insens is better.  The results often look worse to me than \"DeepTaylor\", especially on CIFAR10.  \n\nReview: This paper proposes to improve the interpretation of relu based networks by considering the \"inactive network\" which could potentially become activated by local perturbations instead of just considering the active part of the network (which is locally linear).  I think this is a step in the right direction for the interpretation of relu based networks, although the results are somewhat borderline.  \n\nAdditionally the tasks could be much better, to show situations where an object is present but which is not related to the labels.  This would provide a much clearer test of the model's capabilities.  \n\n"}