{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a one-pixel signature of a CNN classifier, which the paper suggests can be used to detect classifiers that have been trojaned -- meaning that unknown to the CNN provider, the CNN developer has put in a weakness so that the CNN will systematically misclassify if you add a marker to an image, such as a red diamond in the top left. The signature is computed by finding out for every pixel and every class, what input value would most change the class assignment.\n\nThis really isn't an area that I know well, but here are my thoughts:\n\n- From the distance at which I'm working, all 4 of the listed contributions of the paper seem to be basically the same contribution, which is perhaps best expressed as the second contribution. That is, I take section 5.2.3 as the key results of the paper.\n\n - The signature idea seems to basically work, and can detect attacks of the sort described. It can also be used for other things, such as distinguishing a ResNet vs. AlexNet classifier when you're treating it as a blackbox, but I'm not sure how useful this is.\n\n - Indeed, I'm not really sure how useful this line of work is period, but I guess I can see some applications for making sure that no one has monkeyed with a classifier that is being provided to an end user by a third party.\n\n - Computing the signature still seems to me like it could be pretty expensive (image height x width x num channels x num input values), though they claim that's not so bad.\n\n - It seems to me that there are lots of other things you might do to create a signature of a classifier, like looking at it's response to a set of test patterns. I wonder whether they mightn't work just as well as the method presented here, though I really have no idea.\n\n - But this does seem a real weakness of the paper: There are no baselines. If the argument is that this is a good method of backdoored classifier detection, and given that there is not at present any significant literature on this problem, surely the reader deserves to be shown a few candidate simple baseline methods for backdoored classifier detection, both to prove that the problem is not generally trivial and that the method advanced here works significantly better than these baselines.\n\nSo, overall, this paper wasn't clearly interesting and well-done to me, and so I lean to leaving it out, but someone invested in the area might feel differently."}