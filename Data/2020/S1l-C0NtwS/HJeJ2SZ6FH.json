{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper compares two approaches to learn cross-lingual word embeddings: cross-lingual alignment (where separately trained embeddings in different languages are mapped into a shared space) and joint training (which combines the training data in all languages and jointly learns a cross-lingual space). The authors argue that each approach has its own advantages, and propose a \"unified framework\" that essentially applies them sequentially (first train jointly, and then further align them after the necessary vocabulary reallocation).\n\nI have generally positive feelings about the paper. To be honest, I do not like the way the authors frame their work (e.g. the way the method is motivated in Section 2.3 or calling it a \"unified framework\"), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.\n\n\nStrengths:\n\n- Good coverage of related work, including recent publications.\n\n- Thorough evaluation in 3 different tasks, much better than what is common in the area (usually limited to BLI).\n\n- The authors experiment with contextual embeddings in addition to conventional word embeddings.\n\n- Generally convincing results with relevant ablations and reasonable baselines.\n\n\nWeaknesses:\n\n- I feel that framing this as a \"unified framework\" for cross-lingual alignment and joint training is going a bit too far. The proposed method is as simple as first doing the joint training and then the cross-lingual alignment (with a special treatment for vocabulary reallocation). This is just a sequential application of two class of methods, and not what I would understand as a \"unified framework\" (which suggests some form of generalization or at least a closer interaction to me). At the same time, the only \"joint training\" that the authors explore is training regular embeddings over concatenated monolingual corpora, which as far as I know has only been explored by Lample et al. (2018), and I would not consider as a representative example of this family of methods.\n\n- It is not clear how the different vocabulary spaces are handled in BLI. My understanding is that if the query is in the source vocabulary subset the retrieval is done over the target subset, and if it is in the shared subset it is the same query word that is returned, but this is not clear at all.\n\n- I think that the issue of \"oversharing\" is magnified in the paper. I understand that this is directly connected to the reallocation step in the proposed method, and it of course makes sense to also map words that predominantly appear in a single language. However, in relation to the downstream tasks themselves, oversharing only seems relevant for BLI, where one needs to delimit the retrieval space and avoid returning the query word (which is of course its own nearest neighbor) unless it actually exists in the target language. This is connected to my previous point, and I think should be discussed in isolation.\n\n\nOther minor things to improve:\n\n- Please make Figure 2c an actual table instead of an image."}