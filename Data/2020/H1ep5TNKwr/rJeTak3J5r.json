{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach to learning embeddings associated with nodes in a graph. Inspired by Hebbian learning, the representations of a node are iteratively updated to be similar to representations of its neighbors. The update is performed by adding a scaled vector sampled from a Gaussian centered on a neighbor's representation to the current node's vector. After learning, the embeddings may be used for tasks such as graph reconstruction, link prediction, or product recommendation.\n\nContributions include:\n* Proposal of an approach to learn embeddings of nodes in a graph in which representations of a node are updated by scaled and Gaussian-perturbed versions of its neighbors' representations.\n* Experiments demonstrating reconstruction and link prediction performance of the proposed approach on several datasets, as well as product recommendation perfomance on a large retail dataset.\n  \nThere are several significant concerns with the paper as it currently stands. The three most pressing issues are as follows:\n1. The proposed algorithm is not situated relative to related work.\n2. The paper does not provide the reader with enough details or precision to be able to replicate the work.\n3. Experiments do not compare to any baselines other than random embeddings.\n  \nThe paper suggests that the proposed algorithm is a form of Hebbian learning because the representation of nearby nodes in the graph are encouraged to be similar. However, this idea has long been used for learning node embeddings (for example, LLE encourages representations of a node to be predicted as a linear combination of neighboring nodes). The connection seems loose other than a superficial similarity in the update rule and naming the algorithm after Hebbian learning is somewhat misleading.\n\nThe algorithm is reminiscent of message-passing inference in a continuous Markov random field with pairwise potentials encouraging nearby nodes to have similar representations. I am not an expert in graph embedding approaches, but I would be surprised if the approach could not be easily related to classical approaches such as MDS or LLE.\n\nThere are several notational/clarity issues:\n* j is used for both the node index and the embedding of the node itself (equation 1-2). Replacing the j on the left hand side with w_j would resolve ambiguity and bring the equations in line with Algorithm 1.\n* Equation 2 is inconsistent with equation 1 because they both specify different distributions over the same embedding. Framing equation 1 as an initialization and equation 2 as providing the conditional distribution p(w_j | w_i) may make the situation clearer.\n* Equation 3 is a mixture of Gaussian distributions, yet \\delta_j is a vector added to the current node embedding. Instead, first write \\tilde{w}_i as a sample from the Gaussian and then let \\delta_j be the weighted sum of the samples.\n* Equation 3 is not consistent with equations 4-5. Equation 3 suggests that a node is updated by summing over neighbors and then applying the update. But Algorithm 1 suggests that nodes are updated based on only a single neighbor at a time.\n* What does it mean when the negative embedding is propagated with a small transition probability? This should be described mathematically.\n* It is misleading to call the graph a Gaussian hierarchy, since a hierarchy implies that certain nodes are higher than others.\n* How are the \"transition probabilities\" set for an unweighted graph? Specifically, the GrQc dataset doesn't appear to have edge weights.\n* What values of the variance and \\tau hyperparameters were used?\n* How are reconstructions and link predictions computed?\n  \nExperimentally, the proposed approach is not compared to any baselines other than random embeddings. The claim made in the paper that the method compares favorably is thus not backed up by results. The results in section 3.2 should be described in greater detail. If the items are nodes, then how are edges and weights determined?\n\n Other specific comments:\n* What is the connection between the current work and hyperbolic geometry of Nickel & Kiela (2017)? The proposed algorithm does not rely on hyperbolic geometry so this seems like a non sequitur.\n* Algorithm 1: Rather than describing the algorithm in terms of the intended application (products), it would be useful to describe it in general terms and then use retail products as specific application.\n* Figures 2 and 3 are not particularly useful. The most important information for the reader or practitioner is how various methods compare on the same dataset, not how a single method performs across different datasets.\n  \nQuestions for the authors:\n* How is the proposed algorithm similar/different to related approaches for learning node embeddings?\n* What are baseline results for related algorithms on the datasets experimented upon?\n* What is the role of the variance scaling? How do the results change if the variance is reduced to 0 immediately after random initialization?"}