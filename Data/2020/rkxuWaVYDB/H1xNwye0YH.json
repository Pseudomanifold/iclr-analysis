{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the design of adversarial policies (where the action of the adversarial agent corresponds to a perturbation in the state perceived by the primary agent). In particular it focuses on the problem of learning so-called optimal adversarial policies, using reinforcement learning.\n\nI am perplexed by this paper for a few reasons:\n1)\tWhat is the real motivation for this work?  The intro argues \u201ccasting optimal attacks is crucial when assessing the robustness of RL policies, since ideally, the agent should learn and apply policies that resist *any* possible attack\u201d.   If the goal is to have agents that are robust to *any* attacks, then they cannot be robust just to so-called optimal attacks.  And so what is really the use of learning so-called optimal attacks?\n2)\tThe notion itself of \u201coptimal\u201d attack is not clear.  The paper does not properly discuss this.  It quickly proposes one possible definition (p.4): \u201cthe adversary wishes to minimize the agent\u2019s average cumulative reward\u201d.  This is indeed an interesting setting, and happens to have been studied extensively in game-theoretic multi-agent systems, but the paper does not make much connection with that literature (apart from a brief mention at bottom of p.2 / top of p.3), so it\u2019s not clear what is new here compared to this.   It\u2019s also not discussed whether it would ever be worthwhile considering other notions of optimality for the adversary, and what would be the properties of those.\n\nSo overall, while I find the general area of this work to be potentially interesting, the current framing is not well motivated enough, and not sufficiently differentiated from other work in robust MDPs and multi-agent RL to make a strong contribution yet.\n\nMore minor comments:\n-\tP.3: \u201cvery different setting where the adversary has a direct impact on the system\u201d => Clarify what are the implications of this in terms of framework, theory, algorithm.\n-\tP.4: You assume a valid Euclidean distance for the perturbed state.  Is this valid in most MDP benchmarks?  How is this implemented for the domains in the experiments?  What is the action space considered? Do you always assume a continuous action space for the attacker?\n-\tP.5: \u201cwe can simply not maintain distributions over actions\u201d -> Why not?  Given the definition of perturbation, this seems feasible.\n-\tP.5:  Eqn 4 is defined for a very specific adversarial reward function. Did you consider others?  Is the gradient always easy to derive?\n-\tP.6: Eqn (5) & (6): What is \u201cR\u201d here?\n-\tP.7: Figure 1, top right plot. Seems here that the loss is above 0 for small \\epsilon.  Is this surprising?  Actually improving the policy?\n-\tP.7: What happens if you consider even greater \\epsilon?  I assume the loss is greater.  But then the perturbation would be more detectable?  How do you think about balancing those 2 requirements of adversarial attacks?  How should we formalize detectability in this setting?\n-\tFig.2: Bottom plots are too small to read.\n-\tSec.6:  Can you compare to multi-agent baselines, e.g. Morimoto & Doya 2005.\n-\tP.8: \u201cWe also show that Lipschitz policies have desirable robustness properties.\u201d Can you be more specific about where this is shown formally?  Or are you extrapolating from the fact that discrete mountain car suffers more loss than continuous mountain car?  I would suggest making that claim more carefully.\n\n"}