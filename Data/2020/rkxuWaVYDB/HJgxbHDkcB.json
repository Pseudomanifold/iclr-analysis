{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper investigates adversarial attacks on learned (fixed) policies. In particular, they devise optimal attacks in the sense that e.g. the agent\u2019s objective should be minimised by the attacker. It is assumed that the attacker can manipulate the observed state of the agent at each time step during testing in a restricted way such that perturbations are small in the state or feature space. The paper derives attacker value functions when the attacker\u2019s goal ist to minimise the original agent\u2019s average cumulative reward and shows how to maximise them using gradient methods. In order to show when attacks are more likely to be successful, a theorem is presented and proved that shows that the resilience of a policy against attacks depends on its smoothness. Experiments show that a) attacks that are trained for optimality w.r.t. to minimising the average reward of the agent outperform a baseline method that only optimises a related criterion. b) Agent policies that are trained with DRQN, yielding a recurrent controller, outperform non recursive ones. \n\nOverall, I think this paper can be a good contribution to the subfield of adversarial attacks on MDP policies. In my view, the derivations and overall structure and results of the paper are sound. However, I would have liked to get a better understanding and motivation for the investigated problem setting, such as projected applications and state or features spaces that could be manipulated in the proposed way.\n\nPros:\nPaper is well written and generally easy to follow.\nThe derivations look sound and are well motivated. Proposition 5.1. can help in understanding how to train resilient agent policies. \nExperiments confirm that optimising for optimal attacks does indeed find empirically better attack strategies. \n\n\nCons: \nNo real application was presented. While applications are imaginable, a real application would have been beneficial. In particular, getting an idea of how real pertubations may look like in a realistic domain. The same holds for the assumption that the state (or features) can be manipulated at each time step, but only slightly. \nThe implications of Proposition 5.1. were never tested, as far as I understand. You only test DRQN vs DQN, which validates your POMDP assertions, but other comparisons are missing.\nThe tested Open AI Gym environments are very basic, with the exception of Pong. This reiterates my application argument.\n\nNotation/Writing: \nOn page 5, last paragraph you should use a different letter for J in order to reduce notational confusion.  \nSection 6 is not on the same standard as the other sections in terms of writing, e.g. 'We have obtained from 3 policies\u2018 , some stray \u2019the\u2019, some plural vs. singular issues. \n"}