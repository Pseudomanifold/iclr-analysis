{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe authors of this paper propose a novel adversarial attack for deep reinforcement learning. Different from the classical attacks, e.g., FGSM, they explicitly minimize the reward collect by the agent in a form of Markov decision process. Experiment results demonstrate that the proposed approach can damage the well-performed policy with a much bigger performance drop than gradient-based attacks.\n\nPaper strength:\n\n1.\tThe paper is well-organized and easy to follow.\n2.\tModel the adversary of reinforcement learning (RL) system as another MDP and solve it with RL is novel and interesting. The proposed attacking diagram can be devised in a pure black-box setting and also can be incorporated with white-box attacks. \n3.\tWith such a strong attack, the authors derive an upper bound on the impact of attacks and shed light on new research studying the robustness of deep RL approach.\n\nPaper weakness:\n1.\tThe author should give more details about how you use a gradient-based exploration to guide the adversary. From my point of view, I think the black-box attack is more practical and interesting. I would like to see the clearer comparison of optimal attack in a pure black-box setting with gradient-based attacks.\n2.\tThough FGM is not as efficient as the proposed optimal attack, they are simpler than a learning-based approach. Please describe the details and cost of training the attack agency, e.g., the hyper-parameter, number of training iterations.\n3.\tWhile the conclusion of smooth policies is more resilience for adversaries is interesting, I would like to see the evaluation results of such a novel finding. \n"}