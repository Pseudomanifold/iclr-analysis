{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors propose RPR - a way to randomly partition and quantize weights and train the remaining parameters followed by relaxation in alternate cycles to train quantized models. The authors show the results on latest network architectures on ImageNet dataset - ResNet18, ResNet-50 and GoogLeNet and the results either compare to other quantization methodologies and in the case of GoogLeNet they surpass the state of the art ADMM method by 1.7% top-1 accuracy\n\nThe current experimental section does not have any runtime or model size numbers, it would be great if authors could talk about that and maybe add that information to get a more apples to apples comparison on not just accuracy but other factors.\n\nAlso, while Figure1 is a useful visualization of how weights change in the RPR method what is missing in the paper is a more technical/mathematical explanation of the procedure as well as detailed discussion on the hyper-parameters and how things change if we choose a different ratio for the partitioning. \n\nAs it is, the paper currently is easy to read, seems like a good idea and has good results but I feel it is important to add more detail in both methodology and in experimental results especially have more ablations and understand what is truly happening to the model."}