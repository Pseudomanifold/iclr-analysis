{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose Random Partition Relaxation (RPR) to quantize the network parameters to binary or ternary value. The key idea of the proposed method is to randomly keep small fractions of full-precision weights during the training of the quantized network. In this way, small fractions of the full-precision weights can compensate for the accuracy loss from the quantization. Experimental results on ImageNet show that the proposed RPR achieves comparable performance. However, the novelty and contribution of this paper are limited. My details comments are as follows.\n\nStrengths:\n1.\tThe authors transform the quantization to a mixed-integer non-linear program (MINLP) problem.\n\n2.\tThe authors propose to solve the MINLP problem through alternating optimization. Specifically, the authors propose Random Partition Relaxation (RPR) to randomly relaxing the quantization constraint on small fractions of the weights.\n\n3.\tExperimental results on ResNet-18 and ResNet-50 show the effectiveness of the proposed method.\n\nQuestions and points needed to be improved:\n1.\tThe novelty and contribution of this paper are limited. The idea of the proposed method is the same as INQ [1]. Both methods split the network into two disjoint groups and only quantize one group of weights. Base on INQ, the authors only make a small improvement. Therefore, the proposed method is incremental and the contribution is limited.\n\n2.\tThe definition of the quantization function is not clear. How to quantize the network parameters into binary or ternary value? \n\n3.\tThe effect of the initialization in Section 3.2 is unclear. No ablative experiments are conducted to show the effectiveness of the initialization.\n\n4.\tThe comparisons in Table 1 are unreasonable. INQ [1] quantizes all layer of the network. However, the proposed RPR does not quantize the first and last layers in the network, which reduces the error brought by quantization. \n\n5.\tThe comparisons in Table 1 are unfair. The performance of the baseline model (Top-1 Error: 69.76%) is better than the competitors (. Hence, directly report the accuracy of the quantized model is unreasonable. More results regarding the accuracy gap are required.\n\n6.\tThe results in Table 1 are not convincing. For ResNet-18 and ResNet-50, ADMM[2] achieves better performance than the proposed RPR. However, for GoogLeNet, the performance of ADMM is worse than the proposed RPR. More explanations on these phenomena are required.\n\n7.\tDoes the initial value of freezing fraction (FF) affect the performance of the proposed mothed? More ablative studies on this term are required.\n\n8.\tSome mentioned methods lack references, such as the straight-through estimator (STE).\n\nMinor issues:\n1.\tIn page 1, \u201ccompute cost\u201d should be \u201ccomputation cost\u201d.\n\nReferences:\n[1]\tAojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. In Proc. ICLR, 2017.\n[2]\tCong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with ADMM. In Proc. AAAI, pp. 3466\u20133473, 2018. ISBN 9781577358008.\n"}