{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a method for weight quantization in neural networks, RPR (Random Partition Relaxation) which extends previous approaches for weight quantization. The difference to previous approaches is that RPR iteratively selects a random fraction (the constrained weights) of continuous weights (called relaxed weights) to quantize during optimization and increases this fraction over time until all weights are constrained, while maintaining the ability to use SGD as the main workhorse for optimization.\nThe authors demonstrate cases for constrained weights to binary and tertiary cases while keeping continuous weights for input and output layers.\nDuring experiments the authors show that (a) their method is competitive on most benchmarks with simpler methods but can be beaten in some conditions and (b) RPR yields strong results for the GoogleNet architecture.\nThe paper overall is written concisely and well organized, though it is light on technical formalism and generalizable technical insights.\n\nComments:\n1. How is training evaluated here? Do the authors repeat training K times (over i.e. random seeds)? The results on testing contain no error-bars to evaluate if the experiments were lucky or are reliable. This is particularly important to show the value of an iterative optimization scheme as chosen here since 'lucky' runs can dominate results. I would find this a necessary requirement here to trust the results and to understand how much variance the iterative optimization and the masking process introduce to the training.\n\n2. How robust is the method to the scheme for setting FF (fraction of constrained weights) vs learning rate? How much damage would strange settings do to the model? Is there a reliable heuristic for picking the scheme? There is not nearly enough information on this.\n\nReferences:\nIn ICLR last year, Frankle et al introduced the Lottery ticket hypothesis. In this paper, progressive masking and retraining of NNs is studied as a mechanism for sparsification and masking is heuristically performed by quantizing weights close to 0 (by some threshold) to the value 0.\nIt would be worthwhile for the authors to connect their idea to these observations and to include a discussion of those results, as it seems to indicate that quantization/sparsification/masking over iterations of SGD training share useful dynamics here.\n\nDecision:\nAs the paper stands I find it technically and experimentally not mature enough. The process is certainly interesting, but seems neither empirically nor theoretically studied well enough in the current form."}