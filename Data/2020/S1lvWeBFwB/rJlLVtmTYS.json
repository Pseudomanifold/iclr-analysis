{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThe paper proposes a new training scheme of optimizing a ternary neural network. In the proposed method, during the training phase, a randomly selected portion is kept with full precision and the rest of the weights are quantized. The two steps are conducted iteratively while in each iteration, the portion of the full precision weights, termed as freezing fraction (FF), is decreased until 100%.\nPros:\n\u2022\tThe proposed method is interesting. Adding a small portion of full precision weights during the quantization could potentially improve the model performance.\nCons:\n\u2022\tThe paper is not well written.\no\tIntroduction does not introduce the proposed method and the contributions. \no\tMany statements in the paper are not supported by any evidence.\n\u2022\tIn the paper, it states that \u201cComplex network compression schemes cannot be applied at this point as decompression is often a lengthy process requiring a lot of energy\u201d. However, this is not supported by any evidence. Indeed, many compression methods do not need to be reconstructed during inference time.\n\u2022\tIn the method section, the paper is claiming that performing GD for one full epoch before advancing to next iteration can speed up the convergence. However, this is not supported by any experiments or theoretical proof.\n\u2022\tIt is not clear about the benefits brought by using FF to quantize the network iteratively. The implementation of the brute force search methods may also improve the model performance since the experiments are all conducted on pertained models. \n\u2022\tIt would be interesting to see some results on 8 bit quantization or any other bit width that the quantization cause no performance drop. My concern is that, since the proposed method using brute force method, larger bit width may potentially take longer time for the optimization based on Eqn. (4). Hence, it would be clear to have some experiments on the optimization process.\n\u2022\tThe improvements brought by the proposed methods are not significant based on the  given experiments result.\n"}