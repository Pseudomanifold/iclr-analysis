{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to learn a Transformer in embedded spaces defined via $m$ hash functions, with application to (fixed length) sequence-to-sequence classification for NLP. The method differentiates itself, in large part, by formulating model outputs in terms of $m$ distinct softmax layers, each corresponding to a hash of output space $\\mathcal{Y}$. The relative low dimensionality of each hashed output space $d \\ll \\vert\\mathcal{Y}\\vert$ helps to avoid computational bottlenecks associated with wide softmax layers.\n\nFor a given labelled pair $(\\mathbf{x}, y)$, the model outputs a matrix $\\mathbf{P} \\in \\mathbb{R}^{m \\times d}$, whose $i$-th row is trained using the $i$-th hash $h_{i}(y)$. At test-time, predictions are made by finding tokens $y^{*} \\in \\mathcal{Y}$ whose hashes best 'align' with model outputs $\\mathbf{P}$. By way of example, target alignment might be defined in terms of the aggregated log likelihood\n\n    $\\ell(y; \\mathbf{P}) = \\sum_{i=1}^{m} \\log(p_{i, h_{i}(y)})$,\n\nwhere $p_{i, h_{i}(y)}$ can be understood as the element of $\\mathbf{P}$ corresponding to the $i$-th hash of token $y$.\n\nAt this time, the work is significantly hindered by a lack of clarity. This issue begins with the chosen notation, which routinely obscures otherwise simple points. Similarly, the section on test-time predictions (Sect 2.5) is needlessly hard to follow. Given its pivotal role, this section feels strangely rushed. Some unanswered questions I had include:\n  a) What happens if two tokens' hashes collide all $m$ times?\n  b) How were hash functions inverted and what was the cost of doing so?\n  c) How does the test-time throughput of the proposed compare with that of alternatives?\n\n\nQuestions:\n  - Did you compare against different approaches to sparse softmax, such as LSH-based methods [1]?\n  - What was the impact of approximate vs. exact inference and which was used during experiments?\n  - How important were embedding matrices $E^{I}, E^{O}$? What happens if you directly feed hashes into the Transformer or use random projections for $E$?.\n\n\n[1] \"Deep Networks With Large Output Spaces\", Vijayanarasimhan et al, 2015"}