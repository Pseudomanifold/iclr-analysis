{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the task?\nComprehensive study of the contribution of different components in Multilingual BERT to its cross-lingual ability.\n\nWhat has been done before?\n(Wu & Dredze, 2019) and (Pires et al., 2019) identified the cross-lingual success of the model and tried to understand it. However, both works treated M-BERT as a black box and compared M-BERT\u2019s performance on different languages. This work, on the other hand, examines how B-BERT performs cross-lingually by probing its components, along multiple aspects. Some of the architectural conclusions have been observed earlier, if not investigated, in other contexts.\n\nAuthors claim that \u201cPires et al. (2019) hypothesizes that the cross-lingual ability of M-BERT arises because of the shared word-pieces between source and target languages.\u201d is not entirely correct. Pires et al. (2019) did show M-BERT\u2019s ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap. There were results (e.g. Figure 1) showing that while performance using EN-BERT depends directly on word piece overlap, M-BERT\u2019s performance is largely independent of overlap, indicating that it learns multilingual representations deeper than simple vocabulary memorization.\n\nPires et al. (2019) also showed that structural similarity is crucial for cross-lingual transfer\n\nWhat are the main contributions of the paper?\nFirst comprehensive study of the contribution of different components in Multilingual BERT to its cross-lingual ability. Novel findings about the effect of network architecture, input representation and learning objective on cross lingual ability of M-BERT\nMethodology that facilitates the analysis of similarities between languages and their impact on cross-lingual models by mapping English to a Fake-English language, that is identical in all aspects to English but shares no word-pieces with any target language.\n\nWhat are the key dimensions studied/analyzed?\nDifferent components/aspects of Multilingual BERT investigated:\n(i) Linguistics properties and similarities of target and source languages  (has been studied in prior work)\n(ii) Network Architecture (novel)\n(iii) Input and Learning Objective (moderately novel)\n\nWhat are the main results? Are they significant?\nLexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an important part of it.  \n\nStrengths \nNovel findings about the effect of network architecture, input representation and learning objective on cross lingual ability of M-BERT\nWeaknesses\nPires et al. (2019) work has been misrepresented. (see above for more details)\nPires et al. (2019) did  study linguistics properties and similarities of target and source languages for Multilingual BERT and had similar findings as this work.\n\n\nQuestions\nWhat kind of difference in the numbers is considered significant by the authors ? For example, according to them, an increase of more than 2 points in accuracy in Table 3 (e.g. 1 head vs. 6 heads) is considered insignificant. But a decrease of a point in accuracy in Table 5  (e.g. enfake-ru NSP vs. No-NSP) is significant.\n"}