{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper evaluates the cross-lingual effectiveness of Multilingual BERT along three dimensions:\n- Linguistics\n- Architecture\n- Input and learning objective\n\nIn each of these three dimensions, the authors run experiments to test why BERT is effective at cross-lingual transfer.\nFor simplicity, they run their experiments on B-BERT which is trained on two languages. The fine-tuning is done on language, and the zero-shot performance is tested on the other one.\n\nPros:\nI found the en-fake experiments enlightening. The authors find that wordpiece overlap is not as important for cross-lingual transfer as was suggested by previous papers (Wu & Pires).\nThe idea of creating a unicode shifted version of English and use it for testing is a first of its kind and quite interesting.\nMost experiments were well motivated and the authors draw good conclusions about the need for more depth, that only a few attention heads are sufficient.\nThey end with an experiment that shows that the cross-lingual effectiveness drops significantly when the premise and hypothesis are in different languages. This is a good motivating experiment to end the paper on.\n\nCons:\n- The architecture experiments were not that insightful and they authors did not reach concrete suggestions on a minimum number of parameters or a minimum depth.\n- While the two language setting is easier to experiment with, I wonder how these conclusions will change if they were repeated with the 100+ language version.\n- For structural similarity, it would have been more concrete if the authors were able to visualize and show that the newly created en-fake still aligned with corresponding similar words in the other languages. This would have proven that despite no wordpiece overlap, similar words still align.\n\n\nMinor comments:\n- typo: \"training also training also\"\n- bad grammar - last para in page 1\n- I found the introduction was filled with grammar and bad English. Please fix.\n- Why are the numbers in Table 4 in a different format?\n- 3.4.2 It's not clear if this is a clear trend. The authors claim that lang-id helps.\n- Please explicitly state the sentencepiece or wordpiece setup in a central piece. I found the detail hidden in section 3.1.2\n- With the word vs char vs wordpiece experiments, I think more care should be taken to make sure that the number of parameters remains the same across all three setups. e.g. with only a few chars as vocab, the model has far fewer parameters. This should be compensated for."}