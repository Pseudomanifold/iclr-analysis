{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nCONTRIBUTIONS:\n\nC1. Cross-linguistic token overlap. Fake-English: English, but with the Unicode codes of English characters all shifted by a large constant so that there is no overlap between Fake-English characters and those of actual languages, but the language-internal structure remains that of English. \n\nC2. A bilingually-trained BERT, pretrained on languages L and L\u2019, is then trained on a downstream task in L then tested on that task in L\u2019. The task is Cross-Lingual NLI (XNLI) or Cross-Lingual NER. L\u2019 is Spanish, Hindi, or Russian. L is English or Fake-English. Comparing the success at test when L = English vs. when L = Fake-English, it is shown that eliminating all token overlap between L and L\u2019 has a small effect (less than 1.5% on XNLI, less than about 3.5% on NER). (Table 1)\n\nC3. Several architectural parameters of BERT are varied holding the others roughly constant (same tasks as C2, with L = Fake-English). This shows that depth (Table 2) and level of tokenization matter (Table 7), while little effect results from varying the number of attention heads (Table 3), number of parameters (Table 4), whether the next sentence prediction task is used for training (Table 5), or whether the language of an input is explicitly given (Table 6).\n\nC4. Testing cross-language entailment on XNLI by B-BERT shows that there is a large reduction in performance when the hypothesis and premises sentences are from different languages. \n\nRATING: Weak reject\n\nREASONS FOR RATING (SUMMARY). Aside from the invention of Fake-English, which as far as I know is original and a clever approach to assessing the importance of token overlap in cross-language transfer, the other contributions are reporting results of mechanical changes. The paper\u2019s contributions are useful, but do not reach a level of generality, originality, or depth justifying presentation at ICLR.\n\nAlthough it did not factor into my rating, I would like to point out that saying \u2018structural similarity is important\u2019 and saying \u2018word-piece overlap is not important\u2019 is saying exactly the same thing twice, since the gain not attributable to word-piece overlap, by their definition, equals the gain due to \u2018structural similarity\u2019, which is a concept otherwise undefined and unmeasurable. "}