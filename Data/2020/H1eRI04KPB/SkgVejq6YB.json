{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: \nThe paper investigates a strategy for factoring out dimensions in multi-scale generative flow models. The strategy relies on a heuristic that employs the availability of per-dimension log-determinant terms (e.g. as affine couplings) to decide which half of the dimensions to factor out. Quantitative experimental results support the proposed strategy\u2019s efficacy.\n\nRecommendation: Weak Reject\nThe paper misses several important comparisons and baselines - both in terms of placing the proposed method in a broader context (e.g. how could the method be used in multiscale i-ResNet), as well as providing experimental comparisons that would definitively demonstrate the method\u2019s contribution (e.g. is it the heuristic dimension splitting or the unfactored pre-training that provides the benefit.\n\nThe applicability of the proposed method appears to be overstated. The paper suggests that the method is applicable to any multi-scale flow architecture. However, it depends on the availability of per-dimension log-determinants. As the paper states, these values are readily available for methods that rely on affine couplings (e.g. RealNVP, Glow), but not for methods such as i-ResNets or FFJORD, that also employ multi-scale architectures to factor out variables.\n\n\nMajor comments:\n1. Section 2.1 introduces flows as a mapping from data x to the latent code z, and gives the log-likelihood formula (2) that can be optimized during model training. This presentation is at odds with Section 3, where the authors talk about sampling a latent variable that fixes the \u201clog-latent density\u201d and only leaves the log-determinant term to be optimized (e.g. \u201c[...] the log-latent-density term depends on the choice of the latent distribution and if fixed given the sampled latent variable. [...] once the variable is sampled, maximizing the log-det term results in maximized likelihood.\u201d The two presentations are at odds with each other and should be reconciled, ideally by altering Section 3 since it does not reflect the way flow models are trained.\n2. The authors made an implicit assumption that the log-det term is decomposable into per-dimension component. As mentioned earlier in this review, generally speaking this is not true and methods such as i-ResNets or FFJORD do not (directly?) permit such decomposition, while they do employ multi-scale architectures akin to that of RealNVP. This assumption manifests itself in several places in the paper, which are listed below. Either the authors could make this assumption and the limitations imposed by it very clear, or they could provide additional information on how their method extends to methods that do not provide per-dimension log-det contributions.\na) In Section 3 \u201cThe total log-det term is nothing by the sum of log-det terms for contributed by each dimension\u201d. This is obvious (and possibly holds) only for affine couplings.\nb) In Section 3 \u201cNevertheless, such an analogy can be extended for other flow models which involve a multiscale architecture [...]\u201d. As mentioned, I am not convinced this is true.\nc) In Section 4 \u201cSince our method focuses individually on the dimensions using a heuristic which is always available in flow model training, it can prove to be have more versatility in being compatible with generic multi-scale architecture\u201d. Same as above - in fact I believe that the opposite may be true.\nd) Section 4 does not mention methods that employ multi-scale architectures whil not allowing for discerning per-dimension contributions of the log-det. Mentioning these methods is important for accurately positioning the authors contribution relative to the existing literature.\ne) The method is only applied to the RealNVP architecture. The claim of it being generally applicable to multi-scale architectures would be much stronger if the comparison would include other multi-scale architectures. Here, I strongly recommend i-ResNets or a similar architecture based on log-det approximation to the entire coupling layer.\n3. Unless I missed it, the authors do not mention how they aggregate the log-det values. They are available per datapoint. Are they aggregated over the entire dataset?\n4. In Section 4 the authors imply that the contribution of a dimension towards the total likelihood is dominated by the log determinant (\u201c[...] as the contribution by the variable (dimension) towards the total log-det (~ total log-likelihood)\u201d). This is not obvious, and I am not sure it is always true - it probably depends on the choice of the base distribution p(z).\n5. The authors show experimentally that the use of their variable factorisation heuristic improves log-likelihoods over the baseline RealNVP model; they further perform ablation studies to confirm that preferential factorisation of low log-det dimensions provides best log-likelihoods and qualitative results on the CelebA dataset. However, several points remain unclear.\na) What are the relative contributions of pre-training without dimension factorisation and the proposed factorisation heuristic? Do the experiments shown in Table 2 all employ the pre-training scheme? If so, this should be clearly stated. If not, this set of experiments should be performed.\nb) The networks used in the experiments use between m=1 and 4 factorisation layers. So in total, there are 2^m possible decisions of which half of the variables at those layers to factorise. These numbers are not so large as to prevent us from training all possible combinations of the networks. How does the proposed heuristic compare to just training all (or a few sampled) factorisations decided a priori? This comparison should also investigate the effect of using pre-training.\n6. In the qualitative comparison (Section 5.2) the authors argue two points (i) RealNVP with the proposed method generates finer details on CelebA; and (ii) provides a reasonable latent space that can be interpolated in.\na) With regards to (i), I personally find it difficult to argue about fine details in what are effectively low-resolution images. The differences (which are already hard to see and easy to imagine) could be present due to minute changes in the training code/procedure/random seed/etc. I would be more confident about these differences if the authors could confirm that the methods were trained using the same code base, random seeds, dataset shuffle and framework versions.\nb) How do the qualitative interpolation results compare to RealNVP without the proposed factorisation heuristic?\n\nMinor comments:\n- In Section 2.1 \u201cLet x be a high-dimensional random vector with unknown true distribution x ~ p(x) [...]\u201d. Here x appears to refer to the data or a sample from the data distribution, and not just a random vector. This could be made clearer.\n- In Section 2.1 the \u201cjacobian\u201d should be written with a capital \u201cJ\u201d\n- In Section 2.1 and elsewhere, when denoting outputs of intermediate flows f_i as y_i, it is mentioned that y_0 = x, but not that y_k = z. If the latter is true, mentioning this would improve clarity.\n- Section 3 \u201cIn a multi-scale architecture, it is apparent that the variables getting exposed to more layers of flow will be more expressive in nature [...]\u201d. I do not find this to be an obvious fact. Perhaps the authors could provide an intuition for why this is true or demonstrate this on a toy example? My confusion may stem from the fact that I am unsure what it means for a variable (i.e. a dimension we factor out) to be more expressive.\n- Some phrasing is a little unusual (e.g. \u201cless (more) log-det tems\u201d instead of \u201csmaller (larger) log-det term\u201d; \u201cto be have\u201d).\n- In Section 5.1 \u201ccode length\u201d is introduced without context. The connection between code length, log-likelihood and compression can be made more clear.\n"}