{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper aims to build a deep clustering algorithm with interpretable latent topology. The authors combine VAE, clustering assignment hardening and SOM. The authors further extend the model to deal with sequence input, by adopting temporal smoothness regularization and language model. \nGood performance in clustering is achieved, experimental results also show that the topological latent structure is captured and the full (sequence) model captures more temporal information than the basic model.\nHowever, the effect of SOM seems to be complicated and not fully explored. \nConsidering that VAE+SOM has been introduced before, the novelty of this paper is to combine a few techniques/tricks to strengthen the clustering performance.\n\nSome concerns:\n1.\tRepresentation\nGood clustering does not guarantee to capture a generally good representation. For example, clustering algorithms lead to a good interpretation of \u201ccolor\u201d of images that do not need to be good at clustering the semantics of images. If we consider all the factors that generate the data, it is not easy to design metrics to cluster all these factors. \n2.\tVector Quantization/K-means\nCan you review, compare and contrast your work with some recent discrete representation learning works? There are some works (e.g. VQ-VAE (-2)) pursing discrete representations via vector quantization or the Gumbel-softmax trick. The learned representations are also highly structured, can be used for visualization and can be used for further clustering (though might not be as good as clustering-driven approaches).\nHow\u2019s the qualitative difference in the topology of centroids between your method and vector-quantization-based approaches? \n3.\tThe role of SOM\nThe clustering performance mostly comes from clustering assignment hardening, while SOM does not clearly benefit the performance of clustering according to table one. \nIn figure three, you show that SOM\u2019s row in affecting the clustering performance. As tuning SOM would lead to different trends in NMI and purity, what is your strategy in determining your beta?\nHave you ever checked what happened to the centroid topology when varying beta? \n4.\tSome suggestions on experiments\n.\tTo fully understand the benefit of each component of your loss, you can try a variant of VarTPSOM without enforcing \u201csmoothness\u201d \n.\tHow is the beta affect VarTPSOM in sequence evaluation?\n.\tIn your table one, it might be good to compare with some recent discrete latent variable models (as mentioned in bullet point one). \n.       It might be good to try the number of clusters other than 64 in your experiments, towards understanding how it affects the performance.\n\nMinor:\nDenote N as the number of samples, and K as the number of centroids. At the bottom of page 3, you need O(K) to calculate each s_{i,j}. At the beginning of page 4, based on {s_{i,j}}, you either need to store intermediate computational results for acceleration, or you might need O(NK) to calculate each t_{i,j}. The whole process would be much slower than k-means/vector quantization, which seems to be not good for large enough data set.\n"}