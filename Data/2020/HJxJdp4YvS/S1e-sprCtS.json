{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes combining the latent space of a variational autoencoder with two losses that regularize the latent space. The first loss is the cluster hardening loss in Aljalbout et. al [https://arxiv.org/pdf/1801.07648.pdf]. This loss attempts to convert from a  soft-assignments of points (in latent space) to cluster centers (where the assignments are based on similarities computed via a Student t kernel) to a hard assignments of points in latent space to cluster centers. The transformation is posed as the minimization of a KL divergence.\nIn the model under consideration there is assumed to be a grid of \"cluster centers\" or centroids that all points must cluster along.\n\nThe second loss (new in this paper), penalizes the similarities between a point and a centroid from being far away from the similarities of the points to the neighbors of the centroid (on the grid). i.e. this loss tries to ensure that neighboring centroids on the grid correspond to similar points in latent space.\n\nFinally, an extension to temporal data is proposed. The temporal model is as follows: an encoder used at each time step to obtain the latent representation and a third loss to encourage that the latent representations across consecutive time steps stays close to each other is incorporated into the learning algorithm. The latent representation is presented as input to an RNN which is used to reconstruct the data.\n\nOn NMI and cluster purity (evaluated on MNIST, fashionMNISt), the model outperforms two closely related models (the SOM-VAE and DESOM). Similarly on clustering time series data from physionet, the proposed method outperforms the SOM-VAE. The model also compares results on mean squared error (in predicting the time series 6 hours before ICU dispatch) but their baseline is an LSTM model *without* a latent variable -- a fairer baseline would be against their own model with only the variational bound used for learning. The paper also visualizes what is encoded in the centroids.\n\nOverall this paper's contribution is the use of a VAE (rather than an autoencoder as in related work) that contains a latent space regularized to favour learning cluster structure. The paper provides good empirical evidence to suggest that the combination of the proposed losses alongside the VAE does yield better clustering performance. However, I find the addition of the two losses somewhat ad-hoc and little in the way of explanation is provided for when we should expect such a model to work and when it may not. There is not much of a discussion regarding the complexity of learning with the proposed losses but it looks like a simple algorithm for learning with Equation (1) would have to use the entire dataset to compute it. Could you comment on the scalability of the learning algorithm?\n\nMinor comments, there are several places that need editing for grammar and context:\n * The equation at the top of page 4 needs editing within the subscripts of the summation since i is overloaded\n * line 29 talks about \"the observed centroids\", but centroids are not mentioned until much later in the paper\n * expand AE the first time it is used as an acronym\n"}