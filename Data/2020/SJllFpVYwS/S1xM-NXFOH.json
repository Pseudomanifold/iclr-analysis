{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Overall:\n\nMy understanding of the method is this:\nWe take a pre-trained GAN and generate some small number of samples.\nWe annotate those samples w/ per-pixel annotations.\nWe then train a model that takes GAN intermediate representations and yields semantic segmentations, so that we can generate a large\ndataset of (GAN Sample, Semantic Segmentation) pairs, which can then be used to train a semantic segmentation model.\n\nIf you want to train a semantic segmentation model for a large data set of images with no per-pixel annotations,\nthis could potentially be useful.\nHowever, if you buy this motivating scenario (I think I do, but I don't know much about semantic segmentation),\nwhy couldn't we have taken the labeling budget we spent to label the GAN samples, used it to label\nsome fraction of the real images, and then use some semi-supervised learning technique on those.\nIs the answer 'semi-supervised learning doesn't work yet for segmentation, even though it does seem to be working well for object recognition'?\n\nRegardless, the writing of this paper leaves a lot to be desired.\nI think that the authors would have to substantially improve clarity in order for me to reccommend acceptance.\nI would like to see other reviewers more familiar with semantic segmentation weigh in as well:\nhow realistic is this problem setting?\nIf we found ourselves in this problem setting, would this be a sensible way to solve the problem?\nThe work would benefit from clarifying those points.\n\nDetailed Comments:\nPresumably you did not mean for your title to be 'TEACHING GAN TO GENERATE PER-PIXEL ANNOTATION CONFERENCE SUBMISSIONS'\n\nThe introduction is pretty abrupt...\n\n> We propose method for joint image and per-pixel annotation synthesis\n*a* method\n\n> We show that separate semantic segmentation network trained on synthetic dataset generalizes on real images.\n*a* separate\n\n> The show that GANs lear internal\n*they*, *learn*\n\n> Interestingly, training takes a few minutes and Decoder successfully learns on a small number of training examples\nCan you be a bit more specific than this?\nHow few images can you use?\nHow costly is it to annotate those images.\n\n> After Decoder is trained we can create a large dataset of pairs of GAN-generated images and corresponding masks predicted by Decoder.\nAt this point in the paper, I don't know why you want to do this?\nPresumably to train a segmentation model later, but the paper should read in such a way that\nI don't have to guess that.\n\n> sythetic\ntypo\n\nMinor: table 1 and 2 should have captions that tell you how to read them.\nOtherwise readers have to scan the text for the description.\n\n\n> We test two variants of backbone\nThis is the first I've seen backbone mentioned.\nWhat is that?\n\nI don't know much about semantic segmentation.\nCan you give intuition for why the jump in Intersection over union is bigger than\nthe jump in accuracy?\n\nFor fig 3, why are the first and 3rd examples so dark?\nTHe top and bottom results look identical for the 2nd and 4th examples,\nso the darkness seems important.\n"}