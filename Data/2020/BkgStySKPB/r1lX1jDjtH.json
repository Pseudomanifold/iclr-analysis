{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presented a multi-view learning method that is based on negative sampling in contrastive learning. The core idea is to set an anchor view and the sample positive and negative data points from the other view and maximise the agreement between positive pairs in learning from two views. When more than two views are presented, the learning objective is a sum over all possible combinations of two views. The performance of the proposed model is good, and the ablation study is interesting. \n\nComments:\n\n1. The core concept, or at least one of the core concepts, in multi-view learning is the conditional independence.\n\nNormally, the underlying assumption in multi-view learning is that, given the class label, the samples from multiple views are conditionally independent from each other. Therefore, the goal is to learn distinctive representations from different data sources/disjoint populations, so then after learning, the ensemble of them is able to capture a set of diverse aspects of the data. A \"side-effect\" of learning from multiple views is that individual views indeed get improved by learning from others. Meanwhile, self-supervised learning is the case when the input data to the designed learning system is also the target of the system. \n\nThe paper presented an idea for self-supervised learning from multiple views, which is not exactly the same, but still in the same regime. This concept could be used to explain some empirical findings in this paper. Since it is expected, there is even no need in conducting experiments. \n\n\n\n\n2. My main concern of this paper is the novelty, however, the empirical results are strong.\n\nThe paper mainly presented a simple yet effective method for self-supervised learning from two views, and the generalisation is a sum over all possible combinations of two views. The method itself has already been proposed many years ago as mentioned in the related work section in the paper, and the generalisation was also described in prior work, which makes me doubt the novelty of the paper. \n\nThe earliest work to the best of my knowledge is [1], and later on there are a couple workshops [2,3] on multi-view learning which largely settled the field of learning from multiple views from neural networks', kernels', and bayesian perspectives. Many things mentioned in this paper have already been discovered at that time. \n\n3. The theoretical justification is not as strong as the generalised CCA.\n\nCCA has been applied in the field of multi-view learning and self-supervised learning for long, and it was initially proposed for comparing the correlation between two sets of samples of two random variables. A successful generalisation is the generalised CCA which is capable of learning from multiple views. The formula of GCCA as referred in [4] is simple and elegant, and then the extension of using neural networks is also straightforward. Since people has relatively clearer understanding of CCA itself, the generalised version or the kernel version of it is also well-understood. \n\nA nice theoretical understanding of contrastive unsupervised learning is provided in [5], and I recommend the authors to study.\n\n\n[1] de Sa, Virginia R. \"Learning classification with unlabeled data.\" Advances in neural information processing systems. 1994.\n[2] ICML Workshop, \"Learning With Multiple Views\". 2005\n[3] NIPS workshop, \"Learning from multiple sources\". 2008\n[4] Benton, Adrian, et al. \"Deep generalized canonical correlation analysis.\" ICLR workshop 2017.\n[5] Arora, Sanjeev, et al. \"A theoretical analysis of contrastive unsupervised representation learning.\" ICML 2019."}