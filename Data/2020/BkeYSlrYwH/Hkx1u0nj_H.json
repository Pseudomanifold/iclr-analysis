{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper proposed an ensemble method (CIKD) that train multiple agents and\nuse knowledge distillation to transfer knowledge from the current best agent to\nsub-optimal agents periodically.  According to the reported results, CIKD is a\nsimple yet effective approach to improve sample-efficiency and final performance.  \nThe experimental results are sufficient, and the ablation studies are conducted thoroughly. It is shown that both selecting the best agent and using KD to\ntransfer knowledge are effective comparing to other naive alternatives. \n\n\nI recommend the acceptance of this paper. \n\nThe paper proposed a novel approach (CIKD) to improve the sample-efficiency of the state-of-the-art. The proposed ensemble approach is aligned with our intuition, and it is effective. The authors proposed to train several agents at the same time and randomly select one of\nthe agents as a behavior policy during each rollout. Then the collected trajectory is used to update the policy of all agents. Meanwhile,\nthey keep tracking the performance of each agent and use the current best agent to conduct knowledge distillation to other agents periodically. \n\nThis paper first conducts experiments to show when consolidating\nthe SAC with CIKD, both of the final performance and sample-efficiency can be improved. Then a set of ablation studies verified the best agent selection strategy, and the knowledge distillation\nstrategy is necessary for the ensemble method. \n\n\nInvestigation on the reasons for improvement:\nThough extensive ablation studies have shown the effectiveness\nof each component of CIKD. It is still not clear why this approach\ncan be effective. \nIntuitively, it is possible that the exploration from a set of agents would outperform\na single agent. The measure of exploration efficiency could help in explaining the results. Furthermore, better exploration not necessarily\nleads to better performance and sample-efficiency. Does knowledge distillation serve as a better alternative to exploit existing data? \n\nModel/algorithm agnostic\nThe proposed method is more convenient to be applied with off-policy approach when the policy is in the form of softmax. Is it also applicable\nto other approaches? \n\nExperiments:\nHow do you determine when to stop the KD process? As mentioned in section 5.5, if we conduce KD fully, all students would be just imitating\nthe teacher's behavior. It seems the key is to tune a good termination\nthreshold for each task? Are there any guidelines to set up this threshold?\nDo you have some automatic way to terminate the KD procedure?\n\n\nMinor:\nL1, P5, \"how to CIKD improves the sample efficiency\" \n\n"}