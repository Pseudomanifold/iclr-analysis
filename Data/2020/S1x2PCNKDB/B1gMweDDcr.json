{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work considers imitation learning, i.e. the problem of learning an unknown reward function from expert demonstrations. The paper proposes Task-Relevant Adversarial Imitation Learning (TRAIL), a modification of Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) which introduces an additional constraint to the discriminator optimization, namely that the discriminator must not be able to successfully discriminate between expert and agent samples on some predetermined invariant set $\\mathcal{I}$. Technically this constraint is realized with an additional term added to the standard GAIL objective that minimizes the negative (i.e. reversed) discriminator objective whenever discrimination accuracy exceeds 50% (i.e. randomness) on $\\mathcal{I}$. Given that the invariant set $\\mathcal{I}$ includes environment samples not relevant for the task, TRAIL thus aims to prevent discriminator overfitting to task-irrelevant features (e.g. object or agent appearance) and regularizes the discriminator to extract more salient, task-relevant features of expert behavior.\nIn experiments on various robot manipulation tasks (Block lifting/stacking/insertion with and without distraction), the paper shows that TRAIL outperforms Behavioral Cloning (BC), standard GAIL, Deterministic Policy Gradients from Demonstrations (DPGfD) as well as additionally introduced improved GAIL baselines that employ common regularization techniques (early stopping and data augmentation).\n\nI think this work could be accepted since it clearly demonstrates a key issue of GAIL (tendency to overfit to task-irrelevant features) and proposes a simple solution for this issue that empirically proves to be effective. I especially appreciate the comprehensive ablation studies that isolate effects from data augmentation and actor early stopping. The paper overall is well written and easy to follow. \nI must admit, however, that I am not too familiar with this line of research and might be missing some context, e.g. recent competitors.\n\nSome questions that I have:\n(1) Could you elaborate more on how to select the invariant set $\\mathcal{I}$ in practice? The paper proposes random policies or initial frames, but as I understand experiments are only reported for the latter design choice. How would performance differ for different choices? What if the invariant set $\\mathcal{I}$ is misspecified, i.e. includes task-relevant samples? Would TRAIL be robust?\n(2) How is the invariant set $\\mathcal{I}$ chosen for the stack, insertion, and stack banana experiments in Figure 8?\n(3) What if actions would be included in objective (2) again?\n\n\n#####\nMinor comments\n- Is the legend in Figure 14 wrong? Table 1 results and the plots in Figure 14 are inconsistent (e.g. on Lift distracted \"TRAIL\" is best in Figure 14 which should be \"TRAIL - augmentation\" according to Table 1.)\n- Section 5.1: \"Finally, to disentangle the importance of actor early stopping, ...\" >> \"disentangle\" somewhat is a signal word in ML that readers directly associate with representation learning. Maybe just \"show\"/\"infer\"/\"understand\".\n"}