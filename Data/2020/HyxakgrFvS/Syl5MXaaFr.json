{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a novel lifelong learning approach, called side-tuning. The idea of side-tuning is that they firstly have a fixed base network and when a new task arrived, they learn the task with a smaller network which transferred the adaptive knowledge from large base network while freezing  it. It can get fine-tune strength without catastrophic forgetting problem.\n\nThe approach looks practical that they only train a smaller side-network for new tasks. But the proposed model might have several crucial weakness. \n\n- It is extremely dependent to the base (pretrained) model. When the base knowledge will be largely different to the arriving tasks, it looks hard to appropriately train the model.\n\n- There is no knowledge transfer among arriving tasks. N side networks can not communicate among them which means they can not manage the redundancy problem among side networks.\n\n- There is a few of ablation study. Why does the model requires to element-wise combination among base parameters and side parameters? It seems that the architectural choice of the model and hyperparameter selection procedure is heuristic.\n\n- Even the model requires less number of 'trainable' parameters, the machine inevitably requires to memorize not only side-network parameters but also base network parameters. Then, in my understanding, the authors are required to compute the total capacity as addition of them, not only with trainable ones.\n\n- Also, the idea looks quite similar to piggyback, or Progressive neural networks that both approach kept the parameters of base network intact. The authors also require to compare with them.\n\nMallya, Arun, Dillon Davis, and Svetlana Lazebnik. \"Piggyback: Adapting a single network to multiple tasks by learning to mask weights.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n"}