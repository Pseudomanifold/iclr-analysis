{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to tackle many tasks including among others lifelong learning or transfer learning. To do so, they propose to perform at the feature level an alpha blending between a fixed pretrained representation and a learnable (possibly smaller) network. Such a design allows to avoid catastrophic forgetting by construction and represents a tweakable middle point between a feature extractor and a fully finetuneable network.\n\nI am strongly leaning towards rejection. This paper provides numerous experiments but this should not replace novelty or presenting as novel something already present several times in the literature. For example, the proposed method does not differ much from \"Progressive Neural Networks\" by Rusu et al. if you switch off their lateral connections. \n\nFurthermore, several network adaptations papers are missing from the references and also present themselves as tweakable middle points between feature extractor and  a fully finetuneable network. Among others: \"Universal representations: The missing link between faces, text, planktons, and cat breeds\" by Bilen et al., \"Incremental Learning Through Deep Adaptation\" by Rosenfeld et al. or \"Efficient parametrization of multi-domain deep neural networks\" by Rebuffi et al. The latter also proposing an additive adaptation all along the network which preserve the original filters for the original task."}