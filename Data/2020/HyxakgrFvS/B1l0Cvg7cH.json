{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies continual learning. Fine-tuning from previous task will make the network forget previous knowledge, but directly using features from previous tasks\u2019 network will cause underfitting. This paper proposes a method that is in the middle of fine-tuning and using fixed feature. A new small network is added to the architecture to help the training of new task. The method is evaluated on several datasets. This paper proposes interesting insight but the contribution of the proposed method is very limited.\n\nComments\n1.The setting in this paper is not a rigorous continual learning setting. Continual learning setting require model not to incrementally grow with task number. The novelty of keep all previous model and merely add new network for every task is very limited. \n2.The novelty of the proposed model is very limited. There are several works that proposed more advanced idea than this work such as [A] and [B]. The latter also proposed a method that avoid keeping all previous model in evaluation.\n3.The paper argue that the proposed method needs to train only smaller amounts of parameters for each task, but there are several works on this subject as [C]. Please conclude these methods in related works and compare with them.\n4.If the architectures of base model and side model are different, how to combine the features from these two networks? \n5.What is Side-tune (R) in Table 5? Why it outperforms Side-tune for a large margin?\n6.What is the relationship between the scale of side networks and the difficulties of tasks? How to make sure the architecture that to be used for a certain task? \n7.There is no need to arrange many unchanged images in Figure 5 that takes up much space in the main body of the paper.\nFrom above reasons, I vote for rejecting this paper.\n\n[A] Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n[B] Flennerhag, Sebastian, et al. \"Transferring knowledge across learning processes.\" arXiv preprint arXiv:1812.01054 (2018).\n[C] Rosenfeld, Amir, and John K. Tsotsos. \"Incremental learning through deep adaptation.\" IEEE transactions on pattern analysis and machine intelligence (2018).\n"}