{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "- This paper claims to provide a \u201calternate\u201d view of pretrained embedding as commonsense repository. But this is not a new view at all. It is well known embedding can encode commonsense knowledge, which is a reason it helps a wide variety of recent commonsense related tasks (and there has been considerable analysis on what kind of coomonsense is learned and helpful).\n\n- The paper also claims to propose three training criteria, which are not new as well. In fact, much work has been performed to learn representation for different semantic orientation, dimensions and relations (e.g., polarity like full/empty is one special case of contrasting meaning, which has been studied a lot in the distributed representation paradigm for years).\n\n- The paper has not been carefully written yet; e.g., even in the abstract, there is a typo like \u201ctypically trained an evaluated\u201d. \n\nI do not think this paper, claimed as a position paper, adds any new positions to the existing literature. I do not recommend it for the conference.\n"}