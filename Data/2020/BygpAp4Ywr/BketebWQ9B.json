{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a adversarial learning framework that tries to align the hidden features of data with simple prior distributions.  A training strategy similar to GAN  was exploited. The proposed framework was argued that it can well deal with the adversarial perturbations. Some experiments were conducted, verifying that the proposed algorithm seems useful and robust.\n\nThe main comments are listed as follows:\n\n(1) To the reviewer, the adversarial framework to use a simple prior to align the hidden features seems new. However, in the literatures, there are some adversarial training algorithms which did similar things. For example, C1 tried to force the hidden features of deep learning follow a Gaussian distribution with the smooth regularizer. Though different,  I believe they share some common motivation, the authors may want to discuss and even compare this reference.\nSimilarly, in the literature, the so-called Center Loss will also basically force the means of different data are as further as possible, this is also relevant to the paper.\n\n(2) The authors have reported a series of experiments, which is great. However, they only evaluated their model's robustness in case of PGD attack. This is very different from many adversarial literature which normally would discuss various attack such as l_2 attack, FSGM, and even black-box attack.  The evaluations may be more convincing  if more attacks can be tested on the proposed method.\n\n(3) Further to (2), it is noted that the paper simply compared their approach's robustness  with Madry's adv, it would be more convincing to compare the other recently adversarial training algorithms like VAT (C2).\n\n(4) It is good that the paper proposed a different way in dealing with adversarial examples. In comparison, the current work studying adversarial examples is based on the robust framework of minimax trying to impose the worst-case perturbation. It would be interesting to see if the proposed work can be also further applied in the minimax  framework and examine if a further robustness can be achieved.\n\n(5) Some visualization may be interesting. For example, a visualization how Q would change as the training continues. This may be used to check the convergence property of the proposed algorithm.   \n\nC1:Manifold Adversarial Learning, S. Zhang et al.,  https://arxiv.org/pdf/1807.05832\nC2:Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning, T. Miyato et al. arXiv preprint arXiv:1704.03976, 2017."}