{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Using Deep Learning and especially, GNNs seems to be a popular area of research. I am no \nexpert at optimizing code performance, so please take my review with a grain of salt. The algorithmic contributions of the paper are as following: \n\n(a) GNN that combines static code and dynamic execution trace. \n(b) Binary encoding of features leads to better performance in comparison to categorical and scalar representations. \n\nThe results show that the proposed method outperforms existing methods on standard benchmarks in the program execution community. \n\nFrom a machine learning stand point, the contributions are straightforward and the results make sense. I have the following questions: \n\n(I) Authors argue that binary representations are better because of their hierarchical nature. They mention that they can generalize even if not all combinations of bits are seen, but a subset is seen in a manner that every bit has been flipped a couple of times. I don\u2019t agree with this reasoning, as seeing the individual bits flip has no guarantee that a NN would generalize to a new combination of bits unless the distance in the binary code makes sense. Is there some special way in which the binary code is constructed?\n \n(ii) Transfer learning experiments: Its unclear to me if the comparison presented in the paper is a fair one. Comparison is made against Ben-Nun et al. pre-training on LLVM IR. I am not sure how different is LLVM IR dataset from the algorithm classification dataset. If the dataset is very different, then obviously a lot of pre-training will only result in modest performance gain. What happens with Ben-Nun method is pre-trained on the same dataset as the proposed method? Also, what is the difference in performance between the cases when the proposed method is applied to algorithm classification with and without pre-training? \n\nOverall, the paper is a application of GNN to optimizing code execution. The technical innovations are domain-specific and do not inform the general machine learning community. Given lack of expertise in the area of program execution, I cannot judge the significance of the performance improvements reported in the paper. \n\nGiven my current concerns, I cannot recommend acceptance. I might change my ratings based on the review discussions and the author\u2019s responses to the above questions. \n"}