{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors analyze the bias in the straight-through gradient estimator using the framework of harmonic analysis of boolean functions. Based on this analysis, they propose three methods to reduce the bias of the straight-through estimator, resulting in a less-biased estimator that is the same computational complexity as the original. They evaluate this estimator on a series of generative modeling tasks where they demonstrate improvements over existing methods, including the ability to train a very deep stochastic network.\n\nI enjoyed this paper -- the exposition is clear, the ideas are (to my knowledge) novel and make sense, and the experimental evaluation is thorough and convincing. I recommend an accept. \n\nI skimmed through the proofs in the appendix so cannot with absolute confidence vouch for their correctness.\n\nOne small piece of feedback: I found the most confusing part of the paper was the section on the 'bernoulli splitting trick'. It might be helpful to pull some of the appendix material into this section to make it a little less sparse.\n"}