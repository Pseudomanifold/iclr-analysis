{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Straight-Through is a popular, yet not theoretically well-understood, biased gradient estimator for Bernoulli random variables. The low variance of this estimator makes it a highly useful tool for training large-scale models with binary latents. However, the bias of this estimator may cause divergence in training, which is a significant practical issue. The paper develops a Fourier analysis of the Straight-Through estimator and provides an expression for the bias of the estimator in terms of the Fourier coefficients of the considered function. Motivated by this expression, the paper proposes two modifications of Straight-Through which may reduce the bias of the estimator, at the cost of the variance. The experimental results show advantage of this improved estimator over Gumbel-Softmax and DARN estimator.\n\nWhile I really like the premise of the paper, I feel that it needs a significant amount of additional work. The text is currently fairly hard to read. The theoretical part of the paper does not quantify the variance of the estimator. The experiments are a bit unfinished and do not include ablations of the proposed modifications of Straight-Through. Most importantly, I think that in the current form the theoretical and the empirical parts of the papers are not well-connected. Because of this, I believe that the paper should currently be rejected, but I encourage the authors to continue this line of work.\n\nPros:\n1. Theoretical analysis and empirical improvement of the Straight-Through estimator is an important avenue of work.\n2. The paper makes a solid contribution of deriving the Fourier expansion of the Straight-Through estimator bias.\n3. Based on this expansion, the paper proposes an algorithm with reduced bias. The algorithm is simple to implement, practical and appears to work slightly better than DARN.\n\nCons:\n1. The key weakness of the theoretical part of the paper is that it focuses on the bias of the estimator, but does not quantify the variance, especially after the modifications. If reducing the bias was the only goal, one could use unbiased (but high-variance) estimators such as REINFORCE or VIMCO.\n2. The final algorithm appears to be the DARN estimator combined with relaxation by uniform noise (\u201cBernoulli splitting uniform\u201d) and scaling. The paper does not have an ablation showing how the uniform noise and scaling perform on their own.\n3. There are a few incorrect statements that I\u2019ve noticed.\n* \u201cAs a side contribution, we show that the gradient estimator employed with DARN (Gregor et al., 2013), originally proposed for autoregressive models, is a strong baseline for gradient estimation.\u201d - MuProp paper compared to this estimator under the name 1/2-estimator\n* In Lemma 1 the \u201cREINFORCE gradient\u201d is just the exact gradient of the expectation, not a stochastic REINFORCE gradient.\n* \u201c To the best of our knowledge, FouST is the first gradient estimate algorithm that can train very deep stochastic neural networks with Boolean latent variables.\u201d This paper uses up to 11 latent variable layers, while [1] has trained models with >20 latent variable layers (although their \u201clayers\u201d have just one unit).\n4. The derivation of \u201cBernoulli splitting uniform\u201d trick is confusing and contains a lot of typos. For instance, the text before eqn. (14) implies that the distribution of u_i is U[-1, 1], which cannot be right and does not correspond to Algorithm 1. The statement that this trick does not lead to a relaxation is odd, since the function is being evaluated at non-discrete points.\n5. There are generally many typos and some poor formatting in the math. For example, in eqn. (6) the coefficients are off by one: it should be c0 + c1 z1 + c2 z2^2 + \u2026 . The equations (10) and (11) are poorly formatted. The notation \\partial_z1 f(u_1, u_2) in eqn. (14) is strange. In many places p^{i->\u00bd} is denoted as p^{1->\u00bd}.\n5. I don\u2019t think I understood the idea of representation scaling (Section 4.4). The eqn. (16) would suggest that the scaling should optimally be set to zero, which is just saying that the gradient is unbiased when the model does not use the latents. There is no other practical guidance on choosing this coefficient. Furthermore, one can always absorb the global scaling factor into the succeeding weights layer of the model, so this trick can probably be replaced by a modification of the weights initialization.\n6. The experiments are missing a comparison to the Straight-Through Gumbel-Softmax estimator, introduced in the original Gumbel-Softmax paper. This is a popular biased estimator for Bernoulli latents, e.g. used in [1] [2]. Another interesting comparison would be [3] which proposes a lower-bias version of Gumbel-Softmax.\n7. Figure 2 is missing the line for REBAR, even though this line is referred to on Page 8. Figure 2 and Figure 4 are both labeled as training ELBOs, despite the plots being different.\n\n[1] Andreas Veit, Serge Belongie \u201cConvolutional Networks with Adaptive Inference Graphs\u201d ECCV 2018\n[2] Patrick Chen, Si Si, Sanjiv Kumar, Yang Li, Cho-Jui Hsieh \u201cLearning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks\u201d ICLR 2019 https://openreview.net/forum?id=ByeMB3Act7\n[3] Evgeny Andriyash, Arash Vahdat, Bill Macready \u201cImproved Gradient-Based Optimization Over Discrete Distributions\u201d https://arxiv.org/abs/1810.00116"}