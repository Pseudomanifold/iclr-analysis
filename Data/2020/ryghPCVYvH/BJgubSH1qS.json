{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is a very good paper, building on the idea of Restricted Kernel Machines (drawing a nice parallel between Restricted Bolzman Machines and tools available in the Kernel modelling literature). In this manuscript, the author(s) extend the work to a generative model setting to achieve multi-view generation -- a generative model that can explain correlated variables from a common subspace. The manuscript is well-written and easy to follow and the algorithmic details are clear. Image generation is illustrated on standard datasets (MNIST / CIFAR / CelebA). \nWhile the framework and learning algorithm are good, and novel extensions to what appears to be previous work of the authors, I am less persuaded by the empirical work. Latent variable-based generative modes such as this (and this is motivated in the introduction to the paper) should be judged on if they can extract anything useful about the problem domain in the latent representations that we can interpret. This is not the case here --  the results presented are examples of images that the models can generate. No critical appraisal is given about when the models might fail or when one ought to resort to this approach and not a sample from the plethora of variants of VAE we read about. What have we learnt about images / hand-written characters / faces of popular people from a study like this?\nFrom the above empirical results point of view, I do not think this manuscript is ready for publication, despite what I see as the elegance of the framework. "}