{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work propose a new iterative pruning methods named Continuous Sparsification. It will continuously prune the current weight until it reaches the target ratio instead of iterative prune the weight to specific ratio. The author gives a good analysis but the experiment is not yet convincing enough.\n\n1) This work actually presents compression algorithm with little connection with lottery ticket. As a lottery ticket discussion, it does not give the comparison between lottery ticket and random initialization based on new pruning method. As a pruning method, it does not show the results on common models like VGG and DenseNet with different depth. Figure. 3 only gives results of ResNet-18 while the setting is not the best setting. Normally we need to train at least 120 epochs. It also does not give the experiment on ImageNet. Thus making the conclusion less meaningful\n\n2) The author should also compare the continuous sparsification with one-shot pruning methods (non-iterative) to see the advantage of continuous sprsification."}