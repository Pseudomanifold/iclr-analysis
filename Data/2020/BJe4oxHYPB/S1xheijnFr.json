{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel objective function that can be used to jointly optimize a classification objective while at the same time encourage sparsification in a network. The lottery ticket hypothesis and associated work shows that the iterative pruning of a network can lead to a sparse network that performs with high accuracy. On the other hand, the work of Zhou et al. shows that sparse masks (dubbed \"supermasks\") may be learned without training the parameters of the network. In a sense, this paper tries to combine these ideas by simultaneously training a network while also optimizing the mask.\n\nI think this paper serves as a reasonable contribution to the ever-growing \"lottery ticket hypothesis\" body of work. The paper is mostly clear, and the idea for joint optimization is very reasonable. It's not tremendously original (in that it basically combines two ideas that are already in the literature), but in spite of that, I still think this paper warrants being accepted to ICLR.\n\nFor me, the most interesting scientific point is about the issue of rewinding. In particular, the fact that continuous sparsification can find winning tickets without any parameter rewinding is fascinating and deserves further investigation. Do the authors have any sense for why this works, when prior work suggests that rewinding is necessary for sufficiently complicated models and datasets?\n\nA minor point, I think there's a typo on page 6 in that the paragraph beginning \"Results are presented in Figure 2\" both instances of \"SP\" should be \"SS\" instead."}