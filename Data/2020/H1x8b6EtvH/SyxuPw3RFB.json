{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an algorithm to compress index data representing the sparsity of a (deep) neural network. The ultimate goal is to reduce the storage requirements for the masks.\n\nThe algorithm is based on non-negative matrix factorization so the original mask is decomposed into two smaller matrices leading to memory savings depending on the rank of the matrices.\nIn addition, the paper proposes a tiling approach to further reduce storage requirements. \nResults on selected architectures and datasets show some improvements compared to naive binary index representations\n\nOn the positive side, I see an interesting approach to pruning a neural network, assuming the storage cost is guiding the pruning algorithm. If this was integrated into the training process, the optimizer could lead to an optimal solution. \n\n\nOn the negative side, I find the paper not easy to follow/read. A few comments on this regard:\n\n- The motivation is not clear. The paper mostly targets unstructured sparsity (thus the need for sparse matrix representation). However, not really sure if just addressing the storage requirements would have the proper impact. In the end, the original matrix needs to be recovered from the factorization.\n- The organization of the paper is also confusing.  Section 2 is about factorization, NMS, then the MNIST case study \n- I missed a clear algorithmic section. The title/ conclusions suggest pruning, while the introduction suggests index compression (to me quite different). \nAlgorithm 1 is for the matrix factorization part but how is this integrated into the entire training / fine-tuning process? That is very confusing to me.  \n\n- Interestingly, page 3 starts by suggesting magnitude-based pruning is sub-optimal. However, during the experimental section and the last part of section 2, the paper suggests pruning weights with a large magnitude will damage the performance. That is unclear to me.\n\n- To my understanding, the experiments are not very convincing. Why only those selected architectures and within those pruning only FC layers (AlexNet)? Or, according to table 2 caption, part of the network is pruned using magnitude-based methods and the rest based on the new pruning. That is very confusing. This means the proposal is for pruning or for representing the indexes? If for pruning, why not applying to all the layers in the network? How do we distinguish between the relevance of magnitude pruning and the proposed method?\n- How are the different hyperparameters set (S, Sp, k..). \n- How does section 3.2 fits with the rest? Methods are vaguely described and then one of them used in the experimental section. Very confusing.\n- Table 5 suggests non-zero weights are quantized... I guess I am completely lost now :).  "}