{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a new network pruning method that generates a low-rank binary index matrix to compress index data, and a tile-based factorization technique to save memory. The binary index can achieve larger compression ratio than the CSR index, and the  low-rank binary index can further reduce memory usage. The results for various networks, including DNN, CNN and LSTM, have shown the effectiveness of the propsoed method. The paper is well-written and easy to follow.\n\nIn addition, I have some concerns:\n- Discussion about the relationship between your method with binary neural networks [1,2], especially the networks with binary weights [1].\n- There are no comparison with the state-of-art methods on pruning and index saving, such as deep compression [3] and CNNPack [4].\n- The experiments are not convincing, e.g. only pruning FC5 and FC6 layers in AlexNet on ImageNet dataset.\n\n[1] Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" Advances in neural information processing systems. 2015.\n[2] Hubara, Itay, et al. \"Binarized neural networks.\" Advances in neural information processing systems. 2016.\n[3] Han, Song, Huizi Mao, and William J. Dally. \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\" arXiv preprint arXiv:1510.00149 (2015).\n[4] Wang, Yunhe, et al. \"Cnnpack: Packing convolutional neural networks in the frequency domain.\" Advances in neural information processing systems. 2016."}