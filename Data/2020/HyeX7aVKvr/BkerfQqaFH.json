{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed to address the zero-shot adaptation of tasks by the defined and learned \u201cmeta-mappings\u201d. Technically speaking, this work is built upon HyperNetworks, while its major contribution lies in the homoiconic embedding/treatment of a data point, a task, and a meta-mapping. Despite this fascinating and intriguing idea itself, I am still in doubt about its \u201creal\u201d power, considering that a few important references/baselines are missing.\n\nPros:\n-\tThe ideas of introducing a meta-mapping and treating it similarly or the same as a learning a task itself are novel to me.\n-\tThis work innovatively tackles the problem of zero-shot task adaptation, which is indeed challenging.\n-\tThe case study of the proposed framework in the card game is interesting.\n\nCons:\n-\tThe major concern for this work is its lack of discussion and comparison with state-of-the-art meta-learning baselines. \n    o\tIn fact, the meta-mapping this work learns is the relationship between tasks, and the meta-task the authors mentioned is a group of tasks. In this sense, I strongly suggest the authors to go through the recent work [1], and compare with it. Even the datasets constructed in that paper can be used as a benchmark to validate the proposed method in image classification.\n    o\tMore recent work on explicitly learning the embedding of a task should be noted and compared, including [1][2]. The meta-mapping in this work is still based on embedding of a task, i.e., z_func, which is obtained by the function network. So is it possible to replace the embedding with recent SOTA algorithms?\n    o\tOther basic meta-learning algorithms like MAML[3] and many others should still be compared and discussed, especially in the basic meta-learning setting in Figure 2.\n-\tMany important details and ablation studies are missing, making the work less convincing.\n    o\tWhy don\u2019t merge the results Fig. 2 and Fig. 3 and compare them in a single figure, so that the contribution of meta-mappings can be shown. \n    o\tHow do you train the domain-specific encoder and decoder? Are they generalizable to a wide range of more complicated tasks like image classification? \n    o\tIt would be great to analyze the limitations of the proposed algorithm, especially for the case where a newly coming task belonging to an novel and unseen meta-task arrives. How insightful or predictable is the proposed zero-shot adaptation method in these cases?\n    o\tHow do you obtain the lower-bound (solid line) and the upper-bound (dashed line) of the performance in the figures?\n    o\tWill the dimension of Z influence the performance? Since data points, tasks, and meta-mappings contain different amount of information, investigating the effect of the dimension of Z is key for the robustness of the method.\n-\tThe paper is too expatiatory with many confusion notations and redundant parts, which make it quite difficult to follow.\n\nReferences:\n[1] Hierarchically Structured Meta-learning, ICML19\n[2] TADAM: Task dependent adaptive metric for improved few-shot learning, NeurIPS18\n[3] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n"}