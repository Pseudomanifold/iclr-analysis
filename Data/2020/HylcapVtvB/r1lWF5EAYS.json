{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes methods for improving the performance of differentially private models when additional related but public data is available. The authors propose to fine-tune the private model with related but public data. Two heuristics for selecting data to fine-tune with are presented. \n\nI vote for rejecting the paper. My main concerns revolve around the paper's contributions. It neither makes core contributions to differential privacy nor to active learning. What it does instead is present a post-processing procedure which boils down to fine-tuning differentially private models with public data. Arguably the main contribution of the paper is the heuristics for selecting public instances for fine-tuning. These rely on several design choices (why k-means? how are points from within a cluster sampled -- uniformly at random, some other heuristic? how are uncertain private and public points matched, by solving an assignment problem or something else? ) that are not well justified or have missing details and more importantly do not clearly outperform baseline acquisition functions - selecting points based on predictive entropy or the difference between two largest logits. Finally, I am unconvinced that the paper\u2019s premise of having access to related public data is realistic. \n\nThis would likely not change my opinion of the paper, but it would be good to substantiate the claims made in the second paragraph of the paper \u2014 the performance of differentially private models degrades with increasing model size. If the relationship is as clear as the paragraph makes it sound, then it would be great to include a graph with the x-axis being the number of parameters and the y-axis being the difference in performance between a regular and a differentially private model. One would expect to see the difference increase with model size."}