{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends existing work on certified robustness using smoothed classifier.  \nThe fundamental contribution is a framework that allows for arbitrary smoothing measure, in which certificates can be obtained by convex optimization.  \nA good number of technical contributions\nFramework for certificate under arbitrary smoothing measure -> Theorem 1, and proof in A.4 (good use of duality)\nFull calculation/result of certificates under different divergneces in Table 1.\nReasonable set of empirical evidence. \n\nOverall a lot of good things to be said, below are some questions/comments that could improve the paper:\n*Technical*\nPersonally, I cannot get a lot of value out of the distinction between full-information and information-limited certification.  It\u2019d be great if I can get some clarification on this.  \n*Experiments*\nGenerally, more details of the experiments should be included.  \nHow are the convex optimization problems actually solved (e.g., what methods/tools)?  \nHow much more computational overhead is there?\nOddly, seems like we\u2019re missing CIFAR10 results completely. \n\nSec 5.1., \nWhat does each dot in Figure 3a represent?\nSec 5.2, \nSomewhat strangely, Figure 3b is results on l0 perturbation, but not l2.  What happens for l2 when we use M>1?  How does this compare to other extensions (likely the SOTA) like [1]?\nSec 5.3,\nIt is unclear from the writing whether the comparisons to previous works were done on the same ResNet architecutre with the same clean accuracy.  Please clarify. \nI\u2019m not sure why we need the Librispeech results.  It\u2019s not motivated clearly.  Also, from writing it seems the adversary zeros out a segment.  It\u2019s unclear if this is a reasonable kind of attack to expect on speech.  If I block out a segment of the speaker, we probably don\u2019t expect any system to do well on speaker recognition.  I suggest removing this result, or somehow make it a lot more better motivated/conducted.  Clarify if I missed reasons why simply showing your method works on speech is impressive.\n\nHere are suggestions on writing:\nContribution --- in both the abstract and introduction, the experimental results should be stated clearer.  Be more specific, e.g., \u201cShow SOTA certified l2 robustness on X,Y,Z, establish first certified robustness on Librispeech, and first results on certified l0, l1 robustness on A,B,C.\u201d\nMore broadly in the introduction, please motivate why \u201cadversarial attacks as measured by other smoothing measure is important\u201d.  Past studies focus on l2-norm not just because they are do-able, but also white noise (which is naturally measured by l2-norm) is something to expect in practice.  Justify why l0, l1 would also be important.\n\nI recommend accepting this paper, but would do so with more passion if some of the comments/questions can be addressed.\n\nBest,\n\nReference:\n[1] https://arxiv.org/abs/1906.04584\n"}