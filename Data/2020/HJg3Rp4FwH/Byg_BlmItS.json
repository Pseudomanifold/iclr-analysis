{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "I enjoyed this paper overall, and I think the idea is a good one. However there remain significant issues with the paper that preclude me giving a good score. Firstly, there is almost no discussion of the environment model. Anyone who has worked on Model Based RL will tell you that the details here are crucial. This deserves a full discussion, and a comparison to other methods in the literature.\n\nNext, the experimental results really aren't convincing. The dependence on random seeds is worrying, and isn't as common in model free algorithms as you claim, which are mostly robust to seeds (the good ones at least). The fact that the best policy is risk *averse* is very strange, since these are estimates combining both the epistemic and aleatoric uncertainty (which is somewhat unfortunate), which means being risk averse would lead the agent to not explore. That is very worrying and makes me think that something very strange is going on with the models. In fact since the policy is deterministic and the environment / rewards are practically speaking deterministic, the uncertainty here is actually mostly epistemic and so a c < 0 means the agent is disincentivized from exploring.\n\nThere should be more discussion about the fact that the policy is deterministic. Is this merely to make estimating V^pi easier?\n\n\"Next, we provide a convergence convergence analysis and show that maximizing this utility function\nU(\u03c0) is equivalent to maximizing the unknown value function V (\u03c0).\"\n\nWord convergence appears twice in a row, but more importantly this is totally missing! Where is the analysis?\n\nIn the algorithm you write:\n\"Update {fb} and r\u02c6\u03c6 using SGD\"\nBut on what data? Presumably sampled from D but this isn't mentioned.\n\nIs it the case that the policy is updated *only* using the model based rollouts? I.e., the reward signal is never used in the policy gradient but only used to train the models? If so, this seems quite fragile and I would like to see a comparison of different approaches here.\n\nTable 2 is unreadable and needs to be explained.\n\nIt would appear that you are missing a reference to the very relevant UBE paper, which also deals explicitly with the uncertainty of the value function estimates: https://arxiv.org/abs/1709.05380\nIn fact I would be curious to see any way that these two approaches could be combined (though that would be follow up work).\n"}