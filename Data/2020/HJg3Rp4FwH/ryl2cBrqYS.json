{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "Summary:\nThe main contribution of this work is introducing the uncertainty-aware value function prediction into model-based RL, which can be used to balance the risk and return empirically. \n\nMethodology\nThis work uses a linear combination of the mean and standard deviation of value function to capture the uncertainty in learning state value function. \n\nIt is not clear how to convert the objective function from Eq 2 (expectation over the initial state) to Eq 5 (expectation over all states). Those two objectives are not equal.\n\nIt is not clear how does the uncertainty in model prediction (dynamics and reward function)\ncan be alleviated through the proposed method, as claimed in the introduction. \nIt seems the novelty part lies in considering the uncertainty of value function estimation. \nHow does this relate to solving the limitation of model predictive control? \n\nWhat is the objective function for learning reward function r_\\phi?\n\n\nExperimental results:\nThe experiments are not sufficient to demonstrate the effectiveness of the proposed method. \nIt would be more convincing to compare the proposed method with a few more model-based approaches on more tasks. The results of MBPO is better\nthe proposed POUM in one of two tasks. The performance of\nMBPO on Reacher-v2 and Pusher-v2 is missing? \n\n\nWriting:\nThis paper has many typos and the presentation is not very clear.\n- Section 4.1 \"in Section 3.4,\" \n- Last paragraph in P3: convergence convergence analysis \n- \"shows that POUM has a sample efficiency compared\"\n"}