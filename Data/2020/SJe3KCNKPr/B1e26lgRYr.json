{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This manuscript proposes an approach to reduce memory access and computation in Recurrent Neural Networks.  Specifically, they train a second \"little\" neural network to approximate a pre-trained \"big\" network and use simple rules to switch between the little and the big network.  The approach can provide some speedups while reducing the total number of memory accesses and the computational cost in exchange for a mild decrease in predictive performance.\n\nWhile this manuscript proposes a reasonable contribution, it lacks real comparisons to many of the common competing methods that hinder the interpretation.\n\nWeight sparsity/pruning is a very common approach that has shown the ability for larger speedups than what is shown here.  I disagree with the assessment that \"those methods require extensive retraining via regularization.\" Realistically, you can take a pretrained model and add the penalties with mild re-training and extensive reuse of code.  The result is also simpler to implement.  I would argue that this is less work than the proposed approach, which requires switching rules and a second trained network.  I don't know which is better, but the authors should actually evaluate whether their approach improves over the more popular approach.\n\nThe authors should give better discussion and motivation on the random projections.  This is an area with very deep theory, yet the rules are provided without a rationale.  Realistically, where does the sparsity level in the random sparse matrix come from?  Why use the rule for k in (3)?  The authors should motivate and discuss this section more.\n\nAlso note that there is existing literature on learning multiple models and switching between them, for example:\nBolukbasi, Tolga, et al. \"Adaptive neural networks for efficient inference.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\nAs there is a lot of similarity in the motivations, you should discuss that line of research in your related work."}