{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors design a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup RNN inference.\nIt cannot only mitigate the memory-bound problem to speedup RNN inference but also leverage the error resilience of nonlinear activation functions by using the lightweight little module to compute for the insensitive region and using the big module with skipped memory access and computation. They also conduct several experiments to evaluate their approaches.\n\n\nStrength:\n(1)\tWell written in general.\n(2)\tContributions clearly stated and justified\n\nA couple of minor questions:\n\n(1)\tThe organization in Section 3 can to be improved. It is better if the author can give a brief overview of their method first and then go into details.\n(2)\tSome of the technical details necessary for understanding the soundness of the techniques are either missing or are poorly explained. For example, in Section 3\na.\tthe authors did not mention how to construct the HH module\nb.\tthe authors did not provide detailed information of how to conduct dimension reduction since this will affect the performance\nc.\tmany mathematical notations and equations need to be revised to increase the readability. For instance, there is no information in the paper that explain why the authors design functions in such certain way (such as equation (2) and (3))\nd.\tthe authors did not provide enough detailed information about how to select the quantization methods since there are lots of approaches such as static (uniform) or dynamic quantization, where different methods may have different impacts on the final performance \ne.\tthe authors mentioned that they have tried both sigmoid and tanh activation function to find the sensitive region. However, they do not provide enough reason to do so, how about using other non-linear activation functions\n\n(3)\tThe organization in Section 4 can to be improved. It is better if the author can introduce the motivation of each experiment.\n\n(4)\tParameters of the evaluation are unclear or missing. For example, \na.\twhat is the data size, what is the dropout, learning rate, how many time stamps for the RNN modules\nb.\twhy the authors only use single-layer LSTM and why they select 750 and 1500 hidden units in the experiments\n\n(5)\tWhile the authors have applied their models on other existing method, they do not provide good discussion of results and such model seems old (released in 2016). It would be great if this approach can also be applied on other newly released models\n(6)\tSome tables need to be reorganized. For instance, for table 6, there needs some space between the title and the table.\n(7)\tWhile the paper has good coverage of the prior work, I do suggest the authors can also cite or discuss some newly designed models (in 2019).\n"}