{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper \u201caims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. \u201d The author\u2019s empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. Specifically, the authors empirically study \n- the ability of networks to recover latent variables\n- the effects of extreme overparameterization\n- the effects the training method (e.g. batch size)\n- latent variable stability over the course of training\n\nIn line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. This is an interesting and useful observation, particularly since it at first sight appears to be in disagreement with some earlier work (the authors suggest explanations for the differing observations). \n\nAs the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. \n\nThe authors perform studies on a range of different real-world and synthetic datasets. \n\nThe paper is well-written, well-structured, and easy to follow. Relevant literature has been cited. The appendices contain a wealth of details that will make this work reproducible. \n\nDecision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger ICLR papers. \n\nA small gripe: the authors promise \u201c a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings\u201d. I would argue that \u201cmaking precise\u201d is too strong for what the paper actually delivers. I suggest rewording this."}