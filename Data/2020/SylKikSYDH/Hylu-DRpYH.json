{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates a so-called \"compressive transformer\" approach. The idea is to compress distant past memories into a coarse-grained representation while keeping a fine-grained representation for close past memories.  A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. Particularly, the authors propose a new benchmark PG-19 for long-term sequence modeling.  \n\nOverall, I found the work interesting and experiments are thorough and strong.   It is always great to see a new benchmark released to the community.  That being said, I have concerns regarding the paper.  The authors put huge amount of effort into the experiments but only describe the proposed technique in a very rough and abstract way, lacking necessary technical details to formulate the technique. What is the mathematical formulation of the problem?  How exactly the compression is carried out on various network architectures is not clear after reading the paper.  Also, I guess many readers including me do not have a perfect understanding of Fig. 1 although it shows something intuitively. (What is the difference between different colors? What is the difference between sequence, memory, and compressed memory?  What do the arrows mean? There is no explanation whatsoever either in the figure or in the caption).  This is the major concern I have regarding the paper.  Despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper.  "}