{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a simple approach for document-level machine translation. The idea is to use a language model on the target side and a reverse translation model to choose the best document-level translation. This is theoretically justified by Bayes\u2019 rule and the assumption that the sentences are conditionally independent. The authors implement this idea using a reranking model that rescores 50 candidate translations generated by a standard Transformer model for forward translation. \n\nThis is interesting work and the experimental results demonstrate the effectiveness of the approach. However, I am concerned about the (missing) comparison between the proposed approach and the approach that combines backtranslation and a document-level translator (e.g.  Doc-transformer). It seems to me that one could backtranslate a large monolingual corpus and use the resulting parallel documents as additional training data for a document-level translation model. How does the proposed approach compare to such a backtranslation approach?\n\nAnother concern is the speed of translation. It seems to me that the computational cost required for generating 50 candidates and reranking them is quite high. I would like to see some experimental results on the actual speed of translation. The aforementioned backtranslation approach should not have this problem, which also makes me unsure about the usefulness of the proposed approach in practice."}