{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a mete-learning approach to learn a loss function from old tasks which can generalize well to new tasks. The benefits of the proposed approaches are 1) data-driven a loss function and 2) allowing the usage of extra side-information to design the loss function.\n\nOverall, the presentation in this paper is hard for me to understand technical details and see the difference with existing methods. Please see the questions below. I am glad to discuss problems with the authors' reply during the rebuttal.\n\nQ1. \"M\u03c6(y; f\u03b8(x)) that predicts the loss gave the ground truth target y and the predicted target f\u03b8(x)\", \"the purpose...loss function, or a meta-loss, which generalizes across multiple training contexts or tasks\".\n- It is better for the authors to visualize all y v.s. f_{\\theta}(x) for all experiments. They have done this only for \"Section 4.2.1 SHAPING LOSS FOR REGRESSION\". It is better to do this for all the experiments. In this way, we can see why meta-loss can better.\n- Besides, it is also better to show some example samples where the learning loss is significantly different from the human-designed one. This helps the reader better understand why the meta-loss can better.\n- Finally, the authors claim the extra-information used in the meta-training is helpful. How can we see this point? There is not a step-by-step ablation study on this point.\n\nQ2. Except for problem setup, i.e., learning a loss function, what are novelties in using meta-learning techniques?\n\nQ3. The authors present three usages of the proposed framework in Section 3. Could the authors describe one in detail and then briefly mention the other two usages instead of writing them with the same importance? In this way, readers can understand materials and novelties better. \n- For example, I do not understand how exactly gradients are updated on meta-level. The description in Section 3.1. is too brief. \n\nQ4. Why the convergence speed of the meta-learner is important? e.g., Figure 4(b-c).\n\nQ5. We have some basic restrictions for \"loss function\", i.e., loss(x, y) >= 0 for any x, y; loss(x, x) = 0. How such basic requirements are ensured by the learned meta-loss?\n\nQ6. Could the authors add more explanation in the experiments and motivation in the main text? Currently, the authors just describe what they have done in the proposed method and what they have observed in experiments, just a list of facts (see Q1)."}