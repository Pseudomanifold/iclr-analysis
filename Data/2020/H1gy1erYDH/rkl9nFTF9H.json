{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The submission proposes to train a GAN on discrete sequences using the straight-through Gumbel estimator introduced in Jang et al. (2016) in combination with gradient centering. The proposed approach is evaluated on COCO and EMNLP News in terms of BLEU and Self-BLEU scores, Fr\u00e9chet Embedding Distance, Language Model Score, and Reverse Language Model Score.\n\nMy assessment is that the submission is below the acceptance bar, mainly due to clarity and novelty concerns. The proposed approach does have empirical backing, but I would argue that it is a very straightforward application of the straight-through Gumbel estimator to GANs, which is itself similar to existing work on applying the Gumbel-softmax estimator to GANs (Kusner & Hern\u00e1ndez-Lobato, 2016). Detailed comments can be found below.\n\nThe submission does not feel self-contained. For instance, it borrows notation from Jang et al. (2016) without explicitly acknowledging it, and my personal experience is that reading Jang et al. (2016) beforehand makes a big difference in terms of clarity in Section 2.2.\n\nThe notation is inconsistent and confusing, and gets in the way of understanding the proposed approach. Here\u2019s a (non-exhaustive) list of examples:\n\n- The reward function is first introduced as f_\\phi(\\mathbf{x}) above Equation 3, but all subsequent mentions of the reward function use f_\\phi(\\hat{\\mathbf{x}}).\n- The \\mathbf{m}_\\theta variable is introduced in Equation 5 and is immediately replaced with \\mathbf{p}_\\theta, which adds notational overhead without any benefit.\n- The difference between \\hat{\\mathbf{x}} and \\hat{x} is not explained in the text. From the context I understand that \\hat{x} is a categorical scalar in {1, \u2026, V}; is this correct?\n- In Equation 6, x_1, \u2026, x_V are used to denote the *values* that \\hat{x} can take. This clashes with the previous convention that \\mathbf{x} is a sequence sampled from p_{data} (Equation 1). Given that convention and the difference between bolded and non-bolded variables discussed above, I would have expected that x_1, \u2026, x_V would correspond to the categorical values of elements of the \\mathbf{x} sequence. That contributes to confusion in Equation 9, where \\mathbf{e}_{x_t} and p_\\theta(x_t) are *not* time-dependent.\n- Equation 8 sums over time steps, but the first summation that appears in Equation 8 does not make use of the temporal index. There is also a symbol collision for T, which is used both as the sequence length and as the \"transpose\" symbol.\n\nAs a result, the proposed centering method and the rationale for it is still not entirely clear to me. In particular, is the gradient centering approach necessary to avoid the drawback of score function-based approaches (i.e. the generator is only given feedback on the tokens it samples), or does the non-centered, straight-through variant of the proposed approach also avoid this drawback?\n\nI\u2019m also not convinced that the centering heuristic is a crucial component of the proposed approach when the biggest improvement observed over the straight-through baseline is obtained by adding spectral normalization. I would argue that the proposed approach is a straightforward application of the straight-through Gumbel gradient estimator to GAN training, which is similar in spirit to work by Kusner & Hern\u00e1ndez-Lobato (2016) (not cited in the submission) -- the main difference being that the latter uses the Gumbel-softmax distribution directly and anneals the temperature parameter over the course of training. A comparison between the two would be warranted.\n\nReferences:\n\n- Kusner, M. J., & Hern\u00e1ndez-Lobato, J. M. (2016). GANs for sequences of discrete elements with the Gumbel-softmax distribution. arXiv:1611.04051."}