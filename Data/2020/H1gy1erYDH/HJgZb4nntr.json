{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose CaptainGAN, a method using the straight-through gradient estimator to improve training of the generator for text generation.\n\nThe paper is well-written and the evaluation seems thorough, comparing to relevant baselines.\n\nComments:\n\nFigure 3: the caption refers to Caccia et al. for results on LeakGAN, MaliGAN and seqGAN, but unless I\u2019ve missed it, RelGAN hasn\u2019t yet been introduced by name as a baseline? The citation is given in the opening part of the introduction, in an enumeration, but isn\u2019t revisited later in the text - not even here where the results of the model are introduced. Given that it seems, according to the presented results, to be the most competitive of the GAN models that the authors are comparing to, maybe it\u2019s worth adding more contextual information on RelGAN to the Background section?\n\nFor their method, the authors should report an average performance over several random seeds and provide the standard deviation / confidence intervals, for the readers to be able to assess the stability of the method and the significance of the improvement reported in the results.\n\nI find Section 5.5. particularly interesting, as well as the reported perplexity in Table 2. The authors provide 3 bullet points to explain the unusually high perplexity of the generator on the training and validation data. I feel that the explanations that are given are at the moment vague and not visibly backed by data, therefore being speculative. Obviously, point 1) is hard to quantify - but point 2) could possibly be at least partially quantified - if the hypothesis is that names, places, punctuation marks etc play an important role in the reported perplexity score, then maybe the authors could test this by correlating model perplexity on sentences with whether those sentences contain these types of words?\n"}