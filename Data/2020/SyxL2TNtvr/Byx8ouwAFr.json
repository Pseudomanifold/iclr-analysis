{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a criterion called Unsupervised Disentanglement Ranking (UDR) score. The score is computed based on the assumption that good disentangled representations are alike, while the representations can be entangled in multiple possible ways.  The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method.\n\nThe problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable. However, existing metrics for hyperparameter tuning and model selection requires ground-truth labels. This paper allows measuring model performance without supervision, making the hyperparameter tuning and model selection possible in practice.\n\nIt looks like some parts of this paper need rewriting. In the abstract, it is not mentioned at all what is the proposed approach. Most paragraphs in the introduction section review the related work and background but do not introduce what assumption and strategy the proposed method adopted.\n\nIt looks like the proposed UDR is theoretically supported by Rolinek et al. (2019). However, the proof given by Rolinek et al. (2019) is for $\\beta$-VAE, where the regularization can be turned into the constraint on KL divergence. I do not think the \"polarised regime\" holds for other disentangled model, for example, TCVAE, where a biased estimation of total correlation is introduced in the objective function. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models.\n\nThe computational process of UDR is heuristic and somewhat arbitrary. There is no theoretical guarantee that UDR should be a useful disentanglement metric.  Although the UDR is supported by some experiments, I am not convinced that it is trustworthy for more complex real-world datasets.\n\nEquation (3) looks problematic. Note that it is possible to train a Bidirectional Generative Adversarial Network (BiGAN) that can generate complex images based on a uniform distribution (Donahue et al., 2016). The encoder of the BiGAN can be considered as the inverse of the generator, which maps images back to the uniform distribution. This suggests that under the encoder-decoder framework, it is possible that latent variables can be informative even the posterior distribution matches the prior distribution. Although VAEs are trained using a different strategy, I do not see why the posterior needs to diverge from the prior distribution for informative latent representations. The encoder might simply be the inverse of the decoder under a certain scenario.\n\nIn summary, this paper focuses on solving an important problem. However, the proposed method is not well supported by theorems as it seems. The paper also appears to contain minor technical issues. Therefore, I am inclined to reject this paper.\n\nReferences\nDonahue, Jeff, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. \"Adversarial feature learning.\" arXiv preprint arXiv:1605.09782 (2016)."}