{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "*Paper summary* \n\nThe authors design a learnable time-series pre-processing, which they refer to as a learnable group transform (LGT). This is a generalization of the wavelet transform, which maps a time-series signal onto the affine group. In the wavelet transform, multiple scaled and shifted versions of a mother wavelet are \"inner-producted\" with a signal; the resulting coefficents are the output of the transform. In the LGT, a more flexible transform that just scaling and shifting is applied to the shape of the mother wavelet, which is piece-wise linearly stretched. This elegantly encompasses time-warping and many other wavelet-style transforms into one learnable preprocessing step.\n\n*Paper decision* \n\nI would like to recommend this paper be accepted. It is clearly written and the idea is simple; that said, the idea is an elegant generalization of the wavelet transform. (I admit my own expertise is not in time-series data, so I may be mistaken). The experiments are also simple, but straightforward and easy to reimplement.\n\n*Supporting arguments* \n\nWhat I like about this paper is that the idea is a straightforward generalization of the wavelet transform through the lens of group theory. Furthermore, other transforms, such as the short-time fourier transform, or methods such as time-warping are easily encompassed by the method. By making the transform learnable, the authors reduce the number of in-built assumptions in the problem. \n\nI believe the idea is novel; although, since this is not my area of expertise, I defer to other reviewers who may wish to contest this.\n\nI do wonder, however, what the interpretation is behind some of the learned LGTs. Typically, a group transform is favored because of certain symmetry properties of the task at hand. For instance, wavelet transforms are shift-covariant, reflecting the shift-covariance of underlying signal statistics.\n\nExperimentally, I think there was a nice selection of toy and harder examples. I found the visualized filters and group transforms in the appendix really interesting. I think it would have been nicer to see some more analysis of the interpretation of the learned LGT in the main text though. Further, it would have been nicer to see some ablation studies conducted, such as varying the number of degrees of freedom in the LGT and seeing how that affects performance.\n\n\n*Smaller questions/notes for the authors*\n\n- The explanation of what a group is is quite high-level and I think if you did not know what it was beforehand, it would be hard to understand what one is. For instance, just after introducing groups, homomorphisms are mentioned. I would guess that someone who has never seen a group beforehand, would have no idea about homomorphisms.\n\n- In equation 6, is \\star a convolution or a cross-correlation? It looks like a cross-correlation to me.\n\n- I really liked the connection drawn between the group transform and time-warping. This is an aspect I personally had never considered.\n\n- Would it be possible to move some of the material on the learned filter transformations to the main text and write some analysis, even if it is only qualitative? I would find that fascinating.\n"}