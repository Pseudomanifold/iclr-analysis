{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "A typical Wavelet Transform is built through the dilation and/or rotation of a mother wavelet, which can been viewed as a group action on a mother wavelet. This work proposes to extend this construction beyond the Euclidean group, and to supervisedly learn operators that will be applied on a mother wavelet. Competitive numerical performances are obtained.\n\nOverall, I think that re-thinking the way a Wavelet Transform is designed, is an interesting direction of research, but I think some of the theoretical tools developed in this paper are not dedicated to achieve this purpose. In particular, the group/representation properties seem to not be used, and the authors could simply consider a specific subset of invertible mapping on $\\mathbb{R}^2$ which would be applied on the mother wavelet and lead to a Wavelet Transform. In other words, the overall formulation could be simplified.\n\n\nPros:\n- In general, the numerical experiments are at the level of the state of the art.\n- Parametrizing a subset of the group of increasing function and its application to signal processing tools is novel, to my knowledge.\n\nCons:\n- Some very relevant elements in the literature review are missing. Learning or using an underlying group of symmetry that will be combined with a deep neural network is not novel, cf: https://arxiv.org/abs/1601.04920 ; in particular for reducing the number of parameters, filters or samples: https://arxiv.org/abs/1809.06367 ; http://proceedings.mlr.press/v48/cohenc16.pdf ; https://arxiv.org/abs/1809.10200 ; https://arxiv.org/abs/1605.06644 - I think the authors should discuss at least one or two of those papers, if not all.\n- The performance on the bird detection task is good but the improvement compared to other work is not clear, given that some supervision in the first layer is incorporated.\n- Subsections 2.2 and 2.3 are difficult to parse because the authors introduce a lot of equations or notion that are not useful to understand their algorithm/method. The equation (6) seems wrong to me (one should consider t->s_i(-t) and not t->s_i(t) and b seems missing in the second line).\n- Figure 2 is difficult to read because of the illustrative graphics. Maybe a block schema would be easier to parse.\n- It seems to me that no-where the group properties are used, such as the stability to composition. In this paper, the authors simply try to parametrize a diffeomorphism to dilate the mother wavelet. From my understanding of 3.3, the subset of function used to approximate $G_{inc}$ do not form a subgroup as well, contrary to the Euclidean case, where for instance discrete rotations in the case of images are a finite group.\n- In subsection 4.1(Table 1), a comparison with a wavelet transform followed by a linear operator is compared with the proposed method. I find this result surprising :\u2028LGT/nLGT/cLGT/cnLGT and the WT are some linear methods whereas the STFT is non linear. As the WT should be unitary, if the linear classifier method is reasonably trained, then both methods should lead to the same result, except if the data are poorly conditioned. In which case, this experiment would not be meaningful. I think the authors should comment more this result because it is surprising.\n- I slightly disagree with the sentence \"in the case of WT, the precision in frequency degrades as the frequency increases\". Actually, the heisenberg principle is optimally optimized by wavelets, meaning that the area of the frequency/spatial support on a spectrogram is constant. On the contrary, the STFT has a lack of localisation (and thus the \"precision\" is not constant along frequencies). Maybe this could be rephrased slightly.\n- Given the filter learned in Figure 5, one can wonder if a foveal approach (i.e., Foveal Wavelets) could perform similarly? It would be interesting to display the littlewood-paley plot(i.e., the sum of the modulus of the filters in the Fourier domain) of this representation to understand the nature of this operator in the Fourier domain.\n- I think group actions could be considered instead of representations: it would be simpler to understand for a potential reader.\n\nTypos: \n- abstract \"in order to transform the mother ..\" > \"in order to transform a mother..\"\n- page 7 \"this variation is as not captured as well\" > \"this variation is not captured as well\".\n"}