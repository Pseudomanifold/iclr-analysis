{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Paper proposes three improvements upon the gradient episodic memory (GEM) method [Lopez-Paz,2017]. The three improvements address 1. A way of selecting which exemplars to store (originally random), 2. The strictness of the constraints is loosened by considering slack vectors, 3. The update is improved by promoting positive backward transfer, not only limiting the gradient to the constraints imposed by exemplars of previous tasks, but aiming to improve also for previous tasks by preferring gradients which have high cosine similarity with the gradients on exemplars. Results on CIFAR 100 and MNIST permuted are presented. Noteworthy is the improved backward transfer on CIFAR 100 when compared with GEM.\n\nConclusion: The proposed improvements seem sensible. I liked especially the one which aims at improved backward transfer. However, the paper should have built upon the more recent A-GEM paper. Also, the proposals show strange behavior in the ablation study, and I am not convinced they all contribute to better performance. Finally, the gain with respect to GEM is very small. I, therefore, recommend a weak reject.\n\n1.       The authors somehow missed the more recent paper \u2018EFFICIENT LIFELONG LEARNING WITH A-GEM\u2019. Their analysis should compare with this paper. This new paper does not address the points addressed in this paper, but even so, since it obtains generally slightly better results but is much more efficient, it is a better starting point. The relaxing of the constraints (with slacking vectors) might be less efficient in A-GEM which already relaxes the constraints.\n2.       Improved exemplar sampling: The authors say \u2018we cannot afford to assign memory to the examples whose margins are negative\u2019, why not ?  I would like to see this ablated.\n3.       Why does the average accuracy start so low ? Are they also averaging over the unseen classes? Does it not make more sense to just average over the seen classes (those considered until the current task)?\n4.       The gain with respect to GEM is very small in Figure 1.\n5.       Could the authors explain their validation protocol (A-GEM also makes a point of that).\n6.       I think there is a typo in the equation, should have a j index on the left side, and should be A_jn-A_nn\n7.        Ablation study should be better written (it takes a while to understand to which of the three proposals the abbreviations link). The results of the ablation do not convincingly show the merits of two of the three proposals. These only work when combined with \u2018threshold\u2019. No reason/explanation is provided for this strange behavior. (error in the legend the support+soft should be green)."}