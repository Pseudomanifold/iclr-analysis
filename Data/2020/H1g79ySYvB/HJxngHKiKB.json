{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an extension to gradient episodic memory (GEM) to improve its performance and backwards transfer. Specifically, the proposed method selects \"support examples\" to represent each task (versus the last M examples for GEM); introduces slack variables to ensure the constraints imposed by GEM are not too restrictive; and uses cosine similarity between sample gradients to encourage backwards transfer.\n\nThe ideas proposed are interesting, and the paper is easy to follow.\n\nHowever, I have a number of concerns which I think, unfortunately, preclude publication at this point.\n\nPrimarily, while I believe the ideas are intuitive and novel, the experimental evaluation does not appear to support the claim that the proposed method significantly improves GEM (only a minor improvement is shown at best). This is further compounded by the fact that variances over multiple seeds/runs are not reported, which makes it difficult to gauge any statistically significant performance improvement.\n\nSome claims in the paper also need to be tempered:\n- The abstract suggests performance improves \"remarkably\", but experiments do not support this.\n- The last paragraph of Section 4.2 declares that adding a offset to the Lagrangian, as suggested by the GEM authors, \"is an ad-hoc practice and lacks rationality: it is not faithfully follow their mathematical formulation\". This may come across as confrontational, and I don't feel the point is valid in this case, given that the soft gradient constraints introduce a similar trade-off parameter.\n\nThe language is also imprecise and conversational at times; I would suggest another read-through and careful rewording for clarity. For example, in 4.2.2, \"this new constraint imposes cosine similarity actually...\".\n\nLastly, I'm not sure about this, but does the link to the GitHub repository break anonymity? Given the language difference, I'm not sure what the url refers to; but wonder if it could be a name.\n\nThe ideas have potential, and I suggest the authors explore avenues to further improve the efficacy of the method and expand the experimentation, and improve some of the writing in terms of language and claims made.\n\n(A note on the score: I don't think this is a 1 under the old ICLR system, but unfortunately have to recommend it be rejected in its current state)"}