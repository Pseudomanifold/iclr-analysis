{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This article present an approach to assign an importance value to an\nobservation, that quantifies its usefulness for a specific objective task. \nThis is useful in many contexts, such as domain adaptation, corrupted sample\ndiscovery or robust learning.\nThe importance values may also be used to improve the performance of a model for the task.\n\nThe importance values are learned jointly with that model. \nA small neural network called by the authors a Data Value Estimator (DVE) is\nlearnt by the authors to estimate sample selection probabilities, which will\ndictate which instances will be used for the main model that tackles the\nobjective task. \nWhile the main model is trained through usual mini-batch gradient descent, the DVE can\nnot be, since the sampling process is not differentiable. \nIt follows that the DVE is trained with a RL signal, that follows\nthe variation of the loss throughout the learning process.\n\nThe method proposed by the authors is new and show very significant results\nover existing methods. It is scalable, while many of the presented approaches are not.\nIt is said to have a much lower computational burden than some existing methods,\ne.g. LOO or Data Shapley.\nThe paper is very well written, and the method is illustrated on several datasets,\nfrom different domains.\n\nHowever, it seems that many approaches that did not suffer from the same complexity\ndrawbacks of LOO and Data Shapley were not compared to this work. While some\nof the presented approaches are recent, e.g. ChoiceNet (2018), others are more established,\ne.g. domain adversarial networks (DANs, Ganin et al 2016) and are not compared for\ndomain adaptation tasks. Given that the contributions of the authors are solely empirical,\nit is necessary to compare their approach to other scalable domain adaptation approaches.\nThe approach proposed by the authors also features many hyperparameters, with\nfixed chosen values, and the architecture of the DVE is not precised, which may impair the\nreproducibility of the paper. \n\nThe authors should provide code if not already provided.  \nThere is a mistake on the legends of Figure 2 and Figure 3, since accuracy\nshould increase when removing the least important samples."}