{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present an approach to optimize determinantal point processes directly (by gradient descent, instead of . via approximations), so that diversity could be modeled in objective functions for deep learning systems. The approach taken is to express the DPP term as an L-ensemble in the spectral domain over the gram matrix. They also generate sub-gradients for cases where the gradient does not exist (when the gram matrix is not invertible).\n\nThe approach is interesting, and the problem (modeling of diversity constraints) seems important. They have experimental results (on metric learning and image learning tasks) to show that optimization with the DPP + wasserstein  gan constraint (to ensure features lie in a bounded space) result in better quality. However, they do not discuss the performance (comptuational) impact, or compare their approach to other approximation based systems (such as the approach of Elfeki et al)."}