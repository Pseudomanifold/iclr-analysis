{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper builds on the line of work of developing latent variable models (LVMs) for collaborative filtering with implicit feedback (e.g. which items a user has previously clicked on).  While variational autoencoders allow convenient construction of nonlinear LVMs, they are trained by maximizing the multinomial likelihood for the items selected.  Thus the training objective is not perfectly matched to the evaluation objective, which is typically something like NDCG @N or RECALL @N, neither of which is differentiable.  This paper proposes an actor-critic RL approach to train the nonlinear LVM directly for the NDCG loss.  The idea is to create a critic model that learns to approximate NDCG, and to train the actor to optimize this approximate objective.  However, they find that they are unable to build an effective critic when taking the predictions and ground truth directly.  However, they find that with a simple set of 3-features, they can build an effective critic that gives improved or competitive performance on several metrics on 3 large-scale datasets.  When their method (RaCT) is not the best, it is beaten by EASE, which is computationally much more expensive.\n\nThis work seems to fit naturally into the line of work on nonlinear LVMs.  While porting MLE training to RL training is a standard operation these days, in this case it wasn't entirely trivial -- one couldn't just use REINFORCE and be done.  Introducing features and an actor/critic setup isn't an amazing innovation, but the method works well, has computational benefits, and has compared to many other models on 3 datasets.  The paper is very polished and nice to read so that somebody can really learn from it. I vote for accepting this paper.\n\nSome specific questions and comments:\n- In the 'Pitfals of VAEs' section, it says \"A preserves the ranking order of the target, while B does not.\"  I'm seeing that A and B give the same rankings, so I don't understand the comment.\n- I don't quite understand the explanation for why REINFORCE wouldn't be effective.  I'm more convinced by the fact that the critic failed to learn when given the full prediction set and ground truth."}