{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper attempts to give theoretical support for using shared representations among multiple tasks.  The architecture has already been proposed in another paper.  The main contribution of the paper is the theory that it claims to support this architecture.  However, I am dubious that the architecture achieves the claimed bound.  One, I do not see how the analysis of this paper is connected to this specific architecture.  Two, the analysis follows from an existing paper by Farahmand (2011), which has already established a bound on the difference Q* - Q^{\\pi K}.  This paper considers the same difference, separately for each task, so that the same bound from Farahmand (2011) can be trivially used.  The shared layer h would affect the Q-values, but considering only the difference Q_t* - Q_t{\\pi K} abstracts away how sharing representations is helpful.  Simply averaging the norm of the difference between Q_t* - Q_t{\\pi K} as if they are independent ignores the fact that the Q_t\u2019s are all dependent due to the shared layer h.  Also, I do not think that the proof of Theorem 2 and Theorem 6 can use the bound of Farahmand (2011).   Because the definition of approximation error epsilon_k in Farahmand (2011) is very different from epsilon_{avg,k} used in this paper, this warrants the analysis to start from the beginning, deriving the bounds from relating Q_t* - Q_t^{\\pi K} to (\\epsilon_{avg, k})_{k=0}^{K-1}.  The experiments compare known algorithms, and I am unsure how they support the theoretical bounds.   \n\nOther comments:\nInside the definition of (T*Q)(s,a), the probability measure P(s\u2019|s,a) would be P^{(t)}.  Thus, T* is not one optimality operator shared amongst all tasks,  but one for each task.  The notation should reflect this.  Stating the descriptions of k, n explicitly in section 2 would be helpful.  I had to read to section 3 to be sure that k stands for a sequence of number 0 to big K and i stands for 1 to n with n being the number of samples.   Also, giving an intuitive explanation of gaussian complexity (1) and its use would also be helpful.    "}