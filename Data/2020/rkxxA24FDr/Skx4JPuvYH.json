{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "= Summary\nA variation of Neural Turing Machines (and derived models) storing the configuration of the controller in a separate memory, which is then \"softly\" read during evaluation of the NTM. Experiments show moderate improvements on some simple multi-task problems.\n\n= Strong/Weak Points\n+ The idea of generalising NTMs to \"universal\" TMs is interesting in itself ...\n- ... however, the presented solution seems to be only half-way there, as the memory used for the \"program\" is still separate from the memory the NUTM operates on. Hence, modifying the program itself is not possible, which UTMs can do (even though it's never useful in practice...)\n- The core novelty relative to standard NTMs is that in principle, several separate programs can be stored, and that at each timestep, the \"correct\" one can be read. However this read mechanism is weak, and requires extra tuning with a specialized loss (Eq. (6))\n~ It remains unclear where this is leading - clearly NTMs and NUTMs (or their DNC siblings) are currently not useful for interesting tasks, and it remains unclear what is missing to get there. The current paper does not try show the way there.\n- The writing is oddly inconsistent, and important technical details (such as the memory read/write mechanism) are not documented. I would prefer the paper to be self-contained, to make it easier to understand the differences and commonalities between NTM memory reads and the proposed NSM mechanism.\n\n= Recommendation\nOverall, I don't see clear, actionable insights in this submission, and thus believe that it will not provide great value to the ICLR audience; hence I would recommend rejecting the paper to allow the authors to clarify their writing and provide more experimental evidence of the usefulness of their contribution.\n\n= Minor Comments\n+ Page 6: \"As NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences.\" - I don't understand the connecting between the first and second part of the sentence. This seems pure speculation, not a fact."}