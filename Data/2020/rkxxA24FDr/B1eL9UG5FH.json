{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors discuss an interesting idea for memory augmented neural networks: storing a subset of the weights of the network in a key-value memory. The actual weights used are chosen dynamically. This is similar to having programs/subprograms in a classical computer. To the best of our knowledge, this idea was not previously studied, and it was a missing part of the study of memory augmented neural networks until now.\n\nIn the abstract authors mention a von Neumann architecture. However, the von Neumann architecture assumes that the program and the data are stored in the same memory, which is not the case for what the authors propose (a significant limitation is that the vector size of the memory doesn\u2019t match the number of network weights). It is more reminiscent of the Harvard architecture. Please correct!\n\nAt the end of section 3.1 and in Eq 6 and 7 the authors claim that they use the regularization loss that makes the keys different to prevent programs from collapsing. However, this is only one way of collapsing. The other way is more reminiscent of what tends to happen in routing networks, where the controller network (here P_I) learns to attend to a single module (here memory slot) or attend to all of the modules with roughly equal weight. Are such collapses experienced during this work? If not, what could be the reason?\n\nThe main difference between routing networks and NSM is that in the former the selection is done in activation space, while in NSM it happens in weight space. Thus one should expect the same exploration/exploitation, transfer/inference tradeoffs, module collapse, etc to happen [1]. What makes it better/worse compared to these methods?\n\nThe method is mainly tested on methods used to test MANNs. However, because of its relation to the routing networks, it would also be interesting to compare them to [2] or [3] to see whether they suffer from the same problems.\n\nBecause the program distribution is rarely close to one-hot (all programs are contributing to some degree), could you please provide as a baseline a version where the program distribution is fixed and uniform?\n\nIn section 3.2, the last 2 sentences of the first paragraph, in one sentence the authors say \u201cwe store both into NSM to completely encode a MANN\u201d and in the next they say \u201cin this paper, we only use NSM to store W^C\u201d. Please make this consistent.\n\nIn the last sentence of the first paragraph of section 3.2, the authors say \u201cis equivalent to the Universal Turing Machine that can simulate any one-state Turing Machine\u201d. Could you further clarify this?\n\nIn the second paragraph of section 3.2, the authors say \u201cP_I simulates \\delta_u of the UTM\u201d. Shouldn\u2019t \\delta_u also include the state transition function, which is not included in P_I?\n\nSeveral sentences mention \u201cdirect attention,\u201d which is described as a neural network producing the weights directly. Why call this attention at all? Isn\u2019t this a form of fast weights? Probably they should be called fast weights or dynamic links. \n\nHow does this relate to the original work on this? Note that the first end-to-end-differentiable systems that learn by gradient descent to quickly manipulate the fast weights of another net (or themselves) were published between 1991 and 1993 - see the references [FAST0-3a] [FAST5] [FASTMETA1-3] in section 8 of http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html\n\nThe authors also claim to \u201cWhen the memory keys are slowly updated, the meta-network will shift its query key generation to match the new memory keys and possibly escape from the local-minima\u201d. Why? What drives the network to use new memory keys in this case?\n\nIn section 3.3, \u201cP_I is a meta learner\u201d. This is not the correct term here, because P_I is not leveraging old knowledge to learn faster (nor in improving the learning algorithm itself, nor in the recently more popular transfer learning sense), but it learns to select which weights to use.\n\nIn figure 2b, could you show the mean with std?\n\n\u201cstates of the generating automation\u201d should be automaton instead.\n\n\u201cAs NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences\u201d - Why this indication of a causal relationship here? Why would requiring fewer training samples make it generalize better?\n\nIn the sequencing tasks (section 4.3), how does the network know which task to perform? Is the task label one-hot encoded and concatenated with the input? From Fig 13 and 14, it seems to be a fixed concatenation of inputs, without any indication which task is which, and then a fixed concatenation of the outputs is requested from the network. Could you clarify, please?\n\nWe found Figure 5 very confusing. At first it seemed like the X-axis is \u201ctime\u201d (where we assumed the network is trained on a sequence of tasks C, RC, AR, PS), and the title above the subplots indicates for which task the performance is plotted over \u201ctime\u201d (which in this case would correspond to the time steps after completion of the training phase indicated on the X-axis). However, the second subplot shows perfect RC performance after having been trained only on C. We probably misunderstood the plots: the title plot is the \u201ctime\u201d, and the X-axis shows just the datasets (so the order is not important) - so the plots don\u2019t show the performance on the specific dataset over the course of training as assumed initially. But if so, why are they connected by lines and why not use a bar plot? It would be more interesting to see \u201ctime\u201d on the X-axis, so one can see how performance degrades while training on new tasks. \n\nIn Figure 5, what is the average performance supposed to show? If you average the performance over each training phase, the dataset trained first will yield better performance than the ones trained later. This is because the last trained one will have a performance near chance for most of the time it is measured, while the first one will have the best performance on the first measurement and will degrade somewhat because of catastrophic forgetting - but hopefully it will still be better than chance.\n\nIn Figure 5, numbers on the Y-axis: the first 10 and 100 place digits are misaligned.\n\nIn the last paragraph of section 4.4, \u201c- over 4 tasks\u201d, the - is confusing because it is not part of the formula. Please rewrite the formula in a different style or remove the -.\n\nIn section 4.5, the authors write that the best hyperparameters they found are p=2 for 5 classes and p=3 for 10. What could be the reason for p and the number of classes being so unrelated?\n\nIn section 4.6, \u201cDNC is more powerful and thus suitable for NSM integration\u201d - this suggests that the reason why it is suitable is that it is more powerful, but that is not the case since it was integrated into NTM, too.\n\nIn section 5, \u201cthey impede modularity\u201d. Why? Maybe they don\u2019t increase it, but why impede? \u201cwhich looses modularity\u201d - Why? The original RNN is not modular either. Why is this even less modular?\n\nIn figure 7 etc. it would be nice to have a title for the topmost subplot, too.\n\nWhat is a dynamic n-grams task? Could you clarify or include a reference?\n\nCould you clarify how the white-black-yellow-orange input charts should be understood (Fig 3d, Fig 15, etc)?\n\nHow to understand the preservation plots (Fig 3d)?\n\nAdditional references besides [FAST0-3a] [FAST5] [FASTMETA1-3] mentioned above:\n\n[1] Rosenbaum et al, Routing Networks and the Challenges of Modular and Compositional Computation\n[2] Chang et al, Automatically Composing Representation Transformations as a Means for Generalization\n[3] Kirsch et al, Modular Networks: Learning to Decompose Neural Computation\n\n\nWe think this paper is quite interesting, and might improve our rating provided the comments above were addressed in a satisfactory way in the rebuttal.\n"}