{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is focused on simplifying the use of larger datasets (via pretraining models) for the purpose of transfer learning onto smaller domains/datasets. An alternative view of this paper is that it is focused on a more privacy-friendly manner of doing data selection in a client-server manner.\n\nIn particular the paper proposes an interesting client-server architecture which allows for servers to potentially hold on to large datasets and have pretrained models. On the other hand, clients can leverage these models while sending minimal information to the server so as to get the server to return a subset of the data -- which the client can in turn use for pre-training / joint training. \n\nIn this case the proposed technique is composed of a few steps: On the client side, the data is partitioned into a few clusters from which pretrained models are trained. Next these models are shared with the client, and then used to \"adapt\" the model (e.g. one additional layer on top of the pretrained model) to figure out the best performing (pretrained) model (and thus effectively the best server data clusters). Lastly data from these good-performing clusters can be appropriately sampled and sent to the client.\n\nOn the plus side I liked this vision of a server-client manner of interacting and pulling datasets. The basic skeleton of the overall infrastructure also makes sense to me.\n\nThat said I had a few concerns which make me believe that the paper could do with more work and experiments before it can realize its potential impact. In no specific order:\n\n- In general I would have wanted a far more nuanced understanding of the efficacy of the proposed transfer learning / data selection methodology. There have been numerous works in this domain (not just restricted to vision) and it felt like there wasn't really any comparison with any of the more common approaches to the problem.\n\nFor example: One common family of approaches to performing such data selection is to run some clustering or PCA-like dimensionality reduction and then find clusters in the larger dataset closer to the clusters / basis vectors of the target set.\nAnother set of techniques directly work in a common embedding space to find similar data points\n\nWhy wasn't any such approach considered / discussed?\n\nThere are also many interesting \"active\" learning style approaches to the problem which allow for you to iteratively select data based on performance on some available target dataset. Those too would be valid in this setup and fair comparison points right?\n\n- The evaluations also felt discombobulated from the motivation/exposition of the approach. The paper motivated by saying there exist multiple large pretraining datasets than can be used, but in the evaluations only one single dataset was used for each task -- the two datasets weren't even combined! To me the ability to combine different datasets was something that held significant appeal about the problem and the paper and I really would have wanted to see that showcased / some positive evidence towards the same.\n\n- On a somewhat related note, the paper motivated by saying that some of these datasets are so large that clients cannot afford to download them or pretrain on them. If that is the case is 20-40% really going to be that different? Again given the motivation in the paper I would have liked to see some deeper analysis on this.\n\n- Significance testing is a key empirical practice and one I would request the authors to add.\n\n- It also felt that the paper's exposition and techniques were somewhat unclear -- they seemed to be focused on classification tasks (e.g. the superclass partitioning) but then were trying to generalize to non-classification problems without satisfactory explanations of how these approaches would generlize\n\n- On a more minor note, I felt the discussion in the paper is very specific to vision tasks since language understanding tasks have very different trends and techniques (e.g. BERT -- where more pretraining data only helps the model). I would actually try to clarify this scope accordingly earlier in the paper.\n\n"}