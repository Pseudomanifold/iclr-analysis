{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper focuses on transfer learning from large source server data to target client data. The server does not have access to the target data due to privacy and the proposed method finds a useful subset of source data to pretrain a network as initialization for subsequent target training. Since the source and target data may have different label spaces, self-supervision by classifying image rotations is used as a proxy for target set performance. The problem is interesting and important, but the current version is not satisfactory, especially the writing, which can be greatly improved.\n\nFor the partitioning schemes described in Sec. 3.2.1, it is not clear how the input images with different sizes can form the feature vectors of the same length. As specified in the text, the source dataset contains images from various domains and thus may have different image sizes and label space.\n\nAccording to Sec.3.2.1, the accuracies z_i is between 0 and 1 already, does this mean the normalization step in Sec.3.2.2 is not necessary? The temperature is set as T=0.1, why? How sensitive the system to this parameter?\n\nWhy Eq.(1) is equivalent to Eq.(8)? How does the assumption help? A rigorous derivation would be helpful.\n\nExperiment\n- Fig.3 is not very convincing. Only one target dataset does not mean a lot about their correlation. What is the correlation coefficient?\n- How many experts are used for each setting (the k in k-means)?\n- The work of Cui et al. (2018) is mentioned in the Related Work but not compared in the experiment. It would be more informative if the empirical comparison is provided.\n\nWriting and presentation\nGrammar\n- The very first sentence in the main text has grammar errors: In (the) recent years, we have seen (an) explosive growth in both the number and variety of A.I. applications.\n- Commas are excessively used to a point that parsing can be difficult at places. For example, in Section 3.1, \"Generally, we will assume that multiple tasks, each \u2026, are available, and denote this by Y\". The tasks are denoted by Y?\n- We follow (Gidaris et al., 2018) which *is* demonstrated.\n- We define its corresponding label \\hat{y} by XXX. Without looking at Gidaris et al. (2018), it is not clear what the label is.\nPresentation\n- The gating function in Eq.(2) is not accurate: there is a missing index i.\n- y_j in Eq.(4) is not defined. The \\mathcal{L} takes two arguments in Eq.(3) but only one argument in Eq.(4).\n- Eq.(5) is also confusing. What is the output of the expert network, i.e., e(r)? Is it a scalar indicating the rotation, or a four-dimensional vector showing the probabilities of being each angle? Assuming it is a scalar, does {e(r(x,j))}_{j=0}^3 mean the set of outputs of four images? Why taking argmin over images? Is [] regular bracket or indicator function? j is abused too much here.\n- Sec.3.3.2, z_i is a scalar so should not be bold (also w_i); p is a scalar in (6).\n"}