{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an architecture that captures equivariance to certain transformations that happen in text, like synonym words and some simple transformation over word order.  \n\n* General comments: \n  \nIncreasing compositional generalization using equivariance is a very interesting idea. Sections 1-3 are well written and the solution of modeling the translation function as a G-equivariant function is well motivated. \n\nSection 4 is far less clear. In its current form, it is very hard to understand the model construction as well as the design choices. This section should be significantly improved in order for me to increase my score. A direct by-product of the confusing writing is that the experiments cannot  be reproduced.\n\nThe experiments show improvement in one out of four tasks, where the single phrase \u201cAround right\u201d is held from the training set. There are no examples, not qualitative analysis, no ablation experiments. Overall, more evidence needed to convince that the approach is useful. In addition to deeper error analysis, the authors can hold out other phrases (e.g., \u201caround left\u201d, and many others). \n\n* Specific comments which I hope the authors address:\n\n1. To the best of my understanding, the authors do not explicitly specify the group G that they want to be invariant to. Is it a product of a few cyclic groups? (a cycle for each set of words that are interchangeable?)\n\n2. The authors suggest using G-convolution, i.e. the group convolution on G. This is in contrast to the (arguably) more popular choice of using linear layers that are G-equivariant (as in, for example,  deep sets (Zaheer et al. 2017), Deep Models of Interactions Across Sets (Hartford et al. 2018),Universal invariant and equivariant graph neural networks (Keriven and Peyr\u00e9 ) and in general convolutional layers for learning images).\nI have several questions regarding this choice:\n2a. Can the authors discuss the differences/advantages of this approach over the approach mentioned above? It seems like the approach mentioned above will be more efficient (as there is no need to sum over all group elements)\n2b. In order to use G-convolution, one has to use functions defined on G. Can the authors explain how they look on the input words as functions on G?\n2c. How is the G-Conv actually implemented? \n2d. Can the authors provide some intuition to what this operator does? \n\n3. Is the whole model G-equivariant? The authors might want to clearly state this. To the best of my understanding, this is the main motivation of this construction.\n\n4. It might be helpful for readers that are not familiar with deep learning for NLP tasks to provide a visualization of the full model (can be added to the appendix)\n\n5. Why are words represented as infinite one-hot sequences? Don\u2019t we assume a finite vocabulary? This is pretty confusing.\n\n6. As a part of the G-RNN the authors apply a G-conv to the state h_{t-1}. What is the dimension of this hidden state? How does G act on it? \n\n7. Please explicitly state the dimensions of each input/output/parameter in the network (this can be combined with the illustration above illustration)\n\n* Minor comments:\n\nSection 4.1 pointwise activation are in general equivariant only to permutation representations\nPage 2 - typo - \u2018all short\u2019-> \u2018fall short\u2019 \n"}