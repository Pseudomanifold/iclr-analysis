{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for the uncertainty estimates for Neural Network classifiers, specifically out-of-distribution detection. Previous methods use an ensemble of independently trained networks and average the softmax outputs. The authors investigate this method (ensembles of ReLU networks) and observe three fundamental limitations:\n\u201cUnreasonable\u201d extrapolation, \u201cunreasonable\u201d agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some out\u2013of\u2013distribution inputs, but do not contribute to the classification (CONSTANT FUNCTIONS ON THE TRAINING MANIFOLD).\n\nTo mitigate these problems the authors proposed the following:\n\n- Changing the activation function of the last hidden layer to the sin(x) function, and they claimed that this is going to guard against overgeneralization.\n- Use larger than usual initialization to increase the chances of obtaining more diverse networks for an ensemble.\n- They claimed that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-distribution detection on standard data sets.\n\nThe paper addresses an important problem, out of distribution detection, by proposing a Fourier network which is somewhere between a ReLU network (small initialization) and a nearest neighbor classifier (large initialization). The authors claimed that this leads to an out-of-distribution detection which is better than either of them.\n\nThe paper is well written and easy to follow. The authors did an interesting and precise investigation in how to force the confidence score to decay like a Gauss function by proposing to use the Fourier transform of such a Gauss function. By doing so they get the advantage of ReLU (ability to generalize) and prevent the network to become arbitrarily certain of its classification for all points. However, the authors claimed that when they switch the activation function to sin(x) the increase of |x| will usually stop at the first maximum or minimum of sin which is (around \\pi/2). However, the authors did not explain how they get this result (i.e the value \\pi/2). It would be interesting if the authors could show results for the case greater than or less than (\\pi/2) to show the difference.\nIn addition, Figure 3 shows that the ensemble of ReLU networks is overconfident in most of the area, whereas the ensemble of Fourier networks is only confident close to the input, and in the discussion of constant function of their training manifold the authors discuss some example.\n\nI would like to ask the authors: \n\nDid the Fourier networks learn the input distribution?\nHow are defined: usual initialization, small initialization and large initialization?\n\nThe experiment section is adequate. However, it would strengthen the paper if the authors compared against other approaches such as: \n- Predictive uncertainty estimation via prior networks, NeurIPS 2018.\n- Generative probabilistic novelty detection with adversarial autoencoders, NeurIPS 2018.\n"}