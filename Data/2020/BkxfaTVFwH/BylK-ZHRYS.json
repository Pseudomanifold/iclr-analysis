{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a probabilistic generative latent variable model representing a 2D image as a mixture of latent components. It formulates the scene generation problem as a spatial Gaussian mixture model where each Gaussian component comes from the decoding of an object-centric latent variable. The contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents. This allows the model to capture autoregressive dependencies among different components and thus help generate coherent scenes, which has not been shown in the previous works. In the experiments, the authors compare GENESIS with MONet and VAEs qualitatively and quantitatively and show that the model outperforms the baseline in terms of both scene decomposition and generation.\n\nThe proposed model seems like the right direction to improve upon MONet and it is nice to see the generation results. It is also nice to see the fully probabilistic modeling of the problem. Although it would improve over the MONet, I'm nevertheless not sure if the framework of sequential component generation (applying both MONet and GENESIS) can be a robust approach to more complex scenes, e.g., with a larger number of objects. Also, the mixture-based approach seems not guarantee the object-level decomposition. For example, if in the scene there are many objects of the same shape, size, and color, I think the proposed model may not properly distinguish them, as shown in some of the wall patterns in the experiments. But, I'm not sure what would happen if K is set to a large number to deal with this. Then, it would have the problem of long-term dependency in sequences.\n\nSome comments and questions:\n1. It would be good to show the qualitative result of the simplified Genesis-s and its qualitative result in the FID experiment.\n2. Is the VAE baselines (BD-VAE and DC-VAE) trained by maximizing the ELBO? If it is the case, will the result of the FID experiment section be different if we train the VAE baseline with GECO?\n3. Some detail of the implementation is missing, e.g. how do we model the posterior q_{\\phi}(z_{k}^{c} | x, z_{1: k}^{m}).\n"}