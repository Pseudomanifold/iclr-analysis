{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a new neural network architecture called simplicial complex networks. The authors draw inspiration from topological data analysis in order to motivate this new architecture. The SCN architecture is motivated through a parametric sub-divisioning of the input space into a complex of d-dimensional simplices. The authors give a universal approximation theorem for a model of this type, and give a simple algorithm to perform the sub-divisioning. The resulting network output is a convex combination of vertices after l nested sub-divisions. The authors present a number of toy experiments to validate their claims.\n\nAlthough I believe the authors present an intriguing idea, I ultimately tend for a rejection of this paper. I have three major reasons for this: \n(1) Quality of writing and clarity is lacking: Citations are all done in-line, symbols are being used without proper explanation, and many sentences are just purely broken and impossible to understand. \n(2) The authors do not properly compare their approach to the most common architectures in use: Fully-connected nets and CNNs. What are the shortcomings and benefits of SCNs as compared to these architectures? \n(3) Usefulness of the approach. I believe this ties in with point (2). The authors make no explicit mention of the computational complexity of their approach, or how the number of parameters depends on the number of input dimensions. Since the simplicial complex is defined on the input space the number of parameters scales with the number of input dimensions, and becomes restrictively high. This explains why the author use only low-dimensional toy examples."}