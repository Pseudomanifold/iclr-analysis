{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new oversampling method for class-imbalanced datasets, which is called Adversarial Minority Over-sampling (AMO). Its contribution is that it performs over-sampling using the samples of the majority classes, not the minority classes. It generates adversarial examples of the majority class and labels them as one of the minority classes. The purpose of using the majority class samples is that it can solve the problem of overfitting minority classes by using samples from minority classes too much, which is a typical problem of ordinary over-sampling techniques. For this purpose, the authors propose a new optimization objective that generates synthetic samples by transforming the majority class samples and devises a rejection criterion to determine whether the generated sample is appropriate, motivated by the concept of effective number of samples. They also proposed the distribution of the appropriate initial seed point of the generation through newly designed criteria.\n\nI do not think this paper is acceptable in the current state, and there exist some ambiguous issues where clarification and more supporting evidences are required. However, as a new novel approach to mitigate the over-fitting issue, if the following questions are answered reasonably, this paper may become acceptable. \n\n1.\tAs the authors mentioned consistently, the classifier g that generates adversarial examples is unreliable and potentially over-fitted to minority classes, which means that the classifier g cannot learn the general features for the minority classes. It is clear that the classifier g would make reasonable adversarial examples for the majority classes, but it is unclear that the generated examples by g are reasonable to be labeled as one of the minority classes, a target class. Although the classifier g would classify the generated example as the target class, it does not mean that it captured the general features of the target class and generated the adversarial sample using them. Instead, I think it is possible that the resulting synthetic example can be labeled as any minor class, not just as the target minor class. In the reported results, the proposed method would learn how to distinguish each major class from the last and fail to generalize in minority classes and make any example with the only constraint that it does not have the feature of starting (major) class, regardless what the target class is. Thus, I ask the authors to a) provide the evidence that the model g generates the samples with general features for each minor class and b) explain clearly how it is possible. c) I also suggest the authors to show the recall in addition to the accuracy in Tables 1 and 2. \n\n2.\tI am wondering why f does not have to classify synthetic examples as their target classes, as stated right below Equation (2) in page 3. It seems reasonable for f to classify the example exactly like g. Even in Figure 4, it seems that the classifier g attempted to generate \u2018Truck\u2019 class example, but it failed, as the model f classified it as \u2018Bird.\u2019 It would be great if the authors clarify how the classifier work in this manner and what is the expected outcome of f.\n\n3.\tAlso, I am wondering why the accuracy of major class examples decrease from 2% to 7% consistently if AMO is applied to ERM or LDAM in CIFAR-10 and CIFAR-100 dataset. I conjecture it is because the features of the starting example are not fully \u2018erased.\u2019 Also, I think that the proposed method affects the performance of the majority class in page 2. In this respect, I would like to see how many samples of major classes are correctly classified previously but misclassified after the model was trained with synthetic adversarial examples, to check the effect of adversarial examples on learning the major class. \n\n4.\tIt is a minor point, but it needs clarification on how the train/test set was constructed in both g and f, e.g., whether they have the exactly same training set.\n"}