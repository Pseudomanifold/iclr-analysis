{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "# Readability Review\n\nI am a complete outsider to the field of theoretical analysis of model parallelism, although I'm well familiar with deep\nlearning and backpropagation in general. Thus, I'm going to review this paper in terms of readability and writing flow\nfor an outsider rather than provide domain-specific feedback.\n\nThe intent of the paper is clear for an outsider. The proposed solution is not.\nAt a high level, my understanding is that the work proposes to parallelize backpropagation by updating multiple\nblocks/layers of a model in parallel, which requires sometimes using \"stale\" gradients/activations from previous\ntimesteps as the \"real\" ones have not been computed yet. This is communicated reasonably well in Section 1. However, the\nspecific parallelization insight is never explained at a high level and lost in notation (especially for a reader\nunfamiliar with the recomputation technique). I appreciated Figure 3 that attempted to illustrate the process on an\nexample with K=3, but it was not self-sufficient to communicate the intuition. Figure 3 makes use of the concepts\ndefined in Section 3.3, and thus illustrates Algorithm 2 rather than explains its high-level ideas. Without a high-level\nintuition, by the time the reader gets to Section 3.3, they are already lost.\n\nLayer-wise Staleness is introduced in Section 3.1 but is only used in Theorems 1-2. It doesn't seem to have an effect on\nthe actual algorithm, only on its analysis. As such, it's confusing when introduced. Instead, I'd recommend starting\nSection 3 with introducing timestamps and a notion of possible computation across timestamps (especially explaining how\nit differs from vanilla backpropagation). This understanding is crucial for Section 3.2, but the notation \u0394t\u2096 is not.\n\nFigure 1 needs a number of improvements to adequately present the state of the field for an outsider on a first reading:\n* K (number of blocks/layers) is not defined in Section 1, only later in Section 2, but Figure 1 uses it.\n* Column 1, \"Method\", needs citations to link it to the literature without having to read the main text.\n* Not all of the methods mentioned in the main text appear in the figure.\n* It is unclear whether all the computation boundaries must be aligned. For example, in vanilla BP the \"Forward\" phase\n  at K=0 must finish before the \"Forward\" phase K=1 starts. Is it also true when, say, in FR the \"Forward\" phase at K=1\n  ends right when the \"Backward\" phase at K=0 begins, or is it just an artifact of how the figure is drawn?\n  More generally, the figure shows the time when each phase happens but doesn't show the dependency graph between phases.\n\nWhy does Algorithm 1 appear? It's just vanilla BP.\n\nThe colors in Figures 4-5 are difficult to distinguish, especially different shades of blue or red.\n\nI struggle to see why DSP would improve the test accuracy of a network aside from just making the training faster. The\nnetworks in Table 3 go through the same number of updates, correct? DSP uses noisier gradients for its updates, which\nmight act as a regularizer, but this is just speculation. One could try to test it by adding random noise to the true\ngradients of BP (with magnitude similar to the expected magnitude of the BP-DSP divergence at a corresponding epoch). If\nit similarly improves the performance of that network, that might be evidence towards this hypothesis. In general,\ngradient noise does not necessarily improve the network performance unless tuned to the architecture/task.\n\nIn addition, the numbers in Table 3 must be listed with their corresponding standard deviation across multiple runs.\nOtherwise it's difficult to ascertain whether the DSP improvement is real or thanks to a lucky seed.\n\nSpelling:\n- Section 3.1: \"...the data is forwarded...\"\n\nIn summary, I have to give this paper a Weak Reject score for readability. I'm happy to increase it if my concerns are\naddressed in a new PDF version during the discussion period.\n"}