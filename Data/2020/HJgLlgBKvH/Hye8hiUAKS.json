{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes an approach for model parallelism without forward or backward locking (i.e., layers don\u2019t have to wait to perform computation). They maintain forward and backward queues per block of layers, where each block of layers is on a different device. Each block pops activations from the previous block\u2019s forward output queue, computes the output activations and pushes into its forward output queue. Then it pops the gradient queue from the next block, computes gradients of its parameters and the corresponding input, updates its parameters, and pushes the input gradient into the queue. This technique introduces gradient staleness. They evaluate ResNets on CIFAR-10, CIFAR-100, and ImageNet.\n\nComments:\n* There are a number of prior works that explore the same idea of pipelining forward and backward calculations that are not cited and compared [1, 2, 3]. The novelty of this work is low given these works.\n* This paper is poorly written. It is overly notation heavy and does not give a clear description of how the method works. The diagrams only add to the confusion instead of making it easier to understand. It takes a long time to understand what ends up being a very simple method.\n* The baselines in this paper are weak. The ResNet-164 gets 92.8%, but the original ResNet paper [4] has a ResNet-101 that gets 93.6% accuracy. This seems like a nitpicking detail, but the the problem with asynchronous training is not that it can diverge, but that the performance is often lower. Without having a strong baseline, it\u2019s hard to tell whether the stale gradients are affecting performance. The ImageNet baselines are similarly concerning.\n* No GPipe comparison in the experiments. Furthermore, the GPipe speedup comparison is unfair because these seem like two separate implementations of a training pipeline. It\u2019s hard to tell whether this approach will have speedups over GPipe if the same codebase is used.\n\nIn conclusion, the lack of comparison with prior work, poor writing, and weak experimental results leads to a suggestion of rejection.\n\n[1] \u201cPerformance Analysis of a Pipelined Backpropagation Parallel Algorithm\u201d. Alain Petrowski, Gerard Dreyfus, and Claude Girault\n[2] \u201cAMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks\u201d. Alexander L. Gaunt, Matthew A. Johnson, Maik Riechert, Daniel Tarlow, Ryota Tomioka, Dimitrios Vytiniotis, Sam Webster\n[3] \u201cPipeDream: Fast and Efficient Pipeline Parallel DNN Training\u201d. Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, Phil Gibbons\n[4] \u201cDeep Residual Learning for Image Recognition\u201d. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"}