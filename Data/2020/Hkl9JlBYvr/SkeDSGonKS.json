{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a new deep reinforcement learning method that can efficiently trade-off exploration and exploitation. An optimal policy for this trade-off can be solved under the Bayesian-adaptive MDP framework, but in practice, the computation is often intractable. To solve the challenge and approximate a Bayesian-optimal policy, the proposed method VariBAD combines meta-learning, variational inference, and bayesian RL. Specifically, the algorithm learns latent representations of task embeddings and performs tractable approximate inference by optimizing a tractable lower bound of the objective.\n\nThe paper is well-written and easy to follow. The combination of meta-learning, variational inference and BAMDP is a clear and neat way to approximate Bayes-optimal policy. The idea also sounds practical for RL as it can approximately solve larger tasks with unknown priors. Experiments on Gridworld and Mujoco show the effectiveness of the proposed method. On Gridworld the performance of the proposed algorithm is close to the performance of the Bayes-optimal policy.\n\nOne concern for this paper is the level of novelty, as each major component of the proposed solution has been explored quite extensively in the existing literature (as mentioned in the related work section). \n\nIn addition, since comparing many existing Bayesian RL methods, VariBAD meta-learns the inference procedure. This can add additional computation complexity to Bayesian RL, which is not explained or mentioned in neither the method part nor the experiment. I hope the authors can add some discussions on this aspects"}