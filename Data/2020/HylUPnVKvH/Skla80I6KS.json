{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents Mix&Match, a formalization of mixed image sizes training for conv nets. One core idea is to set a computational budget which can then be \"spent\" on either training with higher image sizes or training with larger batches / more duplicates. The paper also presents a gradient smoothing technique along with a batch norm calibration procedure to deal with issues related to mixed size training.\n\nGenerally speaking, I do like the ideas presented in this paper. The insights used to justify the Mix&Match approach are not groundbreaking, but worth reading. The experiments are also comprehensive -- assuming we are ok with considering only image classification. However, there seem to be a general lack of details/explanations or some unsupported claims in some crucial parts of the paper. I provide six examples here:\n\n1) I am not convinced by the third contribution (\"We show that reducing the average image size at training leads to a trade-off between the time required to train the model and its final accuracy\"). As pointed out by the authors themselves in Sec. 2.1, Tan and Le 2019 already shows this. Can the authors point out more clearly what is the basis of novelty for this contribution?\n\n2) The benefits of gradient smoothing are not well described. First, ablation study in Appendix B (Fig.5) only shows a slight advantage (to take the authors' own words) to gradient smoothing. Is the \"slight advantage\" significant? Moreover, this advantage only stands for test error, which would suggest that gradient smoothing is more a regularizer, and this is not discussed in the analysis. Perhaps the benefits of gradient smoothing would be more obvious if a larger set of sizes S would be chosen (on another problem than CIFAR-10, of course)? Finally, the paper states how \"gradient smoothing is designed to adapt globally\", which I actually consider as a weakness. In particular, it thus assumes that the variance ratios are the same for all layers. Is it the case? Any difference would even be amplified by the L2 norm applied on the gradients (instead of L1 for instance)...\n\n3) The batch norm calibration seems to be of little use in practice, where the size of images are more likely to be completely arbitrary instead of a few discrete values. As a result, one would either have to perform the calibration for every size or assume a \"close enough\" calibration using (for instance) the nearest power of 2 to be sufficient. The former would require a lot of computation time while the latter was not discussed in the paper. Can you expand on the benefits of batch-norm calibration for practical problems?\n\n4) CIFAR-10 results in Fig.2 and Table 2 look dissimilar. The ResNet-44 baseline accuracy (92.84%) does correspond to the ~7.1% error in Fig.2. However, the 94.46% accuracy for D+ regime should translate to a <5.5% error, which is not what we see on Fig.2 (about 5.9%). Similarly for B+ regime (94.3% accuracy in Table 2 vs. ~6.7% error in Fig.2).\nSure, the difference is small, but so are the differences between baseline and MixSize approaches. Is there something I overlooked here?\n\n5) ImageNet experiments (Table 2, Fig.3-4, and the related explanations) are unclear to me. The paper states:\n> While the original training regime consisted of images of size 224\u00d7224, our proposed regime makes for an average image size of S\u0304 \u00d7 S\u0304 = 144 \u00d7 144\nBy using a smaller image size, batch size or batch duplicates can be increased. This is then compared to a baseline network using 224x224 fixed size images. However, considering the paper advocates for mixed sizes training against fixed size, the baseline should, to be fair, be the most efficient fixed size. Looking at Fig.1, we see that training on fixed 160x160 size results not only in a slightly better evaluation accuracy at 224x224 (about 1%, if I had to guess solely with the figure) but also in a much lower computational burden.\nAs a result, the difference between \"Baseline\" and \"D+\" approach would decrease, but above all, the D+ computation budget would have to be computed using 160^2/S^2 instead of 224^2/S^2, which would lead to a decrease in performance. Similarly, B+ regime would not be 2.7x faster, but 2.7 / (224^2 / 160^2) = 1.38x faster. Admittedly, this is still faster, but by a much lower amount.\nLooking at Fig.4, the same issue arises. Fig.4b does present all Fixed S approaches, but not Fig.4a which is the crucial one since it directly compares to Fig.1. \"Mixed S=144\" would probably still fare better, but not by the same margin.\nOverall, just by looking at these results, it is unclear if the best approach should be mixed size training with D+ regime or simply fixed size training with S=160. Could these results and analysis be updated to provide a fair comparison with fixed size approaches?\n\n6) The \"profound impact on the practice of training convolutional networks\" is not obvious. First, in practice, pre-trained networks probably lead to a much faster training anyway. And if a pretrained network is not available (say, a custom architecture), then the practitioner has to cross-validate the stochastic regime over several alternatives, as the authors did on ImageNet (Sec. 5.1) to take advantage of MixSize. Computational time budget for training is also unlikely to be a huge issue in most cases -- sure, there may be specific applications where it is, but then we are not talking about general impact. What is the actual use case in practice for MixSize?\n\nIn summary, the paper limited contributions are nice, but not enough to compensate for all these issues. In themselves, each of them is not a deal breaker, but all put together, it makes it hard for me to advocate for acceptation. I would be more than pleased to change my rating, however, if the claims and experiments were revised and better explained."}