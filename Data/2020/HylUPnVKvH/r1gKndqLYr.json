{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduces new mix-size regime for training CNNs. Several image sizes are mixed, and the input size can be changed for each mini-batch. As a result, trained model generalizes better to different image sizes at test-time. In two suggested training regimes, the model either is training faster or achieves better accuracy at test-time on both fixed and varying size testing dataset. Modifying the image size leads to several difficulties in training the model. To overcome these difficulties the paper suggests a couple of methods: gradient smoothing to decrease the gradients\u2019 variance and batch-norm calibration to correctly collect mean and variance statistics in batch-norm layers.\n\nOverall, the paper addresses an important problem and provides a significant methodological contribution. The problem is clearly demonstrated by a figure showing poor generalization to different test time image sizes. The table with gradient correlation between different spatial size of images convincingly motivates the mix-size idea. Possible severities of training in mix-size regime are considered. The experiment section provides results on CIFAR10, CIFAR100 and ImageNet with different network architectures. Suggested methods outperform the baseline and provide a trade-off between the training time and final accuracy.\n\nThere are, however, several questions/drawbacks, which I list below.\n1.\tExperiments in Section 5 lack the comparison with not only baseline, but also with one of the previous attempts to adapt training with mixed image sizes. Is it possible to compare, for example, either with (Touvron et al., 2019), or with one of the works on progressive resizing? It seems that Appendix C is somewhat close to this comparison, can you add it to experiments in Section 5.1? It can be added even on the same Figure 2.\n2.\tThe Table 1 suggests a conclusion, that faster learning could be achieved using some regime with the mean input size increasing over iteration. However, in main experiments some balanced stochastic regime without mean input size increasing was chosen. The experiment in the Appendix C evidences that two regimes with random (but increasing) image sizes works better than non-random increasing. However, these two regimes are not compared with the models trained without size increasing. On the Figure 6(b) from Appendix, it seems that these two models are indeed slightly better: final error 5.3-5.4% vs 6%. However, it\u2019s not clear, whether the experiment in Appendix C and Figure 6(b) in particular is also CIFAR10, ResNet44. Aren\u2019t these two regimes a further improvement of the approach? Moreover, as they are inspired by an observation in Table 1, they should be in the main part of the paper. Also, is it D+ or B+ regime at Figure 6(b)?\n3.\tFor the full self-sufficiency of the paper, a brief description of batch augmentation technique, used in the experiments, is required. Although there is a reference, batch augmentation is not just related work but an important element of one of the two suggested training regimes (D+). As it\u2019s not very commonly used approach yet, I believe it requires some description.\n4.\tFor two significant changes in training procedure, namely gradient smoothing and batch-norm calibration, the paper provides an experimental assessment of gradient smoothing in the appendix. Is it possible to experimentally assess the influence of batch-norm calibration for varying image sizes? For example, to compare with fine-tuning from (Touvron et al., 2019).\n5.\tIn Experiment 5.1, there is no description of the baseline. The sentence is probably needed, that baseline is a model trained with fixed image size.\n"}