{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a mixsize regime which uses different sizes of input in training classification networks. The method also utilizes batch augmentation and varying batch sizes, plus gradient smoothing and BN calibration. The paper obtains better results than baselines and more robustness when testing under different resolutions.\n\nThe method is well-motivated in section 3, seems quite reasonable and the result improvement over baseline is impressive in Table 2. But I do have several important concerns.\n\n1. In computer vision, typically only in image classification the network takes in a fix-sized input. In object detection and semantic segmentation, all state-of-the-art models naturally accept images of different resolutions. In detection this is typically by the use of some form of pooling and in segmentation the output size naturally scales with input size. So I suspect the novelty of the paper is not enough: it basically brings a common practice (varying input size) used in detection/segmentation into classification.\n\nAlso, I wonder whether transfering the model trained with mixsize could improve the results when the classificatio model is used as a backbone in object detection/segmentation, given the later two already vary the input sizes. However, the evaluations are only done in a classification setting.\n\n2. The mixsize regime seems to complicate itself by incorporating batch augmentation. Why is this needed? If the main point of the paper is to argue training with multiple sizes of input helps, then it should isolate other factors like batch augmentation, and keep the batch size in each iteration fixed instead (in my understanding this means removing D and keeping B fixed). If B and D are to keep the computation budget fixed, I'd rather see this achieved by extending the training iterations, because B and D can influence the results by themselves. This was not clearly explained in the paper. \n\nIf B and D are crucial for the improved convergence speed/accuracy, then the main point of the paper should not be only using different sizes of input but use them together with the B and D, and should compare itself with the counterpart that use B and D but not with different input sizes. This is important in helping readers understand what is actually useful here.\n\nThe use of gradient smoothing and BN calibration is more understandable since they explicitly address the resolution inconsistency issue.\n\n3. The ImageNet performance in Table 2 improves quite a lot over the plain baseline, but comparisons with other more advanced methods (e.g., [1,2]) seem to be missing. \n\nOther questions:\n1. Is the correlation calculated for p(x^32, x^24) using the same model or different models (i.e., one model is trained on 32, the other is trained on 24)?\n2. Why is gradient smoothing only used in B+ but not D+?\n\nGiven the rather complicated regime coupled with B and D (2), what actually is useful is not clear, and the experiments may not be a faithful reflection of the abstract/introduction. Also considering the novelty issue (1), I'm leaning to reject the paper.\n\n[1] mixup: Beyond Empirical Risk Minimization.\n[2] Fixing the train-test resolution discrepancy. \"Using a lower train resolution offers better classification at test time\". \n"}