{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach to analyzing the properties of \"perceptual metrics\" used in deep learning image generation methods. \"Perceptual metrics\" are computed by measuring distances between images not in the pixel space, but i na feature space of a pre-trained cNN. The proposed analysis method, inspired by studies of human perception, is based on measuring the response of these features to sinusoidal gratings of varying frequency or orientation. Based on these responses, the paper proposes a \"Perceptual Efficacy Score\" that should measure the importance of certain feature in the feature maps for the performance of a perceptual metric. Experiments show that indeed distances measured between features with high score better correlate with human judgement of image similarity than distances between features with a lower score.\n\nI find the paper quite interesting, but lean towards rejection at this point. This i smainly because the experiments seem somewhat anecdotal and incomplete, see below for further details.\n\nPros:\n1) Application of methods from psychology/neuroscience to artificial neural networks is an interesting avenue of work. Moreover, better unsderstanding of \"perceptual metrics\" is of wide interest for various image processing applications.\n2) The proposed score seems to indeed correlate quite well with the importance of features for human judgement of image similarity.\n3) Presentation is mainly clear.\n\nCons:\n1) Experiments are not very exhaustive and at times a bit confusing. For instance:\n1a) Results are sometimes presented in a confusing way. In Figure 4 first of all it is not quite clear what points correspond to I guess each point is an image) and, second, it is not very obvious that the correlation is higher i none of the plots. In tables 1 and 2 it is confusing that different percentiles for H and L are used for different networks/layers. Is this based on some tuning? Then the tuning process should be clearly explained. Moreover, it might be useful to report the full curves of performance as a function of the percentage of features used.\n1b) There are no baselines and there is not much justification of computing the \"Perceptual Efficacy\" score the way it is computed. What if one uses only the orientation-based score? Or only the frequency-based? What if one selects the most relevant features in a data-driven way (based on correlation on a training set)? What if one selects subsets of features randomly? \n1c) While the method is inspired by methods used for studying natural vision systems, there is no connection to human experiments. It would be interesting to see a comparison of frequency and orientation tuning of features in a CNN to human cells (as I understand, the latter should be available in prior works?).\n1d) It would be great to see the selected features be used not only for offline image similarity assessment, but also for training image processing models - in the end, this has been the main use of \"perceptual metrics\". Do they lead to improved results?\n1e) Since the paper is about (subjective) image quality, it might be useful to show some qualitative results, potentially in the appendix if space is an issue.\n\n2) There are some issues with the presentation:\n2a) I had a hard time understanding what exactly \"Contrast Sensitivity Function\" and \"contrast masking\" are.\n2b) Minor issues:\n- In the abstract: \"trained object detection deep CNNs\" - I guess image classification is meant\n- Beginning of Section 2: \"Section. 2\", \"convolution layer as collection channels\"\n- Section 4: \"corresponds the the peak:\n- Section 5.2 \"Berkeley-Adobpe\""}