{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2317", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a new pruning method that dynamically updates the sparse mask and the network weight.\n- Different from the other works, the proposed method does not require post-tuning.\n- A theoretical explanation of the method is provided.\n\nMethods\n- In this method, the weight of the baseline network is updated not by the gradient from the original weight but pruned weight.\n- Here, pruning can be conducted by (arbitrary) a pruning technique given the network weight (Here, the author uses the magnitude-of-the-weight given method from Han.et.al).\n\n\nQuestions\n- See the Concerns\n\nStrongpoints\n- The author provides the simple and effective pruning method and verifies the performance with a sufficient amount of experiments.\n- The author argues that the method is applicable to various pruning techniques.\n\nConcerns\n- It seems that the paper omits the existing work (You.et.al - https://arxiv.org/pdf/1909.08174.pdf), which seems to share some contribution. The reviewer wants the author to clarify the differences and the strongpoints compared to the work.\n- The main pruning&update equation (DPF) does not seem to force the original network w to become sparse, such as by l1-regularization. So, the reviewer worried that the method might not produce sparsity if the initial weights are not that sparse.\nIf the reviewer missed the explanation about this, clarify this.\n- Regarding the above concern, what if we add regularization term in training the original network w? \n- As far as the reviewer knows, the proposed method improves the sparsity of the network, but most works choosing the strategy actually cannot meaningfuly enhance the operation time and just enhances the sparsity. Does the author think that the proposed method can enhance the latency? If so, a detailed explanation or experiment will be required.\n\nConclusion\n- The author proposes a simple but effective dynamic pruning method.\n- The reviewer has some concerns regarding the novelty, real speed up, and guarantee of the sparsity. \nHowever, the reviewer thinks that this work has meaningful observations for this field with a sufficient amount of verification, assuming that the author's answers for the concerns do not have much problem.\n\nInquiries\n- See the Concerns parts."}