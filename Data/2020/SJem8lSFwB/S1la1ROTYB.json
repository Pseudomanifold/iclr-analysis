{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors proposed a novel model compression method that uses error feedbacks to dynamically allocates sparsity patterns during training. The authors provided a systematic overview of a good number of existing model compression algorithms depending on the relative order of pruning and training processes. The effectiveness of the proposed algorithm is illustrated by comparing its generalization performance with 6 existing algorithms (and their variants) with two standard datasets and various networks of standard structures. The authors also showed the convergence rate and the fundamental limit of the proposed algorithm with two theorems. \n\nThis paper is well-written and very pleasant to read. I would like to accept this paper. But since I have never actually done research in model compression, I would say this is my 'educated guess'. \n\nSome quick comments:\n1. I did not go through the proofs of the two theorems. But it seems that there is a typo in the definition of strong convexity on Page 4: '\\Delta f(w)' should be '\\Delta f(v)'. I assume that this is just a typo. \n2. Sorry again for not knowing the details of the baseline algorithms. According to Table 1 and Table 2, the proposed method (DPF) outperforms all the baseline algorithms, without a single exception, which looks suspicious for me. After reading the paper, I still don't understand why this should be the case. Is this due to some implementation details? Can you think of some scenarios that the proposed algorithm may not be the one to go with? In other words, when the experiment seems to show that one algorithm absolutely outperforms all the other existing algorithms, there should be some take-home message on why, or some known limitations of the proposed method. \n"}