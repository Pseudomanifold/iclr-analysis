{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies using dynamic computation to alter the number of Transformer decoding layers each token uses to translate a given sentence. The paper considers using two variants of losses:  aligned training - same layer wise prediction loss for all tokens, and mixed training: loss on the output of a random layer for each token. For sampling the different layers the paper considers different distributions based on likelihood and the prediction rate.\n\nThe paper experiments these different training strategies on IWLST and WMT datasets. Training with token level sampling based on likelihood results in models that have smaller average exit (number of layers used in prediction) while preserving the BLEU scores of the standard Transformer training. Overall I believe the problem considered is interesting and the paper did a good job in setting up the problem and explaining the experimental setup results.  \n\nThe paper mainly needs to improve in discussing existing work. Universal transformers also study dynamic computation based on input tokens. While the training setup is different here, without the large shared Transformer layers, and this paper mainly focuses on the dynamic halting strategies, it is important to discuss these differences in detail in the paper.\n\nIntroducing a classifier in each layer (W_n) increases the number of parameters and compute by N. How do you handle this? \n\nThe exit loss (eqn 3) is a cross entropy loss, which will have trouble when q^* and q have different supports. Doesn\u2019t a dirac delta q^* cause problems here?\n\n\nMinor:\n\nIntro: first line of second paragraph needs to be rewritten.\n\nThere is a conflict in writing in both abstract and introduction as some sentences say that current models use the same computation irrespective of hardness of the input, followed by discussion of Universal Transformers, which do adaptive computation based on input hardness. The writing flow needs to be fixed in both abstract and intro.\n"}