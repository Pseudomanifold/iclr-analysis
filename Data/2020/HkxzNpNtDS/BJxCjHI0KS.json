{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThere have been two recent related tasks proposed in vision-langauge settings: vision-langauge navigation (VLN) where natural language turn-by-turn instructions must be decoded by an agent in an indoor environment to reach the goal location and Navigation from Dialog History (NDH) where dialog between two humans trying to reach a goal is input to an agent to try to reach a goal location. This paper uses these two tasks' data in a multi-task manner to try to generalize better between indoor environments especially unseen environments which are not in the training set of the agent. \n\nAnother proposed innovation is to use an auxiliary task of environment classification but via a gradient reversal layer such that the learnt latent representation input to the classifier should not overfit to foibles of the environment but should (hopefully) learn a representation that captures the intrinsics of the environment necessary for generalization. \n\nComments:\n\n- The paper is well-written and easy to understand! Experiments are thorough and has all the ablations one would ask for. Thanks!\n\n- Overall I like the paper but have a number of comments:\n\n1. Why not try more sophisticated methods of multi-task learning like 'MetaLearning' by Finn et al 2017 (MAML). It is common knowledge that straight up averaging across tasks is not as effective as doing the bilevel optimization in MAML. \n\n2. Why do RL at all? Already the authors are doing BC (naive form of imitation learning) but they could just do robust imitation learning like DAgger, AggreVateD, etc. The setting is already such that one has a natural oracle (which the authors are already using via BC in the objective) which is the shortest path planner during training time. Then combined with MAML one can do Meta-IL as in 'One-Shot Imitation Learning via Meta-Learning' Finn et al 2017. Note that imitation learning is exponenially more sample efficient than RL and removes all the reward-shaping complications. \n\n3. In Section 2 for error correction \"Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention\" by Nguyen et al. CVPR 2019 is directly relevant. \n\n4. Also for generalizaton performance these papers are directly relevant:\n\n\"Building Generalizable Agents with a Realistic and Rich 3D Environment\" Wu et al ICLR 2018\n\n\"Learning and Planning with a Semantic Model\" Wu et al \n\n\n\n\n"}