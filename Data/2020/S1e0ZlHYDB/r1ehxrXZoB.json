{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper demonstrates an interesting application of progressive compression to reduce the disk I/O overhead of training deep neural networks. The format encodes the trade-off between data fidelity and I/O bandwidth demand naturally, which could be useful when I/O is the bottleneck.\n\nMy major concern is that the paper should be clearer about the setting.\n* Does your work target the case where data cannot fit in RAM and should be fetched from local disk or through network? However, the datasets used in the evaluation look small and could fit in RAM.\n* How are mini-batches created? You mentioned in the related work that previous work (Kurth et al., 2018) lets each worker sample from a local subset instead of performing a true sampling of the whole dataset. Does your work perform true sampling? How much benefit does it give?\n* Is disk I/O really a bottleneck in training? There are many evidence [1][2][3] of almost linear scalability in training ResNet on *full* imagenet across hundreds or even thousands of GPUs. These work focus heavily on network communication rather than disk I/O. Does your setting differ from theirs? How does your approach compare with their techniques for optimizing disk I/O?\n\nThat being said, I think this approach should be appealing when the I/O bandwidth is limited and dynamic. Examples include training on edge devices, or federated training where data needs be fetched via ad-hoc network.\n\nOther detailed comments:\n\n* Figure 1 is not very informative and quite puzzling. There is no definition of quality at that point.\n* Sec 2 paragraph 3. What is the issue of data augmentation with the standard JPEG compression? Does your compression ease data augmentation?\n* Sec 3.1 paragraph 1. \"This is turn enables ...\" -> \"This in turn enables ...\"\n* How to decide the number of scans? Does it have impact on the I/O efficiency?\n* Evaluation\n  - I'm not familiar with Ceph. Why choose this particular environment? Does it bring in extra overhead (e.g., communicating with metadata server). What does the network topology look like? Is the data loading stall (figure 7) due to network congestion?\n - It worth evaluating more tasks such as detection and segmentation to measure the impact of compression.\n\n\n[1] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour, Goyal et al.\n[2] Massively Distributed SGD: ImageNet/ResNet-50 Training in a Flash, Mikami et al.\n[3] Image Classification at Supercomputer Scale, Ying et al.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}