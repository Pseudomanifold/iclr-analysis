{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "### Summary\n\nThis paper proposes to optimize the state marginal distribution to match a target distribution for the purposes of exploration. This target distribution could be uniform or could encode prior knowledge about downstream tasks. This matching can be done by iteratively fitting a density model on the historical data from the replay buffer, and training a policy to maximize the log density ratio between the target distribution and the learned density model. Experiments are performed on two domains: a simulated manipulation task and a real robotic control task. Overall, the paper is well-written.\n\n\n### Review\n\nRecommendation: weak reject for the reasons below. The main reason is that this paper ignores very similar prior work which is not properly credited. \n \n\nThe algorithm proposed here is very similar to the algorithm proposed in [1]:\n- the objective proposed in  equations (1) and (2) is the same as the second objective in Section 3.1 of [1]. \n- Algorithm 1 here is almost identical to Algorithm 1 in [1]\n\nThe work of [1] is only briefly mentioned in the related work section, and from the description there seems to be a fundamental misunderstanding of it. \nIt says \"their proposed algorithm requires an oracle planner and an oracle density model, assumptions that our method will not require\".\n\nMaking oracle assumptions is a tool for proving theoretical results, not a feature of an algorithm. An oracle can be any subroutine that one has reason to believe works reasonably well, and how well it works or not is captured in its accuracy parameter (usually \\epsilon).\nThey are used to break down a  more complex algorithm into simpler subroutines (called oracles), and deriving a guarantee on the complex algorithm in terms of the quality of the oracles. \nFor example, [1] assumes a density estimation oracle, which could be instantiated as a kernel density estimator, a VAE, count-based density estimation in the tabular case, etc. \nIt also assumes a planning oracle, which could be instantiated using any method for learning a policy (PPO, SAC, policy iteration, etc), or some search method if the environment is deterministic. \nThe accuracy of the oracles are reflected in the \\epsilon_0 and \\epsilon_1 parameters, which then show up the guarantee for theorem 1. \n\nTheorem 1 of [1] also shows that the entropy of the policy mixture (i.e. replay buffer) matches the maximum entropy over the policies in the policy class, which is one of the main theoretical claims of the work here. \n\nGiven this, I don't see this paper as making any new algorithmic or theoretical contributions. On the other hand, [1] had a very limited empirical evaluation and it would be valuable to have a more thorough empirical investigation of this type of method in the literature. This paper partially does that in the sense that they apply more modern methods (VAEs rather than counts/kernel density estimators) on more complex tasks (a simulated manipulation task and a real robot), and their experiments seem well-executed with proper comparisons. However, since the primary contribution of this paper seems to be empirical, I don't think the current experiments on two domains are enough. \n\nI think this paper could be fine for publication with a fairly significant rewrite placing it in the context of prior work, and expanding the experimental section. \nMy suggestions are to add experiments on several other continuous control tasks (Mujoco/Roboschool) as well as hard exploration Atari games (Montezuma, Freeway, Pitfall etc), to see how well the density estimation works in pixel domains (and the effect of historical averaging). I would be willing to raise my score if these changes can be made within the rebuttal period.   \n\n\n[1] \"Provably Efficient Maximum Entropy Exploration\", Hazan et. al. ICML 2019. \nhttp://proceedings.mlr.press/v97/hazan19a/hazan19a.pdf\n"}