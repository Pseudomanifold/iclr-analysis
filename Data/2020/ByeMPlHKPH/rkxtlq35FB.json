{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes Mobile Transformer, an efficient machine translation model, which achieves state-of-the-art results on IWSLT and WMT. The Mobile Transformer is base on long-short range attention (LSRA) modules that combine a depthwise convolution branch to encode the local information and a self-attention branch to capture the long-range information.\n\nThe main contribution of this paper includes\n1. bottlenecks are not beneficial to 1D attention models\n2. having both convolution and attention modules in parallel performs better and more efficient than having one of them alone. While LSRA is included in the search space of Evolved Transformer, surprisingly, their searching algorithm doesn't discover it. Evolved Transformer has either two convolution branches or two attention branches in parallel.\n\nThe paper is well written and easy to follow. The experiments are quite solid; however, it would be if the authors can report how Mobile Transformer performs on other language pairs or other NLP tasks. \n\nQuestions:\n1. Do the attention maps in Figure 3 come from the average of multiple heads or just one of them? \n2. The constraint for the mobile setting is set to 10M parameters. Can you justify why you choose it? In my opinion, memory footprint or inference time on mobile devices can be more realistic. \n3. Regarding the design cost shown in Figure (b). Does the number for Mobile Transformer include the cost of all the experiments you ran to search for your Mobile Transformer? \n4. I wonder if LSRA can also be applied to other tasks such as language modeling or reading comprehension.\n5. In terms of inference latency, how much faster Mobile Transformer is compared to Transformer and LightConv?\n6. Have you considered having the trade-off between having more parameters in the encoder or decoder?\n7. Have you done any analysis on why all tokens in Figure (c) assign high weights to the <EOS> token?"}