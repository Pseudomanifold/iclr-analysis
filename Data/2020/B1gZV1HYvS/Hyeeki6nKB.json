{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, a multi-agent imitation learning algorithm with opponent modeling is proposed, where each agent considers other agents\u2019 expected actions in advance and uses them to generate their own actions. Assuming each agent can observe other agents\u2019 actions, which is a reasonable assumption in MARL problems, a decentralized algorithm called CoDAIL is proposed. For each iteration of CoDAIL, (1) each agent trains opponent models (other agents\u2019 policies) by minimizing either MSE loss (continuous actions) or CE loss (discrete actions), (2) samples actions from those opponent models, (3) updates individual rewards (discriminators) and critics and (4) updates policies with multi-agent extention of ACKTR (which is used in MA-GAIL and MA-AIRL as well).\n\nThe experiments in the submission show that there is a significant gain relative to baselines (MA-GAIL and MA-AIRL) in OpenAI Multiagent Particle Environments (MPE) in terms of (true) reward differences and KL divergence between agents\u2019 and experts\u2019 state distributions.\n\nI think the empirical contribution of this work is clear to be accepted, but I give Weak Accept due to the following comments:\n\n- I think there\u2019s a similarity between Theorem 6 in MA-GAIL paper and Proposition 1 in the submission. I hope the difference between Proposition 1 and Theorem 6 to be clarified. \n\n- Proposition 2 seems to me redundant because it\u2019s neither important for theoretical analysis in 3.3 nor for the experiments. I believe a few sentences are enough to describe why authors choose \\alpha=1 (or equivalent explanations).\n\n- The authors suppose fully observable Markov Games in the paper, but it makes me confused when I consider the experiments in the submission. For example in Cooperative Navigation, each agent\u2019s observation includes (1) position vector relative to agents and landmarks and (2) their own velocities (which cannot be observed by other agents directly). Since authors argue CoDAIL is a decentralized algorithm, I think agents are not allowed to use others\u2019 observation for opponent modeling, but it seems that agents fully utilize others\u2019 observations. I hope it to be clarified and if that\u2019s the case, I wonder if we can regard CoDAIL as a decentralized method. \n\nI\u2019m willing to increase my score if my questions are clearly answered. "}