{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed four notions of interpretability in a fully-connected feedforward neural network: (1) sparsity of the weight matrix. (2) stability of the representation (zeroth-order stability is essentially Lipschitz continuity). (3) Faithfulness, which measures the correlation between the performance gain and the weight matrix norms. (4) Disentanglement, which measures the cosine similarity between different rows of the weight matrix. The author also claimed to have extended the deep learning Alternating Direction Method of Multiplies (dlADMM) to solve interpretability-constrained optimization problems.\n\nThe paper is well-written and well-organized, but the contribution is insignificant. Here are my reasonings:\n\n(1) The theorem proving the zeroth-order/first-order stability of fully-connected network is not interesting. For example, we know ReLU networks should be piece-wise linear functions, thus they are indeed Lipschitz continuous. However, the Lipschitz constant can be sometimes so large that adversarial examples can happen.\n(2) The authors claimed that dlADMM can be used to solve the interpretability-constrained problems. However, the splitting strategy is used to optimize with respect to feature maps (a_l) as well as the network parameters. In a CNN, feature maps typically requires much more memory consumption than the network parameters. Therefore I doubt the feasibility of the proposed dlADMM.\n(3) Even though the authors claimed that dlADMM can be used to solve interpretability-constrained problems, I do not see any experiments that incorporate the interpretability-constraint into the framework (maybe except for the sparsity constraint). How do you (or have you) encode the stability, faithfulness, and disentanglement into the constraint when solving the problems?\n\nSome other issues:\n(1) Mean value theorem does not hold for vector-valued functions, therefore the proof in the appendix is not correct. Although it is not hard to use an integral bound to prove the same result.\n(2) The font size of tables and figures is too small.\n(3) The format of the equation in section 4 needs to be changed.\n(4) In definition 2(a), locally first-order stable --> locally zeroth-order stable.\n(5) The definition of ||_{p,q} in Definition 1 needs more explanation.\n"}