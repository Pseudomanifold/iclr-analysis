{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a framework to evaluate the interpretability of fully connected feed-forward neural networks. The framework includes four interpretability properties and the paper gives quantitative measures for them. The paper provides some theoretical analysis on the stability, proving DNNs are globally stable for smooth activation functions and locally stable for nonsmooth activation functions. \n\nI did not check the proof, but this seems to be an incremental extension over existing work (e.g., that of (Alvarez-Melis & Jaakkola, 2018a; Alvarez-Melis & Jaakkola, 2018b; Zhang. et al 2018)). In addition, experiments are provided on several benchmark datasets to help understand the interpretability of different layers of networks. \n\nI found the paper is somehow hard to read. Grammatical and formatting errors are all over the paper. Even the citation to the some of the most related work has mistakes (e.g., inconsistent author names (Melis vs. Alvarez-Melis)?). I would suggestion a significant rewriting. \n"}