{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an exploration method based on the TD-error as an adversarial reward signal. The authors show that this reward signal has interesting exploration properties. They compare it empirically to RND and epsilon-greedy, showing better performance. Besides, they perform additional ablation studies to better investigate the properties of their approach.\n\nThough the paper proposes an interesting concept, it suffers from many weaknesses, some easy to fix and some which will require more work.\n\nHere are some remarks, in random order:\n\n- at the end of the introduction, the authors mention inspiration from computational neuroscience models, but they do not come back to this aspect in their work. To me, these remarks should be removed from the paper and kept for another paper about the biological significance of the model. In the conclusion, the authors come back to the \"biological concepts of curiosity boredom, and exploration\", I would rather say they are psychological concept, and the authors should have a look at developmental psychology and developmental robotics if they really want to contribute in this respect (but not for this paper).\n\n- some related work references are dispersed in the introduction, in Section 2, in the beginning of Section 3.2, in the end of Section 3.3 and in a few other places. The authors should build a proper \"related work\" section. Globally, the paper is poorly organized, e.g. Section 3.2 refers to Section 3.4 etc.\n\n- given that Q_x's reward function is the unsigned TD-error, I would like to see whether QXplore can deal with problems taking both some positive and negative rewards.\n\n- the authors mention RND and DORA as baselines, but only compare to RND. What about DORA? Despite the excitement it generated when published, I suspect RND is a rather weak baseline. There are many other exploration frameworks, the paper would be much stronger if the comparison was with respect to many other methods, such as GEP-PG (Colas et al. , ICML 18), Novelty search approaches, etc. To me, the weak comparison is the biggest weakness of this work.\n\n- the authors compared themselves to approaches based on TRPO while they were using TD3 (this information is hidden in Appendix A and should be moved in the main paper. But then, is the difference in performance due to using TRPO which is known to be less sample efficient? Again, this makes the comparison very weak, the authors should rely on the same algorithm from both sides.\n\n- the caption of Table 1 should be explicit about which algorithm comes from which paper.\n\n- I found Section 4.5 very weak, it looks like mere \"handwaving\" and would deserve a proper quantitative study if the authors want to keep it. In my opinion, the authors should remove it for now and move Appendices E and F to the main paper instead. As is, Appendix E is very poor (by the way, the caption and the figure legend do not match, so we don't know which is which) and I had to look for the number of seeds until I found it hidden in Appendix F.\n\n- Appendix C shows that, though the authors try to minimize the importance of this fact, their algorithm is very sensitive to initialization. It is very honest of them to have kept this study in their paper, but I'm afraid it strongly speaks against the algorithm.\n\n- As described, SparseHaflCheetah is not that sparse (-1 everywhere and 0 when you succeed, as this reward scheme already favors exploration). It would be more informative to use 0 everywhere and 1 when successful.\n\ntypos:\n\nEq (1) and (3) should finish with a dot as it is the end of a sentence.\np3: MDP's => MDPs\nSection 3.3, second sentence: avoid starting a sentence with a symbol.\nSection 4, the second sentence is not a sentence (no main verb)\np14: is also no subject to it => not "}