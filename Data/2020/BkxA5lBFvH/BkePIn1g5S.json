{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposes to adapt RL agents from some set of training environments (which, in the current instantiation, vary in some simple respect) to a new domain. They build on a framework for model-based RL called PETS. \n\nThe approach goes as follows: \n\n2-step process\n * train probabilistic model-based RL agents in a \u201cpopulation of source domains\u201d\n * dropped into new environment use \u201cpessimistic exploration policy\u201d\n\nThen at test time, in order to compute estimates for the rewards for each action the authors use a \u201cparticle propagation\u201d technique for unrolling through their dynamics model .\n\nThe action is chosen by looking at the sum of the 0 through kth percentile rewards. \nThis is a weird choice. Why are they looking at a sum over quantiles vs a quantile itself?\n\nThe claim is that the models from the first stage capture the epistemic uncertainty due to not knowing z.\nHowever, the authors give a too scant a treatment of what these uncertainty estimates really mean.\nFor example, they appear to only be valid with respect to an assumed distribution over z.\nThe paper\u2019s experiments however focus in large part on what happens when the model is evaluated \non values of z that were outside the support of the distribution over training domains. \nIn this case, any benefit appears to be ill explained by the underlying motivation.\n\n\nThe next step here is to finetune the model as data is collected on the new domain.\n\nAuthors propose heuristics for this finetuning that include\n1. Drawing experiences from the past experiences (under different domains) and \n2. \u201ckeeping the model close to the original model\u201d, via some sort of regularization presumably.\n\n>>> \twhy isn\u2019t the exact nature of how they \u201ckeep the model near the original model explained in the text?\n\tperhaps the authors mean that 1. and 2. are one and the same (1 as  means to achieve 2)\n\tif this is the case, then the exposition should be improved to make this more clear.\n\n\nSome important details appear to be missing. For example, how many distinct source domains are seen during pretraining? Do they set z different z for every single episode of pretraining? Some language here is unclear, for example what precisely does an \u201citeration\u201d mean in the context of the experiments? \n\nThe choice to report \u201caverage maximum reward\u201d seems strange if what the authors care about is avoiding risk. Can they explain/justify this choice or if not, present a much more comprehensive set of experimental results?\n\nThe figures tracking catastrophic failures vs performance resembles those in \n\u201cCombating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear\u201d  https://arxiv.org/abs/1611.01211\nThis raises some question about why they don\u2019t if concerned with \u201ccatastrophic events\u201d model them more explicitly. \nElse, if the return accurately captures all desiderata, why to we need to count the failures?\n\nIn short this is a simpzle empirical paper that makes use of heuristic uncertainty estimates, \nincluding in settings when the estimates have no validity. The writing is reasonably clear\nand the ideas are straightforward (which is perfectly fine!). A few of the decisions are unnatural,\na few are ad hoc, and a few details are missing. Overall my sense is that this paper \nhas some good qualitities, including the clarity of much of the exposition, \nbut it\u2019s still below the mark to be an impactful ICLR paper. "}