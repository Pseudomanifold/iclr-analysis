{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tries to address the safe adaptation: given a model trained on a variety of past experiences for some task, train a model learning to perform that task in a new situation while avoiding catastrophic failure. \n\n\nPros:\n- The idea of training on data from varying quartiles, with the goal of preventing overly-conservative models, is quite intriguing and inspiring.\n\nCons & Question:\n- Motivation:  \nCautious exploration or optimizing the best worst-case performance is conflicting with the philosophy of exploration, such as UCB. As stated in the introduction, \u201cenables fast yet safe adaptation within only a handful of episodes.\u201d Intuitively, we can not expect to be safe and fast at the same time. It would be better to discuss why cautious exploration can ensure fast and safe adaption, which would be more interesting. Additionally, in Figure 3, some fast adaption methods, such as MAML, should be compared to be more persuasive.\n\n- Method:\n 1. In equation (1), sum_N \u2014> sum_i.\n 2. This work formulated safe adaption as minimizing the risk of catastrophic failure. What\u2019s the relationship between \u201cthe generalized action score\u201d and \u201crisk of catastrophic failure\u201d? The \u201cgeneralized action score\u201d is the main difference with PETs. However, it is a little bit hard to follow the idea from \u201crisk of catastrophic failure\u201d to \u201cthe generalized action scores\u201d. \n 3. \u201cModel-based RL agents contain dynamics models that can be trained in the absence of any rewards or supervision.\u201d \n \u201dSince dynamics models do not need any manually specified reward function during training, the ensemble model can continue to be trained in the same way as during the pretraining phase.\u201d I am confused about these sentences. Without the reward, what\u2019s the purpose of RL? \n\n\n- Experiments:\n1. As stated in experiments, three meta-learning approaches have been deployed as baselines, including GrBal, RL^2 and MOLe. However, the experimental results are missing. Why meta-learning baselines do not work? Are there any explanations?\n2. There are many robust RL baselines, such as \n [1] Pinto L, Davidson J, Sukthankar R, et al. Robust adversarial reinforcement learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017: 2817-2826. \nIt would be better to compare with robust reinforcement learning work since there are no other baselines apart from meta-learning methods.\n"}