{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "\nThis paper proposes word2ket - a space-efficient form of storing word embeddings through tensor products. The idea is to factorize each d-dimensional vector into a tensor product of much smaller vectors (either with or without linear operators). While this results in a time cost for each word lookup, the space savings are enormous and can potentially impact several applications where the vocabulary size is too large to fit into processor memory (CPU or GPU). The experimental evaluation is done on several tasks like summarization, machine translation and question answering and convincingly demonstrates that one can achieve close to original model performance with very few parameters! \n\nThis approach would be very useful due to growing model sizes in many areas of NLP (e.g. large pre-trained models) and more broadly, deep learning.\n\nPros:\n1. Novel idea, clear explanation of the method and the tensor factorization scheme. \n2. Convincing experiments on a variety of NLP tasks that utilize word embeddings. \n\nCons:\n1. (Minor) While this is not the focus of the paper, it would be useful to have at least one experiment with a state-of-the-art model on any of these tasks to further strengthen the results (most of the baseline models used currently seem to be below SOTA).\n\n\nMinor comments:\nAbstract: stain -> strain\nPage 2: $||u|| \\rightarrow ||w||$"}