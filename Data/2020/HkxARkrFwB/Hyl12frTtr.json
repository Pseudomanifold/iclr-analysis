{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents two methods to learn word embedding matrices that can be stored in much less space compared to traditional d x p embedding matrices, where d is the vocabulary size and p is the embedding size. Two methods are proposed: the first method estimates a p-dimensional embedding for a word as a sum of r tensor products of order n (tensor product of n q-dimensional embeddings).  This representation takes rnq parameters which can be much less than p, since p = q^n. The second method factorizes a full d x p embedding matrix jointly as a tensor product of much smaller t x q matrices and can obtain even larger space savings. Algorithms for efficiently computing full p-dimensional representations are also included. When only dot products are needed, the p-dimensional representations do not need to be explicitly constructed.\n\nIn my opinion the terminology from quantum computing and entanglement is an unnecessary complication. It would be better to simply talk about the special parametric form of the embeddings , which allows efficient storage. Tensor product representations have been used for embeddings before (but not with the goal of efficiency) (e.g. Arora et al 2018) https://openreview.net/pdf?id=B1e5ef-C-\nThe paper covers related work briefly and does not compare experimentally to any other work aiming to reduce memory usage for embedding models (e.g. using up-projection from lower-dimensional embeddings, or e.g. this paper: Learning Compact Neural Word Embeddings by Parameter Space Sharing by Suzuki and Nagata.\n\nThe experimental results on summarization, machine translation, and QA show that the methods can obtain comparable results to models using traditional word embeddings while obtaining savings of up to one-thousand fold decrease in space needed for the embeddings.\n\nThe experimental results seem to conflate the issues of the dimensionality of the word embeddings versus that of the higher layers. For example, in the summarization experiments, word2ketXS embeddings corresponding to 8000-dimensional embeddings are compared to a standard model with embeddings of size 256. The LSTM and layers for the word2ketXS model would become quite large but their size is not taken into account. In addition, the activation memory is often the major bottleneck and not the parameter memory. These issues are not discussed or made explicit in the experiments.\n\nOverall the paper can be a strong contribution if the methods are stated with less quantum computing jargon, the overall parameter size and speed of the different models is specified in the experiments, and more specific connections to related work are made. Ideally, an experimental comparison to a prior method for space-efficient embeddings.\n\nQuestion: What is the role of pre-trained Glove embeddings in the word2ket models? Was any pre-training done on unlabeled text?\n\n\nSome typos:\n\nSection 1.1\n\n\u201cmatrix, as the cost ..\u201d  -> \u201cmatrix, at the cost\u201d\n\nUnder Eq (2)\nI think you mean w instead of u \n\nSection 3.2\n\nF_j: R^t -> R^p , do you mean R^q \n"}