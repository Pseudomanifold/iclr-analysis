{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel deep reinforcement learning algorithm at the intersection of model-based and model-free reinforcement learning: Risk Averse Value Expansion (RAVE). Overall, this work represents a significant but incremental step forwards for this \"hybrid\"-RL class of algorithms. However, the paper itself has significant weaknesses in its writing, analysis, and presentation of ideas.\n\nThe main strength of this work is its empirical section. The proposed algorithm is fairly compared against many relevant baselines on a variety of continuous control tasks, on both the Mujoco and Roboschool simulators, and demonstrates significant performance increases across all of them. The visualization in Figure 4 is interesting, and provides good insight into the issues with DDPG and STEVE, and the reasons for the success of RAME. Based on these results, the authors have convinced that RAME is a state-of-the-art algorithm.\n\nHowever, in spite of its strong performance on benchmarks, I believe that this paper needs a significant overhaul/rewrite before publication.\n\nOne major criticism I have is on the authors' treatment of the different types of uncertainty. The introductory sections spend a large amount of time on the differences between aleatoric and epistemic uncertainty, and various other related concepts. But when it comes to the actual new algorithm, that the authors only barely touched upon the (very important) point that V[Q^DVE] is actually a *mix* of epistemic and aleatoric uncertainties. When minimizing this term, it's not clear whether epistemic or aleatoric uncertainty is actually being reduced. (Based on the experiments, which report only expected values, not CVaR or anything of the sort, it seems the authors care only about reducing epistemic uncertainty; if so, it's unclear why they choose to minimize a term which includes aleatoric uncertainty too.) A more principled understanding of this quantity seems essential to this line of work. Similarly, \"risk\" typically refers to aleatoric uncertainty, but the risk-senstive component of RAME computes the confidence lower bound w.r.t. a mix of aleatoric and epistemic uncertainty. Calling this algorithm \"risk sensitive\" is likely to generate confusion, in my opinion.\n\nA few other points on presentation of ideas:\n- Switching from a deterministic to a stochastic model is a trivial extension of MVE/STEVE, and way too much time is spent on this point (even going so far as to name a new algorithm!). Section 4 contains no new insight, beyond \"deterministic models can be bad in stochastic environments\", which is obvious. Consider re-thinking your experiments for this section to help readers better understand *what* goes wrong.\n- In my opinion, RAME is as much a successor to TD3 as it is to STEVE. Taking the lower outcome of an ensemble of size 2 is equivalent to applying a penalty based on the stdev of the outcome distribution, with \u03b1=1. The introduction of the paper should be re-written to clearly point out that that RAME ~= TD3 + STEVE.\n- I'd also like to see a lot more ablations, on at least a few environments. There are at least four factors that need to be teased apart: 1) deterministic vs stochastic model 2) lower confidence bound penalty 3) adaptive \u03b1 for LCBP 4) STEVE reweighting. Does RAME require all of these elements to perform well? How do these elements interact?\n\nSome feedback on the writing:\n- There are various small factual errors. For example, in the very first paragraph, the Dyna algorithm is attributed to Kuturach et al, instead of Sutton (1990). (The algorithm from Kuturach is ME-TRPO.)\n- The notation is not very good; there are lots of symbols flying around everywhere, and it makes the ideas (which are fundamentally very simple) a bit difficult to parse. For example, naming each parameter-set individually, everywhere, is unnecessary.\n- There are some strange non-sequiturs and overall lack of flow.\n\nI think that this work has a lot of potential, and am especially impressed by the empirical results. I recommend rejection in its current form, but hope to see a revised version of this work appear at a conference in the near future."}