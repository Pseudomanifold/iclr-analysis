{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of high function approximation errors facing the stochastic environments when trying to combining model-based reinforcement learning (RL) with model-free RL. The paper begins by showing that previous methods like model-based value expansion (MVE) and stochastic ensemble value expansion (STEVE) can perform even worse than the pure model-free DDPG algorithm in a rather noisy environment. Following the ideas of MVE and STEVE, It then proposes a risk averse value expansion (RAVE) to replace the target Q function in the actor-critic algorithm, which is built upon an ensemble of probabilistic models (PE) and adopt the lower confidence bound as a surrogate of the target value as in the risk-sensitive RL. A simple yet intuitive approach for adaptively selecting the confidence bound \\alpha is also proposed. The experiments show that RAVE does improve over the state-of-the-art algorithms in several different environments, with a better draw-down control. In general, this paper is well-written and the idea of RAVE is novel as far as I know. But since I'm not very familiar with the specific literature of combing model-based and model-free RL, and since the idea of RAVE is relatively straightforward (but admittedly practically powerful and theoretically interesting), I choose to give a conservative accept to account the possibility that some existing works have followed very similar approaches. \n\nSome minor comments: 1) What is the loss of the negative log-likelihood in (4) for? 2) The authors may want to explain clearly what is the variance indicating in (8) more clearly, although one can guess that it is closely related to (5). "}