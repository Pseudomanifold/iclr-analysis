{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a regulariser that encourages the formation of line attractors in RNNs. This regulariser works on a neuroscience-motivated formulation of RNN, bringing the Jacobian of the dynamic system close to identity. This paper is well-written, with a good coverage of background material from machine learning to computational neuroscience.  While the alleviating the exploding and vanishing gradient problem for simple RNNs is an interesting direction, I think the empirical results are not sufficient to support claims in the paper.\n\nThe paper starts by criticising initialisation and reparametrisation-based techniques. However, I am not convinced why such methods limit the expressive power of RNNs. In fact, one may argue that initialisation is a milder constraint compared with an explicit regulariser, since regularisation affects the entire learning process. It seems that such initialisation requires less tuning (i.e., just identity) compared with the regulariser weights (a rather wide range of choices). Either a theoretical justification or strong empirical results are required to support this claim. However, both are missing in the current paper. \n\nExperiments:\n\nFirst, the baseline results for even toy problems (e.g., addition) are unclear. Despite the results in the paper show the advantage of the proposed method, direct comparison with results from other papers are missing. For example,  when the T > 150 the results from Le et al, 2015 (A Simple Way to Initialize Recurrent Networks of\nRectified Linear Units) were much better compared with baseline results in the paper. This could be due to the smaller size of the models (40 vs 100 hidden units in Le et al.). For clear and direct comparison in this case, models with comparable size should be used in these experiments. Despite this, I wonder why the performance of addition and multiplication seem even worse than the much smaller model reported in Hochreiter and Schmidhuber 1997? (see table 7 and 8)? \n\nActually, it would be helpful to test the proposed method on more practical tasks such as language (at least synthetic ones) and speech modelling, which would bring more impact on the wider community.\n\nA few technical questions:\n\n- Is the form of eq. 1 necessary, or can the method be adapted for the more standard formulation of RNN used in machine learning? It seems that A can be interpreted as a skip connection\n\n- Eq.3 can simply be referred to as \u201csoftmax\u201d\n\n- Please comment on the algorithm in section 3.4 in comparison with more standard variational approaches, such as stochastic variational inference with reparametrisation as in variational RNNs (e.g., Chung, et al., 2015, A Recurrent Latent Variable Model for Sequential Data). Is \u201cstepwise annealling\u201d always necessary?\n\n- Eq. 7 as a measurement of the match between trajectories still depends on z. Is there an additional expectation over p(z)? Is \u201cfreely simulated trajectories\u201d the prior over trajectories? If so, what\u2019s the form of this prior?"}