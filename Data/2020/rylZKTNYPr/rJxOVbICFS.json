{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a regularization scheme to improve the ability of RNNs in capturing long-range dependency in the latent space. The proposed model then uses EM for inference and achieves superior performance in sequence modeling tasks over LSTMs and iRNNs. \n\n+ The motivation of the line attractor is novel and effective. The special RNN model studied in the paper has strong connections with neuro-dynamics models. \n+ The paper is well motivated and clearly written. The illustration about the line attractor is particularly interesting. \n+ Good experimental performance on multiple sequence modeling tasks including addition, multiplication and sequence MNIST.\n\n- The paper is building a generative model for sequences. It\u2019s not clear to me why VAE or variational RNN type of approaches cannot be used in this setting. One might think of replacing the Gaussian prior with more complex distributions. The inference procedure can also be significantly simplified with variational inference.\n- The step-wise annealing together with EM inference is not scalable, which prohibits the model from applying to large-scale sequence modeling tasks. \n\nMinor comments\n- \" All code used in this work is freely available on the github site ... . \" Remove this sentence\n"}