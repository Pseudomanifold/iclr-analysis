{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a deep method for anomaly detection (AD) that unifies recent deep one-class classification [6] and transformation-based classification [3, 4] approaches. The proposed method transforms the data to $M$ subspaces via $M$ random affine transformations and identifies with each such transformation a cluster centered around some centroid (set as the mean of the respectively transformed samples). The training objective of the method is defined by the triplet loss [5] which learns to separate the subspaces via maximizing the inter-class as well as minimizing the intra-class variation. The anomaly score for a sample is finally given by the sum of log-probabilities, where each transformation-/cluster-probability is derived from the distance to the cluster center. Using random affine transformations, the proposed method is applicable to general data types in contrast to previous works that only consider geometric transformations (rotation, translation, etc.) on image data [3, 4]. The paper conclusively presents experiments on CIFAR-10 and four tabular datasets (Arrhythmia, Thyroid, KDD, KDD-Rev) that indicate a superior detection performance of the proposed method over baselines and deep competitors.\n\nI think this paper is not yet ready for acceptance due to the following main reason: \n(i) The experimental evaluation needs clarification and should be extended to judge the significance of the empirical results.\n\n(i) I think the comparison with state-of-the-art deep competitors [6, 4] should consider at least another image dataset besides CIFAR-10, e.g. Fashion-MNIST or the recently published MVTec [1] for AD. On CIFAR-10, do you also consider geometric transformations however using your triplet loss or are the reported results from random affine transformations? I think reporting both would be insightful to see the difference between image-specific and random affine transformations.\nOn the tabular datasets, how do deep networks perform in contrast to the final linear classifier reported on most datasets? Especially when only using a final linear classifier, the proposed method is very similar to ensemble learning on random subspace projections. Figure 1 (right) shows an error curve that is also typical for ensemble learning (decrease in mean error and reduction in overall variance). I think this should be discussed and ensemble baselines [2] should be considered for a fair comparison. Table 2 also seems incomplete with the variances missing for some methods?\nFurther clarifications are needed. How many transformations $M$ do you consider on the specific datasets? How is hyperparameter $s$ chosen?\nFinally, I think the claim that the approach is robust against training data contamination is too early from only comparing against the DAGMM method on KDDCUP (Is Figure 1 (left) wrong labeled? As presented DAGMM shows a lower classification error).\n\nOverall, I think the paper proposes an interesting unification and generalization of existing state-of-the-art approaches [6, 4], but I think the experimental evaluation needs to be more extensive and clarified to judge the potential significance of the results. The presentation of the paper also needs some polishing as there are many typos and grammatical errors in the current manuscript (see comments below).\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. Well motivated anomaly detection approach that unifies existing state-of-the-art deep one-class classification [6] and transformation-based classification [3, 4] approaches that indicates improved detection performance and is applicable to general types of data.\n2. The work is well placed in the literature. All relevant and recent related work is included in my view.\n\n*Ideas for Improvement*\n3. Extend and clarify the experimental evaluation as discussed in (i) to infer statistical significance of the results.\n4. I think many details from the experimental section could be moved to the Appendix leaving space for the additional experiments.\n5. Maybe add some additional tabular datasets as presented in [2, 7].\n6. Maybe clarify \u201cClassification-based AD\u201d vs. \u201cSelf-Supervised AD\u201d a bit more since unfamiliar readers might be confused with supervised classification.\n7. Improve the presentation of the paper (fix typos and grammatical errors, improve legibility of plots)\n8. Some practical guidance on how to choose hyperparameter $s$ would be good. This may just be a default parameter recommendation and showing that the method is robust to changes in s with a small sensitivity analysis.\n\n*Minor comments*\n9. The set difference is denoted with a backslash not a forward slash, e.g. $R^L \\setminus X$.\n10. citet vs citep typos in the text (e.g. Section 1.1, first paragraph \u201c ... Sakurada & Yairi (2014); ...\u201d)\n11. Section 1.1: \u201cADGMM introduced by Zong et al. (2018) ...\u201d \u00bb \u201cDAGMM introduced by Zong et al. (2018) ...\u201d.\n12. Eq. (1): $T(x, \\tilde{m})$ in the first denominator as well.\n13. Section 2, 4th paragraph: $T(x, \\tilde{m}) \\in R^L \\setminus X_{\\tilde{m}}$.\n14. $m$, $\\tilde{m}$, and $m'$ are used somewhat inconsistently in the text.\n15. Section 3: \u201cNote, that it is defined everywhere.\u201d?\n16. Section 4: \"If $T$ is chosen deterministicaly ...\" >> \"If $T$ is chosen deterministically ...\"\n17. Section 5, first sentence: \u201c... to validate the effectiveness our distance-based approach ...\u201d \u00bb \u201c... to validate the effectiveness of our distance-based approach ...\u201d.\n18. Section 5.1: \u201cWe use the same same architecture and parameter choices of Golan & El-Yaniv (2018) ...\u201d \u00bb \u201cWe use the same architecture and parameter choices as Golan & El-Yaniv (2018) ...\u201d\n19. Section 5.2: \u201cFollowing the evaluation protocol of Zong et al. Zong et al. (2018) ...\u201d \u00bb \u201cFollowing the evaluation protocol of Zong et al. (2018) ...\u201d.\n20. Section 5.2: \u201cThyroid is a small dataset, with a low anomally to normal ratio ...\u201d \u00bb \u201cThyroid is a small dataset, with a low anomaly to normal ratio ...\u201d.\n21. Section 5.2, KDDCUP99 paragraph: \u201cTab. ??\u201d reference error.\n22. Section 5.2, KDD-Rev paragraph: \u201cTab. ??\u201d reference error.\n\n\n####################\n*References*\n\n[1] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592\u20139600, 2019.\n[2] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga. Outlier detection with autoencoder ensembles. In SDM, pages 90\u201398, 2017.\n[3] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018.\n[4] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[5] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai. Triplet-center loss for multi-view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1945\u20131954, 2018.\n[6] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Mu\u0308ller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393\u20134402, 2018.\n[7] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, A. Binder, E. Mu\u0308ller, K.-R. Mu\u0308ller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019."}