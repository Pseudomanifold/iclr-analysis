{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "Summary: This work looks at knowledge distillation: using a large teacher model to train a smaller student model that could have, in this case, as little as 62x fewer parameters. They look at BERT models and propose two ideas to improve effectiveness of knowledge distillation: dual training (encouraging alignment between the teacher and student wordpiece embeddings) and shared variable projection (aids in direct layer wise transfer to the student model).\n\nPositives: Model compression is important, especially for cases like BERT where models are massive. There are lots of works that work on similar things showing that compression is possible for these sorts of models, so studying best practices is an important area of research. The overall experimentation, idea development, and execution shows promise in both techniques.\n\nConcerns: I think the work is a great start of a line of research but don't think the experimentation has enough threads to weave together to tell a super convincing story. I'd like to see an investigation of different ways of compression rather than just at the hidden dimension level. What happens if you vary the number of layers? What happens if you compress non-uniformly for each layer, keeping later layers less compressed.\n\nOverall I think this paper requires more experimentation and analysis to be complete."}