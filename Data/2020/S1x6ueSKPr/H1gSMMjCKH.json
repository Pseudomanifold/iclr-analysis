{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper aims to compress the size of large pretrained BERT models for faster inference and cheaper storage. Their method is based on knowledge distillation into a smaller student BERT architecture that has a smaller vocabulary, as well as smaller embedding and hidden dimensions. Their method has 2 steps: 1) dual training which randomly chooses tokens from the sequence to segment using the student vocabulary, and the rest of the tokens are segmented using the teacher vocabulary, as well as 2) shared linear projection layers to align the trainable parameters of student and teacher models by minimizing l2 loss. They present experimental results showing that they can compress BERT base by up to 60x with only a small decrease in accuracy.\n\nStrengths:\n- This paper comes at an important time where larger models have been shown to achieve strong performance while requiring too many resources to train and evaluate. This paper makes a step towards smaller models that retain performance while enabling faster inference and cheaper storage.\n- The paper is generally well-written and easy to understand. Figure 1 was really helpful.\n- The authors evaluate on several downstream tasks and show that compression can be achieved while retaining performance, outperforming the knowledge distillation method proposed in [8].\n\nWeaknesses:\n- The novelty is somewhat limited as compared to [8], essentially the difference is to also vary the vocab used between student and teacher models, and also to use up and down projections for knowledge distillation in case the intermediate layer dimensions are different (in which simple matching does not work).\n- Why is a random subset of words segmented using the student vocabulary? Since the student model has a much smaller vocab size, it seems to make sense that common words are 'selected' by the student model and segmented, while rare words that are not important in masked language modeling or downstream tasks are not used in the student model, similar to [1,2] where common words are used in the vocab for language modeling and remaining words ignored. Could you expand more on the design decisions here?\n- One concern I have is the lack of comparisons with other approaches that do not use knowledge distillation but are still relevant in terms of reducing the vocab size/model parameter size. For example, [1,2] consider only modeling the common words (wordpieces/subwords in this case) for language modeling. A similar approach could be used to train a student BERT model with distillation to a smaller vocabulary. Likewise, you cite many related papers on quantization but do not compare to the performance of quantizing pre-trained BERT models and only storing the quantized weights for downstream tasks. There are also papers that studying reducing the vocab size/embedding matrix using low-rank approximations [3,4], codebook learning [5,6], and dropout [7]. I believe these are all fair comparisons. The only comparison is to [8].\n- There are some ablation studies to show that both dual training and shared projection layers are important. However, from Table 3, there are certain datasets and compression factors where only using DualTrain gives best performance.\n- From the experimental results, it is inconclusive as to whether shared up or down projection is preferred. Equation (4) simply says L_p, and not whether up or down projection was specifically better, so I'm assuming the authors tried both.  From Table 3 the results are quite inconclusive. It seems unreasonable to train 2 models and take the better one, so I was wondering whether the authors had any intuitions on which one in different settings.\n\n[1] Luong et al. Effective approaches to attention-based neural machine translation, EMNLP 2015\n[2] Chen et al. Compressing neural language models by sparse word representations, ACL 2016\n[3] Nam and Quoc. Integrating low-rank approximation and word embedding for feature transformation in the high-dimensional text classification, 2017\n[4] Grachev et al., Compression of recurrent neural networks for efficient language modeling, 2019\n[5] Shu and Nakayama. Compressing word embeddings via deep compositional code learning, ICLR 2018\n[6] Chen et al., Learning k-way d-dimensional discrete codes for compact embedding representations, ICML 2018\n[7] Chen et al., How large a vocabulary does text classification need? A variational approach to vocabulary selection, NAACL 2019\n[8] Sun et al. Patient knowledge distillation for bert model compression, arXiv 2019"}