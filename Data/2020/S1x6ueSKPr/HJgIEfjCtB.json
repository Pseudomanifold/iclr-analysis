{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes techniques for improving the compression rate of Knowledge Distillation on BERT models. There are two main ideas. The first is the reduction of the parameter by reducing the vocabulary size of WordPiece model. The observation that a large fraction of parameters in BERT are dedicated to WordPiece embeddings is interesting, as this hasn't received much attention in other works on BERT compression. The second idea is to add a loss function that measures the difference of parameters between teacher and student in the learned projection space. These two ideas are quite complementary in that one can apply one idea to vanilla knowledge distillation without having to apply another simultaneously.\n\nMy major concern on this paper is that experiments are not designed to measure the impact of each design choice. First, it is unclear whether DualTrain-based methods outperform PKD with 3 layers because having more layers is more advantageous than having more hidden dimensions. While PKD doesn't straightforwardly allow different hidden dimensions between teacher and student, it is a reasonably straightforward idea to train a shared projection matrix to enable it; concurrent work https://openreview.net/forum?id=rJx0Q6EFPB actually does that. If this was considered as too significant extension of PKD, then authors should've experimented with vanilla knowledge distillation with fewer hidden dimensions. Second, DualTrain-based models have both smaller vocabulary size and smaller hidden dimension size, and it is unclear which one played the greater role in successful distillation.\n\nI would suggest authors to carefully set up baselines that we shall precisely measure the impact of each design choice.  Vanilla knowledge distillation with the same vocabulary as teacher and smaller hidden units shall allow us to understand the impact of decreasing hidden units. By comparing this with DualTrain models with the same vocabulary size, we shall now understand the benefit of decreasing the WordPiece vocabulary size.\n\nAuthors make a very insightful note in the discussion that smaller WordPiece vocabulary would imply longer sequence lengths, which may degrade the performance of BERT models. This is an interesting observation, and authors should not avoid running experiments on tasks with longer sequences because it is likely to have negative results. By actually running experiments and breaking down metrics with different sequence length buckets, the research community shall better understand the impact of vocabulary size and the paper will be made stronger.\n\nMinor comments:\n\n1) Table 3 is somewhat difficult to understand because each approach is making a different tradeoff between accuracy and model size. Plotting the accuracy metric vs. model size curve may more effectively visualize the tradeoff.\n\n2) I don't think the variable C in equation 3 was defined. Since there are two vocabularies, it might be useful to clarify the variable and clarify whether there is a loss function for words in teacher vocabulary.\n"}