{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "> What is the specific question/problem tackled by the paper?\nThis paper tackles a restricted multi-task setting where the task is known. The main contribution is a new architecture for training a task conditional model. The new architecture is reminiscent of an encoder-decoder-classifier with ladder (latent) connections, the decoder is conditioned on task ID. The claim is this is a type of modulation, it is unclear. Results on three multi-task datasets show that the proposed method is slightly better than compared methods and single task learning. There is no theory or loss function to analyze.\n\n> Is the approach well motivated, including being well-placed in the literature?\nIn my opinion, this is lacking. The assumption that task ID is known is fairly severe. Unfortunately the prior works cited also have this restriction, whereas few papers under the topic of continual learning have removed this limitation. This assumption/drawback needs to be clearly mentioned in the paper and discussed if it is realistic? A related shortcoming is that the training data simultaneously comes from all the tasks, whereas prior work has looked at the more interesting setup where tasks arrive sequentially and incrementally. \n- Reference [1] seems relevant and should be cited as it shows context dependent gating of tasks / modulation as well. Other missing references e.g. learning without forgetting (LwF) [2] and [3]. \n- There is not a clear explanation to think that this is modulation since the result is only passed through a residual connection. More importantly there is no discussion on these important issues. I found the writing to be brief and sketched. \n- in the introduction, it would be good to define multi-task learning with the assumption made clear. It would be good to introduce what you mean by TD and BU clearly\n- Another drawback is assuming the tasks being encoded as integers, whereas there might be a continuous task space with interpolation, or hierarchical task structure.\n- \"However, all of these works modulate the recognition network\nchannel-wise, using the same modulation vector for all the spatial dimension of the feature-maps.\" - why is this not enough? A nontrivial explanation or discussion is needed. Simply extending to W(t, y, x, ch) would increase performance by a little.\n- how is the proposed model different from a conditional model like a task conditional classifier? Also in experiments.\n- How is the proposed model different from an encoder-decoder? The impact of \"modulation\" is not clear.\n- \"We can scale the number of tasks with no additional layers.\" - task conditional classifier can also scale in this way to the number of tasks. This claim is not valid.\n- Page 3: \"uncorrelated gradients from the different tasks\" - need not be uncorrelated, but still can be interfering\n- next about Kendall (2018) and Sener (2018) - need to compare and contrast to them.\n- Last para on page 3 seems not relevant.\n- Modulation equations: this seems specific to CNNs. How would you extend this technique to beyond CNNs to recurrent units or even simpler MLPs? Modulation as a technique has been successfully applied in these architectures as well.\n- \"added to the input tensor X through a residual connection\" - this is not clear at all. Are the residual connections not shown in Fig 1(d)? \n- \"it to be unfeasible due to their large dimensions\" - can you explain please? later you say \"To avoid the unfeasible computation burden of directly optimizing W\"\n- Fig 1d, would be good to mark the modulation arrows in a different color\n\n> Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\nHaving said that, I like the experimental section even in the restricted setup. But a lot of details are missing. It is not surprising that there is slight increase in performance over channel modulation due to the increase expressivity. \n- table 1: why is there degradation in the LL task across all methods? the introduction of an additional task seems to bring the performance back up. It seems to be a weakness of your method. Please improve the discussion. I'm inclined to think that the tasks are not uncorrelated, as claimed by the authors. \n- table 1: how did you arrive at the number of parameters like 1.12x? Doesn't the separate BU and TD nets mean you have at least 2x parameters compared to single task? It seems the larger number of params in single task is mainly coming from the hidden layers?\n- table 1: it would be fair for the comparison methods to have equal number of parameters as the proposed method.\n- Missing experimental comparison to Kendall et. al. 2018\n- Missing details about reproduction of results from Sener (2018)\n- An important baseline would be to show image sensitive full tensor modulation without the new architecture. Similar to XdG.\n- Another baseline should be a task-conditional classifier that takes task as input along with the image. \n- ablation study: what are the auxiliary losses? I could not find any details.\n- The third experiment with CUB seems to use a different loss function that the other methods. This is somewhat hard to evaluate.\n- number of parameters are not reported for the CUB experiment\n- \"where only a single pixel is labeled as foreground, blurred by a Gaussian kernel\" needs more details about the smoothing\n- In the CUB experiment, due to lateral connections, the top-down result 224x224 image is not the only input to the BU2 classifier, the interpretability argument is weak.\n- Why did you choose these 4 questions from CLEVR? There are many interesting types of questions that can be handled.\n\n"}