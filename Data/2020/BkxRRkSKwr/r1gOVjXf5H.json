{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe authors proposed a method for generating hierarchical importance attribution for any neural sequence models (LSTM, BERT, etc.) Towards this goal, the authors propose two desired properties: 1) non-additivity, which means the importance of a phrase should be a non-linear function over the importance of its component words; 2) context independence, which means that the attribution of any given phrase should be independent of its context. For example, in the sentence \"the film is not interesting\", the attribution of \"interesting\" should be positive while the attribution of \"not interesting\" should be negative.\n\nFollowing these two properties, the authors designed three algorithms to post-hoc analysis the importance of a given phrase p.\n1. [Sec 3.2] eq 4. expected differences in model predictions between the a sentence that contains p and the same sentence with p removed. The expectation is computed over the conditional probability Prob(sentence | p in sentence). In practice, the authors use eq 3 as a proxy to eq 4.\n2. [Sec 3.3] eq 5.  expected differences in the activation values of each layer. The expectation is computed over the conditional probability Prob(context-dependent representations | phrase-dependent representations).\n3. [Sec 3.4] eq 8. similar to 1 but we replace the phrase p with padded tokens.\n\nThe authors conducted experiments on SST and Yelp. Results show that their proposed context-independent attribution correlates better with a trained linear model's coefficient, achieves higher human trust.\n\nDecision: reject.\n\nWhile I found the idea of marginalizing out the local context interesting, I think the paper still needs more work on its formulation, experiments and writing.\n\nFormulation:\n1. In eq 3, the expectation is taken over the difference between the prediction on the sampled sentence and the one with the phrase removed. This may be problematic for longer inputs (a pargraph), where the overall prediction may not change a lot when you remove a single phrase (since the evidence is everywhere). For example, consider the input: \"The movie is the best that I have ever seen. It is remarkable!\". Removing the word \"best\" alone doesn't alter the prediction much. \n2. In eq 5, the expectation is computed over P(h | beta). It is NOT THE SAME as sampling words p(x_{\\delta} | x_{-\\delta}) and then consider their hidden states.\n3. In Sec 3.1, you mentioned that CD is limited since the decomposition of activation sigma evolves context information gamma, and you resolved this by marginalization. But it seems to me that the computation of element wise multiplication also evolves context information. How do you deal with these?\n3. What's the difference between eq3 and eq8? Are you just changing from remove the phrase completely to replace it by mask?\n\nExperiments:\n1. The performance of CD in Table 1 seems very different to the original CD paper (which is 0.758 for SST and 0.520 for Yelp). I am not sure what contributes to this big difference. Is it the trained model or data splits?\n2. Table 1 shows that your methods achieves higher correlation to linear model's coefficients. But why shall we consider linear model's coefficients as the ground truth for the learned neural model? For example, the fine-tuned BERT achieves lower correlation corresponding to the LSTM. Does that mean the BERT model performs worse than LSTM?\n\nMissing related references:\n1. Explaining Image Classifiers by Counterfactual Generation\n2. Rationalizing Neural Predictions\n3. Learning to Explain: An Information-Theoretic Perspective on Model Interpretation\n4. L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data"}