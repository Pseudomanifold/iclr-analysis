{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper proposes a method for selecting a subset of a large dataset to reduce the computational costs of deep neural netwoks. The main idea is to train a proxy model, a smaller version of the full neural network, to choose important data points for active learning or core-set selection. Experiments on standard classification tasks demonstrate that this approach can yield substantial computational savings with only a small drop in accuracy.  \n\nThis paper is well-written and was easy to follow, with a clear motivation. The paper does good job of demonstrating that the proposed algorithm is effective through a comprehensive set of experiments. Overall, I favor acceptance and would be willing to increase my score if the following are addressed:\n\n1) Training for a smaller number of epochs was mentioned as a possibility to save computation. In the experiments, this was only done for one of the settings. Is this because models trained for fewer epochs are ineffective for data selection? \nLooking at Fig. 5b, it seems like the error drops significantly around 14 min before plateauing. In practice, for a new dataset, it could be difficult (or impossible) to know when to stop training a proxy a priori so that it achieves good performance relative to a larger model. It could be interesting to look at the effectiveness of a proxy at various points during training to see if the benefits are sensitive to the training time.\n\n2) For the active learning experiments, how many data points were added to the training set in each round? I could not find this number in the paper. Also, how many rounds of data selection were there?\n\n3) Is there an explanation why in Fig. 2b and Fig. 7c, the resnet20 proxy performs better than the larger (and more accurate) models? In particular, it even outperforms the 'oracle' baseline, which I find surprising.\n\n4) On a similar note, in Fig. 7, for small subsets (30%), the random baseline outperforms all of the SVP settings. Why is SVP ineffective in this case?\n\nMinor comments/suggestions:\n- In the abstract: \"improvement in data selection runtime\" Is this \"data selection runtime\" different from the total runtime? If not, it could be clearer to simply state it as the \"total runtime (including the time to repeatedly train and select points)\". I was unsure if this was a different measure.\n\n- Did the authors try further reducing the model capacity? With the success of the smaller models, it seems natural to try pushing further in this direction.\n\n- Since one of the findings was that models with similar architectures were effective as proxies but not models with different ones, perhaps there could be even higher correlation if the proxies were initialized with the exact same weights as a subset of the full model. \n\n- The writing in Table 1 (and the ones in the appendix) is a bit small.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}