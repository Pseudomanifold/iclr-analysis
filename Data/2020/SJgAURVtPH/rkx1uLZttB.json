{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper address a series of problems in vision-and-language focusing on a representation that has 3D visual features. This include: referring expressions in 3D, image generation from language. The key novelty appears to be adding 3D features to support these three reasoning tasks, as well as a new dataset. \n\nSummary of review: The paper needs some more work before it can be published. It can become a much better paper if the following is handled. (1) Clarify the novelty explicitly; (2) Evaluate the novel components using well-described ablation experiments, SoTA baselines, and demonstrate how the novel representation yields better reasoning. I would also recommend to focus on fewer tasks but with more convincing evidence. \n\nStrengths: \nThe paper addresses exciting problems at the interface of embodied V&L, and 3D vision. Adding 3D may be an important component for understanding complex scenes, and is necessary for some types of spatial reasoning.  \n\nWeakness: \n(1) The paper tries to do \"everything\", hopping from one task to another in a way that makes the logical flow hard to follow. It claims victory in three different problems:  spatial \"affordability\" reasoning, referential object detection, instruction following, while generalizing better across camera viewpoints, and gaining from language-driven feature learning. As an example, Fig 1 illustrates 4 different tasks and architectures, with little insight into how they are related or can be integrated.  \n\nMore importantly, the paper does not delineate clearly which components of the model are novel. For example, which parts in the representation (Sec 2.1) are new , and which are identical to Tung et al (50)? My recommendation to the author is to reorganize the paper to clarify the main novel contribution (the representation), the novelty in he method, and the novelty in the data. \n\n(2) The writing style of the paper keeps the description vague about many of the technical details. This makes the work impossible to reproduce and the paper harder to review.\nAs an example, the paper does not describe the dataset explicitly. It states \"We consider the CLEVR dataset\". One would think they augmented the CLEVR dataset with 3D information, but then it appears that each scene is rendered from multiple azimuths and camera positions. What are the properties of the dataset? How many samples, objects, images? How was it generated? Is the train/test split compositional?\n\n(3) experiments: \n(3a) Affordability is tested on 100 sentences. This is an unusually small dataset that is easy to overfit, specially if multiple experiments are performed aloing time (\"grad student overfitting\"). More details are needed about what measures were taken to avoid this. \n\n(3b) referring expressions. The performance of the baseline is surprisingly poor. RR on CLEVR reports much higher IOU. Here, performance is reported in as 3/5, 0/5 which makes it hard to compare with previous baselines. \n\nminor: \n-- what is the meaning of colored frames in Fig 3? \n-- Fig 2 Image generation. \n   quantitative analysis w.r.t ground truth? w.r.t. baseline? \n   comparison to other image generatiojn from text (e.g. HIntz et al ICLR 2019)?\n   How were these three exaples selected?\n-- Only F1 scores are reported for RR. Need to report IOU as is commonly done.  \n-- Avoid over-hyped promises about common sense reasoning. \n"}