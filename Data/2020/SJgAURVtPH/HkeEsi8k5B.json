{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents an approach for learning 3D visual representations by using a set of 2D TGB images as input. Then, they associate a `natural language' utterance and it's parse tree to this spatially aware 3D feature representations  and show benefits of multiple downstream tasks. \n\nI liked the paper mainly because of the approach and the exhaustive evaluation and analysis. However I am not a 100% sure about the novelty and big promises and bold claims. I have given a weak reject because of the concerns mentioned in the weaknesses\n\nStrengths: \nThe paper has an interesting motivation - 2D feature representations present several limitations because they do not obey intuitive physics constraints and are therefore are not a suitable representations. Instead a 3D visual feature representation that are mapped to descriptions / instructions is more beneficial. \n\nThe paper validates the advantage of 3D feature representation over 2D feature representation through an exhaustive list of experiments by empirically showing improvements on range of applications - such as affordability reasoning, referrential expression detection and instruction following. \n\nWeaknesses: \n- Beyond a proof of concept that 3D feature representations are a useful way of incorporating spatial common sense into language understanding, the dataset will potentially be useful for other applications too. \n\n- In section 2.2, the authors mention if the two generated objects interpenetrate in 3D, we sample objects appearances and locations until we find  configuration where objects do no 3D interpenetrate. I wonder why does this constraint need to be hard-coded and not be learned implicitly by the model? \n\n- The paper also uses several terms which might be misleading. For instance - the description of the scene or instruction is generated based on a template but has been called natural language everywhere. Given that the language is templated and uses a limited vocabulary, it doesn't capture all variations and multiple paraphrases of the same description. Additionally the network is called `modular neural networks' which reminds of Neural Modular Networks; Andreas et al, 2015.  \n\n- The paper is also fairly difficult to follow and is quite dense. It took me multiple passes on the paper to fully comprehend every experiment and modeling choices. I think the paper would greatly benefit from explicit discussions around novelty. Specially in terms of modeling, what's the contribution on top of Tung et al (50). Is it the \"modular neural network\" which depends on parse tree of the description or instruction and high supervision. \n\n- The proposed approach, as mentioned in limitation, needs high amount of supervision. This makes the method unlikely to scale to real applications. Some discussion towards how this would scale to current datasets / simulators / Graphics engine would be great. "}