{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors trained many deep networks and compared the represented functions by which test examples they predict correctly or correctly throughout training. At points during training, there is a subset of test examples that all models get right and a subset that all models fail on. For models that generalize well, these two subsets tend to be large.\n\nStrengths:\n- Understanding why deep learning works is an important research area.\n- The authors conducted many experiments using different models and datasets.\n\nWeaknesses:\n- The paper overstates the results in my opinion.\n- It is not surprising that some train/test/OOD/artificial examples are easier or have higher probability under the training distribution and are thus correctly classified by all models, some are incorrectly classified by all models, and for others the models disagree. This is expected to break down when no generalization is possible in Figure 9, where all examples have low probability under the training distribution.\n- The consensus score that directly measures similarity of the learned functions does not seem to fully support the conclusions of the authors. For example, Figure 3 shows no clear order in which examples are learned --- most mass is spread out until the models become good at the whole dataset.\n- The histograms show many bins, where the left most and right most bins indicate model agreement. This can be misleading because there are many more bins for the disagreement. What do these add up to?\n\nClarity:\n- The paper is lacking structure. For example, the scores should all be defined in Section 2, which currently defines two scores with more definitions added in the experiment section. It could also help to introduce paragraph headlines.\n- The writing is often verbose. For example, redundancy in the introduction should be reduced.\n- The use of notation and equations is imprecise and more confusing than helpful. While the equations could be improved, I think the scores can more easily and clearly be described in plain words.\n- Text in the figures is unreadable because it is too small.\n\nQuestions:\n- What is the motivation for including the consistency score, which is just average accuracy for each example, compared to the consensus score? It seems that the latter better compares similarity of the learned functions.\n- What dataset was used to train the fully connected models in Figure 10? Why did the models use ELU activations and dropout, did this affect the results? Perhaps the results are explained by simplicity of the dataset, so all examples can be learned together rather than after another. The smoothness in the distribution might be explained by the higher epoch resolution compared to the other figures.\n\nSuggestions:\n- The paper could be strengthened by investigating what the easy/hard examples have in common. Do the models learn certain classes first? Or do they learn the least ambiguous examples first?"}