{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This is an empirical study that looks into the similarities/differences among neural network architectures. Here, similarity is defined by the predictions made on the training and test examples (i.e. two binary vectors for each neural net architecture). In addition, the authors look into how such similarities/differences evolve over time as training takes more epochs. The main conclusion obtained is that neural network architectures are nearly equivalent.  \n\nI have reservations against the conclusions of the paper and I believe the experimental results do not support the main conclusion. The experiments point to a much simpler explanation: that some examples are easy to classify while some examples are hard to classify correctly. First, if you look into the figures, such as Figure 1.a, you will find that bimodality is actually quite weak. Yes, about 77% of the examples are classified correctly by all neural nets, and about 6% are classified incorrectly by all neural nets, but there is still about 17% of examples that are classified differently by different neural nets. If all neural nets were need equivalent, then a much smaller proportion should fall in the middle. \n\nSecond, as pointed out by the authors themselves in Section 4.3, other learning algorithms such as SVM and Decision Tree, tend to fit those examples which neural networks learn first. This is a strong indicator that the experimental results can be explained using the data, not the \"equivalence\" of neural network architectures. \n\nThird, the authors demonstrate that bimodality is not an inherent part of neural networks by demonstrating that it does not hold for the Gabor patches datasets. This is a third clear evidence that bimodality was an artifact of the data itself, not the architecture. \n\nAside from this, the paper itself is not ready for publication. There are other issues such as: \n1- The authors repeatedly refer to the \"order of learning\" in the introduction without formally defining it first. I was able to understand what this means indirectly after reading their definition of \"learned at least as fast as\" another example. I suggest a formal definition is mentioned in the introduction since they repeatedly mention this property in that section. \n2-  The claimed relation to effective generalization is weak. The authors only show that when the labels are shuffled, bi-modality disappears. Against, as I mentioned earlier, this is consistent with the explanation mentioned above since if labels are random, then all examples are equally hard. \n3-  The term \"consistency\" already has a well-defined meaning in the literature. I suggest that the authors use a different term, otherwise it can be quite confusing to the readers. \n4- All of the experiments are done on images. The authors mention that the same phenomena is observed in other domains. If so, I suggest to include such experiments in the paper. \n5- Figure 3 seems to be a wrong figure. It is supposed to show \"consensus\", not consistency. \n\n\n\nSome additional minor comments: \n- There are phrases that are ambiguous. For example: what is \"perpetual similarity\" in Page 1?\n- \"Reproduce-able\" in Page 2 should be one word \"Reproducible\". \n- In all figures, the axis are hard to read. \n- \"networks instances\" should be \"network instances\" \n"}