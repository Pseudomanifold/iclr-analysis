{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper focuses on the problem of continual learning with limited memory storage. Specifically, the training data is arrived sequentially (might not be i.i.d.) for a model to exploit and there is not enough storage capacity to keep all the data without compression. This problem is important in many real-world applications with massive amount of data collected. The authors propose an approach named Stacked Quantization Modules to compress the data so that they can be stored efficiently. Each module is an auto-encoder with quantized latent representations. Several aspects including the communication between these stacked modules, and which level will a specific sample be compressed at, are taken into account in the algorithm design. In the experiments, the authors show some quantitative evaluations on CIFAR10 and ImageNet that the proposed method surpass several baseline methods. A qualitative visualization of LiDAR data reconstruction is also demonstrated. Overall I think the paper is tackling an interesting problem with an effective and novel solution. \n\nI have a few concerns that I wish the authors could help to clarify. First, in the VQ-VAE, each image is quantized to be H*W*D, where each D-dimensional vector is represented by the index of the nearest neighbor in the embedding table of each module. I checked the paper but could not find a place that discuss how this embedding table comes from. It is pre-defined with some pattern or is it learnt somehow? \n\nWhat is the latent space size of each module when trained on CIFAR10 and ImageNet? \n\nThe experiments on ImageNet only select 100 classes out of the 1000 classes. Would this method extends to large-scale datasets? How would the form of the tasks (in case of number of classes per task) affect the results? \n\nThere seems to be some typos. For example, the end of the first paragraph of Sec. 4.1 mentioned \"line 13\" of Alg. 4, which is not referred correctly as Alg. 4 only has 10 lines. "}