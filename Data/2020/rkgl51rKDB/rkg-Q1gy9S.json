{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary ###\n\nIn this paper, the authors focus on the problem of meta-reinforcement learning (meta-RL). Specifically, the authors consider the setting of meta-RL for goal reaching tasks where each task corresponds to an unknown goal. Existing meta-RL algorithms directly train for a policy that output low level actions, which might be inefficient in this goal-reaching setting. In this paper, the authors combine the hierarchical RL framework of HAC[1] with the probabilistic task context inference method of PEARL[2], and propose the meta-goal generation for hierarchical RL (MGHRL) algorithm. In this algorithm, a two layer hierarchical policy is used where the high level policy generate goals for the low level goal-reaching policy to reach. In order to adapt to an unknown goal, the high level policy is conditioned on the output of a task inference module to generate goals for the unknown ground truth goal. The goal-reaching policy would then use the generated goal to interact with the environment.\n\nThe authors evaluated the proposed method on simulated robotic manipulation tasks and compare to PEARL as baseline. The experiment results show that the proposed method outperforms the baseline method significantly, especially under sparse reward settings.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in learning fast adapting goal-reaching policies. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\nPro:\n\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative illustration to explain the architecture of the hierarchy of policies. The experiment results are easy to interpret.\n\n2.  The authors provide a detailed description of the configurations and the hyperparameters for each experiments. Such description would be very helpful if the results in this paper are to be reproduced.\n\nCon:\n\n1. The experiments presented in this paper do not include appropriate comparisons to baseline methods. While indeed the proposed method outperforms PEARL, this comparison is inherently unfair. PEARL is a general meta-RL algorithm, which can adapt to arbitrary variations of reward functions and dynamics in the distribution of tasks. The proposed method only applies in the setting of goal reaching meta-RL, where each task corresponds to an unknown goal. With this information artificially encoded into the hierarchical architecture, the proposed method should certainly perform better than any general meta-RL algorithms. Therefore, directly comparing the proposed method to any general meta-RL algorithm is unfair. Instead, the authors could compare with baseline methods with builtin goal-reaching components, such as the following one: train a goal reaching policy using HER[3], and then meta-train a goal conditioned reward function using standard supervised meta-learning methods. At test time, find the goal that maximizes the adapted reward function, and then feed that goal into the HER policy for evaluation. Note that this baseline is different from the proposed method in the way that the goal reaching and goal inference were done separately both using existing methods.\n\n2. I\u2019m not convinced about the novelty of this paper. The proposed method seems like a straightforward combination of HAC and PEARL, and it seems to me that the two methods are combined in order to apply an existing meta-RL algorithm in a goal reaching setting rather than to create a better general meta-RL algorithm.\n\nThe idea in the paper is well presented and carefully investigated. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement given the lack of proper baselines. Therefore, I would not recommend acceptance before these problems are addressed. \n\nReferences\n\n[1] Levy, Andrew, et al. \"Learning multi-level hierarchies with hindsight.\" (2018).\n\n[2] Rakelly, Kate, et al. \"Efficient off-policy meta-reinforcement learning via probabilistic context variables.\" arXiv preprint arXiv:1903.08254 (2019).\n\n[3] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Advances in Neural Information Processing Systems. 2017.\n"}