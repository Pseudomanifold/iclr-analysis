{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the problem of leveraging past experience to quickly solve new control tasks. The starting point (and perhaps the main contribution) is the observation that some tasks have similar high-level goals, while differing in how those goals are achieved. To that end, the paper introduces an meta-RL algorithm that, given a new task, attempts to solve it by adapting a high-level, goal-setting module, and learn a new, low-level policy to reach each commanded goal. The proposed method might be viewed as a combination of PEARL [Rakelly 19] and HAC [Levy 19]. The proposed method is compared against state-of-the-art hierarchical RL and meta-RL methods on four robotic manipulation tasks. The proposed method outperforms the baselines on each task.\n\nWhile the proposed method is quite strong empirically, I am leaning towards rejecting this paper because many of the claims made in the paper are not empirically validated. While much emphasis is put on the hierarchical aspect of the algorithm, I don't think that the tasks used in the experiments require hierarchy to solve (see [Plappert 18]). In the introduction, the second claim is that the proposed method \"focus[es] on meta learning the overall strategy \u2026 [and] provides a simpler and better way for meta RL.\" While the experiments show that the proposed method learns better than baselines, I don't think the paper show that the proposed method learns some sort of \"overall strategy.\" I don't think that the proposed method is simpler than the baselines.\n\nA second concern is that I'm confused about the experimental protocol. If the high-level policy outputs a desired XYZ position for the gripper (Section 5.1), how can the high-level policy indicate when the gripper should be closed to pick up the block? How is the reward function for the low-level policy defined? PEARL doesn't have access to this extra information (the reward function), right?\n\nA third concern is the large number of grammatical errors in the paper.\n\nI would consider increasing my review if (1) a new plot were added to visualize the commanded subgoals (I have a hunch that the high-level policy directly outputs the true goal, obviating the need for hierarchy and contradicting the claim that the method \"learns to generate high-level meta-strategies over subgoals\"); (2) the experimental protocol were clarified; and (3) the number of grammatical errors were significantly reduced.\n\nOther comments:\n* \"inefficient to to directly learn such a meta policy\" -- Why? Also, \"to to\" is repeated.\n* \"Deep Reinforcement learning\" -- \"Reinforcement\" shouldn't be capitalized.\n* \"failing to generalize\" -- Can you add a citation?\n* \"it would be quite inefficient to directly learn such a policy\u2026\": Doesn't [Plappert 18] do exactly this?\n* \"When the tasks distribution is much wider \u2026 these methods can hardly be effective\u2026\" -- Where is this claim substantiated? Also, \"tasks\" should be singular.\n* \"sparse reward settings which is\" -> \"sparse reward settings, which are\"\n* \"the above mentioned problems\" -> \"the problems mentioned above\"\n* \"our algorithm focus on meta learning \u2026 which provides a much simpler \u2026\" -- Where is it shown that the proposed method is simpler? Also, \"focus\" should be \"focuses\"\n* \"1991),which\" -- Missing space\n* \"complex tasks which requires\" -> \"complex tasks that require\"\n* \"Nachum et al \u2026 set of sub-policies\" -- Run on sentence.\n* \"... human leverage\u2026\" -- Don't humans also transfer low-level knowledge across tasks, in addition to high-level knowledge? Also, \"human\" should be plural.\n* \"algorithms.The\" -- Missing a space, I think.\n* \"PEARL leverages \u2026 latent variable Z\" -- This sentence doesn't make sense as written.\n* \"z's\" -- This should not be a possessive.\n* \"Good sample efficiency enables fast adaptation \u2026 and performs structured exploration \u2026\" -- Isn't the first part true by definition? Why does good sample efficiency perform structured exploration?\n* \"a goal is a 3-d vector\" -- If the goal output by the high-level policy is the XYZ coordinates of the \n* \"SAC, such non-hierarchical\" -- Grammar doesn't make sense here.\n* \"such non-hierarchical RL method has been proved to perform badly before on \u2026\" -- Can you add a citation? Generally, \"proved\" is reserved for mathematical proofs.\n* \"In this paper, We have\" -- \"We\" should not be capitalized."}