{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a linear symmetric quantizer for integer accelerators called LLSQ, which learns the quantization scaling factor using simulated gradient as update policy. Their main contribution is enabling inference on integer-only hardware by covering all parameters of all operators in convolutional networks, including weight, bias, activation and scaling factor. To address the quantization noise issue in bias parameters, they adopt Straight-Through Estimator and fine-tune the parameters after quantization. To improve inference efficiency, they apply BN layer fusion. They conduct experiments on public datasets for image classification and object detection to conclude that LLSQ achieves lower accuracy degradation compared to previous work. Finally, they test the quantized model on a specialized integer accelerator, showing the feasibility of the quantization on real hardware.\n\nIn conclusion, this paper generalizes linear symmetric quantization to all parameters in order to deploy the network on specialized integer neural network processors for efficient inference. In terms of algorithmic contribution, this paper introduces the scaling factor as a learnable parameter of the quantizer, but lacks enough theoretical justification. Therefore, I would consider weakly accepting the paper.\n\nFor the algorithm, the following should be addressed.\n1.\tIn scaling factor updating policy, the simulated gradient performs better compared to EMA. However, the motivation for choosing this policy is unclear, and there is no mathematical derivation of the gradient value.\n2.\tIn this study, pretrained network weights in full precision are used, thus the quantization procedure should take fixed weights as inputs. However, in section 3.3, the quantization scaling factor is updated in the training phase of the network, with weights being updated at the same time This seems to be a contradiction, and the experiment results where quantizer is trained with network weights fixed should also be presented.\n\nFor the experiment, the following should be addressed.\n1.\tThe training time for different bit-width settings should be included in the results.\n2.\tThe reason for leaving the first and last layers in full precision is unclear, and doing so may go against the objective of deploying the quantized model on specialized integer hardware according to the paper.\n"}