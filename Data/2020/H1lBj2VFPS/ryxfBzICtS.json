{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The submission proposes to train a linear symmetric quantizing function for integer processors.\n\nThe proposed idea makes sense at a high level, and the empirical results look somewhat compelling, but the write-up is not particularly clear (see clarity-related comments). I found it hard to extract a complete picture of how the proposed approach operates: from the leftmost diagram in Figure 1 I can infer what high-level steps are involved, but I wouldn\u2019t know how to re-implement the approach from the textual description itself.\n\nClarity-related comments:\n\n- The submission never explicitly states what x^r and p(x^r) correspond to in the network. My understanding from the context is that x^r is the scalar value of a model parameter or activation, and that p(x^r) is the empirical distribution over all model parameters and activations. Can the authors clarify?\n- Some kind of pseudo-code for the re-training of the weights and activations (Section 3.3) would help clear up reader confusion. At this point, are only the weights and activations quantized in the network? How is backpropagation handled (and if the straight-through estimator is used, why is it only mentioned in Section 3.5)? How do parameter and alpha updates interleave?\n- How many alpha values are there? Section 3.3 gives the impression that there is a single quantization parameter shared by all parameters and activations, but Equation 3 uses different alpha values for the weights and activations.\n- Section 3.4 is difficult to parse due to bad notation. If the output of the convolution layer is fed into the batch normalization layer, it would be clearer to reuse symbols (i.e. change \u201cx\u201d to \u201co\u201d in Equation 4).\n\nAdditional comments:\n\n- Organization of the different sections could be improved. Related work discussion is scattered throughout three different sections, namely Introduction, Motivation, and Related Work.\n- Writing quality could be improved  (e.g., \u201c*Except that*, some designs rely [...]\u201d, \u201cEdge or embedded neural network accelerators *are generally having* three primary design goals [...]\u201d) but has a relatively small negative impact on readability."}