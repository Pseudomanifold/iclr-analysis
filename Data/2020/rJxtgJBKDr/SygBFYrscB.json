{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThis paper proposes a new way to do transfer learning. Specifically, authors first train a big source ConvNet and then for each task, they train a small ConvNet in which each layer subscribes to some k channels in the corresponding layer of the source ConvNet. Authors show that this model works better than methods that fine-tune the last few layers of the source network and performs close to costlier methods like progressive networks but with lesser parameters and higher throughput. Experiments on 5 tasks verify their claim.\n\n\nMy comments:\n\nOverall, this is a very interesting paper.\n\n1. This is an interesting model to do transfer or lifelong learning but only for ConvNet architectures with image data. To avoid overstating the results, I request the authors to highlight this limitation in both the title and the abstract.\n2. Page 3, para starting with \u201cIn detail\u201d: Is the ResNet50 for delta model pre-trained or not? I know it is not pre-trained based on future paragraphs. But it is good to clarify it here.\n3. Sharing the same source network across multiple tasks during inference time is useful only when all the tasks take the same input. This is a very restricted application. This needs to be elaborated and highlighted in the paper.\n4. I would like to see the LF results included in the paper even though it has catastrophic forgetting issues.\n5. In Figure 4, the x-axis represents training throughput or inference throughput? I guess it is training throughput. Also, are the models trained for all the tasks in parallel (as described in serving all the tasks at once section) or separately? Even though I can guess answers for these, it is better to make these explicit in the paper for the benefit of the readers.\n6. It is never a good idea to show test curves for a task. Please remove the test curves from Figure 4. Instead, use a separate validation set and show validation curves.\n7. Are the authors willing to release the code to reproduce their results?\n\nMinor comments:\n\n1. Section 1, second para, 1st line: \u201cwee\u201d should be \u201cwe\u201d\n2. Table 1: Fix grammar in MP description.\n"}