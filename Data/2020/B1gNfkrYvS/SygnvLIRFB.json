{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Authors argue that the routing mechanism is unnecessary in a capsnet and one can just simply aggregate. They scale the matrix product introduced in Hinton 2018 to rank 3 tensors to decrease the number of parameters. Furthermore, they eliminate the free-form initial convolution layers.\n\nThe motivational derivation on equation 3 is simply wrong. c_ij is not a scalar or a learnt parameter. It is a function of c_ik at previous iteration for all other k. equation 3 would have been correct for a scalar or a learnt parameter but not a function of other parameters that just multiplication with W will not bring into the equation. Removing c_ij all together reduces capsule networks into simple CNNs, because there is no notion of agreement (cluster finding, attention) anymore. It is just a CNN with more weight sharing now. Also arguing that since number of parameters involved is jut 7 percent it is not important is just nonsense. The importance of a parameter is not based on the quantity. Several routing iterations may indeed be unnecessary, but a grouping mechanism need to still validate if the votes agree or not or it will be simple pattern matching like normal CNNs. \n\nCapsNets where proposed as a network that have comparable classification accuracy with CNNs with the extra benefit of the transformation generalization. If the classification accuracy was the primary goal one would have just used ResNets or etc. There is complete ignorance to the main point of CapsNets (viewpoint generalization) throughout the paper. No experiments showing any viewpoint generalizability of the current proposed network. \n\nAs for Adversarial Robustness of EMCapsNet, they also have fewer parameters incompare to the baseline CNN. I would argue the white box rebustness exactly comes from the aggregation method (here essentially an xor function is replaced with addition). "}