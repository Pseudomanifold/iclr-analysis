{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a new architecture that takes inspiration from capsule networks and CNNs. Specifically the authors propose a layer called the capsule layer which takes a rank 3 tensor as input and outputs another rank 3 tensor. \n\nThe authors also discuss routing procedures but ultimately there proposal does not have any routing procedure, and their architecture can be thought in a more straightforward manner as a generalization of a CNN. The fundamenta lconjecture in the paper is that \" the strong modeling ability of CapsNets come from this tensor to tensor mapping between adjacent capsule layers\" and the authors demonstrate this empirically on CIFAR and MNIST dataset.\n\nThe basic way in which the authors generalize CNNs is by using elementwise-multiplication and addition between 3D tensors to create an algebra (strictly speaking a ring, but nothing stops the authors from using elementwise division as well) over these elements. After this step the 3D tensors can be treated simply as scalars. In essence this ties the weights that can act on each 3D tensor and results in parameter reduction. \n\n\nThe strong point of the paper is the great empirical performance of the proposed neural network architecture on the CIFAR and MNIST datasets. Unfortunately, it seems to me that this is not nearly enough work to conclusively show the importance/utility of this idea. There are many other datasets/benchmarks such as Fashion-MNIST, CIFAR-100, and larger datasets such as ImageNet that are now considered essential for acceptance of a purely empirical contribution such as a new neural network architecture.  \n\nBecause of the above reasons I cannot recommend acceptance of the paper in its current state.\n\n"}