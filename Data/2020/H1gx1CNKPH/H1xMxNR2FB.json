{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n- The paper proposes augmenting transformer neural networks with KNN-based information fetching modules that can access relevant external knowledge, combine knowledge from different sources, and integrate the information into seq-to-seq architectures. The authors apply their proposal to generative dialog modeling, and apply it to two dialog datasets.\n\nStrengths:\n- The paper is well-written and well-motivated.\n- The authors evaluate their approach on 2 publicly available datasets and compare it to existing approaches, showing improvements in terms of F1.\nThe authors conduct a human evaluation to compare their approach against other approaches.\n\nWeaknesses:\n- There are some details that are missing from the paper, for example details about the mapping operator, and the specific representations of E_i for the datasets used.\n- Parts of the analysis are rushed (e.g., sections 6.2 and 6.3).\n\nQuestions/Comments:\n- In the human evaluation, is there a difference between the ratings for seen and unseen topics for the Wizard of Wikipedia dataset?\n- Section 6.3 (especially the part on the effect of gating) can be improved with additional information/analysis."}