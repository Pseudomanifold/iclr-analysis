{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a model-based approach to safety in RL, where the agent uses a transition model to plan ahead to avoid actions that can lead it to unsafe states. They call the planning component an imaginative module. The agent takes the baseline state as input - that can be used to define either a safe or unsafe state, that is used in the planning component. The authors claim that using these two techniques they can tackle both the safe exploration (not violating safety constraints during learning) and irreversible side-effects (unintended irreversible behavior due to poorly designed reward-function).  They validate their approach on two grid world environments and self-driving car simulators.\n \nThis paper should be rejected because of the assumptions it makes goes against the very task they are trying to solve. In the sense, the task is trivial given the assumptions they have. \n\n1) The inconsistent assumption regarding the access to trajectories to learn a model. \nThe authors start with the assumption that the agent does not have access to the model (Sec 3) , and they explicitly learn the model. However, in the very next section (Sec 4), the authors assume that they can deploy a number of agents that interact with the environment randomly and collect that data to learn a complete transition model. Note that this assumption is wrong because:\nIf the random data agents are \u201csafe\u201d, i.e., don\u2019t violate any safety constraint or cause any harmful behavior in the environment, then it is equivalent to assuming the agent having access to all the data to learn the model. This is a very big assumption that essentially says the agent has access to the model, which defeats the purpose of the safe-exploration problem. \nIf the random agents are \u201cunsafe\u201d, i.e., they can violate the safety constraint, then it goes against the very claim made about their method being able to respect the constraints throughout the learning process. \n\n2) The assumption about the baseline state(s).  \nThis is also a pretty big assumption to have, that is not acknowledged in the paper. If the agent already has the set of all the states it needs to avoid (or the set of states that are safe), then along with the assumption regarding access to the model, solving reversibility is significantly easier task then the general safe exploration problem [1, 2]\n\n3) The results reported in Figure 4 are not statistically significant. The experiments are only run over 3 random seeds [3] \n\n4) Can you give a few more details about the assumptions? In terms of how realistic they are or how essential they are to the method.\n\n\nThings to improve the paper that did not impact the score:\n- The negative side-effects problem that this work address is only based on reversibility criteria. \nClaim about learning the dynamics model is sample efficient is unsupported. \n\n\n\nReferences: \n[1] Berkenkamp, Felix, et al. \"Safe model-based reinforcement learning with stability guarantees.\" Advances in neural information processing systems. 2017.\n\n[2] Dalal, Gal, et al. \"Safe exploration in continuous action spaces.\" arXiv preprint arXiv:1801.08757 (2018).\n\n[3] Henderson, Peter, et al. \"Deep reinforcement learning that matters.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"}