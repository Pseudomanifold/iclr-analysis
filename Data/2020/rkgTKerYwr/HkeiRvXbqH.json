{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper works on the problem of learning a representation for a \"network\" (as far as I can understand, a graph). It is realized by applying a graph network-based autoencoder with three losses to preserve the network structure, attributes, and labels. The main technical contribution of this paper is to learn the multi-task learning weight together in the training (section 4.6 and algorithm 1).\n\nAs far as I can understand from algorithm 1, the joint training is applying gradient descent to the loss weight to *minimize* the total loss. I am very confused about why it works: will all the loss weight converge to 0 so that the final loss is minimized to 0? Or will it just learn higher weight for easier tasks and lower weight for hard tasks?\n\nAlso, as raised by the open review, the paper does not compare to the baseline method, making the contribution unclear.\n\nI am not working on this field and I fail to understand the paper in 2 hours. Please point out if my above questions are invalid. My current rating is weak reject.\n\n"}