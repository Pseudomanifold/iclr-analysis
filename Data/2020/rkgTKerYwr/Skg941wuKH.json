{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a unified auto-encoder framework for learning labeled attributed network embedding in a multi-task setting.\n\nThe recommendation is a weak reject. The main argument is:\n- The key contribution is to learn the loss weighting layer together with the network parameters.\n- It is not clear how the weight is updated exactly as it is not described in the text and also not much information can be found in Algorithm 1 other than it is updated by the gradient of the corresponding loss. Also it is not clear if it is different or very different from related works say [1].\n[1] GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks (ICML\u201918)\n- The paper is well written and the empirical evaluation is carefully carried out. Also, it claims that this is the first work to learn the multi-task weights for labelled attributed network embedding. Yet the original contribution seems limited. \n\n\nSome specific comments:\n\u201cWe define an adoptive loss weighting layer which simultaneously \u2026\u201d\n-> \nWe define an *adaptive* loss weighting layer which simultaneously \u2026\u201d\n\nSection 5.8\n\u201cTraining the model with different loss weights can effect the performance in multiple tasks.\u201d\n->\n\u201cTraining the model with different loss weights can *affect* the performance in multiple tasks.\u201d"}