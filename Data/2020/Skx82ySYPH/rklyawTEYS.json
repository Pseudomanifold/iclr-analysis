{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper is devoted to self-supervised learning of local features (both detectors and descriptors simultaneously). The problem is old yet not fully solved yet, because handcrafted SIFT is still winning the benchmarks. This work mostly follows and improves upon SuperPoint (DeTone et.al 2017) and the follow-up work UnsuperPoint (Christiansen et.al 2019) architecture and training scheme.\n\nThe claimed contributions are following:\n  - use the recently published Neural Guided RANSAC as additional auxilary loss provider\n  - allowing the \"cells\" to predict keypoint location outside the cell while learning\n  - special procedure for improving descriptors interpolation\n\n\nThe experiments are performed on HSequences dataset (wrongly called \"HPatches\", as HPatches dataset is literally image patches, not full images), showing noticable improvement over the state of the art.\n\n\nStrong points:\n\n - Method is sound, paper is mostly well written and results are good (may be too good, see questions).\n \n \n \n Questions:\n \n  1) Regarding descriptor interpolation, which is claimed as contribution. It is not clear for me, how different it is compared to SuperPoint one, which also do descriptor upsampling, so that network output is H x W x [256], i.e. full resolution. Could you please clarify the differences to it? Also, in Figure 2 it is not clear, how one can do \"feature concatenation\" for blocks with different spatial resolution.\n\n  2) Why the IONet is used only for training?  Wouldn`t it better to actually learn everything end-to-end, which is already done in paper and evaluate? \n  \n  3) How is association in training (e.g. on Fig.3) done, if multiple cells in img2 returns keypoint close to the same keypoint in img1? \n\n  4) HSequences consists of two subsets: Illumination and Viewpoint. Could you please report results per subset instead of per whole dataset? Could you also please specifically report results for the following image sequences: graffity, bark, boat, especially for 1-6 pairs and visualize matches (same way as in Figure 4-6)?\n  The reason that I am asking these, is results looks like too well and I suspect overfitting to a points, which are suitable for estimation (small) homography, not general-purpose points.\n  \n  5) Could you please explain in more details, how did you do homography estimation precision benchmark? Specifically, was Lowe`s second nearest neighbor ratio used for filtering out wrong matches? If not, could you please repeat this experiments with it, at least for SIFT matches?\n  \n  \nSmall comments:\n    - list of contributions in abstract is inconsistent with 3rd paragraph in Introduction, which also lists contributions.\n    \n\n***\nOverall I like the work, but there are unclear moments to me. "}