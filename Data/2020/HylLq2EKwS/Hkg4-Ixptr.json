{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to learn a \"virtual user\" while learning a \"recommender\" model, to improve the performance of the recommender system. The \"virtual user\" is used to produce the \"reward\" signal for training the recommendation system (which is trained using RL). Jointly learning the recommender and the \"virtual user\" provides a synthetic feedback loop that helps to improve the performance of the recommendation system. The paper formulates the training of the \"virtual user\" as an inverse RL problem and uses adversarial regularizer.\n\nThe paper proposes an interesting idea but more experiments (and explanation) is needed to bring out the usefulness of the work. In general, the writing needs to be polished as well.\n\n===============\n\nFollowing are the items that need to be improved:\n\n## Significance of the results\n\n* The results in Tables 3 and 4 provide only marginal improvements over the baseline. These improvements do not appear to be statistically significant. It would help if the authors comment on why these results appear significant and also provide variance values/curves for the results.\n\n## Role of the feedback general F\n\n* Is there any separate loss for training F or is it always trained along with \\pi?\n\n* Is the feedback capturing some sort of \"memory\" or \"past preferences/behaviors\" of the user? If yes, wouldn't using a recurrent recommender also capture these effects without needing the feedback model F? Note that I am not criticizing the choice of F. I am trying to understand the role played by F (in addition to the recommender pi).\n\n* If there is no separate loss for F, I wonder how would the performance change if the F network was to be removed and the reward value was to be fed into a recurrent recommended. (The paper seems to have considered a special case where a non-recurrent recommender is used with the reward value). The reason I am stressing on this is that one of the key distinctions of the authors' work is the use of feedback generator and it would be useful to quantify the benefits on this modification.\n\n\n## Others\n\n* How is the static representation, x, computed? From eq 16 (appendix), it appears that x is a binary vector that captures what items have been purchased/reviewed. Is that correct? If yes, wouldn't x have an enormous dimensionality?\n\n* The recommendation system is operating in a sequential decision-making setup. The formulation of R and F do no consider any sequential information. For example, let us say that the recommender recommends the items a1, a2 and a3 in 3 timesteps. The reward at time 3 is a function of x and a3 and the information of a1 and a2 is not used.\n\n* The loss function has many components and I want to make sure I understand what gradients flow where. So please correct me if I missed something;\n\n    * supervised learning loss (from the real data) trains pi and F (equation 6).\n    * pi and F collaborate to get a high reward from R (since we do not have the ground truth corresponding to R). No gradient flows to R.\n    * Adversarial game between pi and R - which is used to update R.\n\n* I understand that the loss is defined according to the output of the last step but do the gradients flow through all the intermediate steps?\n\n* The training/inference procedure seems to be doing something strange. Let us assume that T = 5. So the 5 items are recommended to the \"virtual user\" and only the 5th item is recommended to the real user. Now the recommendation of the 5th item depends on the first 4 items that the real user has not seen.\n\n* In equation 15, what does the subscript F stand for?\n\n* What algorithm is used to train the policy pi?\n\n=================\n\n\nThe following are the items that should be corrected in the future iterations of the paper. Please note that these things did not impact the score.\n\n* It seems that the irrelevance of an item (for a user) is treated the same way as missing information about the relevance of the item. Could the authors discuss this more in the paper?\n\n* Is there any reason why this approach is only used with CF methods?\n\n* Some writing choices seem to make the problem statement more complex than it is. For example, the authors discuss how their work is different than traditional RL instead of simply stating that their work is set up as a POMDP.\n\n* Articles are missing (or mixed) up at many places.\n"}