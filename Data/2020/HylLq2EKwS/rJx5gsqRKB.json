{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Although I assume somebody well-versed in the recent collaborative filtering literature would not have trouble, I had too much difficulty understanding the setup and the model to be able to recommend the paper for acceptance.  Perhaps if you clarify the following questions in your rebuttal and the final version of the paper, I can view the paper more favorably:\n\n- In the Problem Statement section, it says that we assume that there is an unknown user preference vector a_i for each user with elements in reals.  How should we think about what a_i is?  Is it a binary vector? A probability distribution over items?  An arbitrary vector of reals? Nonnegative reals?  \n- In the problem statement, we have that \"recommender predicts preferred items . . . and the user generates feedback to the recommender.\"  Can you make this concrete?  Would feedback be like a binary label for whether each of the recommended items was of interest?  Or whether any were of interest? \n- In general, I had a very difficult time figuring out the difference between the information in x_i and \"feedback\".  Is it that x_i only has information about what user i selected, but no information about what the user was presented and did not select?  Is the feedback then the combination of what was presented and whether or not it was selected?\n- In Section 2.2, we discuss a_i^t as the recommended items provided by the recommender, sampled from the action space A -- this seems like a different a_i from the one in the Problem Statement section?  If so, this is an unfortunate notation clash.\n- If we're producing sets of items at a time, as your wording frequently indicates, should we think of the action space A as a set of sets of items?  \n- When describing the \"recommendation-feedback loop as a Markov Decision Process\", you introduce a transition probability over user preferences given actions.  Are you understanding a user's preferences to be changing over time?  Or is it just our estimate of the user's preferences that are changing over time?  \n- In Figure 2, does the Recommender have state that can accumulate all the virtual feedbacks v_i in each step?\n- After equation (1), it says that v_i is \"the embedding of user feedback to historical recommended items\", but in Figure 2 it seems like v_i can only have information about the preceding recommendation and estimated reward, rather than cumulative.  Can you clarify?\n- Is the reward estimator estimating something that could actually be observed?  Suppose we had a user in the loop -- can you give an example of what a reward would be for a particular set of actions?  Would it be 0/1 for whether the user clicked on one of the actions?   \n- In the setup for the learning task, Section 3.1 says that x_i is the \"historical behaviors of user i derived from the user-item matrix X and \\tilde{a}_i is the ground truth of the recommended item for the user based on his/her behavior x_i.\" I don't understand this.  x_i has all the items the user has clicked.  What new information is in \\tilde{a}_i?  Is it referring to the a_i in the problem statement, some vector representing preferences?  Is such a vector ever known during training?  Please clarify.\n- In the paragraph before equation 7, you say \"given the recommended policy and the feedback generator\" -- should that be \"given the recommender policy\"?\n- In Figure 2, we see the recommender producing what seems to be a vector a_i.  Is this a set of recommendations?  Or a distribution over items from which we could sample recommendations? Or a single recommendation, in which case the graphic should change?"}