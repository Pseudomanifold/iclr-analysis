{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an adversarial representation learning approach. The key difference with prior work is that the objective function is built around balanced error rates, one for classes, that is eventually used for classification, and two adversarial for predicting each of the protected attributes. Authors argue that proposed approach can simultaneously achieve accuracy parity and equalized odds.\n\nThe notion of accuracy parity does not seem to be very meaningful. For example, predicting uniformly at random seems like an intuitively fair classifier with EO gap 0 and DP gap 0. However it will not necessarily have error gap of 0 (i.e. satisfy accuracy parity), making me wonder if the notion of accuracy parity makes much sense.\n\nI am not really sure what is the Err0 + Err1 metric used in Figures 1 and 2. Is it not normalized and can vary between 0 and 2? In which case it seems counterintuitive for performance quantification. If it is normalized, then results on COMPAS do not make sense. Err0 + Err1 of all methods is above 60%, which is worse than predicting uniformly at random.\n\nPlease report your TPRs for classes grouped by protected attribute when reporting the results in the context of group fairness. Further for Adult dataset, reporting balanced TPR as a measure of accuracy seems to make more sense given the class imbalance.\n\n"}