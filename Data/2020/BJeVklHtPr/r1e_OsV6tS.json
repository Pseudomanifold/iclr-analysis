{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper conducts extensive experiments to verify two practical benefits of batch normalization. i) It increases the final test accuracy and the largest stable learning rate; ii) it enables efficient training with larger batches and a larger learning rate. In addition, the authors propose a new initialization scheme, \u201cZeroInit\u201d, to train a deep ResNet to improve the test accuracy. My detailed comments are as follows.\n\n\nPositive points:\n\n1. The experiments are sufficient. In this paper, the authors conduct extensive experiments to explore the benefits of batch normalization, and verifies the effectiveness of the proposed \u201cZeroInit\u201d.\n\n2. The method is effective in some cases. Specifically, the proposed \u201cZeroInit\u201d outperforms batch normalization when the batch size is small, and it is competitive with batch normalization when the batch size is not too large.\n\nNegative points:\n\n1. The importance and novelty of the empirical study should be emphasized. The practical benefits of batch normalization can be also found in other papers. For the first benefit, most studies (Bjorck et al. 2018) have found that batch normalization is able to improve the test accuracy. For the second benefit, batch normalization requires a large batch size and a large learning rate (Santurkar et al., 2018). Therefore, what is the difference between this paper and others? More critically, it is necessary to explain why batch normalization has these benefits. It would be better to provide empirical or theoretical justifications to support these. \n\n2. The motivation of the proposed \u201cZeroInit\u201d is not clear. (Balduzzi et al., 2017) states that \u201cthe correlations can be preserved by initializing deep networks close to linear functions\u201d. It is not clear how \u201cZeroInit\u201d preserves the correlations? \n\n3. Why initialize the scalar multiplier and biases to zero? What are the benefits of the zero initialization? Actually, the scalar multiplier and biases can be randomly initialized. When they are randomly initialized, what is the performance of the initialization? It is an important baseline to justify the effectiveness of the proposed initialization method. \n\n4. The technical details of \u201cZeroInit\u201d are not clear. It would be better to express the proposed initialization \u201cZeroInit\u201d in the mathematical formulation.\n\n5. The proposed initialization \u201cZeroInit\u201d is designed for deep ResNets. How to extend it to the other deep neural networks?\n\n6. This paper states that \u201cthe empirical success of batch normalization \u2026improves the conditioning of the loss landscape. However, our results conclusively demonstrate that this is not the case\u201d. Does it mean that batch normalization does not improve the conditioning of the loss landscape? However, the empirical results cannot justify this statement and explain the success of batch normalization.\n\n7. Some results of the figures are missing. In Figure 1, the experimental results of w/o batch norm with varying batch sizes (2^0 ~ 2^5) are missing. Similarly, Figure 2 also has missing results. Please provide more discussions about these missing results.\n"}