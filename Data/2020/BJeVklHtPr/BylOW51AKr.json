{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The name \"ZeroInit\" is very confusing, because that is how FixUp was called initially https://openreview.net/forum?id=H1gsz30cKX , perhaps the authors should consider a different name. I will call it \"NewZeroInit\" in my review to avoid confusion.\n\nThe paper focuses on training image classification networks without batch normalization. The authors claim that effectiveness of batch normalization, and methods which attempt to eliminate it, should be tested on a wide range of learning rates. On experiments performed on CIFAR they find that batch normalization is able to achieve high accuracy even with very high learning rates, in line with Goyal et al. 2017. Based on this, they propose a simplification of FixUp for image classification, in which they remove the need in progressive scaling of initialization, and propose to remove weight decay regularization, while adding dropout on the last layer. This \"NewZeroInit\" is tested on ImageNet and compares favorably to batch normalization and FixUp.\n\nThe closest studies are FixUp and Goyal et al. 2017, with the difference that FixUp studies both image classification ResNet and seq2seq approaches in the absence of batch normalization, and Goyal et al. show a wide range of large scale experiments on full scale ImageNet, whereas \"NewZeroInit\" studies small scale CIFAR dataset. It is thus unclear if \"NewZeroInit\" transfers to seq2seq.\nThere is also \"Bag of Tricks for Image Classification with Convolutional Neural Networks\" by He et al 2018 (missing citation) which evaluates a similar set of tricks on ImageNet ResNet-50 with batch normalization. In particular, they show that removing weight decay from BN bias and setting scaling gamma to 0 initially significantly improves the results.\n\nOn page 4 the authors say \"Given access to sufficient hardware, this will enable practitioners to dramatically reduce wallclock time of training (Goyal et al.)\". It is not clear what they mean, since Goyal et al. already enabled the reduction by increasing learning rate and minibatch size on ImageNet, whereas the results authors show are on small CIFAR dataset.\n\nOn page 5 the authors mention that they introduce bias to each convolution and classification layer, which is surprising because it is a standard way to composing a convolutional network.\n\nOverall, the most significant contributions of the paper are:\n - a study of minibatch size on CIFAR\n - removing weight decay from FixUp on ImageNet\n\nAlso, I am interested in the following results:\n - clear comparison of FixUp with \"NewZeroInit\" for image classification\n - ImageNet ResNet-50 results with dropout regularization in the final layer\n - ImageNet ResNet-50 results with FixUp, dropout regularization and no weight decay.\n - (optionally) seq2seq with NewZeroInit instead of FixUp.\n\nWithout these results it hard to judge the novelty and contributions of the paper, so I propose reject."}