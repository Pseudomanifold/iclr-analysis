{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a semi-supervised learning model for semantic dependency parsing using CRF Autoencoders. \nThe encoder adopts the model of Dozat & Manning (2018), which formulates the semantic dependency parsing task as independently labeling each arc in a directed complete graph.\nThe decoder is inspired from Corro & Titov (2019) and propose to generate sentences for multiple times (each time conditioned on a different head) and combine to yield the decoding probability.\n\nThe paper is written clearly, but there are some weak points, which hurt the quality of this paper.\n\n1. Novelty. \nThe idea of CRF Autoencoder is not new. Further, the encoder in this paper is not a CRF, so it is confusing to say \"using CRF autoencoder\". The main novelty seems to come from the design of the decoder.\n\n2. Experiments.\nFor semi-supervised learning, showing the proposed model achieves improvement over the supervised baseline is usually not enough. Comparing to self-training (a classic and straightforward method also capable of learning from both labeled and unlabeled data), at least, is needed.\n\nNoting that the main novelty comes from the decoder design, the readers usually expect to see how the new decoder design improve over a baseline decoder, e.g. consisting of simple class conditional distributions, as in the original paper of CRF Autoencoder ( Zhang et al. (2017) )."}