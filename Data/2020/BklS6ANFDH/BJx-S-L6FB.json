{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper considers the task of semantic dependency parsing (where the parses look similar to dependency parses but are DAGs instead of trees). The paper focuses on semi-supervised learning using auto-encoding loss: given a sentence s, the encoder defines a distribution p(Y | s) where Y is an adjacency matrix, and the decoder defines a (pseudo-)distribution p(\\hat s | Y, s) (Note the conditioning on s, which is missing in the paper). Both models can be factorized based on the arcs y_{ij}, so training and inference are both efficient. On standard benchmarks, the proposed method shows improvements when there are more unlabeled data available.\n\nThe method is similar to other works on autoencoder for semi-supervised learning: maximize p(\\hat s, Y | s) for labeled data and the marginalized p(\\hat s | s) for unlabeled data. Unsupervised learning improves the F1 scores by about 0.5 to 1.5 depending on the amount of data, which is decent for the SDP task.\n\nThe derivation is mostly sound, but there are some possible improvements to make it more rigorous:\n\n- The decoder is technically p(\\hat s | Y, s), with conditioning on s. So the factorization should be p(\\hat s, Y | s) = p(Y | s) p(\\hat s | Y, s). The prediction should be maximizing p(Y | s, \\hat s), which turns out to be equivalent to maximizing p(\\hat s, Y | s).\n\n- Page 3: h^head W h^dep should have a transpose on h_head.\n\n- Page 4: Not sure why GCN is mentioned here. My intuition for the reconstruction probability is that it measures whether the head x_k is helpful for predicting the next word in the language modeling task. The assumption here is that the head is useful for predicting the dependent words, but distracting for predicting non-dependent words.\n\n- The mention of LSTM on Page 4 caused me to think that the model is not arc-factored. However, since the LSTM only depends on s and the *gold* output sequence \\hat s_{1:i-1} = s_{1:i-1} (not a sample from the softmax), the model is arc-factored.\n\n- Equation 4 could be written as U(m_k^head \\odot m_{i-1}^pre), where \\odot is elementwise product.\n\n- The loss L(s) could just be a sum of two summations, one for labeled data and one for unlabeled data.\n\n- Appendix A is a bit unnecessary since the models are arc-factored. The third expression in the appendix should have \\prod_ij in the parenthesis.\n\nOther comments:\n\n- After reading the derivations a few times, I am convinced that the models are indeed arc-factored. However, this means that the model does not use CRF anywhere. (The CRF autoencoder paper uses CRF for the encoder model, while in this work the encoder is a directed graph.) It might be better to remove the mentions of CRF throughout the paper.\n\n- Is there a mechanism to prevent non-DAG graphs?\n\n- The work on semantic parsing (e.g., Poon & Domingos 09, etc.) are unrelated to SDP despite the name similarity.\n\n- One interesting experiment to run is to use all the labeled data + a large amount of external unlabeled data.\n\n- The paper briefly mentions some work on structured VAEs [https://www.aclweb.org/anthology/D16-1116/ | https://www.aclweb.org/anthology/P18-1070/ | https://arxiv.org/pdf/1807.09875.pdf]. The paper should draw a stronger connection to them. For instance, highlight that the arc-factored model makes it possible to avoid high-variance RL-based training."}