{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1393", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper focuses on semi-supervised semantic dependency parsing, using the CRF-autoencoder for integrating unlabeled data to train the model in a semi-supervised style. The paper has obtained a certain improvement in the results, indicating the effectiveness of semi-supervised training on low resource labeled data tasks. \n\nMy comments are given as follows.\nWriting:\n1.1. The author claims CRF-autoencoders in the title, which is not introduced in the text, especially the encoder part, even without any manifestation, important details are ignored.\n\nModeling\n2.1. From the model description part of the paper, Encoder and Decoder seem to have nothing to do with each other and can be regarded as two separate models.\n2.2 For the output Y of the Encoder is enough to obtain a semantic directed acyclic graph through the search algorithm. It is not clear why the author should choose a decoder for word generation. More importantly, the decoder does not guarantee the generation in the input words, how to locate the position of the generated word in the original sentence, use copy-generator? The author needs clarification.\n2.3 .In 2.2 DECODER, dependency graph is into m sub-graphs w.r.t each word, in which each corresponding word plays as root. I understand $s_k$ is only regarded as head, namely, the out-degree of each word, wonder why not consider $s_k$ as dependent (any computational difficulty occurs?). Expect to see experiments or necessary explanations for this concern.\n \nEvaluation\n3.1. For the use of unlabeled data, the pre-trained language models may be a good choice too. The reported results show that the LM target is superior to autoencoder in natural language understanding. Thus I expect to see using the pre-trained LM instead of using CRF-autoencoder for exploiting unlabeled data. Furthermore, is it possible to conduct experiments to see if the supposed pre-trained LM can work together with the proposed CRF-autoencoder?\n3.2. From the comparison between the supervised method and the baseline, the improvement of the results is not obvious enough and thus requires significant test. \n3.3. D&M baseline is not well explained. The results that the paper reports lack of comparison with those from related work.\n\nSuggestions:\n4.1. I notice the Decoder generators adopts LSTM, why not BiLSTM? If so, then each word may sense both head -> dependent and dependent -> head.\n4.2. For both encoder and decoder, it is suggested to replace LSTM RNN by Transformer. The authors may present additional experiments with Transformer or explain why LSTM setting has been good enough.\n4.3. Figure 1 is not so intuitive, especially for (b), in which Illustration of the decoder does not show the procedure of sentence generation.\n"}