{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper proposes to explain how a classifier makes decision by interpolating and sampling paths in latent space by auto-encoding frameworks. They propose a method providing series of examples highlighting semantic differences between decisions. They formalize the notion of a semantic stochastic path and introduce semantic Lagrangians. Experiments are conducted on MNIST and CelebA, with probability paths showing auto-encoding explanatory examples.\n\n\nMy main concern with the paper is that it does not sufficiently address why it is important to generate auto-encoding examples in the way they do. I'm not convinced that these path examples are enough explanatory or represent the decision of the black-box model with sufficient fidelity. I think the paper would be clearer if the importance and applicability of their methods were explicated. I'd suggest rejecting for now, though be open to changing this assessment.\n\nI find the experimental results hard to interpret and support the claims. Below is a non-exhaustive list of examples.\nFigure 2 & 6: The paper gives interpretations on different patterns presented in the figures. How can one access the fidelity of the plausible explanations?\nFigure 5: I suppose 'interpolation' is for the proposed method. How does it imply that 'the proposed saliency map decorrelates with the randomized version' compared with other lines?"}