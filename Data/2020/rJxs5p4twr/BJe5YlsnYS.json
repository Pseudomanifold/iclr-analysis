{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses how to extract human-explainable reasons for particular\noutputs from a black-box classifier that provides class probability outputs.\nAutoencoders provide a smooth (stochastic) path between contested to\nuncontested input. The autoencoders and path are optimized together to find an\nexemplary sequence of inputs in latent space that quickly change the output\nclass from the contested to uncontested label.  Localizing the classification\nchange region to a few inputs can help humans demonstrate or explain why a\nblackbox neural net made a contested classification.  The input space path\nwhere meaning rapidly changes once are called \"semantic paths\".\n\nThe mathematical presentation is quite general, finding semantic paths by\nminimizing a line integral of a Lagrangian.  Some of the papers most\ninteresting sections are in the appendix, where several possible Lagrangians\nare presented.  The main body of the paper presents 2 simple examples (more in\nappendices) showing examples where one can find a few images that visually\nallow one to \"explain\" the reason for a misclassification.\n\nSections 1--4 frame the problem and its potential legal scenario nicely.\nSections 5--6 present the Lagrangian formalism, other loss function components\nand the optimization procedure quite nicely.\n\nHowever, the experimental results section, 7, was difficult for me to follow.\nI did not understand the point about random litigation end pairs trying to\narrive at \"this specific path\".  Fig. 4 was not really understandable.  Which\nalgorithms corresponds to \"our saliency maps\" (vs. traditional \"saliency\"\nmethods)? And names of traditional saliency methods appear nowhere else in the\npaper, so need to be identified and merit at least a reference (perhaps Adebayo?).\n\nWhile the problem setting, general approach and theory is interesting and seems\nsound, the Results section must be clear for this paper to be accepted.\n\n---- Misc comments: ------\n\nI read, but did not verify proofs line by line.\n\nThe authors punt on how one might determine x_T --- some discussion, even if\nonly in the appendix, might be nice.\n\nThe paper describes stochastically generated images. For legal purposes can I\nselect a single unsampled preimage? Are VAE results significantly different\nfrom plain AE? Considering plain AE might also remove some complications\nfrom presenting the core idea of calculating a path-based loss.\n\nFigures displaying the autoencoder outputs might be improved by also providing\nthe original t=0 and t=1 images.  Fig. 2, for example, shows different t=1\nsevens.  I wondered which one most closely resembled the original.\n\n- \"the roll\" --> the role\n- Monotonous means boring, dull, tedious. So I would find some other way to\n  describe your \"close to monotonic\" functions, in Sec. 6 and the appendix D.4\n- Grad CAMP --> Grad-CAM\n- \"a randomized layers\"\n- Sec 8: \"are also provided\" --> \"have been performed\" (?)\n- A. Appendix seems empty.\n\nThe point in section 8 about finding a path along the input data manifold is\ngeneral, so think whether it might be better in Section 4 where the contrast\nwith off-manifold paths in adversarial training is made.\n\nI found appendices D.3--D.5 with examples of path loss functions quite interesting.\n\n\n"}