{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents and evaluates an anomaly detection algorithm that is used for identifying anomalies in satellite data. The considered satellite data originates from multiple sensors that are divided into sensor groups. The anomaly detection algorithm roughly works as a 4 stage process: \nRaw data is converted into a set of features per sensor that aggregate the last 10 min of sensor data, resulting in a tensor X with dimensions (time, features, sensors)\nA tensor decomposition method is used to decompose X into matrices A,B,C. \nMatrix A with dimensions (time, latent factors) is used to cluster data using k-means. \nTime points are considered anomalous when the latent feature vector is too far away all cluster centres. \nThis algorithm is experimentally compared to baseline methods that only use one sensor at a time. \n\nOverall, I think this paper is not fully ready for publication based on a) inconsistencies in notation that make it rather hard to read at first and b) open questions regarding the baselines, and c) missing experimental evaluations that may give insights into why the algorithm works better then the baselines. Moreover, besides those shortcomings, d) I am not convinced that ICLR is the right venue for this paper. Although the data set could provide an interesting application, the paper provides no theoretical contributions and the algorithm only pipelines known algorithms in a problem-specific way. In my view, a more application-focussed conference would be a better fit. \n\n\nDetailed comments.\n\nd) in my view is the most important issue. As mentioned above, the paper contains no theoretic contributions to the field of anomaly detection. Instead, it solves data-specific challenges for anomaly detection by pipelining a variety of methods in order to predict anomalies. While this approach is fine for solving an application, I am not convinced that it warrants a publication at ICLR, regarding in particular that the experimental evaluation does not provide any insights into why the algorithm works better than the baselines. That is,\nc) I am missing experiments that show the contributions of the individual choices that were made for each stage of the detection process. E.g. how much influence does the adaptive thresholding have on the performance? (Appendix C doesn\u2019t quantify that properly.) The same would be interesting for the other stages. \n\nb) You state that the baseline methods are single-telemetry-based. However, you do not explain how those methods detect anomalies for multi-telemetry data as presented in Tables 3,4. Do those methods detect an anomaly whenever any of the feature/telemetry-specific methods detect one?\nAlso, Table 3 is not correct and doesn\u2019t match the analysis in the text. Labels do not seem to be correct. Does Subsystem1 have 1 anomaly and Subsystem2  2?\n\na) \n-P4: Indices for the 3 dimensions of input data X are sometimes denote I_1,I_2,I_3 and sometimes I,J,K. \n-P4: the rank or number of factors is sometimes denied by lowercase r and sometimes by uppercase R, e.g. in the definition of the reconstruction error. Here, the running variable is denoted by lowercase r.  \n-P5, \u201aCalculating anomaly score\u2018: Shouldn\u2019t time samples have r dimensions instead of n?\n-P5, last paragraph of Section 4: \\mu should be defined by the normaliser w instead of n.  "}