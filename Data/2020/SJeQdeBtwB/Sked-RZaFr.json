{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:  The paper propose a cycle gan variants combined with RNN for time series anomaly detection. The setting is assuming training model on a given normal data, then applying the trained model to detect anomalies.  Different detecting criteria are studied in the experiments. \n\n1. It is not clear the advantage of using GANs in anomaly detection of the proposed algorithm, and it is questionable if using GANs is really useful. The authors only provide hand-waving explanation.  For example, the papers says  \" Therefore, once the Critic is trained, it should assign more or less stable scores to the normal sequences and a significantly different score to an anomalous sequence.\" in p5.  I'm doubt if it is true.  The GAN objective only says the score on x~p(x) and g(z) are similar. Also, the score for normal samples can also have a distribution since we only do mean matching in W-1 distance.  I think a stronger analysis with certain assumptions is necessary for making this statement and justifying the proposed algorithm.  Some possible route can be found in \n\nChang et al., Kernel Change-Point Detection with Auxiliary Deep Generative Models, ICLR 2019.  \n\nAlthough their setting is slightly different, where they focus on change point detection.  They provide a testing power lower bound explanation of using the critic of GANs, but they require some early stopping. Otherwise the guarantee won't hold.  I'm wondering if the analysis can be extended to here. Also, if the proposed algorithm requires the early stopping or not? If not, why?  \n\n2. The experiments are not conclusive.  The simple LSTM predictions beats the proposed algorithms in some cases.   Given there are only two datasets, it's hard to say if the proposed algorithm is really better. Also, the author proposed so many combinations, it is also not clear which one should be favored based on the table. \n\n3. Many related works are missing.  In addition to Chang el al (2019) mentioned above, there are many related works of using GANs in time series detection problem, e.g. BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series, and many others. \n\n\nTo summarize, the motivation and the advantage of using GANs is not well justified in addition to experiments.  Also, the authors only study two datasets, and the results are not very conclusive. "}