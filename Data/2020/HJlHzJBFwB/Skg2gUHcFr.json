{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Overall I liked several results presented in this paper. The findings in Figure 2 gives clear illustration on how Bayesian classification models distinguish between in-distribution difficult-to-classify data and out-of-distribution data, namely uncertain predicted mean and large predicted variance. Though I believe this eventually depends on what kind of \"kernel\"s are used to correlate data points in the prior, throughout the paper I assume meaningful \"kernel\"s are used (for Bayesian NNs this is rooted in the inductive bias of neural networks). \n\nAnother result that I liked is in experiments we can clearly see the advantage of considering the bayesian predictive distribution over a single predictive mean. As demonstrated by BDK-SGLD vs. BDK-DIR-SGLD. \n\nThe proposed idea is a simple and meaningful improvement over previous works. Though the contribution is quite limited, the authors present it with great clarity, which I appreciated. However, I do find some discussions in the paper unnecessary and would expect for more technical contributions. For example, I didn't see the argument for the whole section discussing amortization gap. Everything seems straightforward given the hypothesis F is of enough capacity, which obviously does not hold in practice.\n\nMany other concerns are summarized below:\n* What if the teacher predictive distribution is far unlike a Dirichlet? How much is the discrepancy between teacher and student predictive distribution? Theoretical or empirical evidence is needed for this modeling choice.\n* One importance advantage of Bayesian classification models is that they can capture the covariance between predictions  of different data points. By amortization this advantage no longer exists.\n* In the paper the authors keep mentioning that the method can be applied to GPs but I don't see experiments or algorithms for it?\n* The concentration model is parameterized using an exponential activation, how does this activation affect the performance?\n* The distilling process is done on a held-out dataset. Which may not be wanted because an advantage of Bayesian classification models (eg. GPs) is that all hyperparameters can be automatically selected by marginal likelihoods and don't need a held-out validation set.\n* MMD/wasserstein distances are cool but they require also samples from the student, which adds more variance to the distillation process.\n* The experiment setup is extremely unclear to me. What is \"uncertainty measures\", are they used as metrics for detecting out-of-distribution data, how are AUROC/AUPR calculated using the uncertainty measures? I can guess the meaning but the paper should be more clear about this.\n* I found most numbers convincing except that sometimes BDK-SGLD outperforms BDK-DIR-SGLD, if I understand it right, the predicted mean of BDK-DIR-SGLD should be as good as BDK-SGLD?\n\nMinor:\n* On page 4, above Eq. (4) there is a broken figure link.\n* On Page 7, \"To save space, we only present the best performing uncertainty measure (E, P or C)\". What is \"C\" here?"}