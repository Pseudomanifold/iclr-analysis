{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Bayesian models maintain the posterior distribution for predictions, which might bring up big computational costs of multiple forwards or big memory costs of multiple particles. To resolve the computational and memory issues at predictions, this paper proposes to distill Bayesian models into an amortized prediction model, avoiding the original multiple forwards. Specifically, in classification, they distill the predictive probabilities into an amortized Dirichlet distribution. They evaluated different distillation metrics, including KL divergence, Earth moving distance, and Maximum mean discrepancy.  Empirically, they evaluate the proposed method over out-of-distribution detection. They demonstrate that their method achieves comparable performance with much speedup.\n\nStrengths, \n1, This paper is well-written and the ideas are well-presented. They evaluated the proposed method over different Bayesian models (MCDP & SGLD) as well different metrics (KL, EMD, MMD), and demonstrate the effectiveness of their method. Overall, this paper is very comprehensive.\n2, As evaluated and validated in the experiments, the proposed method vastly reduces the inference time at test phase. \n\nWeakness,\n1, The paper kind of lacks of novelty. Basically the proposed method distills a Bayesian models into an amortized Dirichlet distribution, which is straightforward. \n2, The baselines such as MCDP-KL, MCDP-EMD are strange, it is wired why you would distill the predictive distribution of a single point to a Dirichlet distribution. And I think it is probably unfair, as distilling the single-point distribution to the Dirichlet under KL, EMD, MMD might require large amount of particles, which they don't have.\n3, Related to (2), more baselines should be compared with to better demonstrate the method's effectiveness. 1) performance of the un-distilled MCDP and SGLD models. 2) BDK and DPN for the MCDP models. 3) MCDP and SGLD with fewer particles. The paper claims to achieve 500x speed up, while I reckon the performance of MCDP and SGLD won't deteriorate a lot if you use only fewer particles. \n4, It would be interesting to see experiments other than out-of-distribution detection, such as calibration. \n\nMinor Issues,\n1, The paper has several un-complied references, such as above eq(4) and appendix D.\n2, The \\Tau(x | \\theta) in Figure 1 is a typo.\n3, Assumption 1 should be put forward to the main articles for comprehensiveness of Lemma 1."}