{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work presents a method for using generative models to gain insight into sensitive user data, while maintaining guarantees about the privacy of that data via differential privacy (DP) techniques. This scheme takes place in the federated learning (FL) setting, where the data in question remains on a local device and only aggregate updates are sent to a centralized server. The intended application here is to use the trained generative models as a substitute for direct inspection of user data, thus providing more tools for debugging and troubleshooting deployed models in a privacy conscious manner.\n\nPros:\nGiven the growing computational power of mobile devices and the importance of privacy for large-scale deployment of machine learning, this work is a timely contribution that could augment the ML pipeline for at-scale applications dealing with sensitive data. The authors do a good job of fleshing out the intended use cases of their training scheme, and present a pair of experiments that are well-chosen for illustrating the utility of generative models when dealing with private data.\n\nCons:\nAlthough likely of practical use, the work seems to be lacking in novelty in several respects. First, the techniques developed here represent a fairly straightforward merger of DP and FL tools without much in the way of qualitatively new offerings. While the authors do develop a new GAN training scheme that works in the FL setting, this adaptation is also pretty straightforward, and mostly follows the approach laid out in [1] for training recurrent neural nets.\n\nSecondly, this paper comes in the midst of many other works aiming to integrate different combinations of generative models, privacy, and distributed training (as pointed out in the related work section). While the particular combination of techniques here differ from those in previous work, the authors don't attempt to justify why their training scheme should be preferred over these prior methods. And although their experiments are useful for understanding the general utility of generative models trained in a private and decentralized setting, they unfortunately don't permit any direct comparison with the experiments used in these previous papers.\n\nVerdict:\nFor the reasons given above, I cannot recommend acceptance of this work.\n\n[1] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang, Learning differentially private recurrent language models, ICLR 2018\n"}