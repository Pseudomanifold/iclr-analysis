{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a feedback mechanism in the GAN framework which improves the quality of generated images in the context of image-to-image translation. The key contribution is that the discriminator not only predicts the probability of an image being real or fake, but also outputs a map which indicates where the generator should focus in the next iteration in order to make its results more convincing. The paper explores ways of obtaining such a map 1) by summing feature activations of the discriminator on a specific or group of layers  2) by predicting it via augmenting the capacity of the discriminator. After such a map is obtained, it is concatenated with the input image and fed iteratively to the generator. \n\nThe proposed setting have been tested on the setting of supervised and unsupervised image translation on 4 datasets. Quantitative experiments show that the proposed approach improves over other baselines. I think this paper introduces an interesting and important new GAN framework. However, I feel that the paper requires a major revision strengthening the experiments, before it can be reconsidered for ICLR: \n\nMore qualitative results showing comparisons with other algorithms should be shown for Day-Night, Apple-Orange, Horse-Zebra. The only comparison available in the paper is on the well constrained problem of segmentation maps to images.\nMore quantitative experiments should be provided for other datasets (perhaps using FID and KID). It is not entirely clear if the produced results are better than cycleGAN\u2019s (my subjective analysis is that the results of cycleGAM look better on Fig. 2).\nThe paper is closely related to [Huh et al: Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks] in that they share the idea of a feedback mechanism from the discriminator. It hence seems reasonable to compare with this approach.  \nI was struggling to understand precisely how the StarGAN results were obtained on CityScapes: As a multi-modal image-to-image translation model, StarGan takes as input, an image and a binary vector pointing to which modality to transform the image into. In the case of CityScape, there is no such multi-modality (at least none that is provided as ground truths, via for example, a binary vector). More details on this process would make the experimental section clearer. \nTable 4.1 is often used however such a Table does not exist (probably Table 1,2 was meant here).\n"}