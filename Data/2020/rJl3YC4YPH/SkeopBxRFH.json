{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "[Summary]\nThis paper proposes a GAN with an attention-based discriminator for I2I translation, GuideGAN. The proposed discriminator provides the probability of real /fake and an attention map which reflects the salience for image generation. They apply their method to CycleGAN. GuideGAN is evaluated on the Cityscape dataset, horse2zibra, apple2orange, and day2night, compared to UNIT, CycleGAN, StarGAN, cGAN, and pix2pix.\n\n[Pros]\n- Quantitative results\n\n[Cons]\n- Novelty is restricted. Even if there is no work using attention in the discriminator, it is hard to tell that the use of attention is not novel. \n- The used datasets are not challenging. Why the quantitative results are evaluated on the Cityscape? How about FID on other datasets such as CelebA and Summer2Winter, or higher-resolution data?\n- The quantitative scores are not impressive. The gaps look insignificant.\n- How large are additional parameters? and How long does it spend training compared to CycleGAN?\n- The authors describe two concatenation methods. How about the results of simple RGBA?\n- Why post hoc and RAM are evaluated on different datasets from each other in qualitative results?\n- User study will be helpful.\n- Basically, this architecture can be applied to various GANs. Do the authors have any result on other GANs?\n \n\n[Minor]\nIn Figure 1, M is used without the definition.\nIn row 2 of page 3, follows --> follow\n"}