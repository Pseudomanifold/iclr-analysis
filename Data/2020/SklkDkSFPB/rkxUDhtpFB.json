{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1) in the introduction, it's not clear what attention transfer means \n  2) why did they specifically choose these four blocks? \n  3) (3.1) substitute blocks $S$: ``each using $N$ lots of $N \\times k \\times k$ filters\" should it be ``$N$ number of $k \\times k$ filters\"? \n  4) (3.2) ``consider a choice of layers $i = 1, 2, \\dots, N_L$\" what is $L$ and why does the number of layers depend on $L$? \n  5)  (3.2) Why is that specific $f$ chosen? \n  6)  (3.3) They could elaborate more on why they chose Fisher potential instead of other metrics for architecture selection. Currently, they only provided the intuition of the metric. Since the paper suggests the Fisher potential is a crucial part of their method, they could provide more theoretical justification about this choice. \n  7) (4.0) How did they decide on the hyperparameters? \n  8) In the introduction, they suggest that the major novelty of their method ``assigns non-uniform importance to each block by cheapening them to different extents\", but in their method they only randomly assembled the mixed-blocktype models. \n  9) Section 4.1 suggests good mixed blocktype yields low error but they didn't address how good mixed blocktype can be found "}