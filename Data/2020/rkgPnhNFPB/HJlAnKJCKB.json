{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors claim to establish a Lipschitz bound on neural networks, initialized randomly, and trained until convergence.  They also claim to establish probabilistic concentration of the resolvent of the Gram matrix from a mixture of k distributions with varying means and covariances.  The authors also present the spectrum and leading eigenspace of the Gram matrix for representations by CNNs of images generated by a GAN.\n\nIt is not clear to this reviewer what exactly the authors have proved and what significance it has.  For example, in Proposition 3.1, the authors talk about dynamics with a learning rate, where W <- W - \\eta E where E has standard normal entries and \\eta is a learning rate.  It is unclear what the learning problem is, or why the learning rate is modifying a random modification.  I understand the general idea that Lipschitz functions of random-quantities-that-enjoy-concentration-estimates also enjoy concentration estimates.  This is well known from the random matrix theory literature.  The authors need to make a claim about how this informs either the development or understanding of deep learning technologies.  The authors should consider what the implications of their results could be, and then do the research to establish that those implications hold up.\n\nThe authors claim that their results constitute \"a first step towards the theoretical understanding of complex objects such as DL representations.\"  This claim is false.  This very conference is on Learning Representations, and presumably at least one paper in the past 7 years makes progress towards the theoretical understanding of Deep Learning representations.  \n\nBecause of the overly bold claims with insufficient clarity and justification, I recommend that this paper be rejected."}