{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary of the paper's contributions:\n\nThis paper introduces two new notions of robustness for randomized classifiers, which are based on the notions of differential privacy (DP) of randomized mechanisms. Specifically, the D_\\infty robustness and D_{MR} robustness of a random classifier are defined based on \\epsilon-DP and \\epsilon-zCDP, respectively.\n\nThe paper proves lower bounds on the noise level of a random classifier for which it can be certified D_\\infty robust and D_{MR} robust. Further, it is shown that the lower bounds are achieved by random classifiers constructed using Gaussian noise and exponential noise for l_2 and l_\\infty robustness, respectively.\n\nMajor criticisms: (1) The paper does not give sufficient motivation for studying D_\\infty robustness and D_{MR} robustness. (2) The paper makes several unsubstantiated claims regarding the optimality of different noise models for adversarial robustness.\n\nDetailed comments:\n\n- All the claims made in the paper regarding the optimality of different noise models  are specific to D_\\infty and D_{MR} robustness. However, they are written in a way to imply that the claims also hold for the standard l_2/l_\\infty robustness which is studied in adversarial ML literature (especially in the abstract and intro section). The authors should make clear the relation between D_\\infty and D_{MR} robustness and the standard notion of robustness of a classifier. Does one imply the other?\n\n- Does the relaxed notion of robustness of a random classifier g (Definition 3) imply a robustness guarantee for the final output of the random classifier, i.e., \\argmax_c P(g(x) = c) ?\n\n- The experiments use the same setup as in Cohen et al, but the results are not compared with those in Cohen et al. It is not clear how to judge the significance of these results without comparison to any other method of evaluating robustness.\n\n- \"However, it is known that adding Gaussian noise often does not lead to \\epsilon-DP, but rather (\\epsilon; \\delta)-DP (Dwork et al., 2014) which has an additional parameter \\delta and thus is harder to be incorporated in our framework. To alleviate this issue, we employ Maximal Relative Renyi Divergence as the probability distance measurement to define another type of robustness, namely D_{MR} robustness.\" - This does not provide sufficient justification for studying D_{MR} robustness.\n\n- A comparison is made between Theorem 10 & Corollary 11 in the paper to Theorem 1 in Cohen et al. However, it is not clear how the result in the paper is better or even equivalent to the one in Cohen et al. D_{MR} robustness seems to be an approximate notion of robustness, while the result in Cohen et al gives perfect robustness within a ball of a certain radius. The radius r in both papers scales linearly with \\sigma. It is said that \"a smaller c yields a larger r compared to Cohen et al.\" It is not clear why that is useful.\n\n- In Theorem 12, each entry in x is restricted to be in the range [0, r/\\sqrt{d}]. This means the l_2 norm of x cannot be more than r. Then, how is it meaningful to discuss the (2r, D_{MR}, l_2, \\epsilon/2) robustness of an algorithm on this data, with the radius of the robust guarantee being 2r?\n\n- In Theorem 12, the lower bound on the expected l_\\infty norm of the random noise is shown to be independent of d, while the expected l_\\infty norm for Gaussian noise scales with \\sqrt{\\log d}. I don't think it is correct to claim that Gaussian noise is \"near\" optimal from this analysis.\n\n-  In Definition 23, it is not clear what Loss(Y||Y') is.\n\nSuggestions for improvement:\n\n- The authors should make it clear in the abstract that the optimality of the noise models is with regards to the newly defined notions of robustness.\n\n- It would be worthwhile to discuss how D_\\infty and D_{MR} robustness differ from standard notions of minimax robustness.\n\n- One possible way to motivate the relaxed robustness introduced in Definition 3 is to link it to the robustness of the randomized classifier in Definition 1.\n\n- Please consider using \\left( \\right) instead of ( ).\n\n- For experiments, it would help to compare D_\\infty and D_{MR} robustness alongside the standard l_2 robustness. In addition to training the network on Gaussian augmented dataset, it might be worthwhile to compare it to other baseline approaches as done in Cohen et al."}