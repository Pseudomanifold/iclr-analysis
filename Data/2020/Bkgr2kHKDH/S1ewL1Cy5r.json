{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to perform both neural architecture search and pruning+quantization under the same evolutionary search process. However, as it is computationally expensive to quantize and fine-tune a model and then measure the resulting accuracy, the authors propose to use a machine-learned predictor as a surrogate. \n\nI have two main concerns with this paper: \n\nFirst, I think the method is not really am end-to-end joint optimization approach as motivated in Eq 4. By using a machine-learned predictor to approximate the accuracy, there are no guarantees in this method. It is misleading to call it end-to-end joint optimization. \n\nSecond, I think more details are needed to convince me to trust the results; currently I do not know if all the comparisons are fair. In particular:\n\n(a) It is difficult to discern how much computation is needed for this machine-learned predictor. The use of transfer as illustrated in Fig. 2 is a reasonable idea, but in practice how do we know if this predictor is good enough? How many samples do you actually used in the experiments, and how do you kow that number is sufficient? \n\n(b) The the paper makes claims that need to be backed up, e.g. in pg. 6: \"In fact, we find that 80k data pairs is a suitable size to train a good full precision accuracy predictor.\" Is this referring to this dataset, or in general? \n\n(c) Table 2 compares many methods, but it is not clear if they are apples-to-apples comparison, esp. in terms of search space and resource constraints. It is also not clear if all the computation is reported in the Search Cost and CO2 columns. \n\n(d) Sec 5.1 says \"Our small model (Ours-B) can have 2.2% accuracy boost than mixed-precision MobileNetV2 search by HAQ (from 71.9% to 74.1%); our large model (Ours-C) attains better accuracy (from 74.6% to 75.1%) while only requires half of BitOps. How do you define the small model and the large model? Were they just two models that handpicked from the final population of evolutionary search? What criteria is used to pick them? \n\n(e) Figure 6 measure predictor accuracy, which is extremely pertinent. But it is not clear how the data (train and test) were created. Was this based on generating the quantized dataset, which is expensive? \n\n\n"}