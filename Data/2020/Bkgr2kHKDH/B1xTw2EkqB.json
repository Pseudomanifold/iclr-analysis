{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims at optimizing the process of automatically searching for network architectures. It starts by proposing three parts to the process, of model search, connection pruning and weight quantization. The claim is then presented that executing the three in sequence is sub-optimal, as each step independently de-centers the previous. The proposed solution is then to integrate the three steps in a single end-to-end method, improving performance. While this is in principle interesting, the method proposed depends on strict experimental conditions which limit broader application.\nI would like to highlight the following points:\n- The paper is both unnecessarily and extraordinarily complicated. The core idea is simple and in itself actually elegant, but I find myself unable to suggest a simple set of edits which would bring this paper into publishable state.\n- The \"super network\" at the core of the proposed method acts as an upper bound of the complexity that can be found. The architecture search mode is then of ablation, selecting and reducing parts until an acceptable solution is found. This may work on ImageNet starting from knowingly oversized structures, but attacking an unknown problem would require a significant amount of trials, effectively limiting performance and efficacy.\n- The literature review is excessively technical, surprising as such a comment may sound. The role of such a section is typically to introduce the already-published foundation required to understand the standing of the presented work. In this case instead it lists a broad selection of very similar work, with each corresponding minute difference, which requires the establishment of an expertise in the specific subfield only to be able to evaluate the scale of effective contribution. That an extremely high percentage of the cited work has not seen peer-reviewed publication greatly limit the reader's interested into such an investment.\n- The claims presented often sound overinflated and purposely generic (super-fast speed, orders of magnitude), and lack statistical significance. An attentive read highlights a careful selection of performance metrics, axis scales in the figures, and relatively minor contributions under most metrics. This work does not improve orders of magnitude over SinglePathNAS (admittedly the inspiration of this work) under any metric, cost included. Such selling techniques are often frown upon at top conferences such as ICLR."}