{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Notes: \n\n  -This paper suggests the use of VAEs with stronger priors along with more powerful regularization of the decoder (especially its curvature).  This lowering of curvature is seen as \"flattening\".  \n\n  -Paper uses the normal VAE ELBO.  \n\n  -The normal prior over-regularizes the approximate posterior.  One proposal is to use a \"hierarchical prior\": integral(p(z|zeta)*p(zeta), zeta) where zeta is a normal distribution.  So basically a function can transform the prior.  \n\n  -Importance weighting with q(z|x) has been proposed as a way to define a valid learning objective for this setting.  \n\n  -Another objective using lagrangian is called \"VHP-VAE\".  \n\n  -This paper extends VHP-VAE with jacobian regularization, which is approximated (the paper doesn't say so but I think it's a first order taylor expansion).  \n\n  -Paper also uses mixup in the latent space to provide regularization at points farther from the data.  \n\n  -With this mixup objective the mixing is also done to consider extrapolations in addition to interpolations.  \n\n  -The resulting latent space does indeed look much better (Figure 1).  \n\n  -The condition number is also way better (2a, 2b).  \n\n  -In figure 2, the background color indicates the degree of magnification (so the VAE-VHP has greater variability in distances?)  I found this figure a bit hard to itnerpret.  \n\nComments: \n\n  -This paper cites Mixup but there are two more papers to consider here: Manifold Mixup (ICML 2019) and Adversarial Mixup Resynthesis (Neurips 2019) which both considered mixing in a latent space.  AMR considered in an autoencoder, and Manifold Mixup is also relevant because its theoretical analysis explicitly considers flattening although in a somewhat difference sense (and both are different from what's done here).  \n\n  -The object tracking experiments don't seem very convincing to me (just looking at table 2 at least).  \n\nReview: \n\n  This paper considers augmenting the hierarchical VHP-VAE with a criteria in which the jacobian is approximately regularized at interpolations and extrapolations between different points in z space.  The experiments suggest this is an important problem with VHP-VAE and also that it's successfully addressed.  "}