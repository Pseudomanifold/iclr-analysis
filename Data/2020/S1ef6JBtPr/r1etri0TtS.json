{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a new algorithm named Multi-Agent Soft Actor-Critic (MA-SAC) based on the off-policy maximum-entropy actor critic algorithm Soft Actor-Critic (SAC). Based on variational inference framework, the authors derive the objectives for multi-agent reinforcement learning. In experiments section, the authors compare the proposed algorithm with the previous algorithm called Multi-Agent Deep Deterministic Policy Gradient (MADDPG) on several multi-agent domain.\n\nComments:\n- Based on inference, the authors derive the objectives as the equation (8) and (9). However, the proposed objectives are almost identical to SAC. First, the objectives for Q functions are just replacing \\hat{Q} in the equation (7) in SAC by \\bar{Q}, which has a very similar meaning. Also, the objectives for policy \\pi are exactly the same as in the SAC with only the added index. Thus, the proposed algorithm seems to be a naive extension of SAC into multi-agent cases. To avoid such questions, authors need to emphasize the difference from simple extension.\n- Is there a reason not to use additional neural networks to estimate value function like SAC even the proposed algorithm is based on SAC?\n- Additional experimental results are needed to ensure the algorithm since there is no theoretical guarantee. "}