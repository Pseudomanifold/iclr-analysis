{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes Generative Adversarial MDP Alignment (GAMA) for imitation learning. Given a set of paired MDPs, GAMA learns a state mapping f and an action mapping g such that one MDP can be reduced to another. For a new test MDP pair (x,y) where expert demonstrations are available for y, GAMA can use f to map a state of x to a state of y, mimic the expert behavior, then use g to map the expert action back to an action in x. The reduction is theoretically motivated, the optimization is based on adversarial learning and finally, experiments on common Gym environments show that GAMA can achieve effective transfer.\n\nPros\n- The writing is great and easy to follow\n- The method is theoretically motivated\n- Experiments prove effective\n\nCons\n- The proposed method may not work well for complicated environments\n\n(1) In the discussion after Def.4, given an alignment task set D_{x,y}, how do we know whether a common (w.r.t. all MDP pairs) reduction exists? In other words, how do we know that the MDP pairs are from the same equivalent class (joint alignable) in practice?\n\n(2) The alignment needs to learn a lot of components: state mapping f, action mapping g, domain dynamics P^x. The domain dynamics can be difficult to learn for complicated environments, which may jeopardize the learning of f and g as a result because they depend on the accuracy of the learned dynamics. The experiment only uses the hidden representation for the image as input. Such lower-dimensional representation is not always available.\n\n(3) Experiment:\n- What happens for the UMA in Fig.4 top-right?\n- Table 1 only has the results of three alignment tasks. How about the rest?\n- What are the error bars in Table 1 (and also Fig.5)? Based on how many runs?\n\nMinors:\n- It is more common to use \"state-action pair\" instead of \"state, action pair\".\n- Sec.6.1, task task exemplify"}