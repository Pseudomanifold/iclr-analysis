{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper looks into the problem of training agents that can interact with their environments to verify hypotheses about it. It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. The reward depends on how correct the agent's prediction is. A second formulation uses MDP to explore the states and has a special action (Answer), which predicts the validity of the hypothesis based on the last sequence of N states visited. This is one side of the problem. The authors carry out such experiments and conclude that this doesn't work. \n\nThen, the authors exploit the structure of some hypotheses (such as triplet hypotheses of the form pre_condition, action_sequence, post_condition), which are easier to test. They conclude that taking this structure into account helps. \n\nOverall, the paper is well-written and the literature review section is quite excellent. However, I have reservations against the formulations that the authors used. I would appreciate it if the authors present their argument in the rebuttal. \n\nFirst, in the plain formulation of MDP, a policy produces an action according to the current state only. The authors add (Answer_True, and Answer_False) to the list of actions in MDP. So, if the agent is trained on some hypotheses, the agent will essentially learn to identify for each h which state s that can be used to to verify h (either prove or disprove it). To me, this is essentially memorization, and the agent cannot learn to predict the validity of new hypotheses. So, it seems that formulating the problem using MDP is not reasonable to begin with. \n\nSecond, when the agent exploits the structure of the hypotheses, the problem becomes nearly trivial. It would have been interesting if, somehow, the agent learned the strategy of trying to alter the preconditions or postconditions on its own, but this is not the case in the paper. The formulation essentially tells the agent that it should alter the preconditions and postconditions so that we have enough information about the validity of h that can be fed into a prediction network. I think that the fact this works is not that interesting. \n\nSome minor comments:\n- I suggest that all acronyms be defined in the paper before they are used. \n- In the reward functions, why did the authors use C instead of just using 1. \n- In Page 4, \"The agent is is spawned\" has a typo. \n- In Page 5, \"so we can in principal only\" has a typo. \n- In Page 7, \"as it paves the towards\" has a typo. \n\n"}