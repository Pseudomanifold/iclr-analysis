{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a certifiable NN training method, \"polyhedral envelope\nregularization\" (PER) for defending against adversarial examples.  The defense\nis based on the same linear relaxation based outer bounds of neural networks\n(KW/CROWN) used in many previous works.  The paper makes a few new (but small)\ntechnical contributions:\n\n1. this paper uses a different loss function (7), which is essentially Hinge\nloss on the lower bounds of distance to decision boundary. Previous works like\nKW used cross-entropy loss on the lower bound of prediction margin instead,\nwhich was based on minimax robust optimization theory. But I am not fully\nconvinced if the new loss function is better or not.\n\n2. in (5), the authors solve the bounded input case more carefully than\nprevious works.  (5) is trivial to solve in the L infinity case and has been\nused in previous works like (Wong & Kolter 2018, Gowal et al., 2018 and Zhang\net al., 2019); but solving it for other norms requires some efforts, and this\npaper proposes a good solution for it (Algorithm 2);\n\n3. In previous works like KW/CROWN, to find the largest certifiable radius, a\nbinary search is needed. The authors proposes a very small improvement to the\nbinary search process by setting the lower bound of search to the largest\nepsilon that is certifiable using the current linear relaxations obtained from\na larger epsilon.\n\nThe authors does not improve any bounds proposed in KW/CROWN, and they reuse\nthe same bounds. I see the main contribution as the new hinge-like loss\nfunction for training, and a more careful procedure to find the largest\ncertifiable radius in bounded input case.\n\nEmpirically, the improvement of the proposed algorithm is limited - based on\nTable 1 it is hard to say if PER is better than KW or not. PER+at outperforms\nKW sometimes, however it is not a completely fair comparison, as we can add a\nPGD based adversarial training loss to KW as well, as done in DiffAI (Mirman et\nal., https://github.com/eth-sri/diffai).\n\nQuestions:\n\n1. In my personal experience I usually found Hinge loss not as effective as\ncross-entropy loss in deep learning based tasks probably due to its\nnon-smoothness. The claim that (7) is better than cross-entropy loss is that it\ndoes not overregularize the network. The authors should provide more evidence\nto show if this argument holds, e.g., plotting the norm of weight matrices\nduring the training for the two losses to show that it can reduce\noverregularization.\n\n2. I think the metric ACB KW and ACB CRO (average certified radius of KW/CROWN)\nin Table 1 and 2 are confusing and not fair. In KW and CROWN's evaluation,\ngiving an epsilon, if an example cannot be certified due to epsilon to large\n(i.e., ||A|| \\epsilon + b > 0), certifiable radius will be count as 0 (flat\nline in Figure 1(a)). In this paper, the authors instead in this case use -b /\n||A|| as the certifiable radius. This is merely a different way of evaluation,\nand I don't see this as a contribution, as the \"improvement\" does not come from\na tighter bound.  In the same sense, I don't think Figure 1(a) and the\ndiscussions on page 3 are appropriate characterization of KW/CROWN. PEC uses\nexactly the same linear bounds as in KW/CROWN, and has the same certification\npower.\n\n3. For L2 based perturbations, in Table 1, the epsilon used for MNIST is too\nsmall. It is better to use an epsilon that is aligned with previous works. For\nexample in Wong et al., 2018 (https://arxiv.org/pdf/1805.12514.pdf), page 22,\nyou will find the epsilon used for MNIST and CIFAR.\n\n4. As discussed above, it is probably not fair to compare PER+at with KW. A new\nbaseline like KW+at should also be considered.\n\n5. For norms other than L infinity norms, solving (5) for getting $d$ can be\ntime consuming (Algorithm 2). How much additional time does it comparing\nto KW?\n\nOverall, I cannot recommend accepting this paper due to its limited theoretical\ncontribution as well as unconvincing empirical results comparing to previous\nmethods. I suggest rephrasing some parts of the paper and providing more\nexperimental results as discussed above.\n"}