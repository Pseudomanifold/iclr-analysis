{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach for computing more refined estimates of robustness in comparison w/ existing linear approximation approaches that only give a yes or no answer with regard to robustness guarantees for a given lp-norm ball with radius epsilon. The nice thing is that as the linear-approximations get better, the contributions in this paper would continue to help. \n\nThe paper makes two key algorithmic/theoretical contributions:\n1. An approach to obtain a better estimate of the radius of the l-p ball where the NN is provably robust. This result is fairly straightforward, and relies on computing the distance of a point to the boundary of adversarial polytope.\n\n2. An approach to exploit the fact that the pixel values are restricted to specific bounds, which might allow us trim away some regions from the l-p norm balls around a given input image w.r.t which we want to be robust, while computing the robustness. This I think is a more interesting contribution. \n\n\nI am leaning towards a reject, however I am open to changing my score. I have several key concerns:\n\n1. Verified Training: Why is there no comparison with IBP and IBP+Crown (Zhang 2019) -- it seems like an appropriate comparison to make. Particularly, when the current paper refers and discusses both of the above works. \n\n2. I am not sure that comparison with CRO entirely suffices in my opinion. Would it be possible to compare with the tighter SDP based approaches (Raghunathan et al., NeurIPS'19 and Dvijotham et al., UAI'19)? Is there a specific reason to not compare (other than that the SDP based approach is not a linear approximation, and probably is much slower)? \n\nMy main concern here is the utility of pushing boundaries with the linear approximation, while there are potentially tighter relaxations?\n\n3. You claim no overhead compared to CROWN. Don't the greedy-optimization steps add some overhead, or am I missing something? How expensive are they? (It's possible I might have missed some discussion in this regard. If so, please point me in the right direction and that should suffice)\n\n4. Can you plot the distributions of the certified epsilon? Are there a few samples for which you can certify a much larger epsilon (than just saying not robust) or are there a lot of samples where you can only show a tiny bit of robustness (compared to CROWN saying not robust)? \n\nThe gains in the average robustness are somewhat small, and these gains alone are not convincing without being able to see how these gains were obtained. \n\nMinor Comment:\nMissing reference to MixTrain for B' < B helps. \n"}