{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose SLAC, an important extension of the recently introduced soft actor critic (SAC) algorthm, which operates on a learned latent state, rather than an observed one, and therefore aims to jointly learn to represent high dimensional inputs and execute continuous control based on this representation. \n\nSLAC is based on a dynamic, non-linear Markov generative model and incorporates structured variational inference to learn the latent state distribution. The generative model and amortized variational inference support the learning of strong latent expected future reward estimates (Q functions that condition on the latent state), which the policy, which conditions directly on the observations (i.e. image) is distilled against for fast inference. The paper demonstrates solid gains over existing techniques, brings together recent work under a rigorous framework, and is a pleasure to read.\n\nStrengths:\n-Novel formulation, SOTA results, well written.\n\nLimitations:\n-While the most important ablation, the role of making the primary latent variable stochastic, is investigated, a deeper investigation of what makes the model more effective than existing techniques would be insightful, and further strengthen the paper.\n-Related, the approach seems closest to PlaNet in structure, but rather than being used for planning, is executed directly as an off-policy actor-critic algorithm, generalizing SAC. A discussion, and possibly some additional experiments to explain the differences and understand the tradeoffs would strengthen the paper. The authors mention \", contrary to the conclusions in prior work (Hafner et al., 2019; Buesing et al., 2018), the fully stochastic model performs on par or better.\" Why?\n\nMinor:\n-Figure 6 partially stochastic in figure, mixed in text.\n\nOverall:\nA strong paper, that brings together and generalizes existing work, with strong experimentation and SOTA results. Definite accept.\n"}