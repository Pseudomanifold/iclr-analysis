{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper explores fast normalization schemes for bilinear pooling in deep neural networks. The scheme basically approximates an SVD through K iterations of the power method, applied to a random vector. The performance is compared, in terms of computational time and accuracy degradation, with max pooling, sum pooling, and other SVD-based normalization schemes in bilinear pooling. \n\nI am not too familiar with this area, so I cannot speak to whether this idea is novel or whether the practice of bilinear pooling is widely used; however, it seems from the experiment that the idea works well and can greatly increase performance in smaller neural nets (VGG is used) on complex tasks. Additionally, using the power method to approximate an SVD makes sense, and the tests seem fairly extensive. \n\nOne comment is that I find the method seems a bit complicated in description. Why is the expectation subtracted? Why are so many steps needed after the power method is completed? Additionally, the details for the back propagation seems not very relevant to the method idea, and could be moved to the appendix, with just a description on computational complexity in the main text. While I believe that each step was important, I think the motivation behind each step should be emphasized over the mechanics, which are a bit convoluted. \n\nOtherwise, I think the paper is a nice contribution."}