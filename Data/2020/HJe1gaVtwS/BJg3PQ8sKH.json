{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a rank-1 update normalization (RUN), which supports the normalization on compact bilinear features and feasible to be plugged into end-to-end training. RUN uses power method to estimate the bilinear matrix, which is more friendly to GPU. The idea of RUN is well motivated but not surprising. The experimental results show RUN achieves comparable accuracies with much lower running time than NS iteration and SVD-based normalization. \n\nThe presentation of this paper should be improved. It is necessary to summarize the procedure of RUN like Algorithm 1 and 2. The main results in Section 3 are desired to be presented by Theorem or Proposition. Some detailed derivation could be deferred into appendix.\n\nSome questions:\n\n1. It is unclear how to obtain (16) from previous derivation. Where does \\epsilon come from?\n\n2. Can you provide non-asymptotic results of (18) and (21) (which may be rely on the eigengap)? \n\n3. It would like to plot how the accuracy of each algorithms varying with epochs and running time. \n\nMinor comments:\n\n1. Should v_i in section 3 be standard normal distribution?\n\n2. In the sentence between (11) and (12): normalization distribution -> normal distribution.\n"}