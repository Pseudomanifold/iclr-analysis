{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors consider a bilinear matrix normalized network and how to implement the bilinear matrix normalization, which uses SVD, in a manner that allows end-to-end training. The key idea used is to use the fact that singular values of a matrix can be found via the power method, which requires repeatedly multiplying a matrix with a vector and normalizing the resulting product.  Experiments are demonstrated on some multi-class datasets of fairly small size. The writing could have been better and i found the exposition somewhat hard to follow. Here are  a few comments and questions.\n\n1. It was hard to follow the RUN procedure. I was able to figure out the use of power method, but it was not clear to me what the key result of Section 3 is? I would suggest that the authors rework the exposition in this section to clearly state what the main result is.\n\n2. The latter half of Section 3 shows the derivatives w.r.t. F. I appreciate the authors working through these calculations, but I do not see why it is important. It was not clear to me what the authors wanted to express via the calculation of the derivative.\n\n3. The datasets used for experimentation are rather small. Why is this the case? What is the limiting factor?\n\nI like the idea of using power iterations and exposing the power iterations as a sequence of layers. However, the exposition needs a lot to be desired."}