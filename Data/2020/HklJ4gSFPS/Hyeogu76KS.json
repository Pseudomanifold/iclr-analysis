{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The paper proposes a model of \"inference-driven representation learning\" that is inspired by \"neural encoding in the biological systems\". This boils down to training a model to learn representations that are suited for a given task (and not general-purpose representations as an auto-encoder would do).\n\nThe paper is sparse on a lot of details - about the datasets, about the proposed approach, and the experimental results. There is no quantitative comparison with any baselines and many results are described in vague terms without explicit quantitative results (for example, \"Performance of the model did not change significantly\"). In the current form, it is extremely difficult to decide if the paper makes any meaningful contributions.\n\n=============\n\nFollowing are the main concerns:\n\n* In absence of any baseline, the empirical results are meaningless. For example, the overall performance of 98% does not mean much aa we do not have any estimate of how hard the tasks are. Moreover many results are described vaguely (pointed out above).\n\n* The dataset should be described in detail. Even information about the dataset size is missing. Given that the dataset is procedurally generated, it would be useful to consider some meaningful ablations which could be hard to perform with real datasets. For example, If the reward distribution is changed during training (for the 2 arm bandit), how quickly can the model adapt to the new distribution?\n\n* The model description is lacking a lot of details. For example, how is the problem statement (P) \"encoded\"? How is the problem represented before feeding to the model?\n\n* Ingesting the dataset sequentially imposes a dependence on the order in which the data is presented. Now it is no longer just a representation learning problem, the model has to learn long-term dependencies as well (when propagating gradients). How is this handled properly (in the context of issues like vanishing and exploding gradients)?\n\n* In the first paragraph of the introduction, the statement \"propose one possible way...unlabelled data\" is somewhat misleading. It suggests that the representations can be learned without needing any task-specific labels which is not the case. The authors here use \"labels\" to refer to the underlying regularity in the data. It is quite common for the learning algorithms to not assume access to this kind of information.\n\n\n=============\n\n\nFollowing are the things that could be improved in the future version of the paper but did not affect the score:\n\n* Figure 1 does not seem to be cited anywhere.\n\n* On page 5, line 2, it seems that Figure 2 is cited by mistake.\n\n* Words are missing at certain places (eg \"representations that implicitly shaped by\").\n\n"}