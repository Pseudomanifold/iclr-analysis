{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper presents an analysis and numerical evaluation of SGD with momentum for non-convex optimization problems. In particular, the main contribution of the paper, as the authors claim in the abstract, is showing that stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster. \n\nComments:\nSGD with heavy ball momentum (or stochastic heavy ball method) is one of the most popular methods for training neural networks. As the authors mentioned the method is widely employed in practice, especially in the area of deep learning. However, despite the popularity of the method both in convex and non-convex optimization, its convergence properties are not very well understood.  \nI consider any meaningful direction for understanding the convergence of this method a nice fit to ICLR, however i find the presentation of this paper somehow confusing, especially Section 3 which is supposed to be the main contribution of the paper. \n\n1) I understand the motivation of the authors and what they tried to communicate but i find that there \nis no satisfactory explanation of what Theorem 1 is actually saying. For example In case that $\\beta=0$ the method is the popular stochastic gradient descent method. Does the theorem covers the known results of SGD appeared in previous works? \n\n2) In addition, the theorem states\" If SGD with momentum (Algorithm 2) has ....APAG...APCG_T...GrACE ...\". How we can guarantee that the above 3 conditions is satisfied from the Algorithm 2? There is also no satisfactory explanation of why the authors analyze Algorithm 2 and not Algorithm 1. \n\n3) The presentation of Section 3.2.1 is also not clear. I am suggesting to the authors to explain in more details the theoretical results of their paper and highlight why the 5 lemmas of this section are important to be in the main part of the paper. I think a notation subsection will be useful for the reader.\n\n4) I am suggesting the authors to include in the introduction the more known variant of SGD with momentum:\n$$\\omega_{t+1}=\\omega_t-\\eta \\nabla f(\\omega_t, \\xi_t)+\\beta(\\omega_t-\\omega_{t-1})$$\n5) The authors name their methods \"SGD with stochastic momentum\". The method is either \"SGD with momentum\" or \"Stochastic heavy ball method\". SGD with stochastic momentum is something different (see for example [Loizou, Richtarik 2017] from paper's reference section)\n\nMinor Comment:\npage 3, bellow eq(4): the first (3) ----> Problem (3)\n\nMissing references:\nThe authors did an excellent work on reviewing the literature review on SGD with momentum. Bellow find three recent related works that could be mentioned:\n\nAybat, Necdet Serhat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. \"A universally optimal multistage accelerated stochastic gradient method.\" arXiv preprint arXiv:1901.08022 (2019).\nLoizou, Nicolas, and Peter Richt\u00e1rik. \"Linearly convergent stochastic heavy ball method for minimizing generalization error.\" arXiv preprint arXiv:1710.10737 (2017).\nLoizou, Nicolas, and Peter Richt\u00e1rik. \"Accelerated gossip via stochastic heavy ball method.\" In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 927-934. IEEE, 2018.\n"}