{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper provide an extensive review of current advances in uncertainty estimation in neural networks with the analysis of drawbacks of currently used uncertainty metrics and comparison on scale the recent method to estimate uncertainty. The paper covers a lot of uncertainty metrics and a wide range of methods. The paper focuses on in-domain uncertainty estimation complementing the recent similar review on out-of-domain uncertainty estimation.\n\nIt seems that the paper provides the analysis missing in the current literature. Whereas as mentioned Yukun Ding in a public comment there is a related work on identifying issues with popular uncertainty metrics, the mentioned paper is missing the through comparison of the methods for estimating uncertainty. \n\nSuch kind of thorough analysis (especially performed on scale on large datasets) and comparison is of obvious interest to the community as well as objective comparison of the current state-of-the-art. \n\nThe paper is clearly written and easy to follow.\n\nBased on this, I believe this is a strong technical paper and it should be accepted. However, the analysis in the paper is not overwhelmingly exhaustive. Some of the arguments on that are listed below.\n\nBelow is the list of comments/thoughts for potential improvement of the paper:\n1.\t\u201cIn this case, a model is expected to provide correct probability estimates:\u201d \u2013 may be not the best choice of words, because for out of domain uncertainty estimation we still expect a model to provide correct probability estimates\n2.\tThe first paragraph on page 3 seems to better fit in Section 2, for example, on the very beginning of Section 2.\n3.\t\u201cComparison of the log-likelihood should only be performed at the optimal temperature.\u201d and others alike \u2013 personally, I do not support this kind of formatting for a scientific paper\n4.\t\u201ccan produce an arbitrary ranking of different methods. <\u2026> Empirically,\u201d \u2013 in the current form it seems that the first statement is somehow theoretically justified and then additionally it is confirmed empirically in this paper. I believe that the authors use empirical observation itself as the justification of the first statement, if that the case it should be reworded here. For example, \u201ccan produce an arbitrary ranking of different methods as we show below/ as we show empirically. We demonstrate that the overall \u2026\u201d If my belief is incorrect and there are other grounds that justify the first statement that it is required a reference after this statement.\n5.\tItalic and non-italic LL usage is unclear\n6.\tHaving \u201cBrier score\u201d emphasised as a paragraph, it seems that there should be a paragraph log-likelihood as well\n7.\t\u201cIn that case, both objects and targets of induced binary classification problems remain fixed for all models\u201d \u2013 do the authors consider in this case all out-of-domain objects as having a positive class and all in-domain objects as having a negative class? Because the models are still going to make individual misclassification mistakes\n8.\tFigure 2 \u2013 legend occupies too much space of the plot occluding almost a third part of the plot. Maybe taking the legend out of the plot to the right and squeezing the plot to make a room for the legend would be a better solution\n9.\tIn eq. (4) and (5) subscript DE is not defined\n10.\t\u201cSSE and cSGLD outperform all other techniques except deep ensembles\u201d \u2013cSGLD was not applied on ImageNet, therefore this statement is a bit misleading\n11.\tColour of SWAG in Figure 3 is not very clear. Only excluding other colours I can determine which line is SWAG. Similar to cSGLD, is seems that SWAG was not applied on ImageNet. Why is that if that is the case? And it should be clearly stated at least in experimental setup in Supplementary  \nFor colours in general, lines in legends are very thin and it is difficult to assess their colour. I appreciate the authors compare a lot of methods and therefore have to use a lot of colours, but it is quite difficult to assess them even on screen not to mention if the paper is printed out. Could the authors please use thicker lines in legends at least?\n12.\t\u201cBeing more \u201clocal\u201d methods\u201d \u2013 without any context in the main paper this referral to \u201clocal\u201d methods is unclear. Also it is good to add a reference to Appendix review of the considered methods in the main text.\n13.\tMissing details of what kind of augmentation is used in Section 4.3. Is it the same as training augmentation specified in Supplementary? It would require a reference to Supplementary\n14.\t\u201c(Figure 1, Table REF)\u201d \u2013 missing number for Table\n15.\t\u201cOur experiments demonstrate that ensembles may be severely miscalibrated by default while still providing superior predictive performance after calibration.\u201d \u2013 unclear which experiments demonstrate this and superior in comparison to what\n16.\tThe issues of uncalibrated log-likelihood and TACE are clearly shown in the paper, whereas the issues with misclassification detection are only verbally discussed. An illustrative example, at least a toy thought example, could really improve the paper here\n17.\tThe chosen main performance metric is not very convincingly motivated. It is clear why it is based on calibrated log-likelihood, but it is not very convincing why one cannot just used calibrated log-likelihood as a performance metric, why one should base the metric on deep ensembles instead. Also from the long-term perspective, if the community comes up with methods clearly outperforming deep ensembles, the metric would need to be based on one of these new methods \n18.\tThere is an indirect uncertainty metric that is not mentioned in the paper \u2013 uncertainty used in active learning (see, e.g., Hern\u00e1ndez-Lobato and Adams, 2015. Probabilistic backpropagation for scalable learning of Bayesian neural networks)\n19.\tFigures 4 and 5 are too small\n20.\t\u201cthe original PyTorch implementations of SWA\u201d \u2013 SWA is not considered in the paper\n21.\t\u201chidden inside an optimizer \u2026 The actual underlying optimization problem\u201d \u2013 it seems that the ICLR audience should be familiar with \u201cactual optimization problems\u201d rather than using blindly the optimizer. It is always good to explicitly write down an equation that is used in a paper, but this wording seems a bit off for ICLR \n22.\t\u201c\\hat{p}(y^\u2217_i = j | x_i, w) denotes the probability that a neural network with parameters w assigns to class j when evaluated on object x_i\u201d \u2013 it should be \\hat{p}(y_i = j | x_i, w), y^*_i is observed\n23.\t\na.\tWhy was dropout applied only for limited number of architectures and not applied on ImageNet at all?\nb.\tWhy wasn\u2019t cSGLD applied on ImageNet\n24.\t\u201cOn CIFAR-10/100 parameters from the original paper are reused\u201d \u2013 it is better to repeat the reference here\n\n\nMinor:\n1.\tFont size in eq. (10) should be the same as the rest of the paper\n2.\t\u201cOr models achived top-1 error of\u201d: \u201cOr\u201d - ?, \u201cachived\u201d -> achieved \n3.\t\u201cfor a 45 epoch form a per-trained model\u201d: \u201cform\u201d -> \u201cfrom\u201d?"}