{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper aims to conduct few-shot learning on unlabeled data (instead of on training tasks with few-shot labeled data per task). The proposed algorithm is a trivial combination of existing clustering method and a few-shot learning method, i.e., the clustering provides pseudo labels, from which a series of few-shot training tasks are generated, and then traditional few-shot learning method can be applied afterward. Many existing techniques are integrated, e.g., k-reciprocal Jaccard distance, DBSCAN clustering, prototypical network, triplet loss, etc. The paper reported the experimental results on Omniglot and Market1501.\n\nThe paper suffers from several drawbacks: 1) lack of novelty and originality; 2) problem setting is not convincing enough; 3) only a comparison on a very easy and small dataset (Omniglot) is shown, while the comparison on Market1501 is missing; 4) lack of comparison and discussion of other related works.\n\nDetailed comments:\n\n1. A simple combination of two existing techniques (clustering followed by few-shot learning) without any in-depth analysis cannot be justified as a reasonable contribution for an ICLR paper.\n\n2. The unsupervised few-shot learning setting does not make much sense in practice: we can usually collect many labeled data to generate few-shot learning tasks since these labeled data is not required to be drawn from the same distribution as the target few-shot task (e.g., the test tasks). For example, for few-shot image classification, we can always generate training tasks from ImageNet data. In practice, we only suffer from few-shot labeled data on test tasks. Assuming that none of labeled data is available can lead to unnecessary degrade on few-shot learning performance. The authors need to present more evidence or practical application scenarios to support the practical value of this problem setting.\n\n3. Omniglot is too easy and too small for most of the recent few-shot learning methods. More challenging datasets such as ImageNet or at least its easier subsets (e.g., miniImageNet, tiredImageNet) should be considered.\n\n4. No comparison to any baseline is shown on the other dataset Market1501.\n\n5. More related works need to be compared and discussed: \n\n-Constructing learning tasks via clustering has been studies in \u201cHsu, K., Levine, S. and Finn, C., 2018. Unsupervised learning via meta-learning. arXiv preprint arXiv:1810.02334.\u201d This paper shares the same idea when constructing the unsupervised episode training tasks. \n\n-Self-supervised techniques to boost the performance of few-shot learning has also been studies in \u201cSu, J.C., Maji, S. and Hariharan, B., 2019. Boosting Supervision with Self-Supervision for Few-shot Learning. arXiv preprint arXiv:1906.07079.\u201d and \u201cGidaris, S., Bursuc, A., Komodakis, N., P\u00e9rez, P. and Cord, M., 2019. Boosting Few-Shot Visual Learning with Self-Supervision. arXiv preprint arXiv:1906.05186.\u201d This paper uses a self-supervised method called \u201cpseudo labeling\u201d, but other self-supervised methods should also be compared.\n"}