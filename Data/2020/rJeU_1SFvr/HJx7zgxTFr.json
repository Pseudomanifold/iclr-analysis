{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a novel training scheme for GANs, which leads to improved scores w.r.t. state-of-the-art. The idea is to update the sampled latent code in a direction improving the inner maximization in the min-max problem. The work considers both, the gradient direction and a direction motivated by the natural gradient which is shown to yield excellent performance. The overall scheme is motivated as an approximation to the (prohibitively expensive) symplectic gradient adjustment method. \n\nWhile the work contains some typos, it was easy to read and overall very well written. The experimental results are impressive and clearly validate the usefulness of the approach. Therefore, I recommend acceptance of the paper.  \n\nOn the theoretical side, I feel some parts can be improved. I'm willing to further increase my rating if some of the following points are addressed:\n\n1. The connection to SGA is a bit hand-wavy, as terms are dropped or approximated with the only reasoning that they are difficult to compute. In some approximations (e.g. Gauss-Newton approximation of the Hessian) one can argue quite well that certain second-order terms can be dropped under some assumptions (e.g. mild nonlinearity, or vanishing near the optimum).\n\n2. I had some troubles Sec 4 related to the natural gradient.\n(a) What is p(t | z)? Is it the same p as in (37), Appendix C? I find the argument that the hinge loss pushing D(G(z)) < 0 during training hand-wavy, as for natural gradient p(t | z) should always be a valid distribution. There are also some typos which made it hard to follow the arguments there. It probably should read \"an ideal generator can perfectly fool the discriminator\" (not perfectly fool the generator). \n\n(b) Maybe it would be easier to directly argue that one wishes to approximate SGA as well as possible, rather than taking a detour through the natural gradient.\n\n3. Why is the increment \\Delta z clipped in Algorithm 1? Is there a theoretical justification? If the goal of the clipping is to stay inside the support of the uniform distribution, shouldn't it rather be z' = [z + \\Delta z]? A soft clipping (e.g. performing a mirror descent step) might give better gradients.\n\nTypos, minor comments (no influence on my rating):\n- In Eq. (5), it should be \\partial^2 in the last \"Hessian-block\" multiplication.\n- presribed -> prescribed\n- Eqs. (7) and (8) might be easier to parse if one uses a different notation to distinguish between total derivative and partial derivative, i.e., write on the left side df(z') / d\\theta. Also, I think it is clearer to write in the last terms in (7) and (8) \\partial f(z') / \\partial \\delta z instead of \\partial f(z') / \\partial z'.\n- In Appendix B.1, shouldn't it be \\gamma=(1+\\eta)/2 instead of  \\gamma=\\eta (1+\\eta)/2?\n- I've found ELU activations to work well in GAN models which involve the Jacobian w.r.t. z. Maybe it can stabilize things here as well.\n"}