{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: This paper proposed a variational word embedding method, by using vMF distribution as prior and adding an entropy regularization term.\n\nStrengths:\n[+] In the experiment parts, the authors describe the experiment settings in detail.\n[+] Detailed descriptions for each equation is given.\n\nWeaknesses:\n[-] Template: It seems that the authors use the wrong template, which is not the template for ICLR2020.\n[-] Appendix: Although the author mentioned 'appendix' many times, I cannot see the appendix.\n[-] Motivation: The connection between motivation and proposed method seems weak. The authors argue 'interpretable' in their title and abstract, but their method and experiments do not show this point explicitly. \n\nQuestions\n[.] The experiments seem a little weak. The vocabulary size is 10K and the corpus is not so big, and I wonder whether the performance of the proposed method will be better for large corpus and longer training time.\n[.] For Equation 8, we should have a guarantee on the concentration of partition functions. Is it still true for vMF distribution? \n[.] What is the advantage of vMF distribution? "}