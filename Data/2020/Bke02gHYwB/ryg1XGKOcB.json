{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is about learning word embeddings. In contrast to previous work (word2vec's Skipgram) where a word is represented by a fixed word and context vector, here each word is represented by a von Mises-Fisher distribution and a context vector. \nThe paper claims that the resulting embeddings are more interpretable, and that the inner product of two context vectors represents their point-wise mutual information.\n\nMy main concerns are the following:\n\n- Representing words by distributions is not a novel idea; it was previously done by Brazinskas et al. (2017): Embedding words as distributions with a bayesian skip-gram model (BSG). They however used Gaussian distributions and not von Mises-Fisher. BSG is acknowledged in the paper, but only small scale comparisons are performed (15 million) while the BSG paper uses a lot larger data sets. There is therefore not a meaningful comparison to the most relevant previous work.\n- Word similarity experiments are not enough to justify this approach. BSG at least showed the strength of using distributions to represent words by showing that different samples could constitute different word senses/meanings. There is no such analysis here.\n- The claim that the resulting representations are more \"interpretable\" is not backed up by any evidence at all, even though the word \"interpretable\" is in the title and the list of contributions.\n\nGenerally this paper could benefit from proof reading and editing."}