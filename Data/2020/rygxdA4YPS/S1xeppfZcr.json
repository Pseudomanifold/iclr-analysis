{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Authors use the PL condition to motivate a formula for learning rate selection. (they bound their loss by a quadratic with fixed curvature alpha, and use this to get learning rate).\n\nTheir analysis very closely mirrors one presented in \"Gradient Diversity\" paper, it uses the same assumptions on the loss. IE compare A.1 in the paper to the B.5 in \"Gradient Diversity\"\n\nGradient Diversity solves for batch size B after which linear learning rate scaling starts to break down, while this paper instead fixes B and solves for learning rate. Two results are comparable, if you take their learning rate formula in section 3.2 (need formula numbers) and solve for B which gives learning rate halfway between B=1 and B=inf, you get the same expression as \"critical batch size\" in equation 5 of gradient diversity paper.\n\nIt's not immediately obvious how to invert formula in Gradient Diversity paper to solve for learning rate, so I would consider their learning rate formula an interesting result.\n\nI also appreciate the breadth of experiments used for evaluation. \n\nThe biggest issue I have with the paper is that I can't tell if it's better of worse than linear learning rate scaling from their experiment section. All of their experiments use more iterations for AS evaluation uses than for LSW evaluation. They demonstrate better training (and test) losses for AS, but because of extra iterations, I can't tell that the improvement in training loss is due to number of iterations, or due to AS scaling."}