{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents and evaluate an algorithm, AdaScale SGD, to improve the stochastic gradient decent in distributed training. The proposed algorithm claims to be approximately scale-invariant, by adapting to the gradient's variance. The paper is well-written and generally easy to read, although I didn't check all theory in the paper. \n\nThe approach is evaluated using five benchmarks (networks / dataset combinations) from different  domains. The results are promising and seems solid. \n\nThe paper is good and I like it, although I think the novelty and contribution is slightly too low for ICLR. The kind of tweaking and minor optimizations to provide some adaptivity (or similar) in existing and established algorithms and approaches that is presented in this is paper is very important from a practical perspective. However, from a scientific perspective it provides no significant contribution.\n"}