{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors propose a new domain adaptation framework through label propagation. Specifically, the labels are propagated using a manifold structure. The clusters of features in each domain are aligned to be close by enforcing the cycle consistency. Moreover, DANN and LGAN is used in the final objective function to align the distribution and enable local manifold consistency. Experimental studies have been done to verify the effectiveness of the proposed method.\nOverall, the paper is easy to follow. The idea of label propagation by enforcing the cycle consistency through graphs is interesting. The proof of theorem is simple and correct. My major concern is on the significance of the paper, which is limited. The overall objective consists of four important terms in which L_class, L_dann and L_local are from the existing works. The only contribution is on the L_cycle. However, the cluster alignment idea has been studied in [ref1], while the current work completely ignores [ref1]. Although the ways of aligning the clusters are different between the two works, more discussion and comparison with [ref1] are necessary. Here are some detailed comments:\n(1)\tThe authors claim the scale parameter \\delta is learned. Can you show the learned \\delta for different domain pairs and discuss how the \\delta affects the final performance?\n(2)\tIt is not clear how matrix T and W are associated. It seems that T is the normalized W. However, they are completely in the same form. Why does W still preserve the identity block matrix I after normalization?\n(3)\tThe theorem 1 is for ideally clustered source data, which may not be possible in reality. How the perturbation of unideal clustered source data, e.g., some clusters have overlapping, affect the final performance? The discussion is necessary as this is the case for the complex real-world data.\n(4)\tEq.(3) uses L_1 norm. Have you tried any other norms?\n(5)\tThe results of table 1 for the other baselines are directly from the paper. It is not clear whether the same experimental setting is applied, which makes the comparison less convincing. \n(6)\tTable 2 demonstrates L may not bring benefits. Although the authors analyse the reason, i.e., the performance of the approximation of the local manifold, it is necessary to give some principled guidelines to guide the usage of L or not when given a new task.\n(7)\tThe comparison with [ref1] should be given. \n(8)\tRegarding the selection of the Hyper-parameter in appendix, the authors discuss some possible ways. Which one is used in this work? Is it following (Bousmalis eta al. 2016)? Moreover, why set different ranges for different hyper-parameters? Table 3 shows different tasks use different hyper-parameters. How to set these hyper-parameters given a new task, especially when no target labelled data is available?\n(9)\tHow the batch size is selected? Is it based on the final performance? The transfer performance seems to be sensitive to the batch size. The authors should give more details on this point. \n[ref1] Cluster Alignment with a Teacher for Unsupervised Domain Adaptation. \n"}