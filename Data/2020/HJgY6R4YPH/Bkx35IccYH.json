{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper involves several common-seen techniques of domain adaptation, source label guided classifier training with a softmax cross entropy loss, GRL guided domain discrepancy minimization with a domain discriminator, and label propagation with manifold learning.\n\nThe label propagation is the novelty of this work, which proposes a cycle consistency on the source labels, for local class manifold alignment.\n\nThe local manifold consistency loss in Eq.(4) seems to have the similar physical insight with Eq. (3), but introducing the GAN model, which is similar to the perceptural loss. \n\nI have some concerns.\nThe LGAN from the paper is a pre-trained generator without re-training, as can be seen from Eq.(5) and (5). So, how do you have a control on its adaptation of LGAN to the DA problem of this paper? I also think that this seems to be an incremental method by simply introducing LGAN into the traditional DA model for data augmentation.\n\nFrom Eq.(6), the L_cycle and L_local are the new terms based on the traditional DA. Actually, I have some concern that because of the domain distribution difference, the similarity matrix Wts between target and source data cannot well reflect the true similarity. I understand that the L_dann loss can align the marginal distribution across domains. So, this is an problem of \"chicken first or egg first\", which depends on the training strategy of the model.\n\nThe experimental comparison is not strong between the proposed and the CDAN. Actually, there lacks of an ablation analysis, by removing the L_local and L_cycle.\n\nAt least, from the given experiments, the performance of the proposed losses is not significant, with at most 0.5 percent improvement."}