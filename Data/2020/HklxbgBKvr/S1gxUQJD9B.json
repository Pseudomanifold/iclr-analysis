{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work the authors propose a framework for combinatorial optimisation problems in the conditions that the measurements are expensive. The basic idea is to make an approximation of the reward function and then train the policy using the simulated environment based on the approximated reward function. The applications are shown in a set of biological tasks, which shows that the model performs well compared to the baselines. \n\nThe idea of learning the models of environment (or reward) and simulating the model to train the policy is not novel (e.g., https://arxiv.org/pdf/1903.00374.pdf). Similarly, in terms of formulating the discrete search problem as a reinforcement-learning problem, again there are similar works in the past, which are cited in the paper, but the combination of these two is novel to my knowledge; having said this the paper should discuss relevant works such as the one above. \n\nThe experiments seem convincing to me overall, however, I have the following concerns: \n\n- The performance of the model seems similar to PPO in the large state-spaces (section 4.3), which somehow is disappointing.\n\n- The performance of the model seems very sensitive to the choice of \\tau (Figure 6 right), which is set to 0.5, but it is not mentioned how this parameter is chosen (or at least I couldn\u2019t find it) and how much the performance of the model in the other experiments is affected by the choice of this parameter.  \n\n"}