{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper proposed a memory-augmented neural network inspired by the human cognitive process. Specifically, the paper has 3 model contributions: A modification to LSTM cell, a 2 layer LSTM architecture where the second layer is updated only when the mask in the lower layer is ON, an external memory which is accessed in a sparse way, again only when the mask is ON. Authors show that the proposed architecture is better than LSTM and NTM is a series of synthetic tasks.\n\nMy comments:\n\nOverall it is an interesting idea. However, there are several issues that need to be addressed before the paper is ready for publication. I will accept this paper only if all these changes (suggested below) are made in the revision.\n\n1. Lipton et al. 2015 is not the reference for LSTM! Please give the correct citation.\n2. All the citations are wrongly formatted. Please use citet, citep appropriately.\n3. There are several related works that are not discussed in the paper. The idea of using a mask in the first layer to update second layer is very similar to Chung et al. 2016 (Hierarchical Multiscale Recurrent Neural Networks). Also the sparsity in memory access has already been explored in Rae et al. 2016 (SAM, Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes), Gulcehre et al. 2016 (D-NTM, Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes), and Gulcehre et al. 2017 (TARDIS, Memory Augmented Neural Networks with Wormhole Connections). Even though you don\u2019t compare with them, you should at least discuss the relations.\n4. In section 2.1, you spend more space explaining chrono-initialization than explaining your contribution. You mention your contribution only in the last few lines. I expect more discussion on why you choose to modify the LSTM cell.\n5. Is it right to say that if you remove the mask, then the entire model is exactly the same as NTM? I would like to see more experimental analyses on the sparsity constraint in the mask. What happens if the constraint is not very strict?\n6. Section 2.3 is too verbal. I cannot implement the model by reading this description. Please make it more precise.\n7. The paper will get more clarity by adding the pseudocode of the model operation.\n8. In section 3.1, isn\u2019t d_i(n) = n-i ?\n9. Section 3.1 is a general discussion of the advantage of using external memory. This does not help to differentiate your model from NTM, D-NTM, or TARDIS.\n10. Why not multiple runs for standard LSTM? The comparison is not fair. I would like to see same number of runs for both models.\n11. Table 1 is not precise. What is the metric for each column? Is higher the better or lower the better? I can guess all this. But it has to be explicit in the table. Also, it does not mention the length of the copy task.\n12. Why is ADD task results missing in table 4?\n13. Are the authors willing to release the code to reproduce their results?\n\nMinor comments:\n\n1. Page 1: last para: fix \u201cmodal model\u201d\n2. \u201cneural turing machine\u201d should be \u201cneural Turing machine\u201d .\n3. Lines before section 2.2: \u201cfitter\u201d or \u201cfilter\u201d?\n"}