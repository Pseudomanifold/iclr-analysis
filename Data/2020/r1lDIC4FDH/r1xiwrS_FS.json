{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a memory-augmented network for long-term sequence modeling. The major novelty is a new constrained LSTM structure with a learned binary mask, which is particularly designed to solve the problem of intensive memory access of previous MANNs.\n\n1. It is reasonable to learn a sparse binary gate to reduce the frequency of memory access, but the authors did not further demonstrate its benefit through experiments. I suggest comparing the computational cost of the proposed model with that of the previous MANNs.\n\n2. I find the formulation in Eq. (1) not fully motivated. May I ask for what reason the authors used the trainable vector u and the diagonal matrix D?\n\n3. One contribution is this work is to use the chrono-initializer in the constrained LSTM. I suggest the authors add a reference when it is mentioned for the first time.\n\n4. Some related work in memory-augmented LSTMs was not mentioned or compared with, e.g. RMC [Santoro et al. 2018] and E3D-LSTM [Wang et al. 2019].\n[Santoro et al. 2018] Relational recurrent neural networks.\n[Wang et al. 2019] E3D-LSTM: A Model for Video Prediction and Beyond.\n\n5. Section 2.3 can be more self-contained to include more details of the memory read and write processes."}