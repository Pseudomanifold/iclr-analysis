{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a new optimization algorithm for convex functions with sparsity constraints, based on hard thresholding to enforce sparsity. It is argued that applying hard thresholding in every step is the computational bottleneck of previous works based on hard thresholding, and the paper proposes a variant that alleviates previous works\u2019 convergence problems. This is achieved by having an inner and outer loop in which the hard thresholding is applied only in the outer loop, and in the inner loop block coordinate descent is used  (ie. applying updates to only a subset of the solution\u2019s variables). The paper shows that the convergence bounds of the algorithms compares favourably to previous hard thresholding algorithms convergence bounds, and empirical results show the effectiveness of the algorithm compared to previous ones.\n\nIt is always exciting to see improvements in optimization algorithms as they can lead to improvements in many different problems. Yet, I have the following concerns:\n-The algorithm is strongly based on previous optimization algorithms as the main difference with them is applying the hard thresholding less frequently. This sounds incremental and I think the paper could emphasize much more why this idea is a significant advancement over previous works.\n-The paper could make a better job motivating the sparsity constraints, as it cites papers from 10 years ago which are not representative of state-of-the-art.\n-The paper focuses on optimization time but not on generalization. Note that converging faster to the objective does not imply better generalization. I would have expected at least a discussion about this in the paper. Does the algorithm lead to better generalization accuracy or worse?\n-The proof contains an assumption that it is unclear that has been explained in the paper. In the supplementary material, after eq. 25, the factor sigma is being introduced and the paper says \u201cwe assume that there exists a constant factor sigma making this inequality true\u201d. \n-Definition 1 does not really define hard thresholding. \n"}