{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on risk-aware reinforcement learning, where an agent could be encouraged to take more risk (high reward, high variance states) or avoid risk (low variance states). Risk control is instantiated by different ways of estimating the advantage of a state (max/min instead of average). Experiments on several environments show good performance of the proposed algorithm.\n\nThe paper is written clearly and the approach is straightforward. However, it's unclear what is the objective function the algorithm is optimizing by using a biased estimation of the advantage. It seems to work pretty well in practice, but I wonder how it compares to other risk-sensitive RL algorithms (e.g. those cited in the related work section).\n\nOverall, this paper presents a simple heuristic to steer the policy towards risk-seeking / risk-avoiding directions, but could benefit from either more theoretical analysis or more empirical comparison with other methods."}