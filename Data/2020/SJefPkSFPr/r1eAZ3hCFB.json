{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a modification to policy gradient methods that are computed from advantage function estimates. For a given trajectory of n steps, there are n different advantage function estimates: from 1-step to n-step. GAE (Schulman, 2016) proposes to take an exponentially weighted average of these estimates to compute the policy gradients. This paper proposes instead to use order statistics to compute the policy gradient; e.g. the most optimistic estimate, the most pessimistic estimate, or the most extreme estimate.  The paper introduces a regulatory ratio: the probability of using the averaged advantage estimate vs using the order statistic, for computing the policy gradients. This hyper-parameter is justified on the optimistic case (max advantage), as a way to prevent overtly optimistic estimates. The paper conducts experiments on  different domains (sparse and dense rewards, discrete and continuous actions, fully observable and partially observable environments) which show the effect of choosing different order statistics and regulatory ratio on the policy performance.\n\nThis paper could be accepted as it presents an interesting idea with extensive experiments showing where it works and where it fails, along with some justification for the hyperparameter choices.  But there are a couple concerns about the validity of the method.\n\nOne concern is that the regulatory ratio is only justified for the max case, but not for the min or max-abs case. In addition, the choice of regulatory ratio seems to have a wildly varying impact depending on the choice of the order statistic and environment, for which we get little insights from the paper.\n\nAnother concern is that most of the results are reported only for an ensemble of 4 n-step estimators. In the appendix, the papers reports a comparison with a larger ensemble (12 n-steps estimators), which results in lower performance. This is a bit confusing: using the order statistics (max, min and max-abs), and following the reasoning presented in the paper, I would expect that increasing the ensemble size would result in better performance ( the max/min of the 4 ensemble is n upper/lower bound of the max/min of the 12 ensemble). This paper could be improved with more detailed results on the effect of the ensemble size. While the paper provides arguments for why small ensembles might suffice, it does not explain how the ensemble should be chosen, and what would happen as the ensemble size increases.\n\nFinally, the parallels to human psychology are  bit superfluous. I understand how it might serve as an inspiration for algorithm design, but it is a bit distracting from the technical contribution of the paper.\n\nThings to improve:\n\nWhy is max-abs missing from Figure 2?\nDid you try a Rainbow-style training (changing the regulatory focus periodically over training)?\n\nIn some sentences, it looks as if insure was written in place of ensure. In general, this paper would benefit from proof reading by a proficient English speaker."}