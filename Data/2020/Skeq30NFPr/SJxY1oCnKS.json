{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the optimization behaviour of stochastic mirror descent (SMD) on over-parametrized nonlinear models. It shows, rigorously and empirically, that SMD finds global minimizer that is approximately the closet global minimizer to the initial point, under the same Bregman divergence.  The paper is well written and easy to follow. \n\nPros:\n1. The main contribution of this paper is extending the previous results on linear case into nonlinear setting, which is much more general. The results are non-trivial and interesting. \n2. The empirical studies in the paper is clean and significant. It also supports the developed theory.\n\nThe main concern I have is about assumption 1 in the paper.  While in the paper the authors tend to claim that it is reasonable/mild, it is not obvious to me. \n    a. Many factors are actually involved in this assumption: smoothness of \\psi, smoothness of L_i and f_i, number of parameters etc.. I am not sure if it is still a high probability, when \\eps needs to be small enough so that the intuition behind Figure 2 still holds. \n    b. Is this assumption also made in the referred papers on the top of page 6? Could the authors develop a concrete result, given smoothness coefficients of \\psi and L_i, for this assumption?  \n    c. Without a proper justification of assumption 1, the significance of this paper seems much weaker. For example, one can also assume the PL inequality locally true for over-parametrized function, and develop a convergence result. \n"}