{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Interesting paper! The paper is well organized, well written and easy to read.\nThe main idea proposed in the paper is to modify the activation function $\\sigma(x)$ in a neural network to $\\sigma(ax)$ where $a$ is a learnable parameter. Every neuron could have it's own $a$, or $a$ could be shared across all neurons in a layer or could be shared across all neurons in a network giving rise to different versions. In addition to this, a new term is added to the cost function which favors larger values of $a$. They demonstrate through empirical experiments that this helps for some particular problems in physics and in general for image recognition tasks in datasets with low resolution images. In addition to this, they theoretically argue that, introducing this new parameterization makes gradient descent not attracted to local minima. \n\n1. Results in Fig 4 are evaluated on the same data as training and similarly for Fig 5 and 6, it is just the train loss that is reported. Does this coincide with an increase in generalization performance? Can you report your train and test accuracy for all these cases?\n\n2. How do the Layer and Neuron based activations work when you've convolutional filters? For the neuron one, do you have as many parameters as the number of pixels for every channel? For the layer case do you share only across channel or do you share across the whole layer?\n\n3. Does your conclusions in Fig 5 and 6 hold if you use deeper networks (Resnet50) ? Does it hold on ImageNet (i.e, on larger resolution images)?\n\n4. Why is parameterizing the slope term as $na$ necessary? Does it have anything to do with the way your slope recovery term is defined? In addition to this, do you have projection steps to ensure that $a$ is always positive?\n\nWhile I can convince myself that having an extra degree of freedom in terms of slope may help with activations functions like tanh and sigmoid, I'm extremely surprised that the authors observe improvements in case of ReLU as well. Can the authors offer some reasoning/explanation for understanding this? Also, is it just fitting the training data better or are we seeing better generalization performance (same as point 1). \n\n"}