{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper introduces a parameter to scale the input to an activation function and a loss function to encourage the scaling parameter to be large. The authors test it to learn a discontinuous function as well as several image datasets.\n\nThe concept of learnable activation functions is not new and this work is fairly incremental. However, for the image dataset results it seems the authors are using ReLU for their activation functions. Having a scaling parameter before the input to ReLU and encouraging that scaling parameter to be large is similar to increasing the learning rate. There are no experiments to verify whether or not this is the case. Also, the authors only report loss and not accuracy on these datasets."}