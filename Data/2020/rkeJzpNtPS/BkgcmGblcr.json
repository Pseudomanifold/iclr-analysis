{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, two approaches of locally adaptive activation functions are proposed\u00a0to improve the training of deep and physical-informed neural networks. The so-called adaptation is realized by multiplying the output of the previous layer by a learnable parameter before passing to the activation function. If the parameter is a scalar and applied to the whole layer, it is called layer-wise locally adaptive activation functions (L-LAAF); if the parameter is a scalar and each neuron has a unique one, it is called neuron-wise locally adaptive activation functions (N-LAAF). The motivation of the adding this adaptive parameter is to increase the slope of of the activation function, so that the convergence is faster.\n\nI am not convinced why this approach will work.\n\nFirstly, the parameter a^k will be absorbed into the w^k and b^k, making it equivalent to learning a scaled version of w^k and b^k. For example, if L-LAAF is applied to eq (1) and we redefine w^k := a^kw^k and\u00a0b^k := a^kb^k, then the new form is the same of the original one. The same rationale applies to N-LAAF.\n\nSecondly, I don't understand why adding this parameter will increase the slope of the activation function, unless the authors impose the parameter to be a big positive number. But we cannot make this assumption as the parameter is trainable rather than a constant. To keep or enlarge the magnitude of the gradient, there are more principled algorithms such as normalized gradient (https://arxiv.org/abs/1707.04822) and signSGD (https://arxiv.org/abs/1802.04434), but the author fails to mention or compare.\u00a0\n\nTherefore, I am afraid the proposed method is equivalent to doing nothing to the activation, as the added parameter will be just absorbed to other learnable parameters.\n\n"}