{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Incorporating the local learning approaches into the training of the deep generative models can potentially create a new model that has both the capacity for high-dimensional inputs and flexibility for locally changing environments. However, the local learning approach is limited to using only a simple model, because complex models require a large amount of data and extended training time. This paper overcomes this trade-off based on the insight that the real-world dataset often shares some structural similarities between each neighborhood.\n\nPros:\nThe paper is well-written. It is easy for the reader to understand. The derivations in the paper are correct.\n\n\nThe motivation is plausible. It is reasonable to expect that each local subspace of the dataset shares some structure since most dataset tends to be governed by the consistent rules of the real world.\n\n\nThe numerical experiments show that the locality enables the model to achieve the disentangled representation for each subspace without any label information.\n\nCons:\nThe novelty seems a little straight-forward. The paper just extends the objective function of the VAE to have different parameters for each local subset.\n\nIn consideration of extended training time in the complex model, the paper doesn't provide an evaluation of the efficiency of their proposed model.\n\nThe paper isn't very polished yet. There were more than a few spelling and grammatical errors, please proofread the work and improve the writing.\n\nThis paper in its current form is already fairly good. "}