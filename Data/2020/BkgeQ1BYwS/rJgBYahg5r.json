{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper introduces a new intrinsic reward for aiding exploration. This one\nis based on learning a distribution on parameters for a neural network which\nrepresents the dynamic function. The variance the predictions from this\ndynamic function serves as the intrinsic reward. Results are compared against\nseveral current state-of-the-art approaches.\n\nFeedback:\n\nUsing uncertainty as an instrinc reward to guide exploration is a\nvery active area of research and it would have been more helpful\nto say how this work differs from Burda et al, Eysenbach et al,\nGregor et al. 1.  The underlying algorithms are all very similar\nand differ in only small and subtle ways. The main difference\nwith this work and Pathak et al. seems to be that the variance is\nall coming from one particular conditional distribution rather than\nan ensemble of models, but in Pathak et al it is also a distribution\nover models.\n\nAmortized SVGD is used instead of regular SVI in this work, but\nit is never articulated why to problem benefits from using that\nframework. This paper would greatly benefit from some explanation.\nIt is mentioned as a novel aspect of the work, but never really\njustified at all.\n\nThe experimental results and convincing and do show a substantial\nimprovements over similar approaches in domains in ant maze\nnavigation and robot hand.\n\n[1] Gregor, Karol, Danilo Jimenez Rezende, and Daan\nWierstra. \"Variational intrinsic control.\" arXiv preprint\narXiv:1611.07507 (2016).\n"}