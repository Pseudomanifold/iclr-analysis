{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper focuses on how to partition a bunch of tasks in several groups and then it use multi-task learning to improve the performance.  The paper makes an observation that multi-task relationships are not entirely correlated to transfer relationships and proposes a computational framework to optimize the assignment of tasks to network under a given computational budget constraint. It experiments on different combinations of the tasks and uses two heuristics to reduce the training overheads, early stopping approximation and higher order approximation. \n\nPlease see the detained comments as follows:\n1. The experiments are based on the assumptions that the network structures (how parameters are shared across tasks) are fixed. From my perspective, understanding how to optimize the parameters sharing across two tasks should be the first step to study how to optimally combine the training of tasks. Otherwise, different parameter sharing structures across tasks may lead to different conclusions.\n\n2. It requires optimization of the article structure. E.g., algorithm 1 is important and should be in the main context.\n\n3. It is also related to neural architecture search and it requires some discussions.\n\n4. The paper is over-length. \n\n5. A lot of typos. \n\nNits:\nPage 1: vide versa -> vice versa\n\nPage 3: two networsk -> two networks\n\nPage 6: budge ?? 1.5\n\nPage 7: overlap between lines (under figure 3)\n\nPage 8: half segmented sentence in section 6.\n"}