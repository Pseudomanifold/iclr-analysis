{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a neural network based clustering process where the number of clusters is not known a-priori.  Proposed approach requires conjecturing a generative process (where number of clusters/classes is a random variable) and the model learns to uncover the posterior distribution over clusters given the observed samples.\nOverall I think it is a valuable contribution, well written paper with good results.  \nSpecific comments:\na) Even though the model allows for variable number of clusters, I feel there may be a strong dependence between the number of clusters the model can hypothesize and the number of clusters in the training data.  It will be useful to give further insights into this.  For instance, if an MNIST model is trained with only digits 0-5 training data, how well would it perform in detecting all 10 clusters at test time?  Understanding model\u2019s biases based on training data is one area I feel is important and the paper could add to.\nb) The neural clustering process could potentially be viewed as a transductive inference model for classification of test data.  Typically at test time classification is done for each test sample independently, and the clustering process allows one to bring in other similar test samples to help with classification.  Have the authors considered this and have any comments on potential value / feasibility of this?\nc) In the examples presented in Section 2.3, please clarify how training & testing was done.  Specifically what training data was used (all of MNIST training data?), and the test time clustering was done on a subset of MNIST test data?\nd) Use of \u2018q\u2019 for a neural-network and \u2018q_\\theta\u2019 for posterior distribution is a little confusing, will be better to have different notation for these.\n"}