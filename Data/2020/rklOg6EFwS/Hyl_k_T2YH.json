{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n=======\nNeural Networks (NN) have been shown to be susceptible to various adversarial attacks i.e. if we perturb the \"x\" just a little, the output prediction changes. So, there has been much research devoted to how we can make NNs robust to such attacks. Typically, the adversarial examples that are used to train adversarially robust methods are generated from the correctly classified examples. \n\nThis paper first shows empirically that the adversarial examples generated from misclassified examples by the model h_{\\theta} are as important as the ones generated from correctly classified examples on the CIFAR dataset. The authors then propose a novel objective for the adversarial risk that incorporates the misclassified examples as a regularizer. Next, a surrogate convex objective is derived which is optimized to come up with a new kind of adversarial training called misclassification aware adversarial training (MART). \n\n\nOriginality:\n==========\nThe proposed objective seems novel to me compared to the existing methods, perhaps less so to someone who is an expert in adversarial methods. That said, I definitely found the addition of a misclassification-based regularization term as intriguing. \n\nHowever, I was wondering that maybe the previous approaches did not distinguish between correctly vs incorrectly classified examples while generating adversarial examples was because that is a more general setting. This is so because the idea of classification is tied to a certain model h_{\\theta} and hence is not model-agnostic by definition. Perhaps the strength of the approach by Madry et. al. 18, which is the closest work to this paper, is in being model-agnostic. I hope I am not missing something!  \n\n\nQuality:\n=======\nThe paper is technically sound and is a solid contribution to the literature on adversarial robustness. The motivation,  experimental results, and ablation studies are all very well executed. The results are shown on MNIST and CIFAR-10 image datasets and it outperforms a host of competitive baseline algorithms.\n\n\nClarity:\n=======\nThe paper is well organized and is very well written. The experimental results are very thorough and well explained.\n\n\nSignificance:\n============\nThe paper solves an important problem of dealing with adversarial attacks on deep neural networks. Further, the solution they propose is novel and a seems like a significant improvement over the extend state-of-the-art.\n"}