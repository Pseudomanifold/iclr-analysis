{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper essentially presented a viewpoint, i.e. misclassified examples may have a significant impact on the final robustness.\nThe authors conducted a series of qualitative experiments to verify \n1) Misclassified examples have more impact on the final robustness than correctly classified examples.\n2) For misclassified examples, different maximization techniques may have a negligible influence, but minimization techniques play a critical role on the final robustness.\n3) The authors proposed a new defense algorithm which focus on generating adversarial examples from misclassified examples during the training. The algorithm was shown to improve the final robustness by revisiting these adversarial misclassified examples.\n\nGenerally speaking, the idea, though somewhat straightforward and less elegant, is reasonably presented and also well-motivated. Empirical validations seemed to support the idea and indicated that the proposed approach could improve the adversarial robustness.\n\n\nThere are several major concerns with the paper as follows:\n\n1.\tIt is good and reasonable to put more emphasis on the misclassification examples, since it is likely that the region of the \u201cperturbation\u201d largely overlaps with the region of the misclassified examples.\n\n2.\tHowever, this may be dealt with in a more elegant or systematic way. In the viewpoint of the reviewer, actually a different emphasis can be imposed on each different data point including both the correctly-classified and mis-classified samples. In another word, a more systematical extension or a metric may actually be developed emphasizing more on mis-classified samples. It is suspected that different samples within the set of mis-classified samples (or even in the set correctly-classified samples) could also have a different influence. It is highly likely that a more elegant and mathematic way can be designed for this purpose. \n\n3.\tFurther to point 2, in particular, the regulation w.r.t the correctly classified examples may not be ignored. It is interesting to consider differently the correctly-classified examples due to the trade-off between robustness and standard accuracy. This can also be seen in another submission of ICLR2020 (titled Sensible adversarial learning\u201d) which actually discards the mis-classified samples. I would like to see some additional comments, clarification, or discussions on this point.\n\n4.\tI am curious to know if outliers would be over-emphasized by the proposed idea. Some discussion or even some illustrations on a synthetic case would be interesting. Will the existence of outliers affect the robustness?\n\n5.\tIn Table 2, the best and the last results on FGSM are totally identically, I wonder if this is a wrong copy, which should be further clarified and discussed.\n\n6.\t In the Unlabeled Data experiment part, it is better to compare the results with both UAT++ and RST at the same time rather than separately.\n\n7.\tEquation (4) shows that the two parts on the right side are added together, is that right? This is inconsistent with Equation (7) and the following description.\n\n8.\tIn 3.2, the first paragraph, \u201cmax-margin adversarial training (MMA) (Ding et al., 2019)\u201d appears to be wrong. The correct reference is \u201cGavinWeiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637, 2018.\u201d.\n"}