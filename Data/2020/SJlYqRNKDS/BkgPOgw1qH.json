{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a generalization of AdaGrad, called BAG, that operates on blocks of parameters instead of each individual parameter. The authors also propose a momentum version of BAG called BAGM. Convergence rate results are proved for the algorithm, and some uniform stability results show situations where BAG would generalize better than previous adaptive gradient methods.\n\nOverall I found the paper interesting, although I found the paper very dense and hard to read. The paper would be much easier to read if an effort was made to present the theoretical results in slightly simplified forms. In addition to this, I have a couple more concerns about the paper, which I list below:\n\n1. I am a bit unsure about the what the uniform stability results add to the paper. \na) While Proposition 1 is interesting, is it relevant for the BAG algorithm since it considers a layer-wise training process. \nb) Proposition 2 is also a bit confusion to me. How does generalization get worse as the number of blocks approaches the number of parameters (i.e., gets closer to AdaGrad)?\nc) Proposition 4 looks interesting, but hard to interpret the way it is presented now.\n\n2. Is the better generalization performance observed for BAGM (as well as Adam) simply due to the larger epsilon value used in the experiments? Would such an epsilon value simply turn off the adaptivity as the iterates gets closer to the minimizer (when the gradients are small relative to epsilon), and effectively do SGD? How does performance vary with epsilon?\n"}