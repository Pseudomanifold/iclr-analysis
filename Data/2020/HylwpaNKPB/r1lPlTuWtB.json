{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a variant of GCN referred to as HWGCN to consider convolution beyond 1-step neighbors. The authors first construct a high-order adjacency matrix based on feature similarity and graph structure using LASSO method. Then the high-order adjacency matrix is used in the GCN instead of the original adjacency matrix. The authors carry out experiments on three datasets for node classification. Comparison to several state-of-the-art methods demonstrate comparable or better accuracy.\n\nStrength:\n1. The idea of utilizing high-order neighbors in GCN is interesting given the recent results of worse performance with added depth in GCN.\n2. The authors carry out thorough comparison to state-of-the-art methods across GCN, node embedding and also label propagation.\n\nWeakness:\n1. The major concern is whether the improvement of accuracy can justify the added complexity in model and training. Table 5 shows that the proposed method only has comparable but not superior performance to the baseline methods. However, the proposed method has added complexity in: (1) The time required to find shortest paths between nodes in the graph; (2) The time used in LASSO training to find high-order similarity; (3) The increased density of the graph which leads to higher cost for each graph convolution operation.\n2. The authors only provide evaluation on the accuracy of the proposed method. On the contrary, no running time comparison is provided. The authors should provide running time for (1) pre-processing of the high-order similarity including shortest path finding and LASSO training; (2) the time cost for the graph convolution ops with and without the high-order interaction.\n3. It would be interesting if the authors could provide more analysis and insight into the learned high-order interaction. For example, how level of sparsity is achieved via the LASSO learning? How different is the learned high-order interaction differs from direct similarity between node features and A^p.\n\nDetailed comments:\n1. In equation (5), the authors use the average among neighbors to compute the high-order computation. What will be performance differ if for example just use x_j? \n"}