{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper performs a regret analysis for a new hierarchical reinforcement learning (HRL) algorithm that claims an exponential improvement over applying a naive RL approach to the same problem. The proposed algorithm and the regret analysis performed seem rigorous and well-thought out.\n\nHowever, I think that this paper should be rejected because (1) the algorithm does not appear to be a substantial improvement over existing algorithms, (2) the paper makes strong claims about an exponential improvement over standard RL, but doesn't provide a strong benchmark to compare to, and (3) the paper is imprecise and unpolished, with many grammatical errors.\n\nI would be open to reconsidering my score if a) the authors submit a revised version with significantly cleaned up text, and b) if the authors could provide more information about how their contribution compares to the existing literature.\n\nMain argument \n\nThe paper would benefit from establishing stronger context for the central contributions of their paper. For instance, the paper begins by contrasting HRL approaches with a number of standard RL algorithms, saying that approaches such as AlphaGo do not require high-level planning. This seems surprising; many RL researchers would describe MCTS (the base of the AlphaGo algorithm) as performing planning. It would be great if the authors could go into more detail as to what they view as planning, and why AlphaGo does not do so.\n\nAdditionally, the main comparison the authors seem to make is between HRL and naive RL, which does not provide sufficient context to properly analyse their algorithm. Many algorithms are better than applying a classical RL algorithm naively. As such, it is not sufficient to show that the algorithm proposed by the authors is stronger than a naive approach; it would be better to compare the algorithm to either a) the state of the art (SOTA) approach, or b) a more credible approach than the naive one. Experimental evidence would help.\n\nOne point of comparison is Fruit et al. (2017), which is mentioned as another paper which carries out a regret analysis in a HRL setting. Fruit et al. (2017) contains a number of simple numerical simulations; a similar effort here would help.\n\nAnother issue is that the paper is confusing, with systematic grammar errors and typos. The paper would benefit significantly with some copy-editing/proofreading by a native English speaker. For instance, the title should (presumably) read \"Provable Benefits of Deep Hierarchical RL.\" Such errors appear throughout the paper. Fixing them would make the paper much easier to understand.\n\nFinally, although this did not factor into the score I awarded the paper, the terminology used by the authors is confusing, referring to their setting as \"Deep Hierarchical Reinforcement Learning.\" \"Deep Reinforcement Learning\" is a widely used term in industry, referring to algorithms that apply Deep Learning to RL problems, such as AlphaGo or DeepStack. I would encourage the authors to use a different term to describe the setting.\n\nQuestions to the authors: \n\n1) In what way is AlphaGo not doing planning? What is an example of an algorithm that does planning in a standard RL setting? e.g. what would planning look like in Go?\n2) Did you run any experiments/simulations of your work? If not, why not? \n3) Can you elaborate on what a classical RL algorithm would look like that would serve as a proper benchmark to this algorithm?\n4) In your mind, what is the SOTA algorithm for your setting?\n5) What are some simple domains that your algorithm would apply to?\n\n[0]: Morav\u010d\u00edk, Matej & Schmid, Martin & Burch, Neil & Lis\u00fd, Viliam & Morrill, Dustin & Bard, Nolan & Davis, Trevor & Waugh, Kevin & Johanson, Michael & Bowling, Michael. (2017). DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker. Science. 356. 10.1126/science.aam6960. "}