{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a new method referred to Chordal-GCN to optimize memory usage in large graphs. The authors borrow the ideas from Chordal Sparsity theory to first build a client tree. Then mini-batch updates are carried out individually on each clique from the leaves following the GCN loss. The authors add an additional consistency loss between shared node with children cliques. Experiments are carried out in four networks with comparison to several baselines.\n\nStrength:\n1. The authors study an interesting and important problem to reduce memory usage for GCN in large-scale graphs. The usage of chordal sparsity is interesting and innovative.\n2. The authors carry out ablation study on the consistency loss components in the algorithm.\n\nWeakness:\n1. One major concern is that the reduction in memory usage is not large enough to justify the huge increment in running-time. For example, on Cora dataset, the memory is reduced by 4x while the running time is 16x compared to vanilla GCN.\n2. It is not clear why Chordal-GCN can achieve better accuracy compared to vanilla GCN. Since Chordal-GCN serves as approximation for GCN as the authors claim that using entire graph will provide better accuracy. As a result, the vanilla GCN is expected to achieve better accuracy. It would be better if the authors could provide more intuition and explanations.\n3. The evaluation does not take the graph preprocessing into consideration. The authors should report the time and memory taken to carry out the preprocessing steps as well.\n4. For most real-world large-scale industry networks, it is hard to fit the graph into memory. Though the GCN training part could run in distributed way, it is not clear how to efficiently build the clique tree in similar method.\n5. Given the main purpose of the algorithm is to reduce memory usage for large-scale networks, it is expected to see experiments on larger graphs where the large memory footprint becomes a real issue.\n\nDetailed comments:\n1. The description in Section 2.2 is not very clear. It would be better if the authors could provide a more detailed introduction to clique tree and the algorithms used.\n2. For the Chordal-GCN in algorithm 1, for epoch 2 onwards, do we also add consistent-loss when training leaves as well?\n"}