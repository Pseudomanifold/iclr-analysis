{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a framework for combining value function factorization and communication learning in a multi-agent setting by introducing two regularizers, one for maximizing mutual information between decentralized Q functions and communication messages and the other for minimizing the entropy of messages between agents. The authors also discuss a method for dropping non-informative messages. They illustrate their approach on sensor and hallway tasks and evaluate their method on the decentralized StarCraft II benchmark. The paper addresses an interesting problem, and the authors show that their approach gives good performance compared to alternative approaches even when a large percentage of communication is cut off between the agents.\n\nQuestions/Comments:\n- Implementation details (e.g., hyper-parameters, model size) are missing from the paper.\n- The results are average over only 3 seeds, is this enough to compare different algorithms?\n- How should beta should be determined? \n- The authors present results when 80% of messages are cut off. What is the performance of the model when all communication is cut off for comparison?\n- How does the approach work in competitive environments?\n- The experimental results section is not well organized. The authors mention five question on page 6, but it is not very clear with examples/set of experiments address which question.\n- There are many spelling/grammar errors in the paper."}