{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tackles out-of-distribution samples detection via training VAE-like networks. The key idea is to inject learnable Gaussian noise to each layer across the network in the hope that the variance of the noise correlates well with the uncertainty of the input features. The network is trained to minimize the empirical loss subject to noise perturbation. The paper is well written, and the background is introduced clearly.\n \nAs I understand it, the goal of *out-of-distribution sample detection* is to train a deep network that simultaneously generalizes well and also be discriminative to outliers. However, it\u2019s not clear to me why the proposed method server this purpose; empirical results are not convincing either. My major concerns are as follows:\n \nFirst of all, from my intuition, it would be much easier to train deterministic networks than their counterparts with randomness. Empirically, researchers also often observe near-zero training loss for large deterministic networks such as Dense-BC trained on simple CIFAR/SVHN datasets. Especially, in this case, the training goal is simply to map higher-dimensional inputs to lower-dimensional classification categories. That being said, one would expect the variances go to zero at convergence to achieve lower empirical loss in the case of no additional diversity (or uncertainty) promotion terms. \n \nIt is not clear to me how to avoid degenerate solutions at convergence \nwhile maintaining good testing performance with the proposed training strategy. \nFrom the empirical results, it also appears that all models reported might not be fully optimized? \nThe baseline results are significantly worse than those reported in previous work.  \nSpecifically, \nin table 1, the testing accuracy of Dense-BC trained on CIFAR-100 is only 71.6.\nIn table 2, the reported testing accuracy on CIFAR-10 using Dense-BC is 92.4.\n \nHowever, the results of DenseNet-BC (k=12, L=100, table 2) reported in the original paper are:\nCIFAR10  94.0  (also leave 5K examples as validation set)\nCIFAR100 75.9\n  \nMeanwhile, the reported accuracy of WRN-40-4 trained on CIFAR-10 and CIFAR-100 are 89.6 and 66.0, respectively. However, the corresponding baseline numbers in the original WRN paper are much higher,  \nCIFAR-10  95.03\nCIFAR-100 77.11 \n\nCould the authors comment on that?\n\nReferences:\nGao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger.\nDensely Connected Convolutional Networks\nhttps://arxiv.org/abs/1608.06993\n\nSergey Zagoruyko, Nikos Komodakis. \nWide Residual Networks.  \nhttps://arxiv.org/pdf/1605.07146.pdf\n"}