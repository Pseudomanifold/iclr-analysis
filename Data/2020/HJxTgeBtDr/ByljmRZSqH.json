{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper discusses a methodology to interpret models and model outputs for Named Entity Recognition (NER) based on assigned attributes. The key idea is to bucketize the test data based on characteristics of attributes and then comment on effect of the attribute on the model, the task itself or the dataset bias. \n\nThe empirical evaluation is impressive. The authors have constructed a series of experiments to make their case. The paper is well-written and easy to understand, albeit some of the related work seems a little unrelated to the task at hand. While the authors have tried to state that the method is \"general\" and goes beyond NER, I am not sure if that is the case. The creation of attribute buckets is vital for any further analysis, its not clear how the method can be adapted to more general settings unless such attributes and buckets can be created easily (e.g. using domain knowledge). Furthermore, there is only one problem setting considered (i.e. NER), and for the paper is make claim to more general settings, I would expect evaluations on atleast one more problem setting. I would suggest the authors modify the claims accordingly. This is not to diminish from their contributions in the NER. \n\nThe bucketization idea is not something out of the park novel. It is probably something already being used in practice. However, delineating the procedure and suggesting quantifiable statistics and designing experiments to illustrate how these can be used to draw qualitative conclusions is something that is very  interesting and useful to the community as a whole. The strongest part of this paper is the empirical evaluation that allows drawing interesting conclusions, and suggests a methodology to reach that conclusion. While some of the claims made (e.g. regarding dataset biases) probably require further and deeper analysis, this is a good first step that should foster further research and discussion. "}