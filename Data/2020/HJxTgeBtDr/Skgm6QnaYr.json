{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "TOWARDS INTERPRETABLE EVALUATIONS A CASE STUDY OF NAMED ENTITY RECOGNITION\n\n\n\nThe authors propose an evaluation methodology to study the relations between datasets and machine learning models. This methodology introduces the notion of attributes which describes different aspects of the samples and buckets which group samples according to the attributes. The goal is to give a better understanding of the strengths and weaknesses of an algorithm on a specific dataset according to the attributes, as shown on Fig4.\n\nThe article is very dense and the author chose to present the method from an abstract and generic point of view which makes the reading of the article difficult. In the end, the proposition is a formalisation of the simple error analysis which is commonly done when trying to improve a machine learning system. The advantage of the method could be to introduce some metrics to make the error analysis more automatic. These metrics are given in section 4 but here again only from a formal point of view : it is very difficult for the reader to understand how to interpret them and how to use them for a practical case. \n\nThe paper is 17 pages long with the annex : it would better fit a journal publication or the author should select some of the main results to present them in a conference paper. The aspects of the paper related to learning relations is no put forward enough. \n\n2. Related work\n\n2.1 :\n -supplementary exam : unclear\n\n2.2 : \n- methodological perspective : a bit a repetition of introduction\n- task perspective : not very clear, is the main message  \"it important to understand what in the dataset make the model work ?\"\n\n3 Task\n\nSection is too small to be a level 1 title\n\n4 Attributes\n\nfigure 2 : where are the links to levels ?\n\n4.2 :\n\n  * familiarity : test/train distribution should be the same. Fk computer on train set because it is bigger ? it allows to study the impact of the number of occurrences in the training set. Is it more interesting than a learning curve ?\n\n  * multi attribute familiarity : risk of metric explosion ? how to select the attributes ?\n\n  * eq 3 : spearman not defined\n\n* 4.3\n\n  * metric are defined by formula but it is difficult to understand what is the rationale behind each of them and therefore figure out how to interpret them\n\n  * \"Usually where a, b represent two different models and usually model a has a higher performance (by dataset-level metric)\" : unclear\n\n5 Experimental setting\n\nTable3 : \nthe encoding of the model name is not clear\na metric on all the dataset for each model could be computed to decide which one is the best overall\nhow did you choose the tested combinaisons ?\n\n6\\.2\n\nanalysis of Fig 4 : R-eLen does not existe (R-Ele). what is eta ?\n\ntable 4 : spearman\\**r*\\* ?\n\n6\\.4\n\n* CRF vs MLP : \"... a major factor for the choices of CRF and MLP: **if** a dataset with higher \u03b6MF\u2212et, in which longer entities can benefit more from CRF-based models.\" > missing words ?\n\nWriting :\n\n* \"Concretely\" isn't very natural at the beginning of sentences, same thing with \"Formally\", 'Intuitively' \u2026\n\n* in 4.1 : \"We refer to E, P, K as the sets of entities (i.e. New York), entity attributes (i.e. entity length) and attributes values (i.e. 2).\" => \"We refer to the sets of entities (i.e. New York) as E, entity attributes (i.e. entity length) as P and attributes values (i.e. 2) as K\" would be better\n\n* same thing in 4.3 \"we refer to M = m1,\u00b7\u00b7\u00b7 ,m|M| as a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| as a set of **attributes**\" doesn't really work, \"M = m1,\u00b7\u00b7\u00b7 ,m|M| is a set of **models** and P = p1,\u00b7\u00b7\u00b7 ,p|P| is a set of **attributes**\" maybe\n\n* in 4.2 page 5 : \"the familiarity Fk (p1 , p2 ) is a measure with intriguing explanation \u2026\" : not clear\n\n* 6.3 (3) \"Only using character-level CNN is apt to overfit the feature of capital letters.\" **apt** doesnt work here"}