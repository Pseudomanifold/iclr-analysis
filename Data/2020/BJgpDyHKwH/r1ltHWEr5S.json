{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a method for regularizing neural network weights which encourages diversity in a layer\u2019s activations. The regularizer is motivated by encouraging the weight vectors to be uniformly distributed on the hyper-sphere. This is a challenging optimization problem which can get stuck in many local minima. The authors propose a way to avoid many of the known issues with this regularizer by projecting the weight vectors down to a lower dimension where the optimization problem is less susceptible to getting stuck in a local minima. The authors propose multiple ways to project the weight vectors to lower dimensions with random and learned projections. The authors motivate the validity of their approach by providing some theoretical guarantees that by minimizing the hyper-spherical energy (HSE) in the projected space, they are minimizing the HSE in the weight space. Further, the authors explore each approach and compare their proposed hyper-spherical regularizer to prior work and find it performs favorably. \n\nWhile I am not an expert in this specific area, I would vote overall to reject this paper.  While the authors present a through comparison of their method to other hyper-spherical regularization methods, I felt that there was an insufficient comparison to other regularization methods. I do not feel the experimental results demonstrated the benefit of the author\u2019s approach over other HSE regularization methods and more traditional forms of regularization. \n\nI feel the paper could be greatly improved if the authors provided a comparison with more common regularizers. They provide a convincing demonstration that their method performs better than other HSE-based regularizers, but they do not provide much of a reference for how these methods compare in general to other types of weight regularization, so it is hard to tell from the results here that this improvement is significant. \n\nA main argument for the use of hyper-spherical regularization methods is their generality to be used across domains and while the authors did show this in image and point-cloud classification, their comparison was limited only to other hyper-spherical regularization methods, so again it is unclear how the proposed method compares to simpler regularizers such as a well-tuned l1/l2 or orthogonality regularizer. \n\nSpecifically on the CIFAR10/CIFAR100 datasets, the difference in performance between the reported HSE methods is less than the difference in performance observed between minor architecture changes in the Wide Residual Networks paper (a strong model for these problems), so I was not convinced of the benefit of this method of regularization as opposed to minor architectural or hyper-parameter tweaks to standard methods.\n\nIf the authors were to provide a more thorough comparison with simpler forms for regularization and demonstrated that their method provides a significant performance boost, then I could be convinced to change my score. "}