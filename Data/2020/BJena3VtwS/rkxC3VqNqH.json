{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a Visual Task Adaptation Benchmark (VTAB) as a diverse and\nchallenging benchmark to evaluate the learned representations. The VTAB judges whether the learned representation is good or not by adapting it to unseen tasks which have few examples. This paper conducts popular algorithms on a large amount of VTAB studies by answering questions: (1) how effective are ImageNet representation on non-standard datasets? (2) are generative models competitive? (3) is self-supervision useful if one already has labels?\n\nWhat is the size of the training dataset in the source domain?\n\nThe authors need to compare the conclusion obtained with other works. It seems that there is no new founding in this paper."}