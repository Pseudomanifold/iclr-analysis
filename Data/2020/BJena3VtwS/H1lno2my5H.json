{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper essentially addresses the difficult problem of visual representation evaluation. It attempts to find a universal way of assessing the quality of a model's representation. The authors define a good representation as one that can adapt to unseen tasks with few examples. With this in mind, they propose Visual Task Adaptation Benchmark (VTAB), a benchmark that is focused on sample complexity and task diversity. \n\n- Very clear, well written and well structured. (Although not fully self contained in the main body of the paper - 20 pages of supplementary material!)\n- The benchmark tasks are constrained to unseen tasks, which seems obvious but is often violated when evaluating representations\n- It does a good attempt at covering a large spectrum of realistic domains (19 tasks!) to assess generality. \n- Extensive study is conducted, covering the published state of the art methods in each domain. \n- The study leads to interesting finding, such as promising results on self-supervision and negative results on generation.\n\nOverall, I believe the paper is an important contribution as it provides some interesting analysis of the current state of the art for visual representation learning."}