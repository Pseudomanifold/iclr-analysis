{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to augment the VAE objective with two additional terms: (1) a GAN-like objective that ensures that the generated samples are not distinguishable from real samples, and (2) an additional regularizer to make sure that the latent variable that generated the image can be reconstructed from the VAE encoder. \n\nOverall the paper is written very well and was a pleasure to read. In particular I appreciated the differences between the present work and the many VAE/GAN hybrids in section B of the Appendix. However, after reading the paper a couple of times, it was still not clear to me what the \"main point\" of the paper is. To be more specific:\n\n- From the perspective of obtaining good unconditional samples from a VAE-like model, the authors do not compare their approach against like methods like IntroVAE (https://arxiv.org/pdf/1807.06358.pdf) which also adds an adversarial objective to the VAE. Qualitatively at least, it seems apparent that the generated samples here are worse in quality that the IntroVAE work.\n\n- From the perspective of learning more faithful reconstructions, it seems clear from Figure 5(a) that a regular beta-VAE does better. And to me it is not clear why we would want faithful reconstructions?\n\n- The main novelty of the proposed method is in adding an extra term to the generator loss controlled by lambda (equations 1 and 4). In the appendix the authors experiment with varying the lambda parameter and find that generally having lambda > 0 produces \"better\" metrics, though the choice of these metrics are somewhat questionable. It would be great to see the generated samples as \\lambda is varied (to me this is more interesting than seeing the generations as \\beta is varied).\n\nFurther, I take several issues with the authors' point of view regarding the current state-of-affairs in VAEs:\n\n1. \"However, one of their perceived problems is their reconstruction performance.\" \n\nI somewhat disagree with this characterization. Sure, there has been much work on modifying the VAE objective such that the latent variable is not ignored (i.e. posterior collapse), but the point of these works is not to get \"better reconstruction performance\". \n\n2. \"However, having a model that does not over-fit the dataset can be useful, but in this case the decoder of a standard\nVAE should not be regarded as a generative model\u2014that is not its purpose. If we wish to generate\nrealistic looking images we need to imagine the information discarded by the encoder\"\n\nI am not sure I understand this characterization. The decoder is by definition a generative model. Depending on the decoder/encoder capacities (e.g. PixelCNN decoder vs DeConvNet decoder), different types of information will be encoded in the latent space.\n\n3. \"The job of the decoder in a variational autoencoder is to reconstruct the image only using information\nthat can be compressed. Image specific information is ignored. For example, information about the\nprecise shape of an object is probably not compressible. As a result the decoder tends to hedge its\nbets and has a blurry outline.\"\n\nAgain, all of this is dependent on how the encoder/decoder is parameterized. I do not agree that \"information about the precise shape of an object is probably not compressible\". For example see https://hal.archives-ouvertes.fr/hal-01676326/document\n\n4. \"VAEs are often taken to be a pauper\u2019s GAN. That is, a method for generating samples that is easier\nto train than a GAN, but gives slightly worse results.\"\n\nMy view on VAEs is that they are a way of training latent variable models with likelihood training. One potential application of this is to generate samples, but that is not the only (nor the primary) application. \n\n\nFinally, I hope I am not coming across as nitpicking or overly combative, but I am genuinely confused as to the problem that this paper is addressing. I look forward to discussing further among other reviews and the authors during the rebuttal period.\n\n\n\n"}