{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "While spectral normalization is often used to improve generalization by\ndirectly bounding the Lipschitz constant of linear layers, recent works have\nhighlighted alternate methods that aim to reduce generalization error.  This\npaper shows how to implement these \"stable rank\" normalizations with little\ncomputational overhead.  The authors then apply the method to a wide variety of\nclassification and GAN problems to show the benefits of stable rank\nnormalization.\n\nThis is a good paper and can be accepted.  The added value comes from their\nThm. 1, where they detail precisely how to project a real matrix onto one of\nlower srank while preserving the largest k eigenvalues.  The spectral preservation\nk seems to be a new feature of their method. Full proofs and additional results are\nprovided in appendices.  There seems to be enough information to implement\nthe described methods.\n\nThe paper is carefully written and introductory sections do a great job of putting\nthe problem in perspective.  Very few typos (\"calssification\", run a spell check).\n\n--------- Fun to think about ------ here are some extra comments\n\nSome related older introductory approaches could also be quickly mentioned:\n - linear layers represented as \"bottlenecks\" to enforce low rank explicitly\n - or solving in manifold of reduced-rank matrices directly\n\nFor simplicity, they target the same srank r=c*min(m,n) for all layers, even though only the sum\nof sranks is important.  For CNNs with only a few linear layers is there any observable\ndifference by lightly deviating from this?  Does the first linear layer typically contribute\nthe lion's share to the sum of sranks?\n\nIt is interesting that by only addressing the linear layers of deep CNNs they\nare able to see consistent improvements. [i.e. 3 linear layers after 101 CNN layers].\n This makes me wonder whether future work will also address how \"stable rank\"\nconcepts might be extended to the convolutional layers.  As a starting point, spectral\nvalues of the block-circulant matrices corresponding to convolutions have been\ndescribed [ Sedghi et al. \"Singular Values of Convolutional Layers\" ].\n"}