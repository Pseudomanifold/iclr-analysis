{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, a GraphNVP framework for molecular graph generation is proposed. The main difference from the previously proposed models is the use of the invertible normalizing flow idea for the generative model, which doesn\u2019t require a separate decoder for sampling. This architecture is implemented with coupling layers combined with a multi-layer perceptron. The model is evaluated and compared on QM9 and ZINC chemical molecular datasets.\n\nThis method combines a number of existing techniques to obtain a new model for the molecular graph generation problem. The paper is very well written.\n\nI have several concerns with regards to this model and the proposed algorithm:\n1. How many parameters does GraphNVP model have? The coupling layers should have at least O(LN^2R) and that must be multiplied by the number of MLP parameters of the adjacency tensor which I suppose is of order O(N^2R), is this correct? This number must be huge. How can one ensure that such a model does not overfit? Moreover, 100% reconstruction accuracy is rather an indicator that it actually does overfit, isn\u2019t it?\n2. I\u2019m concerned whether the use of dequantization for this particular application is valid. Indeed, images are usually modeled as vectors taking values in [0, 255] and adding a uniform on [0,1] variable actually corresponds to noise. However, the graph adjacency tensor takes either 0 or 1 values and adding a similar uniform variable (potentially scaled by 0.9) is actually more than simply adding noise. Say one value is 0 and added 0.8, while the other is 1 and added 0.1; the transformed variables are now much closer to each other. Therefore, the likelihood of the transformed variables is going to be significantly different from the original one. Although one can indeed recover the original graph from the dequantized one, I doubt that there is a correspondence between two likelihoods extrema. This also contradicts one of the motivations to this paper that this approach uses precise log-likelihood. Could you please comment on this?\n3. I am confused by this sentence: \u201cOur objective is maximizing the log likelihood (Eq. 1) of z over minibatches of training data.\u201d Does this mean that log(p_z(z)) is the objective? If yes, how does this relate to maximum likelihood?\n4. Given the high cost of wet-lab experiments, the runtime of a method for the drug discovery application is much less important than the quality of the obtained results. Is it actually more important to have a sampling time decrease from 100/400s to 4s or does the higher quality of results should matter more? If the latter, are there any other advantages of GraphNVP over other models?"}