{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a new reversible flow-based graph generative model wherein the whole graph i.e., representative attributes such as node features and adjacency tensor is modeled using seperate streams of invertible flow model. This allows training of generative model using exact likelihood maximization over the underlying graph dataset.The model avoids encoding any domain specific heuristics and thus can be applied to any structured graph data. The paper focusses it applicability for molecular graphs. Given that this approach avoids sequential generation of graph, it is faster by an order of magnitude than prior models for molecular generation. Empirical experiments on couple of molecular graph data suggets that GraphNVP approach performs as well as prior approach but albeit without any rule checker.\n\nMy major concern with such invertible models is \"scalability\". Given that flow-based model are required to retain the original dimension its applicability is limited to low dimensional feature vectors. In the case of GraphNVP, this means limited number of node labels as well as edge labels. Additionally, since it limits adjacency tensor, this would lead to modeling graphs with few nodes. However, if integrated with encoder-decder model some of these limitation can be overcome. Given this major weakness and with limited novelty (i.e., extending to adjacency tensor), I am inclined to reject this paper. I shall improve my rating if GraphNVP is applied to general graph structures - synthetic / real.\n\nFew more limitations:\n1. Although paper claims one-shot generation of graphs, in reality it seems otherwise. Since every layer processes only on single node, overall it operates sequentially from one node to another.\n2. Moreover, this same sequential processing yet again limits it applicability to small graphs. \n3. As in MolGAN, the direct generation of adjacency tensor leads to training with fixed size graphs i.e., through the addition of virtual nodes.  It is not possible to train model with variable number of nodes. \n4. As pointed by authors, their model is not node permutation invariant. \n\nClarification:\n1. Are the function 's' and 't' fixed across time ? For QM9 with max of 9 atoms and 27 layers, each atom attribute is processed multiple times. Are they processed using same functionality of s and t ?\n2. Is it possible to model permutation invariance by augmenting the training data using multiple permutation of nodes such as BFS, DFS, degree, k-node (see GRAN) ?\n3. I understand GraphNVP can reconstruct perfectly. But I fail to note the actual significance of such metric. If it reconstruct 100% or not how does it matter ? What matters is unniqueness, validity and novelty.\n4. Can you please compare inference time ?\n5. How difficult is it to integrate validity checker with your generation process ? Can we have some comparison using it ?\n\nMinor:\n1. In eq (2) please use different notation for layer 'l' and node 'l'.\n2. Page 4, penultimate line: So as functions s and t -? To model functions s and t"}