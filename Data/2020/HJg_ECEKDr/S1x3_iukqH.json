{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an algorithm for generating training data to help other machine learning agents learn faster. The proposed Generative Teaching Networks (GTNs) are networks that are trained to generate training data for other networks and are trained jointly with these other networks by back propagating through the entire learning problems via meta-gradients. They also show how weight normalization can help stabilize the training of GTNs. The paper is well-written overall and easy to follow, except for a few typos that can be fixed for the camera ready. The main idea of the paper is quite simple and it\u2019s nice to see it works well. I\u2019m actually surprised it has not been proposed before, but I am also not very familiar with this research area. For these reasons, I lean towards accepting this paper, although I have a few comments that I would like to see addressed for the camera ready version.\n\nThe authors present experiments where they apply GTNs on image classification and neural architecture search. GTN does indeed seem to do better than the baselines for these problems. However, for MNIST and CIFAR it looks like the models being used may not be that good, as it\u2019s quite easy to obtain better performance than the results shown in the paper. I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is. I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case.\n\nRegarding the curriculum used in Section 3.2, my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach, as you have to learn that curriculum. Thus, even though the result the authors present is interesting and verifies that GTNs learn a useful curriculum, I would also like to see curves of how accuracy improves per computational unit (e.g., the horizontal axis could be CPU training time). This would allow us to see whether learning a curriculum this way is in fact practically useful. It may just as well be that it is too expensive and training without it is faster.\n\nThe authors show example images generated by GTNs and, as they also mention, these images do not look very realistic. It would be good to have some explanation/analysis around this. Could it be that these are images that are \u201chard\u201d for the classifier? (e.g., thinking in terms of support vector machines, do these images lie in or close to the margin of the classifier?). I would love to seem an analysis around this and a couple of proposed explanations.\n\nI would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results."}