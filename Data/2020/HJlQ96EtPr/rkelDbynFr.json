{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper proposed a fractional quantization method for deep net weights. It adds XOR gates to produce quantized weight bits compared with existing quantization method. It used tanh functions instead of a straight-through estimator for backpropagation during training. With both designs, the proposed method outperformed the existing methods and offered sub bit quantization options.\n\nThe use of XOR gates to improve quantization seems novel. The sub bit quantization achieved by this method should be interesting to the industrial. It significantly improved the quantization rate with slightly quality degradation. With 1 bit quantization, it outperformed the state-of-the-art. The results seem thorough and convincing.\n"}