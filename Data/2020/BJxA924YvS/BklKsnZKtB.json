{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This submission takes a critical look at knowledge distillation (KD) and asks the important question whether or not it\u2019s just plain regularization in disguise. It shows some interesting evidence to support this and suggests simpler KD variants based on this insight.\n\nI think it\u2019s a great premise, but I will have to land on Weak Reject because of weaknesses in the suggested Tf-KD versions. I think the Tf_KD_reg is just label smoothing, which removes the primary novelty of the paper. I think this is a grave error, but I do not go below a weak reject since I do like the premise and the observations in the first half and think they hold some merit on their own.\n\nPremise:\u2028\u2028\n\nThe premise of the paper to link KD with LSR and the experiments showing evidence that unintuitive or even poor KD can be effective are great. I think it\u2019s a great starting point and I would like to see more in this direction. I am not surprised by this conclusion either, so it is important to point out.\n\nExperimental thoroughness:\n\nThere are plenty of experiments, many models and datasets, with repeated runs and error bars. High marks for this. However, as I will point out shortly, I don\u2019t think the hyper-parameters were necessarily chosen carefully.\n\nTf-KD_reg:\n\nThe main contribution is Tf-KD_reg, so let's take a closer look. Using an \u201calmost correct\u201d teacher looks identical to label smoothing to me. I provide some mathematical arguments that this is true.\n\nEq. (2) establishes what you get if you blend two distributions. Let\u2019s denote this q\u2019_alpha, since we may want to change the mixing ratio alpha. So, if we define u here as another version of Eq. (1), but taking q\u2019_beta, the results will be exactly q\u2019_{alpha*beta}. What this shows is that if we do an alpha-convex blend with q\u2019_beta, then it\u2019s equivalent to doing a alpha*beta-convex blend with the uniform distribution. With this established, note that Eq. (8) can be written as q\u2019_{(1 - a)*K/(K-1)}. So, when that is plugged in, it simply turns into blending directly with a uniform distribution (i.e. label smoothing) with mixing ratio alpha*(1-a)*K/(K-1). Label smoothing has one parameter alpha, while this teacher-free formulation has alpha and \u201ca\u201d - it\u2019s simply an over-parameterized version of label smoothing. Adding temperature \u201ctau\u201d to the mix is even further over-parameterization. The fact that LSR does worse probably just means that the tables are a comparison between two different smoothing strengths of LSR - one which is better. Try changing the alpha in LSR with a factor (1-a)*K/(K-1) and see if you now get the same results. Please let me know if there is a difference I\u2019m failing to see. I may have rushed the derivations, but I think it's intuitively clear that they are at least very similar, if not identical.\n\nTf-KD_self:\n\nI disagree that we should call this \u201cteacher free\u201d, since it does have a teacher that needs to be pre-trained prior to training the student. The only novelty is that the student and the teacher share the same network. This is a very modest novelty, but I agree it has some benefits, especially when integrating this technique into a deep learning framework, where it would be as easy as a flip of a switch."}