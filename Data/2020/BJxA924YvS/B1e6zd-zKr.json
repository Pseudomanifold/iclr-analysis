{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "First off, the title needs changing as it doesn't make much grammatical sense! \"Revisiting\" instead of \"Revisit\" would be much better. The paper is well written, so I'm surprised the title isn't :)\n\nIn this work, the authors examine the effects of using knowledge distillation in two unusual scenarios. Firstly, when the student network is fully-trained but is weaker than the teacher, and secondly, when the teacher network is only partially trained. In both cases, the student benefits, implying that \"dark knowledge\" between class labels is a nonsense, and all it's really doing is label smoothing.\n\nI must admit, I really like these experiments, and I'm broadly a fan of this paper. I do have a few comments.\n\nIt's important to note that the Hinton et al. paper builds heavily on an earlier work (https://arxiv.org/abs/1312.6184v5), which I would recommend citing. Also, I don't like the use of \"surprising\" in the prose, but that's just down to taste.\n\nSection 2 is good, but it don't understand why there is a separate list of student networks and teacher networks. Why not just put every combination of them? Error bars are good. For Table 1 why does ResNetXt get ResNet18 as a student when the other networks didn't?\n\nTable 10 in the appendix is concerning. I don't see why there are different temperatures and alphas in use. At least with temperature, there is only one case where it isn't set to 20 (why is it 6 for ResNeXt29-ResNet18?). Why does alpha differ so heavily across experiments? This creates the (hopefully unintended) image that your results have been cherry picked. I would strongly recommend either using the same alpha/temp for each experiment, or providing a strong justification why it differs between networks.\n\nThe comparison between label smoothing and KD is neat.\n\nThe first method suggested (Tf-KDself) is *exactly* what the authors use in Born-again nets (https://arxiv.org/abs/1805.04770). I notice that this paper has been cited briefly in the small related work section at the end, but this really should feature earlier as it's not a great look to pass off an older method as something new (although this may be unintentional).\n\nThe difference between the virtual teacher approach and LSR is so subtle that I'm surprised there is much difference in results. The only comparison that I can find is the table in Figure 6 but this is missing error bars (I am using error bars and STDs interchangeably). I would like to see a comparison with error bars (although this could be on say, CIFAR-100 if ImageNet is too expensive).\n\nOn a related note, the table in Figure 4 could benefit form error bars.\n\nPros:\n- Experiments in Section 2 are novel and interesting, as far as I am aware\n- Experiments are conducted across different datasets and networks, suggesting generalisation\n- Paper is well written (apart from the title. Please change!)\n\nCons:\n- The use of different hyperparameters across experiments is concerning\n- Tf-KDself is just a rebranding of Born-Again-Neural networks\n- It is not clear how/why the virtual teacher surpasses label smoothing\n\nI propose the paper should receive a weak accept. It is well written (but please change the title), interesting, and experimentally satisfying. However, I would caveat this by strongly recommending that the authors address my concerns regarding hyperparameter changes. It would be a real shame if this was a case of cherry-picking experiments to suit a hypothesis.\n"}