{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper connects knowledge distillation and label smoothing regularization. With two sets of well designed experiments, train large network with small teacher or poorly trained teacher, the authors claim knowledge distillation has a strong regularization effect. The authors then show that both knowledge distillation and label smoothing loss can be written as linear combination of supervised learning loss and KL divergence loss. Finally, two \u201cteacher free\u201d methods are proposed, use the same network as student and teacher, and a variant of label smoothing.  \n\nI am generally on the fence for this paper. I think it makes sense to view knowledge distillation as a regularizer. It is not a surprising finding but I have not seen it in previous papers. On the other hand, there are multiple ways to interpret knowledge distillation, including categorical relationship, loss smoothing, and regularizer. It is hard to convince me regularization is the main drive because in original KD, even without the supervised part H(q, p), it still works reasonably well. It looks to me this paper is more about label smoothing/regularization than knowledge distillation. But since label smoothing has been shown to be an effective regularizer in previous works, it is difficult to say how much practical value is there. \n\nAbout the two proposed teacher free methods, the first one looks like Born Again Network, the second looks like can be achieved by tuning the label smoothing parameter \\alpha. \n\nA relatively minor issue is how the hyper-parameters (\\alpha and there should be a temperature parameter T) are decided for knowledge distillation.\n"}