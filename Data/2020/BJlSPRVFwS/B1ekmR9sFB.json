{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper analyzes convergence of asynchronous methods on general non-smooth and non-convex functions (typically arising from deep leaning). Stochastic sub-gradient asynchronous methods are of particular challenging when coupled with complicated hardware behavior of modern NUMA architecture. To validate the analysis, and study the impact of momentum, variable partitioning, numerical experiments on deep learning training are given.\n\nA major concern to me is that the assumptions made in the analyze may be too restrictive. For example, Assumption 3.1.1 regarding the unbiasedness of the stochastic sub-gradients may not hold for since the Clarke sub-differentiation is not additive. The Clarke sub-differentiation of |x| and -|x| are not included in their average which is zero, therefore the Assumption 3.1.1 is not true for tame functions (as cited in the paper) . If this assumption were not true, all the proof arguments based on Martingale differences may not be follow to prove Theorem 3.1. \n\nThere are a few typos which make the paper hard to understand. Is the momentum variable  u_i^kc in Algorithm 1 a central variable, as x_i? It seems to be yes since the update needs a lock. But why there is an kc on its index, which is not the case for x_i? \n\nIn terms of numerical results, the system specification is not so clear to me why it includes a diverse asynchronous system settings. Even though Figure 1 shows convergence in terms cross-entropy loss, which is not directly related to validate Theorem 3.1 since it is not clear what is going on with x_t.\n\nThe paper would be of great interest if the global shared memory asynchronous model is made more precise in the main body of the paper. At a first glace, it is not clear what is the benefit (or what insights) using this model to analyze these algorithms, compared to simplistic models in literature. "}