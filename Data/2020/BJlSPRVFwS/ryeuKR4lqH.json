{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a convergence analysis for asynchronous optimisation in the case where non-smoothness imposes the use of sub gradients and non-convexity (combined with asynchrony) introduces more challenging staleness (delay) issues than in the convex case.\n\nstrength:\nthe paper tackles one of the hardest settings to be analysed in distributed optimisation, and packages it in a readable continuous-time framework.\n\nweakness: \nas far as I went into the details, I couldn't understand how the authors tackle one the biggest problem in non-convex + asynchrony: coherence of gradients. \n\nSpecifically, my sole question to the authors is how does their analysis take into account (sub)gradients that are delayed *and* not in the same half-space as the current non-biased estimator of the gradient ? These (sub)-gradients will act in an almost malicious manner. (as usual in distributed computing, where asynchrony can be modelled by a malicious scheduler).\nMaybe my reading made me miss where this is handled, if a precise pointer is given I will upgrade my score."}