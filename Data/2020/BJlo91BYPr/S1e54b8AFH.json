{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies reward inference from demonstrations of irrational experts. More specifically, a set of semantically meaningful experts' behaviors is considered which is derived from modifying the Bellman update. The quality of reward inference from these different experts is measured by two different scores and analyzed regarding properties of the demonstrator. The main finding is that irrationality can be helpful for inferring rewards.\n\nThe problem addressed by the paper is very interesting and relevant to a reasonable part of the community but I argue that the paper is not ready for publication in its current form. In particular, the experiments are too limited to thoroughly support the claims (this requires at least the consideration of more different environments; and to really make the paper impactful some parts of the \"future work\" section should be conducted) and the write-up should be improved from Section 3 onwards to provide more clarity.\n\nA few more detailed points:\n* I see the paper in its current form as a theoretical study on reward inference from irrational experts. To provide insights here, and as these only involves simulations, a rich set of different MDPs should be considered. In the current form it is unclear how general the results are (although I assume that certain findings hold more generally but there is no supporting evidence for that). Maybe even formal theoretical insights can be derived.\n* Regarding the Bellman update, on the RHS it should be $V_i(s')$.\n* Regarding the presentation of the irrational experts. Is there a simpler way of presenting the irrational experts through modified MDPs that the expert tries to solve optimally? Are all updates actually convergent, in particular the pessimistic one?\n* Please provide a formal specification of the reward model used in experiments.\n* Please provide a describe how you do the computation of the posteriors $\\theta$ in the main paper (or at least provide a forward reference to the appendix). Which prior on $\\theta$ are you using (put in main paper)?  \n* What is the precise nature of $\\xi$? I would assume it is a sequence of state-actions but that is not consistent with the definition of the log-loss which suggests it is only actions.\n* Probably more interesting than the log-loss and L^2 loss is the actual performance of an optimal policy using the inferred reward parameters. It would be good to report this numbers. How do irrational experts compare looking at this metric?\n* The discussion of related work should be extended. For instance, R. Shah et al.'s paper \"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference\" should be discussed in more detail and similarities and differences clarified.\n\nMinor comments and suggestions for improving the paper:\n* The definition of Boltz in 2.2.2 can be made more clear. Maybe define the function using actions to connect to the above equation.\n* Correct \"update on the the trajectory $\\theta$\".\n* Please check the usage of $\\theta$ and $\\theta^*$ and make it consistent. I think it would also help to make the log-loss and L^2 distance not look like a function of $\\theta$.\n* Figure 4/5: Explain what we see. I guess the black square is the starting position?\n* Regarding figure 5: Comparing different $\\beta$ values seems more sensible if the norm of $\\theta$ is normalized. "}