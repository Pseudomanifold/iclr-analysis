{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is consider dynamic discrete choice models. It shows in an important class of discrete choice models the value function is globally concave in the policy, implying that for example policy gradients are globally convergent and are likely to converge faster in practice compared to fix-point approaches. \n\nThe paper is very well written and structured. It present convergence results together with a sanity check implementation. However, as an informed outsider, I am also a little bit confused. As far as I understand, Rust (1987) is also using gradient descent. Indeed the problem might not be convex/concave and hence this might get trapped in local minima. Moreover, Ermon et al. (AAAI 2015) have already shown that Dynamic Discrete Choice models are equivalent to Maximum Entropy IRL models under some conditions. Then they provide an algorithm that is kind of close (at least in spirit) to policy gradient. The propose to \"simultaneously update the current parameter estimate \u03b8 (Learning) while we iterate over time steps t to fill columns of the DP table (Planning)\". Indeed, this is still not giving guarantees, but the together with Ho et al. (ICML 2016) it suggests that the take-away message \"use police gradients\" for dynamic discrete choice models is actually known in the literature. This should bee clarified. Generally, the paper is providing a lot of focus on the economics literature. While this is of course fine, the authors should clarify what is already known in the AI and ML literature (including the work described above). \n\nNevertheless, the proof that there are convergent policy gradients for some dynamic discrete choice models appears interesting, at least to an informed outsider. However, this results heavily hinges on e.g. (Pirotta 2015). So the main novelty seems to be in Sections 4.3 and 4.4. Here is where they make use of their assumption. So, the only point, in my opinion, that should be clarified is the usefulness of the considered class. For an informed outsider, this is not easy to see. "}