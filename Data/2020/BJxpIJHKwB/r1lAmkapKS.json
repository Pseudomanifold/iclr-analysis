{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper suggests a method for generating classifier weights for few-shot learning. A multi head self attention module is used on both the support-set and the union of the support-set and a specific query to extract features for the generator. A loss term is added ensuring the support set and the query can be reconstructed from the generated weights (information maximisation).\n\nMajor points:\n1. Missing details and/or reference for the multi-head attention. Even together with the appendix it\u2019s not self contained.\n2. I suspect that the learned model is not really adapting to the specific query as claimed, for the following reasons:\n    a. The \u201crandom shuffle in class\u201d experiment (given the typical accuracy STDs) suggests that the generated weights are not depended on the query.\n    b. lambda3 is 3-order-of-magnitude lower than other loss terms.\n    c. Also, according to the ablation study (lambda3=0) its contribution seems to be marginal at best.\n3.  MetaOptNet is compared to only for tiered-imagenet where it is outperformed by AWGIM, but is not compared to for mini-imagenet where MetaOptNet achieves better performance.\n4. Why the attentive path alone is not enough? It takes into account both the context (support-set) and the query.\n5. It is not clear why X^cp and x^ap (The support-set and query after the attention modules) are reconstructed and not the original X and x_hat."}