{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new model for few-shot classification. Their meta-learning approach falls into the category of weight generation models that, given a new classification task, generate the weights of a linear layer on top of learned embeddings that can be used for classifying query examples of that task. The weights of their model are meta-learned via a sequence of training tasks.\nMore concretely, in each training task, the support and query examples are first embedded using into a d-dimensional space. Then, the support set together with a given query example are processed to form a hidden vector, via two \u201cpaths\u201d. This hidden vector, is fed into a weight generator which outputs the linear weights that will be used for classifying the query example. Both processing paths contain a projection layer, followed by self-attention on the support set, and the second path additionally performs cross-attention for the query example on the processed support set. They observed empirically that when training this system simply using the cross-entropy query loss as usual (with the loss computed using the generated weights for each query) did not lead to significant gains. They therefore added terms in their loss function to maximize the mutual information between the generated weights and the support and query sets. The resulting objective is comprised by terms that maximize the cross-entropy on the support and the query sets and reconstruction terms for the support and query images. \n\nPros\n[+] Interesting new objective for training a few-shot classification model\n[+] Thorough ablation analysis\n[+] Seems to perform well on standard benchmarks\n\nCons\n[-] Lacking sufficient details on LEO. Specifically, the paper largely emphasizes the comparison of their method with LEO. For example, certain of their ablations are described based on their relationship to LEO. Given this, it would be useful to give more background on that method. While it is described in related work, I found that description too high-level to understand LEO\u2019s training objective, its mechanism for conditioning on the support set, etc.\n[-] Some inconsistencies. For example, the last paragraph of section 3.3 suggests that LEO is similar to using only the first two terms of Equation 13 (ie. lambda_2 = lambda_3 = 0). However, later in the paper, they say that the ablation called \u201cGenerator conditioned on S with IM\u201d (which includes the cross-entropy on the support set and reconstruction of support set terms only) is \u201cequivalent to LEO to some extent\u201d. This, however, entails using the second and third terms of Equation 13, seemingly contradicting the previous claim of which version of their method corresponds to LEO. It would be good to clear up this confusion.\n[-] I found the names of the ablations hard to understand. For example, \u201cGenerator conditioned on S only\u201d is the same as \u201cGenerator conditioned on S only with IM\u201d except that the reconstruction term for the support set is omitted? Or is the full objective used, but with the weights w having been generated using only the contextual path? It seems that which path(s) are used to generate the weights is orthogonal to which terms of the loss function are used for optimization. So it would be really useful to clarify exactly what each of the ablations is doing.\n\nAdditional comments / questions:\n- Is the embedding network pre-trained as in LEO (and held fixed during the meta-training?), or are all the weights meta-learned end-to-end?\n- Do the rest of the models that are being compared against also re-train on the union of the meta-training and meta-validation sets? This procedure does not seem standard and I\u2019m wondering if the comparison is fair. \n- A recent related work: \u201cCross attention networks for few-shot classification. Hou et al. NeurIPS 2019.\u201d Their method also modifies the feature extraction of the support set differently for classifying each given query example, motivated by the need to possibly highlight different regions of the support examples depending on the given query. They also use attention to accomplish this. Given these similarities, it would be useful to compare to them.\n- This paper largely emphasizes comparisons with LEO. Along those lines, can a version of the proposed approach also tackle few-shot regression as LEO does? \n"}