{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission proposes AWGIM, a few-shot learning method derived from a mutual information perspective. An architecture is proposed that, conditioned on a support set and a query example, produces a set of weights that should make accurate predictions for the query. Additional terms to the learning objective are added, derived from an information maximization perspective. These terms correspond to reconstruction terms of the (features of) the support and query inputs, as well as a prediction of the support set labels. Results on miniImageNet and tieredImageNet are presented, with comparisons to prior work and an ablation study.\n\nThough the results of AWGIM are pretty good, I have several concerns about this submission:\n\n1. The results on miniImageNet are actually worse than those of MetaOptNet (64.09% and 80.00% for 1 and 5 shot, results which somehow are missing from Table 1). So the claim of state-of-the-art performance is inaccurate.\n\n2. I find the motivation of the method to be lacking. The authors claim that \"estimating the exact and universal classification weights from very few labeled data in the support set is difficult and sometimes impossible\". First, not much more is provided to support this statement. Second, I would instead argue that producing a set of weights *per query* is even more difficult. Indeed, having different weights for different samples suggests more capacity than having a single set of weights, which would suggests an even more (not less) difficult overfitting situation to address. \n\n3. The mathematical foundation of the method is equally unconvincing. At a high level, the authors could better motivate why the mutual information perspective is the right way of approaching the problem. In particular, the end result is simply the addition of reconstruction terms of inputs from the support and query sets and the label of the support set. Moreover, these terms are weighted by hyper-parameters, making the method even less directly related to the original information maximisation objective. Overall, I find this narrative to add unnecessary (I'm tempted to write, inflated?) complexity to the contribution.\n\n4. The writing is not great, and I've find several typos, which a more thorough proofreading could have caught (I mention some I found at the end of this review, but I stopped taking such notes after section 3.2.1).\n\nThis paper is not without value. Putting aside the motivation which I personally find lacking, the final learning objective is interesting, and seeing that the ablation study find that the reconstruction terms are useful is a surprising and interesting result. Assuming a complete rewrite of the paper, it's possible that this work would become much more convincing. On the other hand, I do have some potential concerns about the experiments, mainly that 1) the confidence intervals of Tables 1 and 2 are suspiciously small for AWGIM (an explanation for why that is would be valuable) and 2) I'm wondering whether hyper-parameters were tuned separately for each models in the ablation study (given that there are many variants compared, I doubt they were, though that would be a fairer ablation).\n\nBecause I think a thorough rewrite would be required for this submission to be strong enough, I unfortunately doubt that I'll be changing my score (though I'll obviously read and consider any rebuttal from the authors).\n\nFinally, here are some minor comments:\n\n- \"data receives\" => data has received\n- \"Most of successful\" => Most successful\n- \"as weights generation method\" => as a weights generation method\n- \"need inner update\" => \"need inner updates\n- \"casted to a\" => cast as a\n- \"refer attention to\" => refer to attention as\n- \"of stochastic process\" => of a stochastic process\n- \"achieved be attention\" => rephrase\n- \"product of marginal distribution\" => product of marginal distributions\n- \"that we need to predict\" => and we need to predict\n- \"is show in\" => is shown in\n- \"support set to is\" => rephrase\n"}