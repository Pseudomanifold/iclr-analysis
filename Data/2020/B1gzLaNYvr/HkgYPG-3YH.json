{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, the authors proposed an algorithm for identifying important inputs for the time-series data as an explanation of the model's output.\nGiven a fixed model, the authors proposed to put an auto-encoder to the input of the model, so that the input data is first transformed through the auto-encoder, and the transformed input is then fed to the model.\nIn the proposed algorithm, the auto-encoder is trained so that (i) the prediction loss on the model's output to be small, (ii) the reconstruction loss of the auto-encoder to be small, and (iii) the transformed input of the auto-encoder to be sufficiently sparse (i.e. it has many zeros).\n\nI am not very sure if the proposed algorithm can generate reasonable explanation, for the following two reasons.\n\nFirst, the auto-encoder transforms the input into sparse, which can completely differ from any of the \"natural\" data, as shown in Fig1(b).\nI am not very sure whether studying the performance of the model for such an \"outlying\" input is informative.\n\nSecond, it seems the authors implicitly assumed that zero input is irrelevant to the output of the model.\nHowever, zero input can have a certain meaning to the model, and thus naively introducing the sparsity to the model input may bias the model's output.\n\nI think the paper lacks deeper considerations on the use of sparsity.\nThus, to me, the soundness of the proposed approach is not very clear to me."}