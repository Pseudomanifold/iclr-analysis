{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a single image super-resolution method. The motivation is real and practical.  In almost all existing methods the low-resolution training images are artificially synthesized using the available high-resolution images with bicubic downsampling, which allows LR images to carry more information than real demosaic-upscaled LR images. This mismatch between training and realistic\ndata hinders the practicability of such solutions (not limited to mobile scenarios). \n\nYet, the proposed remedy in the paper is not an answer either. It simply adds a noise image to the HR image, passes it through a convolutional network to construct an LR version, which is then fed into a deconvolutional network. Both these networks are build by residual blocks. Even worse, the paper simply uses zero-mean Gaussian random noise with a standard deviation of 0.05 to simulate the distortions. This is myopic and questionable. \n\nThe introduced dataset is very small, it contains only 447 pairs of images, with an additional 206 images as reference images. These pairs consist of a normal image (considered to be noisy) and a computed image by aggregating 20 images (considered to be clean) using a Blackberry Key2. In other words, the dataset only models a single device and a single camera lens. For practical purposes, this is not much different than using a bicubic sampling and adding noise drawn from a fixed distribution.  \n\nThe paper fails to provide comparisons with the top methods in the NTIRE 2019 single image super-resolution challenge leaderboard. "}