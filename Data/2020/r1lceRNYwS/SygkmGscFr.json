{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\nThe paper applies an existing self-supervised method (Local Aggregation) on videos that was initially developed with images. The paper mainly studies how effective the video representations are for 2 downstream tasks (action recognition and image recognition) with different video architectures and frame sampling strategies.\n\nStrengths\n1) Their method achieves a significant improvement in performance over baseline self-supervised video representation learning methods on 2 datasets: UCF101 and HMDB51\n2) The paper presents though experiments with different architectures and training settings in order to be fair to the baselines.  \n\nWeakness\n1) Comparison to other self-supervised methods trained on Kinetics data but tested on ImageNet are missing. The problem is that the authors state that the features learnt with VIE are competitive for ImageNet classification but there are no other baselines to compare their approach with. Most self-supervised methods on video do not provide results on ImageNet classification. So it is understandable why they don't have these metrics but without them it is difficult to place the importance of their method for the ImageNet classification task.\n2) There are some claims in the paper that remain unsubstantiated. The only form of validation for these claims is the final performance of the downstream task which does not necessarily validate the claims in the paper. For example:\n\ni) \"Benefit from long-range temporal structure\" In this section the authors train the model with temporally clipped videos by dividing the videos into N bins. They compare it with models trained on full videos and conclude that because performance with entire video on downstream tasks is higher, their training mechanism is capturing long range temporal structure present in videos. It is unfair to compare these two models as they both are trained with different amounts of data. The simplest explanation seems that the model that sees full videos sees more data. It is not clear from this experiment if the long-range temporal structure is benefiting the task or more data is benefiting it. \n\nIt is difficult to isolate the source of the performance improvement in this manner. But it might be more easier to test performance on a downstream task that requires some temporal reasoning[3, 4] and check if the embeddings are indeed capturing the structure.\n\nii) \"However, the relatively high performance of the static and two-stream models shows that VIE can achieve useful generalization, even when train and test datasets are as widely divergent as Kinetics and ImageNet.\" This claim is based by comparing the supervised learning and self-supervised learning on Kinetics and then testing on ImageNet. The last layers are usually learning more task-specific features which is why transfer learning performance is worse for Conv5 (Table 5 in Appendix). But Conv4 features are still generalizing well (41.13 is best for VIE-TRN) for the supervised learning models. These are still better than all the ImageNet numbers of VIE-TRN. I see that VIE-TwoStream numbers are much better but he comparison with supervised learning counterparts is not fair as VIE-TwoStream is 2 different models clubbed together and all the Supervised Learning baselines are single model.\n\nDecision\nTheir method showcases the power of the Local Aggregation training algorithm for unsupervised representation learning. Their approach scales well to videos and their paper has interesting experiments. But in the present version there are some unsubstantiated claims in the paper.  \n\nMinor comments:\n\n1) \"has remained a significant artificial intelligence challenge\" - Too informal a description for an academic paper.\n2) \", showing substantially improving on the state of the art \" - improvement over\n3) \"early modern\u201d - please rephrase.\n4) Use of the term TwoStream is confusing as most literature in action recognition[1,2] refers to using RGB and flow as two streams. Two-stream models in this paper refer to using 2 base networks for extracting embeddings. As the two-stream terminology is already used in many papers in action recognition, please rephrase this to something less ambiguous.\n\nReferences\n[1] \"Two-stream convolutional networks for action recognition in videos\" Karen Simonyan, Andrew Zisserman.\n[2] \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\" Joao Carreira, Andrew Zisserman.\n[3] \"The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense.\"1\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic.\n[4] \"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset\" Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray.\n"}