{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents how we can learn video embeddings with unsupervised learning, proposing Video Instance Embedding (VIE). With experiments on public benchmark set and retrieval demo, the authors argue that the proposed method is effective.\n\n1. The authors claim that 'inherently different' videos pushed far apart, while similar videos are aggregated. How does this 'inherently different' or 'similar' is defined? It seems based on the visual signals only, as it relies on unsupervised learning. Some related videos may be visually different, but still may be quite relevant to each other, and vice versa. It may be nicer to discuss about what 'similarity' this paper is trying to learn, as basically this model is for video metric learning.\n\n2. It may be interesting to compare against other self-supervised learning approaches. For instance,\nObjects that Sound (Relja Arandjelovi\u0107, Andrew Zisserman) https://arxiv.org/abs/1712.06651\nIn this paper, the authors propose a model capturing audio-visual correspondence.\n\n3. It will be also interesting to see experiments on larger dataset such as YouTube 8M or Sports 1M. These dataset do not provide access to the pixels though; so I am not sure how hard it is to extract data from YouTube and process them. The action is an area that this kind of model can take advantage mostly, but many other videos may not like that. So, evaluating on general video dataset like YouTube 8M may be useful.\n\n4. The embedding size is not shown. It'd be nicer to see comparison with different embedding sizes.\n"}