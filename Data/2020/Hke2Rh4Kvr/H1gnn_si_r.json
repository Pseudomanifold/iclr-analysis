{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThe authors interpret the standard stacked training of deep generative models as a boosting algorithm.  They provide some theorems about the likelihood of the resulting model, including identifying conditions under which adding layers increases a lower bound on the likelihood.  They perform some experiments demonstrating that the bound increases in practice, and showing some improvements in the likelihood obtained by stacking a Gaussian mixture model on top of recent sophisticated VAEs, comparing this with a number of other algorithms.\n\nThe fundamental novelty in this paper appears limited to me.  The training algorithm abstracts the training of a deep belief network in an obvious way.  The theoretical treatment seems closely related to [1].   The authors of [1] also commented that this method could be viewed as a boosting algorithm.  \n\nThe improvements found by through their application of stacked training with a particular combination of models appear to be small.  Also, if I understand correctly, only an approximation of the test log-likelihood was used in the quantitative results.\n\n\n\n\n\n[1] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"A fast learning algorithm for deep belief nets.\" Neural computation 18.7 (2006): 1527-1554."}