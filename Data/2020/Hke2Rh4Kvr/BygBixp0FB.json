{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a novel approach of cascaded boosting for boosting generative models. \nThe authors derive a decomposable variational lower bound of the boosted model, which allows each meta-model to be trained separately and greedily. All in all, I like the idea of cascading two model for better performance and the VB lower bound derivation fuse the model training nicely. Detail comments are as following.\n\n1.\tWould a global correction step be possible after greedily learned each weak learner? \nWill this step help improve the model performance?\n2.\tVB is notoriously for it local minimal. More experiment details such as how to initialize the model and how will it affect the model performance need to be provide.\n3.\t Like section 4.4 a lot. It will be better to provide metrics other than lower bound for the experiment part.\n"}