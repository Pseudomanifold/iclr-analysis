{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method for adversarial defense based on generative cleaning.\n\nThe paper does not follow any of the best practices for evaluating adversarial robustness, e.g. in these two papers:\n\"On Evaluating Adversarial Robustness\" https://arxiv.org/abs/1902.06705\n\"Obfuscated Gradients Give a False Sense of Security\" https://arxiv.org/abs/1802.00420\n\nFor instance the paper does not use a large number of PGD iterations (10 is too small) and does not check that accuracies go to zero for large epsilon (an important sanity check to reveal gradient masking). In the one place where a larger number of attack iterations is used (100 for BPDA) the gap with adversarial training mostly vanishes.\n\nIn the absence of these best practices it is impossible to assess the validity of the results, so the paper should be rejected."}