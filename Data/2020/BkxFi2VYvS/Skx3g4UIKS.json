{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission introduces a semi-supervised method using auxiliary network for improved semantic segmentation. The authors modify a previous work as their main network architecture and use another small network as auxiliary branch. The framework can work in a semi-supervised setting since they can use confidence map to annotate unlabeled images to train the network. \n\nI give an initial rating of weak reject because (1) novelty in architecture design is trivial (2) the way of using unlabeled images is not new (3) experiments are not supportive (3) performance is not comparative to state-of-the-art. I will illustrate more as below.\n\n1. Architecture is not novel. As I mentioned, the authors adopt DSNet-fast as their main branch with minor modifications. And they use a simplified DeepLab architecture as their auxiliary branch. There is no ablation study or strong motivation to design this network. \n\n2. Using an auxiliary branch to measure confidence and try to get more labeled data is not new. There are many previous literature exploring similar ideas. Please clarify the differences between the literature and this submission in either introduction or related work section. \n    (1) Universal Semi-Supervised Semantic Segmentation, ICCV 2019\n    (2) Improving Semantic Segmentation via Video Propagation and Label Relaxation, CVPR 2019\n    (3) Semi-Supervised Semantic Segmentation with High- and Low-level Consistency, Arxiv 2019\nEspecially for (2) and (3), (2) use unlabeled images with confidence map, (3) has two branches as well. Both of them has results on Cityscapes. \n\n3. In table2 , speed comparison is not intuitive. The methods are evaluated on too many kinds of hardware and can not be directly compared. It is good to re-evaluate all algorithms on a single hardware, or remove this column. \n\n4. The experiment setting is too simple. Divide existing training data into half will make it hard to compare with other approaches. All the experiments in this submission can only show you are better than the baselines, but can't convince me your approach actually works. Because the training distribution is similar, it is easy for the auxiliary network to generalize. I would suggest the authors to use all the training data, and bring additional unlabeled images into picture to see what will happen. In addition, if you use all training data, you can make fair comparisons to many literatures and demonstrate the effectiveness of your approach.\n\n5. What is \"ours\" and \"ours with proposed network\" in table 5, please clarify."}