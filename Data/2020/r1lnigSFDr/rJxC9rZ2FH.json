{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces two novel techniques to help long term signal propagation in RNNs. One is an initialization strategy which uses inverse sigmoid function to avoid the decay of the contribution of the input earlier in time and another is a new design of a refine gate which pushes the value of the gate closer to 0 or 1.  The authors conduct exhaustive ablation and empirical studies on copy task, sequential MNIST, language modeling and reinforcement learning. \n\nThough the experiment section is solid, I still vote for rejection for the following reasons:\n\n1. The writing in the description of the UGI and refine fate is not clear.\na. The authors compares UGI to standard initialization but where is the standard initialization? I do not see \"standard initialization\" clearly defined in the paper.\nb. I am not convinced how the UGI gate help avoid decaying of the input. There is a proposition 2 trying to explain some part of the mechanism of UGI. But the proposition is never proved anywhere and I am not sure why this proposition is important. More explanations are needed. Also this proposition is far away from the place the authors introduce the UGI. The authors may want to refer it in the place introducing UGI.\nc. Similar to proposition 2, proposition 3 is not explained and proved in the paper. It is hard for me to analyze the importance of these two propositions. Overall, propositions 2 and 3 look isolated in the section.\nd. Proposition 1 looks like a definition. Not sure why the authors name it as a proposition.\n\n2. Even though the title of the paper is \"improving the gating mechanism of recurrent neural networks\", the authors try to solve signal propagation problems. It is unclear why \"gate\" is important. Maybe other designs of the recurrent neural network can satisfy better the desiderata the authors want. Based on my limited knowledge, the initialization the authors mention (saturation) is exactly from the need of using a sigmoid gate. The importance of using \"gate\" should be discussed.\n\n3. The authors shrink the space before and after headings in the paper. I think this is not allowed in ICLR. It would be better that the authors correct the spacing in the revised version.\n\n\nMinors:\n1. page 1 second paragraph: repeated \u201cproven\u201d\n2. page 1 second last paragraph: \u201cdue\u201d -> \u201cdue to\u201d \n3. page 2 second last paragraph: repeated \u201cbe\u201d\n4. page 2 Equation (4) and (5): using some symbols like \\odot for element wise multiplication will be good for the readers.\n\n\n"}