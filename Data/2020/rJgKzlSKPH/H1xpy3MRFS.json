{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a regularization strategy for training a neural network classifier using full weight precision that can then be quantized to a low-bit fixed-point representation. The main claimed contributions are\n\n- A solution to the problem of the soft quantization of batch normalization layers, which revolves around regularizing the (folded) shifting coefficient values towards their (post-training) fixed-point quantization.\n- An exponential schedule for the regularization hyperparameters.\n- An overall model that outperforms other quantization approaches.\n\nMy assessment is that the paper is slightly below the acceptance bar. As an outsider to the low-precision DNN literature, the proposed soft quantization strategy for batch normalization layers looks interesting, but the submission suffers from clarity issues (see clarity-related comments), and the evaluation is insufficient to support some claimed contributions (see evaluation-related comments).\n\nClarity-related comments:\n\n- The spacing between paragraphs has been removed to satisfy the soft 8-page limit. This has a negative impact on the submission\u2019s readability.\n- \u201cSince BN layers operate on different statistics after training [...]\u201d The use of the term \u201cstatistics\u201d in this statement is imprecise. Unless there\u2019s an input distribution shift between training and evaluation, shouldn\u2019t the \u201cstatistics\u201d BN layers operate on be the same? Maybe the authors are trying to convey that the way in which these statistics are approximated (using a mini-batch during training vs. using a running average during evaluation) is different?\n- The submission claims to accommodate pure fixed-point models, but the soft quantization of biases is swept aside in the presentation: \u201cSince additions play a subordinate role for complexity, we focus on weight multiplications.\u201d Can the authors clarify whether and how soft quantization is applied to biases?\n- Soft quantization of multiplicative factors is handled in two distinct ways in the model: weights are quantized using a symmetric uniform function, and batch normalization scaling factors are quantized using a logarithmic function. Why are they handled differently?\n- \u201cFurthermore, we determine the layer-wise step-size on pre-trained weights [...]. In the next chapter, however, we see that the actual step-sizes are learned within the batch normalization component.\u201d I have trouble reconciling these two statements. They give the impression that the step size is both determined post-training *and* learned during training. In fact, Equation 12 suggests that the step size is learned. Can the authors clarify?\n- \u201cThis also prevents the regularization parameter from being too high.\u201d I don\u2019t understand what \u201cregularization parameter\u201d refers to. Is it the lambda values (Equation 13), or is it the R values (Equation 13)?\n\nEvaluation-related comments:\n\n- I don\u2019t understand the reasoning behind the bolding of entries in Table 2. In the MNIST section, why is the Add-Net entry bolded but not the TWN and SGM entries, even though they share the same error rate? Is the 0.04% error rate difference between Fix-Net and SGM significant, considering that this corresponds to four additional misclassified examples on the test set?\n- More generally, the presentation in Table 2 doesn\u2019t group together comparable approaches, which leads to incongruities like Add-Net and Fix-Net entries being both bolded in each section despite Fix-Net systematically outperforming Add-Net. \n- As far as I can tell there is no empirical evidence to support the asserted benefits of the exponential regularization schedule.\n- I\u2019m also doubtful of the significance of the claim that the submission achieves state-of-the-art quantization results, given that in most settings bit-sizes do not directly compare to those of related methods.\n\nAdditional comments:\n\n- I don\u2019t believe L2-regularizing weight values towards the closest quantized weight value has a direct probabilistic interpretation, because the way regularization is applied depends on the weight values themselves. If I\u2019m mistaken, what unconditional PDF over weight values would that correspond to?\n- \u201cDue to their lower number of redundancies, DenseNet and ResNet20 are considered as difficult to quantize.\u201d Is this common knowledge? Can the authors point to related work supporting this assertion?"}