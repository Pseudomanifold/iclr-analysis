{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, a soft quantization approach was proposed to train DNNs that can be evaluated using pure fixed-point arithmetic. Fixed-point quantization constraints were applied for batch normalization and ReLU as well. In the experimental analyses, employment of fixed-point calculations reduce computational costs (memory and running time) compared to floating-point arithmetic.\n\nIn general, the paper is well written, and initial experimental results are promising.\n\nMajor and minor issues:\n\n- What is the motivation of using three different quantization functions for three different algorithms (convolution, BN and relu)? How do the results change when you apply the same quantization function for all of the algorithms?\n\n- The work proposes an approach for training a general class of DNNs using fixed point arithmetic. In order to support their strong claim, I suggest authors to perform the following additional analyses:\n\n-- Experiments using larger networks (e.g. larger Resnets etc.)\n\n-- Using larger scale datasets, such as ImageNet etc. This is particularly important since the results of state-of-the-art methods change crucially when they are employed for training quantized DNNs using Cifar-10/100 vs Imagenet, Open Images etc.\n\n-- If you would like to analyze the proposed methods for vision tasks, then I suggest you to apply the proposed methods for training, at least, detection and segmentation models in addition to image classification models.. \n\n\n"}