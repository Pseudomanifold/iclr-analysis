{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents Fix-Net, soft quantization approach to train deep neural networks. The proposed approach is evaluated using three common benchmarks, and the results are promising. The paper is well-written and easy to read in general, but there are some unclear things (outline below).\n\nIt is unclear what you mean by \"soft quantization\". I've read the paper several times but have not found a definition. How does it differ from standard quantization?\n\nThe performance of your fixed-point model with 4-bit weights and 4-bit activation is sometimes better than a full floating-point model. This is surprising. Why? Can explain or elaboration on that?\n\nRegarding batch normalization, it is unclear to me if it is quantized during training, or after training? (See Eq (8), (9), and the surrounding text)\n\nThe results in Table 2 look selective, e.g., why is CIFAR-100 only compared with two other approaches? \n"}