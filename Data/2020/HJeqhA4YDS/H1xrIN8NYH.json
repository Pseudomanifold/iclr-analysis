{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper provides a theoretical study of regularization capabilities of over-parameterized convolutional generators trained via gradient descent, in the context of denoising with an approach similar to the \"deep image prior\".\nThe authors show that when using appropriate upsampling operators, gradient descent biases the reconstructed images towards low-frequency components, while the noise components, which typically consist of high-frequency patterns, take longer to fit, so that early stopping can provide a useful bias for denoising.\nThe proofs are based on recent results for \"lazy\" training of over-parameterized networks, namely neural tangent kernels.\n\nI find the paper interesting and novel, as it is the first to my knowledge to study the implicit bias of optimization for such generator-based denoising approaches, and provides interesting theoretical and empirical denoising results.\nWhile the current results are for a simple architecture with one hidden layer, they may pave the way for a formal study of more complex architectures.\nNevertheless, the paper presents some limitations which should be addressed further in the paper:\n\n* in terms of denoising capabilities, it seems that the results for the non-linear, over-parameterized network are essentially similar to the linear case (with an appropriately designed operator J in Section 3). Are there any benefits to the non-linear case that I am missing? Whether or not this is a limitation of the study, it should be discussed further in the paper.\n\n* Some comments on the empirical results reported in Figure 2: (i) how are hyper-parameters chosen? this seems to be crucial for the effective use of such denoising strategies, and it is unclear how robust these methods are, e.g., to the early stopping time; (ii) there are other methods that only learn on the given image (thus without the need for a training set) which outperform BM3D [e.g. 1,2], it is unclear how the proposed methods compare to those.\n\nother minor comments:\n- Figure 2 \"maintained on a large test set of images\" more details are welcome, including on hyperparameter selection\n- Section 1.1:\n\t* \"demystifying\" -> understanding?\n\t* different notations are used for the (same?) iterates, C^t and C_tau\n- Section 2.3\n\t* \"it follow that optimizing...\": please clarify. I agree that the optimum is the same, but is the implicit bias from initialization the same?\n- end of Section 3: early stoped -> stopped\n- Section 4\n\t* after Definition 2, \"this in turn implies that the jacobian spectrum ...rapidly\": clarify, is it a consequence of the analysis?\n\t* after Thm 1: maybe give some insight about the proof and the use of NTK? also, discuss similarities or differences with the linear case\n\n\n[1] Dabov et al. (2009) BM3D Image Denoising with Shape-Adaptive Principal Component Analysis\n[2] Mairal et al. (2009) Non-local Sparse Models for Image Restoration"}