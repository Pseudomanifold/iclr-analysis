{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper tackles the general problem of training deep models for tabular data. Motivated by the success of decision trees in this field, the author proposed a novel neural network architecture that mimics how decision forests work. Specifically, it proposed a novel neural decision tree module and stacked these models through densely connections; localization and feature selection is used to build ensemble models, referred to as deep neural forests. Experiments on a toy checkerboard dataset, several real-world tabular datasets, and a multi-modal dataset suggest the effectiveness of the proposed model, showcasing that the proposed end-to-end learnable model is on par with GBDT. \n\nPros:\n- Novel/original idea\n- End-to-end learnable, allowing to utilize SGD, which better fits for the large-scale dataset and multi-modal dataset\n- Well written and easy to follow \n- Competitive results over GBDT, which is commonly used in relevant tasks.\n\n\nWeakness: \n- The neural network competing algorithms are too weak; there is one important baseline/ablation missing to justify the proposed architecture. \n- Some of the choices are not justified either theoretically or empirically.\n- Important details are missing\n- Motivation and the proposed approach is a bit dis-connected\n- Potential overhead. Runtime is missing so difficult to judge here. \n\n\nDetailed comments:\n\nI can understand the author wants to mimic decision tree behavior motivated by the fact that the decision tree can be considered as a disjunctive normal form formula. However, it\u2019s not clear to me why this specific architecture of NT module is chosen. There seems no evidence to support that through this specific module we can capture the inductive bias over tabular data better. \n\nIt\u2019s not clear to me if the benefits come from the ensemble or the NDT model proposed. Have you tried the same feature selection and localization trick to ensemble a few FCN models? This would be an important baseline to justify your contribution in Sec. 3.1. Moreover, it should be compared against several other approaches that try to combine the strength of neural network and decision trees, e.g. TabNN. \n\nAt least several key technical choices should be justified empirically through ablation study:\n+/- Orthogonal constraints\n+/- Ensemble (localization / feature selection)\n+/- Dense residual connections\n\nFrom the intro, we can see the paper is motivated by the success of decision tree models, specifically, gradient boosting trees. The comparison is also targeting GBDT. However, the architecture and ensemble scheme is mainly based on the bagging, which is commonly used in random forests. I would suggest the author modify the intro slightly to \n\nMissing details:\n- How each subgroup is decided? Do different choices matter a lot?\n- Why you use elastic net regularization instead of standard l1? Is this choice grounded by any empirical findings?\n\nMinor:\n- You should cite the straight-through estimator for the differentiable sign function. Otherwise, it might make readers think this is original in this paper. \n- Handling multi-modal input data is a bit beyond the scope of tabular data, which is the main focus of this paper. This raises some concerns since in this field there are much more comprehensive neural network models to be considered to compare against than simply combining FCN and CNN. \n\nFinal Remarks:\nOverall I think the paper is still slightly below the bar. I am expecting to see a thorough ablation and comparison to justify the technical contributions."}