{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "About the problem: I consider the problem tackled of very significant importance. As the paper rightfully argues, (boosted) DTs lead competitions in \"tabular\" data while deep learning excels on data with topological relationships between its features, yet we are short of providing the classification efficiency of DTs to DL over tabular data -- or -- explaining why it is worthless.\n\nThe solution adopted by the paper consists in reimplementing the classification algorithm of decision trees (Boolean algebra) using the DL algebra (essentially vector spaces).\n\nFirst, the name DNF needs to be changed. In Boolean formulas, it means Disjunctive Normal Form formulas, which could misleading given the paper's purpose. \n\nSecond, I see no difference between the OR and AND in (unnumbered formula in pg 3). \n\nThird, the architecture looks like it approximate a DNF (Boolean) rather than a DT. This is crucial: in a DT, the set of rules that can be derived from the tree are mutually exclusive. I do not (clearly) see this in the models learned.  This is important if the name Deep Neural *Forests* is to be used. \n\nFourth, I do not see any attempt to bring interpretability in addition to classification efficiency using DL on tabular data. Have the authors considered this as well ?\n\nFifth, the authors have used XGBoost even for small low dimensional datasets (Section 4) -- as far as I remember, XGBoost is heavily optimised for processing huge datasets, and accepts the price of approximations in the classical boosting framework to learn DTs (for ex on the splitting criterion used). Have the authors tried a more conventional but exact alternative for such simple datasets ? If so, what did the curves in Fig 2 look like ?\n\nSixth, the authors provide no statistical comparison to the results in Table 1. I assume the data after the +- are standard deviations (correct the \"Standard error\" in Fig 2). If so, I am quite sure that almost no bold result in the table is significant.\n\nSeventh, the experimental results are interesting, but apart from Section 6, I see no real advocacy to tweak DL to be more tabular -- compared e.g. to tweaking DTs to be more topological. Do the authors think that the paper answers by the affirmative to the question \"is it worth tweaking DL for tabular data ?\". A theoretical section would have been much welcomed, everywhere but in the conclusion... \n\n"}