{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a new model architecture, deep neural forest for tabular data. My first question is that it seems that the deep neural tree structure is just a special case of a fully connected neural network. Consider the AND() and OR() functions, they are a linear transformation with a tanh, which is equivalent to a one-layer neural network with tanh activations. Consequently, the NB() function is a two-layer neural network; NT() is a three-layer neural network and DNT is an N-layer (densely connected) neural network. I think the only difference is that DNT is a neural network with some weights fixed as 1 or 0 and in training, an orthonormality constraint is applied. Orthonormality constraint is also used in neural network training [1]. Hence, instead of a completely new model, I would rather consider DNT as a neural network with a special regularization. Then I would recommend an ablation test to study which part really contributes to the model's performance. For example, can a regular neural network ensemble trained with orthonormality constraint achieve a similar result as DNF?\n\nI did not get the intuition why the author multiplies a Gaussian probability density. It seems that this formulation is not from Meir et al. (2000), so more explanation and discussion on this would be welcomed. Is there a Bayesian intuition behind this?\n\n[1] Bansal, Nitin, Xiaohan Chen, and Zhangyang Wang. \"Can we gain more from orthogonality regularizations in training deep networks?.\" Advances in Neural Information Processing Systems. 2018."}