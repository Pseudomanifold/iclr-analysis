{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThe paper tries to construct a theoretical framework to rigorously analyze the disentanglement guarantees of weak supervision algorithms. In particular, it focuses on two concepts, consistency and restrictiveness which provides a formalism that precisely distinguishes when disentanglement arises from supervision versus model inductive bias. \n\nStrengths\n\nThe framework uses two simple concepts, consistency and restrictiveness for both generator and decoder. It also gives rise to a calculus. It is very useful to demonstrate the conditions under which various supervision strategies guarantee disentanglement. \n\nThe paper also did a good job clarifying how consistency and restrictiveness differ from other disentanglement concepts used in the literature. \n\nWeaknesses\n\nThe paper does not propose effective methods for disentanglement in the weak supervision setting. \n\nThe experimental section uses very toy datasets. It is not clear how the weak supervision signal can come from in realistic applications."}