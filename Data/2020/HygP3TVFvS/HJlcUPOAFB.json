{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a method based on Wick's contractions that can integrate out intermediate layers of a Bayesian neural network with finite width. Similar to the connection between GP and infinite-width NN, the marginalization of intermediate layers could allow Bayesian inference for the network.\n\nThis is a very technical paper with mainly the derivation of the recursion relations and the posterior mean for Bayesian inference. The math is tantalizing. But the main concern is how effective it's gonna be when being used in a neural network inference problem. \n\nThe simulated example shows that there could be a mismatch between theoretical predictions and experimental data when using ReLU. How much effect it's gonna have in a prediction task?\n\nThe recursion relations need to be evaluated numerically for most of cases. Then what's the computational difference between the proposed method and general MC based integration?\n\nI would suggest the authors move some derivations to the appendix and add more experimental analyses and discussion about the model. Like what's the trade-off between approximation accuracy, running time, predictive performance etc, like other papers did. \n\nThe current format is not mature enough to get published I think. "}