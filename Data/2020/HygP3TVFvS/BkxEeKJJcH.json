{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper discusses the behaviour of distributions in neural networks with finite width and claims that those distributions are non-Gaussians. \n\nPros: \nThe paper studies architecture of neural network with finite (but asymptotically large) number of hidden units, which is what we have in the reality. The  non-Gaussian process behaviour is obtained through the distribution corrections in the recursive formula. The authors also showed a regularisation effect, based on those corrections. I believe that the algebra here is correct. Overall, the results are interesting and well-places in the literature and the 'dreams', i.e. future directions, provided by the authors sound interesting. \n\nCons:\nI find a paper is hard to follow. I think it\u2019s better to change a structure or a way of introducing the results. It would be better to state at the beginning in simpler words what are the results and how to get those results from an infinite regime to a finite one. Already giving more explanations for the terms in Equation (KS) would help the motivation. And, as authors also claimed, it would be nice to have additional experiments on commonly used neural networks structures. \n\n\nSome major suggestions: \n* There is a not cited paper that I think is closely related to this work (Vladimirova et al., 2019). They study tails of distributions for the finite number of hidden units . As a result, the tails become heavier-tailed as depth increases, therefore the distributions are further from Gaussians if we go deeper. So obtained non-Gaussian behaviour does not contradict with the one in the published work. \n\n* As I understood, the non-Gaussian process behaviour is obtained by adding the self-energy correction term into the correlation functions. How far the preactivation distribution is from the Gaussian depending on the self-energy? \n\n* Section 2.3 \u2018Related work\u2019 is hard to understand and, therefore, to relate to the current work. For example, there was no words before about a Schwinger operator approach. \n\n* There is only one toy experiment for 2 hidden layers neural networks with different width. Does it work for deeper layers? \n\nComments that didn\u2019t have an influence on the decision: \nI also find a style of the paper a bit too casual: \u2018Inception\u2019, \u2018you guessed it\u2019, \u2018the graduation from the magical school of Wick\u2019s crafts and wizardly cumulants\u2019, \u2018without solicitation\u2019, \u2018let us take off from the terminal point\u2019, etc. \n\nI wonder if this result can be applied to CNNs or ResNets? \n\n\nReferences: \nMariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel. Understanding Priors in Bayesian Neural Networks at the Unit Level, ICML (2019). "}