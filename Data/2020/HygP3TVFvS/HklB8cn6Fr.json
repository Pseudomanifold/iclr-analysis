{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents recursive flow relations of preactivation distributions from lower to higher layers and utilizes them for Bayesian inference. The authors have shown that the recursive flow relations can be reduced to analytic form for a series of activation functions.\n\nThe authors have used nearly the entire paper to present the derivations. I would have preferred instead to see part of the space of the paper dedicated to empirically evaluating its predictive capability against other Bayesian deep learning models like deep Gaussian processes; some of the derivations in the main paper can be deferred to the appendix.\n\nThe paper is hard to follow, primarily due to the cumbersome (at times undefined) notations. May I suggest that the authors consider a simpler configuration of the neural network to ease explication and defer a reader to the appendix for a general configuration? Secondly, is it possible for the authors to also convert some of the expressions to the neater matrix/vector form? It also doesn't help to not use numbers to reference equations.\n\nI have some questions below:\n\nCan the recursive relations be evaluated analytically for other activation functions like tanh and ReLU in the general case?\n\nWhat can the authors conclude from Sections 4.3 and C.3 for those results where the theoretical prediction deviates from experimental data? \n\nCan the posterior belief/distribution be expressed in \"actionable\" form like that in Appendix D?\n\nEquation 7: Can the terms within the expectation be expanded? Currently, they seem to imply that there are the same number m of outputs as the number of input data in each layer ell. \n\n\n\nMinor issues\nPage 1: are build upon?"}