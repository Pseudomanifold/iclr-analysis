{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I am not from this area and don't know much about reinforcement learning.\n\nThe paper discusses zero shot generalization (adaptation) into new environments. The authors propose an approach and then show results on Grid-World, Super Mario Bros, and 3D Robotics. \n\nIn the training environment E1 = (S, A, p) the algorithm sees a bank of exploratory trajectories \\tau_i = {(s_t, a_t)}_{t=1}^{T} but not rewards. The authors  then say that algorithm is tested on the test environment E2. They \" propose to only inform the new task per trajectory terminal evaluation r(\u03c4 ) in E1\" to give the training signal (where r is the reward).\n\nI am a bit confused by this setting. The model never sees any rewards for E1 but it does see rewards for E2? How is this zero shot?\n\nThe authors then propose their approach, I wish some of it had been described more rigorously with math (e.g. the loss etc.) so it was easier to understand for people not in the domain and familiar with some of the terminology. \n\nEmpirically the authors show results for 3 datasets and this seems thorough. "}