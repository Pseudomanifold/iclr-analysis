{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a framework (Scoring-Aggregating-Planning (SAP)) for learning task-agnostic priors that allow generalization to new tasks without finetuning. The motivation for this is very clear - humans can perform much better than machines in zero-shot conditions because humans have learned priors about objects, semantics, physics, etc. This is achieved by learning a scoring function based on the final reward and a self-supervised learned dynamics model.\n\nOverall, the paper is very clear and easy to follow.\nThe presented task is realistic and important, and the paper seems to address it in a reasonable approach. \nHowever, the evaluation seems lacking to me - the evaluation convinced me that SAP works, but I am not convinced that it works better than existing approaches (see below), and especially did not convince me that it is better in the zero-shot test environment.\nThe (anonymized) website contains nice videos that support the submission.\n\nQuestions for the authors:\n\n1. Page 3, 3rd paragraph of Section 3: the paper says that \"The proposed formulation requires much less information and thus more realistic and feasible\" - I agree that this is more realistic, but is it really more feasible? The requirement of much less information makes the proposed formulation much more sparse.\n\n2. A basic assumption in the SAP framework is that a local region score is a sum of all the sub-regions. As phrased in the paper: \"in the physical world, there is usually some level of rotational or transnational invariance\". I'm not sure that this assumption makes sense neither in the Mario case or in other tasks, e.g., robotics. Doesn't it matter if you have a \"turtle\" right in front of you (which means that the turtle is going to hit you), or below you (which means that you are going hit the turtle)?\n\n3. A question about the planning phase - page 5 says: \"We select the action sequence that gives us the best-aggregated score and execute the first action\". Do you select the entire sequence of actions in the new environment in advance? Can the agent observe the new state after every action, and decide on the next action based on the actual step that the action has reached, rather than on the state that was approximated in advance?\nIn other words - what happens if the first action in the new test environment yields an unexpected state, that was not predicted well by the dynamics model; does the agent continue on the initial planned trajectory (that ignores the \"surprise\"), or does it compute its next action based on the unexpected state?\n \n4. Experiments: in Gridworld and Mario - are there any stronger baselines in the literature, or reductions of known baselines to the zero-shot scenario? Are the chosen \"Human Priors\", BC-random and BC-SAP just strawmen? \nSince the main goal of this paper is the zero-shot task, what would convince me is a state-of-the-art model that does possibly *better than SAP on the training level*, but *worse than SAP in generalizing to the new level*. Additionally, are there other baselines that specifically address the zero-shot task in the literature?\n\nMinor (did not impact score):\nPage 2, 1st paragraph: \"... we show that how an intelligent agent\"...\nPage 3, 3rd paragraph: \"... in model-free RL problem\" - missing an \"a\" or \"problem*s*\"?\nPage 3, 3rd paragraph: \". Model based method ...\" - missing an \"a\" as well?\nPage 4, 1st paragraph:: \"... utilizing the to get the ...\"\nPage 4, last row: missing a dot after the loss equation, before the word \"In\".\nPage 7, Table 1: \"BC-random\" is called \"BC-data\" in the text. Aren't they the same thing?\n"}