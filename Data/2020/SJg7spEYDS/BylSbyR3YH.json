{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes GRAM-nets using MMD as the critic to train GANs. Similar to MMD-GAN, the MMD is computed on a projected lower dimensional space to prevent the kernel struggle in the high dimensional observed space. On the other hand, contrary to MMD-GAN, GRAM-nets trains the projection f_{\\theta} trying to matching the density ratio of p/q between the observed and the latent space. The proposed density ratio matching criterion avoids the adversarial training in MMD-GAN, thus can potentially fix the two-player optimization problem. The paper shows improved FID scores and nice-looking CIFAR10 generations. However, one fundamental error leads the paper to a rejection.\n\nStrengths, \n1, Matching density ratios is a novel and interesting idea. If the data lives in a lower dimensional subspace, matching density ratio could probably reveal the subspace. Compared to adversarially training f_theta, the proposed approach could lead to more stable training potentially. \n2, By manipulating E (px/qx - pz/qz)^2, they avoid estimating the high-dimensional px/qx and only estimate pz/qz.\n\n\nWeakness,\n1, The paper needs to be more careful with mathematical expressions. 1) Eq(2) should be MMD^2, instead of MMD. 2) Eq(5) and Eq(6) are wrong, although the used Eq(7) becomes true again. In Eq(5), Eq(6), the integration should be over f(x) instead of x, that is to say $\\int ..... df(x)$.\n2, It is unclear why one needs the regularization in Eq(9). In fact, the major problem of the density ratio estimator lies in that r(x) might be negative, so a clipping might be useful.\n3, The major contribution of GRAM-nets lies in removing the adversarial training in MMD-GAN. Therefore, more empirical comparisons should be made with MMD-GAN. For example, how MMD-GAN evolves in Figure 2 is necessary.\n4, In real image generation tasks, it is beneficial to show the stability of training GRAM-nets, compared with GAN, MMD-GAN as well as WGAN-GP; And Inception scores should also be reported for better validating the effectiveness of the proposed method."}