{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigated the Pareto efficiency of quantized convolutional neural networks (CNNs). \nThe authors pointed out that  when the number of channels of a network are changed in an iso-model size scenario, the lower precision value can Pareto dominate the higher ones for CNNs with standard convolutions. Furthermore, the authors showed that depth-wise convolutions are quantization-unfriendly when going low- precision even in quantization-aware training. We further provide theoretical insights for it and show that the number of input channels per output filter affects the Pareto optimal precision value. The authors then proposed DualPrecision, a simple yet effective heuristic for precision selection of a given network. When applied on ImageNet, DualPrecision outperforms the 8 bit fixed point baseline and prior art in mixed-precision by a significant margin.\n\n1. The wording issues \n- The paper used words like \"iso-model\" and \"Pareto\" at many parts of the draft, making it quite hard to read..  \n\n2. The experiments seem fair. However, the improvement seems quite marginal\nThe proposed method improved the baseline by 0.9% and 2.2% in top-1 accuracy on ImageNet for ResNet50 and MobileNetV2, respectively.\n\nI have to admit that I am unfamiliar with this particular topic and only knows a few classic papers, like for example Xnor-net: Imagenet classification using binary convolutional neural networks"}