{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper discusses two problems in GANs, termed as \u201csample inefficiency\u201d and \u201cpixel-wise combination\u201d by the authors. The former is attributed to small batch size, whereas the latter is exhibited as classifying as real the linear combination of 2 real images. Unfortunately, both the theoretical analysis and the presentation of experiments are limited. Besides, the writing needs to be improved. The paper can be more concise by removing many redundant statements. For example, there are many repeated sentences in the second paragraph of page 2 and section 3.1.\n\nTheoretical results:\n\n-The analysis (section 3.3 and 4.3) is based on much simplified assumptions. Section 3.3 assumes linear generator and discriminator. In addition, the results that the variance of parameter update inversely scales with the batch size is well-known in optimisation literature for stochastic gradient descent (under milder assumptions). It is unclear to me why this result can not be directly applied in GANs (for each update step), and what new insight the new derivation with stronger assumption provides.\n\n-Theorem 3 assumes large classification margin (assumption 2). This assumption is used to analyse the behaviour of GANs after convergence, but it is unlikely as training progress, when the discriminator increasingly struggles to tell real and fake samples apart. There is also an additional assumption that |x_1 - x_2|_2 < \\delta, which means the 2 images are already close in pixel space, so the average of x_1 and x_2 are likely to be another real sample.\n\nTherefore, I think the main theoretical results in this paper rely on strong assumptions, and do not provide enough insight into GAN training in more general cases.\n\nEmpirical results:\n\n-The experimental setups (e.g., model architecture, hyper-parameters) for section 3.2 and 4.2 are not provided.\n\n-The sentence above section 4.3 reads: \u201csome generated images are exactly the same as the pixel-wise averages of the training data\u201d. How these (\u201csome\u201d) images were selected? And how the training images being averaged were identified?\n\n-Section 5.3 claims the proposed method could improve FID for up to 30%, but the baseline results are questionable. Although the paper reported using models such as LS-GAN and SA-GAN (without details about the architectures), the baseline FIDs were much worse than reported in these papers.\n\n-It seems unfair to study the effects of different batch sizes using the same learning rate, as learning rate generally needs to be reduced for smaller batch size. For example, similar study on batch size has been conducted in previous papers including Improving GANs using optimal transport (Salimans et al. ICLR 2018) and large scale GAN training for High Fidelity Natural Image Synthesis (Brock et al. ICLR 2019)."}