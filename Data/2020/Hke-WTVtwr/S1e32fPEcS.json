{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n### Problem and Previous Research\nThis paper tackles the problem of incorporating the sequential structure of words for text processing. \nPrevious research [Gehring et al., ICML'17; Vaswani et al., NeurIPS'17] tackles the problem by adding position embeddings at the feature level. \nSupported by recent empirical results [Shaw et al., NAACL'18; Dai et al., ACL'19], the paper argues that these position embeddings are independent i.e. do not consider relations between neighbouring word positions.\n\n### Contributions\nTo address this key limitation of previous research, the paper proposes to define the embedding of each word through a continuous function over its position (so that embeddings shift smoothly with increasing positions thereby modelling word order). \nThe paper then lists two properties that such a function needs to satisfy and proposes to use the complex space as the target domain of the function. \nExperimental results on text classification, machine translation, and language modelling show gains over classical and position-enriched word embeddings.\n\n### Pros and Cons\nOverall, the paper tackles an important problem in word embeddings and proposes a principled approach to the problem. \nHowever, the paper could be further strengthened by positioning itself with respect to other existing neural network-based approaches that incorporate sequential structure e.g. graph neural networks (GNNs).\nGated-graph neural networks (GGNN) [Beck et al., ACL'18] and graph convolutional networks [Sahu et al., ACL'19] use GNNs on graphs with words as nodes and labelled edges (adjacence, precedence, etc.) between nodes to model the sequential structure between words.\u00a0\n\n### Possible Improvements\nOn the empirical side, GGNN of Beck et al. seems esp. relevant since they show improvements on machine translation.\nA GNN-based baseline to compare against is to use Transformer - Complex - vanilla embeddings as features to a GGNN on the graph with words as nodes and adjacence, precedence labelled edges between neighbouring words.\n[Beck et al., ACL'18] Graph-to-Sequence Learning using Gated Graph Neural Networks\n[Sahu et al., ACL'19] Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network\n\nI am open to revising my rating based on the responses of the authors."}