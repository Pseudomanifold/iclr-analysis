{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a way of reducing the number of parameters in Transformer by splitting some of the linear layers into multiple separate groups. Additional mixing linear layers are added so information can pass from one group to another. In the feedforward submodule, a bottleneck layer is also added to reduce the number of parameters. Small scale experiments on language modeling compared the proposed model to vanilla transformers.\n\nI think the motivation of the paper is good and important for real-world applications of Transformers. However there are several major problems with the paper.\n\nFirst, the proposed method is only marginally better than a vanilla transformer of similar size, and both are much worse than the current sota. Also, the baseline transformer experiment is done by the authors themselves, and it\u2019s not clear how well they tuned it. From table2, it seems they simply reduced the hidden size, but that\u2019s not the only way of reducing parameters. \n\nThe second problem is that the authors completely ignore the fact that the multi-head attention is already doing grouped attention. It splits the hidden states into multiple parts, and each head performs attention separately. In thinking this way, the proposed \u201cgroup attention\u201d feels more like multiplying the number of heads. This also means the authors should compare their model to a vanilla transformer with 2x more heads.\n\nAnother problem is section 3.4. Here the authors claim their model has O(D_m^2/G) parameters, but it\u2019s not true because key and value projections are not grouped and their size is O(D_m^2). Also, the number of parameters in the first layer of the feedforward submodule depends on M rather than G (if I understand it correctly). Despite this, I can\u2019t find the exact value of M in the paper. \n\nOther minor comments are:\n- I don\u2019t understand the reasoning behind not grouping key and value projections because \u201cthey can come from other source domain\u201d. What does it mean and why it prevents grouping? In any case, the experiments only use characters so why not group them as well?\n- The paper has many typos and weird english such as \u201cnatural language process model ...\u201d, \u201cincreased training complexity issues ...\u201d, \u201cwhere also requires \u2026\u201d, \u201cgets incredible achievements\u201d, \u201c... performance compare to Transformers\u2019. \u201d, \u201c... rational behinds.\u201d, \u201cheavy weights\u201d, \u201c... how high similarity ...\u201d, \u201c... since Transformer raises.\u201d\n- Why a batch size of 22? It\u2019s much smaller than Dai et.al, so shouldn\u2019t the learning rate need to be adjusted accordingly?\n- The figure 1c is not exactly consistent with the text. According to eq3, there should be a summation before ReLU. I know a summation of two linear layers can be written as concat + linear, but it would be easier to understand if the figure was consistent with the text.\n- Maybe make it clear in the introduction that a bottleneck layer is also used. \n- The introduction suggests that Transformers only works with large sizes, which is bit misleading. Yes, one needs huge models for reaching SoTA, but there is nothing in the Transformer architecture that requires large sizes. In fact, the experiments in the paper show that a small vanilla transformer outperforms much larger recurrent networks.\n- The group embedding in section 3.1 doesn\u2019t make sense. Embedding simply assigns a vector to each token, so grouping dimensions here doesn\u2019t change anything. It\u2019s the same as having a simple embedding, then splitting it into multiple parts.\n"}