{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper describes a simple yet effective technique for learning temporal point processes using a mixture of log-normal densities whose parameters are estimated with neural networks that also adds conditional information. The method is shown to perform better than more recent techniques for density estimation such as different versions of normalising flows. Experiments were reported on 6 datasets, comparing the approach against flow models and assessing the benefits of adding extra conditional information, performance with missing data, and benefits of sequence embeddings.\n\nThe paper makes an important point which is also my own experience when working with relatively low dimensional problems; simpler neural density estimation approaches such as MDNs usually perform similarly or even better than models using normalising flows. The task here is on learning temporal point processes which have important applications in social networks, criminality studies, disease modelling, etc, but are relatively unpopular within the machine learning community. The paper gives some motivation but I think the authors could elaborate further on the huge number of applications and potential for significant impact from these models. Apart from this the paper is well written and structured, and easy to follow. \n\nThere are not many theoretical innovations as the main contribution is a combination of several well known techniques such as MDNs and RNNs, applied to the specific temporal point process formulation. The main lesson learnt though is that these simpler techniques can perform surprisingly well. With that said, the paper would benefit from discussing the following points:\n\n1) Mixture models are effective in low dimensional problems but require the manual specification of the number of components. How was this done in the experiments and how sensitive the performance is to this parameter?\n\n2) The paper discusses several problems with normalising flows, but in particular the computational cost involved in generating samples or evaluating the density. This is true for some variations of NFs but not for all. For example RealNVP and the recent Neural Spline Flows are efficient in both, sample generation and density evaluation. With this in mind, the paper would benefit from further comparisons to these approaches. Another interesting comparison would be with autoregressive flows, such as inverse autoregressive flow or masked autoregressive flow. They can both  capture sequences and should be able to model inter-event times, instead of an RNN. \n\n  "}