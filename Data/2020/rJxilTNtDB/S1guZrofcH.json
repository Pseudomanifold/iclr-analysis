{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a reverse self-attention on graph neural networks and integrate it with graph-based molecular description. It provides an efficient-automated and target-directed way to estimate the atomic importance without any domain knowledge on chemistry and physics. In particular, it trains graph attention network to predict the molecular property using the molecular network and calculate the atomic importance scores based on the sum of the incoming attention weights.\n\nPros:\nThis paper presents an interesting application of the graph attention network to other scientific areas. And the developed reverse self-attention score, although simple, is useful in estimating the importance of atoms in a molecular network. \n\nCons:\nHowever, the paper also needs some improvement in its evaluation (see detailed comments below).\n\nDetailed comments:\n\u2022\tIn addition to the evaluation metric (7), it is also important to hire human experts to evaluate the performance. For example, just sample some test samples and annotate them by human experts so that they can be compared with the predicted results.  \n\u2022\tIn order to further justify the metric (7), it is also important to report how consistent (7) is with the human evaluation.\n\u2022\tThe method is not compared with any of the previous methods/baselines. At least, it should evaluate how accurate of the proposed automatic ML-based method is when compared to previous (more expensive) human-based method.\n\u2022\tIt would be better to mention earlier what are the molecular property to be predicted by GAT during training. Just a few simple examples would be sufficient.\n"}