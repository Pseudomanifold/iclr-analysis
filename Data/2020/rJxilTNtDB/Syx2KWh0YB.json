{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a method to find predict the atomic importance of an atom in a molecule. In practice this is computed by computing DFT and then using the domain knowledge of human experts. DFT calculation has a huge computation cost and further depends on labor intensive human expertise. In order to solve this problem, the authors represent the molecule as a graph neural network and use it for predicting a property that is related to atomic importance. Then the attention weights are used to determine the importance of each molecule. The authors call this reverse self attention. \n\nI have several problems with this approach.\n\n1. First, the role of attention as a measure of feature importance is debatable. For example, see this paper https://arxiv.org/abs/1906.03731\n\n2. Second, attention is used a measure of feature importance when the true values of feature importance are not available. in this case the author's assume that atomic importance for each atom is available during training (They use this to compute loss) in Eqn 7. If this is the case, why not directly predict Atomic Importance by trying to predict a property  for each node in the molecule graph? That will be a more principled way to do this.\n\n3. The authors don't compare their approach with simple baselines.\n\n"}