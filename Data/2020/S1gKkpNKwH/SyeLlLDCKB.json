{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper compresses policy networks using approaches inspired by neural architecture search. The main idea is to have a fixed size weight matrix, but learn how to share weights, so that the resulting network can be compressed by storing only unique weights values. Both the partitioning and weights are trained simultaneously, inspired by ENAS. The partitioning is modeled by an autoregressive RNN and trained via REINFORCE. The weights are modeled by a single set of weights (as opposed to a distribution) which is then updated by using a gradient approximation based on ES. The experiments carried out include comparing to existing works on policy network compression, ablating against random partitioning, as well as a few experiments meant to increase understanding of the learned partitions.\n\nPros:\n - Overall, paper is well-presented and is generally quite clear on its contributions, place in the literature, and experiment details.\n - The approach is straightforward and has applications in compressing RL policies.\nCons:\n - There should be standard deviations in Table 2 across multiple seeds, as it is unclear whether the differences in reward are significant.\n- There is some discussion on scalability in the introduction as part of the motivation, though would be nice to see some quantitative numbers. Is the current method already highly scalable, or is scalable still a potential that has yet to be reached?\n\nQuestions:\n - What is the computational cost in total CPU time, compared to baselines such as fixed random partitioning? The autoregressive sampling of partitioning seems to be a bottleneck during training, as sampling may be slow, especially when scaling to larger weight matrices. Is this why the partitioning is updated much more sparsely (as shown in Figure 5)?\n - For the gradient estimation in eq 3, what exactly is this unbiased with respect to? Did you mean asymptotically unbiased in the limit as sigma -> 0?\n - Do Chromatic networks always use M partitions? (How exactly were the number of partitions chosen, for Tables 1 and 2?) If I understood correctly, Figure 3 is about the Masked networks from (Lenc et al., 2019). If so, is there a similar method in which the number of partitions of Chromatic networks can be learned (or regularized)?\n - (I may have misunderstood these figures, but:) There seems to be quite a large difference in training curves for different workers, e.g. shown in Figures 4 and 5. The red curve seems to almost always be best, while the green curve is much higher variance. Why is this? I also couldn't tell exactly how many colors there are in these plots (introduction mentions 300?), but wouldn't a mean/std plot be easier to parse (or do the specific colors mean something significant)?\n - Is there a reason why Toeplitz and Circulant have missing numbers in the #bits columns in Table 2?"}