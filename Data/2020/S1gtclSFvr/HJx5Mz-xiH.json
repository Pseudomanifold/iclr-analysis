{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposed an end-to-end phrase-to-phrase NMT model (NP2MT). I think the contribution of this paper is incremental and the idea is of less novelty. In general, the model is largely based on the NPMT model, where modification is the introduce of phrases in the source sentences. Then the author proposed the memory module strategy. In the experiments, the performance improves significant when using out of domain dictionary, but less significant for in-domain dictionary. I also have a concerns about the experiments. The dataset used in this paper seems not convincing to me. By my own experience, the performance on small dataset for either LSTM or Transformer is not stable. The authors just tested the model performance on WMT test set. I think at least the WMT training data should be used for training as well. Another question is the details about the training time and decoding time, since the dynamic programming is used.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}