{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The work is concerned with a very interesting question: \"robustness of the adversarial attacks\" and discusses the following: First, what are different ways of destabilizing a given adversarial attack (different perturbations) and how much each of them are effective. Secondly, what makes adversarial images particularly non-robust compared to natural images and is there empirical evidence for it? Thirdly, is it possible for an attacker to use this knowledge and use a rather universal model of perturbations to make its adversarial examples robust against such (deterministic or stochastic) perturbations?\n\nThe paper is quite well-justified and addresses a very interesting question by unifying the existing results of the literature in one place. First of all, comparing the accuracy-robustness trade-off of different methods are useful.  The idea of modeling perturbations as a general distorted communication channel to have a unification of these methods is very useful. The perturbation analyses along with the results in Figures 2, 3 are novel and can be useful for further theoretical investigation of the source of instability of adversarial images. To the best of my knowledge, the transferability of adaptive attacks against noisy channel defense methods has not been discussed in the literature. The paper is well-organized and well written (except for Section 5 both in sense of the writing and where it appears in the paper).\n\nA few questions and suggestions:\n\n* The discussion of results in Figure2 should be more extensive and more clear. \n\n* Why do you think the L2 of the gradient for adv images has such high variance for both orig and adv class? Is there a relationship between the magnitude and how effective is a perturbation-based defense?\n\n* It should be made clear in the text that methods like randomized smoothing are designed for the purpose of 'certified' accuracy and being robust against attack sizes more than the certification threshold was not part of the method's contributions.\n\n* The use of the term \"compression\" in Section 3 is not entirely correct as it is not \"technically\" correct to say that compression is what is necessarily happening in a general case of a deterministic C(x).\n\n* Fig 1 should be larger."}