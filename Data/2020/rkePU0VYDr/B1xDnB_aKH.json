{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper studies the noise injection as defense methods against adversarial perturbations. It presents several experiments on the relationship between clean and robust accuracy. Conclusions of this study are (1-1) several defense methods have the same underlying mechanism (noise injection) and behave similarly against adversarial perturbations, (1-2) all of the defense methods can be attacked by the same black-box attack, and (1-3) the reason of the correct label recovery by noise injections might be the input instability on adversarial inputs.\n\nThis paper should be rejected because (2-1) some conclusions are not well-supported by the experiments, and (2-2) the paper fails to demonstrate the novelties and their importance of some conclusions.\n\nMajor comments:\n(3-1) Experiment 1 aims to highlight the similarity between the ``perturbations defenses''. However, the \"similarity\" is not defined, and the arguments are mostly subjective. For example, SVD and Gaussian noise appear to have different effectiveness (Figure 1). If the similarity means the observation that large distortions decrease the accuracy, it will not be surprising because all methods should achieve the chance rate accuracy when the distortions are extreme.\n(3-2) What is the point of Experiment 2? The phenomenon that strong defense methods decrease clean accuracy has been observed in many defense papers. The noise injection-based defense methods will not be exceptions. Remarking the existence of the trade-off will not be a strong contribution. The focus of the discussion should be on how to take a good balance or improve both of them.\n(3-3) To my best knowledge, experimentally showing that adversarial inputs have larger gradient norms (Section 4) is novel, and it is potentially interesting. I wonder how large the gradient norms will be for randomly perturbed images."}