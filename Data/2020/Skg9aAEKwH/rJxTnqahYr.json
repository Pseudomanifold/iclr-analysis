{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Visual Hide and Seek\n\nIn this paper, the authors propose a two-agent hide and seek environment, and uses reinforcement learning to train the hider. The authors interpret the learned representation by using the learnt features to do classification.\n\nI tend to vote rejection for this paper, mostly due to my feeling that the empirical contribution is interesting, but not novel enough for this conference.\nThere is still a lot of potential improvement available for this project, which is summarized below.\n\nPros:\n- The environment itself, once open-sourced, can be quite valuable to the community.\nI think it is fair to say that the proposed environment in this project is better than the hide-and-seek environment from OpenAI in that it provides the visual input. \nBy the way, I believe OpenAI environment is also partially observable, where unseen agents\u2019 information is masked out.\n- It is an interesting finding on how meaningful features and performance correlate with each other.\n\nCons:\n- Findings in the project are very practical and interesting, but they do not seem to provide valuable information for future research.\nFor example, to improve the quality of the paper, is it possible to utilize the features and performance correlation to improve unsupervised visual feature learning?\nIs it possible to draw a connection to Psychology? Does this \u201cRepresentation vs. Survival Time\u201d also happens in real-life, or is it just some empirical things that happen in optimization?\n\n- There are a lot more potential to improve the environments. It will greatly increase the impact of the paper by, for example, considering multi-agent training, self-play, unsupervised training.\n\nIn general, I think this project still have a lot of room for fine-tuning."}