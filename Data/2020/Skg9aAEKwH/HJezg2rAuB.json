{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nPaper Summary: The paper studies what an embodied agent that is trained using RL learns in the context of the hide and seek game. There are two agents in a simple environment (one hider and one seeker). The strategy for the seeker is fixed, but the hider learns a policy based on ego-centric visual input. The paper attempts to analyze how the learned representation differs with different capabilities of the agents or environment structure.\n\nOriginality:\n\nThe study of the learned representations in the context of hide and seek is new. There is concurrent work on hide and seek (e.g., Baker et al., 2019), but unlike this paper, they have access to groundtruth location information of the agents and it is not based on visual input.\n\nQuality:\n\n- The conclusions of the paper are either counter-intuitive or are based on some hypotheses and guesses.\n\nFor example, it is strange that \"visibilityreward\" performs worst for \"Awareness of Self-Visibility\". It is exactly trained for that task.\n\nThere are conclusions about \"temporal events\" (e.g., the hider learns to first turn away from the seeker then run away) just based on a single frequency number (Figure 4). There are many other possibilities that can result in the same frequency. If the authors draw conclusions about temporal events, they should show how values change over time. Single frequency numbers cannot be used for such conclusions.\n\n- To check the learned representation, the common practice is to use features on a very different task. However, this paper addresses only two tasks \"Seeker Recognition\" and \"Awareness of Self-Visibility\" which are very close to the original task. One of the models even receives explicit rewards for the latter case.\n\n- No standard deviation is reported for the results. Given the random nature of RL algorithms, standard deviations should be reported.\n\nSignificance:\n\nI expected a more rigorous analysis since this is an analysis paper and there is no new methodology in the paper. Conclusions that are just based on a guess about some qualitative result or based on two state changes as in Figure 5 are groundless. More thorough analysis based on quantitative results are required to justify the claims and provide generalizable conclusions.\n\nClarity:\n\nThe paper is mostly clear, but there is some missing information:\n\n- In Figure 3, which dot corresponds to which model? It is claimed that \"When the model has a weakness, the model learned to overcome it by instead learning better features\". To justify that, we need to know what the dots are.\n\n- What do \"randominits\" in Figure 5 correspond to?\n\n- What is red, yellow, blue and green plots in Figure 6? What is the standard deviation? "}