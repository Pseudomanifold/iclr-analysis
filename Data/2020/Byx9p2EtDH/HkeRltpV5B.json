{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper authors propose a method for transfer reinforcement learning (RL). Specifically they are claiming that RL agents can transfer knowledge between each other about the environment dynamics. In order to showcase their approach they have come up with a new transfer RL task that makes use of some source policies trained under a diverse set of environment dynamics. Their key contributions to solve the task involve a decision aggregation framework that is able to build on top of relevant policies while suppressing irrelevant ones and an auxiliary network that predicts the residuals around the aggregated actions.\n\nI recommend the paper to be accepted since they have an innovative contribution that pushes the needle on the transfer RL literature although I do not think the contribution is substantial. The set of experiments covers a wide range of different standard RL tasks and they provide enough evidence that the approach works. I find it interesting that they are able to extend the approach to the discrete action tasks. \n\nI would however recommend providing more experimental results that provides evidence that the target policy can recover the right policy when the target environment dynamics is the same as one of the source environments."}