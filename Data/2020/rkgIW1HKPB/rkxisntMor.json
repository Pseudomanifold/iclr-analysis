{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "\n###### Overall Recommendation\nI vote for the \u201cWeak Accept\u201d decision for this paper. \n\n### Summary\nThis paper introduces a novel model termed Random Distance Prediction model, it can predict data distances in randomly projected space. The distance in projected space is used as the supervisory signal to learn representations without any manually labeled data, avoiding the concentration and inefficiency problem when dealing with high-dimensional tabular data. Their main contribution is extending the random distance in projected spaces to approximate the original distance information of the hight-dimensional tabular data effectively. Overall, the idea in this paper is interesting and effective, the experiment results in two typical unsupervised tasks (anomaly detection and clustering) also look very promising. However, the writing sometimes has unclear descriptions, given these clarifications in an author's response, I would be willing to increase the score.\n\n### Strengths\n1. The illustration of the authors' idea is clear and concise.\n2. The theoretical analysis of the proposed method is solid and systematic, the validity of the subparts have been proven previously.\n3. The experiment part is well organized. The RDP model are compared with several state-of-the-art unsupervised learning methods in 19 real-world datasets of various domains. The experimental setup is solid with realistic considerations, the results are very convincing and promising.\n4. This paper provides sufficient detail for reproducing the results.\n\n### Weaknesses\nLack of systematic description of the authors\u2019 major contribution. The proposed model looks more like a combination of previous conclusions, which makes readers feel the core parts of this paper build heavily on previous work.\n\n### Questions\n1. What is the relation between \u201crandom distance prediction loss\u201d and \u201ctask-dependent auxiliary loss\u201d?\n2. Are there any solutions and references about how to choose the task-dependent loss L_aux? \n3. Why you shade the second part of the loss function in Figure 1; \n4. How long is it for training the proposed model and getting the experiment results? Does the RDP model still outperform the other algorithms?\n\n### Suggestions to improve the paper\n1.  It would be better to reorganize Section 1 and Section 2, please describe the contribution in a more systematic way.\n2. Add details for the architecture of the model, please give more descriptions about Figure 1.\n3. It might also help to add an algorithm comparison box for the test time for the proposed method.\n\n### Minor Edit Suggestions\n1. It would be better to give more descriptions about Figure 1; the lower right part in Figure 1 is not explained in the caption; the shadow part in Figure 1 is not precise.\n2. Figure 1 was bad organized, please make the legend readable size. \n3. I don't think there exist the proofs of Eqns. (2)-(4) in the reference paper (Vempala, 1998), which was written in Page 4. The number should be revised.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}