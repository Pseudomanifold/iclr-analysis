{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for efficient exploration in tabular MDPs as well as a simple control environment. The proposed method uses deterministic encoders to learn a low dimensional representation of the environment dynamics, which preserves distances between states such that \u201cclose\u201d states in the full MDP are close in the learned representation. An intrinsic reward is formulated based on a measure of novelty, given by distance between new states, and a stored replay buffer.  Along with the dynamics model, a model-free agent employs Q learning to find a good policy. Experiments are performed on 3 tabular environments and the acrobot control task. \n\nPros: \n1.\tOverall the paper is clear and the proposed method makes sense intuitively. The intrinsic reward is cheap to compute and the state abstraction offers a nice way to visualize state differences the agent thinks are important.\n\n2.\tThe method seems to be sample efficient with regard to strong baselines like [1]\n\nCons:\n1.\tIt seems difficult to argue the efficacy of a low-dimensional state representation that doesn\u2019t scale with state dimensionality. As shown in [2] and [3], learning effective state abstractions in high dimensions can require considerably more effort. \n\n2.\tGiven that there exist novelty based intrinsic rewards which compute state abstractions in high dimensional environments [3], I find it hard to see the usefulness of the proposed method. \n\n3.\tThe choice of distance metric for the representational space is not well motivated. As correctly stated by the authors, the L_2 norm will cease to be a good metric as state dimensionality increases.\n\n4.\tThere are too many grid-world experiments. The point of the first two experiments can be made simply using the four-room environment. This could make room for a more interesting experiment such as MuJoCo Ant Maze. \n\nMy main issue with the work in its current form is that the method is too light in terms of technical contribution. Simple methods are ok (even valuable!) but there should be a certain about of rigorous analysis which shows that the simple method can be used as a foundation for further work. For a method which mostly examines tabular environments, I expect some analysis of the methods efficiency with regard to data efficiency -- the main point of the paper. [1] and [4] which are used as comparisons, provide such analysis. If a convincing theoretical analysis is out of reach, then it could be sufficient to provide extensive experimental evidence supporting the claims. In this case it could include an examination of different metrics, additional (ideally more difficult) environments, and comparison to other baselines like [5], [3].\n\nDue to what I see as a lack of technical contribution, I do not recommend acceptance to ICLR at this time. \n\nA more compelling submission would include the following:\n\u25cf\tA more detailed motivation for why the L2 norm makes sense as a distance metric. \n\u25cb\tIn an abstract space, it's more natural to use a statistical distance like the KL or JS divergence. These metrics have drawbacks, but they should be discussed\n\u25cf\tAn analysis of the limit behavior of the proposed method. Given enough time an intrinsic reward should explore every state in a deterministic environment. Does this happen in the limiting case -- if not, is the margin acceptable. \n\u25cf\tMore extensive experiments. This method can clearly admit convolutional architectures so experiments on more interesting environments are viable. Though I believe this would require more complex models such as a VAE, and may change the submission considerably.  \n\nMinor notes\n\u25cf\tSection 3: \u201cwhen [the distance between transitions is less than the slack ratio] the transitions are mostly accurate within a ball of radius \\frac{w}/{\\delta}. This is too vague, what does mostly accurate mean? \n\u25cf\tEq (6), is \\alpha a hyperparameter as well as the learning rate? If \\alpha is just the learning rate than the equation is incorrect, because the learning rate is applied to the gradient of the loss, not the loss itself. \n\u25cf\tThe description of the planning algorithm and Q learning in section 4 is a little sloppy, a clearer description would be appreciated. \n\u25cf\tComputing novelty with respect to a state\u2019s nearest neighbors is problematic at scale. This point should be at least acknowledged.  \n\n\n[1] Osband, Ian, et al. \"Deep exploration via bootstrapped DQN.\" Advances in neural information processing systems. 2016.\n[2] Kim, Hyoungseok, et al. \"EMI: Exploration with Mutual Information.\" International Conference on Machine Learning. 2019.\n[3] Ha, David, and J\u00fcrgen Schmidhuber. \"World models.\" arXiv preprint arXiv:1803.10122 (2018).\n[4] Bellemare, Marc, et al. \"Unifying count-based exploration and intrinsic motivation.\" Advances in Neural Information Processing Systems. 2016.\n[5] Pathak, Deepak, et al. \"Curiosity-driven exploration by self-supervised prediction.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2017.\n"}