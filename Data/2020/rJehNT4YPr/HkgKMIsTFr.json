{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all. Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images. The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology. Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload. \n\nThe proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper\n\nThe idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment. The idea has a cross-disciplinary nature and is fairly interesting to me. I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works. \n\nOne minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them. However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.\n"}