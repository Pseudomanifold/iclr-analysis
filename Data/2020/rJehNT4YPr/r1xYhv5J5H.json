{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small. The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree. Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy. \n\nThe main idea is reasonable, but it requires that the models to compare all perform reasonably well. Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach. \n\nAnother potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes. \n\nAnother question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes? What is a general guideline for one to choose this number $k$ given a new application scenario? \n\nThe unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest. It is also nontrivial to control that the images contain only one salient object per image.\n\nHence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet. \n\n"}