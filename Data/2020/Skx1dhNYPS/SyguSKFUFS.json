{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission proposes an efficient method to do video classification for long-range actions. The authors propose a two-stage method, first selecting salient clips from a long video and then performing inference only on the selected salient clips to achieve good trade-off between accuracy and speed. The framework can be end-to-end trained. \n\nI give an initial rating of reject because (1) paper is not well written, many details missing (2) dataset are not representative (3) performance is not good, and the authors do not compare with state-of-the-art. My detailed comments are as below.\n\n1. The idea of the paper is to select salient clips for efficient video classification. It should be a follow-up work of SCsampler, as stated by authors as well. This submission moves one step further to make it end-to-end, which is straightforward. The contribution is limited.\n\n2. Paper is poorly written, many details are missing. For example, \n    - What are the concept kernels? \n    - How do they learned? \n    - What is the network architecture being used to learn them? \n    - What is K and N in the experiments? \n    - What does the learned concept look like (some visualizations)? \n    - Any ablation studies on with or without concept kernels? \n    - How do you train the network? like learning rate, epochs, etc.\n    - How do you evaluate the model? like input clip length, fusion strategy, etc.\n    - How is the comparison between gating and context gating module? \n\n3. The datasets used in this submission are not representative. The breakfast action dataset is small and outdated. There are  few literature working on that dataset. Charades dataset is ok, but charades is a mutli-labeled video dataset, which is not appropriate to showcase clip selection. I think using a large-scale popular dataset is a must, e.g., sports-1m, kinetics400, HACS dataset, etc. \n\n4. The performance of the proposed technique is not promising. For example, the state-of-the-art method on Charades is already 55+ mAP. However, this submission only achieves 20+ mAP.  I understand this is not a fair comparison. But there are so many state-of-the-methods, such as I3D (CVPR 2017), non-local (CVPR 2018), R(2+1)D (CVPR 2018) and slowfast (ICCV 2019), the authors need at least compare to one of them. Otherwise, your claims are not well supported. \n\n5. SCsampler use a clip of motion and residual images as input to train the sampler, while this submission only use a single RGB frame. It would be better to make a comparison. And this maybe the reason why the performance is not promising. "}