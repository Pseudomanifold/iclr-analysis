{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a frame-selection method for activity recognition in videos. By using a small CNN to process all frames to select a subset, this can greatly improve runtime performance. The proposed approach is straightforward, combining two CNNs: one to select frames and one to classify them. Overall, the paper is well organized (though missing some details) and the results show some initial promise. However, due to some major issues with the experimental results (details below), my initial rating is to reject.\n\nQuestions:\n\n1) How is the input to the HeavyNet formed? Mostly for the 3D networks, is it a concatenation of all the frames where a > 0.5? It seems that it could be difficult for 3D networks to learn good representations when the inputs are not temporally evenly distributed. Did you notice any negative or strange effects of using these inputs to a 3D CNN?\n\n2) Related - how evenly distributed are the selected frames?\n\n3) \"Latent concepts\" - how are these implemented? It is a bit unclear in the current paper. It seems like they are a set of vectors that are learned. It would be helpful to clarify k_i in the paper.\n\n4) Figure 6, right - Is the y-axis also the percent of selected frames or is it accuracy?\n\n5) Section 4.4 \"We observe a drop in performance when using this variant of the Timestep Selector.\" Which timestampy selector is leading to a drop in performance? Is it the frame (blue) or context (orange)? If the context, could this be due to  selecting fewer frames?\n\n6) Section 4.4, \"We conclude that the cost of selecting timestep using LightNet is marginal to that of the HeavyNet and classifier.\" This conclusion is unclear and seems different from what this section is about. Which selector is better/used in the paper? Or does it not matter?\n\n\nExperimental Weaknesses:\n\na) There are claims in the paper: \"report state-of-the-art results on two datasets for long-range action recognition: Charades and Breakfast Actions\" However, there is no comparison to any other work, of which there are many. Section 4.6, which has an experiment on Charades, only provides a graph comparing two models. Of these two, the top performance is only around 26. \n\nThe graph also has the y-axis labeled as % accuracy, but the standard evaluation metric on Charades is mean-average precision as it is a multi-label dataset. Are the reported numbers accuracy (if so, how is accuracy computed for the multi-label setting?) or is it mAP? If it is mAP, the state-of-the-art is significantly higher than 26 (many papers in the 30s, 40s and even 50s now). Thus the state-of-the-art claim is invalid for Charades.\n\nFor these two datasets, there should be a table comparing this papers results to previously reported results in order to claim state-of-the-art. Without that, the claim is unsupported.\n\nb) The proposed \"latent concept kernels\" are not experimentally evaluated at all. Does removing them hurt/help? How many are selected? What is the effect of using more/fewer kernels?\n\nc) This work is quite similar to \"End-to-end learning of action detection from frame glimpses in videos.\" Though that work has a possible advantage of not needed to process every frame, even with a light network. It dynamically selects frames by temporally jumping around the video. It would be good to compare the proposed approach to that previous method.\n\n\n\nOther feedback:\n\nFor Figure 4, it would be helpful to add a baseline using all timesteps for the HeavyNet. This would show the gap between the expensive, full network and the performance using various selected frames and give an expected upper bound to the performance.\n\nFigure 7 could be improved to better show the speed performance gains of the approach. The x-axis is not to scale, making it hard to visually see the difference between the approaches. "}