{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new method of scaling multi-agent reinforcement learning to a larger number of agents using evolution. Specifically, the procedures (EPC) involves starting with a small number of agents, training multiple sets in parallel, and doing crossover to find the set of agents that generalize best to a larger number of agents. This is motivated by the intuition that agents that perform best in small groups may not be the ones that perform best in larger groups. These claims are empirically verified in three games based on the particle world set of environments. \n\nI\u2019m a fan of \u2018automatic curriculum learning\u2019-style methods designed to gradually add complexity to improve final agent performance, and this paper is no exception. The proposed method is simple, but it makes sense. I like the fact that it is both RL algorithm agnostic, and that it can be largely executed in parallel, which means that it introduces only small training time overhead. I also like the proposed method of making the Q function agnostic to the number of agents and entities using attention (although whether these policies incorporate information from previous time steps, or if they can be made to do so). The experimental results are thorough, comparing to MADDPG, a simpler version of their curriculum without evolution, and a recently proposed method for scaling up MADDPG, showing that EPC consistently outperforms all of them, and is more stable. I think the complexity of the environments is also suitable for this style of paper, although it would be nice to see results in a more open and complex domain such as the recent NeuralMMO game.\n\nOverall, I think this is a good paper and I recommend acceptance.  \n\n\nSmall fixes:\n-\t\u2018asThank yoTha\u2019 -> not sure what this means\n-\tN1 agents for of role 1 -> N1 agents of role 1\n-\t\u201cWe adopt the decentralized learning framework\u201d --- even if each agent has their own Q function, if that Q function is centralized (uses the observation of all agents) then training is still centralized \n"}