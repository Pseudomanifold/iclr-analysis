{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, the authors present a method and analysis for generating an artificial datatset to fool pre-trained language models into classifying the sentences, which are random strings of words, as being grammatical. They test their method on BERT and RoBERTa. They take (Ro)BERT(a) that is fine-tuned on CoLA and use it to select \"sentences\" that fool the model. The sentences they use are random strings of words. The method is evolutionary: the top 100 of the first n generations of sentences if selected to spawn off 10 new, slightly altered, sentences, where the sentences are ranked according to the probability of them being classified as grammatical. \n\nI would recommend this paper be rejected. Here are some of the issues with this paper,\n- The results section provides no hard numbers. The paper only descriptions of the results like \"obtaining a result similar to the original score\" and \"the models are always fooled.\" This alone is a disqualifying factor\n- The presented algorithm adversarially selects random strings of words that fool (Ro)BERT(a). From my reading, it seems like this is used as partial evidence that (Ro)BERT(a) is incapable of distinguishing between grammatical sentences and random sentences. The authors claim that since the sentences that are generated from the algorithm are ungrammatical strings of random words, the models can not distinguish \"correct sentences and selected random collections of words\" (section 6.2). I don't think this is true. The algorithm is explicitly designed to pick out these random strings. The fact that an algorithm that does random word selection and substitution doesn't land on grammatically sound sentences is not evidence that the models used in loop are failing at learning concepts of grammaticality. (I'd like to be clear here and say I'm not claiming that these models fully understand grammaticality, I just don't believe this is good evidence that they don't.)\n- The sentences in Table 2 don't appear grammatical to me, though the authors say that they are. Example from table 2, \"NY restaurant student Shelly dove lack.\" These results are used to claim that sometimes the algorithm does converge on grammatical sentences, these conclusion seems unfounded to me. \n- In section 6.4 the authors claim that if a BERT model fine-tuned with some of the algorithm's generated sentences is not \"fooled,\" i.e. if the algorithm with this model converges on grammatical sentences, then the \"dataset was not built to deal with random sentences.\" I think that this is trivially true. CoLA has no random sentences in it, so random sentences are naturally outside the dataset distribution.\n- In section 6.5.2 it is unclear if there is a held out test set of \"fooling\" sentences.\n\nThe paper concludes that RoBERTa is more robust to adversarial attacks than BERT. This would be a neat conclusion but I'm afraid there is no concrete evidence of that presented in the paper. The biggest issue with the paper is the lack of empirical results, we are given no numbers to demonstrate performance making the results impossible to interpret.\n\nSmaller issues:\n- Figure 1 is unnecessary as it doesn't offer additional clarification of the algorithm.\n- There is some odd phrasing through the paper. I'd recommend getting some edits from another native English speaker to help polish the paper."}