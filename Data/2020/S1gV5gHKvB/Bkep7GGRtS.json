{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This is an analysis paper that aims to find weaknesses in pretrained language models by constructing nonsense strings of words that will nonetheless be classified as \"real sentences\" according the model. The paper focuses specifically on BERT and RoBERTa, two related but different pretrained LMs. The authors use an evolutionary algorithm to generate the adversarial examples. They show a handful of examples of nonsense strings that receive high acceptability scores according to BERT and RoBERTa, and report distributions of the models' scores for various sets of generated adversarial examples. The authors do some manual labeling to address the fact that the evolutionary algo might end up generating sentences that are in fact valid, though the details on how they do this manual labeling and what the results are are scant.\n\nEvaluation: I don't think the paper is above the bar for ICLR as is. I think the idea of finding such vulnerabilities in pretrained models is interesting, but this paper does not offer much insight on the topic. The authors' purported goal is to \"discover if and why it is possible to fool [language] models\" like BERT and RoBERTa. I think prior work has established the \"if\"--we know that BERT and similar models can be made to fail using various types of input perturbations (see [1,2,3] below). The particular type of weakness explored here--misclassifying random input as \"acceptable\"--has not been done, as far as I know, but I don't think the results are particularly surprising as they are consistent with what we know pretty well: these models are thrown off by inputs that are outside the distribution of their training data. Thus, I think the paper needs to offer a bit more on the \"why\" part of the question. There is no clear quantitative or even systematic qualitative evaluation of the types of examples that are leading the models to fail, and thus its hard to learn much from the results. More specific questions for the authors below.\n\n* You say you manually split the generated sentences into grammatical/not grammatical--how did you do this? What was the criteria and agreement? Looking at your examples of grammatical sentences, I don't agree that they are grammatical.\n* In Figure 2 vs. 3 you show distributions of scores for cases when the genetic algo converged to generating grammatical vs. ungrammatical sentences. What does this mean? Does this mean that *all* of the sentences that the genetic algo converged to were grammatical? Or just some? Also, you make claims about the bimodality of the distributions in these figures--is it actually the case that all the high-scoring sentences were grammatical and all the low-scoring sentences were not? How did you verify this?\n* Can you comment on the fact that even when the genetic algo. ends up producing \"good\" (i.e. ungrammatical) adversarial examples, the distribution is quite flat. This does not convince me that there is some reliable vulnerability in the LM. I'd expect that if there was some reliable vulnerability, the genetic algo should be able to exploit it and produce a set of examples such that nearly all of them receive high scores.\n* I am concerned about the lack of controllability of the genetic algo. I think it is hard to use this to make any useful general claims. What can we conclude from the fact that a run, or even a handful of runs, failed to find examples that fooled the model? This does not mean the model is unfoolable. Similarly, if the examples found fool BERT and not RoBERTa, that doesn't mean BERT is more foolable in general, only that we happened to find some examples in which that was the case. I think there needs to be more clarity about assumptions and experimental design to help us make useful conclusions on the basis of the genetic algo's output.\n* You use the word \"universal\" to talk about properties that are true for both BERT and RoBERTa. However, it is worth emphasizing that these models are actually remarkably similar in architecture and thus may behave more similarly to one another than two completely unrelated LMs might. Similarly, I appreciate that you run some experiments using different initializations at finetuning, but we are limited in what we can conclude from these experiments too, since all share the same pretrained BERT base and thus are very likely to share similar vulnerabilities.\n* Worth having the paper proofread by a native speaker, some wording is a bit unnatural\n\n[1] https://www.aclweb.org/anthology/P19-1334.pdf\n[2] https://cocolab.stanford.edu/papers/DasguptaEtAl2018-Cogsci.pdf\n[3] https://www.aclweb.org/anthology/P18-2103.pdf\n \n"}