{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies a multiagent dialog task in which the goal of the learning agent is to generate natural language actions that elicit a particular action or emote from the other agent. Specifically the paper compares two Reinforcement Learning agents against an Imitation Learning baseline and shows that the RL-agents can achieve higher levels of task completion on seen and unseen dialogs. The experimental setup features a \"base agent\" that is supervised-trained to mimic human-players. The goal for the RL-agent is to produce dialog utterances that cause this base-agent to produce the desired action/emote. Two A2C-based RL agents are presented: the first chooses from a discrete set of 50 topics - then utilizes a pre-trained model to generate dialog based on the chosen topic. The second relies on a supervised-trained (again from Light's human dataset) model to generate 50 candidate dialog utterances, and then chooses amongst these. The baseline agent (dubbed 'Inverse Model') is trained via imitation learning to mimic the human players. All agents leverage recent advances in transformer architectures. Results show that RL agents can improve on this baseline agent both when given 1 and 3-steps of interaction for both action and emote tasks.\n\nOverall, this paper provides two interesting methods to incorporate RL training into large-scale transformer models while still generating complex natural language actions (assuming access to a pre-trained model or dataset of expert demonstrations). Broadly, I think the task is interesting and the sample dialogs show convincingly natural dialogs from both the RL and environment agents. I can see some promise in this as a proof of concept for a learning system that both interacts naturally and still accomplishes a goal, but I have several reservations that can hopefully be addressed:\n\nIt's unclear what the primary contribution(s) and takeaway from this paper is. I see possible aspects of (1) a new benchmark task for RL-based goal-oriented dialog - but no mention is made of release of models, (2) methods for Imitation + RL training of transformer-based policies - but unclear how novel these methods are, (3) proof of concept system for natural-goal-oriented-dialog - but it's not clear how faithfully environment-agent replaces actual humans.\n\nMy largest concern is the use of the baseline (environment) agent to simulate a human user. Due to the difficulty of accurately simulating human dialog (even in situation contexts such as these), it's difficult to distinguish between genuine advances in the quality of goal-oriented dialog versus overfitting against the simulated human (environment agent). I believe this paper would be greatly strengthened by including a human user study which evaluated the various approaches against real humans, or equivalent analysis to convince a reader that the RL agents are not simply optimizing to exploit weaknesses in the environment-agent.\n\nOn this note, little analysis is presented about the fidelity of the environment-agent. The set of 4 examples in Table 3 helps in this direction, but nothing is shown for 3-step dialogs, and there are no examples of failure. Specifically it would be great to see that the environment agent actually generates relevant responses to the action of the RL-agent and doesn't simply produce the dialogs/actions/emotes seen in the human-human dataset. It would be great to have a much more comprehensive appendix addressing this concern.\n\nHorizon of learning is very short. 1-step RL is contextual bandit learning and A2C is likely not the best choice for contextual bandit settings. 3-step is better, but I wonder if the RL-agent is actually generating dialog that conditions upon the utterances of the environment-agent (instead of just trying 3 of the top-scored actions rather than 1). Some 3-step dialog examples would help demonstrate the presence of contextually relevant dialogs.\n\nIn my view, this task is not true multiagent learning since the base-agent (environment agent) is not learning or changing its policy in concert with the RL-agent. This removes all the non-stationarity of true multiagent Reinforcment Learning. For this reason the last sentence of related work strikes me as false (although I would wholehartedly agree that the Light task, as administered to humans, is indeed multiagent).\n\nFinally I was unable to find any mention of the number of training runs performed for RL agents. As RL algorithms can be subject to learning instabilities and policy collapse, it is common practice to analyze the robustness of an RL-agent by performing several independent training runs for each agent. "}