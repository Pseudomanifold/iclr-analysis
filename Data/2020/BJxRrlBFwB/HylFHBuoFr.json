{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game. To do so, the authors cast the original chit-chat task into a POMDP.\nIn this setting, the observations are the agent's previous utterances,  the other (static) agent's utterances and actions, and the game description. The actions either consist in selecting a topic (sec 4.2) or to select one utterance over a filtered set of human utterances. Finally, they report their results and observe that the RL approaches outperform the supervised learning models.\n\nI have several significant concerns about this paper on several facets, i.e., on the novelty and contribution part, on the algorithm part, and the initial claim.\n\nContribution part: \n  - It is an empirical paper as it assesses how scalable is an approach in a challenging setting. Such papers can be beneficial when they provide insights, extensive experiments, and share the authors' feedback. Unfortunately, those sections are not present in this paper. As a hint, the paper contains 7 pages to explain the method and 1/2 page to list the results and 1/2 page to analyze them, and there are no experimental appendices. Despite unarguable work (the authors perform heavy experiments on a big dataset), there are no simple ablation studies (10, 50, 100 topics? inverse model vs. non-inverse model), no comparison between the two models (e.g. how often the 50 utterances overlap? What are the error made by the models? Are there similar, complementary? Quid of mixture of expert), no qualitative analysis ( errors and/or limit analysis, why actions are harder than emotes?). Besides,  I assume that the authors spend some time fine-tuning the network, but there is no hint or take-away about the training difficulties: what can be generalized to other domains?  I agree that it is impossible to perform *all* those analysis, but I would have like *some* of them\n - Again, I acknowledge the considerable engineering work in this paper. However, it seems that most of this codebase contribution was already published in (Urbanek et al 2019). \n - The paper examines two methods: they either learn to predict the topic or perform topic retrieval. Even if those approaches make sense, they are not novels. For instance, the topic prediction is close to state tracking models where the state would be defined by the topic. There also exists an extensive literature of sentence retrieval, and some relevant works in RL such as (Dulac-Arnold, 2015)\n\nAlgorithm part:\n - This paper focuses on RL, but I have a few concern about the soundness of the experiments. It is also surprising that RL training is summarized in two sentences when it is supposed to be the core contribution of the paper.\n  \nI am particularly concerned with the RL experiments run with n=1. In this scenario, the agent only takes one action before the end of the episode. Thus, there is no planning involved, and applying RL has little interest here. At least, I would have appreciated a note on the impact of exploration.\n\nMore importantly, the core issue is the following: The reward is defined by 1 in case of success, 0 otherwise. If we apply policy gradient for with n=1 (e.g. A2C), and if we ignore the baseline, for now, the setting is strictly equivalent to applying a cross-entropy loss for every new success while discarding failure cases. In other words, the \"RL\" training procedure is equivalent to performing data augmentation with a supervised model.\n\nIn the case of A2C, the loss includes the td-error, an entropy regularizer, and a baseline. Yet, the predicted value function only learns to predict the success ratio as there is no bootstrapping n=1 while computing the td-error. It is thus equivalent to an auxiliary loss. While the baseline makes do penalize negative samples,  the underlying reward definition issue remains, and the current setting is more or less equivalent as performing hidden data augmentation with a supervised training loss.  \n\nNote that this observation does not apply for n=3, and I have no concern regarding those experiments. I am only missing the training hyperparameters.\n\nClaim:\nThe paper states that the paper somehow bridges the gap between the chit-chat setting and the goal-oriented setting. However, they cast the problem into a goal-oriented setting by having a reward for specific emote or action (which terminate the episode.) Even if I acknowledge the paper's motivations, I disagree that the paper provides a new perspective on this issue.\n\nOther remarks:\nFinally, I would have appreciated a sketch of the models, and a table listing the hyperparameters. \nAs the model work as the utterance level, there is no risk of language drift.\n\nIn the end, I recommend paper rejection for the following reason: \n - I feel that the paper is lacking sufficient analysis\n - The RL experiments with n=1 concern me\n - The novelty is extremely limited over the literature and (Urbanek et al 2019). \n - there is no training hyperparameters or experimental take-way which limit the reproducibility\n\nDulac-Arnold, Gabriel, et al. \"Deep reinforcement learning in large discrete action spaces.\" arXiv preprint arXiv:1512.07679 (2015).\n\n"}