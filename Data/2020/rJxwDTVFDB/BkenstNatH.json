{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new understanding of dropout on top of variational dropout, which shows that training with dropout equals to maximizing an empirical variational lower bound on the log-likelihood. This paper shows that the log posterior have the same lower bound when the inference model p(y|x) is defined by different methods, i.e., the arithmetic mean of predictions with different dropout masks, the geometric mean, and a power-mean family as an interpolation between these two cases. This indicates that with the same training objective, different inference methods have different gaps to the posterior lower bound. Intuitively, a smaller gap might lead to better performance. The paper then uses an existing result from Liao & Berg (2017) to show that the gap can be bounded by the variance of prediction probability. With empirical observations, the paper gives an unrigorous conclusion that the deterministic inference with dropout rate 0 achieves the smallest gap. However, this does not hold theoretically due to the extra bias on expectation. The optimality of deterministic inference does not hold empirically due to class imbalance or discrepancy between training and test sets. The paper then proposes two practical solutions for better inference: 1) tuning dropout rate, softmax temperature, and the power mean parameter; and 2) deterministic inference with tuned softmax temperature. By using the first inference solution, the performance on PTB and Wikitext2 LM can be improved by 2-3 on perplexity but is still slightly worse than the SOTA achieved by the mixture of softmaxes.\n\nThe idea of analyzing the gap to variational posterior lower bound for different dropout inference model is interesting. The derivations are correct. The organization is not perfect and readers might find it hard to follow here and there, but the main idea is understandable. Experiments show that the suggested tuning of inference hyperparameters can bring improvements to LM tasks, which is convincing. However, there are still major gaps between the theoretical analysis, the conclusion and the empirical solution (please see the detailed comments). Such gaps make the main contribution questionable and make it as a pure empirical paper on its value.\n\nDetailed comments:\n\n1) Reducing the variance of output prediction can reduce the gap on variational posterior, but how does the gap relate to the generalization error? The current paper only indicates that a small gap gives more consistency between the true objective and the optimized objective defined on the training set: they can be still far away from the expected posterior over data distribution. Hence, it is hard to directly relate \"reducing the gap\" and \"improve the test-set performance\".\n\n2) As the author mentioned in Section 3.4, reducing the dropout rate causes a bias issue on the expectation. So it is not clear whether deterministic inference with zero dropout rate can achieve the smallest gap or not. In this way, the conclusion is only supported by the empirical observations but not the presented theoretical analysis.\n\n3) One main contribution of this paper is the power-mean family of dropout. However, only one member (alpha=0.5) from the family has been evaluated in the experiments, and it does not achieve the best performance in most experiments. So this contribution seems not practically useful according to the empirical result.\n\n4) It is not clear how the prediction variance is reduced gradually in order to generate the results in Figure 1(b). I guess reducing dropout rate is not the correct way to do so since it causes the bias issue and the tightness will be influenced."}