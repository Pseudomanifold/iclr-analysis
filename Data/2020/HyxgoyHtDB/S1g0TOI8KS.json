{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper was confusing and difficult to read.  I think it is trying to devise a methodology that improves upon a given expert policy, but I am not confident whether if this is the objective.  The main contribution of the paper is the algorithm POLISH on page 5.  However, it was unclear how MCTS with UCT is used with the expert policy in the algorithm.  The first mention of the 0-1 loss objective is in section 4.1, and Algorithm 1 on page 5 claims to minimize this loss on line 15.  However, in the experiment, the loss function is then switched to another L(D, \\pi) = D_{KL}(\\pi || \\pi*) + H(\\pi) + Lv(D, \\pi).  Why the switch?  And what is the definition of the third term?  By examining the experimental results, Algorithm 1 seems to outperform PPO.  Since the expert policy is known and given to Algorithm 1, I speculate Algorithm 1 is only replicating the expert policy, which would have outperformed PPO that has to learn from scratch.  Thus, the comparison does not seem fair.  It would be interesting to see how POLISH compares with the expert policy.\n\nOther comments:\n\n* Page 1 in Introduction: \u201cHowever, these models suffer from the problem that even small difference between the learned policy and the expert behavior can lead to a snow-balling effect, where the state distribution diverges to a place where the behaviour of the policy is now meaningless since it was not trained that part of space\u201d.  Do you mean that the algorithm diverges instead of the state distribution diverges?   The agent may incur errors in a space that has not been observed, but it should be able to learn eventually.  So, what is causing divergence in states that have not yet encountered?  \n\n* Page 4 in The POLISH Algorithm Main Algorithm: the 0-1 loss function is defined to be \u201cL(D, \\pi) = 1/|D| \\sum_{s,a*}\\in D (I(\\pi(s) \\neq a*)), where a* is the expert policy\u2019s selected action.  I think it makes more sense to write \u201cL(D, \\pi)\" as 1/|D| \\sum_{s,a*}\\in D I ( a \\neq a* : a ~ \\pi(s)).  Also, in RL, we don't say that the policy receives a reward, but rather the agent receives a reward. "}