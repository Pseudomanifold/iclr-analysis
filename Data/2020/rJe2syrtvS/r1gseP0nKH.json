{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "  *Synopsis*:\n  This paper focuses on current limitations of deploying RL approaches onto real world robotic systems. They focus on three main points: the need to use raw sensory data collected by the robot, the difficulty of handcrafted reward functions without external feedback, the lack of algorithms which are robust outside of episodic learning. They propose a complete system which addresses these concerns, combining approaches from the literature and novel improvements. They then provide an empirical evaluation and ablation testing of their approach and other popular systems, and show a demonstration on a real robotic system.\n \n  Main Contributions:\n  - A discussion of the current limitations of RL on real robotic systems\n  - A framework for doing real world robotic RL without extra instrumentation (outside of the robot).\n\n  *Review*: \n  Overall, I think the paper is well written and provides some nice analysis of the current state of RL and robotics. I am not as familiar with the RL for robotics literature, but from some minor snooping around I believe these ideas to be novel and useful for the community. I have a few suggestions for the authors, and a few critical pieces I would like added to the main text.\n\n  Critical additions:\n  1. I would like some more details on your simulation experiments. Specifically:\n    - How many runs were your experiments? \n    - What are the error bars on your plots?\n    - What ranges of hyper-parameters did you test for tuning?\n\n  2. I would quite like the discussion of the real world tasks from the appendix to appear in the main text. Specifically, giving the evaluation metrics you mentioned in the appendix. \n\n  Suggestions/Questions:\n\n  S1: It is not clear if a VAE is the best choice for unsupervised representation learning for RL agents. Although a reasonable choice, Yashua Bengio recently released a look at several unsupervised techniques for representation learning in Atari which you may want to look at: https://arxiv.org/pdf/1906.08226.pdf. \n\n  Q1: Did you try any of the other approaches on the real robotics system? Or was there no way to deploy these algorithms to your specific setup without instrumentation?"}