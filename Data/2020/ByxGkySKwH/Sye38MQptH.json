{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper uses a generative model to assign anomaly scores. By its construction, it can provide provable performance guarantees. Experiments do not make unreasonable assumptions such as the ability to peak at the test data, unlike much previous work.\nMy primary concern is that they should show performance on CIFAR-100 not just CIFAR-10, and I certainly hope these experiments will be included during the rebuttal. Overall experimentation is thorough and competently executed, and the proposed technique is sufficiently novel.\n\nSmall comments:\n\n> adv OOD detection with uniform ball perturbed\nThis is a good way of formulating adversarial OOD detection.\n\nA possibly related work is _Early Methods for Detecting Adversarial Images_ (2016) since it uses covariance matrix information for detecting adversarial examples. This paper should cite _Open Category Detection with PAC Guarantees_ by Liu et al. (ICML 2018) since this also involved provable guarantees for OOD detection."}