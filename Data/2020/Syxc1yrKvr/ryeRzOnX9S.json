{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new training objective for generative models that combines the objectives of VAEs and GANs. The objective is equivalent to minimizing the Jeffreys divergence (a type of f-divergence) between the true probability of the data and its probability under the model.  Furthermore, the objective comes with a knob to tradeoff the relative importance of each of the two terms.  In addition, the authors develop a implicit likelihood formulation which they claim and show empirically to outperform typical explicit formulations typically used in VAEs. \n\nOverall, it is an interesting paper that reuses a few good ideas to develop a novel training objective. The results show that using an implicit likelihood helps (Figure 2) and that it does relatively better than either GAN or VAE approaches. I have detailed comments below about the organization of the paper, some of the experimental claims as well as a few other works which may be good to cite. \n\n\n- Paper organization: I would suggest moving the related work to after the background.\n\n- GANs and VAEs are not models per se but rather training frameworks for generative models.\n\n- While VAEs and GANs can work on many types of data (at the very least continuous), your model seems to be developed for images. Could you make it clear what changes would be needed to apply it to non-image data?\n\n- There are many minor grammatical errors throughout the text.\n\n- It would be useful to provide the full algorithm somewhere (e.g., using an algorithm \"box\")\n\n- Possible related work. It may be worth citing these two paper:\n  - f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization, NIPS'16\n  - Deep Generative Learning via Variational Gradient Flow, ICML'19\n \n- It would be useful to mention early that for IS higher is better and LPIPS lower is better.\n\n- Even though Figures 2 and 3 (to a certain extent) seem to show that results are somewhat robust to the exact value \\lambda how would you propose to set it in practice?\n\n- Figure 3 Left (CIFAR 10), it's not absolutely clear to me that alpha-GAN and perhaps AGE isn't at least as good as your approach. The meaning of the units of the axes is a bit unclear. Do you have a particular reason to prefer your method over these in this case?\n\n  Related: in Table 1, why are there no bolded results for CIFAR + Reconstruction?\n\n- In Figures 5 and 7 the reconstruction of IJAE sometimes seems to be pretty far from the original image (i.e., it's not that it's blurry as for VAEs, it's that the model seems to be reconstructing a completely different image). How do you explain these results?"}