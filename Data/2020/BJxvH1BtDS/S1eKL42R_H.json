{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper applies three-head neural network (3HNN) architecture in AlphaZero learning paradigm. This architecture was proposed in [1] and the paper builds upon their work. In AlphaGo and AlphaZero 2HNN is used, which predicts policy and value for a given state. 3HNN also predicts action-value Q function. In [1], the three new terms are added to the loss to train such a network, and the network is trained on a fixed dataset. The paper utilizes the same 3HNN idea with the same loss, and the contribution is that 3HNN is trained synchronously with MCTS iterations on an updating dataset (\u201cAplhaZero training style\u201d). Learning speed of 3HNN is shown to be higher than that of 2HNN. The special attention is drawn to varying the threshold expansion parameter, as the 3HNN architecture allows to set it above zero, while 2HNN does not. The approach is demonstrated on the game of Hex. Results are presented on two test datasets: positions drawn from a strong agent\u2019s games and random positions. Labels in both datasets are perfect, obtained by a special solver.\n\nI tend to reject the paper, because the demonstrated results suggest that the models were not tuned well enough. Indeed, the paper claims that the parameters were not tuned. The paper claims using threshold expansion > 0 to be one of the main advantages of 3HNN. However, the best model in the experiment is the one with the parameter equals zero. Overall, for a purely experimental paper, the experiments are too crude.\n\nMain argument\n1.\tIt seems that some of NN models didn\u2019t learn at all:\n           \u2022 \tFigure 2, 2HNN model. MSE on both the left and right plots is not improving. Moreover, on the right plot it \n                fluctuates around 1, which is the performance of a random guess.\n           \u2022\tFigure 3, right. MSE of the 3HNN model is not improving. Probably, random positions are too unnatural and \n                nothing similar is presented in the dataset drawn from MCTS iterations.\n2.\tSupmat reveals, that some of the models are in fact learned using the different loss than it is said in the paper. In particular, data augmentation term is sometimes on and sometimes off. A disabling scheme is suggested, depending on the dithering threshold and the number of the moves played before the state s. Some models use the scheme, for others the term is always on. This should be clearly stated in the main text, not in the supmat. \nAlso, there is an experiment in the supmat, when the data augmentation term is always off. The influence of this term is itself interesting, as it is one of the reasons 3HNN is learning q-function at all. However, introduction of this term itself is the contribution in [1]. I suggest to add to the main part of the paper the experiment, comparing three regimes: 1) with the scheme, 2) term always on and 3) term always off. In fact, it is almost done, as all three regimes are used in different figures, but somewhy the final comparison (with other parameters fixed) is not shown and partly concealed in the supmat. It could become methodological improvement of the paper over [1]. \n3.\tWhen the leaf node s is expanded, the v function of the new node s\u2019 = s \u222a a is initialized to predicted q(s, a). It is one of the advantages of 3HNN and allows node expansion threshold. When s\u2019 itself is expanded, how do you merge q(s,a) with backup values during mcts iterations?\n4.\tNothing is said about the architecture of NNs. If the representation of the state is the same as in [1], it should at least be mentioned.\n5.\tHow exactly does figure 2 shows that one iteration AlphaZero-2HNN is 5-6 times slower, than AlphaZero-3HNN. Probably the definition of data point in figure 2 is missing (e.g. one data point corresponds to one MCTS iteration).\n6.\tFigures 3 and 4 basically shows the same experiment, but with different models: AlphaZero-2HNN versus AlphaZero-3HNN with threshold 0 on Figure 3 and AlphaZero-2HNN versus AlphaZero-3HNN with threshold 1 on Figure 4. They could be united in one figure.\n7.\tResults from Figures 3 and 4 suggest, that 3HNN with threshold = 0 is better, than with threshold = 1. However, the paper claims setting threshold > 0 as an advantage. If it allows to save time (as each mcts iteration is faster), maybe they should be compared plotting time on x-axis (like in Figure 2)?\n8.\tPlease provide error bounds in table 2.\n\nAdditional arguments\nArgumentation presented in this section didn\u2019t affect the score. However, it might improve the paper.\n1.\t3HNN predicts q(s,a), which should be equal to v(s\u2019), where s\u2019 = s \u222a a (state after action a is taken in state s). It would be interesting to see how condition q(s,a) == v(s \u222a a) holds during  3HNN models learning. It can be checked on a first dataset (drawn from games), or a special random dataset, containing consecutive positions, could be generated. Probably, this condition could potentially be an additional loss term.\n2.\tAlso, it would be interesting to see how the condition between p and q holds. The higher q(s, a), the lower should be p(a). There is an interesting illustrating figure 7 in supmat, it may be presented in the main text. Also, it could be interesting not only for the first move. \n3.\tThe supmat claims that the motivation for turning off data augmentation term is that it assumes that in the selfplay game both players are selecting the best actions produced by search. Is it connected with the fact, that in the game of Hex all states are either winning or losing, according to theorem proved by Nash? How does this term would work for games with a draw trend, for example chess? In chess, for a lot of states the \u201cground truth\u201d v(s) would be close to zero and there is no action guaranteeing the win.\n4.\tThe paper claims \u201cElo (Elo, 1978) improvement does not always correspond to real playing improvement\u2014the monotonic increase of Elo score did not capture the fluctuation in strength\u201d. Citation needed, what fluctuation in strength is not captured? Is it specific to game of Hex? For example, Elo is used as the main measure of total agent strength in AplhaZero papers, as well as by chess community (both chess programs and human players).\n\nMinor comments\n1.\tPage 4, section 2.2: Even though ... . Our summarization -> Even though ... , our summarization.\n2.\tPage 4, table 1: mvoe -> move.\n3.\tPage 7, bullet point above section 4.4: perfect -> perfectly.\n4.\tPage 7, the lowest paragraph. \u201cFor the value learning, however, due to fast search, the AlphaZero-3HNN learned much faster than AlphaZero-2HNN both on games states produced by strong player (T1) as well as examples produced by random play (T2).\u201d \nIt is confusing, it seems that 3HNN on datasets T1 and T2, however, it was only tested on these datasets.\n5.\tPage 8: imposing an an auxiliary task -> imposing an auxiliary task.\n6.\tPage 9: \u201cproduce playing strength significantly stronger than\u201d \u2013 reformulate.\n\n[1] Chao Gao, Martin M\u00fcller, and Ryan Hayward. Three-head neural network architecture for monte carlo tree search. In IJCAI, pp. 3762\u20133768, 2018b.\n"}