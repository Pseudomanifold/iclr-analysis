{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Main comments:\nThis paper proposes a unified embedding for image classification and instance retrieval. The objectives for both classification and retrieval are jointly optimized. To bridge the gap between image classification and instance retrieval, the generalized mean pooling layer with exponent p is proposed. A repeated batch sampling augmentation is introduced to enhance the performance for both tasks.\n\nThose schemes proposed in this paper are shown to be effective through extensive experiments. However, those schemes are more like good techniques to improve the performance instead of bridging the gap between two tasks, rather than principled algorithm designs.\n\nOverall, this paper proposes several schemes including self-supervision, generalized mean pooling, and repeated augmentation to improve the performance of two tasks. The experimental results demonstrate the effectiveness of those schemes. However, those schemes are more like techniques to improve the performance instead of bridging the gap between two tasks. Another possible explanation is the gap between image classification and instance retrieval is not as large as stated. It is important to justify the claims how those schemes bridge the gap between image classification and instance retrieval in a principle manner (rather than good techniques or tricks).\n\nThe p in the proposed GeM seems has limited influence on both classification and retrieval. AAs shown in Fig.2, except for p=1, the performance of p with other values are similar. The authors mentioned that the GeM is a trade-off for between average pooling and max pooling, why not just use two different pooling ops for two tasks. Maybe GeM is a trick for preserving more local information, but I cannot tell its advantage for balancing two tasks. And the optimal p in GeM is obtained by grid search, which makes me double the real value of GeM.\n\nThe authors mentioned that image resolution used for classification and retrieval is quite different. It is not clear whether the gap between those two tasks is mainly because that the backbone network is not capable of capturing details. This needs to be addressed. In addition, there have been several methods that aim to capture both details and global features [1-3] by designing backbone networks with multi-scale ability\n\n[1] Big-little net: An efficient multi-scale feature representation for visual and speech recognition\n[2] Res2Net: A New Multi-scale Backbone Architecture\n[3] Deep High-Resolution Representation Learning for Visual Recognition\n\nThe authors need to put this work in proper context and discuss the differences. It is also better to add more experimental evaluations against these methods to clearly demonstrate the merits of this work.\n"}