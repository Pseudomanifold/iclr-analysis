{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is motivated by the unstable performance of Transformer in reinforcement learning, and tried several variants of Transformer to see whether some of them can stabilize the Transformer. The experimental results look good, however, I have problems in understanding the motivation, the intuition of the proposed methods, the experimental design, and the general implication to the research community that is using the Transformer in their day-to-day research.\nFirst, the paper was based on the hypothesis of the authors that the Transformer is not stable, however, there is no comprehensive study on the unstability, and deep understanding on the root cause of it. It would be much more convincing to give a form definition of unstability and to add experimental study and theoretical analysis to the motivation part, instead of just based on a hypothesis.\n\nSecond, the proposal of the new structures (e.g., reordering the layer normalization, adding the gating layer) are quite ad hoc. There is not very solid motivation and theoretical analysis on why they could solve the unstable problem of the Transformer.  For example, by changing the order of layer normalization, there are direct identity mapping from the first layer to the last layer, making the information flow smoother. However, why this will make the Transformer more stable? The hypothesis and intuitive analysis are not very convincing. For another example, why replacing the residual connection with the gating layer can make the Transformer more stable? It seems to me that these are mostly heuristics, but not verified or strongly motivated solutions.\n\nThird, if the proposal is sound, it should not be effective only for reinforcement learning. It should be able to improve the performance or stability of the Transformer in general (e.g., in NLP tasks). However, there is no experiments and discussions regarding this.\n\nFourth, the experiments on the reinforcement learning is a little narrow, and many famous RL benchmarks and environments were not tested. This makes it unclear whether the proposed approach is generally effective.\n"}