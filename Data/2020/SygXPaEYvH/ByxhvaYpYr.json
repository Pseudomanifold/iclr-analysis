{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# 1. Summary\nThe paper introduces a pre-training procedure for visual-linguistic representations. The model is an extension of BERT (with transformer backbone) to deal with visual input. Images are encoded using object detectors which regions are masked at pixel level. Experiments show state-of-the-art results on different downstream tasks.    \n\nStrengths of the paper:\n* State-of-the-art results on 3 vision-language tasks\n      \nThe weak reject decision was mainly guided by the following two weaknesses of the paper:\n* Clarity of the paper needs to be improved to make the readers understanding the details of the model (see point 2 below)\n* Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below)\n\n      \n# 2. Clarity\nThe paper reads quite well, although some points need to be improved:\n* How were words split in sub-words (Sec 3.2)?      \n* \"For each input element, its embedding feature is the summation of four types of embedding, ...\": it is not clear how you sum embeddings. E.g., token embedding has 30k dimensions while image one has 2048 dimensions.\n* \"It is attached to each of the input elements, which is the output of a fully connected layer taking the concatenation of visual appearance feature and visual geometry embedding as input\" -> this is not clear; what output are we talking about? What is the geometry embedding? I suggest to describe the two features first and then say at the end of the paragraph that the representation is the concatenation.\n* \"For the non-visual elements, the corresponding visual appearance features are of features extracted on the whole input image\" -> what is the intuition of having the full image here? Some terms do not need to have an image associated (e.g., verbs or articles). Do you take care somehow of that?\n* Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked? To my understanding the answer is no: what's the intuition of this?\n* Segment embedding: is this important? This should be easy to show with an experiment in the ablation study of Table 4?\n* It seems that there is a semantic asymmetry of input to the loss during training when considering only the text information (bookscorpus) and the image-text information (conceptual captions): how is training coping with this? Doesn't it make more sense to have 2 pre-training phases: first on text information only and then on image-text information?\n\n\n# 3. Novelty and Motivation\nThe novelty of the paper is quite limited. It strongly relies on transformer networks and then recent success of BERT in the NLP domain. The proposal is an extension of these two ideas to visual domain.\n\nMoreover, there is a body of concurrent work that is very similar to the proposed idea with slight differences (ViLBERT, VisualBERT, LXBERT, UNITER, B2T2), i.e., using transformers with masking operation on the RoIs. It is not clear what is the intuition related to the differences between the methods, i.e.\n* Why one is better than the other; why should someone prefer this pre-training technique wrt others?\n* Why a unified network (this work) is preferred wrt a two-stream one (ViLBERT, LXMERT)?\nIt seems that everything heavily depends on the experiments and empirical results obtained by trying many variants during the prototyping phase. It is missing a bit of understanding and intuition on the reasons why this technique should be used.\n\n\n# 4. Experimentation\nExperiments are the strength of the paper showing state-of-the-art results on 3 vision-language tasks. Some additional analysis is missing:\n* If masking is conducted on the raw pixel, this makes training much slower since you need to perform inference many times. What is the impact in terms of accuracy? Did you carried out an experiment showing that it is better to mask raw pixels instead of conv maps?\n* How long is the model trained for?\n* What is the performance/accuracy on the pre-training tasks?\n* How important is the segment embedding?\n* Footnote 2 should be in the main text (Sec 4.1). It is too hidden, but very important to let the reader knowing about it.\n"}