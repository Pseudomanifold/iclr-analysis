{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a pre-trainable generic representation for visual-linguistic tasks call VL-BERT. VL-BERT extend BERT by changing the input from subsequent sentence to image regions and modify the caption words has the additional visual feature embedding. The authors pre-train the VL-BERT on the conceptual caption dataset and Wikipedia and book corpus dataset, empirical results show that the VL-BERT achieve the SOTA performance on the VCR, VQA and refer expression tasks. \n\nAs the authors mentioned in Table 5, pre-training the visolinguistic representation for vision and language tasks is very popular recently, and 5~6 similar works have appeared recently. One of the nice features I found on this work is it's joint train with text-only corpus and faster RCNN weight. While ViLBERT designs for easier extendable for other modalities, VLBERT is more focus on the representation learning on the vision and language, since the caption input also combines with the visual feature embedding. \n\nOverall the paper is well written and performs extensive experiments/ablations. There is some specific point that is not clear to me or needs further clarifications from the authors. \n\n1: The authors mentioned the improvement over tuning the visual parameters, I wonder what is the details on that? is the region proposal network's weight fixed? if not, how to avoid the shift on the proposal layer? Is the model still has the visual genome target or objective? Which layer is fixed/updated? and what is the optimizer and learning rate scheduler? \n\n2: I notice there is a change in the textual input which take visual feature embeddings. I wonder what is the performance without these features? What is the visual feature input for textual corpus? \n\n3: For the Masked RoI classification with Linguistic Clues, what if there are overlapped regions? what if the detection label from faster rcnn is incorrect? will this introduce any noise? \n\n4: For VCR tasks, it seems the VL-BERT_base w/o pre-training is performed similar compare to the with pre-training (only 0.7% lower on val of Q->A) I wonder what is the reason of this? Is this show the pre-training is not important for the VCR tasks? \n\n5: The VCR tasks also have the object bounding box correspondence, is VL-BERT take any of this supervision for input? If not, how does the VL-BERT learn the correspondence? \n\n6: For refer expression tasks, the VL-BERT_base is actually worse than ViLBERT on the detected regions. It's not a fair comparison since other models use bert-base model. \n\nOverall, I think this paper is well written and has solid experiment results. It will be great if the authors can further clarify the above questions. "}