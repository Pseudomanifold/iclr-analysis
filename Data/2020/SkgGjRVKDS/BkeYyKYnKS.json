{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a new method to deal with the small batch size problem of BN, called MABN. Compared to BRN, MABN has two main contributions: 1) find two statistics, g and \u03c8, in BP to apply moving average operation without introducing too much overhead; 2) reduce the number of statistics of BN via centralizing weight for better stability.\n\nThough, I still have several concerns:\n1.\tYou mentioned that \u201ccentralizing weights ... to satisfy the zero mean assumption\u201d in Section 4.2. In other words, given E(W-mean(W))=0, E(X_{output})=E(W-mean(W))E(X_{input})=0. This equation holds only when W-mean(W) and X_{input} are irrelevant. However, model weights are updated based on the input and the loss function during training. Do you have any proof to ensure W-mean(W) and X_{input} are irrelevant?\n\n2.\tYour \u201cModified Structure\u201d contains two operations, centralizing weights and reducing BN\u2019s statistics. As much as I know, weight standardization can significantly improve BN\u2019s performance, but the effect of weight centralization, i.e. \u201cpart\u201d of weight standardization, is unclear yet. Therefore, I think a vanilla BN with weight centralization should also be included in ablation study. This experiment can help us better understand the pure effectiveness of reducing BN\u2019s statistics.\n\n3.\tI think the results on COCO should be discussed in detail since object detection is an important task in computer vision and suffers from the small batch size regime. I wonder if it\u2019s possible to compare MABN with SyncBN/GN on a higher baseline, such as finetuning from ImageNet pretrained model or training from scratch with at least 6x scheduler.\n\n4.\tWhat about the FLOPS, memory footprint and practical speed? These results would affect the application value of MABN. It would also be great if you can release the code, which would upgrade my rating a lot.\n\n5.\tWhat is the value of m for SMA? Is this value changed in different datasets? This would affect the performance of the proposed method. "}