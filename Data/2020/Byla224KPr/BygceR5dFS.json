{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a theoretical framework for post-processing methods of word embeddings. Given the framework, the authors derive their own methods and then a thorough experimental analysis follows.\n\nWhile the paper reflects thorough and substantial work - both in the theoretical framework and in the experimental part, I have serious concerns about its clarity and about the experimental results and also some concerns about comparison with previous work. All these, unfortunately, make me recommend a reject decision. Below, are more details:\n\n1. Clarity: There are multiple aspects of the paper that I needed to read a few times before I understood what the authors mean. \n\nFirst, the problem definition is not clear: Even when I reached the end of the introduction, it was not clear to me what is the problem that the authors are trying to solve and what they are trying to achieve. One example of this problem is the title of the paper, which suggests that the author performs an empirical evaluation, while in practice they also suggest their own method. But this is a much broader issue - the author suggest very little motivation to how the post-processing method should work, what improvements it should provide and why we should expect such improvements. At some points I felt that this is more of a clean mathematical exercise than a discussion of methods that should improve word representations in natural language.\n\nIn addition, the authors assume a strong background in a very specific post-processing  literature and do not provide any details about its fundamentals. Only on the beginning of section 3 I learned the fundamentals of that framework and basic concepts such as the Gram matrix. I believe a scientific paper should be self-contained, the motivations, goals and fundamental concepts form previous work should be clearly stated and explained. This is not done in this paper, unfortunately.\n\nFinally, it was hard for me to determine where the survey of previous work ends and the contribution of this work begins. Particularly, it seems that the authors propose a unified framework for the methods in previous work and it is not clear which parts of that framework were already discussed in previous work and which are original contributions of the authors. This makes it also hard to estimate how different the proposed method is form the previous ones.\n\n2. Experimental analysis:\n\nFirst, the authors describe their evaluation tasks very briefly and only in the appendix. This is just a list of tasks with no insight about natural language (please see a related comment in the clarity section of this review). Then, the results are reported as a macro-average over many tasks: Given the large number of tasks this is a very crude average, and there is no way that any real insight into the change/improvement of the vectors can be derived from this report. Finally, the reported numbers reflect very minor improvements, if at all, compared to previous post-processing methods and to the original vectors. Again, since these are macro-averages over many tasks, the conclusions that can be derived are very limited (e.g. it might be that the proposed method does improve on some of the tasks and harm the performance on others, or that it keeps the vectors very similar to the original ones - we have no way figuring out the actual picture).\n\n\n3. Comparison to previous work:\n\nAs said above, it seems that the authors view their work in a narrow context of a very specific literature. In fact, the NLP literature contains a large number of post-processing word embedding methods (often referred to as \"fitting\" or \"specialization\" methods). While these methods sometimes build on external linguistic knowledge (e.g. from wordNet or from other manually crafted lexicons), they have also shown useful with automatically constructed constraints, that are similar to the structural considerations mentioned in the paper in the sense that they do not require expert knowledge, they only build on common-sensical requirements from a good vector space for word meaning representation. Some relevant papers are:\n\nFaruqui, M., Dodge, J., Jauhar, S. K., Dyer, C., Hovy, E., & Smith, N. A. (2014). Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166.\n\nMrk\u0161i\u0107, N., S\u00e9aghdha, D. O., Thomson, B., Ga\u0161i\u0107, M., Rojas-Barahona, L., Su, P. H., ... & Young, S. (2016). Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892.\u200f\u200f\n\nMrk\u0161i\u0107, N., Vuli\u0107, I., \u00d3 S\u00e9aghdha, D., Leviant, I., Reichart, R., Ga\u0161i\u0107, M., ... & Young, S. (2017). Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints. Transactions of the association for Computational Linguistics, 5, 309-324.\u200f"}