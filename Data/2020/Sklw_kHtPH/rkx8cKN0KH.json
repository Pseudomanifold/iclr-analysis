{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The experimental results of this paper are not convincing.  \ni) One should use much more powerful networks.\nii) Different variants of Adam should be considered, also with different learning rate schedules.\niii) One should study how newly introduced hyperparameters affect the results of AdamT. \nI find it hard to trust your conclusions like \"Empirical results demonstrate that our method, AdamT, works well in practice and constantly beats the baseline method Adam\" after inspecting Figure 3. Even much greater difference is performance can be due to a small change of hyperparameters, here you have several new hyperparameters. "}