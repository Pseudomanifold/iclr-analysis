{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a new type of Adam variant, namely AdamT. Instead of using  exponential weighted average, it used Holt\u2019s linear method to compute the smoothed first order and second order momentum. The authors show the convergence of the proposed algorithm in convex regret bound setting and compare the experimental results with conventional Adam algorithm.\n\n- Contribution: the proposed method looks like a direct combination of Holt\u2019s linear method (to replace exponential weighted averaging scheme) and vanilla Adam. The convergence is only established on the simple convex regret bound setting (following vanilla Adam) which is not quite interesting nowadays.\n\n- Clarity: the paper is easy to follow since the main idea is quite simple but lack of necessary demonstrative explanations to show the intuition of using Holt\u2019s linear method, i.e., why Holt\u2019s linear method is better than exponential averaging?\n\n- Soundness: There are a few things that may hurt the soundness of this paper. First, it seems that the proposed algorithm cannot guarantee the term v_t is strictly larger than 0 and therefore has to put an absolute value outside v_t. This is very strange as if v_t close to 0, the overall learning rate would be quite large and may cause non-convergence issue. Second, there is possible non-convergence issue of vanilla Adam algorithm, as pointed by Amsgrad paper. The fix by Amsgrad is to add an additional max operator for v_t term. Here the authors did not adopt this way, I wonder how the authors fix this? If not fixed, then the convergence proof is still flawed.\n\n- Experiments: The authors only conducted experiments on a few small size dataset/architecture and compared only with vanilla Adam algorithm. I would suggest the authors to add ResNet/DenseNet experiments on CIFAR10/IMAGENET and compare with other baselines mentioned in the paper in order to show the performance of the proposed algorithm. Furthermore, I would suggest the authors to also put the test error plot for better comparison as the current result looks far from optimal. For example, on CIFAR10, 40 epochs are far from enough to fully optimized the network and the training loss is largely above 0 currently.\n\nDetailed comments:\n\n- In proof of Lemma B.4, it is not immediately clear why the last step go though.\n"}