{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a new architecture for question answering, that can be trained in an end-to-end fashion. The model expands the entities in a given question to relevant mentions, which are in turn aggregated to a new set of entities. The procedure can be repeated for multi-hop questions. The resulting entities are returned as candidate answers. The approach relies on an index of mentions for approximate MIPS, and on sparse matrix-vector products for fast computation. Overall, the model processes queries 10x faster than previous approaches. The method provides state-of-the-art results on the MetaQA benchmark, with significant improvements on 3-hop questions. The experiments are detailed, and the paper is very well written.\n\nA few comments / questions:\n\n1. Do you have any explanation of why taking the max instead of the sum has a significant impact on the 2,3-hop performance, but only gives a small improvement for 1-hop questions?\n\n2. The same observation can be done for the temperature lambda=1 vs lambda=4, so I was wondering about the distribution of the entities you get on the output of the softmax (in Eq 4). Is the distribution very spiky, and Z_t usually only composed of a few entities? In that case, I guess lambda=4 encourages the model to explore/update more relation paths? Something that works very well in text generation is to not just set a temperature, but also to apply a softmax only on the K elements with the highest score (so the softmax is applied on K logits, and everything else is set to 0). Did you consider something like this? It may prevent the model from considering irrelevant entities, but also from considering only a few ones.\n\n3. Given the iterative procedure of the method, I wonder how well the model would generalize to more hops. Did you try for instance to train with 1/2 hops and test whether it can generalize to 3-hop questions?\n\n4. In Section 2.3, you fix the mention encoder because training the encoder would not work with the approximate nearest neighbor search (I assume this is because the index would need to be rebuilt). However, the ablation study suggests that the pretraining is critical, and one could imagine that fine-tuning the mention encoder would improve the performance even further. Instead of considering a mention encoder, could you have a lookup table of mentions (initialized with BERT applied on each mention), where mention embeddings are fine-tuned during training? The problem of the index is still there, but could you consider an exact search on the lookup table (exact search over a few million embeddings is slow, but it should still run in a reasonable amount of time using a framework like FAISS, and it would give an upper-bound of the performance you could achieve by fine-tuning the mention encoder)."}