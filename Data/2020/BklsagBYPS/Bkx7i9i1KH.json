{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a new goodness of fit measure for generative models, and uses it to get insight into GAN's. While this is an important topic and a novel approach, I do not think the paper delivers on what it promises.\n\nI think this paper should be rejected. First, while it claims to be a general method for generative models it, it is limited to only  GANs and even for GANs it is limited. Second, most of the observations are nice but trivial, e.g. larger latent space leads to larger image.\n\nDetailed remarks:\n- The main point that the training set points x must have p(x)>0 under the model is naturally satisfied for almost all models except GANs such as VAEs, autoregressive model and flow models with standard implementations as the support is the whole space. This is in contrast to the claim in the paper that \"its applications can be extended to other generative networks such as Variational Autoencoders.\".\n- Even for GANs as this measure only looks at the support and not the distribution it is not clear if this measure does more then evaluate mode collapse. While this is an important task, it falls short of the promises the authors claim.\n- The authors claim that \"We demonstrate that our measure being minimized is a necessary and sufficient condition to detect mode collapse.\" but only show that it is necessary.\n- Proposition 1 is a trivial statement.\n- The authors claim that \" mode collapse happens if P(x) > 0 but minz ||G(z) \u2212 x|| > 0\". This is a main point by the authors, but it ignores the probability and only looks at the support. It has been shown that mode collapse happens even in 2d distributions, e.g. veegan paper, where it is easy to get the support to be the whole distribution.\n- The results in sec. 5 are quiet obvious, with a larger latent space you can naturally get a larger support, same as with a mixture model.\n\n\nIn general the method only looks at the support, ignoring the distribution over the support and is therefore very limited in evaluating generative models.  \n\n\nminor details:\n- In eq. 3 the integration should be w.r.t dP(x) for it to be monte-carlo approximated as it is in eq. 4.\n- Not 100% I understand what the authors try to say here - \"we pick the latent variable z and error ||G(z) \u2212 x||2 that corresponds to the smallest error instead of picking the latent variable that Adam Kingma and Ba (2014) finds.\""}