{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an unsupervised method for learning representations of speech signals using contrastive predictive coding. \nThe authors provide results for the speech recognition task, in which they trained their model on up to 8000 hours of speech. The authors provide results on several English benchmark datasets in addition to four low-resource African language datasets. \nThe authors compared their method to the traditional signal processing representations and show that the proposed method is superior. \n\nMy main concern with this submission is its novelty.\nThe proposed method was previously explored in [1] and presented similar results. If I understand it correctly, the main novelty in this work is the usage of bi-directional models together with more data. However, it is not clear what made the improvements. Considering the fact that such an approach was suggested recently by [1], a detailed comparison with uni-directional models is needed.\nFor example, in Table 2, the authors provide results for WSJ dataset, however, with no LM decoding. Can the authors provide experiments of WSJ while using LM similarly to [1]?  Moreover, if the authors wanted to eliminate the effect of LM as they stated in the paper, why not calculating Character Error Rates instead or in addition to Word Error Rates? Again, as done in [1], and in many other papers in the field [2]. \n\nAdditionally, in Table 1 and Table 5, the error rates seem pretty high, especially for the baseline model, did the authors investigated different architectures/stronger ones for these tasks? Different representations such as LogFilterBanks / MFCCs?\n\nI'm willing to increase my score, in case the authors will address my concerns. However, at the moment, I do not see much novelty in this paper comparing to previous work. Additionally, the authors are missing an essential comparison to previous work so we could better understand the contribution of this paper. \n\nMinor comments: \"using a simpler convolutional architecture than is common\" -> should be rephrased.\n\n\n[1] Schneider, Steffen, et al. \"wav2vec: Unsupervised Pre-training for Speech Recognition.\" arXiv preprint arXiv:1904.05862 (2019).\n\n[2] Adi, Yossi, et al. \"To Reverse the Gradient or Not: an Empirical Comparison of Adversarial and Multi-task Learning in Speech Recognition.\" ICASSP, 2019.\n"}