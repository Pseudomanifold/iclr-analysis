{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes \"sparse self-attention\", where only top K activations are kept in the softmax. The resulting transformer model is applied to NMT, image caption generation and language modeling, where it outperformed a vanilla Transformer model.\n\nIn general, the idea is quite simple and easy to implement. It doesn't add any computational or memory cost. The paper is well written and easy to read. The diverse experimental results show that it brings an improvement. And I think this can be combined with other improvements of Transformer.\n\nHowever, there are quite many baselines are missing from the tables. The sota on De-En is actually 35.7 by Fonollosa et.al. On enwik8, Transformer XL is not the best medium sized model as the authors claimed. See below:\n\nNTM En-De: \n- Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019\n- Ott et.al. Scaling Neural Machine Translation, 2018\nNTM En-Vi: \n- Wang et.al. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation, 2018 \nNTM De-En: \n- Wu et.al. Pay Less Attention with Lightweight and Dynamic Convolutions, 2019\n- Fonollosa et.al. Joint Source-Target Self Attention with Locality Constraints, 2019\n- He et.al. Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation, 2018\nLM Enwik8:\n- Sukhbaatar et.al, Adaptive Attention Span in Transformers, 2019\n\nOther comments:\n- More experimental details are needed. What is the value K? How different K values affect performance? What is the number of parameters of NMT models.\n- The claim \"top layer of the vanilla Transformer focuses on the end position of the text\" can't be true generally. Probably only true for a certain task. \n- Where the numbers in Figure 1 come from? Is it a single attention head or average of all?\n- Page 4, \"the high are ...\" probably typo?\n- The related work is missing \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" by Rae et.al., which also uses sparse attention."}