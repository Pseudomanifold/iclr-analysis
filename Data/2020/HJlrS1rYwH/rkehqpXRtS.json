{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a modification to Policy Prediction Networks (PPN) in which the learned transition-, reward- and value function models are used at test-time in a planning procedure. \nA second contribution is the \"pi-Q-backup\" which uses the geometric mean of both the policy and the value function as maximisation target of the planning step.\n\n Overall, I find the idea interesting and the experimental evaluations promising. However, I am voting for \"weak reject\" for the reasons outlined below. If some (or all) of them are address, I'd be happy to raise my score. \n\n- I found the paper hard to understand. In particular, the algorithm PPN on which this work is build is not explained at all, requiring the reader to read the original PPN paper. Including a description of PPN, including it's main features, would greatly help the paper. Second, I am still not sure I correctly undestand when each component is used. As I currently understand it, the main usage of the model during training time is to compute the Advantages in equation (2)? Or are those computed based on rollouts? If so, where is the model actually being used during training?\n- Figure 1 is taken directly from the PPN paper without any reference or citation (as far as I can tell).\n- For the comparison in Figure 5, it would be great if PPN could also be tested with the newly introduced parameter \\beta_i. At the moment, it is hard to tell whether the performance gains are due to \\beta or due to the proposed planning scheme.\n- I'm confused about Theorem 1: Wouldn't we want an upper bound on the difference of means?  Also, what does 'worst-case' mean? Is that for the 'worst' Q-function we could choose? "}