{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new gradient-based stochastic optimization algorithm (with gradient averaging) by adapting theory for proximal algorithms (originally developed for convex problems) to the non-convex setting. The main idea is to first use an averaged gradient plus a quadratic term to locally approximate the non-convex function with a convex smooth function before applying the proximal operator on it. As a result, the algorithm will be able to solve non-smooth (e.g., l1-regularized) and constrained non-convex problems, which will be very useful for optimization problems arising from deep learning.\n\nI think this is a potentially good paper that proposes an algorithm for wide applicability. But I still see some issues that prompts me to ask the following questions:\n\n1. What is the time complexity of solving the sub convex problem at every iteration? The authors did not discuss this in the experiments, but this is very important in evaluating the applicability of the proposed algorithm especially on large-scale problems.\n2. The authors should provide more explanation on the term \\tau (t): It seems it is only used as an auxiliary parameter in the quadratic approximation of f, and that it doesn't affect the convergence asymptotically. But I would imagine it would affect the practical convergence at the beginning of the algorithm?\n3. The authors have repeatedly mentioned that using the averaged gradient v(t) is very important for the convergence analysis of the algorithm. But I did not see how this is the case from the analysis discussed in the paper (I didn't check the proof in reference Ruszczynski (1980)). As this is important in justifying the algorithm, I think the authors should include a discussion to provide some intuition on this in the paper.\n4. Line (15): should vx(t) be x(t) instead? If not, where does the term come from?\n"}