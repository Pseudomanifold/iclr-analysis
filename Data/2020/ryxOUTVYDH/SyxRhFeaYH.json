{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed to identify noisy training examples using ensemble consensus. The authors argued and demonstrated through numeric studies that, to the contrary of some earlier work, training examples with low training loss are not necessarily mislabeled. Rather, the authors hypothesized that examples with high noise require memorization, which is sensitive to perturbations. Thus, the authors proposed to identify and subsequently remove those examples from training by looking at the loss after small perturbations to the model parameters. Examples with consistently low training loss are retained for training. The authors also provided several alternatives of perturbations, including examining the consensus between an ensemble of networks, between multiple stochastic predictions, or between predictions from prior training epochs. Finally, the authors demonstrated the performance of their procedures using numerical studies.\n\nThis paper is well motivated and clearly presented. The idea of identifying noisy examples through ensemble consensus is novel and plausible. The numerical studies are relatively comprehensive and in-depth. I think this is a solid conference paper.\n\nSome questions:\n1. The authors proposed to perturb model parameters in order to find noisy training examples. Is there any reason that the authors did not perturb feature values in order to find noisy training examples? I would suppose that memorized examples are sensitive to feature value perturbation.\n2. The intersection of small-loss examples in Line 13 of Algorithm 1 could be much smaller than (1 - epsilon / 100) * Bb, and could vary in size throughout training. Are there computationally efficient methods that can guarantee the size of the intersection is stable and not too small? I suppose that we do not want the mini-batch size to vary too much throughout training.\n3. How to distinguish hard-to-classify, high-loss examples from high-noise, low-loss examples in Line 11 of Algorithm 1? Using the proposed algorithm, in addition to the noisy low-loss examples, we will also remove those hard-to-classify examples, which is arguably undesirable.\n3. The authors should clearly discuss the computational complexity and space complexity of their proposals."}