{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: \nThis paper proposes a general method for eliminating noisy labels in supervised learning based on the combination of two ideas: outputs of noisy examples are less robust under noise, and noisy labels are less likely to have a low loss. The authors then propose 3 concrete instantiations of the idea, and do a thorough empirical study (including ablations) across multiple architectures, datasets, noise types, and comparing to multiple related methods. The results show pretty convincingly that one of the new methods (LTEC) that uses past networks outputs to build an ensemble performs really well.\n\nCaveats: \n1) I\u2019m an emergency reviewer and had less time to do an in-depth review.\n2) While my research is sufficiently close to review the paper, I\u2019m not an expert on label noise specifically, so I cannot comment much on novelty and related work questions.\n\nComments:\n* The readability of the paper could be dramatically improved by reporting results visually (eg bar-plots) and moving all tables into the appendix.\n* The authors state that peak performance is valid because it *could* have been found using a validation set -- then why not just do that, and report this early-stopping performance everywhere, instead of always two numbers (peak and final)?\n* Please make the perturbation properties in section 3.2 more precise: do you mean \u201cthere exists a threshold and perturbation such that for some (x, y)\u201d? Or \u201cFor any threshold and any perturbation then for all (x, y) it holds...\u201d? Or something in-between?\n* For the \u201ccompeting methods\u201d paragraph, please cite the relevant papers in the main text, not only in the appendix.\n* \u201cover 4 runs\u201d did you randomize the data noise in each run (and in the same way for each method?), or only the network initialisation?\n* Table 5 is cool, but it raises the question: is 1.1epsilon the best, or would performance keep going up?\n* Looking at the actual implementation of LTEC (Algorithm 4), I cannot resist the thought that M=infinity could work even better (at no extra cost): just maintain a monotonically shrinking set of samples?\n\nMinor comments:\n- Define the threshold symbol in section 3.2\n- Define M in Algorithm 1\n- Define (initial) \\mathcal{P}_0 in Algorithm 4\n- Fig 2: can you clarify what green is, does it correspond to \u201cself-training\u201d? \n- \u201clabel precision \u2026 does not decrease\u201d -- well, not a lot, but it does decrease!\n- Fig 1, Fig 2: set max y to 100\n- Table 4: Include M=1 (which is self-training) for comparison"}