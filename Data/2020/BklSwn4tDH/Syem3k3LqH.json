{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a training approach on label noise datasets and outperforms state-of-art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early-stops at minimum validation error (or $\\tau$ memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state-of-art approaches and showed that the proposed method benefits from early-stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches.\n\nPros:\n\nThe proposed method achieves better performance than state-of-art methods.\n\nAuthors have good experiments which evaluate on multiple datasets and algorithms.\n\nAuthors also investigate the relation between model complexity and performance of co-teaching+\n\nCons:\n\nMany recent papers indicate the \u201cerror-prone period\u201d, authors should include related works about early-stopping on label noise training.\nhttps://arxiv.org/pdf/1901.09960.pdf fig1\nhttps://arxiv.org/pdf/1903.11680.pdf fig5\nhttps://arxiv.org/pdf/1906.05392.pdf fig3\n\nAlthough the method achieves good performance, since the idea is a bit straightforward especially after exploring above papers, I am slightly worried about novelty of the ideas.\n\n"}