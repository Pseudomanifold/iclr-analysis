{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "OVERALL:\n\nI should first say that this is reasonably far outside my wheelhouse.\nI have never worked on RNNs or pruning.\nI also have no familiarity with the data sets used.\n\nAll these things being said, I can follow the derivations, and the idea\nseems reasonable and well-motivated, and pruning is interesting\nfor both scientific and practical reasons, and this technique seems to help a substantial amount,\nso I'm inclined to vote for acceptance, with the understanding that perhaps better informed reviewers\n will in the future point out something I have missed.\n\nDETAILED NOTES:\n\n> overparameterized networks require more storage capacity and are computationally more expensive than their pruned counterparts\nI'm with you on the storage capacity, but do any of these pruned networks actually run faster than their non-pruned counterparts?\nI thought you had to work really hard to prune to some kind of block-sparse representation to realize any speed gains.\nThis question is not rhetorical - I know very little about this topic.\n\n> Work for the present volume began by asking the question...\nI like this paragraph for motivation, but perhaps 'volume' is slightly overwrought?\n\n> For our pruning objective, we simply take the\nK weights with largest sensitivity scores, as those represent the parameters which most affect the\nJacobian objective near the initialization.\nIs there some notion of redundancy, where certain sets of parameters affect the jacobian in the same way,\nso that all but 1 element of the set could be pruned, or something?\n\nIn line 13 of algorithm 1, why do we need to sort if in the next step we just mask out\neverything with sensitivity less than D tilde k?\n\nMaybe this is a dumb question, but if you're pruning at initialization, why not\njust initialize a smaller network in such a way that you wouldn't choose to prune any of its parameters?\nAm I misunderstanding what you're doing?\n\n\n> In the prequel, we postulated t\nThe prequel?\n\nFig 1 is interesting, but it raises the question of whether you could recover a simpler\nalgorithm by just modifying random pruning so that it evenly distributes 'prunes' across\ngate and Type.\n\nIn fig 2, why are all the singular values less than 1?\nIt's not obvious to me why that should be true, unless you enforce it w/ the initialization.\n"}