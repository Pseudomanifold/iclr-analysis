{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a reward shaping method which aim to tackle sparse reward tasks. The paper first trains a representation using contrastive predictive coding and then uses the learned representation to provide feedback to the control agent. The main difference from the previous work (i.e. CPC) is that the paper uses the learned representation for reward shaping, not for learning on top of these representation. This is an interesting research topic. \n\nOverall, I am leaning to reject this paper because (1) the main contribution of the paper is not clear (2) the experiments are missing some details and does not seem to support the claim that the proposed methods can tackle the sparse reward problem. \n\nFirst of all, it would\u2019ve been better to have a conclusion section, so the readers can see the contributions of the paper. After reading the paper, I still do not understand what are the contributions of the paper and what're from the previous works. The paper does not provide well justification why CPC feature can provide useful information for reward shaping. The paper does not provide a new method to learn predictive coding. It does not provide a novel reward shaping method (the \u201cOptimizing on Negative Distance\u201d method is very similar to [1]). So, I am not sure what\u2019re the contributions of this paper. \n\nMoreover, I am not convinced that the proposed method can tackle long horizon and sparse reward problems. As the paper discuss in introduction, learning in sparse reward environment is hard because it relies on the agent to enter the goal during exploration. However, the proposed approach seems only able to work in environments where exploration with random policy can generate trajectories that contain sufficient environment dynamics (e.g. dynamics near the goal states). How can the method learn that information without entering the goal? \n\nFurthermore, it seems that the proposed approach only works for goal-oriented tasks (since we need to know the goal state for reward-shaping). I think this should be clearly stated in the paper. \n\nThere are some missing details which makes it difficult to draw conclusions: \n1. How is the \u2018success rate\u2019 computed (e.g. in figure 7 and table 1).\n2. How were the parameters selected (e.g. table 5 in the appendix). Why did you use the default the parameters?\n3. How many runs are the curve averaged over and what\u2019s the shaded region (e.g. one standard error)? Most of the results in the paper seem not statistically significant. \n4. In figure 4, five domains are mentioned but only three of them are tested in the section 5.\n5. Section 6.2 seems irrelevant to the paper. What\u2019s the purpose of this section?\n6. Figure 10 shows the result of using CPC feature directly vs. reward shaping. Are both feature using the same NN architecture, same PPO parameters, and same control setting? Moreover, the reward shaping method assumes we know the goal state but using CPC feature does not. Is it a fair comparison?\n\nThe paper has some imprecise parts:\n1. The definition of MDPs (in section 2) is imprecise. For example, how is the expectation defined? how is the initial state sampled? What does $p\\in\\mathcal{P}$ (last line in the first paragraph) mean where $\\mathcal{P}$ is the state transition function? \n\nMinor comments which do not impact the score:\n1. Figure 1 should come before figure 2. \n2. It would have been better if there is a short description of how the hand-shaped rewards is designed for each domain in the main text. \n\n[1] The Laplacian in RL: Learning Representations with Efficient Approximations\n"}