{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an backpropagation-free algorithm for training in deep neural networks.\nThe algorithm called DeepAGREL does not use the chain rule for computing derivatives and instead is based on direct reinforcement. \nAuthors conduct experiments on MNIST and CIFAR where they find their method to reach performance similar to BP.\n\nNovelty: the method is built upon existing ideas, however, the exact algorithm seems to be novel.\n\nClarity: although it is possible to understand the algorithm based on the provided textual description, it would not hurt to provide a more formal presentation of the main update equation (1). \n\nIt is not clear why authors provide two different expressions for updates in each layer (in terms of fb^{l} and fb^{l+1}).\n\nI would also appreciate a clear discussion on the theoretical properties of the algorithm, for example, is it guaranteed to converge to a critical point of some loss function? Can we derive the update rule from some known optimisation procedure, e.g. REINFORCE?\n\nQuality: Unfortunately I have a number of major issues with this respect.\n\n1. Authors do not clearly pose what are the properties of back propagation that they find biologically non-plausible and how exactly their algorithm is making progress on them. There seems to be some sort of consensus in the literature about these properties and if authors accept it, then it looks like DeepAGREL is not very plausible too, because it does not address the weight-transport issue at all.\n2. Many recent results are not even mentioned in the paper, for example, (Bartunov et al, 2019). From not so recent \u2014 the whole branch of research around target propagation algorithm (Lee et al, 2014).\n3. The experiments are not convincing to me, as the considered network architectures are quite shallow and thus the ability to perform credit assignment can not be demonstrated. Besides that, the locally-connected architecture has only the first layer locally-connected and it does not make much sense to me, as it does not factor out the impact of weight sharing. \n\nOverall, I think the paper is not ready for publication at ICLR.\n"}