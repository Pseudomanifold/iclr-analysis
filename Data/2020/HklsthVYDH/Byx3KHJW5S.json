{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a framework where one component is an attacker network that keeps learning about how to perturb the loss more, and one component is a defense network that robustify learning with respect to the attacker network. The framework is flexible on how the attacker network can be trained, and advances over previous works where the attacker is a human-designed algorithm rather than a learning model. Experiment results show that the framework reaches superior defense performance. The authors also extend the framework to help imitation learning.\n\nOverall the paper is a pleasure to read. My questions/suggestions are\n\n(1) Given that the framework seems natural in design, a deeper contribution would be talking about how to successfully train the framework in practice. The authors talk about the connection of the framework to GAN, and the latter is not that easy to train. However, we see very little information on how to train the framework in the paper. Was it super easy to train the framework (why?), or did the author encounter any difficulties? Are there important heuristics that help train the framework successfully?\n\n(2) While the framework leads to a better defense mechanism (the authors' goal), one could wonder whether it leads to a better attacker as well. Instead of just checking the differences of the attacking examples generated, can we take the inner attacker and see if it is more effective in attacking than PGM and CW? How does the goodness of the attacker improve with time? Do different L2L variants generate attackers with different quality? Do the quality connect with the defense performance?\n\n(3) Section 4 looks distracting to me. It is good to know that the framework can be extended to imitation learning, but the section is best put at a longer version or another paper, rather than occupying a significant amount of space in the current paper.\n"}