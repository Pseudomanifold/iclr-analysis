{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a neural network approach to variable unification and\nreasoning by example as a way to mimic the human ability to identify invariant\npatterns in examples and then apply them more generally in practice.\nThis general idea of identifying invariates and mapping new instances to\nthem is well motivated by the authors, citing work in philosophy of mind,\ncognitive science, and developmental psychology.\n\nThe authors go on to propose MLP, CNN and Memory Network models\nof unification for sequence, grid, and story reasoning tasks respectively.\nExperiments on the sequence and grid datasets demonstrate the data efficiency\nof this approach. MLP and CNN models with unification achieve near perfect\nperformance in fewer iterations (an order of magnitude fewer in the MLP case!)\n than their non-unification enabled counter parts.\nUnification enabled models also demonstrate high performance in a reduced\ntraining set setting (using only 50 training examples).\nWhile this is encouraging, these are very simple toy tasks.\n\nI also am in doubt as to whether the representation of these problems\ncauses some issues. In the sequence task, one question the models are\ntrying to solve is what symbol is the head or tail of the sequence.\nModeling variables over the sequence of symbols here is, in a sense, the\nwrong object of study. The position of the symbols would need to be\nrepresented, e.g.\n\na b c d\n1 4 3 1\n\nwhere I've represented positions as a-d, and the learned invariant about\nhead questions would be:\n\nX:a b c d\nY:1 4 3 1\n\nAs is, by mapping symbols and not positions to variables, one cannot,\nat the variable level distinguish between the two 1s in the sequence above.\nMy guess is that in practice the bi-GRU model that produces embedding\nfeatures of the symbols in sequence is implicitly representing head/tail\npositioning.\n\nSimilar arguments could be made about the grid example.\n\nI don't find the experiments/analysis on the bAbI dataset very convincing.\nFor instance, in the example given in Figure 4b (reproduced below)\nis shown as an example of\ntemporal reasoning, where a symbol Z is mapped to the\nword morning (a symbol distinguishing a time), and the question asked is\nwhere was Bill before school.\nIf logical reasoning is being used to solve this question, surely the\nsymbol 'before' must also be represented as a variable. Its possible that\nthe model is instead learning a trick about mutual exclusivity, i.e. that\nY:school is the only location symbol not mentioned in question but this\ncould fail as a general strategy.\n\n\nthis Z:morning X:bill went to the Y:school\nyesterday X:bill journeyed to the A:park\nwhere was X:bill before the Y:school\nA:park\n\nFigure 4b\n\nIt would make for a much more interesting paper if the authors took\nexamples such as these and formed counter-factuals to probe the way\nthe models are answering the questions. E.g., transforming the question\nin 4b to \"where was X:bill today\" or \"where was X:bill after school.\"\n\nBecause the authors use soft unification, interpretability is difficult\nto assess. Interpretability is crucial here because to claim that unification and reasoning by logical induction\nis being used to solve tasks, it becomes important to show how the neural networks\nmake their decisions. Given the instances of extra variables and one to many\nmappings on the bAbI dataset it seems very likely that the models are not\nsolving many tasks\nusing unification as it would be possible to learn to use the symbols directly\nto learn to answer. As such, I think these issues are not addressed in the\npaper sufficiently to warrant acceptance.\n\n\nMinor Notes\n\n- In definition 1, the definition of Variable is a little confusing because there are two different senses of the word in use. I understand them to be (1) Variable (X) in the logical template that is intended to be learned and used in problem solving, and\n(2) variable (x) in the neural network model that is a soft asignment of\nthe Variable to a default symbol s. It would be nice if this distinction could\nbe noted or made clearer.\n\n- In the definition 2, in the phrase \"is the invariant example such as a tokenized story\" it might be worth stating that the tokens are the symbols in S.\n\n\n- My understanding is that each unique symbol in the invariate is a potential\nvariable. Does this mean there are no co-referent symbols in the invariate?\nWould be helpful to state whether babi contains co-referent expressions\nand how these might affect the model.\n\n\n- It might be interesting to see how model architecture affects variable\nlearning. For example, does a CNN result in more sensible variable\nassignments  than the mlp on a flattened representation of the grid problem?\n\n\n- What is the strongly supervised case? These are token level annotations I think (at least for babi) but it might be good to specify in more detail what\nthey  are.\n\n- The figure and explanation of the UMN are not very clear. From the figure\nis does not seem that the variables interact with the memory at all. More\nspace could be devoted to this section.\n\nPossibly Relevant Related Work\n\nBrenden Lake. Compositional generalization through metasequence-to-sequence learning. NeurIPS 2019.\n\n\n\n\n\n"}