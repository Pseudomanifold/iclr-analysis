{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a novel approach for learning invariants that can capture underlying patterns in the tasks through Unification Networks. This effectively allows the machine to learn the notion of `variable`, which is a symbol that can take on different values. \n\nPros:\nThe authors evaluated and presented empirical results on four common benchmark datasets, showing superiority over plain baseline without unification. \nThey further performed analysis on the learned invariants, and verified the sensibility. \nThe paper overall is well written and structured.\n\nCons:\nDespite its superiority over plain baseline, the paper does not provide thorough comparison with other state-of-the-art methods on reasoning related tasks.\n\nSome of the technical details regarding the choice of hyperparameters are missing. For example:\nIn section 6, what\u2019s the rationale of setting $t$ differently for bAbI solely?\nIn Equation 5, how is the sparsity regularization parameter $\\tau$ chosen optimally for a particular task? A bit more discussion on these choices would be helpful. \n\nOverall, this paper presents a seemingly promising architecture capable of learning and using variables, with the caveat for lack of experiments and comparison with other state-of-the-art methods. \n"}