{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an attack method to improve the transferability of targeted adversarial examples. The proposed method uses a temperature T to convert the logit of the network, and calculates the gradient based on the new logit, yielding the distillation-based attack method. It has been integrated into FGSM and MI-FGSM.\n\nOverall, this paper has the poor quality based on the writing, presentation, significance of the algorithm, insufficient experiments. The detailed comments are provided below.\n\n1. The writing of this paper is poor. There are a lot of typos in the paper. The notations are used without definitions. These make the paper hard to read and understand.\n\n2. Based on my understanding of the paper, the motivation of the proposed method is that the softmax function on top of neural networks can make the gradient unable to accurately penetrate classification boundaries. And the distillation-based method is proposed to reduce the magnitude of the logits to make the gradient more stable. However, if the argument were true, we could use the C&W loss to perform the attack, which is defined on the logit layer without affected by the softmax function.\n\n3. There are a lot of recent attack methods proposed to improve the transferability of adversarial example, e.g., \"Improving transferability of adversarial examples with input diversity\" (Xie et al., 2019); \"Evading defenses to transferable adversarial example by translation-invariant attacks\" (Dong et al., 2019). The authors are encouraged to compare the proposed methods with previous works."}