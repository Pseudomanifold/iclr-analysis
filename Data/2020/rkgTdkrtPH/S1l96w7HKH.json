{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to provide a novel noise-aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework. More specifically, NoiGAN repeatedly utilizes a GAN model to 1) approximate the confidence score for facts identifying reliable data (discriminator) and 2) generate more challenging negative samples (generator). Then, it uses this confidence score and negative samples to train a more accurate link prediction model. The authors validate the proposed model through several experiments.\n\nThis paper reads well and the results appear sound. I personally find the idea of incorporating confidence score into a link prediction model to achieve a more accurate model very interesting. Furthermore, the provided experiments support their intuition and arguments outperforming considered baselines.\n\nAs for the drawbacks, I find the baselines considered in this work outdated missing many SOTA and related works in link prediction and noise detection [1,2, 3, 4, 5]. Further, I believe this work needs more experimental results and an ablation study capturing different aspects of the presented method. My concerns are as follows:\n\n\u2022\tConsidering the existing reverse relation issue in FB15K and WN18, I suggest conducting the experiments on the FB15K-237 and WN18RR from [6] instead. \n\u2022\tI suggest considering more recent link prediction models as baselines.\n\u2022\tI am wondering if the only difference between NoiGAN and KBGAN [7] is incorporating the confidence score in the link prediction loss?\n\u2022\tConsidering the fact that NoiGAN repeatedly retrains GAN and link prediction model, I suggest providing a comparison of computational complexity.\n\u2022\tI am wondering if NoiGAN can only work with pre-knowledge of noisy triples in KG? If not, why didn\u2019t you report NoiGAN performance with 0% noise in Table 3?\n\u2022\tI find utilizing few examples to evaluate the power of discriminator in distinguishing noisy triples (Table 4) not satisfactory at all. I suggest experimenting with more data and providing the per-relation breakdown performance of the discriminator.\n\nOn overall, although I find the proposed model quite novel and interesting, the paper needs more experimental results to validate the idea.\n \n[1] Pinter, Yuval, and Jacob Eisenstein. \"Predicting Semantic Relations using Global Graph Properties\".\n[2] Nathani, Deepak, et al. \"Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs\". \n[3] Bala\u017eevi\u0107, Ivana, Carl Allen, and Timothy M. Hospedales. \"TuckER: Tensor Factorization for Knowledge Graph Completion\".\n[4] Sun, Zhiqing, et al. \"Rotate: Knowledge graph embedding by relational rotation in complex space\".\n[5] Pezeshkpour, Pouya, Yifan Tian, and Sameer Singh. \"Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications\".\n[6] Dettmers, Tim, et al. \"Convolutional 2d knowledge graph embeddings.\", AAAI-18.\n[7] Liwei Cai and William Yang Wang. \u201cKbgan: Adversarial learning for knowledge graph embeddings\u201d. \n"}