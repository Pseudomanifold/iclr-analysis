{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the problem of catastrophic forgetting and proposes a number of methods for overcoming this issue, under various circumstances. The crux of the proposed methods is the a criterion in eq. 1 for minimizing the change in likelihood over the previous datasets. The authors use this as a regularization term and compare and contrast various versions of it with previous work such as EWC.\n\nOn the surface of it, the proposed method(s) are sensible, though if I understand things correctly, the main proposed criterion (eq 2) is very similar in shape to EWC, with the main change being L1 vs L2. I can buy that L1 produces different solutions compared to L2, as argued by the authors around eq. 2, so that's fine. On the other hand, I find the contribution of this work relatively minimal. The dissection of case I-IV is good though these are basically small iterations of eq. 1. If I understand it correctly, most (all?) of the cases could have been analyzed with an L2 penalty instead of L1 -- I think this would have added some value to the overall theme of L1 being a better choice  than L2, no?\n\nA few other comments:\n\n* I have a small but potentially important reservation about the experimental protocol -- I don't see any mention of a validation set, it's not clear to me how the hyper-parameters were actually chosen?\n* While I appreciate the stds being provided over 5 seeds, there are many cases in which the confidence ranges between various methods actually overlap: this is not made clear in the tables themselves (there are cases in which the proposed methods overlap in confidence intervals with the other baselines)\n* There's some discussion in section 6 about the magnitudes of optimal L1 and L2 hparams found by the authors, but I don't know if the \"stronger\" lambda is really an indication of anything (it could be due to optimization quirks related to gradient descent).\n* Are the baseline results (EWC and SI) lifted from the previous papers or done by the authors themselves? It's not immediately clear.\n* I don't know how excited I should be about the fact that all results are on variations of MNIST, it does not instill a lot of confidence that these results would generalize to other domains.\n* I would have liked the authors to create at least some toy examples or datasets where the proposed methods should shine -- right now it just looks like a list of four semi-sensible alternatives that are basically treated as hyper-parameters in the results table and it's not obvious why someone would choose one of these vs another.\n\nAll in all, I can see some value to this work -- certainly a relatively small modification that has some practical benefits, but I'd like to see this better positioned -- empirically, especially -- and better analyzed (cf. my comments about L2)."}