{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a word embedding model to incorporate the sentiment information. The paper provided both maximum likelihood estimation and maximum posterior estimation for the proposed framework. Improved experiment results on word similarity and low frequency embeddings are presented. Overall, the paper incorporates the sentiment information in a neat way. And my main concern is the around the Bayesian inference and the prior knowledge distilled into the model.  Detail comments are as following, \n\n1. The model employed Laplace approximation for posterior distribution. Not quite sure this is a good idea for the Bernoulli case since Laplace approximation is trying to use Gaussian distribution to approximate the region around the mode. How will the MAP solution compare with a full Bayesian solution such as VB or sampling-based methods?\n\n2. Another concern is the prior introduced into the model. Normally prior information will be washed away as the training data grow. Not the case for the low frequency examples that the model performed well on. Would it possible that the improved performance on low frequency example is just a side effect of the biased introduced by the prior? How sensitive will the embedding perform with respect to the prior selected?\n"}