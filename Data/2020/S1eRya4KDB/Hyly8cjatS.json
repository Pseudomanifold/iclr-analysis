{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper aims at extending GloVe word embedding model so that the resulting embeddings should capture sentiments (e.g. \"good\" is positive while \"bad\" is negative). The key idea is to employ an extension term to deal with the fact that some words appearing in text with sentiment information. Furthermore, to deal with the fact that many words an infrequent, besides maximum likelihood estimation, the paper proposes to use bayesian estimation. In the experiments, Stanford sentiment tree (SST) corpus is used. The word embeddings from the two models (each trained on different estimation methods) show their capability of expressing sentiments, compared with popular methods like Glove, word2vec. \n\nI would accept this paper because: \n- This paper is well written, with thoughtful maths details. \n- The proposed models, although are extensions of GloVe, gives interesting (and rigorous) points of how to add sentiment information. \n- The experiments do support what the paper claims.\n\nI would reject it because of the experiments. The dataset (SST) is so small and thus is questionable about the quality of the learned word embeddings and the comparisons. I think there should be better ways, such as word embeddings are trained on massive data (like for GloVe and word2vec), then are fine-tuned on sentiment analysis dataset. Also, I was wondering whether there's a way to collect more sentiment data (like in SE-HyRank paper).\n\n"}