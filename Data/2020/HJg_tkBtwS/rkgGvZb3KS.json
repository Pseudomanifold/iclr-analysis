{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper proposes a practical improvement of the conditional randomization test (CRT) of (Candes et al., 2018).\nIn the study of (Candes et al., 2018), the choice of the test statistic as well as how one estimates conditional distributions were kept open.\nThe authors proposed \"proper test statistic\" as a promising test statistic for CRT, and proved that f-divergence is one possible choice.\nThey further shown that KL-divergence has a nice property among possible f-divergences: KL-divergence cancels out some of the conditional distributions, and thus the users need to estimate only two conditional distributions to compute the test statistic.\nFor estimating those conditional distributions in the test statistic, the authors proposed fitting regression models.\n\nOverall, I think the paper is well-written and the idea is stated clearly.\nThe use of KL-divergence for CRT seems to be reasonable.\nThe proposed algorithms look simple and easy to implement.\n\nMy only concern is on the practical applicability of the proposed algorithms (which, however, may be not a unique problem for this paper, but for all the CRT methods).\nThey require fitting regression models for each feature xj.\nFor high-dimensional data with more than thousands of features, fitting regression models for all the features seem to be impractical.\nFor the imagenet data experiment, the authors successfully avoided this problem by using an inpainting model.\nHowever, this approach is apparently limited to image data.\nI am interested in seeing if there is any promising way to make the algorithms scalable to high-dimensional data."}