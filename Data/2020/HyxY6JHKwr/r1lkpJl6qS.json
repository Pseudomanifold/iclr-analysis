{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\n-------------\nThe authors propose a methodology to train a single Deep Neural Network (DNN) that minimize a parametrized (weighted) sum of different losses. Since the model is itself conditioned by the weights, it allows to train a single model for all weights values, instead of retraining when the weights change.\n\nExperiments suggest that this methodology does not degrade the performances to much w.r.t. retraining on every weight changes.\n\nThe proposed conditioning of the layers is done via a reparametrization (FiLM, Perez et al. 2018) of the weights with a scale $\\sigma(\\lambda)$ and a bias $\\mu(\\lambda)$ where $\\mu$ and $\\sigma$ are MLP. This allows to condition the layer on $\\lambda$, while keeping the number of parameters low.\n\nNovelty\n----------\nThe idea of integrating a family of loss functions with model conditioning has also been proposed by Brault et al. [1], in the context of multi-task kernel learning. Hence I believe this work should be acknowledged.\n\nAs the product of kernels is the tensor product of their feature maps, it would suggest to condition the network's layers by taking the tensor product of the weights with respect to an MLP on $\\lambda$. This could be applied on each layers or simply on the last Fully Connected layer. Note however that it would drastically increase the number of parameters and hence not be a viable solution (or maybe with some channel pooling?).\n\nReferences:\n\n[1] Infinite Task Learning in RKHSs; Brault, Romain and Lambert, Alex and Szabo, Zoltan and Sangnier, Maxime and d'Alche-Buc, Florence; Proceedings of Machine Learning Research; 2019.\n\nQuality\n----------\nThe paper is self content and well written.\n\nExperiments are well detailed and seems to be reproducible. I would be a great addition to release the code in a public repository (with a link in the paper or appendices) if the paper is accepted.\n\nI would also suggest the following experiments:\n    * An experiment showing the time penalty induce by training the loss conditional model. The authors claims that training multiple separate models is inefficient compared to their proposed method. While it seems obvious, it deserve an experiment as one of the claim.\n    * The authors propose to sample one $\\lambda$ per SGD iteration. However it ma be useful to sample more of them. Especially when the set of $\\lambda$ is large (high dimensional)\n    * Possibly use a pre-trained model and only tune the $\\sigma(\\lambda)$, $\\mu(\\lambda)$ MLPs\n\nOverall my decision is weak accept, the paper lacks of novelty and the experiments could be more extensive."}