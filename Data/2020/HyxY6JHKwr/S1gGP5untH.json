{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Generative models often use a loss function that is a weighted sum of different terms, e.g., data-term and regularizer. Let's denote these weights as \u03bb. The paper proposes a method for learning a single model that approximates the result produced by a generative model for a range of loss-term weights \u03bb. The method uses the following mechanisms i) \u03bb-conditioned layers ii) training with a stochastic loss function, that is induced by a (log-uniform) distribution over \u03bb. The performance of the model is demonstrated on the following problems learning \u03b2-VAE, image compression, and style transfer. The models clearly demonstrate an ability to approximate problem solutions for a range of coefficients. The paper is clearly written. The experiments, however, need future discussion.\n\n1) The beta-VAE experiments (sec. 4.1)\n\nQ1. While models demonstrate a reasonable behavior on Shapes3d dataset. The samples and reconstructions on CIFAR10 (Figure 8) indicate that all models are not trained well. If this is the case, conclusions might be misleading, since the approximating output of undertrained models might be much simple comparing to well-trained ones. Authors may want to provide a comparison of the trained models with conventional VAEs (with \u03b2=1), the reference figures for CIFAR10 are provided, for example, in https://arxiv.org/abs/1606.04934.\n\nQ2. Wider YOTO seems to help a lot, but, what happens to the baseline models of increased size?\n\n\"We select the fixed \u03b2 so that it minimizes the average loss over all \u03b2 values.\"\n\nQ3. Was it done directly on a test set, or were validation-data used?\n\n2) Image compression (sec. 4.3)\n\n\"Finally, a wider model trained with a larger batch size (\u201cYOTO wider batch16\u201d) closely follows the fixed weight models in the high compression regime and outperforms them in the high quality regime.\" (Figure 5)\n\nQ4. How is this compared to the baseline with batch16?\n\nQ5. Authors also may want to provide std for provided metrics. The difference does not look statistically significant. \n\nSuggestion 1: It might also be interesting to see if we can use this technique to perform a hyperparameter search. Train the model, select one the best performing set of hyperparameters, and then train models with this best value.\n\nOverall, the paper proposes an interesting technique, that surprisingly, can work for a range of hyperparameters, and potentially have a high practical impact. However, the empirical evaluation is half-baked, specifically has certain methodological drawbacks e.g., perhaps undertrained beta-VAE model, absence of standard deviations while comparing (close) numerical results, and comparing models with different optimization parameters -- the performance difference might be due to optimization. \n\nI recommend to reject the paper, however, I will appreciate discussions with authors and other reviewers, and will consider changing my score in case of reasonable argumentation."}