{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nContributions:\n\nThis paper studies an important problem, i.e., how to provide a theoretical framework to understand quality and diversity in text generation. To this end, the authors first provide a general definition of quality and diversity, and then study a MOP problem which tries to maximize both quality and diversity. By theoretical analysis, the authors concludes that: (i) there is truly a trade-off between quality and diversity; and (ii) quality and diversity can be combined to be a divergence metric. Further, the QDTC method is proposed for text generation. \n\nStrengths:\n\n(1) Novelty: I think this paper is novel. While most previous work on this topic are empirical, this work tries to formally define quality and diversity, and studies the Pareto-Optimality solutions. The authors also proposes a new QDTC algorithm for text generation. All these seem to be a quite novel perspective. \n\n(2) Writing: The paper is carefully written, and clearly presented. \n\nWeaknesses:\n\n(1) Experiments: My biggest concern of this paper lies in its experimental design. I understand that this paper is more like a theoretical paper, but proper experiments are still needed to justify your theory. Details are shown below. \n\na) MSCOCO is a very simple dataset. The sentences inside are simple. I believe the use of MLE for training already provides good results. Therefore, I think only reporting results on simple MSCOCO dataset is not enough. It will be good to add experiments on other datasets as well, as what have been used in the literature.\n\nb) What are the generated text looking like? Do they show qualitative difference when compared with MLE baselines?\n\nc) Experiments are only compared with simple baselines. Also, only BLEU-3 and Disinct-3 are reported. Comparing with other related work will be appreciated, such as SeqGAN, LeakGAN, TextGAN, MaskGAN etc. \n\nd) Two variants of the QDTC algorithm are provided. In practice, which one performs better? \n\ne) Since this task is hard to evaluate by nature, it will be good to include human evaluation, and demonstrate how well your proposed quality and diversity measures align with humans' preference, and how well your proposed QDTC methods compare with baselines.\n\n(2) Clarity: \n\na) I feel section 4 and 5.1 is a little bit challenging to follow, especially all the Lemma and Theorems. Though hard to follow, I still appreciate the authors a lot for all these derivations. Can the authors provide a concise summary on how these Theorems guide the algorithm design? \n\nb) The proposed method and theoretical analysis seem not restricted to text generation problems, and can be applied to image domain as well. Any comments on this?\n\nc) In Algorithm 1, it mentions that MLE pre-training is just an optional step. But in Section 6, it mentions that all the models are MLE pre-trained first. Can the authors show some results without MLE pre-training? Otherwise, it would be better to delete \"(optional)\" in Algorithm 1. \n\nOverall, I think this paper provides a novel perspective on quality and diversity for text generation. However, the experiments are not that satisfying, and could be much improved. "}