{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper first formulates quality-diversity trade-off in text generation as a multi-objective programming problem, such that the Pareto\u00a0 frontier\u00a0is sought to explore. An efficient algorithm to reach the frontier is proposed and experiments are conducted to show the effectiveness of the proposed approach.\n\nSeveral issued should be addressed or clarified. In section 3.1, the population probability of text is assumed to be known, and the size of the true distribution and model distribution is N=|V|^L, which is exponential, so constrained optimization problem in section 4.1 has exponential number of constraints, thus MOP is an intractable. How to overcome this? In Section 5, the authors turn to an optimization problem which only depends on training data. Then what's the purpose to have sections 3 and 4?\u00a0\n\nIn sections 3 and 4, why not use empirical distribution in stead of true but unknown population distribution?\n\nIn section 6, the experiments on synthesis data should be done in the same way as in SeqGAN's paper, such that the results can be compared. For experiments on real data, only experiments on middle-length data set COCO are performed, experiments on long-length dataset such as EMNLP News 2017 should be conducted too, since it is harder to get good results.\u00a0\n\nMany important papers are missing, specially the language GANs falling short paper. Experiments should be compared with this paper and some representative language GANs.\u00a0\n\nM. Caccia, L. Caccia, W. Fedus, H. Larochelle, J. Pineau, and L. Charlin. Language GANs falling short. In Neural Information Processing Systems Workshop on Critiquing and Correcting Trends in Machine Learning, 2018.\n\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. AAAI, 2018.\n\nZhongliang Li, Tian Xia, Xingyu Lou, Kaihe Xu, Shaojun Wang, and Jing Xiao. Adversarial discrete sequence generation without explicit neural networks as discriminators.  AISTATS, 2019.\n\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for language generation. NIPS, 2017.\n\nSidi Lu, Lantao Yu, Siyuan Feng, Yaoming Zhu, and Weinan Zhang. COT: Cooperative training for generative modeling of discrete data. ICML, 2019.\n"}