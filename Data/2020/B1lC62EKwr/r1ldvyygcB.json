{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors consider active deep learning. They propose decomposing predictive entropy into a) vacuity (lack of evidence) and b) dissonance (contradictory evidence). They frame this in terms of \"subjective logic\". In practice this is achieved by having the NN output the parameters of a Dirichlet, which allows an additional degree of freedom describing variance/vacuity. Dissonance is defined in terms of the support of contradictory classes. To get improved estimates of vacuity they augment the loss with a term regularizing the Dirichlet parameters to be small (low precision) at unlabelled points with higher KDE(unlablled points) than KDE(labelled points). They propose initially weighting vacuity and later dissonance as AL proceeds. Encouraging results are presented on simulated 2D data, MNIST and CIFAR10. \n\nI think the basic idea of separating vacuity and dissonance is interesting, and the demonstration of the failings of existing ADL approaches is valuable. It wasn't clear to me how this \"subjective logic\" theory gets you to the specific definitions of vacuity/dissonance, or whether these were just proposed by the authors. Equation 6 seems to come out of nowhere (whereas the rest of the derivations using the Dirichlet are very intuitive). \n\nThe idea of encouraging the network to be uncertain far from data is also reasonable. While some Bayesian models such as Gaussian process regression with a RBF kernel give you this for free, it is certainly true that DL methods do not have this characteristic in general. Regularizing the r to be small seems like a reasonable way to do this, but I'm not convinced by the kernel density estimate part. DNNs can operate on very high-dimensional, structured inputs. Even the simplest of these, images, requires some degree of spatial invariance (achieved using convolutions) to obtain meaningful predictions. I find it very hard to believe a KDE can do anything meaningful in such spaces, even if you could find a sensible bandwidth (which isn't discussed at all). It is possible of course that random selection of unlabelled points to regularize in this way would work just as well. Unfortunately no ablation study is performed, so we don't know what the individual contribution of the three proposals (moving from vacuity to dissonance, augmented loss and Dirichlet likelihood) is. \n\nHow sensitive is the method to the vacuity/dissonance weighting? \n\nThe improvement over competing methods for MNIST and CIFAR10 appears to mostly manifest after the initial 20 or so acquistions.\n\nDo these results extend to batch AL? For many applications that's more important. \n\nOverall I thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is. "}