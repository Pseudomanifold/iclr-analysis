{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: this paper claims to design an unsupervised meta-learning algorithm that does automatically design a task distribution for the target task. The conceptual idea is to propose a task based on mutual information and to train the optimal meta-learner. They also use experiments to show the effectiveness of the proposed approach. \n\nOverall Comments:\n\nI would think this paper requires a major revision. It is written in a very confusing way. Many terms are directly used without a definition. The problem is also not clearly defined. I have tried to understand everything, but I have to give up in Section 3. Overall, I do not think this paper is ready for publication.\n\nDetailed comments:\n\t\u2022 It would benefit a lot if you can clearly define the original meta-learning procedure and then compare that with the one proposed in this paper.\n\t\u2022 Define \u201dhand-specified\u201d distribution. This word does not make sense if you claim this is the difference between the meta-learning procedure proposed in this paper and the original meta-learning algorithm. In this paper, you used p(z) to specify a task. I would think p(z) is also \u201chand-specified\u201d.\n\t\u2022 I am not very sure by what you mean for \u201ctask-proposal procedure\u201d, \u201cgoal-proposal procedure\u201d\n\t\u2022 In the first paragraph of the intro: what do you mean by \u201cspecifying a task distribution is tedious\u201d, is specifying p(z) also \u201ctedious\u201d\n\t\u2022 2nd paragraph of intro: \u201cautomate the meta-training process by removing the need for hand-designed meta-training tasks\u201d. Again, why p(z) is not \u201chand-designed\u201d\n\t\u2022 Why compare with the original meta-RL algorithm on p(z) is not fair? \n\t\u2022 What do you mean by \u201cacquire reinforcement learning procedures\u201d?\n\t\u2022 \u201cEnvironment\u201d, \u201ctask\u201d are not clear when they first appear\n\t\u2022 The word \u201clearn\u201d is used everywhere, and is confusing. E.g. what do you mean by \u201clearn new tasks\u201d, \u201clearn a learning algorithm f\u201d, \u201clearn an optimal policy\u201d, \u201clearn a task distribution\u201d \u2026\n\t\u2022 \u201cReward functions induced by p(z) and r_z(s,a)\u201d: isn\u2019t r_z(s,a) already a reward function? What is \u201cinduced\u201d?\n\t\u2022 What is \u201cmeta-training\u201d time?\n\t\u2022 What is \u201cno free lunch theorem\u201d?\n\t\u2022 The \u201ccontrolled-MDP\u201d setting is actually much easier: perhaps you just need to learn the probability distribution. Then for every r_z, we just solve it. Why not compare with this simple algorithm?\n\t\u2022 \u201cRegret\u201d is not defined when it first appears\n\t\u2022 \u201cThe task distribution is defined by a latent variable z and a reward function r_z\u201d: why \u201cdistribution\u201d is defined by an r.v.?\n\t\u2022 In (2), \u201cregret\u201d should be the (cost of the algorithm) - (the total cost of an optimal policy) \u2014 it is not hitting time\n\t\u2022 (3) is confusing, no derivation is given\n\t\u2022 Based on the usual definition of \u201cregret\u201d, how can a \u201cpolicy\u201d have low regret? Any fixed \u201cpolicy\u201d would have linear regret \u2026"}