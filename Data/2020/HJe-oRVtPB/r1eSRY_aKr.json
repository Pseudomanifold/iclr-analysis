{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper shows the stability of ResNet for the output scale of resblock: $\\tau < \\sqrt{L}^{-1}$ and shows the explosion for $\\tau > L^{-0.5+c}$. Based on this analysis, a linear convergence rate of the gradient descent for the squared loss is also shown. In addition, this paper empirically verifies the efficiency of the initialization of $\\tau = \\sqrt{L}^{-1}$ with/without batch normalization.\n\nContributions of the paper are summarized below:\n- Provides a sharp characterization of the largest scale of outputs of resblocks: $\\tau$.\n- Enlarges the class of ResNets with the global convergence guarantee of the gradient descent w.r.t. the scale of $\\tau$. (A related study [Allen-Zhu+2018b] focused on the scale of $\\tau = 1/L$).\n- Improve the depth dependence in the network width and the iteration complexity compared to existing studies.\n\nSignificance:\nTo theoretically justify the success of ResNets, this line of research is important, and I think this paper makes a certain contribution in this sense, although there seems to be much room for improvements regarding the depth dependence in the network width. (Unrealistically high over-parameterization: $m=n^8L^7$ is still required). In addition, a proposed technique (Theorem 1) to derive gradient bounds is surprising and beyond the intuition. Indeed, a simple way of using a natural spectral bound cannot explain the stability of ResNets with $\\tau=L^{-0.5}$.\n\nClarity:\nThe paper is well organized but there are some technical concerns as described below.\n\nQuality:\nI think the quality of the paper can be improved. \n- Maybe probabilities in some statements are not correct, for instance, a probability in Theorem 2 should depend on the value of $c$.\n- It is better to specify the value of $c$ in Theorem 1. It may take an arbitrary small value at the expense of the probability.\n- In the convergence analysis of the gradient descent (Theorem 5), the dependency on $\\tau$ should be mentioned. Is there no effect on the convergence rate by the choice of $\\tau$?\n\nMinor comments:\n- The network architecture is different from the standard ResNets. Usually, non-linear activation functions are applied after resblocks. Can theoretical analyses be extended to such a setting?\n- Initialization scales of parameters in related studies, as well as $\\tau$, should also be clarified for a fair comparison of $\\tau$.\n- In proofs, citations of theorems and lemmas seem incorrect, e.g., Theorem 1 may be used in the proof of Lemma 3 (and Theorem 3 too?) instead of Lemma 1.\n- Typo In Theorem 1: for all $a > l$  --> for all $b > a$ (?).\n- There is no definition of $h'_{i,l}$ in Lemma 4. Is it a perturbation of a hidden node h?"}