{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a new data augmentation method for meta-learning, named as \"Task Level Data Augmentation (Task Aug).\" The general data augmentation methods add various translations to the original data to amplify the original data. However, in the meta-learning problems, especially few-shot learning, which is experimented in the paper, the smallest unit is rather a task than data. Therefore, the number of data instances per class is not as critical as other learning problems. Therefore, TaskAug increases the number of classes instead of the number of instances per class by generating images that are clearly recognized as different classes from the original data. With an increased number of classes, the number of task instances that can be sampled also increases. TaskAug rotates the natural images by 90, 180, 270 degrees making three new classes. Furthermore, TaskAug put smaller weights on the new classes letting the model prioritize the original classes. The smaller weights are implemented using two arguments, p_max and T. To build a task instance, classes of augmented classes are selected with the probability p and classes of original data are selected with the probability  1-p. The probability p increases from 0 to p_max linearly for T task instances.\n\nOverall, this paper proposes a novel task augmentation method for meta-learning problems and shows that it improves the learning performance from the experiments. However, the class generating method is limited to rotating, which is questionable in terms of novelty and efficacy, and the writing is unclear and unpolished.\n\nAs it is already mentioned in the paper, rotating images do not generate images that are clearly separated from the original class, which the algorithm intended. For example, balls of the natural image dataset, or the alphabet 'O' and the number '0' of character dataset can not be clearly separated by rotating and rotating '6' by 180 degrees generated '9' whose class already exists. The authors only claim that the features from the new classes can provide useful information and do not propose any remedy. Moreover, rotating images is one of the most popular data augmentation methods which are used to amplify the images per class. It is hard to argue that the same translation method does different work.\nAlso, Table 1 and Table 4 show only the contributions of '+ens+val' which is not the main contribution of the paper. The ensemble method is from the Huang et al. 's work, and '+val' additionally uses validation data.\n\nTo improve this paper, the authors can contemplate other method(s) to increase the number of classes which can be clearly separated from the existing classes as the algorithm intended in the first place.\n\nMinor comments:\n\nThere are some unclear parts in the paper.\n1) In Algorithm 1, p<-p_max*min{1,t/T}  means that p increases from 0 to p_max for the first T tasks, but the paper says that p increase from 0 to o_max after T tasks in Section 3.2.\n2) The writing in 4.1.3 is unclear and hard to get what '+ens','+val', and '+ens+val' means. It seems like '+ens' takes ensemble of 60 models acquired during 60 training epochs using training classes, '+val' adds validation classes for the last epoch and chooses the last model, and '+ens+val' adds validation classes for the last epoch and takes ensemble of 60 models. But I'm not sure if I understood correctly.\n3) Why were the validation classes used for the training?\n4) It is hard to understand Figure 3. How about using the same color or same mark for TaskAug 1-shot & 5-shot, Baseline 1shot-5shot, and so on? Because 1-shot and 5-shot are clearly separated in the graph.\nSome typos/errors: Section 4, line 4, probably -> probability"}