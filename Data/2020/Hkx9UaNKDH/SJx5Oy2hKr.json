{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary & Pros\n- This paper proposes a task-level data augmentation technique that augments labels while data augmentation. This process makes novel classes, so generalization might be improved, especially on few-shot tasks.\n- The authors also use an ensemble technique and the validation set to improve the performance.\n- The proposed method improves performance consistently across methods and datasets.\n\nConcerns #1: Why the proposed method is working?\n- Throughout this paper, the verification of the proposed method is unclear. For example, in Section 3.2, the authors said \"Despite the two problems, the fundamental features of the novel images can provide useful information\", however, they do not explain why and how such novel images, especially rotated images, can improve few-shot learning. A more detailed explanation and (theoretical or experimental) verification should be presented in the paper.\n\nConcerns #2: +ens and +val\n- First, using the components +ens and +val is not standard protocol for few-shot learning. In the protocol, the validation set is typically used for only finding a best model while training. Then, the best model is solely used to evaluate few-shot learning classification accuracy on the test set. Many few-shot learning papers report their scores using the standard protocol, for example, the reported ProtoNet in Table 1 does not use ensemble/validation [1]. Thus the comparison in Table 1 is unfair.\n- The components, +ens and +val, are orthogonal to the proposed method, and they are not a contribution of this paper. However, in Table 1, the authors emphasize the two components. Thus, I highly recommend removing such orthogonal components.\n- Table 2-4 show the ablation studies on TaskAug, +ens, and +val. The tables use different datasets but provide the same message. Thus I think the tables might be over-presented (roughly 1 page) compared to the message (TaskAug, +ens, +val can improve the performance).\n\nConclusion:\nThis paper proposes a somewhat novel task augmentation technique for few-shot learning, but I think its contribution is limited. Moreover, +ens and +val techniques seem to be over-claimed, and explanation and verification are not enough.\n\n[1] Kwonjoon Lee et al., Meta-Learning with Differentiable Convex Optimization, CVPR 2019\n"}