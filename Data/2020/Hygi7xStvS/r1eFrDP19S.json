{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides a method for lossless compression of text. It's heavily inspired by the language modelling methods that have been developed for the purposes of predicting the next character/word in a sentence, and it uses this idea as its backbone. The only difference is that the results are presented in the compression setting.\n\nI think we should reject this paper due to the following reasons:\n- I don't see enough of a difference between this and previous work\n- the results are nowhere near SoTA for compression, despite the method being sold to this community\n- there are other papers that do lossless neural compression that could have been used to make a comparison rather than making no comparison at all. For example, \"Practical Full Resolution Learned Lossless Image Compression\" (CVPR 2019) provides a framework for image rather than text, but that could be adapted to this field without any major changes (predict convolutionally characters, rather than RGB values).\n- there's no comparison even with BERT (how well it do to predict the next character vs. this)...\n- no runtime numbers\n- no reproducibility discussion (i.e., how can I guarantee that my decoder can get exactly the same numbers as my encoder so that I can decompress on a different machine)\n- no discussion about whether files were created/decompressed (this is ABSOLUTELY CRUCIAL for compression papers to discuss)\n\nOverall, I am not excited about this paper, and unless the authors put a lot more into it, there's just not enough novelty to justify a publication at ICLR."}