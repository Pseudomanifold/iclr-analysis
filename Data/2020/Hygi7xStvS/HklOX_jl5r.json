{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n\nThis paper explores the effectiveness of the Transformer architecture to the lossless data compression problem.\nIt also proposes a method to periodically revisit tokens that were already compressed for adopting the task setting of data compression, which is essentially online learning of sequence models. \n \nThe authors conduct their experiments on the enwik8 benchmark.\nThey show that the Transformer architecture obtains state-of-the-art results.\n \nThis paper is basically easy to follow, but several typos and statements that should be improved.\nThe problem setting to tackle is interesting.\nHowever, applying a deep neural network approach to data compression problem has already been discussed in several previous studies.\nTherefore, the novelty of this paper is somewhat limited.\n \n \nMy main concern of this paper is that the proposed method was only evaluated on a single benchmark data.\nI believe that it is a bit weak to support the effectiveness of the proposed method.\nThe authors should evaluate their method on several benchmark datasets that have different aspects, such as settings with easy and hard to compress. \n \n \nMinor comment:\nIn Section 4.2, there is a missing citation.\n... we do not use Adaptive Inputs (Baevski & Auli, 2018; ?) ...\nPlease check and fix it.\n\n"}