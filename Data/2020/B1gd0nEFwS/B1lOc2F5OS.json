{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a domain adaptation problem with the source domain not available in addition to previous universal domain adaptation. The paper employs a two-stage training procedure to solve the problem.\n\nIn the first procurement stage, the paper aims to extend the source classification boundary to out-of-domain data and improve the ability of the source classifier to reject out-of-distribution samples. The proposed method generates negative data lying between clusters and far from the cluster centers and uses them to train the feature extractor, the classifier, and the backbone network. The idea of exploiting generated data has been used by previous works [1], but the paper proposes a novel generation method, which is new and useful for universal domain adaptation.\n\nHowever, the generation process has some problems. First, the paper only considers the samples between two clusters. What if considering the samples between several clusters? For data in D_{n}^{(b)}, when the probability of the data is similar among three or more classes, labeling it as the largest two probability classes is sub-optimal. \n\nSecondly, when the target domain is far from the source domain, target data belonging to the shared classes will be in the area of D_{n}^{(b)}. The classifier will classify these target data as the classes in C_n, which will assign high w'(x_t) in the second deployment stage and degrade the performance.\n\nThirdly, there is no theoretical guarantee of how many samples are enough or how to sample a complete set to cover all out-of-distribution areas.\n\nIn the second deployment stage, the paper uses the sum of the exponential probability of shared and private classes. As stated above, with the first stage, the target shared data are not necessary to have a low probability in the private classes. The performance can be degraded.\n\nThe loss for the second stage is more like a pseudo-labeling method to use the binary pseudo-label of 'shared' or 'private' obtained from the model in the first stage. The paper needs to provide an error bound on the final classification result with respect to the error in pseudo-labeling.\n\nThe paper fails to report the influence of the number of samples on the final performance, which is essential to the generative model based methods.\n\n\n\nMinor points: \nThe notation is a little messy, such as what is the relation between D_{n} and D_{n}^{(a)}, D_{n}^{(b)}.\n\n[1] Hoffman, Judy, et al. \"Cycada: Cycle-consistent adversarial domain adaptation.\" arXiv preprint arXiv:1711.03213 (2017)."}