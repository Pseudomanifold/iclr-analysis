{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a model for learning local pixel motions between pairs of frames using local image representations and relative pixel displacements between agents and objects.  The model learned is compared to the ability of the primary visual cortex where adjacent simple cells share quadrature relationships and capture local motion.\n\n\"The representation theory underlies much of modern mathematics and holds the key to the quantum\ntheory (Zee, 2016).\"\nCan the relevance of this claim be elaborated on?\n\n\"Figure 1 illustrates the scheme of representation.\"\nPlease provide more detail here on what is happening in the figure.  The caption and reference here are not informative to what the figure is representing.\n\n\"We obtain the training data by collecting static images for (It) and simulate the\ndisplacement field ...  We refer to this method as self-supervised learning\"\nThis is not self-supervised learning.  In self-supervised learning the training label/signal is generated by the system.  In this case artificial data is being generated as the displacement between images is sampled.\n\nSince the motion between images is artificially generated what guarantees are there that the model is learning to capture realistic motion behavior?  Why not use adjacent video frames?\n\n\"Note that those methods train deep and complicated neural networks with large scale datasets to\npredict optical flows in supervised manners, while our model can be treated as a simple one-layer\nnetwork, accompanied by weight matrices representing motions.\"\nIs there a comparison on execution times of the different approaches?\n\n\"by obtaining the pre-trained models and testing on V1Deform testing data\"\nIs this a fair comparison if the proposed approach was trained on V1Deform training data and the comparison methods were not.  A more appropriate comparison would be to apply all the methods to infer the displacement fields between video frames which is also a more natural application.  This can be controlled to contain small motions if needed.  Why nt use the MUG dataset here?\n\n\"Displacements at image border are leaved out\" -> left out\n\nSections 5.4, 5.5 and 5.6 show only qualitative results with no comparison methods.  Can the authors provide reasons that other methods could not be used for evaluation?\n\nI am not sure I understand the motivation for the approach.  Why do we need this over other methods that can better capture larger motions.  This needs to be more clear from the introduction.  Why do we care if the approach captures aspects of V1 for the tasks presented?\n\nThe work is sensible and the approach is clear but I found the evaluation and motivation lacking in key areas that I mention above.  The authors should revise and make it clear to the reader why we should care about this problem.  Aligning with V1 is interesting but it does not come into play in the applications of the approach or the analysis so I am not sure why I should care.  The evaluation also needs to be much more convincing before I could recommend acceptance."}