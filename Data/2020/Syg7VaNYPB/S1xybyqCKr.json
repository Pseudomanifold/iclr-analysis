{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new model combining an auto-encoder (AE) and a normalising flow (NF). The model, Generative Latent Flow (GLF), uses the AE to map the inputs to a latent space, which is then transformed using the NF. The approach is intuitively beneficial in that the AE can reduce the dimensionality of the inputs such that the NF mapping becomes much faster, computationally. The proposed method is compared to related methods that use a variational AE (VAE) in combination with an NF, and the similarities are pointed out and studied empirically.\nThe authors compare the performance of GLF to a large number of competing methods, showing very competitive results. In particular, for $\\beta = 1$, GLF significantly outperforms its VAE+flow prior sibling in terms of the Fr\u00e9chet Inception Distance (FID) measuring the quality of the generated images.\n\nThe paper presents a well-motivated method, which is extensively and thoroughly evaluated. The experimental part is the paper's main strength, as the method itself is quite incremental (replacing a VAE with an AE). The authors do, however, spend considerable effort comparing the two versions of the method (using VAE and AE, respectively) both mathematically and experimentally. This is very well done and provides the reader with a good understanding of the behaviour of both models. My main concern is the lack of novelty in the proposed method.\n\nI am also slightly concerned that the paper tries to oversell the method a bit. Among the claimed benefits of the method are 1) better density mapping without over-regularised latent variables, 2) single-stage training, 3) minimal reconstruction trade-off, and 4) faster convergence. As far as I understand, benefits 1, 2, and 3 are shared with similar models (VAE+flow) and are as such not unique to GLF. I am not convinced that benefit 4 is true either, as from figure 3, the convergence rates seem to be similar. GLF clearly reaches a better FID, but this seems to happen before epoch 100, which is the earliest shown. It is not clear from the plot if GLF simply starts out being better or when it gains the advantage. Furthermore, in the discussion just below figure 3, the authors note that \"even with large $\\beta$, GLF still slightly outperforms VAE+flow prior\". I find this to be a stretch - had the training stopped at epoch 400, the conclusion would have been that the methods perform identically.\n\nWhile the authors have clearly put a lot of effort into the paper, they seem to have been rushing for the deadline. There are numerous typos and half-missing sentences (too many to list all, but the worst are pointed out below), so the text needs some polishing before publication. I think it would also make sense to rework section 1 and 2 as, currently, they both present introduction, motivation, and related works.\n\nQuestions:\n- Just above section 3.2, you say that you add a random permutation after each coupling layer. This is not shown in figure 1(b) if I understand it correctly. Here, only a permutation after the entire block is shown. Did I misunderstand the model?\n- Were the model runs in figure 3 also repeated as in table 1? If so, are the standard deviations just too small to be seen or nor shown at all?\n\nMinor comments:\n- \"Auto-encoder\" has inconsistent capitalisation throughout the paper.\n- The very first sentence of the introduction misses an ending.\n- Fourth sentence of section 2 misses an ending.\n- Fifth sentence of section 2 changes the notation - I believe it should be z ~ p(z) here to be consistent.\n- There are many examples of a whitespace missing between a reference and the preceding word.\n- p 2: \"detremine\" -> \"determine\"\n- p 4: \"assumptio\" -> \"assumption\"\n- p 6: \"Frchet\" -> \"Fr\u00e9chet\"\n\n"}