{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method that involves multi-scale inputs for each layer that could be used as network architecture search or data agumentation or \n\nPros)\n(+) The idea looks interesting.\n(+) The experimental results look promising.\n\nCons)\n(-) Many typos when denoting figures and tables. See the minor comments below.\n(-) I believe the authors could organize the paper better. Tables and figures that are referred in a page are hard to find quickly. I recommend the authors refine the paper again for better readability.\n(-) Some notations (such as RS-, RS-NAS, and so on) are so vague that hard to follow.  \n(-) I recommend the authors redraw all the figures for clarity. For example, each legend in Figure 2 is hard to take a look at.\n(-) + the comments below.\n\nComments)\n- When doing feature map resize in terms of the resolution, why Bilinear sampling was chosen? Could the authors provide a comparison with other sampling methods?\n- In the related work section for dynamic neural networks, the authors claimed that \"Most dynamic networks methods sacrifice accuracy in exchange of adaption in inference\", but it seems to be quite overclaimed. As shown in the paper [1],  one can find that the author presented they could improve both accuracy and efficiency.\n- How did you find the architecture shown In Figure 3 in the Appendix? What is Xception? Please specify the details.\n-  Designing the pre-defined spatial list of L looks critical, so the authors should describe L in the implementation details. \n- One of the main problem I think is the training budget issue. According to algorithm 1, the inner loop of \"for j=0,..,len(L)\", the overall training time will clearly take L times longer than that of the training setting w/o resizable training. Thus, it does not seem to be fair comparison in terms of the training budget. Namely, it seems that the authors compared with the other data augmentation methods which spend much less training budgets.\n- Hard to grasp Section 3.5 of Adaptive resizable neural network. ResizeLearner looks being attached at the last stage of the original network after the original network is trained, but there is no further information about what ResizeLearner learns and how ResizeLearner selects the optimal sub-network. \n\n[1] Universally Slimmable Networks and Improved Training Techniques, https://arxiv.org/pdf/1903.05134.pdf.\n\nMinor comments)\n- Wrong section and figure references:\n  - 'It also mitigates the co-adaptation issue which we will discuss in Section 3.3'(indeed it is Section 3.4), 'The network architecture along with feature map resolution and channels number are shown in Figure 4' (it should be Figure 3).\n  - - Figure 3(d) referred to in section 4.4  would be Figure 2(d) indeed.\n\nAbout rating)\nThe authors provided a novel technique about the resizable approach and the experimental results look promising. However, the paper needs to be revised and looks like it does not ready to be published now. If the authors could revise the paper and concern my comments well, I would increase my rating. "}