{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThe paper discusses ways to use autoregressive flows in sequence modelling. Two main variants are considered:\n(a) An affine autoregressive flow directly modelling the data.\n(b) An affine autoregressive flow whose base distribution is a sequential VAE; equivalently, a sequential VAE whose decoder is an affine autoregressive flow.\n\nPros:\n\nThe paper is very well written and crystal clear. I particularly appreciated the motivating example that shows how each layer of an affine autoregressive flow reduces the order of a linear dynamical system by 1, and the connections with modelling temporal changes and moving reference frames.\n\nThe methods are technically correct and well-motivated. The experiments are done well.\n\nOverall, the paper scores high on writing and technical quality.\n\nCons:\n\nIn my opinion, the paper scores low on novelty and original contribution.\n\nIn general, it's not clear to me what the claimed contribution is. More specifically:\n\nIs the claimed contribution new methodology for modelling sequences? In my opinion, using flows as VAE decoders, or adding latent variables to a flow model and training it variationally, are standard applications of existing techniques and I wouldn't consider them particularly novel.\n\nIs the claimed contribution improved modelling performance? The main results are that (a) replacing Gaussian decoders with autoregressive flows improves performance, and (b) adding latent variables to the base distribution of an affine autoregressive flow also improves performance. Both of these results are exactly what one would expect from our experience with these methods. Other than that, the paper doesn't present any results that indicate the particular models used enable us to do things we couldn't do before, or improve against the state of the art in sequence modelling.\n\nIs the claimed contribution useful representations? The motivation for using the flow in this particular way as a VAE decoder is that the flow will model low-level correlations whereas the latent variables will capture high-level dynamics. However, the experiments (e.g. the visualizations) don't support this claim, and the usefulness of the learned representations hasn't been demonstrated in an alternative way,\n\nDecision:\n\nEven though the paper is technically correct and well written, my decision is weak reject because of the lack of novelty and original contribution.\n\nSuggestions for improvement:\n\nMy main suggestion to the authors is to keep up the good work, but also reflect on what the specific contribution of the paper is, and try to make a stronger case for it. Some minor suggestions/corrections follow:\n\nEq. (8): As written, the expression makes little sense as \\sigma is a vector. I understand that there is supposed to be a sum over the elements of log\\sigma, so I'd suggest expressing that more clearly.\n\nEq. (9): It seems to me that the last Jacobian is upside down.\n\nIn general, it would be good to be more thorough on how this paper is similar to related work and how it differs. There is also this related work which may be good to discuss:\n\nLatent Normalizing Flows for Discrete Sequences, https://arxiv.org/abs/1901.10548\n\nIn the particle analogy of the motivating example of section 3.1, it would be good to say explicitly that x is the position, u is the velocity and w is the force, to make the example even more intuitive.\n\nThe paper only considers affine autoregressive flows, but there has been a lot of recent work on non-affine autoregressive flows that are more expressive, for example:\n\nNeural Autoregressive Flows, https://arxiv.org/abs/1804.00779\nSum-Of-Squares Polynomial Flow, https://arxiv.org/abs/1905.02325\nNeural Spline Flows, https://arxiv.org/abs/1906.04032\n\nSuch flows could improve the experimental results of the paper. At the very least, it would be good to discuss them as more flexible alternatives.\n\nIn section 3.2, a third and very significant limitation of the flows discussed here is that they act elementwise on the dimensions (e.g. pixels) of y_t.\n\nIn the experimental section, it would be good to describe on a high level what the architecture of the VAE is, especially the architecture of the prior and the encoder, and the types of distributions used there (e.g. diagonal Gaussians or otherwise).\n\nIt would be good to show samples from the models in the experimental results."}