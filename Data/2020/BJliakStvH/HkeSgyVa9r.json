{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this work, a novel inverse constraint learning method is proposed, where the goal is to find out the constraints over state-action pairs for given demonstration and MDP **including a reward function** (so different from inverse cost learning). The novelty of this work comes from introducing maximum entropy inverse reinforcement learning (MaxEntIRL) framework to previous works [1, 2], and this work mainly focused on the tabular setting. The objective of this work is to solve the optimization in (8), which tries to find out the constraint that maximizes the probability of trajectories that cannot be generated if that constraint is applied. (Such an objective minimizes the normalization constant in (5) and results in maximization of the demonstration likelihood under the constraint.) To solve this objective, the proposed algorithm first computes the feature occupancy (Algorithm 1), and then use those feature occupancy with greedy iterative constraint inference (Algorithm 2 that motivated by maximum coverage problem) to get constraints. Two experiments in the GridWorld show that the proposed method effectively works. \n\nI think this work is quite fundamental, impactful to be accepted at the conference and is possibly extended to practical scenarios (like explainable and safety RL and imitation learning) in the future. One thing I\u2019d like to point out is to enhance the readability by reordering contents and adding some additional explanations to clarify their arguments. There are a few comments and questions I have:\n\n- The exact definition and usage of nominal environments and rewards are still unclear to me. For example in Figure 2 (b), how did you define and get nominal MDP?\n- Since Figure 1 is related to the second experiment, I recommend moving it to the experiment section. \n- At 3.2.1., \u201cBecause constraints are sets of state-action pairs, imposing a\nconstraint within an MDP means restricting the set of actions that can be taken from certain states.\u201d -> Need clarification\n- At 3.2.1., \u201cFor MDPs with deterministic transitions, it is clear that any agent respecting these constraints will not visit an empty state.\u201d -> Why?\n- At (8), $\\{\\}$ to $\\emptyset$\n- In Figure 3, I was a bit confused about the relationship between the threshold and the false positive rate at first glance. What I understood is that a small threshold leads to lots of iteration for constraint selection, which increases the false positive. I want authors to add some comments on that.\n\n Reference\n[1] Chou, Bereson, Ozay, \u201cLearning Constraints from Demonstrations,\u201d arXiv 2019\n[2] Chou, Bereson, Ozay, \u201cLearning Parametric Constraints in High Dimensions from Demonstrations,\u201d CoRL 2019"}