{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this work authors consider a problem of 'model compatibility' of GANs, i.e. usefullness of the generated samples for classification tasks. Proposed 'Boundary Calibration' GAN attempts to tackle this issue by adding non-adversarial terms to discriminator, obtained as outputs of the classifiers trained on the original data. For evaluation, it is proposed to compare accuracies obtained by classifiers trained on generated and on real data (termed 'relative acurracy'). Experiments show that the proposed methods improve such scores.\n\nPros:\n- considered problem seems to be important GAN application which has not yet received too much attention.\n- proposed method seems to improve the accuracy of classifiers trained on generated data.\n\nCons:\n- the paper is poorly written, has multiple typos and often it is unclear what authors mean.\n- the proposed evaluation does not exactly measure the potential improvements from training classifiers with generated data. It would make sense to provide information if generated data can improve classifier's scores, if added to the training data (or small part of it).\n- it is unclear whether or not the proposed technique does not affect the sample quality, as no quantitative metrics are provided.\n\nDetails:\n1. Quantitative scores for quality of generated samples are not provided. Authors instead provide few samples and state that it is difficult to detect the difference. Although sample quality is not the main task here, it certainly is important - otherwise we could train generators solely against the classifier 'boundary calibration' loss terms - this, however would likely lead to adversarial examples. Combining two losses often leads to trade-offs, hence showing that we can improve 'model compatibility' without the loss of sample quality is actually crucial. Metrics such as Inception score [1], FID [2] or KID [3] are desired, especially given relatively poor quality of cifar and mnist samples from all models.\n2.The main metric used is the ratio between accurracies of classifiers (trained on real and generated data). It is hence difficult to tell if the classifiers that were used were trained reasonably and achieved reasonable scores.\n3. In the abstract, authors claim that 'GANs often prefer generating easier synthetic data that are far from boundaries of the classifiers'. Although for some GAN settings the generators might be biased to do so, in general this claim is unfounded, as GANs optimize divergences that are agnostic to classifier boundaries.\n4. It is unclear what kind of classifier-output is used as an input to MMD. Are these continuous logits, discrete class numbers, or one-hot-encoded class identities?\n5. Authors use WGAN and MMDGAN with gradient penalty. It is unclear how gradient penalty is applied to MMDGAN as what should be penalized is the witness function, which is different than in WGAN-GP [4], see e.g. [3].\n6. It is unclear how embeddings of class information are concatenated to discriminator inputs (p.5).\n7. It is unclear to what extent feature selection is deterministic. Authors argue in Section 5.4 that the intersection of top-k features selected from two models should be large. It would be good to provide the same statistics for features selected twice on the same sample.\n\nOverall, the paper currently does not match the quality requirements of ICLR, however it has potential for improvement if the mentioned issues are addressed.\n\nTypos/unclear expressions:\n[p1] 'may not willing' >> 'may not be willing'\n[p1]'with the property similar to the original data is demanding'  >> properties, demanded/in demand\n[p2] 'Although GANs are versatile as aforementioned' - strange wording\n[p2] 'The pioneered work' >> 'pioneering work'\n[p2] 'information of models' >> 'information from the models'\n[p2] effects >> affects\n[p3] related works >> related work\n[p3] distribution of label >> distribution of labels\n[p3] 'generated dataset adopt ' >> 'generated dataset will adopt '\n[p3] 'To known about the boundary' >> ' To know the boundary'/'To include the information about the boundary'\n[p3] 'the a distance'\n[p3] 'the problem to distinguish whether two sets of samples' \n[p4] 'If they are close the sets might be sampled from the same distribution' (?)\n[p4] tries to minimized the MMD\n[p4] would not leads to\n[p6, Table 2 caption] number of estimator used\n[p6] at Appendix\n[p6] depresses the model compatibility (?)\n[p6] can providing\n[p7] to known how\n[p8] our work open >> our work will open"}