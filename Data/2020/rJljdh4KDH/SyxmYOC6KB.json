{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduces Space2Vec, a space representation learning model. The work is motivated by the biological grid cell\u2019s multi-scale periodic representations and the success of representation learning of NLP. So, the key idea behind the model is two-fold. On one hand, utilize the position information and the context associated with the position. On the other hand, the authors build a multiscale point space encoder based on Theorem 1 (in the paper), which was previously proved by Gao et al. (2019).\nThe multi-scale point feature encoder is novel. The experimental results turn out that the whole model is good at predicting features using only location information but does not outperform the RBF kernel (on validation) in terms of using spatial context modeling.\nOne core selling point of this paper is dealing with location distributions with very different characteristics. This is very well motivated at the beginning. More analysis/statistics would help better understand how the model \"theory\" wins Table one. See comments on experiments.  \n\nI have some comments/questions about model architecture and also experimental results/analysis.\n\n1. Regarding Contextual Embedding\n-- The encoder of this paper is not doing much \u201ccontextual embedding\u201d.  The encoder typically encodes features of each position independently thus lead to very local embedding. \n-- The location decoder would reconstruct the same (or similar) type of point features given embeddings of locations of the same type. As the distributions of different types of locations are very different, an encoder capable to deal with multi-scale data is crucial here. \n-- The Spatial context decoder, like a context-dependent language model, would reconstruct the current position features, given the neighboring information.\n-- Overall, unlike many existing pre-training models in NLP with deep encoders, the full model of this paper is with very local encoder, while the decoder does the most work of \u201cgathering contextual information\". \nAs you claim your model to be \"a general-purpose space representation model\", can you describe/specify how you would use your model for other tasks? Will you take some intermediate output of decoders to be representations? Or will you fine-tune the whole encoder-decoder?\n\n2. For experiments:\na.\tI do not prefer saying your method outperforms RBF in the \u201cspatial context modeling\u201d task when you getting worse validation set performance. It is interesting that RBF is stronger in terms of validation.\nb.\tIs it possible for the authors to do some statistics on the different types of locations? For your first task \u201clocation modeling\u201d, should we expect to see that your model does not have very bad performance on certain types of locations while other non-multi-scale approaches do? This is trying to provide better support to one of your core contributions.\nc.    Again, to claim the model can be widely applied, try more tasks?\n\n\nTo clarify my \"experience assessment\", I mean I read many related representation learning papers rather than specific papers related to GIS data.\nMy actual rating for the paper is between weak reject to weak accept (but the system does not have intermediate choices). I would like to hear the author's feedback to further revise the rating.\n\n"}