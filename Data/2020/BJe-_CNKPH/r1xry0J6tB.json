{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and \"pair\" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. \n\nOverall, this is a nice contribution that unifies some parallel lines of inquiry concerning explainability, and attention mechanism for different NLP tasks. The contrast in findings between single sequence (classification) and other NLP tasks and the implications for explainability is interesting, and provides a piece missing from the analyses conducted so far. \n\nThe main issue I have with the paper is that I'm not sure I follow the rationale in Section 4 arguing that because attention \"works as a gating unit\", it follows that we cannot view single sequence tasks \"as attention\". Can the authors elaborate on why the conclusion follows from the premise? In other words, why is attention inherently *not gating*? This seems like an interesting connection, but again I do not see why attention acting as a gate implies that we should not view it as attention at all. Perhaps the authors could elaborate on their reasoning. \n\nSome additional references the authors might consider. Note that the latter two papers, I think, would seem to broadly disagree regarding self-attention and tasks aside from single sequence (classification): \n- \"Fine-grained Sentiment Analysis with Faithful Attention\": https://arxiv.org/abs/1908.06870\n- \"Interrogating the Explanatory Power of Attention in Neural Machine Translation\": https://arxiv.org/abs/1910.00139\n- \"On Identifiability in Transformers\": https://arxiv.org/abs/1908.04211"}