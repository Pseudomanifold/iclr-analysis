{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper tackles the problems of continual learning and catastrophic forgetting in neural networks. It uses methods inspired by Hebbia learning and complementary learning system (CLS), where there are slow and fast weights in the model. \n\nThe paper addresses an important topic. I find the idea of fast/slow weights to be a refreshing and different approach compared to previous work on catastrophic forgetting. \n\nI had trouble following the details of the DHS Softmax, however. This may stem from my inexperience with Hebbian Learning, but here are some questions/suggestions:\n\n- I am a bit confused exactly what is referred to as post/pre-synaptic connection, what is penultimate layer, etc. A figure might be helpful. \n\n- It also might be helpful to write out an equation for the standard Softmax so it can be compared to Eq 2. \n\n- Related to above, I am confused what is indexed by i,j in Eq 4. Compared to Eq 1, where theta only has one index (k), in Eq 4 theta has two indices (i,j). \n\n- In terms of motivation, can you explain why this Hebbian strategy is applied only to the final softmax? \n\nOther questions:\n\n- Does it seem like DHS Softmax is not as strong by itself but works best in conjunction with others, such as EWC? I do not quite follow how they complement each other intuitively. \n\n- Are there any hyperparameters required for DHS Softmax? It seems to be no? \n\n"}