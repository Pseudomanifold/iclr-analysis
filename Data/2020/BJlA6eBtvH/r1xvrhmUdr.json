{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper addresses the continual learning setting, and aims to mitigate catastrophic forgetting, with results on Permuted MNIST, Split MNIST, Vision Datasets Mixture, and their own class-imbalanced version of the Permuted MNIST dataset. The authors propose to augment differentiable plastic weights - a general neural network component - with class-specific updates (similarly to prior work, such as the Hebbian softmax) at the final layer of a neural network, prior to a softmax. While well-motivated in terms of the background and methodology (indeed, this is a simple way to prevent interference in fast weights), and nicely explored experimentally with lots of examinations into the workings of the method, the weak results on the simpler continual learning settings lead me to consider this a weak reject.\n\nThe authors show that fast weights can be applied in the continual learning setting, but alone they do not perform that well on the more challenging datasets, with mixed results on how much better they are as compared to a naive fine-tuning baseline, and they definitely lag behind synaptic consolidation methods. The authors' method combined with synaptic consolidation methods perform the best, but not much beyond the effect of the synaptic consolidation methods themselves. The authors are encouraged to evaluate their method with a large amount of classes, e.g. as done by iCaRL with CIFAR-100 and ILSVRC, with class-incremental training, to show if their method (a) scales (which I would anticipate, given the class-conditional Hebbian update) and (b) can deal with an alternative continual learning setting; a further resource is the work done around CORe50, which I would consider encapsulates more current thinking and practices around continual learning."}