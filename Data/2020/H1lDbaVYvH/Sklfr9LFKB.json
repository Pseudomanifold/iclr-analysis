{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a novel RL algorithm to minimize \u2018surprise\u2019, and the empirical results show the efficiency. The experiments are well-done jobs. However, there are several problems in other parts:\n1. Some terms in Section 2 are too general, such as \u2018surprise\u2019 and \u2018stable states\u2019. Please define these more precise.\n2. Please give an exact description of the MDP you consider about. It seems that the transition probabilities change by time as you mention \u2018entropic environments\u2019. So I guess it\u2019s a finite horizon MDP?\n3. What\u2019s the final situations of the policy and density function? Please give some theoretical results such as convergence or asymptotic properties of the policy and the density functions. At least, the results can be derived under simple MDP and density function settings.\n4. What\u2019s the relationship between the density function you mention in your paper and state distributions in RL?\n5. The agent trying to reach stable states sounds like exploration is discouraged. How do you explain? \u201cProvably Efficient Maximum Entropy Exploration\u201d by Elad et al. 2018 proposes to maximize negative log-likelihood of state distributions to encourage exploration, which is opposite to your setting. How do you think?\n"}