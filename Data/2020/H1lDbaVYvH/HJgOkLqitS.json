{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes Surprise Minimizing RL (SMiRL), a conceptual framework for training a reinforcement learning agent to seek out states with high likelihood under a density model trained on visited states. They qualitatively and quantitatively explore various aspects of the behaviour of these agents and argue that they exhibit a variety of favourable properties. They also compare their surprise minimizing algorithm with a variety of novelty-seeking algorithms (which can be considered somewhat the opposite) and show that in certain cases surprise minimization can result in more desirable behaviour. Finally, they show that using surprise minimization as an auxiliary reward can speed learning in certain settings.\n\nMinimizing surprise with RL presents challenges because, as the authors point out, exploring a surprising state in the present might minimize surprise in the future if the surprising state can be maintained (and therefore made unsurprising). To formulate this notion of surprise as a meaningful RL problem it is necessary to include some representation of the state likelihood in the agent's state representation; so that the agent can learn how it's actions affect not only the environment state but it's own potential for future surprise. This paper takes a pragmatic approach to this by training a density model on the full set of visited states which are reset at the start of each of a series of training episodes. The agent's policy is then conditioned on the parameters of the density model and the current size of the dataset.\n\nI lean toward accepting this paper as an interesting initial step in applying the idea of surprise minimization to reinforcement learning. The proposed approach is simplistic in how they treat surprise, and therefore illustrative, but probably not practical. The experiments are also illustrative, and while it is clear that surprise minimization won't always generate useful behaviour, the paper doesn't overclaim in this regard. I found the motivation for being useful in natural environments, where maintaining some kind of homeostasis is often key, to be well presented. Given that a significant body of recent work focuses on novelty seeking as a means to guide exploration, I think it is a point worth making that there are many reasonable environments where the opposite behaviour is desirable.\n\nI also found the paper to be quite well written and enjoyable to read overall.\n\nGeneral Comments:\nSection 3, VizDoom: \"The observation space for consists...\"->\"The observation space consists...\"\nSection 2.3: \"...as a multivariate normal distribution...\" it looks like covariance is not accounted for so wouldn't it more accurately be representing each component as an independent normal distribution?\nSection 4.1: \"...fireball explorations.\"->\"...fireball explosions.\"?\nSection 4.1: \"...as shown in Figure Figure...\"->\"...as shown in Figure...\"\n\nQuestions for the authors:\n-Out of curiousity, what do the results look like when using SMiRL as a stability reward but without example trajectories?\n-SMiRL is largely pitched as an alternative to novelty-seeking methods. But it seems to me novelty-seeking could be usefully combined since as you point out SMiRL may still have to explore to find stable states. Do you see such a combination as feasible or are the two methods fundamentally opposed?"}