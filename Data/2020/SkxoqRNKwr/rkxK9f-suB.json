{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\nIn this paper, the author introduces a new privacy notation for the attribute attacks. Under the notation, the author has theoretically analyzed the trade-off between privacy preservation and model utility. Based on the theoretical finding,\nthey further propose an adversarial representation learning paradigm to achieve the best trade-off.  \n\nStrengths:\n1. This paper provides an interesting information-theoretic view to study the privacy-preserving machine learning algorithm.\nBased on this view, the author has presented a comprehensive analysis of the trade-off between privacy-preserve and model\nutility.\n2. The topic of studying the privacy guarantee against attribute attacks is important and implies a wide range of applications, such as preventing the model-inversion attack. \n3. The paper is well-written and provides an enjoyable reading experience. \n\nWeakness:\n1. In experiments, the DP based method should be an important comparison method. However, the DP method used in this paper\nseems to be a weak baseline, which injects the noise into the raw data. Many prior works have prooved that injecting noise\ninto the gradient leads to a better trade-off between the utility and the privacy budget [1]. Thus, the author should re-design their DP baseline and provides comparison results.\n2. There is a related work [2] of reducing the privacy leakage of the feature representation, which also takes a view from the information-theoretic view (i.e. a maximum entropy approach). Although this work focuses on the task of image representation, the author also conducts experiments on a non-image dataset, i.e, UCI dataset. The comparative experiments need to be conducted to show the effectiveness of the proposed method.\n3. The notation of the distribution is confusing. It seems that the author referred $\\mathcal{D}$  to both three joint distributions?\n\n[1]  Abadi et al. Deep Learning with Differential Privacy\n[2] Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach"}