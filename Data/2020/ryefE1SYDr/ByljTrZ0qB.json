{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: \n\nthe authors of this paper propose a two-stage model consisting of a Wasserstein GAN and an autoencoder. The goal is to learn an encoding of data within the GAN framework. \nThe idea is inspired by the concept of VAEs. However, instead of maximising the ELBO, the authors propose to learn/represent the generative model by a Wasserstein GAN (first stage). Here, the architecture is crucial: an invertible MLP_1 is used to map from a standard normal prior into the feature space; then, a classical MLP_2 maps into the data space\nIn the second stage, MLP_2 serves as decoder to train an encoder. By combining the latter with MLP_1, data can be encoded into the latent space.\n\nThe authors experimentally show that their method leads to improved reconstructions compared to previous GAN-based methods.\n\n\nIn the following, a few concerns:\n\n1) The authors motivate their approach with the goal of \"encoding real-world samples\" without accepting disadvantages of VAEs such as \"imprecise inference\" or \" posterior collapse\". However, the comparison to VAEs is difficult since the latent representation of the data learned by VAEs differs from the one of LIA.\nFore example, in contrast to VAEs, where similar data is clustered in the latent space, this is not necessarily the case for GANs (e.g. Mukherjee et al., 2019). Experiments regarding the learned latent representation are missing in the paper (the interpolation experiment in the appendix might be a starting point). \n\n2) The authors use posterior collapse in VAEs as a main argument for introducing LIA. However, it is easy to avoid as stated in e.g. Bowman et al. (2015) or S\u00f8nderby et al. (2016), and hence this argument doesn't make a strong case for LIA. \n\n3) It is difficult to interpret the experiments in Fig. 5: the first 10 iterations might not be very significant.\n\n4) Experimental details are missing. I would appreciate to have model architectures in the appendix (even if the authors are going to make the source code publicly available).\n\n5) How were the accuracies of the generations in Tab. 2 computed?\n\n"}