{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes a variant of ensemble learning for deep networks, more specifically a multi-headed network trained by co-distillation. The authors propose some new defitions for ensemble loss, that is an ensembling loss and a co-distillation loss. The authors show that the proposed losses result in better performing ensembles, while not requiring more computational power.\n\nI would vote for rejecting this paper. The paper suffers from an important experimental drawback, in that it does not compare the proposed methods with baselines from the state of the art (other than single models). The authors state themselves in the paper that the proposed losses are different from the losses in the co-distillation papers, which consist of a combination of the two losses. Without a proper comparison to the state of the art, it is impossible to say if the paper provides a significant contribution or not. \n\nFurthermore, the methodological contributions appear to be rather incremental. In order to be publishable, they would need be justified by significant experimental proof, which is currently lacking.\n\nSection 4, paragraph 1, what does \"about three runs\" mean? It's either 3 runs or it's not three runs. If results are computed on more or less than three runs, it should be explicited.\n\nIn short, I would say that the paper is well written and presented, but I think it is also rather incremental and lacks more baselines to warrant a publication at ICLR or a similar conference.\n"}