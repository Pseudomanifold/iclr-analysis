{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed a new approach for the end-to-end training of ensembles of neural networks. The method consists of two steps. The first one is the construction of a multi-head architecture from a base neural network. The second step is training this model using a co-distillation loss function, where different heads are treated as elements of an ensemble.\nThe idea behind EnsembleNet is similar to the multi-head architecture, but instead of just simply multiply upper blocks to obtain several heads, the authors also proposed to shrink these heads. This idea is not significantly different from multi-head model. Also, the authors noticed that the proposed approach almost does not benefit from more than two heads..\n\n\nExperimental results are quite promising and quite broad in terms of the number of problems to which this method was applied. Nevertheless, it would be beneficial to compare the proposed method with other memory efficient techniques such as: e.g. Shake-Shake regularization [1], deep mutual learning approach [2] in which according the table 2 the co-distillation of two models help to boost the performance of each of them and other similar approaches.\n[1]  Gastaldi, Xavier. \"Shake-shake regularization.\" arXiv preprint arXiv:1705.07485 (2017).\n[2]  Ying  Zhang,  Tao  Xiang,  Timothy  M  Hospedales,  and  Huchuan  Lu.   Deep  mutual  learning.   In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,  pp. 4320\u20134328, 2018\n\nOverall, the paper is interesting but the ideas on which it is based are quite simple and not quite new. The experimental part is promising but should include comparison with other similar approaches. Another concern is scalability of this approach to more than two heads.\n"}