{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents \"EnsembleNet\" a multihead neural network architecture where the multiple heads have the same architecture and are all (roughly) trying to predict the same thing.  The diversity of this \"ensemble\" comes from the random weight initialization.  The paper investigates and compares the ensemble loss structure and the co-distillation loss structure.  They make the interesting observation that for the ensembling loss structure, performance is better when one puts a negative weight on the loss of the ensemble (the mean of the multiple heads) while putting larger weights on the performance of the individual head losses. They also show that for the case of square loss, if the ensembling model is a simple average, then the ensembling loss and the co-distillation loss are equivalent. Finally, they present a general strategy for converting a network to an EnsembleNet.\n\nThey hyperparameter tune with both losses and find that the co-distillation loss is marginally better than the ensemble loss structure.  Overall gains over baseline seem moderate.  \n\nI don't think this paper is an accept.  It has a couple interesting insights, but they seem rather subtle... perhaps this would be of interest to people who are specifically researching these types of ensembles.\n\n"}