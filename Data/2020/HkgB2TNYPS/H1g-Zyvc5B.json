{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: This paper addresses the dependence of few-shot classification with Prototypical Networks on \u201cshot\u201d, or the number of examples given per class. Typically, performance suffers if the algorithm is tested on a task with different shot than it was trained on. The paper derives a bound for few-shot performance that depends on the shot. The bound is used to motivate an algorithm which maximizes the inter-intra class variance of the embedded samples. Experiments show that the proposed method generalizes better to different test-time shot, though not significantly better than simply training prototypical networks across different shot numbers.\n\nProblem importance: I think reducing the dependence of few-shot algorithms on rather arbitrary parameters like \u201cshot\u201d is an important and interesting problem. In realistic applications, it\u2019s unlikely that the number of examples for each new class will be the same.\n\nComments:\n- I think this paper is a rather nice reminder not to forget our linear algebra while engaged in deep learning. \n- One main concern is that the method derives quite easily from intuition (low intra-class variance implies tight clusters, and high inter-class variance implies well-separated clusters), and right now I don\u2019t feel that the derivation of the bound is adding much to the paper. Perhaps this bound could be investigated a bit more. For example, is there a way to characterize the tightness of this bound empirically? Perhaps with a linearly separable classification problem where we know R(\\phi) is 1? Alternatively, can the lower bound be directly optimized to find \u201coptimal\u201d intra and inter class variance? Would this be equivalent to the proposed method?\n- Another main concern is that the paper is narrowly focused on one few-shot method, ProtoNets, though it seems like the intuitions and the method could extend to other methods. Could you explain what other few-shot algorithms can use this transformation in addition to ProtoNets? \n- The discussion of VC dimension in Section 3.2 seems quite disconnected from the main idea of the paper, and I don\u2019t see how the conclusion from the VC analysis is used in designing the proposed algorithm. \n- It might be interesting to see how this method fares compared to the \u201cMixed-k\u201d baseline as the number of shots increase even more.\n- Could you discuss how the number of classes (\u201cway\u201d) might interact with this analysis of shot?\n\nWriting Suggestions\n- I believe Equation (2) is incorrect, it doesn\u2019t make sense that p_\\phi = y since p_\\phi is a probability and y is the ground truth label. Perhaps the right parenthesis is in the wrong place, but then it doesn\u2019t make sense to have an indicator of a probability either. \n- There is occasional sloppiness e.g., \u201c[The presence of k in the first two terms of the denominator of the bound] implies diminishing returns in expected accuracy when more support data is added without altering \\phi.\u201d I think it should say the *bound* is lowered, not necessarily R(\\phi).\n- I suggest adding a related work section\n- Consider moving Lemmas 1 and 2 to the appendix, they don\u2019t add much understanding in my view.\n\nI assumed the correctness of the proofs - it would be good to make these easier to follow for a general audience. I'd consider raising my score if the method was put in more context, if the bound could be analyzed further, and if the paper could be more focused. "}