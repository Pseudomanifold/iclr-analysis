{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "What is the specific question/problem tackled by the paper?\n\nThe paper studies whether discrete communication channels between agents are low-entropy. The claim is that agents that try to solve a prediction task subject to a communication bottleneck will exchange low entropy messages, even if these messages are not explicitly encouraged to have low entropy.\n\nIs the approach well motivated, including being well-placed in the literature?\n\nThe paper is well motivated, though I am not an expert in the area.\n\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nThe support for the claims is almost adequate. The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task. Part of the paper's conclusion is that an entropy constraint on messages is not necessary, but maybe it still is necessary to increase the frequency of successful runs, or help faster learning.\n\nThis means that successful runs lead to low-entropy messages, but what about the unsuccessful runs? Do messages have low entropy as well? \n\nI am also somewhat confused by the second set of experiments. The discussion seems to suggest that setting higher temperature in GS creates pressure for lower-entropy messages. Buf if that's the case, then there's a controllable parameter that implicitly controls an entropy constraint and it's no longer clear to me that low-entropy is emerging.\n\nSummarize what the paper claims to do/contribute. Be positive and generous.\n\nI think the paper does an interesting analysis and makes an interesting point about the problem being studied. I am a bit confused by how the experimental setup supports the claims and their consequences. In particular, I have some doubts about the claim that entropy regularization is unnecessary.\n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\n\nI am voting for acceptance.\n\nProvide supporting arguments for the reasons for the decision.\n\nThe paper sets up a clear problem to study and focuses on increasing our understanding around the issue. After reading the paper a few times I am a bit confused about how the experimental setup supports the claims & conclusions. I think the results in Figs. 1-2 adequately support the claim, but the results in Fig. 3 make it unclear whether the temperature parameter is implicitly controlling entropy. The fact that unsuccessful runs were discarded for the first set of experiments limits the implications of the main claim that low entropy emerges, because an entropy regularization might still meaningfully improve the frequency of successful runs. \n\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\nI think if the paper will be improved if it resolves the lack of clarity around the temperature in GS being an implicit entropy-regularization parameter. Perhaps an entropy-regularized setup is a useful comparison to show that it provides marginal benefit over the setup studied, and this might resolve the lack of clarity around the implications of the claims made from the first set of experiments.\n\nApart from these issues I am happy with the choice of topic and execution of the paper. I also appreciate that due care has been taken to present the work as understanding a phenomenon, to avoid any misconceptions about a new method being proposed. "}