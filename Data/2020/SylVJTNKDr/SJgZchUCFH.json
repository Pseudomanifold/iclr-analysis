{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates the phenomenon of entropy minimization in emergent languages. The paper studies two simple speaker-listener prediction tasks where the amount of information that\u2019s needed to be encoded in the speaker\u2019s message can be manipulated. They find, in both games and across training methods, that speaker agents generally learn to minimize the amount of information conveyed in their messages (i.e. minimize the entropy), such that they are still able to solve the task. The paper then conducts some experiments measuring the robustness to overfitting, and find that more discrete channels leads to better generalization when training on partially shuffled labels.\n\nOverall, this paper is very well written. I think the main result, that speaker agents learn to only convey as much information as is needed to solve the task, is interesting and insightful. I do, however, have a slight concern about the novelty of this result. Given that the environments are very simple (consisting of a single message sent by a speaker and a prediction made by a listener), the line between \u2018multi-agent emergent communication\u2019 and \u2018neural network with discrete representations\u2019 is very blurred. This is addressed only briefly in the related work section, with a handful of post-2016 papers cited. I\u2019d personally want to see a more expanded related work section that goes into some more depth into some of these papers, to be able to better judge the novelty of this contribution. \n\nI found the second result, that making the representations more discrete (by lowering the temperature in Gumbel-Softmax) leads to increased robustness to overfitting, to be less clearly explained, especially how it relates to the first result. It seems obvious to me that, if you can\u2019t pass enough information from the speaker to the listener (when there\u2019s a low temperature), then you won\u2019t be able to solve the task at training time if a lot of information needs to be conveyed (as in the case of randomly shuffled labels). The authors describe this by saying: \u201cWith a low temperature (more closely approximating a discrete channel), this is hard, due to stronger entropy minimization pressure.\u201d But this seems misleading to me, since it\u2019s very different from the \u2018entropy minimization pressure\u2019 that was discovered in the first result (which comes about when both agents are able to solve the task, but the listener has redundant information). Thus, I don\u2019t see how this result is either surprising or connected to the first result. \n\nFurther, the main claim to novelty of this second result is that it tests the information bottleneck principle in a \u2018language learning\u2019 set-up. However, since now the communication is continuous, this setup resembles the classic \u2018neural network prediction\u2019 even more closely, and calling it a language learning setup seems down to semantics. Given this, I\u2019m unsatisfied with the comparison to previous work on the information bottleneck (which is not my area of expertise). \n\nGiven the above points, I\u2019d say the paper is borderline it its current form, with a tendency towards rejection. However, I\u2019d be willing to increase my score if the authors can clarify some of my confusion around the second experiment. \n"}