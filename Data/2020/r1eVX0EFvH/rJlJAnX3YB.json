{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper examines the perturbation-based and invariance-based robustness of deep neural networks. They found that models trained to be robust under l_p threat model are more vulnerable to invariance-based adversarial examples. This paper provides several illustrative examples to tell the reasons behind this phenomenon. An attack method for generating invariance-based adversarial examples is also proposed to attack the (provably robust) models.\n\nOverall, this paper analyzes the invariance-based robustness of deep neural networks with norm-bounded robustness, which is an interesting problem. Several illustrative examples provide reasons on why models robust to perturbation-based adversarial examples could be more sensitive to invariance-based adversarial examples.\n\nHowever, my main concern is on the contributions of this paper. A clear contribution of the paper is to study the perturbation-based and invariance-based robustness together, and present several demo examples to illustrate that. Are there any other contributions in this paper? Is the proposed attack method novel compared with previous methods? And what are the differences between the proposed attack with previous methods? The authors could discuss more on the attack method and compare its performance with others.\n\nI'd like to raise my currently rating based on the author feedback."}