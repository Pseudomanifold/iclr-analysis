{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a fine-grained definition for adversarial robustness, dividing adversarial robustness into perturbation robustness and invariance robustness. Same as the previous definition of adversarial robustness (robustness against imperceptible perturbations), perturbation robustness reflects the model's ability to maintain the prediction after a label-preserving transform. Invariance robustness, on the other hand, reflects the model's flexibility against invariance-based adversaries which changes the actual label within the norm ball around clean samples. With examples from the two-sphere experiment, MNIST dataset with large epsilons and a triplet of natural images, it reaches the conclusion that training models to be invariant to any sample within a norm ball is a bad idea when epsilon is too large. \n\nI like this paper in that it points out the problems with some established evaluation protocols for adversarial robustness, e.g., evaluating the robustness on MNIST with a max l_infty norm of 0.4. However, it comes at no surprise that large epsilon gives rise to non-label-preserving perturbations on images. No automated solution can be inspired from the paper. A more valuable direction would be to evaluate the minimum l_p distance between classes, but it seems intractable at the moment. \n\nAlso, it should be emphasized that the invariance-based adversarial examples exist only when epsilon is improperly high for image classification tasks. This can also be verified by the two-sphere experiments. By taking average of the accuracy on the inner sphere and outer sphere, the accuracy against perturbation attack drops before invariance attack with the increasing epsilon, demonstrating invariance attack is only a problem when epsilon is too large for training the model. For other tasks such as language understanding, invariance-based adversarial examples may be a much severe problem for the seemingly robust models. \n\nAs a conclusion, I tend to accept this paper to draw more attention to a better notion of robustness, or developing more sophisticated approaches to defending against perturbation and invariance attacks simultaneously at a larger epsilon. However, the current version is still short of any foreseeable solution. Still, this is perhaps the best we can hope for.\n"}