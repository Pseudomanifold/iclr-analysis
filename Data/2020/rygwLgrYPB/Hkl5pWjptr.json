{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method to normalize activations in neural networks, based on an upper bound of the sliced Wasserstein distance between the empirical activation distribution and a standard Gaussian distribution. I think this feels like a \"borderline\" case to me. The paper clearly has merits, at the same time there're some issues to be addressed.\n\nPros:\n- The idea is clearly presented.\n- Better performance than BN is achieved in many experiments.\n- Empirical evidence in Section 4.3 looks good, suggesting the proposed method does do the job as expected. The means and variances stabilize as training progresses.\n\nCons:\n- While the method based on sliced Wasserstein distances sounds new, the novelty seems limited since the idea of whitening the activation distribution to unit Gaussian was introduced before as mentioned by the authors. The paper claims the random projection may capture \u201cinteraction between hidden units\u201d, but it seems the method proposed in e.g. Huang et. al. 2018 also has projection matrices that might be doing similar things?\n\n- I\u2019m concerned about the actual computation cost of the proposed method. Although the method does not introduce any additional parameter compared to BN or VCL, it seems to require multiple random projections for each layer (s=256 in the experiments)? This could be much slower than the BN. A clarification/comparison of the wall clock running time would be desirable.\n\n- In terms of the image experiments, I do expect to see results with larger datasets/models, though not absolutely necessary.\n\nTypos:\n- Page 5, Eq. 9, x_i should be h_i instead?\n- Page 9, beta^l_j = 0 and ??^_j = 1"}