{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces \"projected error function regularization loss\" or PER, an alternative to batch normalization. PER is based on the Wasserstein metric. The experimental results show that PER outperforms batch normalization on CIFAR-10/100 with most activation functions. The authors also test their method on language modeling tasks.\n\nCaveat: I'm not an expert in this domain. Hence, please take my rating with a large grain of salt.\n\nComments/questions:\n- What's the computational cost of using PER over batch norm?  \n- Related to my other question: For the CIFAR-10 & CIFAR-100 comparison. What was the training time for BN vs PER?"}