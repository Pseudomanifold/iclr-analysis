{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This manuscript discusses the problem of bias shortcut employed by many machine learning algorithms (due to dataset problems or underlying effects of any algorithmic bias within an application). The authors argue that models tend to underutilize their capacities to extract non-bias signals when bias shortcuts provide enough cues for recognition. This is an interesting and important aspect of machine learning models neglected by many recent developments. \n\nThe only problem is that the paper seems to be a bit immature as the exemplar application is too naive for illustrating the idea. The authors\u2019 idea is to assume that\n\u2018there is a family of feature extractors, such that features learned by any of these extractors would correspond to pure bias. Then in order to learn unbiased features, the goal is to construct a feature extractor that is ''as different as this family\" as possible\u2019\nin practice, their claim is texture and color are biases; one should learn shape instead of texture and color. So the family of feature extractors are the ones with small receptive field that can only capture texture and color. Therefore, what they eventually achieved is the unbiased feature extractor only learns the shape of object and avoids learning any texture and color. \n\nSo, the problem is, in practice, it is very hard to define the family of biased feature extractors. It really depends on the dataset and the goal. Texture and color, in general, are still important cues for object recognition, removing this information is NOT equivalent to removing bias. Just as a suggestion, the background scene might be a better definition of bias. However, with the proposal in this paper, it would be unclear how to define the family of feature extractors for describing background. Therefore, the solution given for this important problem seems to be too ad-hoc and not generalizable. \n\nThe second example (that does not have and experiments on) is action recognition; the family of biased feature extractors is 2D-frame-wise CNNs (object recognition). The authors claim that objects are biases for action recognition systems, but again a large part of action recognition is indeed object recognition. Many actions are defined based interaction of humans with objects (e.g., opening bottle or pouring water from bottle). Some objects may be instructing bias in the task, but not all. Again, the proposed solution in this paper cannot disentangle this. \n\nThe authors need to survey previous texture-shape disentanglement works and then compare with those methods.\n\n\n"}