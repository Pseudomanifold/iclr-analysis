{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary & Pros\n- This paper proposes a transfer learning approach based on fine-tuning and reinforcement learning. This approach adaptively computes the importance weights of classes in the source dataset, for maximizing the transfer effect.\n- The authors consider the importance weight as an action, and evaluation on the validation set as a reward. Using this formulation, they learn a policy network (i.e., importance weight) via policy gradient. This scheme does not require explicit similarity metrics between source and target samples.\n- The proposed method outperforms fine-tuning and DATL in several settings.\n\nConcerns #1: Verification of RL-based approach\n- Policy gradient algorithms should update policy networks to maximize the Q-value function. However, the proposed method considers maximizing only an instant reward at the current iteration in a greedy manner. Also, a number of episodes are typically required for learning while the proposed method considers only one episode. Thus it is hard to guarantee that policy gradient can find a good policy.\n- I think this method is more related to gradient approximation of objective (R) than RL because the advantage A_t can be considered as R(theta_t+1)-R(theta_t), so the gradient might be approximated as A_t*dP(lambda)/dtheta_t.\n- In Algorithm 1, computing lambda is deterministic. However, REINFORCE is based on stochastic policy. So I wonder how to apply REINFORCE into the proposed framework more precisely.\n- Detailed ablation studies on RL-based importance weights should be provided to demonstrate the effectiveness of RL, e.g., weight distribution while training, or using multi-step rewards instead of instant (or one-step) rewards.\n\nConcerns #2: Novelty of the proposed method\n- I think the novelty seems to be limited because the proposed method just trains class-wise importance weights using the validation set.\n- For training the weights, the proposed method uses REINFORCE without any modification to specialize this problem, thus I think this paper has a limited contribution.\n\nConcerns #3: Incremental improvements\n- The performance gains of L2TL over fine-tuning seem to be incremental even many samples in the source dataset are irrelevant to the target task (Table 1 & 4). I think ImageNet has many irrelevant samples to the fine-grained tasks, e.g., ImageNet has a small number of bird-related class.\n- For each target task, the proposed method requires all samples (>100GB) in the source dataset while fine-tuning just needs pre-trained models (< 100MB). Despite the high complexity, the performance gain is relatively small.\n- When fine-tuning, a regularization technique can also provide meaningful improvements on such fine-grained datasets, e.g., MaxEnt [1].\n- Overall, to prove the effectiveness of the proposed method, I think it should provide more improvements.\n\n[1] Dubey, Abhimanyu, et al. \"Maximum-Entropy Fine Grained Classification.\" Advances in Neural Information Processing Systems. 2018."}