{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper first shows some potential issues of softmax loss (i.e., cross-entropy loss with softmax function) and then propose the Max-Mahalanobis center (MMC) loss to encourge the intra-class compactness for better adversarial robustness.\n\nThe MMC loss is essentially minimizing the distance between the feature and the pre-fixed class center. Different from center loss, these centers are determined by minimizing the maximum inner product between any two class centers. Since the norm of these class centers are normalized to a constant. It is equivalent to angles. This acutally reminds me of a number of works in angular margin-based softmax loss. Just to name a few:\n\n[1] Large-Margin Softmax Loss for Convolutional Neural Networks, ICML 2016\n[2] SphereFace: Deep Hypersphere Embedding for Face Recognition, CVPR 2017\n[3] Soft-margin softmax for deep classification, ICNIP 2017\n[4] CosFace: Large Margin Cosine Loss for Deep Face Recognition, CVPR 2018\n[5] ArcFace: Additive Angular Margin Loss for Deep Face Recognition, CVPR 2019\n\nI think these works are closely related to what the authors aim to do, and therefore they should be discussed methodologically and compared empirically.\n\nBesides that, I think it is also worth conducting an ablation study for how to determine these class centers. This paper considers to minimize the maximum inner product. There are a few papers listed below that explicitly discusses how to make the class centers uniformly spaced. The authors may consider to compare these methods for determining the class centers. \n\n[1] Learning towards Minimum Hyperspherical Energy, NeurIPS 2018 \n[2] UniformFace: Learning Deep Equidistributed Representation for Face Recognition, CVPR 2019\n\nFor the experiments, the MMC loss indeed shows some advantages over the softmax loss. I am basically convinced by the experiments, although it can further strengthen the paper if the authors can conduct some evaluations on large-scale datasets like ImageNet.\n\nI appreciate the authors provide many theoretical justifications, which is inspiring. Intuitively speaking, I can understand that shrinking the feature space (i.e., make feature distribution more compact) can improve the adversarial robustness. As a result, I think this paper is naturally motivated and is also theoretically sound. The experiments can be further improved."}