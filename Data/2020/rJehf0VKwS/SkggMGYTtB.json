{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper introduces Knowledge Acquisition (KA), i.e., KL-divergence in the reverse order as the loss function to train student models for sequence-to-sequence tasks, and experiments were done on WMT\u201917 De-En and IWSLT\u201915 Th-En translation tasks. \n\nPros: \nThe paper is clearly written. The authors clearly show the reason to use KL-divergence in the reverse order. They provide a concrete analysis of the effects of minimizing the proposed KA loss function to alleviating exposure bias.\nSome figures like Fig. 1 and Fig. 2 helps to show the paper.\n\nCons:\n As the analysis of the authors, KL-divergence in the reverse order is an alternative of KL-divergence for training student models on sequence generation tasks. However, I have a little concern that the contribution is relatively limited since it is known that KL-divergence is not symmetric and the proposed KA (KL-divergence in the reverse order) may be thought a little straightforward. In addition, KA is only investigated on token-level, as the authors said, \u201cdue to practical issues\u201d. \nThe experiments in this paper may be thought insufficient. Specifically, the authors only establish teacher and student model baselines by themselves. It is reasonable that the proposed method could be compared with other similar KD methods, like [1] mentioned in the paper. Another concern is whether the proposed KA method can be applied in current sequence-to-sequence state-of-the-art teacher models, such as GPT-2 [2] and XLM [3], and whether it is still effective. \n\n\nReference\n[1] Yoon Kim, Alexander M. Rush. Sequence-Level Knowledge Distillation. EMNLP 2016\n[2] Alec, Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog 2019.\n[3] Guillaume Lample, Alexis Conneau. Cross-lingual Language Model Pretraining. CoRR abs/1901.07291\n"}