{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors study the training dynamics of natural gradient descent with linear DNNs. \nSpecifically, they showed that the curvature corrected natural gradient descents preserve the learning trajectory of plain gradient descent and only affect the temporal dynamics. A fractional natural gradient \ndescent method that only applies partial curvature correction is proposed to address the numerical stability \nissue of vanilla natural gradient descent.\n\nThe paper is well presented and the derivations of analytical solutions are clear. Although the analysis is \nlimited to the linear case, it does give some insight into the behaviors of curvature corrected gradient descents.\nHowever, it would be interesting to see how these algorithms would perform under the no-linear case. Also, \nwill these results hold for other neural network structures? For example, the Hessian of each layer of\nResNet will be close to orthogonal and how will that affect the NGD?\n\nOverall I think this is an interesting paper with strong results and vote for accepting."}