{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper seeks to improve adversarial robustness by encouraging images that are close to each other to have similar representations. The authors do so by applying small random perturbations to images (they call such images perceptually similar) and adding a loss term to the representation layer. The authors then demonstrate that this method provides some improved robustness, but not as much as standard adversarial training. The authors are careful to do various evaluations to try to establish the validity of their results. \n\nI vote to reject the paper. The key reason is that prior work already uses the same idea that this paper uses, except the prior work has much better results. I am referring in particular to the work of Zhang et. al [1], first published in January 2019, which performs adversarial training on the representation layer. The method of Zhang et. al has far superior results to performing random noise-based training on the representation layer as this paper does.\n\nIf the goal is to enforce similarity of representations for all images within an Lp ball, I feel that the better way to do it is to enforce similarity of representations in the worst case (via adversarial training, as in Zhang et. al) as opposed to doing so via random noise augmentation. The intuition for this comes from the setting when we are only focused on the final logits and not the representation layer. In that setting, simply adding random noise to input images during training does not lead to good adversarial robustness, while adversarial training is much more effective at inducing adversarial robustness.\n\nI would also caution the authors against using the term \u201cperceptual prior\u201d when the authors are simply proposing that the representation layer should be similar when the input images are close to each other in some Lp-norm. The term \u201cperceptual prior\u201d hints at something much more general, while I would argue that this work is still investigating an Lp-ball smoothness prior.\n\nI do appreciate the thoroughness of the experiments that the authors performed, including the ablation tests, and I believe the paper was written fairly clearly.\n\nAdditional feedback:\n\n- One possible benefit of this method is that it is faster than adversarial training. The authors point out that scaling adversarial training to ImageNet is difficult - if this is the case, could you show that your method still works on ImageNet?\n- What do you mean by \u201cover-fitting to adversarial samples\u201d as a primary drawback of adversarial training in the introduction? This was unclear.\n- In equation (1), the superscript in f^j_clean is not explained.\n- Section 4.1 can perhaps be combined with section 3.1.\n- I also want to point out that ALP is a \u201cbroken\u201d adversarial defense [2] because it is not robust to stronger attacks. Thus, this calls into question the results of Tables 2 and 3, where ALP still looks like an effective defense. Perhaps you can run PGD with more steps and more restarts (at least to the point where ALP no longer provides as much robustness) in your evaluations.\n- As a note, if you do PGD adversarial training on transformed samples (the way you do for random noise samples), you can also get robustness to transformation attacks [3].\n\n[1] https://arxiv.org/abs/1901.08573\n[2] https://arxiv.org/abs/1807.10272\n[3] https://arxiv.org/abs/1712.02779"}