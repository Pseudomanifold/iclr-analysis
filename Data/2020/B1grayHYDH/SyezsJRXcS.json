{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a regularizer on logit similarity to improve model robustness to adversarial examples. \n\nThe introduction needs a bit of work to separate the context of the present submission, the submission\u2019s contributions, and the related work. There is also some overlap with content presented in Section 2. Some of the terminology could also be improved to remove ambiguity in the writing. For instance, adversarial examples are not constructed using \u201cnoise\u201d. Because noise is used in Section 2 to refer to adversarial perturbations, it is not clear from the current write up whether later uses of the term \u201cnoise\u201d, e.g., in Section 3, refer to an adversarial perturbation or random noise. \n\nRobustness claims are backed up by empirical validation, however the experimental setup falls short to provide sufficient evidence. To help the authors assess what evidence is sufficient, I would recommend consulting [https://arxiv.org/abs/1902.06705], which provides guidelines for the proper evaluation of robustness to adversarial examples. The report is more comprehensive, but some of the aspects that need to be improved include in particular:\n\n* The attack does not use sufficiently enough PGD iterations. \n* Transferability experiments are mounted with the FGSM only. \n* It is possible that the similarity between logits is a good regularizer against attacks that only target the final prediction of the model (i.e., the output of the softmax) but not adaptive attacks. One such possible attack would have the adversary generate an input that targets earlier layers of the model rather than the softmax. Considering such adaptive attacks would also contribute to strengthening the evaluation of robustness claims made here.\n\nAnother point of comparison that is missing from the evaluation are verifiable training procedures. For instance, PixelDP [https://arxiv.org/abs/1802.03471] and smoothing [https://arxiv.org/abs/1902.02918] would be good baselines to add to the experiments, at least for datasets where results are available (e.g., MNIST and CIFAR10). None of the approaches used in the evaluation are certifiable. \n\nNitpick on page 7: Gradient masking was introduced in [https://arxiv.org/abs/1602.02697] prior to Athalye et al."}