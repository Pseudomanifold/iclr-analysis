{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a new training objective for increasing robustness against adversarial examples. The objective includes not only the standard cross entropy loss, but also the loss of the model over randomly perturbed inputs, and also includes a term that penalizes $\\ell_2$ distance between logits from perturbed images and their clean counterparts. The paper applies this technique to both increase robustness to $\\ell_\\infty$ adversaries and rotations/translations of the input image.\n\nThis paper has major evaluations that make it unfit for publication. In the paper\u2019s comparison with previous baselines, evaluation produces incorrect results. This calls into question the strength (and implementation) of the attack, and therefore the evaluation of the entire proposed work.\n\nDetails:\n\nIssue with evaluation: The paper reports, with PGD steps=100 for CIFAR10 in the $\\ell_\\infty$ $\\epsilon = 8/255$ threat model, that the adversarial accuracies for ALP and the Madry et al defense range around ~94-95%. Neither of these are true, e.g. see [1] for ALP (this paper shows that the ALP defense is easily bypassed with just PGD) and [2] for Madry et al (this readme shows that 20 step PGD gives you ~48%).\n\n[1] https://arxiv.org/abs/1807.10272\n[2] https://github.com/MadryLab/cifar10_challenge#white-box-leaderboard\n"}