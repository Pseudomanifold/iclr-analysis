{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper provides an empirical comparison between several existing graph classification algorithms, aimed at providing a fair comparison among them, as well as proposing a simple baseline that does not take into account graph structural information.\n\nOverall, I found the experimental section very thorough and sound, with proper ways of performing parameter tuning and reporting test accuracies. On the other hand, I found this paper to miss some deeper insights into why some models perform better than others, and what are the challenges provided by these graph datasets. Also, there are other interesting dimensions of comparison that are not considered (see details below), as well as insights that could benefit future work in the area.\n\nAs a disclaimer, I would like to mention that I am more familiar with graph node classification methods, as opposed to whole-graph classification (which is the focus of this paper), so I cannot assess very well the authors\u2019 choice of which models to compare, and which datasets they were tested on.\n\nPositive aspects of the paper:\na)  Extensive experiments, which try to find the best parameter configuration for each of these models.\nb) I appreciate the addition of the structure-agnostic baselines, although I was missing major details about the particular choice of baselines (see below).\nc) The writing is very clear and easy to follow.\n\nPoints where I found the paper lacking:\n1.  As mentioned above, I believe there are insufficient details about the baselines. For instance what is the global sum pooling over? If it is over neighbors, then this is not exactly \u201cstructure-agnostic\u201d. If it is over features, then why was this architecture chosen as opposed to just a multilayer perceptron, without the Deep Sets component? I see the value in an order-invariant component when aggregating features over a node and its neighbors, but why is this necessary at node level? \n\n2.  What is the range of values considered for each parameter in the hyperparameter validation phase? For instance, I found it a bit surprising in table A.3. that all models need 500 patience. \n\n3. I found the discussion over the results to be quite limited. For instance:\n a) The authors point out that the performance on the NCI1 dataset is different than all the others (it is the only dataset where the baseline is not the best). How is this dataset different? \nb) How do these different models compare depending on certain dataset features? Perhaps a model is better than the rest if a dataset has some particular properties.\nc) Are the differences in performance shown in Tables 3-4 indeed significant, given the standard deviations, or these models practically perform the same?\nd) What makes these datasets challenging, such that the best performing models get up to 70-80%? This could provide interesting insights for future work.  \n\nIf the authors clarify some of the issues above, especially about the results discussion part, I believe this paper may indeed be of value to the graph classification community. "}