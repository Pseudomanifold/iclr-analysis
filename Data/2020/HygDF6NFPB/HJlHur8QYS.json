{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\n********** Summary **********\n \nThe paper conducts an empirical study of 5 recently-proposed graph neural networks (GNN). Three questions are studied:\n1) For some selected set of classification tasks (chemistry and social networks) and datasets (9 different datasets), how do the considered GNN models perform, relative to each other, given the same hyperparameter search strategy and a \u201cnested\u201d cross validation scenario.\n2) How much does the structure (graph edges) bring on top of a multi-layer perceptron (MLP) operating on merely the node features?\n3) Do the considered GNN models exploit structural information of the input graphs beyond that of the node degrees?\n \n\n********** Strengths and Weaknesses **********\n \n+ it is shown that the inclusion of nodes\u2019 degree in its feature representation brings a very large performance boost. This is very interesting.\n+ it is shown that on some datasets, the results of a simple baseline -- only operating on the node features, can achieve similar results to the elaborate GNNs. This result is very informative and suggests that this baseline should always be included in GNN works. \n+ some of the results in table 4 contradicts the corresponding papers which can be informative for the practitioners of the field.\n+ the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field.\n \n- reproducibility and replicability problems is not specific to research done on graph neural network and is a caveat for the general machine learning research as discussed by Lipton and Steinhardt 2018., and in fact for the current scientific practice at large as pointed in the recent NSA report [Reproducibility and Replicability in Science, 2019]. So, it may be important to raise and investigate this in different subfields of machine learning, such as GNN, individually. However, the current abstract and introduction of the paper (before the last sentence of intro\u2019s second paragraph), associates this problem specifically to the research conducted in GNN. I strongly believe the better formulation is to refer to Lipton and Steinhardt 2018 as the troubling trend in ML as well as the NSA report in general and then mention that in the current work the authors focus on the subfield of GNN *classification*.\n- page 1: \u201cFor instance, it is often unclear how hyper-parameters have been selected or which validation splits have been used.\u201d. These being *often* the case in GNN research is not backed up by statistics and is not shown to be specific to GNN. Even if it\u2019s assumed the 5 GNN methods , under this paper\u2019s scrutiny, are representative of GNN classification, GNNs are used well beyond graph classification.\n- it should be clearly discussed what are the additional information that this work brings on top of Shchur et al. 2018. Is it only the shift of focus from node classification to graph classification?\n- the two notions of \u201cfairness in comparison\u201d and \u201cablation studies\u201d are sometimes conflated. For instance, the argument against \u201cfairness in comparison\u201d of using one-hot encoding in GIN is not a matter of fairness (since it\u2019s a novel proposal for node features of GNNs) but rather a matter of \u201cablation studies\u201d (to show the effectiveness of individual components of a new GNN method). \n \n- many concerns regarding the nested cross validation:\na) it is important to note that the paper (as described in algorithm 2) does not do \u201cnested cross validation\u201d despite the claim in various parts of the work. Algorithm 2 conducts model selection based on a fixed train-val split. As such, there is no \u201ccross\u201d validation even with the minimum of 2 folds.\nb) the standard deviations reported in table 3 and 4 are unreasonably high. This important observation is not properly discussed. This high std is in contrast to the standard deviation reported in the baseline papers and makes most methods fall into one-std interval of each other. I strongly suspect that this is due to the non-nested cross validation that is done in this work. That is, the hyperparameters are found using a single val set consisting of 9% of all data. A proper nested cross validation would test each set of hyper parameters against k_in validation folds to make a robust inference of best hyperparameters.\nc) there are 3 runs used \u201cto smooth the effect of unfavourable random weight initialization on test performances\u201d, however the 10-fold outer CV should take care of this to some extent. I would argue it would have been much more helpful to use this budget to do a proper CV for model selection using 3 folds.\nd) The datasets are quite small (600-5000 samples). That might make the 10-fold cross validation problematic and overly optimistic in its performance report (and high variance). It might have been better to do say 5-fold cross validation but repeat the whole process twice. Of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated CV error but as far as I am aware there is a general agreement that getting \u201ccloser\u201d to leave-one-out setup is not a good idea, Furthermore, in my anecdotal experience 10-fold CV for a dataset with only 600 samples can really be problematic. \n \n- would the ranking of GNN methods change with different types of the node features being used?  Some GNN setups might be better at representation learning and thus should work better on raw features (as opposed to hand engineered ones).\n \n- how is the number of parameters (or capacity) across different GNN models and the baseline? Has this been taken into account for a fair comparison? For instance, could it be that higher/lower number of parameters explain the differences across GNNs and baseline due to over/under fitting?\n \n********** Disclaimer. I did not thoroughly check the paper\u2019s report of the 5 GNN methods (summarized in table 1). \n \n\n********** Final Decision **********\n\nI believe the paper has merits as well as interesting findings. It is also true that this is an important concern in machine learning research. However, there are many issues that degrades the quality of the paper. Of all those critiques, I suggest \u201cWeak Reject\u201d mainly due to the ones raised regarding a proper nested cross validation; which is the main proposal of the paper.\n \n\n\n********** Minor Points **********\n- Page 1: \u201cOur results put on a fair and unique reference scale many published results which, as we document, were obtained under unclear experimental settings.\u201c: please rephrase; at least one preposition is missing.\n- Page 3: Nested Cross Validation: the brief textual explanation makes the simple idea more complicated than it is. Instead, I suggest to use citation as well as a small figure or short algorithm describing it since it\u2019s the focus of the paper. It will also serve pedagogical purposes.\n- Table 1: precisely define the two mark \u201cA\u201d (ambiguous) and \u201c-\u201d (lack of information).\n- page 7: \u201chigher or equal than\u201d \u2192 \u201chigher than or equal to\u201d\n- in page 6, it\u2019s mentioned that some experiments took more than 72 hours. This sounds excessive for such small datasets. My guess is that this is due to the fact that large number of epochs are allowed (e.g. 5000 in table A.3) in conjunction with extremely small learning rate (e.g. 1e-6). The question is if those extreme hyperparameter settings are actually important for this study?\n- the first paragraph of 6.1 argues that \u201cGNNs are still unable to exploit the structure on such datasets [D&D, PROTEINS, ENZYMES]\u201d. While this might be true, the conclusion is only based on 5 methods and limited by the design choices such as the architecture. It should be toned down.\n- page 5, \u201cFeatures\u201d paragraph is not very clearly written. For instance, from the sentence: \u201cMore in detail, in the former nodes ...\u201d, it\u2019s hard to understand what \u201cformer\u201d refers to.\n- mean and standard deviation should have the same number of decimals (table 3,4)\n- page 4: \u201cMoreover, the authors applied early stopping, which entails the use of a validation set\u201c. Early stopping, in a less common setup, can be used by only looking at the training set and stopping with the same n-patience strategy as used in this work.\n- page 4: \u201cwe conform to the available code and do not use sampled neighborhood\nAggregation.\u201c: needs more explanation.\n \n\n********** Points of improvements **********\n\nThe main point of improvement would be a proper nested cross validation setup. \nThe authors can also consider the ReScience journal: https://rescience.github.io/\n\n"}