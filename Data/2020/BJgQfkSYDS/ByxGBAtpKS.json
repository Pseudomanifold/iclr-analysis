{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper studies the convergence of actor-critic algorithms with two-layer neural networks under iid assumption. Theoretical results show that, in the aforementioned setting, policy gradient and natural policy gradient converge to a stationary point at a sublinear rate and natural policy gradient's solution is globally optimal. \n\n[Decision]\nI recommend accepting this paper. While these results may not have immediate practical interest, the analysis is an important step in understanding the behavior of actor-critic algorithms with neural networks. The final revision needs to be more clear on the limiting assumptions and include a conclusion section that assembles the results.\n\n[Comments]\nThe first important assumption is the architecture of the neural network. The results in the paper consider two-layer neural networks but the abstract implies that the analysis applies to general neural networks.\n\nThe second assumption is that the state-action pairs are sampled iid from the policy's stationary distribution. In reality, these samples are either gathered online (and are therefore temporally correlated), or from a buffer that is also affected by previous policies. The description of results in the abstract and introduction should clarify this setting.\n\nSection G in the appendix shows the analysis for the projection-free method. The projection radius (R) does not seem to play a role in the new algorithm, but the convergence rate still depends on R. Does R have a different definition in this context?\n\nSubsection 3.1 says \"without loss of generality, we keep b_r fixed at the initial parameter throughout training and only update W.\" Whether this modification affects the optimization of the neural network and its convergence rate is not obvious to me.\n\nThe paragraph above Theorem 4.7 defines the stationary point. Is this point guaranteed to, or assumed to, exist?"}