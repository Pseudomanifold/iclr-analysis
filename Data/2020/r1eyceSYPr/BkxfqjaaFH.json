{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Based on recent progress in unbiased MCMC sampling the paper proposes an unbiased contrastive divergence (UCD) algorithm for training energy based models. Specifically they developed an unbiased version of the gibbs sampling contrastive divergence algorithm for training restricted Boltzman machines. The authors demonstrate their method on a toy dataset, simulated data, as well as a reduced version (only the zero digits) of the MNIST dataset and compare the results with the standard Contrastive divergence and Persistent Contrastive Divergence methods.\n\nScore:\nI find the line of work on unbiased estimators important and the (although i\u2019m not an expert) the theory in the paper seems sound. Further the paper is well written and relatively easy to follow. However I do not find the experimental section completely comprehensive and some of the results seem to achieve worse performance than what is reported in the litterature for both the proposed method and baselines (see detailed questions below). Overall I currently score the paper as a weak reject although I can be convinced to bump the score depending on the author feedback.\n\nDetailed Questions:\nExperimental Results:\nQ1) In [Tieleman2008] log-likelihood values for the full MNIST dataset using a) a small model (25 hidden units) where the likelihood is computed exactly and b) an bigger model (500 hidden units) where the likelihood is approximated. On the full MNIST dataset they train using PCD, CD-1, CD-10 and report approximately Log-Likelihoods of -130 and -85 for the small and large models respectively. My questions are:\nQ1.1) In figure 4 you report approximate log-likelihood values on MNIST (only digits zero) of -150 for the different samplers using an RBM with100 hidden units. That seems to be lower performance than the models in [Tieleman2008] while training on a presumably easier dataset?\n\nQ1.2) In figure 4. Can you comment a bit on the variance of your method which seems to be higher, Is there a Bias/Variance trade-off between UCD and e.g PCD?\n\nQ1.3) [Tieleman2008] Reports training times of 1 to 9 Hours for training in on the full MNIST dataset in 2008 and [Hinton 2006] trained large RBMs in 2006. Why is that setting then computationally time-consuming today in your setup - Is there some difference in the setup that I'm missing? \n\nQ1.4) I highly value enlightening small scale experiments and do understand that computational resources are not available everywhere however I think it would benefit the paper greatly if the proposed method is demonstrated on some reasonably sized dataset (at the very least one of full MNIST, Fashion MNIST, FreyFaces).\n\nQ1.5) In Figure 2 you show some interesting figures for the average stopping time and number of rejected samples on the BAS toy dataset. How does these results look on a real dataset like the MNIST zero digit data?\n\n[Tieleman 2008], Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient,\n[Hinton 2006] Reducing the Dimensionality of Data with Neural Networks\n"}