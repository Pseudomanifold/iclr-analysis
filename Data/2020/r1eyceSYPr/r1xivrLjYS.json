{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes an algorithmic improvement that significantly simplifies training of energy-based models, such as the Restricted Boltzmann Machine. The key issue in training such models is computing the gradient of the log partition function, which can be framed as computing the expected value of f(x) = dE(x; theta) / d theta over the model distribution p(x). The canonical algorithm for this problem is Contrastive Divergence which approximates x ~ p(x) with k steps of Gibbs sampling, resulting in biased gradients. In this paper, the authors apply the recently introduced unbiased MCMC framework of Jacob et al. to completely remove the bias. The key idea is to (1) rewrite the expectation as a limit of a telescopic sum: E f(x_0) + \\sum_t E f(x_t) - E f(x_{t-1}); (2) run two coupled MCMC chains, one for the \u201cpositive\u201d part of the telescopic sum and one for the \u201cnegative\u201d part until they converge. After convergence, all remaining terms of the sum are zero and we can stop iterating. However, the number of time steps until convergence is now random.\n\nOther contributions of the paper are:\n1. Proof that Bernoulli RBMs and other models satisfying certain conditions have finite expected number of steps and finite variance of the unbiased gradient estimator.\n2. A shared random variables method for the coupled Gibbs chains that should result in faster convergence of the chains.\n3. Verification of the proposed method on two synthetic datasets and a subset of MNIST, demonstrating more stable training compared to contrastive divergence and persistent contrastive divergence.\n\nI am very excited about this paper and strongly support its acceptance, since the proposed method should revitalize research in energy-based models. While I find the experiments to be somewhat lacking, this is sufficiently offset by the theoretical contributions of the paper.\n\nPros\n1. The paper reads well and introduces all the necessary preliminaries to understand the method. This is important, since I expect many readers to be unfamiliar with the technique.\n2. The proposed method solves an important problem which, as far as I understand, has been the roadblock in large-scale training of RBMs and related models. It is also elegant and fairly straightforward to implement.\n3. The proof of finite computation time and variance is very nice to have. This is because in some cases removing the bias leads to infinite variance, e.g. a parallel submission on SUMO (https://openreview.net/forum?id=SylkYeHtwr).\n\nCons\n1. I don\u2019t think Corollary 1 (convergence of gradient descent to the global optimum) is true for RBMs, as stated on Page 6. This is because the log-likelihood of RBM, or indeed any latent-variable model with permutation-invariant latents, is non-convex. I would suggest removing this corollary and simplifying Algorithm 2 to be regular SGD, as used in the experiments.\n2. There is no experimental comparison of Algorithm 1 (the general version) and Algorithm 3 (the specialized RBM version). It seems intuitive that the specialized version should have lower computation time, but this must be confirmed.\n3. The experimental section may be significantly improved.\n* It is unclear what value of k (number of initial Gibbs steps) from Algorithm 2 is used.\n* The experiments on just the \u201c0\u201d digits of MNIST seem a bit simplistic for the year 2019. It is also not clear what binarization protocol is used.\n* It would be very helpful to provide estimates of the gradient (not log-likelihood) variance of each method to better understand the trade-off between the bias and the variance.\n* I would also like to see the wall-clock time comparison of the methods.\n\nMinor comments\n* Page 1. Of this kind -> of this class. The data distribution p_v (v; theta) -> The model distribution\n* Page 2. Property -> properties. CD-\\tau -- I don\u2019t think you can correctly refer to your method in this way, since it has at least double the computation time of CD for the same number of iterations.\n* Page 3. Provides -> provide. Likelihood gradient -> log-likelihood gradient\n* Algorithm 1 is an infinite loop with no break clause. It would be good to add a break statement after line 5. This would also simplify the discussion of the method.\n* Page 7. I wouldn\u2019t call the fact that CD doesn\u2019t converge on the BAS dataset remarkable, given that it\u2019s been reported by Fischer & Igel 2014.\n* Page 9. The last paragraph stating that the proposed method is not a replacement for CD is confusing. Can you add a short experiment to demonstrate that this combination makes sense?"}