{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nStability is one of the important aspects of machine learning.  This paper views Jacobian regularization as a scheme to improve the stability, and studies the behavior of Jacobian regularization under random input perturbations, adversarial input perturbations, train/test distribution shift, and simply as a regularization tool for the classical setting without any distribution shifts nor perturbations.  There are already several related works that propose to use Jacobian regularization, but previous works didn\u2019t have an efficient algorithm and also did not have theoretical convergence guarantee.  This paper offers a solution that efficiently approximate the Frobenius norm of the Jacobian and also show the optimal convergence rate for the proposed method.  Various experiments show that the behavior of Jacobian regularization and show that it is robust.\n\nReasons for the decision:\n\nPositives: The contribution of the paper seems to be two-fold:  First a theoretically guaranteed and efficient method for Jacobian regularizer, and second, intensive experiments to show the robustness of the Jacobian regularizer.  Each of these points seem to have important contributions for the field.\n\nNegatives:  An issue might be that the latter contribution seems to be orthogonal to the former since there are no experiments comparing with previous methods mentioned in the paper, and it gives the impression that there are two separate stories in one paper.  For instance, there are no experiments comparing computational time between Sokolic et al. (2017). On the other hand, there are only regularization methods (that are not necessarily designed to be robust) used as baselines in the experiments to show robustness, instead of algorithms that are designed to be robust, e.g., domain adaptation methods for Table 2.  It would make the paper stronger to combine these two lines of contributions into a single story.  For example, it might be better to emphasize more that experiments such as Figure S7 was previously not possible due to inefficient implementation.\n\nMinor comment:  It would make the paper stronger to include some of the main related works in the Introduction section."}