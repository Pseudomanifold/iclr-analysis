{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper introduces SKEW-FIT, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage. \n\nThe paper is well-written and provides an interesting combination of reinforcement learning with imagined goals (RIG) and entropy maximization. The approach is well motivated and simulations are performed on several simulated and real robotics tasks.\n\nSome elements were unclear to me:\n- \"We also assume that the entropy of the resulting state distribution H(p(S | p\u03c6)) is no less than the entropy of the goal distribution H(p\u03c6(S)). Without this assumption, a policy could ignore the goal and stay in a single state, no matter how diverse and realistic the goals are.\" How do you ensure this in practice?\n- In the second paragraph of 2.2, it is written \"Note that this assumption does not require that the entropy of p(S | p\u03c6) is strictly larger than the entropy of the goal distribution, p\u03c6.\" Could you please clarify?\n\n\nThe experiments are interesting, yet some interpretations might be too strong (see below):\n- In the first experiment, \"Does Skew-Fit Maximize Entropy?\", it is empirically illustrated that the method does result in a high-entropy state exploration. However, it is only compared to one very naive way of exploring and it is not discussed whether other techniques also achieve the same entropy maximization. The last sentences seems to imply that only this technique ends up optimizing the entropy of the state coverage, while I believe that the claim (given the experiment) should only be about the fact it does so faster.\n- On the comments of Figure 6, the paper mentions that \"The other methods only rely on the randomness of the initial policy to occasionally pick up the object, resulting in a near-constant rate of object lifts.\" I'm unsure about the interpretation of this sentence given Figure 6 because other methods do not seem to fail entirely when given enough time.\n- In the experiment \"Real-World Vision-Based Robotic Manipulation\", It is written that \"a near-perfect success rate [is reached] after five and a half hours of interaction time\", while on the plot it is written 60% cumulative success after 5.5 hours and it is thus not clear where this \"5.5 hours\" comes from."}