{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a noise regularization method which adds noise on both x and y for conditional density estimation problem (e.g., regression and classification). The writing is good and the whole paper is easy to follow. However, I vote for reject, since the novelty is somehow limited, the claims made in the paper is not well supported and experiments are not very convincing. \n\n1. Adding noise on x (e.g., [1]), y (e.g., [2]) is not new. Though it is claimed that this paper extends previous results on classification/regression to conditional density estimation which is a more general case. This claim is not well supported. Experiments are still evaluated in classification/regression tasks.\n\n2. Theorem 1 & 2 in Sec 4.2 only show the asymptotic case, which are quite obvious and seems helpless in understanding the advantage of adding noise regularization in conditional density estimation.\n\n3. Sec 4.1. The explanation that Page 5, ```\"The second term in (6) penalizes large negative second derivatives of the conditional log density estimate...\". It is hard for me to understand. Large positive second derivatives also lead to poor smoothness.\n\n[1] Learning with Marginalized Corrupted Features, ICML 2013\n[2] Learning with Noisy Labels, NIPS 2013"}