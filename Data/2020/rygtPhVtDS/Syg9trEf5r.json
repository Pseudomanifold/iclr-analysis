{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a regularization technique for conditional density estimation. The method is simple: adding noise to the data points, and training on the noisy data points. The paper also further gives an interpretation of the method, as a form of smoothing the curvature of the density function. It further proves the consistency of the method.\n\nPros:\n(i) The paper is well written.\n(ii) The method is simple to implement.\n(iii) The authors demonstrate a clear intuition of what the method does, i.e., as a form of smoothing.\n\nCons:\n(i)  The method itself is not novel. Adding noise to for regularization is a quite common technique used in many different applications.\n(ii) The experiments performed in the paper are all very small scaled. Not very convincing.\n(iii) Continuing the last point, it is unclear whether such technique can be scaled up. The method in another view is to replace the empirical distribution with a kernel density estimate. We know kernel methods don't scale. It's hence questionable whether the method can give any benefits in large scale dataset.\n(iv) If the use of noise regularization is to smooth the density function, how does it compare to methods that enforces Lipschitz continuity? e.g., gradient penalty, or construct a lipschtz constrained network such as in [1].\n\n[1] Sorting out lipschitz function approximation. Anil et al.\n\n"}