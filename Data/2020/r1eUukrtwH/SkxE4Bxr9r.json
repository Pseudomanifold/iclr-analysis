{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Overview: This paper describes the Variational InfoMax AutoEncoder (VIMAE), which is based on the learning principle of the Capacity Constrained InfoMax. The core idea behind VIMAE is that the encoding information is not bounded while network capacity is. The issue that VIMAE can handle, and where VAE fails, is that representations are not informative of input data, due to the information bottleneck idea that VAE is built upon. The authors describe InfoVAE and \u03b2-VAE, which both attempt to solve this problem. The theory behind VIMAE is then described and tested against VAE and \u03b2-VAE, in their abilities to evaluate the entropy of Z, in reconstruction and generative performance, and in robustness to noise and generalization. \n\nContributions: The authors clearly state the contributions of the paper themselves, but in a summary: a derivation of a variational lower bound for the max mutual info of a generative model, definitions and bounds estimation for a VAE, associations for generative quality and disentanglement representation to mutual information and network capacity, and finally proposing the Capacity-Constrained InfoMax.\n\nComments:\nPage 1: \u201cOur derivation allows us to define\u2026\u201d -> this sentence is a bit long and took me a few reads to understand, reword this please.\n\u201cDerivation of a variational lower bound for ...a geneaive* model belonging..\u201d -> \u201cgenerative\u201d\nPage 2: \u201cWe conclude the paper with experimental results and conclusions.\u201d You already said conclude twice in this sentence, I feel like this could be better worded to avoid that.\nPage 5: \u201cIn order to test the assumption that it is sufficient..\u201d -> This wording is also hard to wrap my head around. Too many commas.\nPage 6: \u201cIn figure 1 are plotted the 2d\u2026\u201d -> This sentence could be reworded\n\nMy main complaint with the paper is that there are quite a few places that could be reworded better in addition to these. Please fix this.\n\nYou mention both InfoVAE and \u03b2-VAE yet only test your models against a \u03b2-VAE model. What was your reasoning for this?\n\nThe semi-supervised learning experiment is interesting (the one based on Zhao et al.) VMAE models are able to classify better regardless of noise or sampling procedure, especially the smaller capacity model VMAE-1. I\u2019d like to hear a discussion in the paper about future work and how the authors believe this could be applied elsewhere.\n"}