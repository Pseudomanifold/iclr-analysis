{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "I went over this work multiple times and had a really hard time judging the novelty of this work. The paper seems to be a summary of existing work reinterpreting variational autoencoding objectives from an information theoretic standpoint. In particular, the paper seems to follow the same analysis as in Wasserstein Autoencoders (Tolstikhin et al., 2017) and InfoVAE (Zhao et al., 2017).  It is unfair to say that the objectives were \"derived independently\" since these works are from a couple of years ago. \n\nThe paper also lacks discussion on two crucial works in this space:\n1. https://arxiv.org/abs/1711.00464 shows how to trade off rate-distortion in VAEs.\n2. https://arxiv.org/abs/1812.10539 shows how to learn informative representations by eliminating the KL divergence term and at the same time specifying an implicit generative model (Theorem 1).\n\nre: disentanglement. Unsupervised disentanglement has been shown to be theoretically impossible and several key challenges have been highlight w.r.t. prior work. Again, the relevant paper: https://arxiv.org/abs/1811.12359 has not even been cited. More importantly, the claims around \"more disentangled representation\" are imprecise in light of this work.\n\nA proper discussion on the contributions of this work as well as discussion on the above related works would be desirable on the author's end."}