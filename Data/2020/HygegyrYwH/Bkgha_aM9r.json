{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary :\n1. Classification task with shallow Relu and logistic loss.\n2. Showing fast global convergence rate with polylog width for both training and generalization error under appropriate assumptions\n\nOverall, this paper is very exciting and surprising. At some point, I was trying to prove such results, but couldn\u2019t get it. This paper should be accepted as an oral presentation in ICLR. If the authors can address some of the questions in the comments, I will be happy to increase the score.\n\nAdvantages:\n1. Better results for classification task with shallow Relu in terms of global convergence rate and network width\n2. Showing the essence of the power of over-parameterization that the weights don\u2019t change much\n3. Clear logic and proof\n4. Also discuss the stochastic GD/generalization error? (I didn\u2019t read that part)\n\n\n\nDisadvantages:\n1.Bug in proving Theorem 2.2, a larger lambda is needed (but won\u2019t influence the polylog result). See the below for a fix.\n2.In the proving sketch, why ||W_t-W_0||_F=O(ln t)? In the proof, it looks like ||W_t-W_0||^2_F=O(t), as in the third equation on page 12. More explanation about proof sketch is needed.\n3.Typo\na.The \\odot operation in Equation 5.1 is not defined. According to later computation, this operation seems to be the hadamard product between two vectors. But this notation is not widely used and some brief introduction will be benefinitial.\nb.On page 1, \u201c\u2026 and standard Rademacher tools but exploiting how little the\u2026.\u201d, \u201cbut\u201d seems to be a typo.\nc.On page 2, \u201calso suffices via a smoothness-based generalization bound\u201d, \u201csuffices\u201d should be \u201csuffice\u201d.\nd.Last formula in page 11 missing a \u201c>0\u201d in the indicator function.\n4.(optional) why using ||W_t-W_0||_F instead of ||w_r(t)-w_r(0)||_2, which used in previous work for square loss, for the analysis? Is there any benefit or restriction here? \n5.(optional) Give more insights about intermediate quantities such as \\hat R^(t), \\bar{W}, etc.\n\nComments:\n1. More arguments for polylog width in last section needed. E.g., give a specific case where the gamma in Assumption 2.1 is constant, or comparable to the smallest eigenvalue of NTK; otherwise in the worst case, gamma can be as bad as  the smallest eigenvalue of NTK over n, which ruins the polylog results. To be more specific, we can always set q to be the uniform distribution over [n], then \\|q\\odot y\\|_2 is indeed 1/\\sqrt{n}, hence \\gamma_1\\leq \\sqrt{\\lambda_{max}(K_1)/n}. If K_1 has constant spectral norm(which is the case if all the data points are orthogonal to each other), then \\gamma_1 will depend on 1/n.\n2. For the over-parameterization theory, more references are needed. https://arxiv.org/abs/1902.01028 [Allen-Zhu, Li] is about generalization bound for the over-parametrized networks, https://arxiv.org/abs/1810.12065 [Allen-Zhu, Li, Song] and https://arxiv.org/abs/1905.10337 [Allen-Zhu, Li] are about the over-parameterization bound for more than two-layer networks. https://arxiv.org/abs/1906.03593 [Song, Yang] obtains a better width bound for two-layer neural networks under the framework of https://arxiv.org/abs/1810.02054 [Du, Zhai, Poczos, Singh].\n3. Theorem 2.2 shows that the average loss converges. Does this imply after training for T steps, we obtain good weights with small logistic loss? Can you get results showing the loss is decaying, like Theorem 4.1 in https://arxiv.org/abs/1810.02054 [Du, Zhai, Poczos, Singh]?\n4. On page 8, the lower bound of \\lambda_0 is given as \\delta/n^2. Is this bound tight? Is this lower bound achievable? \n5. Under what assumptions can we prove o(log n), say poly(log log n) width?\n6. What is the role of logistic loss in the proof? In general, if we replace logistic loss with square loss, will this make it harder to train neural networks?\n\n\nThe original analysis might has some flaw/bug:\n\nIn the proof of Theorem 2.2, top of page 12, to show \\hat R^{(t)}(\\bar W)<= \\epsilon/4, the term y_i <\\nabla f_i (W_t) - \\nabla f_i (W_0) , W_0> seems to be forgotten to consider.\n\nThis could be a fix.\n\nNote that above term equals y_i/m \\sum_{r=1}^m ( 1_{[< w_{r, t}, x_i> >= 0]} - 1_{[< w_{r, 0}, x_i> >= 0]} ) < w_{r, 0}, x_i >. We can use concentration to bound <w_{r, 0}, x_i>, such that with high probability, it will be no larger than polylog(n). Correspondingly, we know this term won\u2019t be too small. Adding this extra polylog factor into lambda, we can fix the proof.\n"}