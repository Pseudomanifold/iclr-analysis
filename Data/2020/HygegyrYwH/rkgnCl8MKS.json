{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary and Decision \n\nThe authors of this paper studied the optimization and generalization properties of shallow ReLU networks. In particular, the authors were able to show a width dependence that is polylogarithmic on the number of samples, probability or failure, error tolerance, and a margin parameter. This work is unique in that the authors showed how to bound many key quantities in terms of the margin parameter, and drew a connection with the neural tangent kernel's maximum margin. Furthermore, the overall reading experience was very smooth, although I do have some minor comments later. \n\nThe main concern from me is on the implicit dependence of the margin parameter \\gamma, most notably this can lead to the width dependence to be polynomial in terms of the number of samples and the minimum separation distance. While this concern warrants a careful discussion (below), I believe the paper still offers a nice analysis of shallow networks. \n\nOverall, I would recommend accept for this paper. \n\n\nBackground \n\nThere has been a large number of works studying very wide networks, showing both optimization and generalization results. While there has been great progress, most of these existing results require the width of both deep (and shallow) networks to be very large. For example, even by being polynomial in the number of samples, the networks are already unrealizable in practice. Therefore, guarantees with much better dependence is highly desirable. \n\n\nDiscussion of Contributions\n\nAs the title may suggest, it is a bit surprising that we can show that polylog width is sufficient. Intuitively, we can imagine that the classification margin can grow exponentially more complex as the number of samples increase. I believe the nice result can be attributed to a careful analysis of key quantities such as \\| W_t - W_0 \\|_F in terms of the margin parameter from Assumption 2.1. \n\nSome nice examples of the analysis in this paper include the introduction of the weight matrix \\overline{U} and \\overline{W} as an intermediate between W_0 and W_t, and observing that the activations of the ReLU \\xi_{i,t} do not change very much during training. The tricks together led to a very tight bound on the change in weights \\| W_t - W_0 \\|_F in terms of the margin parameter \\gamma. As the authors mentioned on page 6, this tight control was used to bound the Rademacher complexity later. \n\nThe connection drawn between the margin assumption and neural tangent kernel (Proposition 5.1) is also interesting on its own. The authors intended this result to serve as a justification of the margin assumption (2.1). \n\n\nDiscussion of the Margin Parameter\n\nLet me start by saying I'm not completely certain on how to interpret this margin parameter \\gamma in Assumption 2.1. Perhaps I'm missing some obvious ideas here, but I would still like the authors respond with some more details. At the same time, I don't believe this is a sufficient criticism to reject this paper, as I believe the analysis in terms of \\gamma is still valuable. \n\nOn one hand, if we were to assume the margin condition holds for all possible data points (i.e. Assumption 3.1), then there is no concern about polynomial dependence on the number of samples, and this is certainly a reasonable assumption in some applications. \n\nOn the other hand, many of the previous analysis on wide networks were in terms of a minimum separation distance, i.e. assume there exists a \\delta > 0 such that for all i \\neq j, we have \n\t\\| x_i - x_j \\| \\geq \\delta . \nThe authors have provided a discussion in section 5, including both a worst case bound of \n\t\\gamma \\geq \\delta / (100 n^2),\nby Oymak and Soltanokotabi (2019) and an example where the margin is O( n^{-1/2} ) with high probability. \n\nUsing either bounds on \\gamma, we will have a width with polynomial dependence in terms of the number of samples and minimum separation distance. Therefore if we were to compare against previous works in the same benchmark, i.e. using a minimum separation assumption instead, then arguably this work did not achieve a width that is only polylog in terms of the number of samples. \n\nThat being said, I don't believe the authors were intentionally trying to hide sample dependence inside an assumption. The paper is presented in a very transparent way, and the authors were being honest in chapter 5 about the worst case dependence on the number of samples. \n\nTo summarize, it is unclear to me whether the paper truly achieved a width dependence that is polylog in terms of the number of samples, but the analysis in terms of \\gamma remains a valuable contribution. I welcome the authors and the other reviewers to provide additional comments on whether the title and claims of this paper is appropriate. \n\n\nMinor Comments \n\nFor the sake of improving readability, I also have some minor comments that do not contribute towards the decision. \n\n - On page 5, the first observation bullet point in section 2.2, it is written here that by triangle inequality \n     \\| \\nabla \\widehat{R}(W) \\|_F \\leq \\widehat{Q}(W) , \n    I thought you needed an absolute value on the right hand side, perhaps you should mention that \\ell is strictly decreasing. \n\n - On page 6, in the statement of Corollary 3.3, you are missing \\eta \\leq 1, and perhaps the \\tilde on \\Omega and O should be defined. \n - On the same note, while it is obvious that Theorem 3.2 implies this corollary, it is still worth writing up a proof to compute the constants. \n\n - On page 7, the notation \\Delta_n and \\odot are not defined, I had to infer definition from the proof. \n\n - On page 12, just below the second equation, it wasn't immediately clear how \\widehat{R}( \\overline{W} ) \\leq \\epsilon / 4. I believe it's worth expanding the definitions a bit, and explicitly plug in Lemma 2.5. \n\n - On page 12, in the second last equation, I'm actually not sure where this inequality comes from \n      \\| W_t - \\overline{W} \\|_F \\geq \\langle W_t - \\overline {W} , \\overline{U} \\rangle\n    Perhaps it's obvious, but I currently don't see it. \n"}