{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the author shows that for a two-layer ReLU network,  it only requires a network width that is poly-logarithmic in the sample size n to get good optimization and generalization error bounds, which is better than prior results. \n\nOverall, the paper is well written and easy to follow. However I still have some questions about this paper.\n\nOne of my major concerns is that there might be an important error in the proof of the main theorem. Specifically, in the proof of Theorem 2.2 (page 12), it says that due to lemma 2.5, $\\hat R^{(t)}(\\bar W)\\leq \\varepsilon$. However, Lemma 2.5 only shows that $|f(x_i,W_0,a)|$ is small, and the reason $\\hat R^{(t)}(\\bar W)$ can also be small is not explained in this paper at all. Based on Lemma 2.5, I can roughly get that $\\hat R^{(0)}(\\bar W) $ can be small, but the reason why $\\hat R^{(0)}(\\bar W) $ is small is unclear to me, especially when the network width m is only polylogarithmic in n and \\varepsilon. Without a clear explanation on this issue, the theoretical results in this paper might be flawed, and the polylog claim might not be correct.\n\nMoreover, this paper does not provide sufficient comparison with existing work. For example, Assumption 2.1 looks very similar to the assumption made in Cao & Gu (2019a). The definition of $\\hat Q(W)$ has also been introduced in Cao & Gu (2019a). However these similarities are not mentioned in the paper at all. Moreover, the result of Lemma 2.6, which is also one of the selling points of this paper, is actually very similar to Fact D.4 and Claim D.6 in the following paper:\nAllen-Zhu, Zeyuan, and Yuanzhi Li. \"What Can ResNet Learn Efficiently, Going Beyond Kernels?.\" arXiv preprint arXiv:1905.10337 (2019).\n\nFinally, the authors\u2019 claim in the title that the width of the network is poly-logarithmic with the sample size n might be misleading. In fact, in Section 5, it has been discussed that in certain settings about the data distribution, $\\frac 1\\gamma$ is polynomial of n. However, the width is polynomial with $\\frac 1\\gamma$, which means the width is poly of n in these settings. "}