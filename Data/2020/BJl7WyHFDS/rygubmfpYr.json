{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis work proposes a pyramid non-local block that can use  different scales of content patterns to enhance network capability. This work uses different stride sizes and window sizes to obtain information from difference scales.\n\nThe major issue for this work is that the contributions are very incremental. Compared to the non-local operator, this work uses convolutions with stride to create different Q, K, and V for attention operator. I didn't see much novelty in this work. The detail comments are as below:\n\n1. This work is very incremental. Attention operators have been widely used in various scenarios.\n\n2. Pyramid thing to extract information from different scales has been a widely used method in various tasks.\n\n3. It is very hard to see the improvement in Figure 4.\n\nSuggestions:\n\nMore novelty work should be added to be accepted by top AI conferences like ICLR."}