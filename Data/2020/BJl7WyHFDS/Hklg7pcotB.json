{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work the authors propose the Pyramid Non-local Block (PNB) as a neural network module to estimate similarity coefficients between parts of an image. The PNB, unlike a convolution, can efficiently compute correlations between spatially distant parts of the input. In the pyramid non-local enhanced networks, the PNB is combined with standard dilated residual blocks. Applications of this new network are shown and evaluated on edge-preserving smoothing, image denoising and image restoration, where in all cases the evaluations show a small but consistent gain in PSRN compared to baseline models.\n\nThe PNB is based on prior work proposing such non-local blocks, and the main innovation is the introduction of multi-scale processing in the form of a scale pyramid for the reference and embedding feature maps (obtained with strided convolutions with larger kernels). This makes it possible to compute correlations over larger spatial scales without paying the exponential computational cost.\n\nThe paper cites prior work appropriately and lists the relevant hyperparameters used for training. The examples in the appendices show interesting visual improvements.\n\nAs is, I believe this paper would be a better match for a more specialized venue (e.g. a computer vision conference). The main shortcomings of the paper are its limited novelty (multiscale pyramids are an old \"trick\" in computer vision), and the use of baselines that are behind SOTA for the discussed problems -- e.g. Dai el al, \"Second-order Attention Network for Single Image Super-Resolution\" could be a more current baseline for SISR (instead of MemNet) and Liu et al, \"Non-Local Recurrent Network for Image Restoration\" for denoising (which the authors cite and briefly discuss in the introduction). Also, in the RDN paper (which the authors used as a baseline for denoising) applications to superresolution are presented, with reported results better than the ones achieved with PNEN in Table 5. I found it surprising that a less well performing method (MemNet) was used as the baseline for the SISR task in the present work.\n\nFor edge preserving smoothing, where the largest relative gains in evaluation scores are seen in the present work, the baselines and evaluation protocols seem to be less well established than for the remaining two tasks (the original paper using ResNets for this doesn't use PSNR/SSIM for evaluation). It should also be noted that Zhu et al (2019) formed the ground truth set by asking for human preferences between results of existing algorithms, which makes the interpretation of the score gains less clear than in SISR and denoising where the actual target image is completely known.\n\nSuggestions for improvements:\n* The visual examples in the appendices do a good job of showing the improvements from using PNB. Are there any new failure modes that the baseline networks do not exhibit?\n* For superresolution evaluation, it would be more informative to list (at least in appendices) results for more downsampling factors than 3x (related work seems to examine 2x, 3x, 4x, and 8x). Similarly, for denoising, data for various noise levels would provide more insight into the behavior of the proposed model.\n* In Table 4, and 5, what are the averages computed over?\n* In Zhu et al (2019) which was used a ground truth for the edge-preserving smoothing experiments, each undistorted image is associated with multiple filtered results selected as preferred by human evaluators. How is this information incorporated into the PSNR/SSIM scores in Table 1?"}