{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposed to improve pre-training of language models (e.g. BERT) by incorporating information around entities based on English Wikipedia. The idea is very simple and straightforward: it takes all the anchor links from Wikipedia and replaces some entities by randomly sampling negative ones of the same entity type (according to Wikidata) and adds an extra binary prediction task which predicts if the entity has been replaced or not. \n\nThe model was initialized by BERT (or the authors\u2019 BERT reimplementation) and trained for another 1M steps with the new training objective and reduced % of masking tokens.\n\nThe model was evaluated on a fact completion task (created by the authors on the 10 sampled Wikidata relations) and several open-domain QA datasets and an entity typing dataset FIGER, and achieved significant improvements on the BERT baselines.\n\nOverall, I think this is a strong paper. The idea is simple but effective, the experiments are thorough and improvements over the BERT baselines are significant. \n\nBelow are some concerns I had when I read the paper and also some suggestions on how to improve this paper: \n\n1) I am slightly concerned about the evaluation of the fact completion task and its baselines. \n\n- Why are there only 190-906 candidates for these relations? How were the candidates chosen? Why not use the full set of possible candidates of that entity type?\n\n- I am not sure why you picked the most common entities for predictions. Fact completion for rare entities would be more challenging and practical. Also, the models might favor choosing more common entities as well. \n\n- I am also not sure if the BERT baseline (by using k [MASK] tokens when the candidate answer has k tokens and taking the average of the k probabilities) is a strong one or not in this setting, as BERT was not trained in this way and it is unclear if this would make BERT favor shorter entities or not.\n\n2) OpenQA results (Table 4): there is a very strong baseline coming out recently (an EMNLP\u201919 paper): \n\nMulti-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering. \n\nEven for their BERT-base model, TriviaQA F1 was 67.5, SearchQA F1 was 70.6 and Quasar-T F1 was 59.0. It is okay to not directly compare to their results (the focus is different), but the authors should be aware of their results and perhaps remove the state-of-the-art claim.\n\n3) I\u2019d be interested in seeing more ablation studies on the importance of masking/replacement choices. What is the percentage of entities that have been replaced? 50%? The only thing I can find is that no adjacent entities have been replaced at the same time. How important is that?  I imagine that the percentage of entities that have been replaced should also matter the performance significantly. \n\n4) If I understand correctly, the model was first trained (as BERT) on English Wikipedia + BooksCorpus and then later trained only on English Wikipedia. I wonder how important the first stage would still be. Could add an experiment that trains on Wikipedia only?\n\nMinor suggestions:\n1) Please use \u201cEnglish Wikipedia\u201d instead of \u201cWikipedia\u201d (#BenderRule)\n2) Table 1: don\u2019t put \u201c572\u201d next to \u201cAverage Hits @10\u201d. It is confusing. \n"}