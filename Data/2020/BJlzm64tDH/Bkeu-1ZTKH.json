{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to trained better entity centric text embeddings by switching entities mentioned in the text to some other entities with the same type. The target is modeled as a binary classification task, which is trained jointly with the MLM loss. The authors do experiments on multiple tasks, and the model shows strong performance on all tasks. And the ablation study justifies that \"knowledge pre-training\" is crucial. The idea is novel and the experiment results suggest that the additional \"adversarial\" target helps. The writing is clear in general, but misses some implementation details. \n\nA few questions:\n1. In section 3.1, how do you rank the candidate answers with your model? Do you compute the logits in the same way as the baseline models?\n2. In section 3.2.1, why do you use a different split for TriviaQA? Do you rerun the baseline models on this new split?\n3. In section 3.2.1, the author claims that most answers in TriviaQA, SearchQA and Quasar-T datasets are entities. A interesting metric to evaluate would be how much improvements WKLM obtain on those questions, versus those whose answers are text spans.\n3. In section 3.2.2, the authors mention that they use the CLS token to predict the entity types. How do you train the embedding your CLS token?\n4. For the entity typing task in section 3.2.2, do you fine tune your model or it's evaluated in a zero-shot setting?\n\n"}