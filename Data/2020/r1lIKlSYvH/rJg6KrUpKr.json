{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper is clearly written and well structured. After categorizing difference causes of posterior collapse, the authors present a theoretical analysis of one such cause extending beyond the linear case covered in existing work. The authors then extended further to the deep VAE setting and showed that issues with the VAE may be accounted for by issues in the network architecture itself which would present when training an autoencoder.\n\n\n\nOverall:\n\n1) I felt that Section 3 which introduces categorizations of posterior collapse is a valuable contribution and I expect that these difference forms of posterior collapse are currently under appreciated by the ML community. I am not certain that the categorization is entirely complete but is nonetheless an excellent step in the right direction. One source of confusion for me was the difference between sections (ii) and (v) --- in particular I believe that (ii) and (v) are not mutually exclusive.\n\nAdditionally, the authors wrote \"while category (ii) is undesirable, it can be avoided by learning $\\gamma$\". While this is certainly true in the affine decoder case it is not obvious that this is true in the non-linear case.\n\n2) Section 4 provides a brief overview of existing results in the affine case and introduces a non-linear counter-example showing that local minima may exist which encourage complete posterior collapse.\n\n3) On the proof of Proposition 4.1. In A.2.1 you prove that there exists a VAE whose ELBO grows infinitely (exceeding the local maxima of (7)). While I have been unable to spot errors in the proof, something feels odd here. In particular, the negative ELBO should not be able to exceed the entropy of the data which in this case should be finite. I've been unable to resolve this discrepancy myself and would appreciate comments from the authors (or others). The rest of the proof looks correct to me.\n\n4) I felt that section 5 was significantly weaker than the rest of the paper. This stemmed mostly from the fact that many of the arguments were far less precise and less rigorous than those preceding. I think the presentation of this section could be significantly improved by focusing around Proposition 5.1.\n\na) Section 5 depends on the decoder architecture being weak, though this is not clearly defined formally. I believe this is a sensible restriction which enables analysis beyond the setting of primary concern in Alemi et al. (and other related work).\n\nb) In the third paragraph, you write \"deep AE models can have bad local solutions with high reconstruction [...]\". I feel that this doesn't align well with the discussion in this section. In particular, I believe it would be more accurate to say that IF the autoencoder has bad local minima then the VAE is also likely to have category (v) posterior collapse.\n\nc) Equation (8) feels a little too imprecise. Perhaps this could be formalized through a bias-variance decomposition of the right hand side similar to Rolinek et al.?\n\nd) The discussion of optimization trajectories was particularly difficult to follow. It is inherently difficult to reason about the optimization trajectories of deep auto-encoding models and is potentially dangerous to do so. For example, perhaps the KL divergence term encourages a smoother loss landscape and encourages the VAE to avoid the local stationary points that the auto-encoder falls victim to.\n\ne) It is written, \"it becomes clear that the potential for category (v) posterior collapse arises when $\\epsilon$ is large\". This is not clear to me and in fact the analysis seems more indicative of collapse presented in category (ii) (though as mentioned above, I am not convinced these are entirely separate). Similarly, later in this section it is written, \"this is more-or-less tantamount to category (v) posterior collapse\". I was also unable to follow this reasoning.\n\nf) \"it is actually the AE base architecture that is effectively the guilty party when it comes to posterior collapse\". If the conclusions are to be believed, this only applies to category (v) collapse.\n\ng) Unfortunately, I did not buy the arguments surrounding KL annealing at the end of section 5. In particular, KL warm start will change the optimization trajectory of the VAE. It is possible that the VAE has a significantly worse loss landscape than the autoencoder initially and so warm-start may enable the VAE to escape this difficult initial region.\n\nMinor:\n\n- The term \"VAE energy\" used throughout is not typical within the literature and seems less explicit than the ELBO (e.g. it overlaps with energy based models).\n- Equation (4) is missing a factor of (1/2).\n- Section 3, in (ii), typo: \"assumI adding $\\gamma$ is fixed\", and \"like-likelihood\". In (v), typo: \"The previous fifth categories\"\n- Section 4, end of para 3, citep used instead of citet for Lucas et al.\n- Section 4, eqn 6 is missing a factor of 1/2 and a log(2pi) term.\n- Section 5, \"AE model formed by concatenating\" I believe this should be \"by composing\".\n- Section 5, eqn 10, the without $\\gamma$ notation is confusing and looks as though the argmin does not depend on gamma. Presumably, it would make more sense to consider $\\gamma^*$ as a function of $\\theta$ and $\\phi$.\n- Section 5 \"this is exactly analogous\". I do not think this is _exactly_ analogous and would recommend removing this word."}