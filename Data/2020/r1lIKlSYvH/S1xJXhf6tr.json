{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary\nThe paper theoretically investigates the role of \u201clocal optima\u201d of the variational objective in ignoring latent variables (leading to posterior collapse) in variational autoencoders. The paper first discusses various potential causes for posterior collapse before diving deeper into a particular cause: local optima. The paper considers a class of near-affine decoders and characterise the relationship between the variance (gamma) in the likelihood and local optima. The paper then extends this discussion for deeper architecture and vanilla autoencoders and illustrate how this can arise when the reconstruction cost is high. The paper considers several experiments to illustrate this issue.\n\n2. Opinion and rationales\nI thank the authors for a good discussion paper on this important topic. However, at this stage, I\u2019m leaning toward \u201cweak reject\u201d, due to the reasons below. That said, I\u2019m willing to read the authors\u2019 clarification and read the paper again during the rebuttal to correct my misunderstandings if there is any. The points below are all related.\n\na. I would like to understand the use of \u201clocal optima\u201d here. I think the paper specifically investigate local optima of the likelihood noise variance, and there are potentially other local optima. Wouldn\u2019t this be an issue with hyperparameter optimisation in general? For example, for any regression tasks, high observation noise can be used to explain the data and all other modelling components can thus be ignored, so people have to initialise this to small values or constrain it during optimisation.\n\nb. I think there is one paper that the paper should discuss: Two problems with variational expectation maximisation for time-series models by Turner and Sahani. In this paper, the paper considers optimising the variational objective wrt noise likelihood hyperparameters and illustrates the \u201cbias\u201d issue of the bound towards high observation noise.\n \nc. I think it would be good to think about the intuition of this as well: \u201cunavoidably high reconstruction errors, this implicitly constrains the corresponding VAE model to have a large optimal gamma value\u201d: isn\u2019t this intuitive to improve the likelihood of the hyperparameter gamma given the data?\n\nd. If all above are sensible and correct, I would like to understand the difference between this class of local minima and that of (ii). Aren\u2019t they the same?\n\ne. The experiments consider training AEs/VAEs with increasingly complex decoders/encoders and suggest there is a strong relationship between the reconstruction errors in AEs and VAEs, and this and posterior collapse. But are these related to the minima in the decoder\u2019s/encoder\u2019s parameter spaces and not the hyperparameter space? So that is the message that the paper is trying to convey here?\n\n3. Minor:\n\nSec 3\n(ii) assumI -> assuming\n(v) fifth -> four, forth -> fifth\n"}