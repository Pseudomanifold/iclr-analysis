{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper provides a data-independent way for pruning neutrons in deep neural networks with a provable trade-off between its compression rate and the approximation error. The output of a layer of neurons is approximated by a corset of neurons in its preceding layer. \n\nThe pruning of neurons based on the coresets is shown to be effective when compared with other methods. The authors have validated it on two convolutional network architectures.  \n\nThe paper starts from defining the coresets and introducing the VC dimension, and extends the theorem to more generalized cases. \n\nThe coreset seems to require the activation function to be non-negative, which will possibly limit the scope of application of the proposed theory. "}