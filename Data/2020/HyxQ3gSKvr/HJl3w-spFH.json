{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the autoencoder model combining the usual information bottleneck and the Gaussian mixture model (GMM). Using an approximation to deal with GMMs, the authors derive a bound on the cost function generalizing the ELBO. The performance of the proposed method is tested on three benchmark datasets and compared with existing methods combining VAE with GMM.\n\nWhile the framework and the performance of the proposed method are interesting and promising, some of its main parts are unclearly explained.\n\n- Although Remark 1 well explains the difference between VaDE and the proposed objective functions, little is discussed how this difference affects the learnt model.\n- I wonder why Q_\\phi(x|u) in Eq. (11) doesn\u2019t have to be a distribution. What does the expression [\\hat{x}] mean?\n- Eq. (17) is not explained clearly. Since the equality symbol is used, it is unclear where is approximation. Although this approximation is one of the main parts of the proposed method, little is discussed on the influence of this approximation.\n- Are the information plane and latent representations in Figs 5 and 6 also available for DEC and VaDE and not limited to the proposed method?\n\nMinor comments:\np.5, the math expression between Eqs. (16) and (17): The distribution q(x_i|u_i,m) is undefined.\np.6, l.4 from the bottom: ACC is defined later.\np.7, l.14: Should J be n_u?\n"}