{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes to make permutation-equivariant set autoencoders, by introducing a permutation invariant encoder (based on feature-wise sorting) and at the same time an 'inverse' operation (that undoes the sorting) in the decoder. The achieved equivariance allows the autoencoders to be trained end-2-end without a matching-based loss (like Chamfer) which I think is a very neat result. Also, to address potentially variable-sized sets as inputs/outputs, the authors propose to use a calibrator function that effectively 'samples' the produced features on the set's elements in fixed intervals, that are auto-adaptable in the (variable) set size. \nThe results shown are well placed in the current literature and form an important novelty that should have non-trivial impact in the sub-field. Also, the experiments done are well executed, with ample variability in the nature of the datasets used and I expect them to also be easily reproducible (given the authors' provided code).\n\nSome points I would appreciate to see an improvement:\na. The intro is rough (please see minor-specifics at places that you can improve the exposition).\nb. The Figure1 is not easy to read. The 90degree rotation results in the same set, but as a 'pictorial' image, obviously not.\nc. The \"responsibility problem\" is already very well explained in the Zhang19a. I would appreciate to tone-down in this paper, the \"discovery\" of it as a main contribution.\nd. Experiments 6.2. it appears that the FSPool/unpool model is better *only* when the mask features are been considered. What are these mask-features? Why do they matter?\ne. The way you describe the responsibility problem (discontinuity) is very hand-wavy. It would be nice to explicitly it write it in rigorous math.\nf. Why using the relaxation of Grover et al. helped you to avoid the discontinuity that would be otherwise introduced via standard sorting? (I am not familiar with their exact relaxation, but intuitively, their method been a good proxy for sorting, should suffer from it as well). \n\n\nExplicit Minor Comments on writing:\n(all in introduction)\n-4rth line: \"this\" -> this problem\n-\"Methods like by\" -> Methods like those in \n-\"In this paper, we introduce a set pooling method for neural networks that addresses both issues\" -> which issues? the encoder's collapse and the decoder's inneficiency in matching? Please explain.\n-\"good baselines\" -> sophisticated/non-trivial baselines\n\nAppendix. Table 9, max-pool at \\sigma=0, seems to be the best (please use boldface to indicate it).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}