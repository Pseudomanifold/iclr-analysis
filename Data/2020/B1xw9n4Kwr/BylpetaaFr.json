{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the training dynamics of a neural network model as a dynamical system.  The authors proposed a path-based approach to compute the derivatives that would appear in the H matrix which governs the learning dynamics.  They further utilized this formulation to (1) simplify the analysis of convergence rate of 2-layer neural networks w.r.t. width; and (2) presented an argument for the similarity between added depth in the network and momentum-based optimization; and (3) also argued about the importance of the number of paths for fast convergence.\n\nThe dynamical systems view of the learning process is quite new to me, even though I\u2019m aware of a few recent papers exploring this.  I found the paper to be mostly clear and not too hard to follow, and the dynamical systems perspective interesting.  The path gradient is quite intuitive, but I\u2019m a bit surprised there\u2019s no prior work (at least not discussed in this paper) studying the relationship between gradients and the paths in the network.\n\nIt is a bit hard for me to judge the significance of this work because of my lack of context.  The main implications of the path gradients were presented in section 5.  The first part shows that using their theorem 4.1 can simplify the derivation of the linear relationship between convergence rate and the width of 2-layer nets.  This is a simplification but the original derivation is not complicated either.  The second part tried to draw a relationship between added depth in a network and momentum-based optimization, which I found to be a bit hand-wavy.  In particular, I found the argument that, du/dt prefers the direction of u itself which carries information about past directions therefore this is similar to momentum, to be not convincing.  The third implication argues that we need H to be full-rank for fast convergence, and in order to achieve this we want to increase the number of paths in the network, which is interesting.\n\nThere are a few other things that could be clarified:\n- the terminology in section 4.3 is a bit confusing, w_s seems to be associated with an edge in the graph, but is also referred to as a node, \u201cwe denote the activation value at node w_s with activation(w_s, X_i) \u201d.\n- on page 6 there is a reference to Equation 5.2 which does not exist\n- the results in Figure 4a is counter-intuitive - is this showing wider networks actually converge slower?  Isn\u2019t this against the argument of this paper?\n- it is unclear how the convergence rate lambda values are computed in Figure 4b, the curves in Figure 4a clearly doesn\u2019t follow an exponential decay pattern.\n\nOverall I found this paper presented some interesting ideas, but may need a bit more work to be ready to be published.  Happy to change my judgement however, if other more experienced reviewers can comment better on the significance of this work."}