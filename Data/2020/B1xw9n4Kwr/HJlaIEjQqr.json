{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of understanding the impact of deep neural networks (DNN) model architecture on the convergence rate of gradient descent dynamics. To achieve this goal, the paper follows the recent trend of continuous-time perspective of optimization, and proposes to model gradient descent via the gradient flow, which is a first-order ODE. The induced loss dynamics is then also following a first-order ODE with a coefficient matrix H (that depends on the solution trajectory, and hence non-constant). The paper then claims that the convergence rate is characterized by the minimum eigenvalue of H, and analyzes this H through a straightforward path-based formula obtained by chain rules. In particular, the authors try to explain the effect of width, depth and number of paths on the convergence rate, and validated these through a few numerical experiments.\n\nAdmittedly, the idea of this paper is interesting. However, I think the novelty and rigorousness of this paper is not convincing, as explained in more details below.\n\n1. On the novelty side, characterizing the convergence via the H matrix is not new, and most of the discussions in Section 4 have appeared in exactly the same form in the previous works [13] (two-layer) and [25] (general), which are also cited in this paper. In addition, the path formula is also very closely related (if not completely the same as) with the expansion of G matrix in Section 4 of [25] (where G is the H in this paper), which decomposes the G (or H) matrix into summation over layers. These facts largely lower the contribution of this paper on a high level. \n\n2. On the rigorousness side, the paper is not very consistent in the notation. \n1) The notation is not very consistent. The authors use H in the notation section to denote the matrix, use X_i to denote the data input, but use \\bf{H} and x_i (lower cased) subsequently. \n2) In Section 4, the authors immediately start with the continuous-time perspective, without even mentioning the gradient dynamics or some related ansatz. The authors may want to mention that they use the ansatz w_k=W(kh), where W is a smooth curve, and take h to 0 to obtain the ODE models, as is done in [1].\n3) The notation list at the beginning of Section 4.3 is too long and not clear. In particular, l(p) is not even defined before appearing in (14), and \\sigma_s seems to be overridden by the notation activation(w_s, X_i) and does not appear later in the path gradient, which is weird. Shouldn't there be \\sigma_s inside the formula of (14)?\n\n3. Again on the rigorousness side, the paper is very non-rigorous when stating the claims and theorems.\n1) The authors claim that \"This result will hold for other l_p losses\", but indeed what holds is different for l_p losses (the rate is scaled by p/2).\n2) Section 4.2 does not make any sense. The authors should either directly cite the corresponding content in [25], which are much clearer, or directly invoke the standard linear ODE theory and use the matrix exponential and Taylor expansion to make the explanations.\n3) The paper seems to use a very informal argument that H stays close to its initial value to derive all the theory, which is only empirically checked in the appendix. But given the proportion of the theory part of this paper, I think the paper should either clearly state the assumption as H being constant, or follow the manner of [25] to prove that H stays close to some fixed matrix and use this to prove the other theorems rigorously. Otherwise, the theory part is both hard to understand and verify. \n4) Proposition 5.1 should clearly state whether the statement is in the expectation sense, or high probability sense, or something else. \n5) The explanation in Section 5.2 is rather unclear. In particular, I don't understand why \"eigenvalues of H(g) are pushed in the direction of g\" implies that \"the updates prefer the direction of u\", and why this then further implies something related to the momentum acceleration. The authors should provide a rigorous statement here.\n6) The authors claim at the bottom of page 1 that they show adding a new layer leads to H(t) being decomposed into an adaptive learning rate term and a momentum term. But the adaptive learning rate part is not showing anywhere later in the paper. \n\nMinor comments:\n1) In equation (5), there should be a minus sign on the right-hand side.\n2) In Section 5.2, there is no \"Equation 5.2\". It should be something else.\n3) The authors may want to add citations to [2] (which is a concurrent work with [25] on essentially the same topic) and [3] (which is a predecessor work of neural ODE).\n\n[1] Su, Weijie, Stephen Boyd, and Emmanuel Candes. \"A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights.\" Advances in Neural Information Processing Systems. 2014.\n[2] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" arXiv preprint arXiv:1811.03962 (2018).\n[3] Lu, Yiping, et al. \"Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations.\" arXiv preprint arXiv:1710.10121 (2017)."}