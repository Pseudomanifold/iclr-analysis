{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a simple and intuitive interpretation of the dynamics of gradient descent between labels and predictions rewritten in terms of all possible paths from inputs to outputs in FC networks. The basic starting point is from the gradient flow (gradient descent in continuous time) by Du et al 2018 [13] on the difference y-u of labels y predictions u (eq (6) or (7) in the paper) instead of the standard gradient flow dW/dt = - \u2202L(W)/\u2202w. The coefficient matrix H of the system is known to determine the convergence properties, and this can be rewritten with respect to path-wise sums of gradients through the chain rule (Theorem 4.1).  This provides some intuitive interpretations for several facts discussed in the community: 1) the linear convergence rate is more intuitively obtained compared to the naive derivations (Remark 5.2 with comparison to [13]). 2) the 'depth' of FC network affects the convergence like momentum (section 5.2). 3) the number of paths compared to the number of nodes has a predominant impact on the convergence (section 5.3). The paper also demonstrates several experiments to understand the impact of depth or paths on the convergence.\n\nThough the paper's contribution is a quite simple path decomposition using chain rules for the coefficient matrix H of the gradient flow, it indeed provides several intuitive understandings on the impact of paths onto the gradient-descent convergence. All implications are basically confirming already known things in different (path-based) words, and the impact or novelty is rather small, but nevertheless, it would be informative. \n\nOne concern is about the description in section 4.1 and 4.2 for preliminaries results. I would suggest that emphasizing the fact 4.1 as an already discussed fact makes easy for readers to follow the results and focus more on the paper's contribution. The cited paper by Du et al [13] was published at the last year's ICLR, and the contents in 4.1 and 4.2 would be due to it including the formulation as gradient flow (gradient descent in continuous time). Given that it is already known that H is the determining factor for the convergence, then the contribution and usefulness of the proposed new representation of eq (15) should be discussed. At the first glance, the use of gradient flow w.r.t prediction u instead of parameter w might be (misleadingly) seen as the idea of this paper. The sentences in the abstract can also mislead readers to this. Also check the paper by Du et al in ICML 2019 'Gradient descent finds global minimum of deep neural networks'. \n\nTwo minor points: \n1) parentheses for citing references and eq numbers are quite confusing. Please check the style format for citations.\n2) in Figure 2, pre(w_0) is written as \"presum(w_0)\".\n"}