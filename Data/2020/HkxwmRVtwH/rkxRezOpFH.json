{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an improvement over Probabilistic meta-representations of neural networks by replacing the NN parameterization of the network weights given latent variables by a probabilistic distribution whose mean is distributed by GP. Inference of the induced hierarchical model is achieved by variational inference and various\napproximations when needed.\n\nComments:\n\n1. The authors claim that the proposed method aims to increase the robustness in the small data settings\nand improve its out-of-sample uncertainty estimates. The second part is well justified. \nCould the author elaborate further on the high level intuition behind the first part?\n\n2. What exactly is the gain obtained by replacing a NN parameterization with a GP parameterization?\nIt seems like the proposed method gains the ability to model uncertainty, but potentially incurs performance trade-off \nfrom a series of approximation. \n\n3. Following from comment #2, I am a bit surprised by the lack of comparison against the work of \nKaraletsos although this work was built on top of it. I am interested to understand the mentioned\ntrade-off in practice.\n\n4. Regarding the practical consideration in Eq.(11), I think it kind of defeats the purpose of setting up\nthe latent variables so that \"weights in a layer and across layers are explicitly correlated in the \nmodelling stage\" (Section 2.2). Do all experiments presented in the paper employ this practical \nconsideration?\n\n5. For the text before Eq. (8) should the inducing inputs be xu instead of zu? It seems like a systematic\ntypo here because in the formulation for the lower bound it becomes p(u|xu) again instead of p(u|zu)\n\n6. Is Kuu in Eq.(9) computed by taking Kronecker product of K_in, K_out and K(xu,xu) or just K(xu,xu).\nI am under the impression that it is the former, in which case taking the inversion is costly (cubic in the number\nof latent variables). This would not permit large NN anyway, which kind of defeats the purpose. It explains the choice \nof small architecture in your experiments as well. \n\nOverall comment:\n\nI think the paper presents an interesting idea but I have questions regarding its practical significance as highlighted in my specific comments above. I hope the authors would clarify these so I can converge on a final rating.\n"}