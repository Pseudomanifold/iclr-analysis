{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents two models extended from a meta-presentation of neural networks (Karaletsos et al. 2018) in which neural network weights are constructed hierarchically from latent variables associated with each layer. The mappings from latent space to weight space are used Gaussian Processes instead of neural networks.\n\nThe proposed models include (1) MetaGP which directly replaces neural network assumption by Gaussian process prior and (2) MetaGP with contextual information which further takes into account input information via performing the multiplication between the kernel function over latent space and kernel function over input space.\n\nVariational inference is followed by the pseudo-inducing point approach.\n\nExperiments are conducted in both toy data sets and benchmark data sets, demonstrating several points i.e. uncertainty quantification, inductive bias. \n\nPros:\nThe paper is clearly written.\nIntroducing inductive bias or functional regularization for neural network\nInteresting capability of measuring the uncertainty of out-of-sample data. This can be one of the reasons that the method performed well in active learning.\n\nCons:\nThe approach is incremental or not-so-novel in terms of meta-representation for neural networks.\n\nComments and questions:\nThe prior distribution for latent variable $z$ is not specified. I assume the prior is independent Gaussian.\n$z$ is unknown beforehand. How are inducing locations for latent variable $z$ initialized?\n Do you think that there is a connection between contextual metaGP and residual nets? Can skip connections from the input layer to certain layers be considered to be similar to the idea of incorporate input kernel in the paper?\nCan you comment on the convergence of the estimation of the last term in the variational bound? The MCMC takes two stages of stochasticity: (1) sample $z_k$ and $V_k$ and (2) then estimate $F_k$ using reparameterization. This can make the convergence slow (https://arxiv.org/abs/1709.06181). \nMinor: a missing period in Sec. 6.3 \u201cquadratic kernel In this example\u201d"}