{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper proposes to solve question answering (QA) tasks using a two step procedure. 1) extract sequences of sentences with the relevant facts. 2) Use these as inputs to a \"standard\" model, e.g. BERT.\nThe sentence sequences are extracted using a pointer network over the entire document collections and beam search. \nThe sentence extractor network is trained using heuristically generated \"gold\" sentence sequences. \nThe heuristic uses Named Entity Resolution (NER) and coreference resolution to create a graph of sentences by linking sentences which contain the same named entity. Additionally sentences are linked to the sentence following it in the same paragraph. Given the graph the heuristic enumerates all paths from the question (also part of the graph using NER+coreference resolution) to the answer.\nThe \"best\" path is picked using a scoring function.\nGiven a trained sentence sequence extractor the \"standard\" QA model is trained on the extracted sentence sequences to output the answer.\n\nThe paper reports state-of-the-art (SOTA) results on WikiQA and \"strong\" results on HotpotQA.\n\nI've chosen to not recommend this paper at ICLR, but I'm not very confident in this recommendation. On one hand, the model is bit a kludge and there's not a lot of truly novel ideas presented. On the other hand SOTA results are always worth at least understanding. Some of my uncertainty is because I'm not familiar enough with these two datasets to judge how significant the improvements are.\n\nThe results presented in table 2 show the proposed method having the best score on HotpotQA, but in the body of the text SOTA is not claimed. If this is indeed not SOTA, then please include SOTA in the table such that the reader can understand how well your model performs.\n\nWhy train the sentence extractor in the first place as opposed to using the heuristic directly? If I understand correctly the heuristic could be run at test time, since it doesn't require any labels or anything only present in the training set. Does the authors think that the trained sentence extractor can become \"better\" than the training data? \n\nIs the first step of extracting sentences a good idea in its own right or is it mostly a computational concern? Would the authors still recommend it if they had ~infinite computational resources, e.g. if they could fit BERT on the entire document corpus in memory? If yes, why? What information or bias does this step add or encode?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}