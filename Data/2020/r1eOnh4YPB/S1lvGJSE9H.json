{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper investigates the way decaying the learning rate helps the training of neural networks. First the paper discusses about other existing hypothesis such as the \u201cGradient Descent Hypothesis\u201d by Lecun et al 1991 and SGD explanation by Kleinberg et al 2018. Then the paper tries to find contradicting examples against those two hypothesis with experiments. Then they propose their explanation which suggests that initially fitting noisy data and then decaying it helps it to learn more complex data. Then the paper tries to experimentally explain why the other explanations fail and theirs is better.\n\nThis paper is very badly written. The authors should definitely rethink about the organization of the paper. The first pages is mostly about the background material with figures taken from other papers. The terminology that is being used in this paper is very vague. They used the term complex patterns in the paper, but don\u2019t even explain it until page 6. When the paper explains it, still the notion itself how to compute it in a tractable way is a bit vague for neural networks.\n\nThis paper proposes all those different explanations of how SGD works just as if they are completely orthogonal. However, for example learning both the proposed explanation in this paper and the fact that learning rate decay improves stability can both be true. \n\nThe experimental arguments are quite vague in this paper. The authors should give more details about their experimental setup, for example what is the starting learning rate, ending learning rate, details of scheduling, what type of distributed training method (sync or async SGD?) and etc\u2026 is missing. Without those details it is very difficult to interpret the experimental conclusions in for example section 4.1. In Figure 5, what does the converge interval mean? In Figure 7 and 4 all the curves are so similar, why so?\nThe arguments that learning rate decaying helps models to learn complex patterns and without decay the models can not learn those is a very strong claim and not very well supported in this paper.\n\nThe details of how the PS10 dataset is constructed is somewhat vague. In terms of arguments that decaying learning rates help the neural networks to learn simpler examples first and the harder ones have already been done in other papers such as Li et al 2019 which is also cited in this paper. It is not completely clear what this paper contributes over those other existing papers :)\n\n\nQuestions:\nHow does the cyclic learning rates come into the picture. Can one explain why cyclic learning rates works with the hypothesis that this paper processes?\nHow would different types of annealing methods come into the picture: e.g. linearly decaying the learning rate, exponential decay and etc\u2026\nWhy are there two columns of \u201csupported\u201d in Table 1? What is the difference between them.\nHow does the adaptive learning rate algorithms such as Adam come into the picture with the learning rate decay hypothesis?"}