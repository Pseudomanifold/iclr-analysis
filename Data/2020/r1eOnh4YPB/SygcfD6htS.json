{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions:\nThis paper investigates the use of learning rate decay in deep neural networks.  The main contribution is an empirical analysis trying to understand lr decay. Authors claim that the high-learning rate phase provides regularization and prevents the network to fit/memorize noisy data initially to focus on simple patterns. \n\nAuthors design a set of experiments showing that lr decay allows SGD to first fit \u2018simple patterns\u2019 instead of more complex/noisy one. They also show that the model transferability decreases through training.\n\nIn addition to those experiments, authors provide experimental results which aims at contradicting common beliefs on lr decay.\n\nNovelty/Significance:\nMy main concern is about the novelty/significance of the paper.  Similar argument has already been made in (Nakiran et al., 2019) or (Li et al. 2019), which are cited in the paper, and other works such as \u201cOn the Spectral Bias of Neural Networks\u201d. In addition, prior to this work, the paper \u201cThree factors Influencing Minima in SGD\u201d empirically showed that the noise in SGD (which is controlled by the learning rate) prevents memorization (see Figure 6 in their paper). Although the latter work did not focus the lr decay, it is not clear to me how the main argument of the current paper differs from it. \n\nAdditional comments:\n-\tIt is unclear to me why the model the model before and after decay should have the same training performance in section 4.2. This would assume a specific geometry of the loss function (i.e. that the loss surface is not significantly narrower so that you can reach it with a high-learning rate), but authors do not provide evidence of it.\n-\tIt would be nice to provide more details about the experimental settings. What is the base learning rate? Did you try different learning rate? Do you use with SGD-momentum, weight-decay?\n-\tHow to combine the current explanation of lr decay with the observation that learning rate warm-up has been a successful way to train neural network in the large batch setting?\n-\tThe transferability experiment confounds two factors: the number of iteration steps   and the learning rates. Does training a model with low learning rate, but with a number a step equal to high learning rate lead to lower transferability?\n-\tIt would be nice to validate the hypothesis on more datasets/models.\n\n"}