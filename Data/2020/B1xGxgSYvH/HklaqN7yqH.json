{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This submission provides a new theoretical framework for domain adaptation. In order to tackle the adaptability term in the classical domain adaptation theory, this submission proposes a new upper bound that enlarge the hypothesis space in the adaptability term. A weighted version of this theory is also given. Authors further support their conclusion by empirical results.\n\nPros:\n1. This submission studies an important problem in domain adaptation.\n2. This submission proposes new theoretical insight about compression and adaptability.\n3. The conclusions of this paper can be partially proved by the empirical results.\n\nCons:\n1.\tAs the author says in their future work, the source constraint is too strong that need to control the feature unchanged across all source domain. For this condition is not build on samples but on the support of source domain. It seems that authors use $L_0$ to constrain \\phi\u2019 to have same value with \\phi on source dataset, which may be only a small part with zero measure of source support set. \n2.\tThere is no generalization error analysis for these upper bounds. This submission provides weighted version of the main theory in the section 5. It seems that weighted version of upper bound could be further minimized by find a good weight. But add weight will add variance in the complexity term [A].\n3.\tThis submission adds \\beta term to change the adaptability term of $\\tilde{\\mathcal{H}}$ into the adaptability term of $\\mathcal{H}_0$. The reason why \\beta can be estimated from finite sample is not clarified, which is the premise of being trainable and should be mainly discussed in this paper. We can see that to estimate \\beta is a domain adaptation problem under the fact that the labeled functions are same. \\beta is a term that can\u2019t be computed from small finite samples if there is no more assumption: It is not easy to approximate $\\tilde{f}_s$ uniformly, otherwise the estimation of \\beta will suffer from distribution shift. This submission claimed that the term can be trainable by giving Proposition 2, a proof of the consistency. However, this proposition is built on the assumption that there exists a series of \\phi minimizing the distribution distance to zero. But this is impossible for finite sample estimation, when there will always be generalization error of estimating the distribution distance. Furthermore, it is usually impossible to make the two embedded distributions completely the same in empirical. In addition, if there is a series of \\phi, how to control other terms in the upper bound? Every \\phi will induce new $\\tilde{\\mathcal{H}}$ and $\\mathcal{H}_0$ which will change all other terms. In summary, the main theory in this submission changes the unknown adaptability to a new term that is very hard to estimate. And there is no sufficient empirical or theoretical evidence in this paper that could support the fact that \\beta is small. The contribution is therefore limited.\n4.\tThe theory also fails to give upper bounds on compression term and adaptability term, or some explicit upper bounds for certain hypothesis spaces as examples. Readers could not have a clear image of how large will these terms be. Furthermore, if the support sets of source and target domain coincide a lot, the adaptability of $\\mathcal{H}^s$ will not be too smaller than the previous one.\n5.\tThe organization of the submission makes it hard to read:\na)\tThe symbol of this submission is chaotic. For example, $\\tilde{\\mathcal{H}}$, $\\tilde{\\mathcal{H}}^s$ ,$\\tilde{\\mathcal{H}}_h$ are defined based on $\\phi$, $\\pi(h)$ is defined based on $\\tilde{\\mathcal{H}}$, but all these facts are not revealed in their symbols. \nb)\tFor clarity, all loss functions defined in Section 4 should be stated in a independent line. The Section 5 should be moved to the front of Experiment part.\nc)\tI recommend the authors to restate all new defined symbols as a list on the top of appendix. It really troubles me during checking the proof.\n\nI think this submission discusses about an important problem and provides new insight, but it is not a thorough theoretical work because of above reasons. So, I vote for rejecting this paper.\n\n[A] Cortes, Corinna, Yishay Mansour, and Mehryar Mohri. \"Learning bounds for importance weighting.\" Advances in neural information processing systems. 2010.\n"}