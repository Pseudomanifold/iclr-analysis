{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper takes a model-based approach to \"imitation learning\". It first learns a (flow) \"model\" to assign likelihoods to trajectories being expert-like as well as sample expert-like trajectories. This is then combined with an estimate for trajectories being certain goal conditioned, where the goals come from a route planner. Results are shown in the context of autonomous driving in the CARLA simulator with a PID controller tracking the open-loop plan from the planner.\n\nOne major issue with the paper is that all the main contribution seem to be in the appendix. If I were to summarize it, the paper introduces an interesting way of integrating two different experts to perform learning from demonstration. On the one hand, you have the A* algorithm as an expert providing what waypoints to follow. On the other hand, you have the expert demonstrating how to drive around on the road. The question becomes, how do we reconcile these \"experts\" acting at different abstraction levels so that our system works in a more general setting. This kind of a setting is definitely not as general as the paper tries to make it out to be, but nevertheless it's reasonably broad and useful.\nThe idea to of different subsystems for route planning and path planning are not new, but the way it's done here does seem interesting to me.\n\nNote that Sec 3 related work CILS description seems hard to understand. Moreover, the claim \"MBRL can also plan, but with a one-step predictive model of possible dynamics\", seems incorrect. One can do multi-step predictions and use those to do model-based RL as well. So it's unfair to claim that this other approach is MBRL and yours is not on that basis. \n\nExperiments are interesting although quite dense. It's unclear what the initial state distribution look like for these experiments though. I'm guessing CARLA is otherwise a deterministic simulator?\n\nNow I am going to do something that you are not supposed to do as a reviewer. Tell the authors the paper they should have written, rather than the paper that was submitted. Apologies for that.\nThe terminology of Inverse RL, Imitation Learning and Apprenticeship Learning is a mess in the literature unfortunately and can't blame you not trying to fix it in this paper. However the community would benefit if don't overload these terms. Although all of these fall under learning from demonstrations, it's useful to restrict Inverse RL as trying to figure out the reward function being optimized by an expert, imitation learning as learning the exact behavior of an expert (and therefore not doing better than the expert which are by definition optimal or being applicable to doing something other than the expert) while apprenticeship learning as learning from demonstrations to perform even better at the task even generalizing to other things than the demonstrations. With these definitions under the belt the work in this paper is better positioned as about apprenticeship learning. Learning a model from the expert demonstration about how the world works and then using the same for accomplishing different tasks for which there are _no demonstrations_ doesn't sound like imitation?"}