{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper propose imitative models that learns goal-based probabilistic models of expert demonstrations, and use this to perform test-time planning and control of certain goal-directed behavior. The paper demonstrates extensive and impressive experiments on the CARLA simulator and outperforms existing approaches in the success metric.\n\nThe idea is quite simple: given a set of states, learn a probabilistic model that assigns high probability likelihood to expert behavior. After training, inference is performed to optimize the likelihood of a goal according to an expert prior. The paper discusses extensively how goal-based likelihood functions would be designed for autonomous driving, and the architectures for good q(s|\\phi) in detail, which are of engineering importance in self-driving applications. While I believe the method described might not be significantly novel technically, I believe the paper made nice contributions in terms of the autonomous driving application.\n\nMinor comments:\n\t- It seems like the term \"state\" is used to represent the agent's location on the ground plane, which is also not technically the entire state information?\n\t- Is the argmax in (1) a strict equality? I guess you would assume q(s|\\phi) = p(s|\\phi) for this to be always true?\n\t- How many iterations does it take to converge in Algorithm 2? It seems you would need to updates z{1:T} for all newly encountered goals? \n"}