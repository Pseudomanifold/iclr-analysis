{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n- key problem: expert-like probabilistic online motion planning to reach arbitrary goals without reward shaping thanks to off-line learning from expert demonstrations;\n- contributions: 1) an imitative planning procedure via gradient-based log-likelihood maximization leveraging \"imitative models\" q(future states | features), 2) multiple proposals to define flexible goals in this probabilistic framework, 3) a complete implementation for end-to-end navigation in CARLA, 4) an extensive experimental evaluation showcasing the performance, flexibility, interpretability, and robustness of the proposed approach w.r.t. the previous state of the art and several Imitation Learning (IL) and Model-Based Reinforcement Learning (MBRL) baselines.\n\nRecommendation: weak accept (leaning towards strong accept)\n\nKey reason 1: principled probabilistic framework bringing the best of both IL and MBRL worlds.\n- this planning as inference method is very succinctly and elegantly described in the paper with enough details in appendix (+ code) to suggest a high chance of reproducibility;\n- the flexibility of defining different interpretable goals (6 different types explored in the paper) highlights the versatility of the approach;\n- the additional benefits in terms of plan reliability estimation (Appendix E) are significant;\n- the paper showcases how powerful and useful a good \"imitative model\" can be, therefore, reinforcing the interest of the research community in the important topic of off-line learning from large datasets of demonstrations (without requiring costly on-line data collection).\n\nKey reason 2: thorough experimental evaluation with convincing results.\n- the experimental protocol used is the standard one on CARLA and the results are state of the art;\n- the comparison with related works, including recent ones, is thorough and well explained;\n- the additional claims regarding robustness are substantiated by multiple experiments.\n\nSuggested improvements:\n- Not needing reward engineering is a major claim of this approach, but it seems that constructing goal likelihoods could be seen as a form of reward engineering, no? Table 3 indeed reports significant performance differences (absolute and relative) depending on how the goals are specified, especially in dynamic environments. As the experiments aim at maximizing the same performance metrics, is there a preferred goal type that works well across all experiments? If not, how is \"goal definition\" different than \"reward engineering\"?\n- Could the authors please include a variance analysis (using different seeds) in Tables 3 and 4? Previous papers have reported high variance in similar settings (cf., Codevilla et al 2019), and this is a common issue in IL/RL.\n- How important is knowing \\lambda (traffic light state) perfectly in practice? Can the robustness to noise in \\lambda be experimentally assessed? I would also clarify in section 4 and Table 2 that other methods do not use \\lambda (the traffic light state), which is a signal very strongly correlated with the \"ran red light\" metric.\n- More generally, what is the robustness of this approach to uncertainty / noise in \\phi? Although it is typically available (as the authors mentioned) it is never perfect in practice. Can this be handled in a principled probabilistic way as an extension of the current formulation?\n- The current model does not factor the influence of the agent on its environment (\\phi := \\phi_{t=0}). Is this framework limited to open loop planning, or does this open interesting future research directions towards closing the loop? It seems to be a key open problem to at least discuss in Section 5.\n\nAdditional Feedback:\n- Figure 5 is confusing, not sure it adds much value to the paper;\n- typos in Appendix (\"pesudocode\", \"baselines that predicts\", \"search search\")."}