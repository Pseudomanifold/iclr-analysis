{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "PAPER SUMMARY: The paper proposes a new and efficient implementation of dropout in which multiple dropout samples are obtained from a single input during training. The authors claim that this enhanced dropout technique (i) accelerates training and (ii) improves generalization by achieving lower error rates than standard dropout in training and validation sets. Experiments in image classification tasks for 4 different datasets (CIFAR-10, CIFAR-100, IMAGENET and SVHN) are presented and the effect of the number of dropout samples and dropout ratio are investigated.\n\nREVIEW SUMMARY: Despite the clear computational advantages of the proposed implementation, this paper should be rejected because  (1) the authors claim multi-sample dropout to be a \u201cnew regularization technique\u201d, but the conceptual differences between standard dropout and their proposed method are not clearly stated, (2) one of the main claims regarding \u201cbetter generalization\u201d is not well supported by their experiments, and (3) the experiments do not consider the stochasticity of the methods they are evaluating.\n\nDETAILED COMMENTS: The multiple-sample dropout proposed by the authors leverages a parallel structure to provide significant acceleration in training, which is supported by experimental results. However, their claims regarding the introduction of a \u201cnew regularization technique\u201d and \u201cbetter generalization\u201d are not convincingly proven in the paper. As mentioned in the paper, the proposed technique of multiple-sample dropout is analogous to the original implementation of dropout, but with larger batch size and duplicated samples. Therefore, the advantage of the proposed method is simply computational efficiency, but not a new \u201cregularization technique\u201d. In addition, while the paper claims the method provides better generalization, the results shown in Figure 6, in which the training losses for their multi-sample dropout and original dropout with duplicated samples are not significantly different, suggest that their validation error might not be significantly different either. The experiments shown in Figure 2, from which they conclude that their method generalizes better, are not conclusive since the validation error is evaluated at a stage in which the original dropout might have not converged, given the slower training. Therefore, the observed differences in validation error might come from the differences in training speed, and not from a better generalization ability of the model. Moreover, in the experiments the authors present single realizations of each training scenario, disregarding the stochastic nature of dropout-based techniques. In particular, the validation error curves observed in Figure 3 (where they analyze the effect of the number of dropout samples) seem very noisy, and the differences observed might simply come from the stochasticity of the training process. The authors should have executed multiple training runs with the same set of hyperparameters, and reported the statistics of their performance (same for results presented in Figure 2 and Table 1).  Overall, the advantages regarding training acceleration and computational efficiency are clear and well supported by the experiments. However, the novelty of the proposed technique and their claim regarding better generalization is not well supported theoretically or experimentally.\n\nFor the experiments, the authors should consider:\n1)\tReport validation errors after convergence of both methods\n2)\tRun the training scenarios multiple times and report performance statistics instead of single run performance\n3)\tReport validation curves for experiments in section 3.4 (Figure 6)."}