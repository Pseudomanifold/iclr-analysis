{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a new variation of Dropout -- Multi-Sample Dropout. The method is said to 1) accelerate training, and 2) decrease validation error. To achieve that, the authors propose to average a loss function over several dropout samples per object during training. This leads to faster convergence in terms of iterations, though at the cost of multiple forward-passes. In order to decrease the computational time of one iteration, the authors propose to evaluate the part of a network before the first dropout layer only ones and duplicate the remaining part with different dropout masks and shared weights. This decreases the run time of the naive approach and leads to faster convergence in terms of time in comparison to the original dropout. The authors also show that the model trained with the proposed method achieves lower validation error.\n\nThe only confusing part of the method for me is the prediction. In the second paragraph in section 2.1, the authors state that during inference neurons are not discarded, and only one dropout sample is used for the prediction. This is confusing because using a dropout sample implies the dropping of neurons. Could you please elaborate on this? Moreover, in the first paragraph of section 2.1, the authors state the prediction is based on the average of outputs from the last fully-connected layer, which is different from the inference procedure. Do you use different prediction methods to evaluate the performance of the train and test sets?\n\nThere are several concerns regarding empirical evaluation and baselines:\n(1) The improvements in validation error shown in Tab.1 seem to be insignificant, and confidence intervals are needed for justification.\n(2) To the best of knowledge, there is at least one more paper that proposes a technique for accelerating dropout training --- Fast Dropout [http://proceedings.mlr.press/v28/wang13a.html]. The method seems to be a direct competitor and should be considered as a baseline and be included in the relevant work section with further discussion.\n\nOverall, the proposed approach is heuristic, and the novelty is very limited. Along with empirical evaluation issues, I would suggest improving the work and rejecting the current version.\n\nAdditional comments:\n1. I think it would be beneficial to discuss other works that similarly interpret dropout as an ensembling technique: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning [https://arxiv.org/abs/1506.02142] and Variational Dropout and the Local Reparameterization Trick [https://arxiv.org/abs/1506.02557]."}