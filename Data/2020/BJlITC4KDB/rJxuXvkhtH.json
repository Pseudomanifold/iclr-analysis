{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper propose an ensemble of dropout: it applies multiple copies of a neural net with different dropout configurations (i.e., dropout masks) to the same mini-batch, and the training loss is computed as the sum of losses incurred on the multiple copies. They claim that such ensembling can improve training performance over the original dropout without increasing too much computation. Experiments on several datasets show that the proposed method can achieve slightly better validation accuracies than the original dropout.\n\nMain comments and questions:\n\n1) The contribution of this paper is very limited: it simply applies the ensemble method to dropout (ensemble method can be applied to any existing models). Moreover, this contribution might not be very necessary: dropout is itself an ensemble method, so an ensemble of dropout does not make much sense.\n\n2) The paper keeps claiming that the computation will not dramatically increase because only the computations after dropout neede to be re-computed if applying multiple dropout masks to the same data(batch). This is only true for the case when dropout is used in the last few layers. For a rich class of modern neural nets such as WideResNet and many Language models, this is not true since each block contains dropout and the model is a stack of many blocks. Hence, the advantage claimed here can be only applied to very limited cases. In addition, only the layers after dropout can get the benefits of dropout training since only they get different inputs for different dropout masks but need to produce similar final outputs.\n\nIn Sec 2.2, the authors said their method \"can enjoy similar gains without a huge increase in computation cost because it duplicates only the operations after dropout\". So the computational advantage does not hold if we apply dropout in earlier/shallow layers. For example, if you apply dropout in the first layer, the method needs to execute all the operations after the first layer for each dropout sample (dropout changes everything after it), hence it increases the computation time nearly M times (for M dropout-samples) and loses the computational advantage. This might be the reason why the experiments only consider DNNs with dropout in the last fully-connected layers. \n\n3) The experiments did not evaluate the method on any modern neural nets and only tried small neural nets with dropout only applied to the last one or two fully-connected layers. It is not clear how it performs when used to train the modern models, which might prefer dropout above the last few layers.\n\n4) The dropout with 1-sample in experiments is not a standard baseline in any previous works. The two standard ways to do inference on dropout trained model are 1) one-time inference without any dropout, or 2) ensemble the inference results of multiple dropout samples. One dropout sample suffers from a higher variance. Hence, the comparison is unfair.\n\n5) It is not professional to report only training loss and validation error for some experiments, but report training loss/error and validation error for some others. In fact, it does not make sense to compare training loss/error of different dropout methods, since dropout always reduces training loss but improve generalization.\n\n6) Except for the vanilla dropout, no other baseline is compared. The paper simply ignores many successful variants of dropout proposed in the past several years."}