{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a model for multi-source domain adaptation. All the domains share the same label set. Given an image in domain A, the model learns to generate another image with the same content, but with characteristics that depend on another domain, say B, and on a conditioning image from domain B. A classifier is then trained using this transformed image. The generative mapping relies on parametrized transformations which perform some normalization of their input followed by a shift-scaling transform, generalizing the batch-norm idea. The intuition presented by the authors is that they first remove domain and style information from the input image so as to keep only the content information and then inject domain and style information of an image from another domain. Training is performed via an adversarial criterion. Experiments are performed on two multi-source datasets : digits and Office Caltech.\n\nThe model combines several already existing ideas, but overall propose an original  system for the MS domain adaptation task. The general idea is well motivated and the description is relatively clear. I found however the distinction between domain and style not so convincing, and in the qualitative evaluations I cannot see what distinguishes them. Also it is not clear, what is the role of the whitening operation in the decoder \u2013 why normalizing the \u201cdomain independent\u201d representation with the new domain statistics. The \u201cstyle\u201d paragraph in section 3.2.3 could be improved, as such it is not easy to follow the justification of the different operations.\nThe experimental part in this version of the paper is however not so convincing and many details are lacking so that it is difficult to appreciate the performance of the model. In table 1, where different models are compared, there is no indication on where the numbers come from. Did you rerun all these models or did you pick the numbers from other publications? How do you make sure that these numbers are comparable: in domain adaptation, the performance heavily relies on the choices made for the NN architectures for example so that comparisons with generators of different complexity make no sense. How did you adapt the different models to the multi-source context? Why no confidence intervals? Given the instability of GAN training, the variance might be quite high. How did you perform model selection for your model and the others in the unsupervised context? I understand that you have been using one multisource configuration for selecting the hyperparameters, which corresponds to some form of supervision, what about the  other baselines, did they use the same setting?\nFinally, the ablation study shows that the performance indeed varies when removing or changing some component, but in quite small proportions compared to the gain provided by the proposed model w.r.t. the baselines. If none of the individual components alone is responsible for the performance increase w.r.t. the baselines, where does the improvement come from?\nOverall I found the model an interesting contribution, but the evaluation raises too many questions.\n"}