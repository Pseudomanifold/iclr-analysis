{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an approach where the regularisation is used to optimise whether each layer of a DNN is binary or ternary. The paper presents an equation for performing this along with two examples of the process in use.\n\nThe paper is inconsistently written with work described at different levels in different sections and has an inconsistent feeling about it. For example the introduction seems to stop abruptly before it describes all the parts of the paper.\n\nThe paper seems to contain an idea which might have merit. However, the idea just does not seem to have been developed enough.\n\nMajor concerns:\n1) The authors claim that this is the first attempt at a training algorithm for mixed precision training. However, a simple google search throws up many papers in this area. Many of which are not mentioned in the related work.\n\n2) Equations are not discussed in enough detail, nor are the parameters defined. Or if they are defined they are done so much later in the work.\n\n3) There doesn\u2019t seem to be enough material here to reproduce the work.\n\n4) In the results you talk about \u2018the best\u2019. Given that there has been much criticism over the last two years about good academic practice the fact that you don\u2019t say at least \u2018best out of \u2026\u2019 is worrying. \n\n5) You have magic parameters lambda and gamma. You say that these effect the outcome of the work but in your examples you only state values these are set to. One would expect to see at least some analysis of how varying these values effect the outcome. But better would be to show that you have identified good values for both of these parameters. Ideally would be an evaluation of how others could identify the best values.\n\n6) Figures 3 and 4 are difficult to interpret. They need a clear explanation.\n\nSome more generic comments:\n- The abstract seems to assume a huge prior knowledge by the reader.\n\n- \u20181 bit precision\u2019 - precision seems to have no meaning in this context. Surely just \u20181 bit\u2019\n\n- The related work contains a lot of equations, but no real explanation of what they are.\n\n- \u2018we let \u03b2 very per layer\u2019 -> \u2018we let \u03b2 vary per layer\u2019\n\n- In equation 10 what do I and J represent?\n\n- \u201928 \u00d7 28 gray-scales images\u2019 -> \u201928 \u00d7 28 gray-scale images\u2019\n\n- \u2018For the training session, we pad the sides of the images with 4 pixels, then a 32 \u00d7 32 crop is sampled, and flipped horizontally at random.\u2019 - why?\n\n- \u2018As commonly practiced\u2019 - by whom?\n\n- \u2018which is costly, specially if\u2019 -> \u2018which is costly, especially if\u2019\n"}