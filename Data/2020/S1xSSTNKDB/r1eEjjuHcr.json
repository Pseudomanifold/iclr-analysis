{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors sample ~100k faces from the YFCC100M dataset and conduct an Amazon Mechanical Turk study to annotate the faces with race, gender, and age, and perform experiments in an effort to demonstrate its utility. It is unlikely that this dataset is useful to fairness community for several reasons: 1) There exist comparable datasets 2) The experiments are not rigorous 3) The race categories are irresponsibly constructed 4) The Mechanical Turk study is not rigorous. \n\nThe authors themselves note that the DiF (Diversity in Faces) dataset was also derived from the YFCC100M dataset and contains even more annotated images (~1million). Based on Figure 1, \u201cRacial compositions in face datasets,\u201d simply undersampling DiF for equal class sizes ought to yield FairFace. DiF and FairFace only differ in their treatment of race \u2013 DiF annotates skin color and FairFace annotates \u201crace\u201d categories. Despite these similarities, the authors do not compare DiF and FairFace in their experiment section. The justification for this is unclear as they include another dataset that doesn\u2019t have race annotations (CelebA) in the experimentation. The training and testing sets for FairFace and the considered datasets seem to be generated randomly. Why wasn\u2019t FairFace compared to under/over sampled versions of the other datasets with equal class sizes? If that can achieve the same effect why is FairFace needed? Including these experiments would help improve the paper.\n\nThe usefulness of the proposed \u201crace\u201d category annotations are unclear as they seem to be arbitrarily constructed without regard to the fields of race and ethnic studies, anthropology, or social science. In constructing these categories, the authors skim over separate definitions of race and ethnicity (Schaefer, 2008) but decide to conflate the two, stating \u201cin practice, these two terms are often used interchangeably.\u201d While this practice may be fine in colloquial English this is not sufficient for defining data class boundaries. For example, the authors construct a Latino category stating, \u201cLatino is often treated as an ethnicity, but we consider Latino a race, which can be judged from the facial appearance\u201d with no citations of a similar practice, which aspects of facial appearance are Latino in this context, and an alarming disregard that self-identified Latinx people also self-identify and/or pass as Black/Native American/White. It is unclear whether one face can be annotated as belonging to multiple categories. As these categories are intended to help understand AI applications in a societal context, they should follow the conventions employed by formal fields studying society. \n\nCommon convention in HCI/survey methodology studies is to provide a clear definition of the annotation task. For example in the construction of ImageNet dataset, a definition of the target synset was provided as a Wikipedia link. What is the definition of the annotation task here? Consider again the Latino category. If the Latino category can \u201cbe judged by the facial appearance\u201d what are the facial features the workers are told to look for? If none are provided the annotations are subject to the workers\u2019 bias. It is unclear if the workers self-reported demographics are collected or understood by the authors as a potential source of bias. Adding more details on how the study was conducted would help improve the paper."}