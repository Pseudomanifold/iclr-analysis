{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "First of all, the two physical attacks evaluated in this paper have similar attacking patterns, i.e., mask-based pixel attacks. So it is not surprising that DOA is more robust in these cases since DOA is trained on this attacking pattern.\n\nActually it has been shown that the framework of adversarial training (AT) will overfit to the attacking patterns used in training. For example, PGD-AT models are less robust to simple non-pixel transformation, like rotation, than the standard models [1]. So what DOA does is just substituting the PGD module in AT to overfit the new attacking patterns, which is of limited contribution and novelty. \n\nBesides, AT is not really scalable compared to other simpler defense strategies like input transformation. [2] proposes a simple and effective defense based on different combinations of input transformation and its performance even surpasses some SOTA AT models with less computation. \n\nAnother advantage of these off-the-shelf defenses like input transformation is that they do not depend on the specific details of attacks, so they are more reliable when you are unknown about the potential attacking patterns in practice. In comparison, there is an implicit assumption in AT methods that the attacking patterns in training and test are similar. This is the reason why PGD-AT is not robust facing mask-based physical attacks or simple rotations.\n\nSo under the more completed and flexible physical attacks, a defense based on the AT framework like DOA may not be a good choice. Although AT methods are quite effective and widely studied under the l_p attacks, the authors are expected to consider more factors if they really want to design a robust system in the physical world, rather than just follow or apply the most popular pipeline like AT.\n\nReference:\n[1] Engstrom et al. A rotation and a translation suffice: Fooling cnns with simple transformations. ICML 2019\n[2] Raff et al. Barrage of Random Transforms for Adversarially Robust Defense. CVPR 2019"}