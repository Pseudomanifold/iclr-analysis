{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper aims to provide a defense to physically realizable attacks for image classifiers. First, the paper demonstrates that Lp-ball robustness (obtained via adversarial training or randomized smoothing) do not necessarily result in robustness to physical attacks such as adversarial stickers. Next, the paper proposes a variant of adversarial training (using adversarial rectangles) and shows empirically that such a method results in much improved robustness to the aforementioned physical attacks.\n\nI vote to accept this paper. It has clear motivation, is very clearly written, evaluates proper benchmarks from recent literature, and proposes a new method that shows a clear improvement over benchmarks in literature. The goal is clearly laid out in Section 3, and the paper justifies its claims via various experiments. Overall, I think it is a good contribution to the adversarial examples literature, as it provides robustness against more \u201creal-world\u201d attacks.\n\nIn particular, the results from Fig. 2 and Fig. 3, while not surprising, appear to be done thoroughly, as the authors evaluate against various forms of adversarial training and various degrees of randomized smoothing. Later, the results in Fig. 4 and Fig. 5 show a clear benefit from their method.\n\nI do have a few pieces of feedback that I think could improve the work.\n\nIn section 5.1, do you really only train for 5 epochs? Is this just a fine-tuning procedure after standard training is performed, or is the entire training procedure 5 epochs? That seems especially short for getting any amount of accuracy. I think it is worth clarifying.\n\nIn Section 3 (or in the Related Works), I would suggest that the authors do mention related works about other types of realistic adversarial examples or perturbations, such as those generated via physical transformations like translations and rotations [1] or even common corruption robustness [2]. This is especially relevant since the authors claim to be the \u201cfirst investigation of this issue in computer vision applications,\u201d so it\u2019s worth clarifying that the authors do not claim to be the first work about robustness to all realistic perturbations.\n\nOne thing I would be curious to see is if the DOA training method provides any robustness to standard Lp-ball adversarial examples or to perturbations like rotations.\n\nAdditional Feedback:\n\n- In Related Works, I think there are some places where it would look better to use parentheses around the citations.\n\n[1] https://arxiv.org/abs/1712.02779\n[2] https://arxiv.org/abs/1903.12261"}