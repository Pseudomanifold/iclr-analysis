{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper collects a replay dataset of Atari to propose a benchmark for offline (batch) reinforcement learning. Experiments on this dataset show that offline RL algorithms can outperform online DQN. Then, this paper presents a new off-policy RL algorithm, which uses random convex combinations of multiple networks as the Q-value estimator. The experimental results show that it outperforms distributional RL algorithms (i.e., C51 and QR-DQN) in the offline setting and performs comparably in the online setting. \n\nThe idea of randomly combining Q networks is interesting and paper is well written to demonstrate their key ideas. However, I have some concerns.\n\nWhy are off-line RL algorithms necessary? I am not sure this paper is relevant to the community of standard RL. This paper does not show the usage of their method to a standard RL agent. I am happy to change my score if the authors can clarify the motivation of studying the offline performance of conventional RL algorithms and clearly show the improvement over baseline algorithms in a standard RL setting rather than only show insights for potential usage in online RL.\n\nIn my opinion, the results are not significant to support their claims. Such a small gap (4.9% as shown in Table 1) over baseline models may result from considerable hyper-parameter tunning. In addition, the figures and tables in this paper do not show deviations and confidence intervals.\n\nSince REM has similar architectures with attention networks, it will be better to include an attention-based Q network as a baseline model.\n\nIn the right sub-table of Table 1, I wonder why the performance of online QR-DQN in this paper is much lower than that reported in the original papers (media is about 200%).\n\n"}