{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigated the offline settings of deep Q-networks, and provided relevant comparison among different agents. The authors also proposed two DQN variants by using ensembles, and showed the improvement (mainly from the random ensemble mixture) on Atari games, when compared with baselines. \n\nIn general, this paper explored an important problem, i.e., offline vs. online settings in DQNs. The experimental comparison also looked interesting, as it showed the empirical evidence on the influence of such settings. A lot of the experiments were performed to support the claims, which I really appreciate. On the other hand, it's a bit unclear to me whether offline should be preferred over its online counterpart, given the results presented and potential computational cost. For the proposed REM method, further explanation isolating from the offline vs. online setting could be helpful to understand why it works. Furthermore, some settings in the experiments need to be clarified. If the authors can address my concern, I would be willing to raise my score.\n\n1. To my knowledge, using the logged experience to train an offline DQN agent is an interesting exploration. This approach can also achieve competitive performance, when compared with the online agent. My concern is whether the authors had considered the cost due to this offline setting, as additional time needs to be spent on training another agent to obtain such logged data, which is not required in the online methods. Moreover, Table 1 suggests that the online agents usually outperformed their offline counterparts, from which I am somewhat in doubt about whether the offline approaches should be preferred.\n2. I am also wondering how the performance depends on the agent used to generate the logged data. In the current version, it is from the Nature DQN, but what will happen if we use a different (can be either better or worse) agent? For example, how about using C51 to generate the logged data?  I hope the authors can provide more evidence and analysis on why the offline settings should work. \n3. When training the agents with the logged experience, how did you handle the new data generated? I understand the offline setting may not consider this issue, but am just thinking if such data can be better used.  \n4. The ensemble based DQNs proposed in Section 4 are very interesting, which can even be a separate contribution, as they do not depend on the online or offline settings. From this perspective, I am a bit curious if the authors have more explanation on the benefits of such methods, e.g., reducing the overestimation bias and gradient noise, as shown in the averaged-DQN and the following softmax-DQN work, \nSong et al., Revisiting the Softmax Bellman Operator: New Benefits and New Perspective, ICML 2019, \nwhich the authors may refer to in their paper.\n5. For the curves in Figures 1 and 4, how about the corresponding running time? Such information could be helpful for others trying to reproduce the results."}