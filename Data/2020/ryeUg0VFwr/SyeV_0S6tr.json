{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*Summary*\n\nPaper tackles an important issue, which is benchmarking off-policy deep RL algorithms. The motivation is important, because 1) assessing the performance of off-policy RL algorithm is hard due to the mixed effect of exploration and exploitation while collecting data; hence the paper suggest to fix the data collection process by a standard algorithm (DQN) and then compare the off-policy methods. 2) A fixed dataset will help the reproducibility of the results. \n\nAdditionally the paper assess the performance of some off-policy RL methods (mainly DQN and QR-DQN) and showed that given offline data, these algorithms can obtain a superhuman performance. \n\nBesides the mentioned contribution, authors strived for a simple algorithm (easy to implement), and suggests a new simple algorithm, Random Ensemble Mixture (REM) by training multiple Q-functions and randomly sampling a subset for ensemble estimate. The mina motivation come from supervised learning literature. \n\n*Decision*\n\nI enjoyed reading the paper and I think authors tackled a very important problem. Although a very important step, I vote for a weak reject for this paper as I believe the contribution of the current paper is limited. I made my decision mainly based on the following points:\n\n[Limited Task Diversity]: I agree with the authors that having a fix offline data for testing different algorithms is important; however the suggestion (DQN logged data on Atari2600) is a very limited dataset, in terms of 1) task diversity 2) Data collection strategy. \nFor a benchmark, I would like the dataset to have different strategies for data collection, so that the effect of data collection can be stripped off. In the current assessment, we compare the performance of algorithms condition on the DQN e-greedy data collection procedure. (For example, the right claim to make is: QR-DQN showed a better performance condition on the current data generation process). And it is not clear to me the same observation will hold given a different data generation process (for example a random policy, or a human playing data, or \u2026)\nAlso, I think only focusing on Atari tasks is limited in nature, as many of these games share a similar underlying mechanism. \n\n\n[Overfitting] I believe having a fixed dataset for testing off-policy RL is great, however the current suggestion is very prone to overfitting. I can foresee that if the community start to use this benchmark as a test case, soon we will overfit to this task, in a sense that I try algorithm X on offline data, test on Atari see the results, tune algorithm X test on Atari again, \u2026 This way I finally have an algorithm that only learned from offline data, but is performing great on online Atari. But I am overfitting!\n\n\n[Simplicity Trade-Off] Authors focused on getting a simple algorithm, but to me it is unclear why should we optimize for simple algorithm, and my impression of simple as authors have in mind is a \u201ceasy to implement\u201d algorithm. \nSimplicity is great, but usually I believe we seek a simple algorithm to be able to get more insight into the theory and build more intuition. However, in this work there is a lot emphasis on simple algorithm, but no extra significant insight into off-policy learning has been gained. Additionally, I found the suggested algorithm ad-hoc and of limited contribution. \n\n\nMinor Comments/ Question that did not significantly contribute to my decision: \n\n[Valid Q-function]: authors mentioned a convex combination of Q-values are a valid Q-value, but never defined a \u201cvalid\u201d Q-value. Do authors mean, if Q1 is generated by \\pi_1 and Q2 by \\pi_2 then convex combination of Q1 and Q2, Q^* can be also generated by some policy \\pi^*? \nIf so, this needs clarification. \nAnd if so, I\u2019m not sure why this is important, since in Q-learning the Q-values that we obtain before convergence might be \u201cinvalid\u201d (by the definition above, so that no real policy can generate those values exactly), so what is the point of paying attention to a valid Q values in the first place?\n\n*Improvement*\n\nI think some clear improvement avenues for the future are\n1. More diverse set of tasks: I would like to see some non-Atari tasks included in the dataset.\n2. More diverse data collection strategy: Example can be different exploration strategies than e-greedy. Or even random exploration, or maybe data of human players. Since we don\u2019t want to have an algorithm that is good only for state distribution induced by e-greedy exploration.\n3. More diverse task will help with overfitting by itself. But a good benchmark needs a test case that cannot be exploited. \n4. More insight into why REM is outperforming QR-DQN or other algorithms. Currently I am not convinced that we understand why REM is outperforming, or if it\u2019s only because we designed it to be good for this specific task and dataset? \n\n\nGenerally, I believe authors made a very important step, and I really enjoyed reading the paper; however, I think the current contribution is not sufficient to merit a publication. This makes a good workshop paper at this point.\n"}