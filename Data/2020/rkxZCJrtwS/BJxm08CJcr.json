{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows how the derivatives from a differentiable\nenvironment can be used to improve the convergence rate of\nthe actor and critic in DDPG.\nThis is useful information to use as most physics simulators\nhave derivative information available that would be useful\nto leverage when training models.\nThe empirical results show that their method of adding\nthis information (D3PG) slightly improves DDPG's\nperformance in the tasks they consider.\nAs the contribution of this work is empirical is nature,\nI think a very promising future direction fo work is to\nadd derivative information to and evaluate similar\nvariants of some of the newer actor-critic methods\nsuch as TD3 and SAC.\n\nI have two minor questions:\n1) Figure 2(a) shows the convenrgence of regularizing states,\n   actions, and both states and actions and the text\n   describing the figure states that this is\n   \"expected to boost the convergence of Q.\"\n   However the figure shows that regularizing both states and\n   actions results in a slower convergence than doing\n   them separately. Why is this?\n2) How should I interpret the visualization of the\n   learned Q surface in Figure 2(f) in comparison to\n   the true Q function in Figure 2(g)?\n   It does not look like a good approximation."}