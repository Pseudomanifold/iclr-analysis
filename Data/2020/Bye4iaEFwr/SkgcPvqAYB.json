{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes an improved DPN framework with a novel loss function, which uses the standard cross-entropy loss along with a regularization term to control the sharpness of the output Dirichlet distributions from the network.The proposed loss function aims to improve the training efficiency of the DPN framework for challenging classification tasks with large number of classes \n\nThe proposed improved DPN is very incremental. It only adds a simple regularization term to the standard cross-entropy loss. The regularization term is the precision of the Dirichlet from the DPN. The technical novelty and contribution is not significant. \n\nThe paper claims the proposed loss function allows distributional uncertainty to be modelled separately from data uncertainty and model uncertainty, and the proposed framework can improve efficiency. However these claims lack of sufficient support. \n\nMoreover, determining the source of uncertainty is just a mean of achieving better classification model, not the goal. The experimental results are not very convincing in improving classification over OOD examples due to the lack of comparison with state-of-the-art related works. Note many domain adaptation methods can handle OOD examples. "}