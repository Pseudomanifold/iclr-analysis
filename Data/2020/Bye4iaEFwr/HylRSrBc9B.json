{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a novel loss function using the standard cross-entropy loss along with a regularization term on logits for training the Dirichlet prior network. The benefit of using Dirichlet prior network is that it can distinguish the in-domain noisy data and completely out-of-domain data. For in-domain noisy data, the Dirichlet distribution should be sharp but in the middle of the simplex, while for the OOD data, the Dirichlet distribution should be flat. The Dirichlet prior network is proposed by Malinin & Gales (2018), the new method in this paper overcomes the challenge of training the network based on the KL divergence which cannot work well for dataset with large number of classes. The paper is well written and easy to follow. Here are some comments and questions: \n\n- In the proposed loss function, could you explain what is the reason that you choose to use sigmoid(z_c(x)) instead of \\sum exp(z_c(x))? As you mentioned in the paper,  \\sum exp(z_c(x)) suggests the sharpness of the distribution. Shouldn\u2019t using \\sum exp(z_c(x)) be more direct than the sigmoid(z_c(x))?\n\n- The methods requires OOD dataset for training. The authors used the same OOD dataset for training and test. One concern is that what if the OOD dataset for test is not available at training. What is the alternative plan? How does that perform?\n\n- In Table 1, for Gaussian in-domain dataset, why is \\sum exp(z_c(x)) able to distinguish Gaussian in-domain from original in-domain images? If I understand correctly, Gusaain in-domain should have a sharp distribution which means the differential entropy (D. Ent) is small (as illustrated in Figure 2(d)), and the sum of the exponential of logits (\\sum exp(z_c(x))) should be large. For the original in-domain, it should also have a sharp distribution with small differential entropy and large sum of the exponential of logits. Then I don\u2019t understand why in the table, the AUROC and AUPR for the measure \\sum exp(z_c(x) are very high values while that for the measure D. Ent are very low.\n\n- Could you also compute the sum of the exponential of logits for the synthetic data, since it is the only metric that is evaluated in the real data experiments but not in the synthetic data?\n\n- Could you clarify how you compute the differential entropy of the Dirichlet distribution given an input x? Did you use \\alpha_c = exp(z_c(x))? \n"}