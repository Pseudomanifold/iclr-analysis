{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new regularization method to mitigate the overfitting issue of deep neural networks. Specifically, unlike the regular dropout which randomly zeros out some neurons in each layer, the proposed RotationOut rotates the features with a random rotation matrix. The authors argue that the rotational operation can reduces the co-adaptation of features. The experiments have shown some improvement over existing methods. \n\nI have some concerns about the proposed method as follows:\n\n1. In this paper, the authors use the correlation to measure the co-adaptation of features and prove RotationOut can reduce the correlation. However, it is well known that the rotation matrix is an orthogonal matrix, which cannot decorrelate two random variables. So, how can the rotational operation reduce the correlation? It looks a paradox. \n\n2.  Compared with the regular dropout, RotationOut involves more computational overhead. It's better to show the running time of these methods. \n\n3. In Eq.(15), D is the dimension of features. For a large D, such as 224x224 in the imagenet, 1/D is very small. So, the improvement of co_{Rot} over co_{Drop} is very small. "}