{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows how end-to-end learning can be done through\ncombinatorial solvers by using the derivative of\ncontinuous surrogate function in the backward pass.\nOne elegant part of the method is that no modification\nor relaxation is done to the combinatorial solver in\nthe forward pass and that the backward pass just requires\nanother call to the blackbox solver.\n\nThe idea of constructing continuous surrogate functions\nand using them for differentiating through solvers with\npiecewise-constant output spaces is thought-provoking and\nI can see it inspiring many new directions of work.\nFor example looking at Figure 2 for intuition, one could\nimagine other ways of making the solution space continuous.\nThe solution space of linear programs over continuous spaces,\nas considered in [Elmachtoub & Grigas], the Sudoku example in\n[Amos & Kolter], and related papers, is also piecewise constant and\nit seems like a similar method could be used to bring more\ninformative derivative information to linear programs ---\nhave you considered this as a future direction?\n\nOne of my concerns with this work is that the ResNet baseline in the\nexperimental results seems like too much of a straw man for the tasks.\nI do not see why they should have the capacity to generalize well.\nThis paper shows the ResNet baseline achieve near-zero\ntest accuracy but doesn't compare to other relevant baselines\nthat are mentioned in the related work section:\nfor example [Bello et al, Deudon et al., Kool et al.] for the TSP.\n\nAnd one smaller comment: If one wanted to squeeze the performance even\nmore, would starting the training process with a large \\lamdba\nand annealing it to zero help?\n\n----\n\nElmachtoub, A. N., & Grigas, P. Smart \"predict, then optimize\". arXiv 2017."}