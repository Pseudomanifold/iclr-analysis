{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors present an approach for semi-supervised learning which combines noisy labels with boosting. In a first step, the labeled instances are used to train a set of classifiers, and these are used to create noisy labels for the unlabeled instances. Then, an EM procedure is used to estimate the noise level of each instance. Finally, a version of AdaBoost which accounts for instance noise levels is proposed to create a final classifier. A limited set of experiments suggests the proposed approach is competitive with existing approaches.\n\nMajor Comments\n\nAs a non-expert in this area, I had trouble identifying the novel contributions of this work. For example, many of the results in Section 3 (noise-resistant AdaBoost) seem to replicate, or follow closely, the results of [Natarajan et al., 2013]. Similarly, using EM to assign pseudo-labels has been extensively studied in the literature [Lee, WREPL 2013; Chapelle and Zien, AISTATS 2005; Kang et al., ECCV 2018; Rottman et al., ICMLA 2018]. \n\n\nThe experiments are very poorly described, so it is difficult to gauge if they are valid:\n\nMost importantly, the authors point out that the estimated error rates do not always match the actual error rates. Since this seems to be one of the most important factors of the proposed approach, further investigation should be performed to answer questions like: why is the error rate not estimated well? on what type of datasets? can more/better supervised learners help? In some cases (Diabetes, Thyroid, Heart), the actual noise rate increases with more labeled samples. What does that mean?\n\nSecond, the proposed approach seems to have a number of important hyperparameters, including the number of supervised models trained and their hyperparameters, the parameters of the Beta distribution used as a prior on the noise estimation, and the hyperparameters of the AdaBoost algorithm. Likewise, all of the competing algorithms also have hyperparameters which are known to affect performance (e.g., learning rate for NNs). The paper does not mention how (or if) a validation set was used to select these.\n\nThird, while the caption of Table 1 mentions that 20 trials were used, it is not clear if this was some sort of k-fold cross validation, Monte Carlo, cross validation, the same splits but with different random seeds, etc. Additionally, the variance across the different trials should be given; otherwise, it is not possible to tell if any of the empirical results are significant.\n\nMinor Comments:\n\nThe references are not consistently formatted.\n\nThis paper is very notation heavy. It would be helpful to include a \u201ctable of symbols\u201d for the reader in an appendix.\n\nAdditionally, the notation in the paper is not consistent. For example, both \u201c$M$\u201d and \u201c$\\mathcal{M}$ are used to indicate the number of models trained on the labeled data. Later on, \u201c$\\mathcal{M}$\u201d is also used to refer to the set of trained classifiers. The first bullet point in Step 3 of the pseudocode seems to suggest that each classifier is trained on a single labeled data point. The equation at the bottom of Page 5 used \\theta, but it does not seem to be defined.\n\nThe discussion on experts, spammers, and adversaries could be helpful if this terminology were used throughout the paper; however, it is used in only one paragraph. \n\nThe main body of the paper should mention that proofs are given in the appendices.\n\nFor context, if may be helpful to mention that graph convolutional networks and other representation learning techniques are commonly used for semi-supervised learning (e.g., [Kipf and Welling, ICML 2016]). Those approaches are quite different (and lack any sort of theoretical guarantees, for the most part), though, so empirical comparisons may not be so meaningful.\n\nIt would be helpful to give a sentence or two on the intuition behind what the proofs are showing. For a non-expert, they are very difficult to follow.\n\nDo the various proofs still hold when the datasets are artificially balanced (with respect to the last paragraph in Section 4)?\n\nIt would be helpful to include the performance using the complete labeled dataset for comparison.\n\nStratified sampling could be used to ensure both classes are present in the training data. Also, \u201c0.99%\u201d -> \u201c99%\u201d.\n\nBesides accuracy, some measure like AuROC or the F1 score which account for class imbalance should be given.\n\nTypos, etc.\n\n\u201cLogitboost tested against\u201d -> \u201cLogitboost were tested against\u201d\n\n\n\u201ctherefore is not\u201d -> \u201ctherefore, having a lot of labeled data is not\u201d\n\n\n"}