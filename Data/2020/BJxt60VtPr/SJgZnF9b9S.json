{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a view-prediction inverse graphics model that takes input RGBD streams and produces a 3D feature map of the scene, including regions that are unobserved due to occlusion.  This model is used to learn a 3D visual representation that can be applied for semi-supervised 3D object detection, and for unsupervised 3D moving object detection.  The approach is similar to recent work on Geometry-aware RNNs (Tung et al. 2019), but is applied to more realistic scenes (urban landscapes datasets generated using the CARLA simulator as opposed to simple tabletop arrangements of ShapeNet objects) and makes fewer assumptions about the camera pose (6 DoF camera parameterization as opposed to 2 DoF cameras placed around the objects).  Moreover, the proposed model is evaluated on the downstream 3D object detection tasks to demonstrate the utility of the learned 3D visual representation.\n\nExperiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi-supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim-to-real transfer results (training in CARLA and testing on the KITTI dataset).  The results show that the proposed method: 1) has higher 3D object detection mAP than a view regression-based baseline from prior work (Tung 2019) for settings with little available 3D boundign box supervision; 2) has better 3D detection transfer from CARLA to KITTI (evaluated by mAP) compared to view regression baseline; 3) outperforms the view regression baseline and a 3D motion flow-based baseline on 3D moving object detection (measured through precision-recall curves and mAP); 4) has lower 3D flow error for moving objects compared to zero-motion and view regressive baselines.\n\nI am positive with respect to accepting this work, but find that there are a few unclear points in the evaluation that should be clarified to strengthen the empirical results.  The data is generated from 50 frame sequences at 30fps (i.e. ~1.6 seconds of simulation) where each frame has 6 randomly sampled camera viewpoints in a 20m hemisphere in front of the car.  It would seem to me that the distances between the viewpoints in different frames from these sequences are likely to be quite small, so most of the viewpoint variance would come from random sampling within the hemisphere and randomly selecting one of the views as the target/unseen view. From this description, it is not clear how much variation of unobserved vs observed surfaces exists in the training and test data. It would have been informative to provide some statistics about observed vs occluded object surface area to elucidate the dataset construction.  This aspect of the dataset likely impacts the performance of the method significantly and should thus be addressed a bit more clearly."}