{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the problem of visual representation learning from 2.5D video streams by exploring the 2D-3D geometry structures in the 3D visual world. Building upon the previous work GRNN (Tung et al. 2019), this paper introduced a novel view-contrast objective applied to its internal 2D and 3D feature space. To facilitate the 3D view-contrast learning, this paper proposed a novel 2D-3D inverse graphics networks with a 2D-to-3D un-projection encoder, a 2D encoder, a 3D bottlenecked RNNs, an ego-motion stabilization module, and a 3D-to-2D projection module. Compared to previous work (Tung et al. 2019), view-contrastive inverse graphics networks decode in the feature space rather than RGB space. Experimental evaluations are conducted using CARLA simulator (sim) and KITTI dataset (real). Results demonstrate the strengths of the proposed view-contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation.\n\nOverall, this paper studies an important problem in computer vision with a novel solution using unsupervised feature learning. While the technical novelty is clear, reviewer has several questions regarding the implementation and experimental details.\n\n(1) For 3D box detection on KITTI (see Table 1), the comparisons to state-of-the-art models are currently missing. While the benefit of unsupervised feature learning has been demonstrated, it would be more convincing to compare against the following papers (at least with a paragraph of discussion).\n\n(2) The 3D-to-2D projection module seems very expensive. Can you possibly report the training and inference time compared to baselines? Also, the design of the projection module is a bit counter-intuitive as it has 8x8 convolutions. In principle, such projection should be learning-free or with only 1x1 convolutions (aggregation along depth channel). It would be good to consider such ablation studies in the final version.\n\n-- Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency. Tulsiani et al. In CVPR 2017.\n-- Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision. Yan et al. In NIPS 2016.\n-- MarrNet: 3D Shape Reconstruction via 2.5D Sketches. Wu et al. In NIPS 2017.\n\n(3) It seems that the proposed method assumes slow moving background across consecutive frames. In principle, the view-contrastive objective should mask out new pixels in frame T+1. Also, because the view-contrastive loss is applied at feature-level, reviewer would like to know performance on detecting small objects.\n\n(4) As the latent map update module uses an RNN, it would be good to consider consistency beyond 2 frames (given mask is applied to view-contrastive objective). Curriculum learning could be helpful for further improvements.\n\n-- Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis. Yang et al. In NIPS 2015.\n\n(5) How does the proposed method perform when applied to indoor environments?\n\n(6) Additional ablation study to consider: what if 2D/3D contrastive loss is turned off?\n\n"}