{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a novel sparse representation to improve the phrase retrieval question answering framework DenSPI proposed by Seo et al., 2019 (ACL2019). The proposed sparse representation leverages self-attention to reassign weights to the one-hot representations of the n-gram features with the aim of capturing sparsely activated lexical features.  Experiments are conducted on both the SQuAD and CuratedTrec datasets, and show improved performance over the DenSPI strategy. \n\nThe paper is easy to read. My main concern on the paper is its technical novelty and its performance when compared to the state-of-the-art method. \n\nMain remarks:\n\nSince the idea is almost exactly the same as that of DenSPI (published in ACL2019) except the sparse representation, I consider the novelty of the paper is limited. The only difference to me is that the tf-idf representation in the DenSPI is replaced by the one-hot representation of the n-gram with self-attention. Here the self-attention computation is speeded up by a kernel trick. In addition, although the proposed method improves over the DenSPI strategy, but the performance is still much lower than some state-of-the-art methods such as the Multi-passage BERT model as presented in Table 1 of the paper.\n\nOther comments:\n\n1. Some notations in the paper need to be defined, such as d_{se} and d_{c} on page 4. \n\n2. It would be beneficial to present the computational cost on training the self-attention of the n-gram sparse embeddings. \n\n3. Some observations and heuristic tricks deserve further discussions, such as \u201cadding tf-idf matching scores on the word-level logits\u201d in the Negative Sampling section; \u201cadding dense-only loss\u201d in the Kernel Function section. The impact of these tricks should be extensively discussed and included in the paper. \n"}