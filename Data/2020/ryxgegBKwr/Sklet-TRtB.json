{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method for learning contextualized, word-level sparse representations for the phrase retrieval problem. Their model dynamically computes the weight of each n-gram depending on the context. The authors show that their approach outperforms most competing methods on SQUAD (open) and CURATEDTREC. \n\nThe biggest problem I see is that a BERT-based approach (Wang et al., 2019) outperforms the proposed approach by a large margin and doesn't seem to be discussed. I understand that it's concurrent work, but have the authors investigated if there might be any setting where their method would be preferable? This does seem important.\n\nThe authors further perform an ablation study of different components of the model. The subsequent analysis is short, but I like it, and I think it is helpful in the context of the paper."}