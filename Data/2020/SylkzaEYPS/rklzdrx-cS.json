{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces an encoder-decoder as a differentiable loss function for sequential autoregressive generation tasks and more specifically for summarization. \nThis is done by adding a recorder network that that takes the decoded sequence from the summarizer as input and is trained to output the reference summary.\n\nI see a fundamental issue with this work:\n\n* During inference, authors decode from the probability distribution of the seq2seq model using beam search. \n* But for training (original seq2seq + recorder) authors backpropagate the NLL loss (which is fully differentiable) of the recorder on reference summaries through the softmax probabilities of outputs from the seq2seq model. \n\n>> This whole architecture can be seen as a traditional end-to-end seq2seq model with non-linearity and normalization (softmax) in the middle. \n\nAdditionally: \n>> \"backpropagating through the softmax weights during training and using the argmax during inference\" falls into a long line of work for propagating non-differential objective functions through continuous relaxations \n of categorical latent variables, more specifically the \"straight through\"  and \"gumbel-softmax\" (see refs.)\nThese methods have proven to be a strong alternative to reinforcement learning to train non-differential objectives and have been implemented quite a lot for sequence generation mainly for SeqGANs and even for text summarization connections to this line of work must be established in this paper. \n\nreferences \n- Estimating or propagating gradients through stochastic \u00b4neurons for conditional computation. \nBengio et al. 2013 arXiv preprint arXiv:1308.3432, 2013.\n- CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX\nJang et al. 2018 https://arxiv.org/pdf/1611.01144.pdf\n- GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution\nhttps://arxiv.org/pdf/1810.05739.pdf\n- Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders\nhttps://arxiv.org/pdf/1805.04843.pdf\n- MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization\nhttps://arxiv.org/pdf/1810.05739.pdf\n\n\n\n\n"}