{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThe paper scrutinizes commonly used evaluation strategies for neural architecture search.\nThe first contribution is to compare architectures found by 5 different search strategies from the literature against randomly sampled architectures from the same underlying search space.\nThe paper shows that across different image classification datasets, the most neural architecture search methods are not consistently finding solutions that achieve a better performance than random architectures.\nThe second major contribution of the paper is to show that the state-of-the-art performance achieved by the many neural architecture search methods can be largely attributed to advanced regularization tricks.\n\n\nIn general the paper is well written and easy to follow.\nThe paper sheds a rather grim light on the current state of neural architecture search, but I think it could raise awareness of common pitfalls and help to make future work more rigorous.\nWhile poorly designed search spaces is maybe a problem that many people in the community are aware of, this is, to the best of my knowledge, the first paper that systematically shows that for several commonly used search spaces there is not much to be optimized.\nBesides that, the paper shows that, maybe not surprisingly, the training protocol seems to be more important than the actual architecture search, which I also found rather worrisome.\nIt would be nice, if Figure 3 could include another bar that shows the performance of a manually designed architecture, for example a residual network, trained with the same regularization techniques.\n\nA caveat of the paper is that mostly local methods with weight-sharing are considered, instead of more global methods, such as evolutionary algorithms or Bayesian optimization, which also showed strong performance on neural architecture search problems in the past.\nFurthermore, the paper doesn't mention some recent work in neural architecture search that present more thoroughly designed search spaces, e.g Ying et al.\nIt would also be helpful if the paper could elaborate, on how better search spaces could be designed.\n\nNas-bench-101: Towards reproducible neural architecture search\nC Ying, A Klein, E Real, E Christiansen, K Murphy, F Hutter\nICML 2019\n\n\nMinor comments:\n\n- Section 4.1 How are the hyperparameters of drop path probability, cutout length and auxiliary tower weight chosen?\n"}