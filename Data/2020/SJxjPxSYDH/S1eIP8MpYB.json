{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "-- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks. In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation. \n\nI am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed. \n\n--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don\u2019t believe 1 and 2 are equal.\n\n--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem. The following is the concern:\n\n--In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z. If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.\n\n--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms. \n\nIt is also not clear how the loss function proposed differs from that of the CDVAE, etc.  If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.\n\nAdditional feedback for authors (not part of the main decision reasoning):\n\n- What is dt in Algorithm 1 description?\n\nFigure 1:\n-typo \u201cimplmented\u201d\n-What\u2019s the 3d plot supposed to represent?\nDoesn't the classification loss have a dependency on the input condition?\n\n--What does a \"heavy classifier\" imply concretely? \n\n--\u201cRedundant weights\u201d seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).\n\n--The notation for the proposed parameters theta, theta\u2019, phi, phi\u2019 are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder. In later sections they use theta and theta\u2019 for encoder/decoder resp.\n\n-- \u201cWhen the encoder and decoder networks are sufficiently complex, it is enough to implement each the prior and classification network as one fully-connected layer\u201d \u2192 what do the authors mean \u201cwhen \u2026 networks are sufficiently complex\u201d or do they actually mean when the \u201cwhen the problem is simple enough\u201d?\n"}