{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a compression scheme for the convolutional layers of a neural network. The method is based on the observation that subsequent layers are correlated. The idea is that it is sufficient to encode the residuals of the convolutional filters compared to the previous layer, which is cheaper than encoding the filters by themselves.\n\nAs far as I know, this is the first paper to study the correlation between subsequent layers and try to exploit it for compression. It is an interesting idea that might be worth exploring.\n\nThe writing of the paper is good. It is easy to read and it is well organized. I communicates the ideas well. But I think the paper could be fit in the recommended 8 pages.\n\nI have two concerns regarding methodology. Firstly, the study of the correlation between layers (SVWH) is not rigorous. Eq. (1) is a vague statement saying that the distance between two subsequent kernels is likely to be less than the distance to any other. It is unclear what the distance metric can be, what the probabilities are taken over, whether these statements hold simultaneously for all i,j,u,v and for which datasets. A hypothesis like this should be more explicit and it should be evaluated against the null hypothesis. This hypothesis is the motivation for the whole paper so it should be thoroughly examined.\n\nMy second concern is the evaluation of the method. The baseline is 8 bit quantization and Huffman coding, which does not accurately represent the current state of the art in compression methods. I would recommend comparison against Deep compression or Bayesian compression as baselines. It is also unclear what method was used for quantization. Is it simple rounding or trained quantization as in Deep compression?\n\nOverall assessment:\nThe paper presents an interesting  idea, but it has shortcomings in technical writing and evaluation. It requires a revision before it is ready to be presented at a major conference."}