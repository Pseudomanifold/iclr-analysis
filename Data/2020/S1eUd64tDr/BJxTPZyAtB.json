{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to compress deep neural networks.\nThe authors first observe that kernels in adjacent layers have strong similarities. Based on this observation, the authors propose to quantize the difference between similar kernels instead of the kernels themselves. An inter-layer loss that pushes kernels in adjacent layers to be similar by minimizing their difference is also proposed.\n\nThe idea is simple and straightforward. However, the paper is not very well written. Many symbols and terms are used before been defined or introduced. Moreover, there are some typos even in important formulas, making the paper a bit hard to follow. Some questions are listed here:\n1. What is texture/non-texture and what are texture bits and non-texture bits in the abstract?\n2. The two sides in eq(1) are exactly the same, why there is a \">\"? What does this mean?\n2. What is c_i in eq(2)?\n3. In the whole network, is only the first layer not quantized? If so, from the third layer, are the weight kernels in each layer quantized based on the quantized weight kernels in the previous layer? \n\nThe clarification in the experiment section can also be improved. From Table 1, the proposed method either has a larger model size, or performs much worse than the baseline model. Moreover, the comparison with more recent state-of-the-art quantization models are not provided, thus it is hard to determine the efficacy of the proposed method.\n"}