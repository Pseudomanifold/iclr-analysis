{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The goal of this paper is to explore the relationship between the capacity of the VAE, how well it fits the data distribution and consequently its utility in detecting out of distribution samples. One conclusion from the Nalisnick et. al work regarding the observation that data from SVHN scores higher in likelihood than data from CIFAR on a CIFAR trained VAE is that \"SVHN simply \"sits inside of\" CIFAR-10\u2014roughly same mean, smaller variance\u2014resulting in its higher likelihood.\". Following on this observation, this work notes that the observation of an out of sample point being scored with a higher likelihood can also happen when the model lacks the capacity to learn the ground truth support patterns of the data.\nThe work focuses on Gaussian VAE and discusses three scenarios of model capacity and their relationship to out-of-distribution detection. Empirically, when repeating the out of distribution experiments of Nalisnick et. al using a VAE (but this time varying the capacity of the VAE), the paper finds that VAEs can detect out of distribution samples and that a model trained on CIFAR has lower error on CIFAR samples than SVHN samples. The model verifies that this phenomena occurs due to the negative log likelihood term of the high-capacity model being much lower than that of the low-capacity model.\n\nOverall, I found the paper both interesting and well written. The contributions of this work are to show that the ability of deep generative models to detect outliers\nis closely tied to the capacity provided to the model.\n\nI have a few questions/concerns about the work that I would like clarity on:\n* One of the concerns I have is regarding the choice of architecture to test the VAE. Ostensibly, a way to make the point that this work does that continues on from the literature would be to start with the same VAE that Nalisnick et. al use and then proceed to show that their story changes when increasing the capacity of the underlying generative model. Was this experimental option considered? Essentially, does the experimental evidence herein suggest that the observation in Nalisnick et. al for VAEs was due to the use of a low-capacity VAE?\n* How does varying the choice of conditional likelihood function in the decoder from Gaussian to Categorical, or product of Bernoulli (distributions wherein there is no gamma parameter to consider during learning) change the story?\n* It is a little unclear what role Section 3.3 plays in the overarching claim being made that increasing model capacity (without overfitting) improves the ability of the model to detect outliers.\n* Do you have a sense of how much modeling capacity is *enough* capacity? Or will the answer to this question always depend on the contrast distribution that is considered \"out of sample\"?\n"}