{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes merging the 'Rapidly-exploring Random Tree' algorithm and RL to inform exploration. I think this is an interesting and novel idea, and the paper is quite well written, although some issues remain.\n\nFirstly, is this algorithm applicable to all RL problems? The assumptions seem quite strong, and furthermore it assumes the existence of a 'goal' state, which is not always the case. I would have liked to see the performance on more challenging domains, like the atari suite, to be fully convinced. I think the question of where and when this algorithm is appropriate needs to be significantly expanded upon.\n\nFurthermore it seems that in a more challenging domain the memory requirements would explode, is that correct? In some sense it seems similar to this paper: https://arxiv.org/abs/1606.04460 which should be discussed.\n\nThe name 'Rapidly Randomly-exploring Reinforcement Learning' is a bit jarring since there are no theoretical guarantees about the regret of this approach to indicate that this actually is 'rapid' exploration, at least in the sense that people use it in RL (eg, being efficient with respect to data in the exploration-exploitation tradeoff).\n\nIn algorithm 1 there are several confusing aspects:\n\n* I don't understand what is meant by \"project srand onto Fg\", what is the projection and how is it performed?\n* How do you \"find nearest node to srand in T\"? How is nearness measured? Note that in RL two similar states (eg l2 distance) may be very far apart by many metrics.\n* What is \"\u03c0l(snear, srand \u2212 snear)\"? How is it parameterized? I see the discussion in the appendix but I think it needs more discussion up front.\n* \"update \u03c0\", update how?\n\nDo the theoretical results rely on non trivial measure for the goal states and / or compactness? I'm thinking of an example of a single point (ie measure zero) being the goal state and RRT trying to search for it in a continuous state space, how can it guarantee it will find the measure zero point? It seems the discussion about intuition is lacking here.\n\nI was surprised to see no mention of 'L\u00e9vy flights', which seem distinct but related, and appear to provide a similar exploration heuristic in animals. It would be good to add a discussion about the similarities and differences to this.\n\nAfter SGD I would include a citation to Robbins\u2013Monro, as well as the Leon Bottou one.\n\nIn the related work section you should mention the use of Bayesian utility theory to guiding exploration, e.g., https://arxiv.org/abs/1807.09647, also some of part of this recent book draft may be relevant: http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf"}