{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Reinforcement Learning with Probabilistically Complete Exploration\n==========================================================\n\nThis paper proposes an exploration technique for planning from simulator.\nRoughly speaking, the algorithm uses some initial budget to sample random states and generate some effective demonstration trajectory.\nOnce this trajectory is found, it can be used to form an initialization for a policy gradient method.\nThis leads to improved performance on some mujoco tasks.\n\n\nThere are several things to like about this paper:\n- The problem of efficient exploration in large-scale RL is a big outstanding hole. Particularly finding methods that are compatible with state of the art policy gradient approaches.\n- The proposed algorithm is sensible, and seems to use a reasonable heuristic from planning to generate good \"kickstart\" for policy gradient methods.\n- The general quality of the writing and presentation is pretty good.\n- It's great to see code released.\n\nHowever, there are some places where this paper falls down:\n- Assumptions 1 (and particularly 2) are *not* part of the standard RL problem... but actually show that this is a proposal for planning given a simulator. This is a different problem setting and the distinction is really not clear from the first (several) pages. Further, although the assumptions are stated clearly, I think that this leads to some unfair comparisons (even if the sampled states are taken from the X-axis budget in plots).\n  - Note that this paper is far from the only one that is a bit sloppy on this distinction... and, of course, you can still use an RL *algorithm* to solve the planning *problem*... but it's not clear you can use a planning algorithm to solve the RL problem... and that's what this paper claims... but then Assumption 2 essentially just reduces the RL problem to the planning problem!\n- The claims of \"probabilistic completeness\" are not particularly insightful, in fact, the same is also true of Q-learning with epsilon-greedy dithering! The point of efficient exploration would be that you find this stuff quickly... and I'm not really convinced that this method always would. The quality of \\hat{a}, \\hat{b} seems like it should be very important... but I don't get much insight to that spelled out in the paper.\n- The computational evaluations are not particularly insightful, in that they seems to not give much insight into exactly what is happening. I also wonder whether they are really \"fair\" comparisons given the planning vs RL distinction.\n\n\nOverall, I think that there are interesting pieces to the paper, and the underlying algorithm is also interesting.\nFor me, the confusion between the planning and RL setting is impossible to move past... particularly since the exploration challenges can be distinct in these domains.\nFor this reason, I don't think the paper is ready for publication.\n"}