{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes an approach to automatically adapt the generation of synthetic 3D training data such that a downstream neural network performs well on real data. The idea is to adaptively modify the data generation during training such that the network performs optimally on a validation set. The authors propose a gradient-based optimization pipeline, where they unroll the intermingled data generation and training process and compute approximate gradients for the non-differentiable rendering module using random search.\n\nSimilar approaches have been proposed before, the main idea that sets it apart is random sampling in black-box modules in order to get an estimate for the gradient and to combine this with analytic gradients wherever they are available. I'm a bit surprised that this \"hybrid gradient\" hasn't been used before, but a quick literature search seems to confirm this claim.\nMy main concern with the approach is limited scalability. Random search gets harder for higher dimensions. The scalability issue seems to surface already in this work: The face model experiment would feature 400 rendering parameters, but the authors chose to do dimensionality reduction (I assume in order to make the problem tractable with random search). One would argue that any simulation that can transfer to real data reasonably well will likely need orders of magnitude more parameters than what is handled here. The observation is also in line with the other experiments: The grammar for NYUv2 features camera pose and the poses of a handful of shapes and the CSG experiment is very simplistic.\n\nThe claim that none of the networks has seen a single training image (except for validation) in the NYUv2 experiment is misleading. Clearly, the validation loss provided an error signal throughout the (meta-)training of the approach (c.f Figure 2). In general, the validation set will leak information into the training in this approach. It is thus not surprising that some of the baselines which can't leverage this information (e.g. random \\beta) will perform worse. \n\nFinally, it doesn't seem to be a good idea to only train on synthetic data (when doing computer vision). I'm missing more realistic experiments and comparisons. How would the approach fare against a modern monocular depth estimation model (for example MegaDepth)? Why only evaluate the normals and not also evaluate the depth maps? Most importantly: Can the model ensure that synthetic data provides value beyond, or in combination with, real data?\n\n\nSome parts of the paper are unclear:\n\n- For how many steps is the training loop unrolled? Can you discuss any limitations that the unrolling imposes in practice. E.g. would limited memory be an issue with larger networks and more unrolling steps?\n- It is not clear to me what the \"Basic Random Search\" baseline in Table 2 and Table 4 is exactly. \n- NYUv2: What does it mean to initialize the network using the original model in Zhang et al. 2017? Is that the pre-trained network, or the network architecture initialized with random weights?\n- Section A.2. mentions using RMSProp for the intrinsic image dataset. Was RMSProp only used on this dataset? If so, why?\n\n\nProblems with citations:\n\n- Citing Mania et. al. 2019 for the random search procedure seems like a misattribution. Random search as a DFO method is very old [1], and getting an estimate of the gradient via random sampling has been explored in multiple other work [2,3].  \n- Missing citation: [4] is close to the presented work and should be cited and adequately discussed.  \n\n\nTypos: Page 3, 2nd paragraph \"the generated the\"\n\nSummary: The paper proposes a simple, (to the best of my knowledge) novel technical trick, but it is not clear if the approach would provide value in practical settings.\n\n[1] J. Matyas. Random optimization. Automation and Remote control, 1965\n[2] A. D. Flaxman, A.T. Kalai, and B.H. Mcmahan, Online convex optimization in the bandit\nsetting: gradient descent without a gradient, ACM-SIAM Symposium\non Discrete Algorithms, 2005\n[3] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations of\nComputational Mathematics 2017.\n[4] N. Ruiz, S. Schulter, and M. Chandraker. Learning To Simulate, ICLR 2019"}