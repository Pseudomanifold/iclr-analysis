{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes three self-supervised tasks for learning general audio representations. Inspired by word2vec, the paper introduces one task that predicts neighboring segments given the center segment and another that predicts the center segment given the neighboring segments. The third task is to predict the time difference between two segments. The model is based on a 5-layer convolutional neural network. Experiments are done on several general audio tasks, including keyword spotting, speaker classification, language identification, and audio classification, and pre-training is done on AudioSet. The results are positive, better than Mel spectrograms and worse than the supervised counterparts.\n\nI am giving the paper a score 3. The paper includes a comprehensive comparison, and the baselines are all solid. The weakness of the paper is the lack of novelty, and reasons are given below.\n\nThe tasks, except the one that predicts the time difference, are similar to (Milde and Biemann, 2018). To be fair, (Milde and Biemann, 2018) evaluates their models on speech tasks rather than general audio tasks. However, the novelty is limited if the paper simply ports these models from one set of tasks to another. The results, showing that pre-training helps for many downstream tasks and that fine-tuning helps further, are not surprising. What is missing are why this is the case and what the models are actually learning from different tasks. It will be great if the paper can analyze/discuss what is being learned by the self-supervised tasks and why.\n\nA more concerning problem in the paper is the choice of tasks. As can be seen from Table 2, LSP, MUS, and BSD are probably too simple for evaluation, because there are little to no gaps between the baselines and the supervised networks. In particular, LibriSpeech is not even a suitable data set for speaker identification.\n\nAnother thing worth mentioning is that, though the results are all strong in Table 4, they have not surpassed the supervised results. It seems to suggest that despite all the effort, there is no reason to prefer pre-training on more data.\n\nOne quick question:\n\nEncoder fine-tuning: ...\n--> How do you decide which epoch to stop pre-training, where to start fine-tuning, what optimizer, how the optimizer is initialized, and what learning rate to use?\n\nUnspeech: unsupervised speech context embeddings\nBenjamin Milde, Chris Biemann\nInterspeech, 2018"}