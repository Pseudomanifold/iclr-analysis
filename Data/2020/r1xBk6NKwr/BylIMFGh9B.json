{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "*What is this paper about?*\n\nThis paper proposes a new unsupervised (or self-supervised) learning method to get general purpose audio representations. It proposes three main self supervised tasks, two of them analogous to word2vec\u2019s SkipGram and CBOW. The third task is related to predicting the distance between two audio segments. The authors then test this representations in a range of downstream tasks, showing an improvement when compared to untrained representations.\n\nShort review:\n\nThe techniques proposed in the work are simple and straightforward, which in itself is neither good nor bad. The fact that they present satisfactory results, though, is interesting and it is nice that the authors test them in a range of 6 different tasks. There are a few simple extra experiments which would have increased the impact of this work though. The first of these is that the authors didn\u2019t run any experiments integrating the multiple loss functions. More details bellow.\n\nContributions:\n\nThis work proposes a new learning method to get representations for audio sequences with no labels. He then tests these methods in a range of 6 different downstream tasks.\n\n*What strengths does this paper have?*\n\nThe paper is very well written and easy to follow. \nAuthors propose simple techniques and run a nice range of experiments to evaluate them. \nThey also compare to meaningful baselines, making it easy to asses their method\u2019s performance. \n\n*What weaknesses does this paper have?*\n\nMy main criticism would be that the authors did not run a few cheap and straightforward experiments. They mention in both the main results and conclusion sessions that it would be nice to test merging representations. Simple concatenation of the embeddings would be a very simple experiment to run which would fit nicely in this paper (and maybe not in a future one). They could also have trained the representations with a hybrid loss function combining their already implemented tasks (audio2vec, temporal gap and triplet loss).\n\n*Detailed comments:*\n\nThe reference to \u201cRepresentation Learning with Contrastive Predictive Coding\u201d is wrong. Probably the authors names are not correctly split in the bibfile. Also, the paper is from 2018, and not 2019.\n\nThe text in Figure 1 is very small and hard to read.\n\nIn the Temporal gap explanation the authors say \u201cwe ask the model to estimate the absolute value of the distance in time between two slices sampled at random from the same audio clip\u201d, but then they define \\delta as a normalized distance. \n\nIn Figures 2 and 3, it is very hard to see the lines for Spectrogram and MultiHead.\n\nDo the authors have an intuition for why the finetuned Audio2Vec representations does not fully bridge the gap with respect to the Supervised model?\n\nDo the authors have an intuition for why the AutoEncoder benefits more from the non linear experiment in Table 5?\n"}