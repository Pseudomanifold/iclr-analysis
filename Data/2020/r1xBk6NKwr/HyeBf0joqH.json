{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents two self-supervised algorithms for representation learning from audio, with artificially created supervised tasks. In particular, audio2vec borrows the ideas of CBoW and SkipGram from word2vec, and TemporalGap predicts the temporal distance between two segments. There is an ongoing theme of self-supervised learning in the machine learning community, with the goal of learning general features from large amount of unsupervised data with auxiliary tasks that may correlate with downstream tasks, so as to reduce the sample complexity for downstream supervised learning. The topic of this paper is quite relevant to the community. \n\nThe intuitions behind the algorithms do seem plausible to me; after all, word2vec was very successful for NLP. My concerns of this submission are on the execution of the ideas, and the empirical analysis.\n\n1. While the intuition behind representation learning for audio is to capture the temporal structure/contextual information, the proposed algorithms are not demonstrated on downstream tasks that require such information the most, e.g., ASR. The authors argued that \"Note that the choice of the tasks used for the evaluation is consistent with the selected temporal granularity. As such, we do not consider automatic speech recognition tasks, which generally require process much shorter temporal slices.\" But this argument is not convincing, as the large receptive fields of the architecture (T=975ms) is not an issue,  you just need to reduce the hop size (to sth like 10ms) to get per-frame features for ASR. The potential impact of the work could be higher if demonstrated on more significant audio tasks. \n\n2. For the downstream supervised tasks presented in the paper, the baselines seem a bit weak. There was not much details on the \"naive-Bayes\" classifiers for aggregating segment (with duration 975ms)  predictions to sample-level prediction, and how this choice is justified. I could imagine using a LSTM-based aggregator. Furthermore, if only an utterance-level feature is needed for the downstream tasks, one could even use the features learned by the method of Jansen et al, ICASSP 2018 (in fact their features are learned from AUDIOSET as in this submission).\n\nI am not sure if all tasks/datasets presented here are standard benchmarks; the authors shall try to provide the state-of-the-art performance for them. Ideally, the results shall be close to state-of-the-art with reasonable model architectures. Additionally, some of the tasks seems not very challenging: baseline linear classifier can achieve almost perfect performance on LSP?\n\nAlso, for encoder fine-tuning, it appears fine-tuning the top layers could significantly improve the performance, without increasing model complexity. Then why do not we always fine-tune all layers for downstream tasks, instead of using just linear or shallow networks on top of the features (freezing the parameters of the feature extraction networks)? In fact, in a pre-training + fine-tuning scheme, we would hope representation learning improves the generalization performance of downstream tasks. The experimental results of \"retrain_last_3\" does not show such improvement yet.\n\n3. Some of the experimental results contradicts with my intuition and perhaps more analysis is required to explain why. For example, while I can see that Librispeech as the feature learning dataset is inferior for non-speech audio event based tasks, it is not clear why it does not help on more human-speech related tasks (SPC and LSP, and in fact representation learning on Librispeech does not help LSP?)\n\n4. The authors argued in a few places the choices are suited for \"on device\" federated learning. I think it should be made clearer at the beginning of the paper the problem setup, and how it constrain learning (smaller model size, low computational budget, or limited communication bandwidth etc al). In any case, the justifications seemed somewhat contrived to me. If the goal is to learn generally useful features suited for multiple downstream tasks, I do not expect this can be achieved with a very small model. Besides, there are other ways of making models small, e.g., knowledge distillation or model quatization etc. \n\n5. Impact of encoder architecture size: when you increase the model complexity, should not the \"supervised\" baseline also improve? Are the results in Table 4 comparing against the stronger \"supervised\" baseline or the one in Table 2?\n"}