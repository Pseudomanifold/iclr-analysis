{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a novel way of augmenting a seq2seq model by an external memory module. The obtained system achieves promising results on the PubMed datasets.\n\nAlthough the paper is clear-written and the idea is well-received, I found the idea proposed is relatively incremental, given the advances brought by the end-to-end memory network. The experimental improvement also seems a bit marginal, compared to the chosen baseline model.\n\nThe ablation study is necessary for architecture like this. The result of the regularization term is interesting.\n\nIn theory, the architecture can be plugged in any seq2seq system. Would this technique help to enhance other tasks, such as machine translation or other?\n\nWith that being said, I'd recommend trying adding the memory module to other architectures and experiment it on tasks with more variety.\n\n"}