{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents a method to reward agents via an internal reward signal that encourages curiosity. The curiosity module uses a conditional variational autoencoder which enables the computation of Bayesian surprise. The Bayesian surprise is then used as an internal reward signal that is added to the usual external reward signal. This signal is supposed to help the agent learn the task at hand.  The authors provide experiments in high-dimensional settings comparing the proposed approach with  a competing approach (ICM).\n\n\nI vote for rejecting this paper mainly for the weak experimental validation that is unable to answer if the proposed method is more performant than the baseline ICM. \n\nSupporting arguments:\n\nThe main contribution of the paper as stated by the authors is to use a CVAE and Bayesian surprise. This approach differs from the baseline ICM since in ICM the intrinsic reward is the prediction error. So, the objective of the authors should be to show that, indeed, their idea of combining CVAE and using Bayesian surprise proposes some benefits compared to the baseline. Since this should be the main goal, strong experimental validations should be performed to support their proposed method.\n\nThe experiments have been performed only using 3 seeds. This would be enough if the results of the proposed method would be clearly better in terms of performance when comparing to ICM. However, the two methods seem to have extremely similar performances. For example in table 1 most, if not all, standard errors are overlapping for all methods. This highlights that the results might be the consequence of stochasticity in training/evaluation. See also for example, Figure 2, Riverraid, Breakout, Gravitar, BeamRider, Pitfall, and SpaceInvaders, all learning curves are extremely noisy and unstable. \n\nDue to this poor evaluation one cannot conclude if the proposed approach is beneficial or not. \n\nImprovements:\n\nIn order to improve the paper the authors should be able to at least experimentally validate (in a significant manner, e.g. more seeds or few seeds but highly performant when compared to the baseline) that the proposed approach has some benefits. Those benefits could be in terms of performance or any other characteristic that they might find relevant for the reinforcement learning problem.\n\n"}