{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "\nThis paper proposed a new method for knowledge distillation, which transfers knowledge from a large teacher network to a small student network in training to help network compression and acceleration. The proposed method concatenate the first a few layers of the student network with the last a few layers of the teacher network, and claims the gradient directly flows from teacher to student, instead of through a KL or L2 similarity loss between teacher and student logits. \n\nThe experimental results look good, and extensive experiments have been done on CIFAR, ImageNet and PASCAL VOC. \n\nHowever, the description of the proposed method looks rather unclear. First, the \u2018front\u2019 and \u2018back\u2019 part of networks are very vague. I have to guess that is the first a few layers of student and last a few layers of teacher. And it is still unclear how many layers in student and teacher are concatenated to form the \u2018collaboration network\u2019. How could the authors connect the two subnetwork with different structures?\n\nIt is unclear to me why proposed method is better than AT, FT or FitNets. It looks to me the proposed method use an ad-hoc selected layer to transfer knowledge from teacher to student, and the transfer is indirect because it has to go through the pre-trained subnetwork in teacher.\n\nMinor issue, FT and AT are not defined when they first appear in page 1. \n\nCould the authors show the student and teacher accurayunder standard supervised training in the result tables?\n\nSeveral related works are not discussed, such as\nXu et al. 2018 https://arxiv.org/abs/1709.00513\nBelagiannis et al. 2018 https://arxiv.org/abs/1803.10750\nWang et al. 2018 https://papers.nips.cc/paper/7358-kdgan-knowledge-distillation-with-generative-adversarial-networks"}