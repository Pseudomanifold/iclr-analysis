{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overall the method proposed in this paper is simple but effective, and adequate experimental results are given to show its performance improvements.  However, the literature survey of this paper is not satisfactory.\n\n1. To reduce model size, there are several different ways including efficient architecture design, parameter pruning, quantization, tensor decomposition and knowledge distillation. The authors forgot to mention tensor decomposition and mixed it with efficient architecture design. As for parameter pruning and quantization,  many important papers are missing.\n\n2. Utilizing the \"soft targets\" to transfer knowledge from teacher to student model is not first proposed by Hinton et al. (2015). To the best of my knowledge, it is first proposed in  \nJ. Li, R. Zhao, J.-T. Huang, Y. Gong, \u201cLearning small-size DNN with output-distribution-based criteria,\u201d Proc. Interspeech-2014, pp.1910-1914.\n\n3. Leveraging back part of teacher model's guidance to improve student performance has been investigated by other researchers on OCR tasks in \nDing H, Chen K, Huo Q. Compressing CNN-DBLSTM models for OCR with teacher-student learning and Tucker decomposition[J]. Pattern Recognition, 2019, 96: 106957.\nThey combine student's CNN with teacher's DBLSTM to learn better representations.\n\nIn conclusion, I will give a weak reject currently, unless the authors improve their literature survey and modify their claims.\n\n\n\n"}