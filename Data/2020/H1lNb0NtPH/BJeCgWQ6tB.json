{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "So this paper is interesting. It's sort of pursuing a similar path as recent works that use neural networks to evaluate (e.g., inception score, FID), notably those that optimize some lower bound of an information measure (e.g., MINE). In this case, the setting is \"datasets\", and the thing they are trying to quantify is the difficulty of the dataset as expressed by a lower bound to the lowest possible probability of the 0-1 error, which they should is related to the conditional entropy of the underlying input / label random variables (which makes sense). This direction seems very useful, and using neural network optimization to attempt to crack defining \"dataset complexity\" or \"difficulty\" seems a worthwhile venture.\n\nThat said, I have some (potentially serious) concerns about some of the assumptions and approaches that may harm the validity of the proposal / results.\n\n1) The smooth discretization property assumption seems to directly contradict results from adversarial examples, as these examples precisely a consequence of lacking smoothness. It seems that some constraints on the family of f are necessary to ensure this property (e.g., Lipschitz continuous using weight clipping, gradient penalty, spectral norm, etc).\n\n2) The MINE estimator you use is asymptotically unbiased, and represents a *upper bound* of a lower bound of the KL when the number of samples is \"small\". The bound found in f-GAN is unbiased, yet tends not to perform well. I would also consider estimators found in [1] or use the bias-correcting found in MINE.\n\n3) I feel like the model-agnostic claims are weak, due to the use of a particular family of functions used in the estimation part. Inductive bias must play a part in this, and some datasets \"difficulty\" is intimately connected to the class of functions we optimize our classifiers over. Some of these datasets may work better with some inductive biases, so it would be worth also looking at scores using convnets / LSTMs / transformers.\n\n4) It might be better to relate \"difficult\" to real measures found in the literature. For example, one could remove examples known to be misclassified, etc. It would be good to see that removing these also lowers the DIME score.\n\nOther comments:\nEq 2: m mysteriously appears\npage 6, first paragraph: not normalizing the input worries me quite a bit. Did you show that the score is independent of normalization? This might have real consequences in conjunction with weight initialization, learning rates, etc.\nFigure 2: it is good to see that label corruption correctly correlates with DIME score.\nTable 1: it would be good to report a correlation between DIME and SOTA error.\n\nFull disclosure, I did not go through the proofs in the appendix.\n\n[1] On Variational Bounds of Mutual Information"}