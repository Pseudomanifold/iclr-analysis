{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to distill the information from the original, larger dataset to a smaller dataset. The distilled dataset is computed by backpropagating through the gradient descent optimization procedure of a model using techniques of hyper-gradients. The author analyzes the lower bound of the size of the distilled dataset for linear regression when one gradient descent is used, which motivates the development of approaches using multiple gradient descent. In experiments, the authors compared to several baselines of distilling the dataset and showed that their approach has better performance. Additionally, the authors demonstrated the usefulness of dataset distillation in a continual learning setting.\n\nThe idea of distilling the information from a larger dataset to a smaller dataset can be potentially very useful in speeding up the training process of machine learning models and improving interpretability. This paper makes a nice first step on this direction. However, I cannot recommend accepting this paper because the proposed distilled dataset seems to strongly correlate with a specific model architecture and specific distribution of the weights of this architecture. This severely hurts the usability of distilled datasets. A dataset is model agnostic, and a distilled dataset should ideally also be model agnostic.\n\nThe significance of experiments (especially in section 4.1) is questionable for several reasons:\n\n- The experiments on fixed initialization are nothing but a sanity check. When the distilled dataset is computed from a model with a fixed initialization, the reported results are basically a training performance and have nothing to do with \"generalization\", and will be successful as long as the optimizer performs well. \n\n- The experiments on random initializations are limited. Unless I misunderstood, the random initializations all have the same distribution for models used in training and test. Since the training and test use the same distribution of random initial weights, it is naturally expected to see that models initialized with a fixed distribution of random weights perform well on the distilled dataset. For this reason, I don't think the experiments on adapting pre-trained models to another task with distilled dataset is meaningful. In practice, the distilled dataset will be used to adapt many models, and it is hard to believe that these models are all of the same architecture, and trained on the same dataset with different random weights. \n\n- It is possible that I miss the information, but the authors did not seem to mention how the 2000 different pre-trained models are obtained (even in the appendix). \n\n- The results in Table 2 are not that impressive. Simple baselines such as k-means++ can perform almost as good as the distilled dataset using hyper-gradients using randomly initialized models.\n\nSome suggestions on improving the paper:\n\n- Focus more on experiments that demonstrate the usability of distilled datasets, such as the continual learning experiment. It is more informative if all the comparison to baselines is done in the continual learning setting.\n\n- The analysis on linear regression is nice. To motivate the multiple gradient descent steps approach better I think it is worthwhile to also have some simple analysis on the lower bound when using multiple gradient descent steps for linear regression. The proof should be straightforward and comparing the lower bounds can be very informative.\n\n- In the title of 4.1.1, 'FRO' should be 'FOR'\n\n- The fourth line in the first paragraph of section 4.1.1 is ungrammatical. \n "}