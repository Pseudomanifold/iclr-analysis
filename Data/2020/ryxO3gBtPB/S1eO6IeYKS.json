{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to distill a dataset into a small set of artifical images, which can be used to then train a neural network. The proposed method of doing this is based on a meta-learning flavored formulation where a GD step is done on the synthetic images and then the loss computed  on the real images. The procedure appears to overfit to the initialization of the classification model and thus to resolve this the authors randomly resample the initialization throughout training. Experimental results compare the method to other coreset construction strategies and random sampling. Applications are also shown to continual learning.\n\nI think the general idea of this paper and the questions it poses are very interesting  and in fact a rather underexplored area however the experimental results are quite underwhelming. The original questions posed in the paper are not really answered (can we distill a dataset?). The experiments only demonstrate that an expensive procedure that trains a model can produce synthetic images that give higher performance than a randomly selected set of images. \n\nComments regarding experiments from Sec 4.1\n\nThe results indeed show the performance is greatly improved over using random images from the dataset, however obtaining 45% on CIFAR-10 would hardly be considered \u201cdistilling the dataset\u201d as the full set of data yields performances of 90%+ on this task. I would be more interested to see how many samples are needed to distill the dataset such that it reaches reasonably performance. It is not completely clear why the authors didn\u2019t do this kind of analysis but it might be due to the heavy computational cost of this method, in which case it would be good to discuss that. Similarly for MNIST the performance is quite low (effective use of regularization  and can yield similar performance with a 100 images), kmeans++ based selection gives quite close results to the synthetic procedure for MNIST. I think its also a bit misleading that the authors advertise the fixed initialization performance in the abstract, this performance in my mind is mainly useful for the purposes of analyzing the method.   \n\nI would also be interested to know if the coreset construction baselines can be improved. For example the kmeans++, why couldnt it be done in the feature space of a trained model (since synthetic method also trains a model). I am not an expert in the area of coresets but it would be good to know what is the state-of-the-art. A potentially interesting coreset consturction approach that also requires training a classifier can be found in this ICLR 2019 paper https://arxiv.org/pdf/1812.05159.pdf\n\n\nComments regarding experiments for the continual learning application:\nI posted a separate comment regarding this set of experiments, which I hope the authors can address. Currently it is not 100% clear what is done. For now I will go under the assumptions that the authors are ingesting the entire dataset for each new task, using this to get their distilled data. This seems like an unfair baseline to me for the GEM setup which sees each data point only once and a for a single iteration. The authors introduce an additional step which requires multiple iterations over the whole dataset to obtain the synthetic set of images. This may be more appropriate for comparison in another continual learning setting. \n\n\nA few other observations/suggestions\n- In my understanding the label of the synthetic data is currently fixed. It would be interesting if this can also be adapted or alternatively if a fixed soft label (e.g 0.3 one class 0.7)  can be assigned.\n- I would be interested to know if the target architecuture were different would the results change?\n\n---\nAddendum: After writing this review the authors have responded to my concerns regarding the CL experiments and have begun to recompute some of the experiments using a more rigorous comparison with promising results on permuted mnist. If the CL section is clarified in the revised version (I suggest besides more decriptions in the main text to perhaps add an explicit algorithm block in the appendix) and the more rigorous comparison (single epoch to signl epoch) extend to cifar100 I will consider increasing my score as the method will be competitive for CL.  I do still think the non-CL experiments (which make up the bulk of the paper are somewhat underwhelming as discussed in the review)"}