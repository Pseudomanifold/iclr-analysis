{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Paper presents a sound method of compressing information about the dataset into a set of synthetic sample-target pairs, by the means of backpropagating through the optimisation process into the samples themselves (rather than to initial weights as MAML model would do). Due to optimisation similarities to MAML, it can suffer of similar issues - gradients vanishing as number of steps increases, paired with lack of representational power (in non-linear case) when the number is small. Despite these issues, authors present empirical evaluation on various tasks, both focusing on transfer and continual learning. It is an interesting \"discriminative replays\" alternative to \"generative replay\" models present in the literature (where \"discriminative\" denotes that the replay is not created to generate original data, but rather to achieve loss decrease directly).\n\nIf it was a journal submission, I would recommend \"minor revision\", thus I am voting for a weak accept of this paper.\n\n\nSome material/presentation concerns:\n\n# Overall\n\n- considering a \"fixed initialisation\" scenario is arguably not important, as if one could store the initial network weights, one could also just store \\theta* and have an empty \"synthetic dataset\" to train on. This setup makes perfect sense if authors were to consider multi-task learning, where from one initial theta, one needs to be able to reach minimum of various tasks. Given that this is not the focus of the experiments, I would suggest de-emphasis of these results, or even complete removal. The more practical setups described in the paper are of enough value.\n- please describe what shaded region in Figure 3 means.\n- please increase font in Table 2, it is very hard to read after printing.\n- please increase font in FIgure 2 (b), it is completely unreadable after printing.\n- Authors state \"We do not share the same learning rates across steps as later steps often require lower learning rates.\" but do not provide any details of how these are selected\n\n# Continual learning\n\n- Experiments lack strong continual learning baselines, such as 'Continual Learning with Deep Generative Replay\" (NeurIPS 2017), or subsequent \"Generative replay with feedback connections as a\ngeneral strategy for continual learning\" (arxiv), or \"Progress & Compress: A scalable framework for continual learning\" (ICML 2018). In general, the newest baseline comes from 2017.\n\n# Theoretical section (3.3)\n\n- The bound provided is to some extent trivial, thus I would suggest moving the derivation to the Appendix (and leaving a simple statement, as a Remark, for both existence of one-step-convergent dataset, and for the M >= D), as currently it is presented in a way that might suggest more fundamental result - doing so would improve readability of the paper and help authors reduce its length to recommended 8 pages. What authors are showing is essentially that in linear case, for arbitrary target theta*, one needs to be able to control each dimension, which is a direct consequence of assumed \"independence\" of features.\n"}