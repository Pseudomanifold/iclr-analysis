{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper asks whether the choice of using English as the pivot language for training/generating cross-lingual word embeddings (WE) is empirically well motivated. The paper shows experimentally that choosing English as the hub leads to poor performance on tasks like lexicon induction. Another contribution is the triangulation method for providing dictionaries for all pairs of languages in a Lexicon Induction evaluation dataset (such as the one used in MUSE). Guidelines for how to choose hubs for training Bilingual WE/Multilingual WE are also provided. The finding that a hub language that is neither source nor target leads to good performance appears interesting. \n\nIntuitively, one can argue that the answer to the main research question of the paper is \"no\", because of reasons like linguistic distance, morphology differences etc., that are also noted in the introduction of the paper. A similar research question was asked and addressed in the recent paper by Alaux et al. (ICLR 2019). The paper should at least cite this work and discuss some of the key differences to make it clear what the contribution is. As as result, the current version of the paper makes a limited contribution. \n\nThe quality of the dictionaries obtained through the triangulation method is also not assessed. While the authors argued that doing so will be impractical for all language pairs, surely it can be done for some of them. This will inspire more confidence in the evaluations and the triangulation method.\n\nThe experimental section also can be improved. For instance, limiting to lexicon induction for drawing conclusions about word embeddings is error-prone, as argued in several papers (e.g. Glavas ACL2019, Ruder JAIR 2019 etc.). The experiments should be expanded to more tasks.\nThe experimental comparison is also limited to MUSE, and other popular methods like VecMap are omitted. Expanding here will also strengthen the claims of the paper."}