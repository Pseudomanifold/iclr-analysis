{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper critically examines projection-based methods for learning cross-lingual word embeddings, analysing the particular aspect of choosing a more appropriate hub language in both bilingual and multilingual learning setups. Through a series of experiments, the paper shows that depending on English as the hub language often leads to suboptimal bilingual lexicon induction (BLI) performance for a range of learning conditions with different languages. While the study is well executed in general, and it is good to see some reasonable high-level intuitions empirically validated, I believe that the novelty of the work remains limited, as all the key contributions have been known (or taken for granted) before.\n\n- Contribution 1: the choice of the hub. It is very intuitive to assume that English won't be the best hub for many multilingual setups, so the only contribution of the paper is validating this for the BLI task. Note that Heyman et al. (NAACL 2019) also did a preliminary study on the importance of choosing a hub language when doing MWE learning, so this paper is not the first study that focused on that problem. Furthermore, similar claims have been verified by a recent work of Alaux et al. (ICLR 2019), which is not even cited.\n-- Regarding the work of Heyman et al., the authors decided not to compare to that model because \"the order in which the languages are added is an additional hyperparameter that would explode the experimental space\". However, I believe that some knowledge from typological databases such as URIEL (used in this paper) can also guide the choice of language order for that model which would prevent the experimental explosion.\n\n- Contribution 2: general guidelines for choosing a hub language. The guidelines are well-known from prior work (multilingual systems are preferred over bilingual ones, typological information guides the selection of the hub), and they are again just empirically processed in an extended and more focused way. The results reported are as expected.\n-- I would like to see another experiment when it comes to choosing the appropriate hub language which would increase the novelty of the work. How important is the quality of the starting monolingual embedding space? That is, do the guidelines change if we e.g. have a low-quality English space trained on a smaller corpus compared to an English space trained on a very large corpus? On a more general note, imagine that you have detected language X as the best hub for the current setup? If we obtain (in a controlled envirnoment) a lower-quality embedding space for the hub language X, will X still remain the optimal hub choice for the same setup? What will happen there?\n-- Although the paper states that the general guidelines how to choose a hub language are offered, the paper fails to clearly stress how to do this in practice. I would like to see more explicit guidelines here. In addition, it would be interesting to measure how much we lose if we use the second or third best hub language instead of the optimal one. Is that really a problem or not?\n\n- Contribution 3: there has been more work on collecting parallel data, especially for lower-resource languages recently, such as the WikiMatrix project or JW300. Extracting training and test data from such resources is trivial and only a byproduct of that work. Therefore, I don't see it as a major contribution. Furthermore, using the PanLex dictionaries to get suitable data sets for resource-lean languages has also gained traction recently, and there are other sources that can be used.\n\nAlso, the work focuses only on the BLI evaluation, which as discussed by e.g. Ammar et al. (2016) or Heyman et al. (2019), or Glavas et al. (2019) should not be used as a single evaluation task to really establish meaningful comparisons between different embeddings. In order to fully understand the implications of choosing different hub languages in different settings, more experiments are neeeded related to downstream evaluation. It would definitely be very interesting to measure how the choice of the hub language can affect multilingual parser transfer or cross-lingual classification/NL inference systems, etc. I believe that those experiments are a must-have for the empirical paper liket this one. \n\nIn summary, while this study has merit as it empirically validates some very intuitive hypotheses, I do not see it making a great impact on the field of cross-lingual word representation learning (especially due to its limited novelty, see above), and the paper might be a better fit for a more NLP-focused conference.\n\nOther questions:\n- Why have the authors limited their choice of methods only to MUSE, since more robust and powerful methods have appeared in the meantime? Why are VecMap (Artetxe et al., ACL 2018) or RCSLS (Joulin et al., EMNLP 2019) not used as (stronger) baseline models?"}