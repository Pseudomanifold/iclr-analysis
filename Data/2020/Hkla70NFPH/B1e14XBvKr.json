{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper explores the importance of the pivot language when learning multilingual word embeddings. While previous work has been severely English-centric, the paper introduces a new dataset covering all combinations of 49 languages, derived automatically from the MUSE dataset through triangulation, and analyze the performance of two existing mapping methods in it for different choices of the pivot language.\n\nI think that the paper is well written and technically sound, and both the new dataset and the analysis done are valuable contributions. However, I am not convinced that it has enough novelty and substance for the venue: a similar point was already made by Alaux et al. (ICLR'18), although the authors do not seem to be aware of it, the dataset was derived automatically from an existing one using an error-prone (and rather trivial) procedure, the analysis is somewhat limited (e.g. covering only two mapping methods) and, while still valuable, the results do not seem very significant to me. More concretely:\n\n- The main point that the paper tries to sell is that the choice of the pivot language is important, and always picking English, as previous work did, is suboptimal. I have been actively working on this area, so when I read the title and the abstract I was excited to learn more about this finding. However, as I read through the paper, my excitement went down, and I was left with the feeling that the pivot language was not that important after all. In Table 2 and 3, the difference between English and the best configuration is around 1 point, which is rather small for this task. To make things worse, even that difference is questionable, as the best configuration is chosen in the test set (more on this below).\n\n- I think that picking the best configuration in the test set, and comparing that to one of the specific configurations, is methodologically problematic. To make an analogy, let's say that I want to claim that the random seed is important when learning cross-lingual word embeddings with a given method, and picking 5468721, as some implementation does by default, may be suboptimal. So I make multiple runs with different random seeds and show that, indeed, 5468721 is rarely the optimal random seed. This would not surprise anybody, as the same thing applies to pretty much any work in ML. So, coming back to your work, is the choice of the pivot language anyhow different from a random seed? I do believe that there is something more than just randomness in your results, but unless I am missing something, you are not controlling this factor, and the differences you report look too small to ignore it.\n\n- The whole idea of the paper is actually not very novel. Alaux et al. (ICLR'2019, https://openreview.net/forum?id=HJe62s09tX) also showed that mapping all embeddings into English was suboptimal, and proposed an alternative approach. The authors do not seem to be aware of this work, as they do not even cite it. I am not saying that there is nothing new or interesting in the paper, but this is clearly a weakness, as the work done is not put in context appropriately.\n\n- The evaluation is limited to two methods, and some influential systems, like VecMap, are left out. I understand that asking to evaluate dozens of methods would be not be reasonable, but I think that the limited scope of the analysis is a weakness if you want to make general claims on the limitations of previous work.\n\n- Somewhat ironically, the proposed dataset is automatically derived from the MUSE one in an English-centric way. The authors use a simple triangulation approach for that. This is error-prone, as the authors acknowledge and try to mitigate with a post-processing that detects and removes some wrong entries. To make things worse, previous work (Kementchedjhieva et al., EMNLP'2019) has also found issues in the original MUSE dataset, which your derived one will unavoidably inherit. For that reason, I am not entirely convinced about the value of the resulting dataset.\n\n- The paper focuses exclusively in Bilingual Lexicon Induction (BLI), which is common practice in this research area but has been recently questionned. Glavas et al. (ACL'2019) showed that BLI-centric evaluation was problematic and, as mentioned before, Kementchedjhieva et al. (EMNLP'2019) found issues in the MUSE dataset, which you automatically extend. This does not invalidate all your work, but I do see it as a weakness.\n\n\nOther details that did not influence my score but I think the authors should address:\n\n- The term \"pivot\" seems more appropriate than \"hub\". \"Hub\" is usually used with a different meaning in the context of cross-lingual word embeddings (in relation to the \"hubness problem\"), and \"pivot\" seems more standard for what you mean.\n\n- Iterative refinement methods are named \"MUSE-like\", which seems unfair, as this technique was not introduced by MUSE. Why not just say \"iterative refinement\" (or \"self-learning\")? Otherwise you should credit the original authors that proposed this idea, which I believe are Artetxe et al. (ACL'2018).\n\n- \"We hypothesize that learning mappings for both language spaces of interest (hence rotating both spaces) allows for a more flexible alignment which leads to better downstream performance [...] Note that this contradicts the mathematical intuition discussed in Section 2 according to which a model learning a single mapping (keeping another word embedding space fixed) is as expressive as a model that learns two mappings for each of the languages.\" Assuming that the mappings are orthogonal, this is not just a \"mathematical intuition\", it is a provable fact, so it does not make sense to propose a hypothesis that contradicts it. If you find a surprising pattern that you cannot explain, you should acknowledge it, but proposing a hypothesis that is known to be false does not make any sense.\n\n- This is somewhat subjective, but I dislike the term \"European language\" as it is used in the paper. It does not make much sense from a linguistic point of view (Turkish is not an Indoeuropean language), and it is even questionable from a geographical point of view, which should not be very relevant, anyway (e.g. English, Spanish and Portuguese have more speakers outside of Europe than within Europe, and most of Turkey is actually in Asia)."}