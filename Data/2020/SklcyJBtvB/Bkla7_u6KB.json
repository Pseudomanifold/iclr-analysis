{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work addresses the problem of off-policy evaluation in the presence of positivity violations, i.e. some actions are not observed in the logged policy. As the paper points out, positivity violations can lead to unboundedly bad estimates when employing IPS. The authors propose three methods to deal with this problem. The first uses only the observed actions, the second and third use extrapolation and augmentation to provide an approximation to the off-policy problem. \n\nI found a few pieces of this paper confusing. In section 3.2 it is proposed that a surrogate reward function be used for actions with unknown support, but the left hand side of the equation would seem to imply that the ratio still needs to be known in order to get an estimate. Perhaps an indicator function is missing? \n\nIt is also not made plain what assumptions are being employed in order to allow for extrapolation. From what I can tell, the authors are swapping out a positivity assumption with a smoothness assumption on the reward function. However, I don't think I see this spelled out within the text. \n\nOverall, I think this is a promising approach (the empirical results certainly bare that outO but to my eyes it lacks sufficient detail and specificity. "}