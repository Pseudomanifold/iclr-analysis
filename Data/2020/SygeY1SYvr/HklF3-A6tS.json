{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "<Paper summary>\nThe authors argue that the popular benchmark datasets, Omniglot and miniImageNet, are too simple to evaluate supervised few-shot classification methods due to their insufficient variety of class semantics. To validate this, the authors proposed clustering-based meta-learning method, called Centroid Network. Although it does not utilize supervision information during meta-evaluation, it can achieve high accuracies on Omniglot and miniImageNet. The authors also proposed a new metric to quantify the difficulty of meta-learning for few-shot classification.\n\n<Review summary>\nAlthough the main claim of this paper seems correct, it is not sufficiently supported by theory or experiments. My score is actually on the border line, but I currently vote for ``weak reject, because some points in the paper are ambiguous yet. Given clarifications in an author response, I would be willing to increase the score.\n\n<Details>\n* Strength\n + The paper is well-organized. Especially, the examples shown in the introduction greatly help understanding of what the authors argue in this paper.\n + A novel study on quantifying the difficulty of meta-learning.\n + The proposed CentroidNet performs well in the experiments.\n \n* Weakness and concerns\n - Does CentroidNet really work without labels during ``meta-validation\"? As far as I understand, ground truth clusters of the support set defined by the labels are required to compute the accuracies. Therefore, the labels seem to be required to validate the performance of the model. I think it should be ``meta-test.\"\n - The authors state ``The most intuitive way to train ..., we did not have much success with this approach\" in 5.3, but it is counter-intuitive. If the class semantics are similar among episodes, ``the most intuitive way\" should work, because it can learn the common semantics via meta-training. Further discussion about why it does not work is required.\n - The high performance of CentroidNet does not support the claim on the insufficient variety of the class semantics. According to ablation study, adopting Sinkhorn K-means is the most important factor to improve the performance. It means that adopting weighted average like in [R1] can also improve the performance of ProtoNet, which results in substantial difference in the performance between ProtoNet and CentroidNet that can deny the claim. \n - The definition of CSCC is not convincing. First, I could not get the meaning of ``unsupervised Bayes accuracy\" (supervised Bayes accuracy means 1 - Bayes error rate, right?). Second, CSCC seems to mainly quantify the importance of the supervision information during meta-learning, which is not directly related to the difficulty of few-shot learning problem. Intuitively, difficult few-shot learning problems should lead to lower supervised Bayes accuracy, which does not necessarily decrease CSCC. Third, what we can induce via comparing CSCC is not clarified in theory. The discussion in 6.3 is too subjective and specific for the case of training with ILSVRC/all datasets.\n - This paper lacks citing some closely related works [R1, R2].\n\n[R1] ``Infinite Mixture Prototypes for Few-Shot Learning,\" ICML2019\n[R2] ``A Closer Look at Few-shot Classification,\" ICLR2019\n\n* Minor concerns that do not have an impact on the score\n - Another arXiv paper related to this work: ``Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML\"\n\n"}