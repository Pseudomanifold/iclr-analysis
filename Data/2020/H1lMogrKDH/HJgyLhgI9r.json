{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "I wanted to first thank the authors for their work on connecting biologically inspired CWA to deep learning. The paper overall reads nicely. \n\nThe authors make the assumption that reviewers are fully aware of biological terminology related to CWA, however, this may not be the case. It took a few hours of searching to be able to find definitions that could explain the paper better. I suggest the authors add these definitions explicitly to their paper (perhaps in supplementary), also an overview figure would go a long way. I enjoyed reading the paper, and before making a final decision (hence currently weak reject), I would like to know the answer to the following questions:\n\n1. Have the authors considered the computational burden of Equation 3? In short, it seems that there are two summations (one for building the probability space over measure h) and one right before e_j. This is somewhat important, if this type of neural network is presented as a competitor to affine mapped activations. \n\n2. It would be nice to have some proof regarding universal approximation capabilities of CWA. In my opinion it is, but a proof would be nice (however redundant or trivial - simply use supplementary). \n\n3. I was a bit confused to see CWA+BN in the Table 1. In introduction, authors write \u201cBut CWA networks are by definition normalized and range-limited. Therefore, one can conjecture that CWA plays a normalization role in biological neural networks.\u201d Therefore, I was expecting CWA+BN to work similarly as CWA for CIFAR10. Please elaborate further on this note. \n\n4. Essentially, the CWA changes the definition of a layer in a neural net. Do authors see a path from \u201cCWA works\u201d to \u201cCWA works better than affine?\u201d. If so, please elaborate. Specifically, I am asking this question \u201cWhy should/must we stop using affine maps in favor of CWA?\u201d. Now this may or may not be the claim of the paper. It\u2019s ok if it is not; still showing competitive performance is somewhat acceptable, but certainly further insight would make the paper stronger. \n"}