{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This submission aims to improve unsupervised representation learning by combining several known methods into a single framework. The representation is tested on a few-shot learning task, by taking the nearest neighbor in the learned space. The combination of techniques results in impressive results.\n\nMy decision is reject, because of mainly two reasons: 1. The paper does not relate its method well to the literature. This leads to inadvertent claims of novelty. These missing key citations also means there is little discussion and analysis around what makes this work different from previous works. 2. The experimental evidence is sparse, with only a single table comparing against four previous papers. There is also little ablation studies and analysis.\n\nRelevant work:\n\nThe idea to add a \u201creconstruction loss\u201d to the vanilla GAN is similar to works like Adversarial AE [1] and BiGAN [2]. The latter is the only one cited in this paper, but only in the results table and there is no mention in related work. The formulation is not exactly the same though, but quite similar. For instance, if we call the x -> z network E, then this paper adds a reconstruction loss to L(z, E(G(z))) while [1] adds it to L(x, G(E(x)). This difference should be pointed out and discussed. Is there a benefit of one over the other? What about the BiGAN formulation? If this particular formulation does not offer any improvement, then this is not a novelty.\n\nRotation for the purpose of self-supervision from Gidaris et al. is not cited. The paper mentions that rotation has \u201cstate-of-the-art performance reported in the literature\u201d, acknowledging that this is not a novelty; however, a citation is missing.\n\nUsing object center bias (which is used in the masking strategy) has plenty of prior work, but there is no discussion around this. I\u2019m not sure what the key citations in this area should be, but searching the web for \u201cobject center bias\u201d yields several starting points. I believe this formulation with a triplet loss is novel, but it should be more clear how novel this is compared to prior object center-bias literature.\n\nExperimental evidence:\n\nThe core method itself does representation learning on any unlabeled dataset. This is tested using unsupervised N-way K-shot classification with nearest neighbor. A direct comparison with the related task of semi-supervised N-way K-shot is unfortunately not possible, so the number of comparisons to prior work is small. However, I don\u2019t see why this work couldn\u2019t be used in an unsupervised pretraining/classification fine-tuning setting that the self-supervised literature has embraced. Having a second results table that compares to this body of literature would make this work much more compelling, especially if the numbers are competitive.\n\nI think the paper could also use more ablation studies and analysis. It would be interesting to see ROT alone (it always good to compare with the original implementation), METRIC alone, and ROT+METRIC. It would be interesting to see numbers comparing Adversarial AE, BiGAN, and this method. There are many hyper parameters too, so an insight into how sensitive it is to get these right would be interesting.\n\nMinor:\n\nThe notation for D is a bit confusing. Sometimes D without subscript refers to the real/fake classifier, and sometimes to the reconstruction.\n\n[0] Unsupervised Representation Learning by Predicting Image Rotations - Gidaris et al. ICLR 2018\n[1] Adversarial Autoencoders, Makhzani et al.\n[2] Adversarial Feature Learning, Donahue et al. ICLR 2017"}