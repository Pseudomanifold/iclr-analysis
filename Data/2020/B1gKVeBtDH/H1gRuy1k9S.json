{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, a simple decomposition\u00a0for transformer models is proposed to speed up the computation for a specific type of problems, where the input has paired, such as a question and an answer for QA tasks. The basic idea is to encode the question and the answer with self-attention alone, and then apply the co-attention on top of them, so that the complexity is reduced from O(p+q)^2 to O(p^2+q^2). Such a decomposed transformer is trained by distilling from a full transformer, where the knowledge distillation loss, layerwise similarity loss and task loss are adopted as the objective functions. Empirical studies show that the proposed method can be up to 3.5 times faster than the full transformer (BERT) model, while only losing a small amount of accuracy.\n\n1. The paper lacks a large body of related and representative works in machine reading comprehension, where such a decomposition is widely used, such as BIDAF (Seo et al.), QANet (Yu et al.), R-Net (Wang et al.) among many others. If those works are properly acknowledged, the novelty in this paper will become slim.\n\n2. The improvement from O(p+q)^2 to O(p^2+q^2) is negligible. On one hand, they are of the same order, so there is no theoretical contribution. On the other, as the author acknowledges, the method can only work on pair-input tasks like QA. However, most of the questions in QA are quite short, so the terms related to q can be ignored and the only bottleneck is p^2, making the difference very minor.\n\n3. Since the method is essentially a distillation, the full transformer model is still needed for supervision. That is to say, it cannot work without the original large model.\n\n4. There are many ways for distillation. For example, one can just use smaller hidden size or a shallower network, or their combinations. Those are straightforward baselines but they are not mentioned or compared with in the paper.\u00a0\n\n\n"}