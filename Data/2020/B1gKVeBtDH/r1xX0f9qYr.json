{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors proposed an opinion that it may not be necessary to apply the sequence-wide self-attention over all layers in the BERT model. They proposed a decomposition to a pre-trained Transformer that allows the lower layers to process segments of the input independently, enabling parallelism and caching. The experimental results show that decomposition enables faster inference, significant memory reduction while retaining most of the original performance. \n\nI think the study problem in this paper is very meaningful. However, this paper has some syntax errors. I hope the author can read their paper carefully and correct grammatical mistakes. \n\nQuestions:\nIn Figure 2, the authors show the average variance of passage representations when paired with different questions at different layers. But I don\u2019t understand why the variance of the lower layer is smaller than, the higher one. Would the authors like to provide some explanations?\nThe authors mentioned that the information loss in the lower layers can be potentially compensated for by the higher layers. Would the authors like to provide some theoretical proof and experimental proof?\nThe authors used the Bayesian Optimization to tune the hyperparameters \\gamma, \\alpha, and \\beta, however, they don\u2019t post the results of the \\gamma, \\alpha, and \\beta in the experimental results. \nIn section 4.2, the authors said the search range is [0.1, 2.0] for the 3 hyper-parameters, which confuses me?\nIn Table 1, would the authors like to explain why the model performs differently in different datasets? Why in the QQP, the model achieves 2.0x Inference Speedup, while it achieves higher Inference Speedup in other datasets?"}