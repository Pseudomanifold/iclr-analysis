{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a way of reducing the computation of pre-trained Transformer models by splitting the lower layers into two parts: one corresponding to a question and one corresponding a passage. Auxiliary losses are added during the fine-tuning stage to recover the lost performance due to the split. While this method alone cannot bring more than 2x speed-up, the authors show that caching of question part when it has to be compared to multiple passages can bring additional saving in computation time. \n\nThe paper is well written and it was easy to understand. It has thorough experiments showing the benefit of the proposed method.\n\nHowever, I am doubtful of the impact of this paper. The original problem domain is quite limited and it\u2019s not clear if the method would be useful in other scenarios. The method is only relevant when one is using a pre-trained transformer model on two part inputs, and also one part is often have to be compared to multiple other parts. I think the paper can be made stronger by showing the method can be generalized and used in more diverse scenarios."}