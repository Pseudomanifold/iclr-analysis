{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers scaling distributed stochastic gradient descent to large number of nodes.  Paper proposes novel asynchronous variant to decentralized SGD, called PopSGD. It models asynchrony with the population model. Paper theoretically analyzes the proposed method and shows that in the convex case PopSGD has a linear speedup in the number of nodes compared to the sequential training on one node. However, in the non-convex setting, the PopSGD rate doesn\u2019t have a linear speedup. The paper also provides experimental evaluation of PopSGD where they scale PopSGD up to 1000 of nodes in the convex optimization; and also apply PopSGD to neural network training on ImageNet.\n\nMy score is weak reject. The major reason is that it is not clear how does this work theoretically and experimentally compares to the previous asynchronous variants of decentralized SGD (Lian et al. (2017b) AD-PSGD and Assran et al. (2018) SGP) or to the centralized SGD (parallel mini-batch SGD) baseline; and what are the benefits of the proposed method. \n\nConcerns that should be addressed: \n\n1. No theoretical and experimental comparison with the baselines (see above).\n\n2. Extension to the non-convex case: there is no linear speedup in the number of nodes n. Does this mean that it is better to use Centralized SGD (which has speedup in the number of nodes)? The comparison should be made explicit. \n\n\n3. The paper is a bit too long (10 pages) and contains some repetitions. Consider to shorten a bit. (e.g. procedure of splitting data between nodes was described twice on page 2 and 4; proof overview on page 6 could be merged with two steps on page 7)\n\n\n4. The procedure how to sample nodes uniformly was not discussed in the paper. Moreover, it is also not clear why \\Theta(n) updates could be done in parallel. When intersection happens, many nodes would have to wait for the previously selected pairs to finish computation.  \n\n\n5. Why are the local learning rates required? When scheduler samples the nodes uniformly it can also transmit them the global time count.\n\n\n6. In the description of data distribution (page 2, paragraph 4-5, page 4, last paragraph): what if there is more samples than available nodes? do the nodes exchange samples or only gradients? Is it possible to have part of the full dataset on every node without sharing it with anyone? \n\n\n7. Page 3, line 3: Lian et al. (2017b) and Assran et al. (SGP) also showed that they don\u2019t require global synchronization. \n\n\n8. Page 4, line 2: The AD-PSGD rate does have a linear speedup in the number of workers n, so the claim should be corrected. \n\n\n9. Page 7: all lemmas and theorems hold only for the global stepsize \\eta_t. Would it also hold with local stepsizes \\eta_t^i? \n\n\n10. Extensions for arbitrary graphs: would it be possible to have one theorem for all possible graphs and see how graph parameters (e.g. spectral gap or others) influence the convergence rate? \n\n\n11. Experiments: I didn\u2019t understand the definition of mult constant and what does it control. Re-phrase this paragraph. \n\nMinor comments: \n- page 1, last line of the paragraph 2: \u201cparameter obtained by node i at time t\u201c -> \u201cstoch. gradient obtained by node i at time t\u201c? \n- page 2, line 4: \u201cvariants(e.g.\u201d -> \u201cvariants (e.g.\u201d\n- Usually \\mu is used for strong convexity parameter. \n- page 3, paragraph 2, \u201cwe emphasize that convexity is not enough\u2026\u201d the purpose of this sentence is unclear, what is enough then or why is that important to know? \n- page 3, related work: Nedic at al. Nedic et al. (2017) -> Nedic et al. (2017). The same for the other citation. \n- page 3, related work: PP model is not defined. \n- How can PP model result in a multigraph? If two samples pairs have the same nodes, then they need to be processed sequentially, so it can be modeled with two graphs for different time steps. \n- Population protocol model (page 4): \u201cstates store real numbers\u201d -> can they store vectors instead? \npage 5, estimating time and the learning rate section: what happens if the V^i is equal to V_j? Who updates its value? \n- Figure 1(a) was not discussed at all in the text. \n- Page 6, Notation and preliminaries: why it is required that T = O(poly n) is not explained.\n"}