{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper provides a solution for stable and consistent optimization of a differentiable neural architecture search method (i.e. DARTS). Specifically, this paper introduces a grouped variable pruning algorithm based on one-level optimization and optimization complexity matching. The experimental results show that the proposed method leads to state-of-the-art performance on CIFAR-10/100 and ImageNet datasets.\n\n- This paper mentions that when we randomly select operations at stage 1, the final result is almost the same as that of the proposed method. How about the grouping of operations? When we randomly group the operations, how would the results change? It would be better to provide such results to clarify which parts of the proposed method contribute to the final performance.\n\n- If possible, it would be better to compare the proposed method with recent state-of-the-art methods such as XNAS, FairNAS, PCDARTS, and ScarletNAS. "}