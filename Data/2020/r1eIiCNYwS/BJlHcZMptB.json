{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper introduces a way to train transformer models over document graphs, where each node is a document and edges connect related documents. It is inspired by the transformer-XL model, as well as Graph Neural networks. They apply this model to answer multi-hop questions on the HotPotQA dataset and outperform the previous SOTA. The model particularly improves performance on the bridge style questions of HotPotQA. They are able to do this in a single step, rather than a multi-stage process as done by previous approaches.\n\nStrengths: The model is described in a very detailed manner with contrasts drawn to previous models, which provides excellent motivation for the decisions taken by the authors. I enjoyed reading section 2 as it very succinctly describes previous approaches and introduces transformer-XH. The paper has very detailed and insightful ablation studies, including hop steps and hop attentions, and other graph structures.\n\nWeaknesses:\n\nIt took me a lot of effort to figure out that the transformer-XH is only applied to the bridge questions, and part of the overall gains are due to a better retrieval pipeline on the comparison questions. Please explicitly make this clear. \n\nAlso, there seems to be a lot of gain even on single-hop questions, and its not clear if overall performance improvement can be attributed to modeling the graph structure, as opposed to other confounding factors. Can the authors elaborate a bit more on why this might improve performance on single hop questions?\n\nVery good evaluation on HotPotQA, but would be even stronger if this were applied to at least one other task/dataset.\n\nQuestions: \n\n1. Any intuition as to why the EM performance improvement on single-hop questions is the about the same as the performance improvement on the multi-hop questions (~5%)? \n\n2. In Example 2 in Table 4, it is not clear from the text as to why the BERT pipeline fails to get the correct result. If I understand correctly, both models use the same document graph construction method? Is this not the case? i.e. does the pipeline model have access to the exact same documents that form the Transformer-XH's document graph? Could you explain this cascade error here a bit more?\n\n3. I assume that we can use directed as well as undirected edges in the document graph? Would be good to clarify this. \n\n4. In equations 11 and 12, are you missing normalization operators to specify a distribution, perhaps a softmax?\n\n5. \"Our pipeline\" in Table 1 is confusing. Would be nice to mention in the caption that it is a baseline constructed using BERT on your retrieval method etc."}