{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper is proposing an extension of the Transformer matching and diffusion mechanism to multi-document settings.\nTo do so, the authors introduce a special representation for gathering document level information which is then used for propagation among documents latent representations.\nThe extension seems quite simple and natural.\nThe method is evaluated on multi-hop machine reading over the hotpotqa dataset in the Fullwiki settings.\nHowever, it could have made sense to evaluate the method in the distractor settings too.\nIn this context, the evidence graph where the model is trained is built using the canonical retrieval technique.\nThen, the method is using a pre-trained NER model to extract entities on the question and the candidate documents on Wikipedia for matching.\nFinally, a BERT ranker model is used to re-rank the retrieved candidate documents.\nThe proposed method seems to heavily dependant on this hand-crafted extraction process.\nUnfortunately, one concern is that the reasoning model, while been quite original, is not tested in large scale retrieval cases to assess its robustness.\nIndeed, the number of retrieved documents to create the evidence graph seems to not have been mentioned.\nThe method improves the current state of the art."}