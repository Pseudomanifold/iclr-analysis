{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper adversarially trains models against l_p norms where p is of there different values. They then propose a method which does somewhat better than the obvious way of adversarially training against more than one l_p perturbation.\nThe motivation for the paper is limited, in that they suggest previous works have suggested adversarial training itself \"overfits\" to the given l_p norm. This isn't surprising that it works, since the straightforward baseline works. They make it seem surprising by suggesting that ABS suggested adversarial training is doomed and cannot provide robustness to l_1, l_2, l_\\infty norms simultaneously. The other motivation is that this is a step toward studying an expanded threat model, but the authors have not demonstrated that the learned representations are any bit more robust to common corruptions (could the authors show the generalization performance on CIFAR-10-C or generalization to unforeseen corruptions?). Without further evidence, we are left to believe this only helps for this narrow threat model. Overall the paper is deficient in creativity and generality, so I vote for rejection.\n\nSmall comments:\n\n> take more time than a single norm,  it is a step closer towards the end goal of truly robust models, with adversarial robustness against all perturbations.\nPlease show model performance on CIFAR-10-C since if the model is more robust, it should hopefully be more robust to stochastic adversaries.\n\n> has claimed that adversarial training \u201coverfits\u201d to the particular type of perturbation used to generate the adversarial examples\nWouldn't this be that l_\\infty training fits specifically to l_\\infty examples, not that robust optimization cannot handle more than one norm at a time? Who is claiming that?\n\n> First, we show that even simple aggregations of different adversarial attacks can achieve competitive universal robustness against multiple perturbations models without resorting to complex architectures.\nI am not sure this was in doubt. The phrase \"universal robustness\" is misleading.\n\nHow were the budgets chosen for l_2 and l_1? Those values seem small."}