{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces an invertible deep generative model architecture for modeling molecular graphs. The model is based on graph residual flows (GRF), which is a graph variant of normalizing flows. The GRF model is a refinement of the GraphNVP generative model, which is also invertible,  but which does not seem to work very well for sparse low-degree graphs (such as molecular graphs). \n\nThe main new idea seems to be to replace the regular coupling flows of GraphNVP with residual blocks: the residual blocks ensure invertibility while also helping to \"mix\" the information in the (tensor) representations in each layer better. This leads to more compact generative models. Due to the residual structure of the encoding model, the inversion (backward mapping) is a bit more expensive since it requires solving a fixed point problem in each layer, but in principle it can be done provided the layer weight matrices have small spectral norm.\n\nExperimental results on two molecular datasets seem to confirm the validity of the proposed architecture.\n\nThe paper is nicely written and the techniques/results are clearly described. However, I have two main concerns:\n\n- Conceptual novelty seems to be limited. The method seems to be a basic modification of the GraphNVP approach by introducing ResNet-like skipped connections. Not entirely sure if this counts as a major contribution. \n- Experimental results do not seem to suggest improvements over the state of the art. I am not an expert in molecular generation, but looking at Tables 1 and 2 seem to suggest that the smaller number of parameters in GRF (compared to GraphNVP) come with dramatically reduced performance measures pretty much across the board. This seems like a weak result So I do not know whether the tradeoffs are worth it or not.\n\nOther comments:\n- The detailed pseudocode of Algorithm 1 isn't really necessary considering it is just a fixed point iteration.\n- Unclear why the reconstruction error in Fig 2 does not monotonically decrease for the ZINC dataset.\n- Unclear why Fig 3 suggests smooth variations in the learned latent spaces. I can only spot mode collapses and sudden jumps. It might help to plot VAE/GraphNVP embeddings here too.\n\n"}