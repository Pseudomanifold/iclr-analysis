{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers training of DL models in a decentralized setting. Previous work (Tang et al 2018b, Koloskova et al 2019, Tang et al 2019) introduced communication compression in order to reduce communication cost in decentralized SGD. This paper shows that the naive way to apply quantization to compress communication can fail (but, none of the previous works used this naive way). The paper proposes a novel decentralized SGD algorithm called MONIQUA which communicates only with compressed vectors. The paper theoretically proves that MONIQUA asymptotically converges with the same speed as D-PSGD (Decentralized SGD with full communications) for non-convex functions. \nThe main benefit of the proposed algorithm is that in contrast to the baselines, MONIQUA doesn\u2019t require any additional memory and computation overhead connected to that. \nThe paper also allow to combine MONIQUA with asynchronous communications (AD-PSGD) and decentralized data (D^2), which is novel, previous baseline didn\u2019t allow asynchronous communications. Paper experimentally validates MONIQUA and show that it converges faster than the baselines on early optimization stage, but omit comparison of final results.\n\nMy score is weak reject. The method is interesting and elegant, doesn\u2019t require additional memory, theoretically convergent with a good speed and allows asynchronous communication. However, it looks a bit incremental. In contrast to the baselines, MONIQUA does not support arbitrary communication compression. Experimental comparison is shown only for the beginning of the optimization where the algorithm doesn\u2019t achieve state of the art accuracy. \n\nMajor concerns: \n\n1. Superior experimental results are shown only for (i) the constant stepsize schedule and (ii) only in the beginning of the optimization. The algorithm is unable to achieve s.o.t.a. test accuracy and has severe accuracy drop of 10-20%. It is unclear if MONIQUA is able to close the accuracy gap. \nTo get good test accuracy the exponentially decaying learning rate schedules are usually used when training ResNet on Cifar10. However, that is unclear if this learning rate schedule could be supported by the algorithm. I see that theoretically the same result would be not possible. Once the learning rate dropped, ||x_i - x_j|| has to be smaller than the new theta, which is not true. \n\n2. In contrast to the baselines, MONIQUA doesn\u2019t support arbitrary quantization. The quantization level is fixed. For example, for the ring 8 case (which was used in experiments), the number of bits required to converge is at least 5 (I calculated it using expression for B on page 5). This also questions if MONIQUA will be able to converge to good accuracy in extreme bit-budget. \n\n\nOther concerns that should be addressed: \n\n3. The spectral gap is usually not a constant: for example for the ring topology it decreases as 1/n^2. That should be clarified in the convergence rate and the number of bits required to communicate. \n\n4. In the experiments, to ensure fair comparison, the tuning details of the averaging rate for DeepSqueeze and Choco baselines, and theta parameter for MONIQUA were not given.\n\n5. Reproducibility of the experiments: Many experimental details are omitted: \n   - What are the values of hyperparameters used in the plots?\n   - How did you model asynchrony in experiments?\n   - How did you choose the parameter \\theta in experiments?\n\n6. In (1), why x is \\in [-1, 1]^d? What if this assumption doesn\u2019t hold in (4)? The Theorem 1 is not clear then.\n\n7. Why taking mod \\theta is required? Cannot you assume that ||Q(x) - x|| <= theta delta? The whole analysis will stay true.\n\n8. In Asynchronous communication section, why \\tau_k are the same for all the workers? Cannot different workers have different delays? \n\n9. Why in experiments D-PSGD doesn\u2019t converge when the data are decentralized? This is not consistent with the theory, D-PSGD should converge just slower. \n\nQuestions: \n10. Is there a way to compute theta it in practice beforehand? \n\nMinor comments: \n- x mod y is not defined when x is a float number.\n- Intuition behind Moniqua: do you assume that worker 1 has only one neighbour (worker 2)? does m_1 is one dimensional or high dimensional? \n- Algorithm 1, the notation \\bar X_k might be confusing as in the proof big letters correspond to matrixes. Consider to change on \\bar x_k.\n- why some equations are numbered and some are not? \n- In Asynchronous communication section: what are the assumptions on W_k? \n- it would be helpful for the clarity to remind what is the stochastic rounding quantization in configuration of experiments section. \n- which are the other algorithms you talk about in footnote 6? \n- In wall-clock time evaluation section: how floored outputs could be represented as 16-bit integers? aren\u2019t the vectors which you quantize always smaller than one? \n- wall-clock time evaluation section: the statement that \u201calgorithms diverge\u201d might be confusing.\n- Aggressive quantization add  \u201d.\u201d\n- why the number of epochs differ in different experiments? Fig. 3(a) and Fig. 4(a) ?\n"}