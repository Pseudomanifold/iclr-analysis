{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a \u2018Learning to Transfer via Modeling Multi-level Task Dependency\u2019 for multi-task learning\u2019, which uses the attention mechanism to learn task dependency.\n\nIn the introduction, authors claim \u2018most of the current multitask learning framework rely on the assumption that all the tasks are highly correlated\u2019. I don\u2019t think this claim is correct. In fact, most state-of-the-art multi-task learning models can learn task dependency via different forms.\n\nIn the proposed network, different tasks have their own encoder, which leads to a large number of model parameters especially when there are a large number of tasks. This situation becomes even worse when each task has a limited number of labeled samples.\n\nThe attention has been used in multi-task learning. Authors can google \u2018multi-task learning attention\u2019 to find related works. Of course authors need to compare with those related works.\n\nA typo: \u201cis theposition-wise mutual attention between\u201d"}