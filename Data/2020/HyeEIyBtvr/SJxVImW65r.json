{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper relates to automatic neural architecture search techniques. Current methods have certain drawbacks: Some train all network paths to convergence, which wastes computational efforts in unpromising paths, whereas some don't train all the branches uniformly, which can lead to unfair comparisons.\n\nThe authors propose a model to balance the two issues mentioned above. Their aim is to produced balanced training while trying to reduce conflicts between the different potential network paths.  So their algorithm Has two phases, one where it randomly builds a block in the network and another one where it discards vertices form the layer that are below a certain threshold. \n\nThe experimental results seem sound to me, and I think this is a reasonable approach. \n\nHere some general comments that would help with clarity:\n\n1) I think the authors should explain more clearly what \"Matthew Effect\" in the introduction\n\n2) It's not very clear to me how th_\\alpha is computed. Could this please me made more specific. Section 4.4 says that \"only the operators with performance much lower than the average of others will be dropped.\" Is this approach conservative? Did they try different thresholds?\n\nThere are several minor typos that the authors might want to correct.\n\n1) There is a couple of spaces missing like between \"2018)have shown\" in page 1\n    - The word \"probability\" and p_1, p_2 in (1)\n    - Eq6 on page 5\n\n2) It is customary to use commas before and after \\ldots if one is listing a sequence. The authors don't do this in any of their lists, and this is very strange. \n\n3) In the description of Algorithm 1, I'd change \"S_max is denoted as\" for \"S_max denotes\"\n\n4) In (7) I think there is a \"{\" and a\"}\" missing before and after the o_{l,m_i}. It's set notation. \n\n5) In the discussion, they write the word \"differently.\" Would it be better to write \"by contrast\"?"}