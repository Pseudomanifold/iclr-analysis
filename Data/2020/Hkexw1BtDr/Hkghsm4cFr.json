{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper aims at solving graph-based combinatorial optimization problems using a new paradigm for Deep Reinforcement Learning. In contrast with standard Markov Decision Processes used for combinatorial DRL, the authors advocate the use of \u201cdeferred\u201d MDPs capturing more complex actions (which can choose a subset of nodes), and more sophisticated transitions, which combine an update phase together with a cleanup phase. For the DRL architecture, an Actor-Critic framework is trained using a proximal policy optimization approach. The paper specifically focuses on the Maximum Independent Set (MIS) problem. Comparative results on various instances reveal that this approach clearly outperforms the S2V-DQN algorithm and is competitive with some usual combinatorial solvers (CPLEX, ReduMIS).\n\nThe overall approach is interesting and the experimental results look promising, but it is quite difficult to accept the paper in its current state due to the following reasons:\n\n(a) Though the paper is comprehensible, it is littered with spelling mistakes (just take the first sentence: problem -> problems). For the camera-ready version, it would be nice to use a spell/grammar checker.\n\n(b) The authors focus on the MIS problem, but in the introduction, they claim that their approach can be applied to \u201cany\u201d combinatorial optimization task. Well, that is a bit of an overstatement, due to the immense variety of NP-hard optimization tasks! On the one hand, the MIS problem is already taking an important place in many areas of computer science, and I would suggest concentrating on this task - by changing the title and the summary of the paper, and by just pointing out in the conclusion that the present DRL approach might be applicable to other problems. On the other hand, if the authors are convinced that their DRL approach is generic, then the paper should include other well-known APX-hard problems such as, for example, Min Feedback Node Set, Min Balanced Cut, or Min Set Cover. \n\n(c) The paper is missing a related-work section. To this point, the MIS problem should be presented in more detail with some theoretical results about its approximability (to within a linear factor, as shown by Boppana and Halldorsson, 1992), and its fixed-parameter tractability (it is W[1]-hard). From an algorithmic viewpoint, the ReduMIS technique is mentioned, but there are many other heuristic approaches and kernelization methods. Notably, OnlineMIS (Dahlum et. al. ESA\u201916) is orders of magnitude faster than ReduMIS on various large instances. Furthermore, the kernelization technique developed by Chang et. al. (SIGMOD\u201917) and its parallel version (Hespe et. al., JEA\u201919) is known to be much more efficient than the kernelization algorithm (VC-Solver) suggested in ReduMIS. Besides heuristic algorithms, very little is said about deep learning approaches for solving MIS. To this point, a more detailed presentation of  Dai et. al.\u2019s approach  (NIPS-17) would be relevant, by explaining the similarities and differences with your approach. Finally, nothing is said about the recent deep architecture proposed by Li et. al. (NIPS\u201918) for solving MIS. Though they adopt a supervised learning approach, their GCN architecture shares strong similarities with the Actor-Critic framework in the present paper.\n\n(d) The deep auto-deferring policy, presented in Section 3 looks interesting, but it is quite unclear. The main idea lies in deferred MDPs, for which the size of the action space is in $\\Omega(3^N)$ because any subset of nodes can be chosen at each state. For such huge MDPs, it is not clear that we can converge to a stable policy in polynomial time. Furthermore, the authors are mixing Actor-Critic (using a graph convolution net) and Proximal Policy Optimization, together with diversification rewards, which makes the overall framework unintelligible. A graphical representation for explaining this complex framework would be welcome. Furthermore, I don\u2019t see how the notion of diversification reward is implemented in the PPO approach for training the agent.\n\n(e) In the experiments, some competitors are missing. As mentioned above, ReduMIS is nowadays dominated by more efficient kernelization/heuristic techniques for solving MIS. I would suggest to instead use OnlineMIS. Moreover, the choice of S2V-DQN is relevant as an RL competitor, but it would be interesting to also use the GCN architecture (Li et. al., NIPS\u201918) as a supervised competitor. Finally, the SMT solver Z3 is known to be quite efficient for MIS instances (as already observed by Li et. al.), and a comparison with it would indicate whether ADP is competitive (or not) with the best generic solvers.\n\n\n\n\n"}