{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows a simple geometric proof for the necessity of depth in relu networks. In particular, there exists a set of functions such that bounded depth neural network needs exponential size to estimate, while linear depth neural network only requires bounded width.\n\nHowever, the contribution itself is not too significant in that the theorems are already known in literature in a more complete and general form. Showing an illustrating 2 dimensional example is definitely an interesting and more accessible results for education and better understanding, but not a significant contribution to the knowledge of deep learning theory. Also the theorem statement and proof need to be made rigorous in terms of definition, mathematical reasoning etc.\n"}