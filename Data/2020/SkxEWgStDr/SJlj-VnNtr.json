{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a proof that deeper networks need less units than shallower ones for a family of problems.\nMore exactly, the authors exhibit a series of indexed problems f_i, such that:\n- for a given depth, there are problems that require a  exponential number of units to be approximated by a network, and\n- for any problem within the family, it is possible to approximate it with a \"reasonably-sized\" network.\n\nThe main advantage of the approach is that the proof is based on relatively simple geometric concepts.\n\nThe problems f_m are binary classification problems, where 1 class is inside a 2D polygon with m sides, inscribed in a circle of radius 1 centered on 0, and the other class is outside.\n\nFor a given m, the paper provides the architecture of the network approximating m as a composition of geometric transformation.\n\nI like the simplicity of the geometric proof. However, the family of functions sounds simple and not representative of the complexity of real problems. In particular, the positive region is a simple convex region. This makes the generalization of the conclusion of the proof to real problems not particularly convincing.\n\nFor a more minor comment:\nI would say that Paragraph \"Number of regions in a line-arrangement of n lines\" could be shorten as it is a pretty  standard math result (not just from deep learning). At least, Fig 5 could be removed.\n\nFor Paragraph \"A 1 hidden-layer ReLU network is a line arrangement\": I think this is also well known now, and could be shortened.\n\n\n\n\n\n"}