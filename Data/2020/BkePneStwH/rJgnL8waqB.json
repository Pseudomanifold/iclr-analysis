{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper addressed multilingual natural language inference. The motivations of the authors are two folds: 1/ to have only one model for all the languages instead of one model per language and 2/ achieving good results in a zero shot setup where only the english labels are available. \n\nPrevious work proposed first to learn multilingual language model in a self-supervised manner for the initialization. Then, fine-tuning it on the final task, separately for each different language. It results in multiple models, one for each language.\n\nThe authors proposed to fine-tune the model with all the data from the different languages simultaneously, resulting in only one model with comparable results.\n\nIn addition, they also proposed a method based on distillation where only the english targets are used. While the model doesn't require the label data for the other languages, the scores remained similar to the previous experiments.\n\nFinally the authors proposed to combine both methods. It compares favorably, obtaining 4 points of improvement over SOTA in the zeroshot setup.\n\nPros:\n-the motivation for having only one model is interesting\n-the results are promising\n\nCons:\n-one of main motivation of the paper is to achieve zeroshot as opposed to previous work. To that purpose, the authors chose to keep only the translated non-en input and not the non-en target. In XNLI, all the non-en training data, including target, are automatically translated from the English data. Therefore, the authors did not use less human annotated data and their approche still requires automatic translation. Hence, the motivation to perform the task in a zero-shot scheme, as opposed to previous work, doesn't seem correct. \n-the paper was not always easy to follow and would benefit from more clarity.\n'large margin of 5.9 and 4.2 points' in 3.2.1, please add the reference to table 6.\n-the method 'significantly' improved results. I didn't see any significance measurements and it would be important to add them.\n\nOverall, using all the data together seems like a natural and effective approach to and achieves good results through only one model. \nHowever, the motivation behind 'distillation' is to perform in a zeroshot scheme. It seems abusive to me since it actually requires the exact same amount of human labels than the previous works. "}