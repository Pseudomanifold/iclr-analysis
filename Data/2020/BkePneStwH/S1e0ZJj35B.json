{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "What is the task?\nMultilingual natural language inference (NLI)\n\nWhat has been done before?\nCurrent state-of-the-art results in multilingual natural language inference (NLI) are based on tuning XLM (a pre-trained polyglot language model) separately for each language involved, resulting in multiple models.\n\nWhat are the main contributions of the paper?\n[Not novel] Significantly higher average XNLI accuracy with a single model for all 15 languages.\n[Moderately novel] Cross-lingual knowledge distillation approach that uses one and the same XLM model to serve both as teacher (for English sentences) and student (for their translations into other languages). The approach does not require end-task labels and can be applied in an unsupervised setting\n\nWhat are the main results? \n A single model trained for all 15 languages in the XNLI dataset can achieve better results than 15 individually trained models, and get even better when unrelated poorly-translated languages are removed from the multilingual tuning scheme.\n Using XD they outperformed the previous methods that also do not use target languages labels. \n\nWeaknesses :\n1. Combining XD with multilingual tuning is not effective in improving average results or even in case of target languages\n2. Final system is adhoc as experiments on a particular set of languages have been used to support claims. For example, Urdu was excluded to get the best MLT model. Only 4 languages were used while combining XD and MLT\n3. Findings, methods and experiments are not strongly novel.\n4. Paper was not an easy read.\n\nStrengths:\n Using XD they outperformed the previous methods that also do not use target languages labels. \n\n"}