{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "First, the authors propose to train a model for natural language inference (NLI) on multiple languages simultaneously. In particular, they translate English examples into all target languages and fine-tune a pretrained language model on all thereby obtained data at once. This is different from the previous state-of-the-art approach which consisted of, after translating from English into target languages, fine-tuning one NLI model for each language individually. The authors show that their approach is superior to training individual models for each language. For evaluation, XNLI is used.\n\nSecond, they introduce cross-lingual knowledge distillation (XD), where the same polyglot model is used both as teacher and student across languages to improve its sentence representations without using the target task labels. The main idea is that the same sentence in all languages should receive output representations as similar as possible.\n\nThe paper seems okay to me and the experiments seem solid. However, the results are not particularly surprising and the methods are not very innovative. The writing could be improved. \n\nThis paper could further be improved in the following ways:\n- A more detailed investigation which combination of languages improve performance (and why?).\n- Similarly: A combination of MTL and XD doesn't seem straightforward. Why? What is learned?\n\nSmaller comments:\n- Articles are missing frequently (e.g., \"we substitute the word prediction head with classification layer\" -> \"we substitute the word prediction head with a classification layer\")\n- Table 5: \"w/0\" -> \"w/o\"?\n- Have you run any significance tests?"}