{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions:\n This paper proposes to explore nonnormal matrix initialization in RNNs. Authors demonstrate on various tasks (Copy/Addition, Permuted-SMNIST, PTB, enwik8) that chain-like nonnormal matrix initializations can outperform orthogonal or identity initialization in vanilla RNNs. However, nonnormal RNNs underperform their gated counterpart such as LSTM. Authors also show results where they use their initialization scheme in update gate of a LSTM.\n\nComments:\nThe paper is well written and pleasant to read. The paper structure could be a bit improved. For instance, section 2 is named \u201cResults\u201d while 2.1 which take significant part of the section is about some prior results in (Ganguli et al. 2018). It would be better to have it under an explicit prior work section. \n\nThe description of the experiments reported in Figure 2. is a bit vague: what is the training/evaluation data?, do you train all the model parameters or only the linear layer?, what is the type of noise used? It is unclear to me how robust is the observation made in Figure-2. Do you see similar behavior with different noise-scale and other non-linearity such as tanh?\n\nThe experimental section provides convincing data showing that non-normal initialization schemes outperform orthogonal and identity initialization in vanilla RNN. However, it would be nice to add some comparisons with prior works. It is unclear how the current method compare with nn-RNN of (Kerg et al. 2019) and the unitary-RNNs. \n\n Why the score reported for the 3-LSTM in Table 3.  is underperforming 3-layer LSTM baseline used in (Merity et al., 2018), reported in Table 1.?  In addition, did you try saturating non-linearities for the RNN experiments?\n\nOverall, I think the method is promising, but comparison with prior work is missing. I would encourage the authors to compare their approach with unitary-RNN, and nn-RNN to better demonstrate the significance of their works.\n\nAdditional remarks:\n-\tSNR could be defined more precisely in the introduction. In particular, the introduction states that the stochasticity of SGD is a source of noise which is true. But the model presented in section 2 seems to focus mostly on input noise? \n"}