{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR. The new dataset is the first to account for all of the following fundamental aspect of videos: temporal ordering, short- and long term reasoning, and control for scene biases. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. In contrast, the authors show that with the proposed dataset, models without spatiotemporal reasoning largely fail.\n\nThe paper should be accepted as it addresses a major shortcoming of all existing video understanding datasets. It does a good job at summarizing the deficiencies in existing datasets, clearly motivating the need for a new dataset. The claims are backed up with solid experiments, ablating models and data parameters adequately. It is mostly well-written (except for section 4 which would benefit from extensive proofreading) and does a good job at covering relevant work. One drawback is of course the synthetic nature and limited domain of objects and actions. On the other hand, this makes the setup highly controllable and reliable. I like the fact that each task comes both with both static and moving camera.\n\nImprovements and Questions:\nSome relevant datasets are missing. For example, the Moving MNIST and Robot Pushing datasets could be added to Table 1.\n\nI suggest having a train / validation / test split (like CLEVR), rather than just a train and validation split.\n\nIn particular for Task 3 more frames seem to give dramatic improvement. Why did you not run with more than 64 frames? \n\nDid you consider downsampling the videos to allow running on all the frames?\n\nI\u2019m missing details on the resolution of the generated videos?\n"}