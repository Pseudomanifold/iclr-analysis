{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper revisits the generalized reparameterization gradient [Ruiz et al. 2016] and derives it from a perspective of the transport equation [Jankowiak & Obermeyer, 2018]. While this is an interesting perspective, I think the paper is hard to read, vastly overstates its contribution, misses highly related works and provides no empirical improvement based on the presented experiments. Due to this, I believe the paper should be rejected.\n\nThe abstract of the paper does a good job of summarizing its claims. Since I believe these claims to be significantly overstated or incorrect, I will go through the abstract and comment on it.\n\n> However, the reparameterization trick is based on the standardization transformation which restricts the scope of application of this method to distributions that have tractable inverse cumulative distribution functions or are expressible as deterministic transformations of such distributions.\n\nThis is not correct, since [Jankowiak & Obermeyer, 2018] and [Figurnov et al., 2018] show how to apply the reparameterization trick to distributions with intractable inverse CDFs.\nThe reparameterization equation eqn. (5) in this paper is the same as eqn. (6) from [Figurnov et al., 2018], up to an error: the (\\nabla_z \\phi(z, \\phi)) should be matrix-inverted. The same error appears further down the page, between the two remarks.\n\n> In this paper, we generalized the reparameterization trick by allowing a general transformation. [...] We discover that the proposed model is a special case of control variate indicating that the proposed model can combine the advantages of CV and generalized reparameterization.\n\nThis generalization has already been done in [Ruiz et al., 2016], including the interpretation of the estimator as REINFORCE + control variate. The control variates based on null solutions to the transport equation have been explored in [2], which is not cited.\n\n> Unlike other similar works, we develop the generalized transformation-based gradient model formally and rigorously.\n\nI find this statement deeply problematic on two counts. First of all, the language used here is dismissive of other people\u2019s work. While we should always strive to improve the work of our colleagues, these contributions should be phrased positively and politely. Secondly, I haven\u2019t found the derivation in this paper to be particularly formal or rigorous. For example, Theorem 3.1 does not state any requirements for f(z), even though I am quite certain that this theorem does not apply for non-continuous f(z). Finally, I find the statement (2) of the Theorem 3.1, when taking into account the Eqn. (6), trivial: it simply says that E [g(z) + h(z)] = E [g(z)], if E [h(z)] = 0.\n\n> Based on the proposed gradient model, we propose a new polynomial-based gradient estimator which has better theoretical performance than the reparameterization trick under certain condition and can be applied to a larger class of variational distributions.\n\nThe proposed improvement is a polynomial control variate for the reparameterization gradient. Highly related variance reduction techniques have already been explored in [1] and [2]. The important disadvantage of the current work, compared to these previous works, is that this control variate is not adaptive: the CV coefficient is set to a constant, rather than updated, say, based on the gradient information. This means that this control variate may actually increase the variance if the coefficient is not guessed correctly.\n\n> In studies of synthetic and real data, we show that our proposed gradient estimator has a significantly lower gradient variance than other state-of-the-art methods thus enabling a faster inference procedure.\n\nI don\u2019t think the actual experimental results support this claim. Figure 1 shows a small improvement on a synthetic problem. Figure 2 shows that the proposed method does not outperform the alternatives in terms of convergence speed: RSVI up until 10^3 seconds and IRG afterwards.\n\nMinor typos:\n* a same variance -> the same variance\n* I\u2019ve found the notation \\int^{z_i} a bit confusing, it\u2019s probably best to specify the lower limit as well\n* euqation -> equation\n\n[1] Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams \u201cReducing Reparameterization Gradient Variance\u201d NeurIPS 2017\n[2] Martin Jankowiak, Theofanis Karaletsos \u201cPathwise Derivatives for Multivariate Distributions\u201d AISTATS 2019"}