{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of having compact\u00a0yet expressive KD code for NLP tasks. The authors claim that the\u00a0proposed differentiable product quantization framework has better compression but similar performance compared to existing KD codes.The authors present two instances of the DPQ framework: DPQ-SX using softmax to make it differentiable, and DPQ-VQ using centroid based approximation. While DPQ-SX performs better in terms of performance and compression, DPQ-VQ has the advantage in scalability.\n\n- Significance\nIt's understandable that the size of the embedding is important, but there's been a lack of explanation as to why this should be done only through KD codes. Hence, it is doubtful how big the impact of the proposed framework is.\n\n- Novelty\nJust extending and making Chen et al., 2018b's distilling method to be differentiable has limited novelty.\n\n- Clarity\nThe paper is clearly written in most places, but there were some questions about the importance and logic of statements.\n\n- Pros and cons\nCompared to Chen et al., 2018b, there is no need to use expensive functions, and performance is better. But, the baseline consists only of algorithms using KD codes; there might be many disadvantages compared to other types of algorithms.\n\n- Detailed comments and questions\n1. It is true that the parameters for embedding make up a large part of the overall parameters, but I would like some additional explanation of how important they are to learning. It is usually not necessary to train the entire embedding vector on GPU, so it would not be a big issue in the actual learning process.\n2. In a similar vein, it would be nice to show which of the embedding vector size or the LSTM model size contributes significantly to\u00a0performance improvements.\u00a0If LSTM model size contributes more, the motivation would be weakened.\n3. It would be nice to add more baselines such as Nakayama 2017 as well as the standard compression/quantization methods used in other deep networks. And please explain why we should use KD codes to reduce embedding size. Also, why the distilling in Chen et al., 2018b is a problem?\n4.\u00a0Did you run all experiments just one time? There is no confidence interval.\n5. DPQ models have different compression ratios depending on the size of K and D. It would be great to show the change in PPL according to the compression ratio of DPQ models.\n6. Can we apply it to pre-trained models like BERT?"}