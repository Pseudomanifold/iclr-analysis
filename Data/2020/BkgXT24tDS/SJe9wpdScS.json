{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper presents an approach based on power-two quantization to compress the weights of neural networks. The authors elaborate on a coding scheme that adds several quantized values with different log scales (typically 2^(-2k) and 2^(-2k+1) for varying k, for the sum of two log scales). More importantly, they emphasize on the importance of weight scaling and learning the clipping threshold. experiments are carried out with several versions of ResNet on CIFAR-10 and ImageNet, and the authors show better performances than competing baseline for a fixed quantization budget per float.\n\nThe results are better than the non-compressed baseline (CIFAR with 3 and 5 bit/flotat, ImageNet with 5bit/float.) This seems surprising. Is there any clear reason why? Shouldn't the performance decrease as a result of the compression?\n\nWhile the results are good, a large part of the paper is dedicated to the idea of summing powers-of-two quantizers. The motivation comes from the sub optimality of uniform quantizers. Some ablation studies demonstrate that the scheme power-of-two is better than uniform. Nonetheless, the results in Table 2 indicate that uniform already achieves very good performances (already better than the non-compressed network for top-1 accuracy), so that most of the gain seems to come from weight normalization and learning the clipping threshold.  \n\nThus, the advantage of the exact scheme that is proposed compared to other methods is unclear to me. There already are well-known algorithms for non-uniform quantization (e.g., Lloyd). Lin et al. (\"Towards Accurate Binary Convolutional Neural Network\", cited in the paper) already addressed the non-uniform quantization by learning different binary bases. While the results of the paper are better than a number of previous results, it seems to me that most of the gain does not come from the quantizer but rather from the other elements of the method. \n\nThe contribution then seems a bit incremental, as it is mostly weight normalization +  straight through estimator to learn the clipping constant by minimizing directly the loss function. One way to make the contribution stronger is to isolate this part to see its effect more generally on several non-uniform quantization schemes.\n\nother comments:\n- is there any (possibly intuitive) justification for the fact that the results of the compressed networks are better than the non-compressed ones?\n"}