{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "= Summary\nA Graph Neural Network extension integrating a global supernode explicitly into the message passing process. Concretely, message passing along the graph edges is alternated with message passing to/from a fresh super-node. Experiments show that this improves results of a number of common GNN architectures on four datasets.\n\n= Strong/Weak Points\n+ Simple but useful extension of the existing super-node idea\n+ Experiments on a number of datasets and baseline GNN architectures, providing ample experimental evidence of the usefulness of the method.\n- Writing is overcomplicated and uses a lot of jargon (\"transmitter unit\", \"warp gate\", \"intermodule hyperspace\"). I found the text entirely impenetrable and instead simply focused on Fig. 3 + the actual equations.\n\n= Recommendation\nThis is a nice contribution of minor novelty, with empirical evidence of its usefulness. I believe the paper should be accepted to a large conference such as ICLR.\n\n= Minor Comments\n- Fig. 3: Inconsistent \"intra-module\" (top) vs. \"intra module\" (bottom) \n- Concurrent work in https://openreview.net/forum?id=B1lnbRNtwr discusses a \"sandwich\" model which alternates graph message passing with (essentially) a Transformer layer applied to all nodes. This idea seems related (in that it alternates local and global information exchange)."}