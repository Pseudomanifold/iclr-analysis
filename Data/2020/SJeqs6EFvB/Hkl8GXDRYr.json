{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "JavaScript is the standard programming language of the Web; according to the stats it is used on 95% of the (at least) 1.6 billion websites in the world.  Compared to other programming languages, it posses certain unique characteristics which are also responsible for making it so popular for the Web.   In this interesting work the authors aim to provide a novel data-driven system for detecting and automatically fixing bugs in Javascript.  The authors provide motivating examples behind their research. Indeed, Javascript poses unique challenges as described eloquently in Section 2, and there is a lot of space for improvement. Static analyzers have non-trivial limitations, while there is space for improving data-driven approaches. This is what Hoppity aims to achieve by translating the program into a graph, embedding the nodes and the graph as points in a d dimensional space using graph neural networks, and then using a controller that uses LSTMs decide the action to be taken among a predefined set of possible actions or a single step graph edit. These actions are reasonable, and are able to fix many bugs assuming the right sequence of actions is performed. For the purposes of learning, the author(s) use a corpus crawled and preprocessed from Github that contains more than half a million programs. Overall, I found this paper very interesting to read, and with large potential impact in practice. The paper contains a solid engineering effort, starting from the dataset collection and its preprocessing, to using state-of-the-art machinery to develop Hoppity. Therefore, I support its acceptance. However, some things were not clear from the writeup, and I hope the author(s) of the paper can give some insights. \n\n- What is the effect of parameter T, i.e., the number of iterations? How is it set in the experiments? Clearly, the authors have a knowledge of what T should be since they have preferred programs with fewer commits.\n- Following on my previous point, given the sequence of changes/graph transformations you perform, what is the distribution of the 'edit distance' (i.e., number of hops to fix a bug) in the dataset that you have? While the author(s) have \nalready provided a lot of stats in the Appendix, it would be interesting to see such a plot. This distribution could be insightful and serve as a rule of thumb for understanding the effect of T. \n- What is the effect of the beam size? Can you plot  the accuracy  as a function of the beam size? \n- Have you tried points with more than 500 nodes to see how the size of these graphs affect the performance of Hoppity? \n- Can you provide further details on the running times and the GPU specs?   \n- The evaluation does not put enough emphasis on false positives/false negative analysis. Is it the case that bug-free programs are treated as such? \n-  Have you tried Hoppity on other programming language(s)?  Do you expect such an improved performance over baselines for other languages (e.g., C++) as well?\n\n"}