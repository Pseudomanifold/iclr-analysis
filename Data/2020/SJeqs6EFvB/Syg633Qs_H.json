{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a graph tranformation-based code repair tool. By representing source code as a graph a network is asked to take a series of simple graph edit operations to edit the code. The authors show that their method better predicts edits from existing code.\n\nOverall, I find the problem interesting and the neural approach of the authors reasonable, principled and interesting. However, the evaluation is either badly written or the authors have not understood the related literature they are comparing to. It is thus unclear how this model compares to alternatives. I believe that this is good work that needs to eventually be published, but it's not ready at this time.\n\n* In 3.2.1 \"Value\" paragraph \"Instead of predicting the replacement value using a language generative model (Chen et al 2018, Allamanis et al 2018), we let the model to choose from either the values appearing in the current file.\" \n  \n   -The Chen et al. (2018) paper is indeed a translation-style model, but thanks to the copying mechanism, the model can also choose from values appearing in the current input.\n   - The Allamanis et al. (2018) is *not* a generative model and it can only select a misused variable for those that exist on within the scope (for the Variable Misuse Task)\n\n* The authors compare to the VARNAMING (Section 6.2) task of Allamanis et al. (2018). This is odd, since to my understanding, this task is about alpha-renaming, i.e. semantics preserving renaming of all identifiers of a variable. This is not comparable with the REP_VAL action which simply replaces the value of a single leaf node from the value table. The VARMISUSE objective (picking one variable directly from the value table) is the closest analogue, although it performs no localization. \n\nThus, it is not surprising that a char-level language model cannot predict the names of the elements in the value table. So, I don't think that we learn anything from the current comparison.\n\n* It's unclear to me why the REP_TYPE action is relevant to VARMISUSE task of Allamanis et al. (2018). This seems to simply be a classification task (among all known/valid types of the tree) given an existing position.\n\n* Given the above two points, it's unclear what the comparison with the baseline (Table 3) means and what one can deduce from the evaluation: The Allamanis et al. (2018) work tries to find the correct variable at a given location; this work tries to pick a location and find a different value from current one. The Allamanis et al. (2018) and Cvitkovic et al. (2018) uses additional semantic information (as edges), importantly data flow and control flow. This work uses none of those. I am not sure that I can see how the comparison here can work.\n\n* In 3.1 \"Unlike previous approaches, we additionally add value nodes that store the actual content of the leaf nodes [...]. The purpose of introducing this additional set of nodes is to provide a name-independent strategy for code representation and modification\". I am not sure what's unlike previous approaches. For example, the Allamanis et al. (2018) and the Cvitkovic et al. (2018) work, connect the same variables with various ways (e.g. data-flow edges) in a way that provides a name-independent strategy for code representation. I would fing it surprising that a principled comparison between Allamanis et al. (2018)/ Cvitkovic et al. (2018) and this work will yield any improvements: all papers use the same model (variations of a graph neural network) and this work uses strictly fewer information (no data flow, control flow edges). A comparison would simply compare how the two graph-extraction methods compare, but nothing valuable otherwise.\n\n* The authors of this work frequently refer to Chen et al. (2018), but never compare with it. I would expect that this model beats the Chen et al. (2018) work, but this needs to be shown. In my opinion this is a much more relevant baseline, compared to the comparison with Allamanis et al. (2018). If the authors insist on comparing with that work, then a \"fair\" comparison needs to be made. Comparing between REP_VAL and VARMISUSE given the location of the node-to-be-repaired, is probably the only reasonable option. Comparing with Vasic et al. (2019) for the REP_VAL/REP_TYPE would also be reasonable.\n\n* The correctness of the NO_OP predictions are never evaluated. Does the model \"know\" when to terminate or does it keep editing? It's unclear what is the overall accuracy (for a full edit).\n\nOther questions:\n\n* The \"ADD\" operation predicts two locations: one for the parent and one for the left sibling of the node. Does this mean that for a given node the \"ADD\" operation will never be able to add a left-most child (since it has no left sibling)?\n\n* It has been recently found that code corpora collected by scraping GitHub may contain a disproportionate amount of duplicates (Lopes et al. 2017, Allamanis 2018). It is unclear if the authors have taken any steps to detect and remove duplicates that would affect their results.\n\n* In Table 2: Does \"Beam-3\" mean \"accuracy in the top 3\" or does it mean \"accuracy of the top prediction when the beam size is 3\"? The difference between \"Beam-1\" and \"Beam-3\" is surprisingly large. Can you explain why?\n\n* In the TAJS comparison, it is unclear if any negative examples are added: it's unclear what's the false positive ratio of HOPPITY vs TAJS.\n\nMinor:\n\n* Abstract: \"Github\" -> \"GitHub\"\n* 3.2 REP_VAL: \"Tthe\"-> \"The\"\n* Sec 7: \"Vasic et al. (2019) present a pointer network...\" It is unclear if this work outperforms the Allamanis et al. (2018) work as the comparison is only partial. Another ICLR submission https://openreview.net/forum?id=B1lnbRNtwr suggests that this is not the case.\n* The citation of Chen et al. (2018) should be capitalized correctly \"Sequencer\"->\"SequenceR\"\n\n\n## References\n\nAllamanis, Miltiadis. \"The Adverse Effects of Code Duplication in Machine Learning Models of Code.\" arXiv preprint arXiv:1812.06469 (2018).\n\nCvitkovic, Milan, Badal Singh, and Anima Anandkumar. \"Open Vocabulary Learning on Source Code with a Graph-Structured Cache.\" arXiv preprint arXiv:1810.08305 (2018).\n\nLopes, Cristina V., et al. \"D\u00e9j\u00e0Vu: a map of code duplicates on GitHub.\" Proceedings of the ACM on Programming Languages 1.OOPSLA (2017): 84."}