{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to learn from bugfixing commits to fix errors in other code. There are several good contributions of the paper, but the main one is to design a language of instructions that fix a program and to formulate the prediction to be a sequence of such instructions. The model, however, is insightful in other ways as well. The lack of naming features or generation of names makes it to point to other identifiers in a program - a major problem for most models for code. Instead, the proposed model only builds embeddings from the structure around the variables.\n\nThe work is also well evaluated, on real dataset and also it attempts to compare to the best available static analysis tools. The kind of bugs addressed by the work was also not considered in previous papers. One thing that comes to the examples of the discovered bugs is that even when the fix is shown, the user would still need detailed explanations on why was it a bug.\n\nSo, I also have questions for the authors:\n1. Do you think it is possible to observe a similar reasoning to the reasoning in your text for why the buggy examples from Figure 1 are wrong, if the activations of the neural network are exposed or with other NN debugging technique.\n2. It seems that a specific sequence of actions is provided in the training data and that sequence is left-to-right edits to apply the fix. In this case, doesn\u2019t it make sense to apply restrictions on the Location primitive similar in spirit to the attention masking (see here: http://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention )\n\nMinor:\n- NO_OP was used in some places (pages 4 and 5) and STOP in others (figure 2).\n- N_k(v) is not defined.\n"}