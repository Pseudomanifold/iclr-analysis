{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n# Summary\nThe paper compares 3 ways to account for the variability of the dynamics model in the TD error computation:\n(i) be robust (take the lowest target value obtained when evaluating the value-function on the training distribution of models);\n(ii) be Bayesian (average over models);\n(iii) domain randomization (compute TD error as usual but randomize the dynamics to obtain a more diverse data set).\n\nThe comparison is carried out using the MPO algorithm. An entropy term is furthermore added to the MPO objective to yield a new E-MPO algorithm, but it didn't affect the performance much (cf. Figs. 5-6 and 7-8).\n\nThe paper is mainly an empirical study.\n\n# Decision\nThe engineering effort of running many experiments and ablation studies is appreciated. However, there are big questions to the evaluation methodology and the contribution of the paper, that preclude me from recommending it for publication in its current form.\n\n# Concerns\n1) First, my concern is the problem setting and evaluation methodology, and in particular Table 3 with varied parameters. The authors chose to have 3 values during training, and then evaluate the optimized policy on further 3 testing environments. There are many questions to this table. I give only a few.\n        - For most environments, training and test ranges are disjoint (e.g., train on 1.00\u20131.05m and test on 1.15\u20131.25). Why? Would your results hold if they are not disjoint?\n        - Fig. 14 shows that when trained on a single model with parameters closest to the evaluation range (right column), there is no difference between the algorithms on the Cartpole and a minor difference on the Pendulum. That seems to imply that it is sufficient to just train on one environment which is most similar to the evaluation environments to perform well in the experiments. Is it true? Do you have a counterexample?\n        - Natural question, of course, what do you propose to do when varying multiple parameters? Discretizing over several values will quickly yield exponential explosion.\n        - Did you try using distributions over ranges instead of fixed parameters? That would be a bit more solid than hand-picked values.\n\n2) Evaluation of domain randomization. The authors say that \"TD errors are then averaged together\" when doing domain randomization. That means, when updating the value function, gradients corresponding to various models get intermixed. It is usually better to fix the domain, then do a few gradient updates using the data from that domain, and after that collect data from another domain. Such procedure is similar to keeping a target network in Q-learning, which is absolutely necessary to stabilize the learning process. It would be interesting to see if domain randomization still performs poorly when applied in this way.\n\n3) It would be helpful to sharpen the main message.\n        - Right now there is the E-MPO introduced, but then it is not really compared to plain MPO (only plots are shown in the Appendix); it is only said that it doesn't hurt to have the entropy term. It seems orthogonal to the robustification scheme in the TD error, so maybe it would make sense to omit the entropy for clarity of exposition.\n        - What is the conclusion? The fact that robust works better than non-robust is obvious even without any experiments. So, the only question one could have is whether it is better to average or to take the worst-case model. I didn't get it from the paper whether there is any conclusion regarding that.\n        - In Conclusion, the authors appeal to some theoretical contribution of this paper, such as proof of contraction in Theorem 1. I would consider it a very minor contribution as it is mainly a recapitulation of known results as both the robust Bellman operator and the entropy-regularized one are known to be contractions. Furthermore, this theorem is not really used in the paper, so to me it seems detached.\n\nSmall: typo in Sec. 7.: you say \"to lengths of 2.0, 2.1 and 2.2\", but according to Table 3, these should be \"2.0, 2.2 and 2.3\""}