{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: \nThe paper proposes a robust variant of MPO and evaluates the performance on control tasks including the cartpole, hopper, walker & the shadow hand. The performance shows that the robust MPO outperforms vanilla MPO and MPO + Domain randomization. \n\nOpen Questions:\n- I am not convinced by the domain randomization performance as the performance increases only marginally over the vanilla MPO and DR has usually performed quite well for robust policies. Can you explain this marginal increase? \n\n- Could the authors please include a qualitative evaluation of the learned controllers? Especially as the Cartpole stabilization  is a linear system one could compare against the analyitc optimal controllers. \n\nConclusion: \nCurrently, I would rate the paper as weak accept, as the derivation is interesting and the approach seems to work quite well in  simulation. However, the work is only incremental and does not propose a new perspective to robust RL. It would be nice to show that the policy is also robust for a physical system. \n\nMinor Comments\n- What does the bar mean, e.g., \\bar{J} & \\bar{\\pi} mean? I cannot find a definition of the bar.\n- Typo in the Background section 'Kullback-Liebler  (KL)' \n\n(I can try to extend my review if the other reviewers disagree)"}