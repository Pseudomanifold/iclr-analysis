{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Overview:\nThe paper is dedicated to studying adversarial attack and defense problems from the perspective of Fourier analysis. They demonstrate that the adversarial vulnerability of neural networks can be attributed to non-zero high-frequency components. Then, the author proposes a simple post average approach to smooth out the insignificant high-frequency components, which can improve the adversarial robustness of neural networks. They conduct extensive experiments on ImageNet and CIFAR-10 to defend existing attacks, including FGSM, PGD, DeepFool, and C&W attacks.\n\nStrength Bullets:\n1. The logic chain of this paper is complete. The author first gives an interesting observation that the non-zero high-frequency components may cause the vulnerability of neural networks. Then they propose a post average technique to reduce the high-frequency components and improve robustness.\n2. The paper is well organized. Some figures, like fig1, are intuitive and interesting.\n\nWeakness Bullets:\n1. How can we get formula (4)? The author needs to provide more explanation.\n2. It is confused about how to pick directional vectors v for input x. Does the author treat the input RGB image (i.e. 3 times 32 times 32) as a one-dimensional vector? Then the author chooses k other vectors in the neighborhood. There need more details and rational explanations. The author can use the visualization method (i.e. tSNE) the illustrate the relationship among samples.\n3. For different sampling strategies, the author needs to provide an ablation study.\n4. The most arguable point is the experiment setting. I think it is a weak white-box experiment setting. The author uses the attacked set which is generated from the undefended model. It means the author doesn't attack his post-processing method. Thus, the improved robustness can be the result of gradient masking introduced by the author's techniques. BTW, it is totally traceable to attack the paper's approach. We can formula equation (7) and equation (8) as two layers in the front and at the end of networks. Then you can calculate the gradient for a complete model setting. The author needs to provides more convincing results.\n5. The author needs to compare with other post- or pre- processing methods, even the normal adversarial training. Also, the paper should contain detailed settings about attacks. For example, how many iterations do you run for PGD attacks? This is an important factor for trustworthy results.\n6. [Minor] Quotation mask mistake in the second row of page 2.\n\nRecommendation:\nDue to the limits of experiments design and setting, this is a weak reject."}