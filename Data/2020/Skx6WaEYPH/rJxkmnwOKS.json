{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper makes three contributions: 1) analyzing the number of linear regions and the Fourier spectrum of fully-connected neural networks with ReLU activations; 2) proposing an ensemble method as a defense against adversarial evasion attacks; 3) analyzing the performance of that defense against state-of-the-art attacks on CIFAR-10 and ImageNet data.\n\nMy main concern is regarding the originality and significance of this work. Most of the results regarding the piece-wise linear geometry of fully-connected neural networks with ReLU activations are known. The authors should reference and discuss the relation of their work in particular to [1]. The Fourier transformation results look straightforward to me. I am not convinced either that they are indeed \"essential\", as the authors write, for analyzing the nature of adversarial samples. In fact, I find the discussion in Section 2.2 rather vague; for instance, what is the formal notion of \"tiny unlearned regions\" of neural networks? The authors mention an experiment in which they observed that an adversarial perturbation on ImageNet led to the crossing of N=5278 hyperplanes \"per layer\", which sounds like an interesting finding, however, more detail is needed to understand how it was obtained.\n\nThe proposed defense is not very original either; the authors should discuss it in comparison e.g. to [2] and [3] who also considered ensemble-based methods to defend against adversarial perturbations. The experimental results in Table 1 & 2 look promising at a first glance, however, [4] has demonstrated that ensemble-based, randomized defenses can be fairly easily defeated (e.g. using expectations over transformations when computing the gradients for adversarial sample generation). The authors need to report the performance of their defense under such white-box settings.\n\nThe strong performance of the FGSM attack in Figure 3 is surprising. PGD is strictly stronger than FGSM, so its defense rate shouldn't be higher than the one for FGSM. I disagree with the authors' explanation that FGSM \"generates much larger perturbations\"; if the epsilon parameter is set properly in the PGD attack, it should exhaust that perturbation budget (assuming that the step size and number of iterations are large enough). On the other hand, I don't understand how the authors were able to constrain the L-inf norm of the C&W and DeepFool adversarial samples (since those attacks aim to minimize L2 norms).\n\n\n[1] Montufar et al., On the Number of Linear Regions of Deep Neural Networks. NeurIPS, 2014.\n[2] Liu et al., Towards robust neural networks via random self-ensemble. ECCV, 2018.\n[3] Cao & Gong, Mitigating evasion attacks to deep neural networks via region-based classification. ACSAC, 2017.\n[4] Athalye et al., Obfuscated gradients give a false sense of security. ICML, 2018.\n"}