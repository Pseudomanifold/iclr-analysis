{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work analyzes the performance of ADAM algorithm under a bounded step size assumption. it proves convergence rate for the deterministic setting and prove a convergence result for the stochastic setting.\n\n\nHowever, I have a few concerns as follows. The first two comments are the major reasons of my rating.\n1. the convergence in Theorem 4.2 is not standard. It is about \"the minimum of the gradients norms\", which does not imply the algorithmic convergence. it should be more clearly stated in the introduction.\n2. The author should include simulations to show whether the theoretical result matches the empirical performance, for both deterministic setting and stochastic setting.\n\n3. Small comment: In section 2, g_i should be a d-dimensional vector since it is the gradient of f. But then what is g_i^2?\n4. For comparison, can you set b=1 and compare with the existing theoretical results of RMSPROP?\n"}