{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper provides convergence analyses for momentum methods using adaptive step size for non-convex problems under a bounded assumption on learning rates. Concretely, a sublinear convergence rate under a general setting and improved convergence rates under KL-condition are provided.\n\nAn interesting point of the paper is that (i) the boundedness assumption on the domain or gradient is not required and that (ii) shows faster convergence rates under KL-condition for non-descent methods. However, a major concern is a uniform boundedness assumption on learning rates. A lower bound is a matter because this condition is verified a posteriori after running methods. This kind of condition is not preferred in general. Indeed, most studies provide convergence analyses without such assumptions. In addition, there are several missing references (listed below) that attempt to analyze the convergence of adaptive methods. To make the position of the paper clear, it would be better to provide a theoretical comparison with these studies. Especially, [LO2019] and [XWW2019] are related to this study. [LO2019] has provided convergence analyses without boundedness assumptions on the domain or gradient and [XWW2019] has provided a linear or better convergence rate of an adaptive method by utilizing the KL-condition. Note that a method treated in [XWW2019] is also a non-descent method because there is no theoretical limitation on the initial step size.\n\n[WWB2018] X.Wu, R.Ward, and L.Bottou, WNGrad: Learn the Learning Rate in Gradient Descent. arXiv, 2018. \n[WWB2019] R.Ward, X.Wu, and L.Bottou. AdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes. ICML, 2019.  \n[XWW2019] Y.Xie, X.Wu, and R.Ward. Linear Convergence of Adaptive Stochastic Gradient Descent. arXiv, 2019. \n[Levy2017] K.Y.Levy, Online to Offline Conversions, Universality and Adaptive Minibatch Sizes. NIPS, 2017.\n[LYC2018] Y.K.Levy, A.Yurtsever, and V.Cevher, Online Adaptive Methods, Universality and Acceleration, NeurIPS, 2018.\n[LO2019] X.Li, and F.Orabona. On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes. AISTATS, 2019.\n\n\n"}