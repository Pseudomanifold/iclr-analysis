{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "After an interesting review of calibration methods, the paper describes two new methods for assessing calibration. The first method, SCE, is an extension of the usual ECE to the multi class setting. The second method, ACE, is a slight variation where bins are computed adaptively.\n\nThe paper is interesting and relatively well written. Although the contribution is rather simple (can be describe in less than a page), I can see myself using the SCE/ACE metrics in the near future.\n\nIn order to strengthen the submission, I still feel that the authors should try to describe why the newly introduced metrics can help in applied scenarios.\n(1) OK, the ECE omits a lot of important predictions (e.g. fig 1) --> give a real application where this matters\n(2) OK, adaptive binning seems a sensible approach. --> give a real application where a difference between 0.99 and 0.999 does make a difference\n\nThe newly introduced metrics *are* interesting, and the theoretical justifications *are* sensible. The paper would be even better if the applied motivations were better described. Since the proposed method does have the potential to be applied in industrial/applied scenarios, it is slightly disappointing that it is presented as another academic exercise.\n\nminor remark: it is not clear why the factor (1/K) is needed in the definition of SCE since the weights are already summing up to one -- this makes SCE comparisons between datasets with different number fo classes more difficult."}