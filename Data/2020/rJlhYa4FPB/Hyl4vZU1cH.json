{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "On its surface structure, the paper looks well organized and existing literature on disentangled representations is well reviewed. However, the paper is extremely difficult to read and it is hard to follow the reasoning of the authors. I hope that the authors can clarify misunderstandings during rebuttal.\n\n\u201cExisting works on disentangled representation learning usually lie on a common assumption: all factors in a disentangled representation should be independent. We argue that this assumption is not sufficient and another assumption is vital for disentangled representation learning: information contained in each factor of a disentangled representation is irrelevant to others, i.e. the containing information about data of factors is isolated. \u201c\n\n-How can the first sentence be claimed if there is not a consensus about the formal definition of disentangled representations as stated in the introduction?\n- Assuming that \u2018.. [Independence] is not sufficient  for disentangled representation learning\u2019. Then you propose an additional assumption that seems to be the central contribution of the present paper:\n\n\u2018information contained in each factor of a disentangled representation is irrelevant to others, i.e. the containing information about data of factors is isolated\u2019.\n\n- I don\u2019t understand this notion of information that must be different than independence, namely that \u2018information contained in each factor [...] is irrelevant to other [factors] \u2019 and \n\u2018the containing information about data of factors is isolated\u2019.\n\nI guess the notions of \u2018data of factors\u2019, and \u2018information being isolated\u2019, \u2018irrelevant information\u2019 need to be clarified. The text in the introduction is unfortunately also not very helpful\n\n\u201c... Since each factor [...] corresponds to a single factor of variation in data, \neach factor only contains the information of the corresponding factor of variation in data.\u201d\n\nI think there is a collection of \u2018factors\u2019 and a collection of \u2018factors of variation in data\u2019, and between these two objects there is a correspondence. The statement \u2018each factor only contains the information of the corresponding factor of variation in data\u2019 implies that a \u2018factor of variation in data\u2019 is an object that posses information that is contained in the factor.\n\t\t\t\t\nAt the end of the introduction, the authors state \n\u2018conditional independence originates from no-sharing-parameter block and the independence\u2019 of noise in reparameterization trick, which can be regarded as inductive biases on model. \n\nThis statement is alarming as it hints at a clash of terminology, the concept of independence in the information theoretic/probabilistic sense -- that is the density of two random variables can be written exactly as the product of marginals -- with \u2018the lack of a functional dependence\u2019. \n(Consider the Box-Muller method for generating independent Gaussian random variables x and y, where x = r cos(2pi u) and y = r sin(2 pi u) where u is uniform on the unit interval and r is exponential with rate 2. x and y are independent random variables but they are both functions of  r and u. Hence parameter splitting is not necessary for independence.)\n\nThen, we encounter (9) and the following statement \t\t\t\t\n\"To conclude, conditional independence originates from no-sharing-parameter block and the independence of noise in reparameterization trick, which can be regarded as inductive biases on model.\"\n\nUnfortunately, this conclusion is not correct. (9) is the defining property of an encoder with a factorized Gaussian -- it does not follow from 'non-sharing' of parameters. To see this, assume each mu_j = f(x), i.e. all means are the same, all parameters are shared. This is not a very flexible encoder distribution but still enjoys the same conditional independence structure, because parameter sharing and independence are different concepts. \n\n(8) is misleading as mu and sigma are not random variables in a VAE.\n\nI suspect there may be some interesting ideas here but the paper clearly needs further iterations for clarification of concepts so it is accessible for the general technical audience.\n"}