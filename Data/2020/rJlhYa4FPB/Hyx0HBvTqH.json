{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThe paper proposes a new technique for unsupervised learning of disentangled representations. The new module is called \"split encoder\", and is derived by adding an information isolation requirement for each of the factors of variation. \nDifferent from  unsupervised techniques which enforce independence of the different factors. Here the relation between the representation and the data is discussed, revealing the importance of conditional independence of factors.  \n\n\n\nPros:\nWhile a bit hard to follow at times, the writing is quite good and the topic is important and relevant to the field. \nI appreciated the theoretical derivations of this work, emphasizing the relation to data when \ndiscussing disentangled representations. The connection between the independent information assumption and the two KL terms is also quite interesting. \nFigure 2 and 3 nicely demonstrate the assumed correlation between MIG and the conditional independence. \n\n\nMajor issues:\n\nMy main issue with this work is the split encoder contribution. I didn't find very strong evidence of significant gain due to the split module. To adopt such modifications in standard architectures like VAE's, a much more rigorous experimentation is needed show possibly multiple levels of independence and not just two (split / no-split). \n\nOne important inductive bias is not discussed i believe, which is the number of factors. This seems assumed to be known in advance. Related to this is how can we prevent the case of combining two factors into one latent dimension? I think this will still fulfill the information requirement...,but could surely hurt generalization-ability. \n\nAs explicitly stated in Section 2, the assumption is not sufficient to guarantee disentanglement and the reconstruction error should indicate that one does not converge to the trivial solution of zero mutual information between z and x. That said, looking at the reconstructions in Figure 5, there seem to be a pretty high error for such easy data. I certainly find it hard to agree with the description: \"the learned representations are well disentangled except some small flaws\". In fact, the Vanilla VAE results appear more appealing and not less disentangled to my eyes.  Also, in Figure 8 the highest MIG is for the vanilla VAE. \n\nIn Section 5.2.2 a result for cars 3D behaves differently than dSprites. This is one reason (among many others) to show experiments on more than 2 datasets. I believe the work by Locatello et. al. speaks strongly for that and should be taken seriously. Even more so when using a single metric (MIG). Moreover, the explanation given on the strength of the enforced disentanglement, can it be verified by increasing betta for vanilla-VAE? I would be much more convinced by this explanation. \n\nLastly, in the end of 5.2.2 the authors state: \"o..obviously have better reconstructions...\". IMO, the results are very much alike. Perhaps specific pointer or zoom-ins to better reconstructions example would be helpful, but from staring at the results for a bit, I can't say there is an obvious advantage to the split-encoder. \n\nMinor issues:\nThis one is not clear to me: \".. And rarely ensuring independence like FactorVAE is not sufficient for learning disentangled..\"\n\nthe analysis in 5.2.1 about dSprites being simple, etc. is not clear to me. \n\nin the kl-div Vs. reconstruction error experiment, what are the different data points? also, how are the recon-errors measured? (what units is 1490-1510?)\n\nsome typos spotted:\n\"cannot ensures\"\n5.2.1: combing -> combining\n\"Therefore, for a single factor...\" --> the variable there should be z_j and not z, i think. \n\n"}