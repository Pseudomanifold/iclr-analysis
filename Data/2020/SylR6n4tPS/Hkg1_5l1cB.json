{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper addresses captioning generation for images and videos, by proposing a novel cyclical training regimen consisting of three steps: decoding, localization, and reconstruction. The experimental results show that the performance on image captioning and video captioning are improved without grounding supervision.\n\nI lean to accept this paper. The motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting. The experimental results show that the proposed method can boost performance both qualitatively and qualitatively. I have several comments and questions below.\n- Why do the authors introduce GVD without self-attention as a baseline? Table 1 and 2 show that removing self-attention degrades the performance. If the combination of self-attention in GVD and cyclical training proposed in this paper is complementary to each other, it does help to improve the overall accuracy.  \n- While the authors develop a cyclical training pipeline, including decoding, localization, and reconstruction, Figure 1 does not show which part corresponds to the decoding phase. The authors should clarify it to make the paper easier to be understood.\n- Equation (5) seems to be strange. $\\theta^*$, a sum of two parameters for each arg max operator, doesn't guarantee that each term in the right side of Eq. (5) keeps its max. This equation seems to be a conceptual one, and the actual training would be performed according to Eq. (7). Therefore, the experimental results might not be influenced by the error in Eq. (5).\n- $\\hat{r}^l_t=\\beta_t^\\top R$ between Eq. (6) and Eq. (7) means that $\\hat{r}^l_t$ is a row vector while $r_n$ seems to be a column vector. Since $R = [r_1, r_2, ..., r_N]$ for $N$ regions, $\\hat{r}^l_t= R \\beta_t$ seems to be appropriate. The authors should correct it. I have a similar comment for $\\hat{r}_t = \\alpha_t^\\top R$ in Eq. (2).\n- In the caption of Table 5, the number equal to or smaller than ten should be spelled out; \"5 runs\" should be \"five runs.\" There are similar errors, such as \"5 GT captions\" and \"1 GT caption\" in Sec. 4.\n- The format of items in References is not consistent.\n- According to Sec. A.4.1, $\\lambda_1$ and $\\lambda_2$ are tuned between 0 and 1. How are the experimental results sensitive to these hyperparameters? Additional experiments using different $\\lambda_1$ and $\\lambda_2$ would be helpful."}