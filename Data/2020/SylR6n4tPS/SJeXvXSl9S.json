{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an architecture that grounds words from a captioning model, but without requiring explicit per-word grounding training data. Instead, they show that it is sufficient to use cycle consistency, verifying that by predicting word->grounding->word the two words are the same. \n\nGeneral: \n\nCycle consistency has been shown to be very useful in replacing explicit paired data, for eample in image-to-image translation (CycleGAN, or the more recent FUNIT). This paper takes it to the domain of vision and language. While the novelty is not very large it seems like a solid step in an interesting direction.  Evaluated on both image and video captioning with substantial localization improvement in the specific relevant eval settings. \n\nSpecific comments: \n\n-- In Table 1, the part of \"Caption Evaluation\" the proposed method is in bold, but it seems that \"Up-Down\" method out-performs the proposed method in B@1 and B@4. \n\n-- Are words that are not nouns/verbs (the/a/are/with/etc) handled differently? It doesn't really make sense to localize them just like object words. \n\n-- The localization model is linear? What would be the effect of richer models on localization accuracy? \n\n-- Qualitative analysis: It would have been useful to add evaluations by human-raters to measure the perceptual quality of the localization. \n\n-- Error analysis? examples and analysis  of failure cases? \n\n"}