{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #775", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents an algorithm for optimizing an function f under the constraints that the square matrix variable x represents \"metric\". In this context, this means that we have also observed a graph G with n vertices, and x is of size n by n, x(i, j) < x(i, e) + x(e, j) if i ~ e and j ~ e are adjacent: this is a generalized for of triangle inequality.\nAuthors argue that the constraint \"x is a metric\" translate into exponentially many linear constraints, which results in to a hard to solve problem\nThe algorithm they propose to tackle this (Algorithm 1) has two subroutines that are shown in Algorithm 2 (Forget and Project). The Project subroutine itself is a projection onto a convex set according to a Bregman divergence, which is not trivial. In this paper I understand that authors only consider metrics of type x' L x where L = C'C >0 is psd\n \nAuthors claim that the sequence created by their algorithm asymptotically converges to the global optimum, and show numerical superiority to baselines.\n\nMajor remarks:\n\nMy general feeling is that the paper overstates its results. The paper has some good contribution, which could be better emphasized.\n\nThe algorithm stacks multiple subroutines which are not necessarily very light. I am skeptical about the numerical efficiency of such algorithms.\n\nTheoretical results are stated asymptotically while interpreted in the text as finite steps results: page 5, after Corollary 1., read \"The algorithm spends the first few iterations ...\" in this case, a theoretical result should support the claim\n\nThe algorithm starts at a stationary point of f. This itself can be nontrivial. Can authors discuss this?\n\nMinor remarks:\n\nmetric and distance to me mean the same, hence the first sentence of the intro doesn't read easily..\n\nwhat is \\cal A line 5 of Algorithm 1? It seems to be a \"list of hyperplanes\" according to the previous text, but it is unclear to me how to build it algorithmically \n\nThe notation L is confusing in Algo 1 MetricViolation: wasn't L the matrix defining the metric?\n\nA few typos: l. 12 Algo 1, e = (i, j), 3.2 \"global optimum [remove solution].\"\n\n"}