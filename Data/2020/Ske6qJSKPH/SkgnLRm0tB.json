{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The results are given only for the CIFAR datasets. Even for these two datasets the authors use very outdated networks, e.g., the best error rate for CIFAR-10 is in order of 6 percent. One should use contemporary/bigger networks, e.g., WRNs published in 2016 would give you about 4 percent. \n1) The initial learning rate for CIFAR-10 and CIFAR-100 are different, respectively 0.1 and 0.0003. The use of such small initial learning rate for CIFAR-100 is not motivated especially given that it is usually in order of 0.05 or 0.1 when resnets are considered. \n2) The authors don't compare to cosine annealing without restarts which is a pretty strong baseline. \n3) The authors compare to SGDR but don't set its initial number of epochs in a way that its last restart convergences at around 200 epochs. \n4) The proposed method has its own hyperparameters which greatly influence the results as shown in the appendix. I suspect that setting these hyperparameters is exactly what controls the slope of the learning schedule. \n\nOverall, the results are not convincing. The authors show that the previous adaptive approaches don't work well on the CIFAR datasets (despite the fact that their authors claimed the oppositve) and I don't think that the paper contains enough material to avoid the situation that futures approaches will claim similar things about the current study. "}