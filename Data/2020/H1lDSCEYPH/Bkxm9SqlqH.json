{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Overview: This paper presents a new generative modeling approach to transform between data distributions via a technique the authors dub \u201cneuron editing\u201d. Their approach for \u201cneuron editing\u201d can be used to learn how neurons in a DNN encode particular transformations in the latent space, with the hope of using it to be able to generate data that are out-of-sample and/or do not lie on the same manifold as the data used to train the generative model. Experiments were performed on the CelebA dataset. The authors further demonstrate the usefulness of their proposed idea by using it to remove noise/batch effects from data and also predicting synergy between drugs by using their idea to model the effect of drug treatments.\n\nWhile the idea behind this paper is interesting and their method does seem to provide improvements over other generative models, the paper is a bit difficult to follow and the applications to biology are not explained well enough to understand the implications of their results. The fact that the authors did not compare to StyleGANs also seems a bit suspicious because these models are sota for the types of problems that this paper addresses. Additionally, the authors should consider rewriting the motivation for this problem to be more general. This is a convincing new method to transform distributions in any setting, but the introduction would lead one to believe its applicability is mostly to computational biology. \n\nDetailed comments:\n- The writing and motivation needs to be reworked in order to ensure that the introduction, results, and abstract match in their tone and content. The abstract uses a face recognition/generation application to motivate the work, the introduction focuses solely on computational biology, the results switch between biology and face recognition.\n\n- Section 2: Three desirable properties for a transformation are listed, but what makes these properties desirable?\nEquation (1): Not obvious why the piecewise function definition is necessary, since the second equation seems to hold for j = 0 and j = 99 as well. This should either be corrected or simplified.\n\n- \u201cMode collapse refers to the discriminator being unable to detect differences in variability between real and fake examples.\u201d Not true. Mode collapse is when a large region of the model\u2019s input space maps onto the small region around a single (often bad) sample.\n\n- Section 3: It would be helpful to see gradual interpolation from the original latent representation of a sample along the direction the neuron edit will be performed, and continued beyond the proposed final latent activations, to demonstrate how this process actually affects the model\u2019s output.\n\n- Doesn\u2019t really seem like the authors tried very hard with the other methods (especially the vanilla GAN) and the omission of StyleGAN, the current state-of-the-art in this kind of transformation learning, is conspicuously omitted. It is highly likely that a well-trained StyleGAN would do better than the other GAN/AE techniques compared against.\n\n- A PCA-based transformation is applied directly to the image data, but it would be interesting (and perhaps more informative) to perform an alignment of the PCs of activations in the latent space.\n\n- Section 4: Again, the motivation distinctly focuses on computational biology when one could easily imagine this approach being applicable to a variety of problems.\n\n- Comparisons with StyleGANs would\u2019ve been appreciated, especially given the fact that they\u2019re now considered state-of-the-art when it comes to modifying the latent space and creating out of sample images as proposed in this work. \n\n- The idea is interesting, though one wonders if there isn\u2019t any other work where the neurons have been \u201cedited\u201d to accommodate different transformations, given that the idea is itself rather intuitive. A more thorough literature review in that regard would be helpful.\n\n- The CelebA experiments help lay establish an intuitive understanding of the proposed technique and were helpful. However, the ideas are a little disconnected with the biological applications of the technique. Better motivation/bridging of the two sets of experiments would be nice.\n\n- A more detailed explanation and analysis of the combinatorial drug application would\u2019ve been helpful to understand the results.\n"}