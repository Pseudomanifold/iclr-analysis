{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses hierarchical deep reinforcement learning (RL), an important problem in control learning and RL. Based on my understanding of this paper and recent prior work, the most important difference between the proposed approach (HiDe) and other recent approaches, such as HIRO and HAC, is that the top-level goal proposal policy uses a learned planner based on VIN and a learned attention mask to decide on a subgoal. There are also other differences, e.g., this policy outputs a goal position that is relative to the agent's position, rather than an absolute position. HiDe seems to demonstrate impressive transferability to both unseen mazes and new agent embodiments, which are important problems to address for hierarchical RL.\n\nIntroducing learned planning and attention into the top-level policy seems to also introduce additional assumptions into the method. For example, it is my understanding that HIRO and HAC use the same state representation, e.g., joint positions and velocities of the agent, as input to their top-level policies. In contrast, HiDe uses a top down view of the maze and the x y position of the agent, which certainly is more privileged information. If this is correct, first, this should be discussed more thoroughly and directly in the paper. Second, the experimental setup should be elaborated on: is HIRO or HAC modified to include the same information for the top-level policy? Or can HiDe somehow be extended to not require this information? I do not think that requiring this information is egregious, but currently the experimental comparison is not clear in this regard.\n\nThe experiments are arguably the strongest part of the paper, and the transfer results and videos are quite nice. But there is still room for improvement. Table 1 and Figure 5 seem disconnected. In particular, the numbers reported in Table 1 are clearly not achieved in Figure 5. Is the figure cut off early? Furthermore, an additional experiment on a more complicated domain would greatly strengthen the paper. A humanoid agent, for example, seems easy to test for the current method. Another option would be the movable blocks tested in HIRO, though it is unclear if this readily fits into the current method's assumptions. In my opinion, as the paper currently rests heavily on the results, this section should be further improved. Doing so would also improve my rating of the paper."}