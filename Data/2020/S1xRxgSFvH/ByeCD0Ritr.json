{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an approach to reduce the number of a neural network by sharing the convolutional weights among layers. To convert the first layer into the right number of features padding is used. The last layer I suppose is instead a normal classifier on the fully connected representation (for VGG) or on the average pooling(for ResNet). Results on different datasets and architectures show that the proposed approach can highly compress the number of needed parameters with a minimal reduction of the network test accuracy.\n\nI lean to reject this paper because, in my opinion is very similar to (\"Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex\" Qianli Liao and Tomaso Poggio), which is not mentioned in related work. This paper, published in 2016 was already proposing the idea of reducing the number of parameters of ResNet by sharing the weights of each layer and therefore consider ResNet with shared weights as a recurrent net.\nIn this paper the setting are slightly different, authors add also a variant with additional 1x1 convolutions and show also results with additional compression. However, in my opinion, the main idea is the sharing of convolutional weights, and this is not new.\n\n\nAdditional Comments:\n- This paper considers only the number of learnable parameters of a network. However, in many cases, for applications, it is more important to save memory (which is not the case as the activations should still be saved for backpropagation) and computation. In my understanding the final computation of the model is actually increased because it uses more channels at lower layers (which corresponds to high resolution features maps). Authors should comment about that.\n- In section 4, VGGNet-like Architectures and ResNet-like architectures the authors mention a baseline E-VGGNet or E-ResNet with exactly the same architecture as the shared weights network (thus same number of channels at each layer), but without sharing. However I could not find the performance of that interesting baseline in the results.\n\n\n\n\n"}