{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose to use the *same* convolutional layer in every layer of a DNN. The network effectively is converted into repeatedly applying the same convolutional filter at multiple scales. The idea is motivated by wavelet decompositions and related work. The authors show that by repeatedly applying the same filter, the number of parameters that need to be stored for a model reduces proportionally to the depth of the network. At the same time, experimental evidence is provided that the performance of these models is not affected, when compared to the baseline (full) model. \n\nCOMMENTS:\n- the paper is well written, but overly verbose. there are several areas where the explanation can be compressed, making room to add more informative details (which are in the appendix), or increasing the size of the figures (which are too small)\n\n- the paper seems lacking a bit in experiments. If the authors can show that the same filter applied L times achieves about the same performance, why not also experiment with different L? i.e. does VGGNet actually need L layers? what if only 2 layers are used? this will help with the overparametrization problem as well. \n\n- Figures 3 and 4 are hard to read. Please increase their size.\n\n- page 5 line 2: how does padding the input with (n-3) empty channels affect performance? If you're learning the filters through backprop, will they not always be learning to fit to 0 ? or am i missing something? \n\n- along the above lines, why not have the input layer to be a different filter with 3 channels and then have a common filter for all upstream layers?\n\n- in multiple places in the text, you refer to the number of \"independent\" parameters. I dont see why the parameters need to be independent. Unless there's some orthogonalization happening at the weights, calling them independent is incorrect. \n\n- paragraph above sec4: you add separate \"linear\" layers for the 'SL' models. Can you describe how many addditional parameters you have to learn?"}