{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to modify a standard CNN by requiring all of its layers to share the same filter set, essentially allowing it to be expressed as an iterative (or recurrent) network.  This also has the effect of forcing the same number of feature channels to be used throughout the network.  For ResNet-like architectures with bottleneck blocks, sharing occurs at the level of the block (3 conv layers in series that are repeated).  Another variant of the sharing pattern inserts unshared 1x1 convolutional layers after shared layers or blocks; this adds some flexibility while still reducing parameters compared to standard CNNs.\n\nOn CIFAR-10, CIFAR-100, and Tiny ImageNet, experiments demonstrate the ability of the sharing scheme to reduce parameters without impacting accuracy (or more drastically reduce parameters at the cost of accuracy) (Tables 1ab, 2a).\n\nHowever, results are less compelling on ImageNet (Table 2b), where SL-ResNet-50 and SL-ResNet-34 are both less accurate than the baseline standard ResNets as well as ShaResNet [Boulch, 2018].  The accuracy gap between SL-ResNet and ResNet on ImageNet (Table 2b) is significant (approx 5% Top-1 and 2% Top-5 accuracy) and might make it difficult to justify use of the proposed method in this setting.  As ImageNet is the most challenging of the datasets used, this is cause for concern.\n\nThere is also a major concern with respect to novelty and related work.  Unfortunately, the paper appears to have completely missed the following highly related publication from ICLR 2019:\n\nLearning Implicitly Recurrent CNNs Through Parameter Sharing\nPedro Savarese, Michael Maire\nICLR 2019\n\nThis prior work proposes a network structure in which a set of L layers share a set of k parameter templates.  The templates and sharing coefficients are learned as part of the standard training procedure.  This prior work demonstrates both parameter savings and accuracy improvements when training networks in this manner.  Additionally, this prior work shows that some learned networks can be converted into explicitly recurrent forms as a post-processing step.\n\nThe paper under review appears be a special case of this prior work with the number of templates k = 1 (shared between all layers).  It is possible this is an important special case, worthy of significant attention on its own.  Notably, [Savarese and Maire, 2019] considered sharing across at most all layers within the same stage of a residual network, rather than all layers in the network.  However, arguing for the importance of this special case would require focused experimental comparison and analysis, which is not present in the current version of the paper.\n\nNovelty is clearly limited in light of this overlooked prior work.  At minimum, citation, discussion, and experimental comparison to the above ICLR 2019 paper is necessary."}