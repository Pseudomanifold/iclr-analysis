{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "*Paper summary*\n\nThe authors build an input transformation invariant CNN using the TI-Pooling architecture of (Laptev et al., 2016). They make some modifications to this, namely 1) they replace the convolutional filters, with input-dependent convolutional filters, and 2) they add a decoder network from the final representation, which reconstructs an input transformation rectified image, to encourage the final representation to be fully transformation invariant.\n\n*Paper decision*\n\nI have decided to assign this paper a reject because of two main reasons. \n\n*Supporting arguments*\n\nOne reason is that the base architecture is not novel. This in itself is not a key issue, but I would expect the authors to have done some in depth analysis or experimentation otherwise to compensate for this. I regret, the authors may just not have known that the ideas were already explored in the literature. The second reason is that the work is not well placed in context with prior works. This is both evident in the lack of referenced works (see below for a list) and the lack of sufficient baselines, against which they compare. For instance, if the authors had considered \u201cLearning Steerable Filters for Rotation Equivariant CNNs\u201d by Weiler et al. (2018) they would have known that their MNIST-rot-12k results are not state of the art as they state. In Weiler et al., the authors report 0.714 test set error on MNIST-rot-12k compared to ICNN\u2019s 0.98.\n\nThis all said, I think the paper is well-written and very clear. The structure is straightforward and the experiments seem repeatable from the descriptions made. The stated aims of the paper are also clear: to learn input transformation invariant CNNs using input-conditioned filters.\nUnfortunately a lot of supporting material and prior work has been missed. I list a lot of them here. \n\nWorks on input-conditioned filters and invariance. These are the most important\n\n-Dynamic Steerable Frame Networks, Jacobsen et al., 2017\n-Dynamic Steerable Blocks in Deep Residual Networks., Jacobsen et al., 2017\n\nWorks on input-conditioned filters:\n\n-HyperNetworks, Ha et al., 2016\n-Dynamic Filter Networks, de Brabandere et al., 2016\n\nWorks on invariance:\n\n-Invariance and neural nets, Barnard and Casasent, 1991\n-Group Equivariant Convolutional Networks Cohen and Welling (2015)\n-Harmonic Networks: Deep Translation and Rotation Equivariance: Worrall et al. (2017)\n-Steerable CNNs, Cohen and Welling (2017)\n-Spherical CNNs, Cohen et al. (2018)\n-CubeNet: Equivariance to 3D Rotation and Translation, Worrall and Brostow (2018)\n-Learning steerable filters for rotation equivariant CNNs, Weiler et al. (2018)\n-Gauge Equivariant Convolutional Networks and the Icosahedral CNN, Cohen et al. (2019)\n\n*Questions/notes for the authors*\n\n- Please address the missing references\n- Are the input-conditioned filters conditional on position in the activations, or are they shared across all spatial locations of the image? This is not clear from the text.\n- The image reconstruction reminds me of Transforming Auto-encoders (Hinton et al., 2016) and Interpretable Transformations with Encoder-Decoder Networks (Worrall et al., 2017). How is your setup different?\n\n\n\n"}