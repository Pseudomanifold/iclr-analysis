{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose to express the weight of a convolutional neural network as a coupled Tucker decomposition. The Tucker formulation allows for an efficient reformulation. The weigths of the sum of Tucker decompositions allows is randomly set at each iteration during training, with each term of the sum having a different rank.\n                                                                                                                                                                                                            \nThe method is interesting, however the novelty is low. There is already large bodies of work on parametrizing neural networks with tensor decomposition, including coupled decomposition.\n\n\nHow does the proposed method compared to the related method DART? And to a simple coupled decomposition?\nHow is the method different to training several network with the same Tucker parametrization but different ranks? What about memory and computational efficiency?\nIn any case, these should be compared to.\n\nThe notation should be kept consistent throughout: e.g. either use t,u,v,w or d1,d2,d3,d4. Notation should be unified in the text and captions (e.g. Table 1).\nIn 3.2, when specifying the size of G, should it be G_r? Same for B and C.\n\nWhy the convolution R should be grouped? Should it not be a regular convolution?\n\nFor ot', u' being the group-size, what is o? It was not introduced.\n\nThe response reconstruction is only useful if the same, uncompressed network is already trained, would not be applicable for end-to-end training.\n\nThe model is a simple 4-layer network, not fully described. An established architecture, such as ResNet should be employed.\n\nExperiments should be carried on ImageNet, or at least not just on CIFAR10.\n\nThere is no comparison with existing work, e.g. parametrization of the network with Tucker [1, 2] or CP [3]\n\n[1] Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications, ICLR 2016\n[2] T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor, CVPR 2019\n[3] Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition, ICLR 2015\n"}