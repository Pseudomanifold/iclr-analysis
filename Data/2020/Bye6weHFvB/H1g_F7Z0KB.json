{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to learn a representation space that is useful for planning. This is done by a 2-step process: (1) learn a metric to reflect locality, (2) utilize that metric to learn a global metric by utilizing UVFAs from Reinforcement Learning literature. While the idea may be an interesting direction to improve sample efficiency of planning, it is not the first work that proposes to combine planning with representation learning, and I do not think the work is clearly presented/motivated/validated sufficiently to deem it acceptable.\n\nAs presented, there are many details that are unclear or imprecise. I list them below.\n\nFeedback:\n(1) The contrast between learning playable representations vs. planning via representation learning (as proposed) is  poorly motivated. The presentation ignores well recognized works in the latter in terms of UPNs [1], VINs [2].\n(2) The RL background section is poorly presented -- as written Equation (4) seems incorrect. Further, is a dynamics model learned to be able to utilize the correct form of Equation (4)? It is never specified. How exactly is a plan extracted for any of the experiments?\n(3) Possibly wrong citation for Equation (2) -- maybe cite the Oord et. al. paper? Further, c vs C -- the notation in general in the paper is inconsistent and poor.\n(4) Algorithm 1, line 4 -- typos for x. Further for Contrastive learning methods to be effective, the negative sampling needs to be much higher. What ratio of samples is used for local metric learning?\n(5) If an actual forward dynamics model is not used (Section 3 end), then as given in Algorithm 2 from pseudocode I'm completely unsure how a plan is extracted -- How is the UVFA used to extract a plan? What is N(1,\\epsilon)? P-norm?\n(6) The Dijkstra psuedocode is rather incomplete.\n(7) How is sampling done with the local metric function? How is a plan generated? These important details are missing.\n(8) Figure 4 is poorly presented/annotated. In the description \"Plan2Vec correctly stretched out learned..\" -- no it doesn't, visually it seems wrapped too. \n(9) There exists literature in RL to combine the 2 step metric learning process to 1 step. This is relevant. [3].\n(10) What is the action space of these domains? Based on visualization in Figure 3 (2), are the actions continuous? What is the action space for Figure 7? These details details are missing.\n(11) Description of the StreetLearn dataset would be useful. Further an example of why Plan2Vec generalizes (last para, Section 4.3) would be useful. Just statement based claims seem rather vacuous.\n(12) The last statement in Conclusion - why? The paper has made an argument against utilizing generative modelling in an unsupervised manner. So why would including it improve it? Such unexplained statements reflect poorly.\n\nQuestions:\n(1) What do you see as the contribution of the work? Why is it new/different from existing literature?\n(2) How exactly do you generate a plan?\n(3) How is the UVFA V used?\n(4) When to use UVFA? When to use Dijkstra? Why are the choices in the experiments as made?\n\n\nSome typos to help future version of the paper:\n(1) Section 2 -- We now overview --> We now review.\n(2) Section 2, UVFAs -- expected discounted future value --> expected sum of discounted future rewards.\n(3) Section 2 completely ignores the discount factor/horizon of an MDP, although the utility here I suppose relies on the horizon aspect.\n(4) Figure 6 explanation is very sloppy (description and body).\n(5) The Zhang et. al. reference in Section 4.2 is unclear.\n\nWhile I am not from the planning community, I am from the RL community - and as presented the paper is ignoring a lot of details, and was extremely difficult to piece together for me.\n\n[1] Srinivas, Aravind, et al. \"Universal planning networks.\" arXiv preprint arXiv:1804.00645 (2018).\n[2] Tamar, Aviv, et al. \"Value iteration networks.\" Advances in Neural Information Processing Systems. 2016.\n[3] Wu, Yifan, George Tucker, and Ofir Nachum. \"The laplacian in rl: Learning representations with efficient approximations.\" arXiv preprint arXiv:1810.04586 (2018).\n"}