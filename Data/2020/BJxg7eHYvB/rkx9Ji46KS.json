{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, an RL-based recomputation method for GPU memory allocation is proposed. The idea is to learn which intermediate variables should be stored or discarded to save memory while reducing overhead. In experiments, several existing methods are compared with the proposed one for various NN architectures such as ResNet and LSTM. \n\nThe idea of applying RL to recomputation is interesting and reasonable. However, the paper is not solid enough due to the lack of comparison with other recomputation methods.\n\nIf I'm understanding correctly, the key benefits of the proposed methods are the versatility (applicable to any NN architectures) and the ability that takes into account the tradeoff between memory reduction and runtime overhead. In this area, however, this paper is not a seminal work. For example, Kusumoto et al. (2019) studied a recomputation method based on dynamic programming in the same setting. The paper should contain the discussion toward this work (pros/cons against the proposed method) and also should compare in the experiments. \n\nAlso, it is better to explain the main limitation of the proposed method. For example, in what scenario the proposed method may perform worse than the existing methods?\n\nKusumoto, Mitsuru, et al. \"A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation.\" arXiv preprint arXiv:1905.11722 (2019)."}