{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposes to use DQN to learn recomputation decisions in training deep neural networks. The authors build a solution that can decide between variable swapping and recomputation with a given memory limits.\n\nStrengths\n\nFirst of all, I like the idea of applying learning for recomputation decisions, the authors propose a reasonable solution to the problem.\n\nIt is nice to have a memory limit as an input to the algorithm, so that most of the memory can be utilized. It is also good to have mixed decisions between swapping and recompute.\n\nWeakness\n\nOne potential weakness of the  method is how generalizable the DQN model could be. From what I can read, it is unclear whether the authors have trained the DQN model on the same set of model architectures and then use the same model to predict the decision on the architectures. If that is the case, it is unclear whether the gain comes from exhaustive random search during the training phase or due to the generalization of the model.\n\nIt would be great to address the generalizability of the proposed method. What will happen if you apply the same DQN to ResNet1k without retraining the models on that architecture? What will happen if you change the memory limit setting to cases that you have not seen before?\n\nMy second complaint is about the evaluations. While the authors have proposed to take both swapping and recompute as set of actions. It is unclear what is the additional gains. The paper can be improved by showing experimental comparisons between methods that only applies recompute or swapping. \n\nFinally, please note that acknowledgement section breaches the anonymous requirement of the reviewing process. Although the above review was written without taking this into consideration.\n\nBecause the above weakness of the paper, I think this paper should not be accepted to the program. I would encourage the authors to improve the paper according to the comments and submit to future venues.\n"}