{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "I believe this paper violates the ICLR anonymous submission policy by including an acknowledgments section that thanks several people by name.  Acknowledgments should not appear in the review version.  I believe this violation is sufficient cause for automatic rejection of the paper and recommend such action.\n\nMy review of the paper content, not factoring in this rule violation, follows.\n\n---\n\nThis work addresses management of GPU memory resources during deep network training by proposing to learn a policy whose action space includes both swapping variables between CPU and GPU memory, as well as recomputing variables on the GPU (trading compute for memory).  A deep Q-network (DQN) is used to learn the policy.  Experiments across training a range of networks demonstrate superior memory savings at lower overhead compared to prior approaches.\n\nThe paper claims a reinforcement learning approach (DQN) is valuable in order to handle training of networks with variable execution flow.  However, most of the benchmark networks, including VGG, ResNet, and UNet have a predetermined static architecture.  Even with the example of stochastic depth ResNet, the expected depth and its variance are known.  GPU and CPU resources and memory capacity are also known ahead of time.  It is not clear to me why reinforcement learning should be a necessary or even preferred approach to scheduling in such scenarios.  Shouldn't it be possible to predetermine the best schedule using some form of constrained optimization?  Why is it appropriate to use reinforcement learning?\n\nCoupled to the above questions, might the experimentally observed performance advantages be attributable to simply having a wider range of scheduling actions (e.g. more flexible combinations of recomputation and swapping) rather than the DQN approach?  An ablation study or statically scheduled baseline that has the same available action set would be a helpful reference point.\n\nResults speak only to memory usage.  How much training time is actually saved?\n\nFinally, I am unsure whether ICLR is the best venue for this work; a systems conference might be more appropriate."}