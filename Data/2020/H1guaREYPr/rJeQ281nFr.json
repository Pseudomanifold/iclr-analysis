{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a multi-modal learning framework that links the inference stage and generation stage for seeking the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. Experimental results show that the proposed network can not only match the relationship between the human face and speech, but can also generate the high-quality human face sample conditioned on its speech.\n\nThe writing and presentation are clear.\n\nMy concerns are as below.\n1) What are the training computational complexity and testing time cost of the proposed method?\n2) How can we determine truncation threshold more elegant? Any theoretical analysis and sensitive analysis?\n3) How did the authors handle model collapse during training? \n4) The format of references should be consistent."}