{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for reducing the representation error generative convolutional neural networks by combining them with untrained deep decoder. The method is evaluated on compressive sensing and super-resolution, where a better performance than the isolated use of Deep Decoders and GAN priors. The main contribution of the paper is not the performance, but the simplicity of this approach.\n\n\n\nFor the title, I would suggest to replace the word Removing with Reducing.\nFurthermore, the clarification of \"GAN prior\" is very nice in the introduction, maybe you could already clarify it in the abstract.\n\nYou should perform a critical grammar check. There are too many commas, for example:\n\"At sufficiently difficult superresolution problems, the Hybrid model outperforms, the Deep\nDecoder, Bicubic upsampling, the BEGAN prior, and the BEGAN as DIP prior.\" -> there should be no comma after \"outperforms\"\nThe sentence from Page 3 to 4 reads strangely, probably a word is missing after \"For our GAN prior, we use the BEGAN architecture, and we demonstrate similar results\"\n\"Philosophically, they hybrid\" -> \"Philosophically, the hybrid\"\n\nFig 6 caption - shouldn't it be 49152 instead of 49512?\n\nYou perform various very good analysis experiments, which is well appreciated. Still, it would be good to think about some more experiments (and include at least one of them in the paper):\n1. You compare to IGAN and show that you achieve similar performance. You describe that a state-of-the-art approach are invertible generative models and that they are very time consuming (e.g., 15 minutes for a 64x64 image). How good would the invertible models be in terms of performance? Could you perform tests as well?\n2. It would be great if you report the runtime of all experiments as well - maybe also the memory usage."}