{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Originality: The paper proposed a new scaling loss strategy for mixed-precision (8-bit mainly) training and verified the importance of rounding (quantization) error issue for low-precision training. \n\nQuality: The authors clearly illustrated the benefit of their proposed loss strategy and the importance of quantization error for two different tasks (image classification and NMT). The experiments are very clear and easy to follow.\n\nClarity: The paper is clearly written with some visualizations for readers to understand the 8-bit training. \n\nSignificance:\n1. The enhanced loss scaling strategy is interesting but the method seems hand-tuning. Is there any automatical way or heuristic deciding way?\n2. The stochastic rounding method is very intuitive. How do you choose the value of \"r\" in the equation? Is it a sensitive hyper-parameter or not?\n\nTypos:\nPage 7: with with roughly 200M ->  with roughly 200M \n"}