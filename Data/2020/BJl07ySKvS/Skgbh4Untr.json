{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "= Summary\nA method for a refinement loop for program synthesizers operating on input/ouput specifications is presented. The core idea is to generate several candidate solutions, execute them on several inputs, and then use a learned component to judge which of the resulting input/output pairs are most likely to be correct. This avoids having to judge the correctness of the generated programs and instead focuses on the easier task of judging the correctness of outputs. An implementation of the idea in a tool for synthesizing programs generating UIs is evaluated, showing impressive improvements over the baseline.\n\n= Strong/Weak Points\n+ The idea is surprisingly simple and applies to an important problem in program synthesis.\n+ The experiments show that the method works very well in UI-generation domain\n- The paper repeatedly claims general applicability to program synthesizers, but is only evaluated in the specific domain of UI-generating programs. I have substantial doubts that the approach would work as well in the domains of, e.g., string manipulation, Karel, or data structure transformations. My doubts are based on the fact that there are easily generalizable rules for UIs (no overlaps, symmetry, ...), whereas other domains are less easily described. This creates a substantial gap between paper claims and empirical results.\n- The writting is somewhat sloppy (see below), which makes it sometimes hard to understand. Names such as \"views\" are used without explanation, and it's not explained how a device is an input to a program (yes, I get what this means, but it makes in unnecessarily hard to follow the paper)\n\n= Recommendation\nI would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider ICLR audience. Improving the readability of the paper would make me improve my rating to a full accept.\n\n= Minor Comments\n* page 1, par \"Generalization challenge\": The second sentence here is 4 lines long and very hard to follow. Please rephrase.\n* page 2, par 2: \"no large real-word datasets exists\" -> exist\n* page 2, par 3: \"even when both optimizations of InferUI are disabled\": at this point, the reader doesn't know about any optimizations of InferUI.\n* page 4, par 1: \"i.e., $\\exists p \\in \\mathcal{L}$\" - $\\mathcal{L}$ is undefined here (will be defined later on the page)\n* page 4, par 2: \"Generate a candidate program $p_1 \\models \\mathcal{I}$\" - in step 2, there are suddenly also $p_2 \\ldots p_n$, which are never explicitly generated. Either adapt this step, or explicitly generate them in step 2 based on the distinguishing input\n* page 7 par 2: \"We use one screen dimension as the input specification $\\mathcal{I}$, the second as the distinguishing input\" - this confused me, as the paper discussed discovering the distinguishing input (page 4, paragraph \"Finding a distinguishing input\"), whereas it sounds here like that input is manually selected."}