{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the problem of generalization of reinforcement learning policies to unseen spaces of actions. To be specific, the proposed model first extracts actions\u2019 representations from datasets of unstructured information like images and videos, and then the model trains a RL policy to optimize the objectives based on the learned action representations. Experiments demonstrate the effectiveness of the proposed model against state-of-the-art baselines in four challenging environments. This paper could be improved in the following aspects:\n1.\tThe novelty of the proposed model is somewhat incremental, which combines some existing methods, especially the unsupervised learning for action representation part that just combines methods such as VAE, temporal skip connections\u2026\n2.\tSome components of the proposed methods are ad hoc, and are not explained why using this design, such as why Bi-LSTM for encoder and why LSTM for decoder.\n3.\tMore definitions about the model should be offered, such as \u201cy^\u2217 is some optimal action distribution\u201d, how to get the optimal action distribution?\n4.\tSome datasets are not sufficient enough for sake of statistical sufficiency, such as recommendation data with only 1000 action space.\n5.\tThe contributions of action regularizations are not validated on experiment section.\n"}