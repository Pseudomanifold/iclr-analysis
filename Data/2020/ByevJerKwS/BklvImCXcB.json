{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, public data is used to bootstrap a model inversion attack. Specifically, public data is used to help train a generative model, which is used to generate images that reconstruct the average representation of a class that was learned by a victim target model. \n\nMembership inference is not a violation of individual training point privacy, it reconstructs an average representation of the class. This can violate privacy in some limited cases where attributes of the dataset were not known previously (e.g., the fact that points from a particular class possess in general a certain feature), but it does not violate the privacy of individuals whose data is used to train the model. This explains why differential privacy does not defend against model inversion (as observed in Section 4.3.6): model inversion is not an attack against data privacy as defined by differential privacy.\n\nThe threat model could be described more clearly in Section 2.1: what does \u201chaving access\u201d mean? Can the adversary observe labels and predictions? From Equation 3, it looks like the ability to query the full model\u2019s output is required to compute C(G(z)). Furthermore, is it realistic to assume that the adversary runs an optimization process with thousands of iterations for each attack run? (See page 6) If this strategy improves upon previous instantiations of the model inversion attack, this would be helpful to measure and report in experimental results. It is a proxy for the adversary\u2019s cost. \n\nThe experimental protocol considers a best-case scenario when the adversary has access to data from the same exact distribution (because public data is obtained by partitioning a dataset into a training and public subsets). This does not correspond to the scenario described earlier, where the adversary accesses public data by searching for images on the internet for instance. The approach will likely perform worse when the distributions used to train the target model and the generator mismatch, as indicated by preliminary results in Section 4.3.2. Expanding these experiments to characterize what knowledge of the distribution is needed for the attack to succeed would help address this limitation. \n\nEditorial comments:\n\nP2 one dominate \n\nP2: unnecessarily emphatic language at the end of Section 2.1\n\nP3: Why choose the Wasserstein-GAN?\n\nP7: Please don\u2019t make assumptions about the gender of adversaries."}