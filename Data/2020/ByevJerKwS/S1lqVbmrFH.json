{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Overall:\n\nInteresting take on the MI problem, which in fact reduces it to a two stage GAN problem where for the second stage the discriminator loss in fact integrates a component due to the model it tries to attack. The analysis is essentially experimental but seems to display that the technique does fulfil its promises.\n\n\nQuestions:\n\n* Is the attack sensitive to the bias of the data used to train the classifier ? For example, if the classifier has been trained using western faces and the attack consists of an eastern face with one eye removed, is this eye going to be biased towards the \u201cwestern\u201d model of the training data ?\n\n* I have had a quick look at the proof of Theorem 1. Isn\u2019t the Theorem an equivalence instead of just an implication ? Also I believe its writing is a bit approximative: what seems to hold (for just one implication) is in fact \\forall label y, IF U >= U THEN SKL >= SKL. Otherwise there is lacking a \\forall label at the end of the statement for the SKL part I suspect.\n\n* The paper assumes that the logistic loss is being used to train the NN model and then does the MI attack using a loss (Lid) that is of the same family. This implies therefore that the loss used is public knowledge, which seems to be a too stringent assumption. This begs the question: did the author try a different loss to train the model and yet used the Lid loss to do the MI attack ? What did happen ?\n\n* Is it possible to put the \u201clatent vector\u201d you search for (Sec 2.2) in Figure 2, so we know exactly where this vector stands in the problem ?\n\n* Assuming my description of the problem above is right (Overall), I suspect that some formal guarantees should / could be obtained on reconstruction provided the problem of MI attack is in fact folded in the training of the GAN. If it is too costly, then perhaps let us assume that we have not one but a set of images with missing parts for MI attack (call it the \u201chole\u201d data). Assuming we can constrain the support of the generated distribution to match exactly or approximately that of the \u201chole\u201d data, we can probably get GAN type guarantees on the reconstruction. Since after all, the GAN used is primarily used to stage the MI attack, did the authors try to fold both problems (GAN training, MI attack) into one ?\n\n* In the experiments, did the authors try the MI attack with random masks instead of just the \u201csquare + T\u201d ?\n\n* I believe there is a misunderstanding on the DP experiments. DP just guarantees that the model learned would be approximately the same if we changed one training data example. This does not guarantee the non reconstruction (approximately, depending on epsilon) of an arbitrary image, *even* if it were part of the training data. You are therefore not attacking DP models-as-in-DP-aimed-at-protecting-against-your-attack, but just showcasing that your technique also manages some reconstruction on those models. The Abstract sentence is highly misleading and the whole paragraph deserves a good polish from this standpoint.\n\nTypos (?)\n\nIn (3), Lprior depends on a variable which does not appear on the RHS. Similarly for the Lid loss the argument is missing in (3).\n\nAlso, the font of Lid is different from the one in the text two lines above\n"}