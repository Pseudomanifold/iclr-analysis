{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a GAN-based model inversion attack method. Moreover, the authors present a theorem that a well-trained model would be more vulnerable to model inversion attack. This paper finds a practical and meaningful problem, model inversion attack, of deep learning. However, the novelty of the paper is limited, and the comparison with baselines in the experiments section is unfair. \n\nStrong points of the paper:\n1) The well-organized structure.\n2) Complete experiments. \n\nWeak points of the paper:\n1) The comparison of baselines is unfair. The EMI[1] does not require the auxiliary knowledge, i.e., the blurred input image as an additional input to the adversary. The EMI only requires the label and confidence score information to generate the output image. As shown in figure 3, it makes sense that EMI's output is more blurred than the proposed GMI. Furthermore, the distance in table 1 shows that EMI is not much worse than the proposed GMI since it does not have the blurred input images. Similar to the EMI, the baseline PII does not have the additional confidence score of the target model. It is quite unfair to compare these the method since they do not receive the same input information. Lastly, please cite and indicate what image inpainting method is selected as the baseline. \n2) The proposed theorem 1 is another significant contribution of this paper. However, the proof of theorem 1 (noted as theorem 2 in appendix) is not convincing. If the insensitive component of the image is very few, and it could be ignored, then the proof is just the objective of the classification model. The proof would be very similar to the maximum a posterior estimation. That gives minimal theoretical insights. If the authors could prove theoretically or empirically that the insensitive component exists and is as much as the blurred background. The proof would be more meaningful. \n3) The authors seem to be very rush in submitting the work. The dropbox link is not anonymous; the abstract is not well-formatted in openreview, and some notation is not correct. \n\nOverall, the paper is far from being published. \n\n[1]M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322\u20131333. ACM, 2015.\n"}