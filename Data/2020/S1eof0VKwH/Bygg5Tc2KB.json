{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method for feature vector synthesis for few-shot learning problems focusing on few-shot classification. The proposed method uses WGAN implemented using MLP architecture for both generator and discriminator (as synthesizing in feature space and not in image space). The method is meta-trained to produce good samples given the current FSL task support set. During meta-testing, the support set\nThe claimed novelties of the paper are:\n* GAN based example synthesis for few-shot classification as a stand-alone method and not a plugin on top of existing FSL approaches as in Zhang et al. 2018.\n* classification regularization for the generator - the synthesized feature vectors are expected to be classified correctly\n* mode collapse regularization - cosine similarity between synthesized feature vectors for the same class are penalized w.r.t. the cosine similarity between their generating random vectors\nThe experiments demonstrate the proposed approach utility on the miniImageNet, CUB, and CIFAR100 (CIFAR-FS?) benchmarks.\n\nI propose to accept this paper if it undergoes a major revision. Specifically, I propose the authors to address the following points:\n\n1. The comparison to previous approaches is lacking, important recent works are missing from the comparison tables, while having better results on all of the 3 tested datasets. One example would be MetaOptNet method (Lee et al. CVPR 2019), but there are others. Now it might be ok to focus on just the comparison to papers that do example synthesys for FSL, yet it is not so at this point, and is a little disappointing in general.\n\n2. The classification regularization is a known trick in GANs, as conditioning on classes is useful, take ACGAN as an example  and there are others. So not clear if novelty could be claimed here.\n\n3. Technical note - in equation (7) it looks like a ratio between two logarithms is used where the arguments are in the range of [-1,1] (cosine similarity?). Then (a) one needs to make sure negative values are clamped; (b) it is a ratio of two negative numbers, so it does not optimize what is claimed in the text, as minimizing (7) would try to maximize similarity between the synthesized vectors, and the intent is the opposite; so in short, is a minus sign missing in front? or the intent was to maximize (7)?\n\nThe main negative point in my opinion is #1, I suggest the authors to focus on that in their revision. If it is not addressed, I would be inclined to suggest to reject at this point."}