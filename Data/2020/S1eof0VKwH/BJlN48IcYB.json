{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a meta-learning method that learns a generative model that can augment the support set of a few-shot learner. The model is inspired from GANs. It optimizes a combination of losses, that includes the WGAN loss, a loss that measures the ability of a prototypical network to classify a query set and a loss that promotes diversity of samples. Positive results are demonstrated on various few-shot learning benchmark.\n\nI'm afraid the submission has some important flaws, that keep me from recommending acceptance.\n\nFirst, the proposed method is not particularly novel. It is a slight variant on \"Low-shot learning from imaginary data\" (Wang et al. 2018), where the main difference seems to be the addition of a WGAN loss and the proposed diversity-promoting loss. There are also some differences in the architecture of the generative model, which here conditions on a prototype of a class (instead of an example) and is trained to samples that can't be distinguished from the prototypes. Also, here the prototypical network that classifies the query set is fed the samples from the generative model, and not the original support set examples (which I find to be an odd design choice), unlike in Wang et al. (2018). Overall, I find that the paper doesn't demonstrate that the proposed method is superior from (Wang et al. 2018). Crucially, it doesn't actually compare performance with that work. A first step in this direction would be to add to Table 1 an experiment where \"CR\" only is used, which would correspond to something close (but not identical) to Wang et al. (2018). But what would be preferable is to compare with Wang et al. (2018) as it is described in that paper. And generally, I don't find that the submission makes a good case for why the WGAN and AR losses are necessary. Indeed, the objective of Equation 4 is trivially solved by having G ignore the noise vector z_i and copy s at its output (which would bring no actual data augmentation). This solution also doesn't correspond to a mode collapse situation either, even though this is the motivation behind the diversity-promoting loss of Equation 7. So, even at a conceptual level, the motivation for the proposed method is not strong.\n\nSecond, the results on miniImageNet are actually not better than the current SOTA. Indeed, for example, paper \"Meta-Learning with Differentiable Convex Optimization\" by Lee et al. (CVPR 2019) reports 64.09% and 80.00% for 1 and 5 shot respectively (and 62.64%, 78.63% when not retraining on train+valid, which is probably a fairer comparison). Personally, I could still be fine with this if a proper comparison with Wang et al. (2018) was made and showed better performance. However, ideally the proposed method would be shown to also improve over the state of the art. I also note that the experiments used image sizes of 224x224, while other papers have used the 84x84 size. This may actually give an unfair advantage to the propose method, which has nothing to do with its core contribution. \n\nThird, the paper is not particularly well written. I've found many phrases in the text that are not grammatically correct. I'd also be willing to be lenient on this point, given that it's still possible to understand the technical contribution, but I'd strongly recommend that they authors better proofread the writing.\n\nFor me to consider increasing my rating, I would like:\n- A proper, convincing and fair comparison with Wang et al. (2018) be presented.\n- A better justification/motivation be given for the WGAN and anti-collapse regularization losses.\n\nFinally, here are some minor issues I've found which the authors should consider:\n- \"susceptible for mode collapse\" => susceptible to mode collapse\n-  \"penalty on the case\" => penalty for the case\n- \"suspect to visual similarity\" => rephase\n- \"Regarding to the\" => Regarding the\n- \"the details how\" => the details of how\n- \"for in a typical\" => in a typical\n- in Equation 5, in the denominator, F(q) should be replace by q\n- in Equation 7, the loss is not defined when the cosine similarity is negative (which can happen, unless both vectors have strictly positive values)\n- why train a softmax classifier at evaluation time, instead of using a prototypical network predictor as in Equation 5?\n- similarly, why use an SVM for the baseline?\n- in Table 1, why is the check mark for cGAN, in the second column, different from other check marks?\n- the CUB dataset wasn't used in Ravi & Larochelle (2017), so it's not the right reference for the splits of that dataset\n- similarly, where do the CUB results come from for Meta-LSTM, given that it wasn't used in Ravi & Larochelle (2017)?"}