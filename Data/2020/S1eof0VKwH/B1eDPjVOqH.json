{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThis work is about data augmentation in few-shot learning scenarios. Specific emphasis is put on the intra-class diversity of the added samples. The main idea is to use a conditional GAN for synthesizing features with high within-class correlation and low between-class correlation with the real samples.\n\nThe main technical contribution is a specific \"anti-collapse\" regularizer for the GAN that forces the generator to carry some information about the latent code. The idea is to use the ratio of the (log-)similarity of two synthesized features and the associated log-similarity of the generating noise vectors as a penalty. The rest of the model proposed is pretty similar to other neural architectures of this kind.\n\nThe experimental results on Mini-Imagenet suggest that the model proposed reaches state-of-the-art classification performance  both in a one-shot and a five-shot setting. However, there seems to be no really significant differences to the LEO model. Similar conclusions might be drawn from the experiments on CUB and CIFAR100. In all these experiments, the  anti-collapse regularizer indeed seems to have some role, but its true significance is a bit unclear to me.\n\nIn summary, I conclude that this paper addresses a highly interesting and relevant problem, but the novelty (in the form of the novel regularizer) is somewhat limited and has a kind of \"ad hoc\" flavor. The experimental results are OK, but I have to admit that I am not fully convinced that the novel regularizer makes a huge difference.  "}