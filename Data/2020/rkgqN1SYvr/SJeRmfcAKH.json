{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the role of initialization for training deep linear neural networks. The authors specifically consider the orthogonal initialization, and prove that with the orthogonal initialization proposed in equation (4), the gradient descent can achieve zero training error in a linear convergence rate. The improvement of the orthogonal initialization lies at the dependence of the layer width $m$, which is independent of the network depth $L$.\n\nThe problem considered in this work is very interesting since there are lots of empirical studies show that good initialization can benefit the training of deep neural networks. However, my main concern about this work is its novelty, especially for the proof techniques used in the current paper. It seems that most of the proofs are similar to the previous work Du & Hu (2019), and the main reason that it can remove the dependence of $L$ seems to be Lemma 4.2, which can be derived using the orthogonal property of the initialization. In this sense, there is not too much contribution for the current paper given the previous work Du & Hu (2019). Are there any other significant changes need to be made in the proofs to get the main results? If the authors can provide the convergence guarantees of the stochastic gradient descent, the contributions would be strong. \n\nFor the proof of Theorem 5.1, is it a straightforward extension of the proofs in Shamir (2018)? What is the main challenge when prove the general $d$ case?\n\n\nMinor comments:\nFor the last equation in (4), why you need $W_L(0)W_L(0)^\\top=mI_{d_y}$ instead of $W_L(0)^\\top W_L(0)=d_yI$ as you used in the later proofs?\n\nThere is no experiment to verify the theory"}