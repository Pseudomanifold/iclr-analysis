{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers regularization based on \"counterfactual\" trajectories.\nNamely, it suggests two losses, action-control and disentanglement regularization.\nIt experimentally evaluates the benefits of such regularization in the StarIntruders environment.\n\nThe paper is well written and explained.\n\nIssues:\n\n1) Authors evaluated the two suggested regularizations in separate. \nI would like to also see numbers from a combination of these.\n\n2) I think the related work is missing a large line of work on \"auxiliary tasks\".\nIt seems to me that this paper would exactly fit within that scope?\n\n3) My main issue is the evaluation.\nThe evaluation is done on a in-house game and compares to very few methods.\nFor a paper that has very little theory and thus most of the value is in the empirical evaluation, I think that is a problem.\nIf authors opted for example for Space Invaders (they do say it is similar) or simply more games, one would have many more existing numbers to compare against.\n\nMinor issues:\n\n1) The first regularization - action control regularization is motivated by the idea that there is always an action that changes the state. While true for most environments, this does not hold in general.\n\nSummary:\n\nOverall, this paper has potential but I don not believe is good enough - I suggest a reject.\nThe main problem is that the idea is relatively simple, there is no theory and thus the crucial piece of the paper has to be the empirical evaluation.\nAnd the evaluation only compares to a single method with no regularization, no auxiliary tasks and reports only experiments on a single game."}