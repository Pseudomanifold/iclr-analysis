{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThis paper presents a new approach for MORL (Multi-Objective Reinforcement Learning), which handles multi-objective as a pre-defined logical language, and estimates Q-function by using UVFA. This approach can handle the case when the final objective is not a linear combination of the individual objectives. It feeds the specification into a GRU (Gated Recurrent Units) as an encoder, concatenate the encoding with the state, and then send it to the Q-function, estimated by a UVFA. I am kind of on the borderline, but still lean to reject this paper. I am happy to change my score based on the reviews from other reviewers.\nStrengths\n- The idea of modeling multi-objective as a logical language is novel. By describing the final objective in a logical language, many more cases that are not linear can be covered.\nWeaknesses\n- Lack of baselines / experiments. It seems the only baseline in this paper is agents trained on a single policy, i.e., no baseline from previous works. It is possible to compare the performance with previous works when the final objective is linear to each individual objective.\n- Only tested on one simple scenario. More scenarios can be included to justify the effectiveness of the proposed approach (e.g., Deep Sea Treasure, SuperMario, etc.).\nPossible Improvements\nAs mentioned before, more baselines and scenarios can be included."}