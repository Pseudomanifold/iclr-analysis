{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "An approach is proposed to learn sparse representations while preserving some structures in the data.\n\nThe idea seems quite nice where we want to learn the structure that induces sparsity instead of simply sparse representations. The idea is to extend a recent algorithm RwISTA by adding reweighting block. The reweighing block changes weights to encode whether coefficients in the model learned by the network are similar or dissimilar to each other. To build the reweighing block convolutional layers are used.\n\nThe paper is slightly hard to read due to many typos, and hard-to-read sentences. Also, I think a more intuitive explanation of how the reweighting helps preserve structure is needed. Right now it is very difficult to understand (except maybe for experts working on similar problems?) Regarding the experiments, most of them are run with synthetic cases. It seems like the approach is compared with several recent approaches though showing good results. On the MNIST data, results are shown where the images are recovered from the sparse representation. I did not really see any substantial improvements in performance as compared to say LISTA. Maybe I am misunderstanding what is being evaluated in Figure 7. Also I am not sure how it shows that the sparse representation is learning the underlying structure? Maybe some re-writing is needed to make this clearer.\n\nI think the paper is interesting but needs some polishing to make it easier to read and perhaps some improved experiments in real datasets.\n"}