{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\n\nThe authors present a framework for visualization and interpretation of medical images classified with a DenseNet: the framework is tested on MURA, for abnormality detection in musculoskeletal radiograph images, allowing to do feature extraction and visualize CAMs. The authors claim that the framework helps clinicians to build trustability in the model and identify hidden features extraction failures.\n\nContent\n\nAlthough the work is interesting, it doesn't bring any novelties, the methods used (feature extraction and CAM) are well known and the paper does not offer any quantitative measurements to back-up its claims.\nThe paper is well written, in my opinion, it is a good paper for a conference/journal at the interface of healthcare and Deep Learning in a more applied field. However, most of the content is too detailed for the conference it aims for: Whole paragraph on how radiography works, which is not important here to explain the results,  too many details on the structure of a DenseNet which is a well-known model in the community, even most of the proposed framework could have been written more concisely. \nThe results are exclusively qualitative, moreover, the resulting images are shown in the appendix which elongates the paper. With more concise writing and a better formating for the figures (not showing all the features for example), the authors could have fit the results with the explanations. As for quantitative experiments, I would encourage the authors to have a look at Evaluating Feature Importance Estimates (Hooker et al) and other experiments in Sanity Checks for Saliency Maps (Adebayo et al). The last paper even gives good explanations on the \"edge detector\" resemblance of CNN, which could help the authors to explain some of their results (the frame in fig 10 for example).\nIt is unclear why CAM was used here and not GradCam or Grad-CAM++ (Chattopadhyay et al) or even more recently Smooth Grad-CAM++ (Omeiza et al.). Even saliency map approaches such as Guided backprop would have been interesting to see in this work. \nThe paper brings results on a DenseNet trained on ImageNet, but it would also have been interesting to see the resulting visualizations with a pre-trained model on an X-ray dataset (for example ChestXray from NIH).\nThe part of how this framework could help clinicians make their analysis faster could be detailed: The framework generates features for each blocks, which seems a lot to analyze in the end, and even if those features offer a good interpretation of where the network is looking and why it is wrong, the authors do not offer a solution as to change this in order to improve the accuracy.\n\nTypo\n\n- in Discussion, first paragraph, last line, \"it will then zoomed\" -> \"it will then be zoomed\"?\n\nConclusion\n\nIn the end, the paper is interesting and I believe the framework would be relevant in a hospital, with maybe more features such as different models, methods of interpretability, etc. However, it doesn't offer any novelty in the Deep Learning field and does not fit for a conference such as ICLR."}