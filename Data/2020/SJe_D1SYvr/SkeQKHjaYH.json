{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an imitation learning algorithm that learns from state-only demonstrations and assumes partial knowledge of the transition dynamics. The demonstrated states are decomposed into \"responsive\" and \"unresponsive\" features. The imitation policy is trained in an environment where the responsive state features are simulated and controllable by the agent, and the unresponsive state features are replayed from the demonstrations. The agent is rewarded for tracking the responsive state features in the demonstrations.\n\nOverall, I was confused by this paper. Isn't it problematic that the imitation agent is trained in an environment where unresponsive state features from the demonstrations are replayed and not affected by the agent's actions? It would be nice to expand Section 4 to explain why it makes sense to reflect the unresponsive component in the transition kernel. It would also be helpful to include some of this information in Section 1.\n\nI am also confused by the method. What is the purpose of observing unresponsive features, if the reward function for the imitation agent is only defined in terms of the responsive features? Is it that the imitation policy is conditioned on the unresponsive features?\n\nThere are several important experimental details missing from Sections 4 and 6. What distance metric was used to define the reward function? For example, was it Euclidean distance in pixel space for the Atari games? The main experimental results in Figure 2 are difficult to interpret without knowing the how the reward function is defined in the eMDP. Could the authors either provide definitions of the reward functions of the eMDPs in the experiments, or measure performance of the imitation agents using more interpretable or standardized metrics (e.g., game score) for the chosen environments?\n\nIt is also concerning that the Google Drive links to code and supplementary materials aren't anonymized."}