{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "SUMMARY\n\nThe authors propose an intuitive new measure (definition 2.1) of feature importance based on a robustness criterion (with two variants of equations 3 and 4). Two optimisers are proposed for finding the most important features according to this measure, in order to explain why a classifier is making a certain prediction.\n\nThe experiments are both qualitative (showing pixel-wise importance maps for image problems) and quantitative (showing how well the various feature importance scoring algorithms do albeit on the measure explicitly optimised by the new proposed algorithms).\n\nCOMMENTS\n\nThe paper organisation is clear enough though the English needs a little work to make it read really nicely.\n\nThe proposed measures are natural and intuitive. Especially the Robustness-$\\hat{S_r}$ is an interesting twist on the more obvious Robustness-$S_r$.\n\nWhile the measures are interesting, the justification for them is somewhat weak. This amounts to \n\n1) Quantitative experiments which seem to only test how well each method works in relation to the very metric which only your proposed method directly optimises - this is nice but not surprising. Also, the way you define the AUC of your measure seems a little strange. I don't know why different baselines appear in different comparisons as in e.g. tables 1 and 2. Finally your curves in appendix A don't seem to have the same number of points for each method in all cases? \n\n2) Qualitative examples with images and text. These are nice, but alas only qualitative. Also, it seems as though not all baselines are included in all examples (even figures 3 and 5, which are analogous, include different baselines). \n\nIt seems like the paper needs either 1) a quantitative evaluation that is not subjective. Surely, the evaluation metric should not match the (novel) objective of the proposed method? Or, 2) a theoretical result in support of the new measures.\n\nThe Reg-Greedy algorithm is a major contribution of this paper, but receives very little explanation. Indeed, perhaps the clearest quantitative statement of the paper is that Reg-Greedy beats Greedy. Is this a common method for optimising w.r.t. a subset? Is it similar to other methods? I felt that Reg-Greedy is a really nice idea but the paper did not do it justice.\n\nDETAILS\n\nPerhaps unifying (3) and (4) by defining a single g that subsumes both cases would be neater.\n\nPlease define g in equation (1) rather than in words after equation (4).\n\nIt should be S_r in the subscripts of (3) and (4)\n\n\"Crutial\" spelling\n\nFINALLY\n\nI'm open to be swayed on any of the above points, pending the author feedback.\n"}