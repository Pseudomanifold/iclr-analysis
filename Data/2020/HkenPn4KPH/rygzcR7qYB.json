{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary & Pros\n- This paper proposes a few-shot learning method that uses self-supervision as an auxiliary label and trains primary and auxiliary labels via multi-task learning.\n- This paper provides extensive experiments for analyzing the effect of self-supervision on various few-shot learning settings: (1) self-supervision can improve various few-shot learning algorithms, ProtoNet & MAML; (2) self-supervision with similar samples can provide more improvements.\n\nConcerns #1: Novelty of the proposed method\n- This paper uses a multi-task learning approach with self-supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi-supervised learning, training GANs. Thus, the proposed method (in Section 3) using a multi-task learning objective with self-supervised losses seems to be incremental.\n\nConcerns #2: Somewhat unsurprising experimental results\n- This paper shows various experimental results, but some experiments seem to be trivial. For example, the performance gap is typically increased when learning harder tasks, e.g., when the number of training samples is decreasing, the performance gap between methods is typically increasing in a fully-supervised setting. Thus I think results in the paragraph \"Gains are larger for harder tasks\" might be predictable. Other examples are Figure 4a and 4b in Section 4.2 because one can easily expect that using training more in-domain samples can provide more performance gain.\n- In the case of Figure 4d in Section 4.2, the authors claimed that the effectiveness of SSL decreases as the distance from the supervised domain increases. However, I think Figure 4d is not matched to the claim. For example, in the case of Dogs, better performance is achieved when using a more dissimilar domain for self-supervision except for D_s=D_ss. So I wonder how to draw the lines in Figure 4d.\n\nSome experimental results provide meaningful messages, e.g., single self-supervision can improve performance significantly while joint self-supervision does marginally. However, the contribution of the methodology is limited and some experimental results seem to incremental.\n"}