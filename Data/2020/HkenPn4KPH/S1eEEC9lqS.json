{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper considers the addition of self-supervised learning techniques in the few-shot learning setting. Extensive experiments are done to show that it can be helpful, including in cases where the labeled data is corrupted. The paper also considers the domain mismatch issue where unlabeled images come from a different domain.\n\nReview: This paper is thorough and clearly written. Applying self-supervised learning techniques to the few-shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. It includes extensive experiments on a wide variety of image datasets, including many additional studies in the appendix. The only possible criticisms of this paper are that it is limited to the image domain (as are most few-shot learning/self-supervised learning studies, so we can probably ignore this) and that it does not produce a huge delta in understanding compared to Gidaris et al. (2019). Gidaris et al. (2019) was posted to arxiv in June, I'm not sure if this work was also on arxiv around the same time, and even if it wasn't I'm not sure what to consider \"concurrent\". However, as the authors note they include additional experimental settings (like the domain-selection idea) that are not in Gidaris et al. (2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. For example the definition of the \"'distance' between a pair of domains\" is only introduced in passing in the midst of Section 4.2 covering domain shift experiments, and the method for training a domain classifier is similarly only mentioned in passing in 4.2.  Of course, it is not really valid to criticize a paper for being too exhaustive. Overall, I recommend acceptance.\n\nSpecific comments:\n- Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2.\n- Snell et al. (2017) needs a \\citep\n- You ought to cite \"S4L: Self-Supervised Semi-Supervised Learning\", which is related to your discussion of connections between self- and semi-supervised learning (though not few-shot).\n- The paragraph beginning \"The focus of most prior work...\" in the Related Work section provides a nice framing of your work and so might make more sense in the introduction.\n- \"we consider self-supervised losses based on labeled data ... that can be derived from inputs x alone\" All labels (can be derived from in the inputs x alone, given an oracle (or human labeler). I think you mean \"that can be derived automatically without any human labeling\".\n- You state \"Our final loss function combines the two losses\". Is there no scalar multiplier on either loss term to trade-off the importance of each?\n- The gains from the self-supervised auxiliary tasks are over and above any gains from data augmentation alone. More experimental details are in Appendix A.5.\" I don't see any information to substantiate the claim that the self-supervised tasks result in a bigger improvement than data augmentation alone, can you provide those details?\n- \"However, does more unlabeled data always help for a task in hand?\" This question actually was addressed somewhat in \"Realistic Evaluation of Semi-Supervised Learning Algorithms\", see sections 4.4 and 4.5 therein."}