{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes new (data dependent) generalization guarantees based on the Jacobian of the model. The authors suggest that if the desired outputs lie into the information space (the subspace spanned by the largest eigenvectors of the NTK), the model will train faster and better generalization will be achieved.\n\nThe faster convergence of the model in the information space is not surprising and was observed by Jacot 2018. The authors make improvements over this result:\n - They present a generalization result, whereas Jacot 2018 focuses only on the convergence on the training set. It is also formulated as a classification problem instead of a regression one.\n- It doesn\u2019t need JJ^t to stay constant during the training.\n\nHowever, the setting considered by the authors to derive their theoretical contributions is too restrictive. The model exposed in section 1 is extremely simplified, as only W can be learned and V is fixed. As a result, the model is in essence completely linear: the goal is, for a given V, to learn a \u201cgood\u201d hidden layer using a linear model and the loss L : h -> ||V phi(h) - y ||. \n\nThe experiment on cifar10 is interesting, especially the section regarding label corruption. A more extensive empirical investigation is this direction would be of great value. The results uncovered are not surprising and predicted by Jacot 2018 (granted, it is interesting to see that the result holds for finite width and non-continuous gradient flow).\n\nI think this paper in its current state is not good enough for two reasons. First, the major contribution is a generalization bound that is derived for a model that is too simple (even simpler than a standard one hidden layer network). Beside this result, the rest of the paper is  incremental, as the link between convergence rate and the projection of the desired outputs on the information space was already made in Jacot 2018\n\nNB: I did not check the derivation of the results in annexes.\n\nNitpick:\n\nPage 2: \u201cour results may shed light on the generalization capabilities of networks initialized with pre-trained models commonly used in meta/transfer learning\u201d seems like a bit of a stretch. While I agree that theories that requires random initialization won\u2019t work for transfer learning, the results presented by the authors don\u2019t really leverage anything particular about pre-training.\n"}