{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Note: The template used in this paper is of ICLR 2019, not ICLR 2020.\n\nThis paper identifies the information space and nuisance space by thresholding the singular values of the network's jacobian, and shows that generally the residuals projected to the information space can be effectively optimized to zero, thus leading to efficient optimization and good generalization.\n\nI believe this paper should be rejected because its motivation and technical framework is not novel enough in that 1) the motivation of decomposition along gradient matrix is already well-founded by a series of paper related to neural tangent kernel 2) the techniques used here also fall in a similar framework. The following is the detailed comments.\n\nFirst, this paper's motivation is to employ the singular decomposition of the jacobian. Actually, the motivation is essentially the same as (Arora et al. 2019) and many other works. The neural tangent kernel matrix defined there is exactly the inner product of two jacobian (or gradient) described here and to employ the singular decomposition is actually corresponding to employing the eigendecomposition of the neural tangent kernel, which appears first in (Arora et al. 2019). The logic behind dividing the singular space into information space and nuisance space is that gradient descending along different directions has different speed, determined by the eigenvalues.\n\nThe framework presented in this paper is based on the assumption that the parameters will not be far away from the starting point. Such an assumption further guarantees the trajectory won't be far away from the linearized trajectory, leading to optimization guarantee. This approach is widely used by many works, and well-known for a considerably long time. Also the paper's proof is complicated and lengthy which hinders its clarity.\n\nTo summarize, this paper definitely contains some rigorous analysis which I appreciate, but it doesn't provide new insights into optimization and generalization for deep nets. The motivation and logic behind is not novel enough, the main theorem neither. So I suggest a weak rejection to this paper in its current form.\n\n[1] Arora, Sanjeev, et al. \"Fine-grained analysis of optimization and generalization for over-parameterized two-layer neural networks.\" arXiv preprint arXiv:1901.08584 (2019)."}