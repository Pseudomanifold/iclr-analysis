{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work summarizes the existing methods of mutual information estimation in a variational inference framework and describes the limitations in terms of bias-variance tradeoffs. Further, the authors care about the self-consistency, namely, independence, data processing, and additivity, which are properties of both entropy and differential entropy. Further, density ratio clipping is proposed to lessen a high-variance problem in estimating a partition function.\n\nIn regard to the clipped density rations, it is not clear why both variance and bias become small when S is close to 1 and using small $\\tau$ in Theorem 3 and 4. In Fig. 2 on the benchmark experiment, $\\tau=5.0$ showed lower variance than $\\tau=1.0$.\n\nThe comparison with MINE and BA is also expected on the benchmark experiment.\n\nAs for the self-consistency tests on images, especially, for the \u2018data-processing\u2019 property, it would better to apply affine transformation such as rotation, translation, and scaling, rather than rows deletion.\n\nIn Section 3.1, $\\Gamma$ is not defined.\n"}