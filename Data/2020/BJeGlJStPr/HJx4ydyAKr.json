{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new distributed algorithm for reinforcement learning. The paper lists three main contributions: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. \n\nI'm not that familiar with RL, however I'm very familiar with distributed training in other contexts. Therefore, the significance of the contributions in the RL domain is a bit unclear to me. However, the contributions in the area of distributed training is relatively fair. The introduction of a circular buffer is not very novel. Further, the trade-offs / adaption of update frequency etc. are standard ways to improve performance in distributed training. \n\nThe evaluation of the proposed algorithm is reasonably well done (considering the page limits), with a suitable set of benchmarks (although relatively few). The results are promising and could have a significance for practitioners."}