{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a VAE architecture that separates a fixed content representation and time varying features of an image, that can be used to learn a representation of image transformations in a temporal setting.\n\nThe paper is well written and the ideas presented are interesting, but in my opinion not novel enough or thoroughly demonstrated to justify acceptance:\n\n- there is a very relevant work that is not mentioned by the authors and that can be seen as a generalization of the model presented in this paper: \"Disentangled Sequential Autoencoder\" by Li and Mandt (ICML 2018), which introduces a model that is also disentangling a content and a temporal representation of sequential data. This is basically the more general model introduced by the authors of this submission in the beginning of section 2, without all the assumptions made in the rest of section 2. A comparison with this related work would help assess the differences in terms of modelling power and in performances.\n\n- The assumptions made in this work are fairly strong for most interesting applications, in particular the fact that the content cannot change across time steps.\n\n- To me, the issue with the novelty of this model would not be a big problem if the authors focused more on showing its usefulness in different applications (e.g. medical domain or RL as mentioned in the conclusions). However, the authors only demonstrate the TEVAE on relatively simple experiments that are only tailored to simple image transformations.\n"}