{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a compression technique for neural nets based on matrix factorization. Inspired by an SVD low rank approximation, the idea is to replace each n x m parameter matrix by a multiplication of n x p and p x m matrices, with p sufficiently small. This is applied as a systematic architecture change, rather than only after training is complete as in related work. Experiments with LSTMs and Transformers for neural MT over several language pairs show that the technique works better than pruning weights with small values.\n\nStrengths: very clearly written, very straightforward technique, impressive experimental results.\n\nWeaknesses: missing experiments on larger datasets and other problems, missing comparisons to more competitive compression techniques.\n\nI don\u2019t think the paper builds a convincing enough empirical case for the proposed technique. The main problem is that the good performance of the compressed models could be solely due to regularization, since the corpora used are small by NMT standards. Further experiments with regularized models (eg by optimizing dropout) and larger corpora would be needed to exclude this possibility. Also, since this is an ML conference, and the technique is extremely simple and general, results in other settings besides NMT should be provided.\n\nAnother problem is the comparison to compression baselines. The results in table 5 show that post-training factorization looks competitive at 22% reduction; why not show similar results for the broader range of compression levels in tables 1 and 2? Distillation is another popular technique that should be compared to. Finally, I am not an expert on pruning, but it looks like there has been more recent work than See et al (easy to find by chaining forward on Scholar), some of which claims even more impressive compression rates than reported here.\n\nFurther comments:\n\nThe fairly large advantage of LSTM over Transformer in tables 1 and 2 (26.7 > 26.0) indicates that Transformer might not be trained/configured properly\n\nIn table 1 and 2, should either change \u201cSize reduction\u201d heading to \u201cRelative size\u201d or make entries positive."}