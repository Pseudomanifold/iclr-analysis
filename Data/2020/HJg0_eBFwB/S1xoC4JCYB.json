{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the model compression for deep neural networks of machine translation. Unlike previous works compress the model after all the models are well-trained, the paper proposes to compress the model using matrix factorization during training. The paper then shows some experimental results. \n\nI agree with the paper that in-training compressing can help prevent performance degrading. However, doing in-training matrix factorization is too time/computational consuming and may not be practical unless some time efficiency results are provided. The paper only compares with pruning and after-training matrix factorization in the experiments. As there are many model compression methods as listed in Sec.5.2, it is better to compare more advanced methods than the pruning method. Finally, I cannot see why the proposed method can only work on translation tasks. I am wondering has any other tasks such as computer vision been tested? What are their results? And why does the proposed method work/not work?\n\nTo summarize, the paper is weak in both technology and experiments. I vote for a reject of the paper.\n"}