{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers training neural networks with a low rank constraint on the layers for parameter reduction and as a regularization. The paper considers machine translation task, and experiments with LSTMs and Transformers. The paper uses IWLST 2014 dataset for experiments with English to German, Portugese and Turkish translation pairs.\n\nThe paper enforces this factorization of layers during training, rather than post training compression techniques. This is appealing as it results in smaller models during training.\n\nThe paper while considers an important question about training neural networks with a low rank constraint, I find it sadly lacking in details and is hard to understand the exact experimental setting and the reason for the shown gains. \n\nWhat are the number of steps each model is trained for? What is the stopping criterion used? Are the same settings (batch size, learning rate, stopping condition, optimizers, regularization) used for training all the models compared in Table 2, for example? What is being factorized in the attention layer? Are you factorizing all the query, key, value and projection matrices? What do you do for multihead attention? Multihead already adds a low rank constraint. Do you add another rank constraint on top of this?\n\nIt looks like different batch sizes are used for training factorized vs standard models, as shown in Fig 2, is it the case even with the results in Table 2? Even in Fig 2, it is not clear that factorized models offer an advantage. What happens if the training is continued for more iterations in that figure? Are the gains mainly because of using  a larger batch size or because of the regularization effects of low rank constraint?\n\nAlso if we compare the results in Table 1 and Table 2, we see that Transformers have smaller BLEU score than LSTMs. It seems like the models are not sufficiently trained, and may be if they are trained for long enough (to see competitive BLEU scores), it is not clear if the regularization will still help in that setting.\n\nMost of the existing works observe that try to apply a low rank constraint during training observe negative effects in performance, resulting in reliance on  post training model reduction techniques. What is the difference in the approach followed by this paper that actually helps in performance with this additional regularization, compared to existing works? As far as I can tell from the paper, the approach is to just replace the standard layers  with factorization, which is known to hurt the performance (Kuchaiev and Ginsburg 2017).\n\nFinally the paper is only experimental, having experiments only on IWLST 2014 seems quite limited and hard to know if the conclusions will generalize to other datasets/tasks. The paper needs more experiments to sufficiently conclude the advantages of the low rank regularization.\n"}