{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper (Recurrent Layer Attention Network) describes a method for aggregating information over layers, using a combination of RNNs \"controlling\" the scaling of feature volumes from CNN activations. The experiments and ablation studies in this paper are detailed, and show the workings of the proposed mechanism. Given comments on the ICLR submission page, codifying the relationship between DIANet and this work inside the paper itself would be beneficial. While the baselines chosen are sensible, it would be of great interest to see this method applied to a nearer-to-SOTA architecture (likely a ResNet variant) as well, given the plethora of open source implementations to choose from for the benchmarks, and the direct applicability of the proposed method. The same holds for the instance segmentation experiments, although I do understand the cost of running experiments in that domain - the proposed method should be easily applied to *any* segmentation architecture utilizing a ResNet trunk. \"Recurrent Layer Attention network achieves significant performance enhancement\" seems a bit of an over-reach given the existing experiments, the more modest claims in the introduction \"RLA network achieves similar or superior results\" make more sense. \n\nThe primary issue with this paper is flawed grammar and generally difficult-to-read \"flow\" of the writing itself. Rewriting with a careful eye to these problems has potential to raise my score - as it stands I cannot recommend the paper for acceptance, even though the core work, experiments, and architecture all seem promising to me. More work on the writing portion of this paper would greatly improve things, along with experiments pushing the edge of performance based on recent open source codebases."}