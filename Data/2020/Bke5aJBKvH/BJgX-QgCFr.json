{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a modification of a CNN architecture where an LSTM processes the activations of each layer to provide a per-layer attention mask, that is subsequently applied to the activations before passing them to the next CNN layer.\n\nWhile the idea might have some merit, the paper in its current state is clearly not ready for acceptance at this conference. The writing must be improved. The paper fails to convey a clear description of the idea and of the experimental results, to the point it is difficult to evaluate the proposed method. Please have your submission proof-read for English style and grammar issues.\n \nDetailed feedback:\n\n1) At the end of Sec 2 the authors state that there are only a few works to exploit RNN to directly enhance the performance on vision tasks that don\u2019t require sequential decisions. This is not accurate, in fact there are many examples in the literature, such as:\n \n     a) CNN-RNN: A Unified Framework for Multi-label Image Classification by Wang, et Al.\n     b) ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks by Visin, et Al.\n     c) Regional Latent Semantic Dependencies by Zhang, et Al.\n \n2) The content of the \u201cContext: simplified representation of feature volume\u201d section is obscure to me. What are max and average pooling applied to? Each layer? The last? Does the length of this \u201ccontext\u201d representation depend on the architecture or is it fixed? How is this context used?\n \n3) Model complexity: the manuscript reports that the model complexity is independent of the network depth. This seems inaccurate though: to my best understanding some of the implementations of the context function would indeed require additional per-layer capacity. Also, please also include considerations on the increase in time complexity.\n\n4) The authors find that their IA+RLA variant works worse than the base RLA model alone, but leave further investigation as future work. I find this (and the authors\u2019 intuition on why this happens) unsatisfactory. Unless the IA+RLA variant is properly explored in the experimental section, it should be removed entirely from the paper. There is no point in introducing a variant that in my opinion had no particular merit in the first place, works worse than the base model and is not explored in depth in the experiments.\n\n5) Introduction: the sentence \u201c[deep neural networks] successes have been limited to domains where large amounts of labeled data are available\u201d is incorrect. Indeed, neural networks have been used successfully in many domains where labelled data is scarce, such as the medical images domain for example. Please remove the sentence.\n\n\nMinor:\n- The caption of Figure 1 is not sufficient to understand the figure, nor the model. Please improve it.\n- The experiment section should start with the main experiments and end with the ablation study, rather than the opposite. Also, the datasets should be introduced before the experiments on such datasets, not after.\n-  Please use bold for all the experiments in a reasonable range (e.g., 0.5% or 1%) from the best in the tables. A 0.1% difference (MS Coco experiment) is statistically insignificant, and should be acknowledged as such."}