{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes an attention mechanism to improve the performance of convolutional networks. Rather than the more common purely layer-local attention from Transformer-style models, the authors here use a separate parallel stream to carry attention information across the layers. This is an interesting an novel idea, but unfortunately the paper is let down by being extremely unreadable: Due to a combination of many grammatical errors, and what appears to be plain sloppy writing, it's virtually impossible to follow the main ideas. E.g. phrasing the attention stream as a a recurrent network is either extremely confusing or plainly incorrect: There is no time series input, so is the implied recurrence over processing steps, similar to Neural Turing Machines or the more recent Universal Transformer model? Are the weights even shared across layers? Or is the only thing that makes it \"recurrent\" that the structure of an LSTM cell is used? \n\nOverall, this seems like a nice extension of the ideas behind Squeeze-and-Excite networks, i.e. extracting global features from convolutional feature maps and using them for an attention-style reweighing. The results are not earth-shattering, but outperforming S&E indicates to me that the authors are onto something with the proposed attention mechanism. While the manuscript is certainly far from publishable in its current form, I would welcome to see a revised version submitted to a different forum.\n\n\nA couple of examples just from the abstract (I am not commenting on the rest of the paper): \n- \"attention module\" -> \"attention modules\"\n- \"Main goal of\" -> \"The Main goal of\"\n- \u2018Recurrent Layer Attention network,\u2019 -> \u2018Recurrent Layer Attention network\u2019,\n- \"concurrently propagating\" -> \"concurrently propagate\"\n- \"of proposed\" -> \"of the proposed\"\n- \"scaling coefficients(i.e., layer attention)\" -> \"scaling coefficients (i.e., layer attention)\"\n- \"Recurrent Layer Attention network\" -> \"The Recurrent Layer Attention network\"\nNote that these issues are just cosmetic (minor grammar issues and sloppiness), it gets far worse in the main text and many sentences are just not understandable at all to me. \n"}