{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThe paper focuses on the quantization of Neural Networks (NNs) for both weights and activations. The proposed method jointly quantizes the weights to signed integers while inducing sparsity. The idea is to leverage both importance sampling to sample from the normalized weights or activations distribution together with Monte-Carlo methods. Indeed, a weight can be \"hit\" zero times (inducing sparsity) or multiple times (thus being quantized as an integer).\n\nStrengths of the paper:\n-  The authors exploit the rescaling-invariance of ReLU NNs to sample from the CDF of weights by normalizing them. Furthermore, to increase the number of weights to sample from, they normalize per layer (instead of per input neuron). This is a nice application of the scale-invariance properties of ReLU NNs that has been studied theoretically (see Neyshabur et al,\"Path SGD\").  \n- The experiments are performed on an impressive variety of tasks and models: vision models on CIFAR-10, SVHN and ImageNet (image classification), Transformer on WikiText-103 (language modeling) and WMT-14 (machine translation), LSTM on WikiText-2 (language modeling), DeepSpeech2 on VCTK (speech recognition).\n- The method is clearly explained (in particular by Fig. 1) and the combination of importance sampling together with Monte-Carlo methods in this context seems novel and of particular interest for the community. \n\nWeaknesses of the paper:\n- Although the results are performed on a wide variety of tasks, the compression rates obtained are not on a par with the quantity and abundance of experiments provided. For example, the authors use 8 bits per weight on ImageNet (Table 3), which is a x4 compression ratio compared to fp32 (or x2 compression ratio compared to the model trained in fp16, which is becoming the standard way to train large models, see the apex toolkit provided by Nvidia and its use for instance). As a comparison, see the recent release of PyTorch 1.3 and their tutorial on static quantization: 0.9 percentage points loss in accuracy on MobileNetv2 on ImageNet (4x compression ratio). Using mode epochs, this results can be reduced as stated and the methods used are standard (source: https://pytorch.org/tutorials/advanced/static_quantization_tutorial)\n- The argument that the quantization does not need any pre-training is true but maybe its practicality seems limited: companies would be willing to spend some time *once* to get a proper quantized model (including any re-training) before deploying it after to millions of devices. \n- I raise some concerns about the inference time when the activations are quantized. Indeed, while the weights are compressed offline, the activations need to be compressed online. The sorting phase is definitely not linear and may reduce inference time (note: this is concern and not an affirmation). Moreover, does the \"linear time and space\" complexity take into account this sorting phase?\n- Minor typos/incomplete sentences: for example in 4.2, \"Sorting groups smaller values together in the overall distribution\".\n\nJustification of rating: \nThe proposed method is nicely explained and interesting. However, the results are not convincing enough yet. I encourage the authors to (1) publish their code so that the community can build upon this interesting work and (2) to pursue in this direction as quantization has both research and industry implications. "}