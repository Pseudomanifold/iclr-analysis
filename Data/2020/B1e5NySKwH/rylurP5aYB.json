{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThe paper introduces a novel method for quantizing neural network weights and activations that does not require re-training or fine-tuning of the network. To this end the paper notes that weights and activations in a ReLU network can be scaled by a multiplicative factor, in particular such that weight-magnitudes of a layer sum to one, thus defining a probability mass function. The main idea of the paper is to cleverly sample from this PMF such that a histogram of the sampled values produces an approximate integer-representation of the original weights. The quality of this approximation can be refined arbitrarily by taking more samples, however, the value of the bin with the highest count defines the maximally required bit-precision, such that higher approximation accuracy comes at the cost of requiring larger bit-precision. The method is evaluated on several networks on CIFAR-10, SVHN and ImageNet, as well as some language models. On the image classification tasks, the paper also reports previously published relevant results from competitor methods for comparison.\n\nQuality, Clarity, Novelty, Impact\nThe paper is well written, the main ideas are presented clearly and concisely and necessary experimental details are given in the appendix. The main idea of the paper is neat and I have not seen its application to neural network quantization before. Unfortunately I am not sure how the paper exactly fits within existing methods: while the method could in principle be used to train low bit-width networks (<4bit), the results suggest that in this domain the method is outperformed by other methods. Particularly <=2-bit weights and activations are interesting since they can be used to construct networks that replace multipliers with much faster and more energy-efficient bit-wise operations (XNOR bit-counting). But this regime seems not to be the strong suit of the proposed method. On the other hand models in the >=8-bit regime can be readily obtained with most major deep learning frameworks (e.g. Tensorflow lite and Pytorch), such that having to train or fine-tune a model is rarely a problem, unless one does not have access to training data. But even for the latter, many papers have proposed fine-tuning on dummy-data, or not fine-tuning at all (using linear or non-linear quantization schemes). If this is the targeted regime, then the literature review is missing quite a few important references that should also be compared against in the experiments section. So if the method is specifically targeted at the regime between 4- and 8-bit when re-training or fine-tuning is not possible (for some reason), then the motivation, literature comparison and experimental comparison should be much more targeted at methods that perform well in this regime. While I agree that the main idea is neat and it is nice to see it performing reasonably well, I currently do not see how the method provides strong and convincing advantages over the existing body of methods. I am very happy to be convinced of the opposite during rebuttal by the authors or other reviewers of course. Currently however, I would suggest to take a bit more time and think about particular use-cases that the method is targeted at and flesh this out much stronger - I am not sure whether the rebuttal phase is sufficient for this and therefore (currently) vote for rejection.\n\nImprovements\na) The paper and the method needs a clear focus. I personally would roughly group methods into >=8-bit or <2-bit. The first one is becoming the new out-of-the-box standard and the latter allows for very efficient multiplier-free implementations (even on standard hardware to some degree, particularly for ternary/binary networks). Anything in-between might be interesting but would also probably require non-standard hardware, or variable precision hardware to be practically useful. If the target regime is between 2- and 8-bit, please expand the literature review accordingly and compare against relevant methods previously reported that operate well in this regime. \n\nb) If the emphasis is on training-free methods, also adjust the literature and comparisons accordingly, but also have a strong point when and why training-free methods are important (especially when they come with an overhead for each forward-pass during test-time). This does not mean to shorten the literature review or remove comparisons, but add the most relevant ones given that there is such a large body of network compression methods published in the last three years.\n\nc) I currently have a hard time judging to which degree the benefits reported could actually be exploited by (somewhat realistic future) hardware. Is it possible to sketch an efficient way of implementing the activation-quantization (together with quantized weights of course) on some low-bit-width hardware? While the results show that the method can work in theory with <8-bit activations, I am not sure how the quantization scheme could be efficiently implemented on actual hardware. Is it possible to do some back-of-the-envelope calculations on how much computational overhead the quantizations would add to each forward-pass (e.g. for one of the CIFAR-10/ImageNet networks). For instance, how many 32-/16-bit floating point operations would the sampling of activations roughly require, and how many would it then correspondingly save by performing 4- to 8-bit operations? Or could the whole scheme be implemented in low bit-width (<16bits)?\n\nComments\nii) Related to c) above: at which point would the computational overhead for sampling quantized activations in each forward-pass exceed the cost of retraining a network that can natively deal with low-bit activations. I don't expect a quantitatively precise answer here, but just something that gives a rough idea of the orders of magnitude. Currently I find it very hard to judge how expensive the sampling would be after deployment.\n\niv) In the results-tables the fractional numbers for the bit-widths (e.g. 5.6bit) are a bit misleading. While it is ok to report them, the comparison against other methods should probably use the bit-widths rounded up to the next integer. This should be mentioned at least in the text."}