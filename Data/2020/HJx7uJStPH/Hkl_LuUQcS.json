{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors consider the task of supervised music source separation, i.e., separating out the components (bass, drums, voice, other) out of a mixed music track. In particular, the work considers the task of supervised source separation where the individual target tracks are available during training time. The main contribution of this work is the improvement of an end-to-end waveform-to-waveform separation models through a number of architectural changes that allow such waveform-to-waveform models to perform comparably with other current state-of-the-art methods that instead operate in the spectrogram domain.\n\nOverall, the paper is generally well written and the method is easy to follow. However, I have a couple of concerns about this work, based on which I would rate the work as \u201cweak reject\u201d:\n\n1. The specific architecture proposed in this task does appear to improve performance for this particular task, but it is not clear to me that the conclusions drawn based on the study in this work will be generally applicable to other related tasks. Also, the final model is still only comparable to the spectrogram-based methods overall, and appears to do a slightly worse job in separating the \u2018vocal\u2019 and \u2018other\u2019 tracks than these baseline methods. As such, I\u2019m unsure about the impact of this work.\n2. The section describing the evaluation metric SDR wasn\u2019t very clear to me. Perhaps it would be better to just refer back to (Vincent et al., 06) and just describe what SDR captures instead? Or alternatively, more details could be added to explain the computation more clearly.\n3. Section 4.2: The authors mention that they multiply each source by +/- 1. I didn\u2019t follow why this is being done. Could the authors please clarify.\n\nMinor comments:\n1. The notation in Equation 1 is slightly confusing because \u2018s\u2019 is used to index both the mixture and the sources. I think x \\in \\mathcal{D} would be clearer. Similarly I wonder if x_s \\in \\mathbb{R}^C \\times \\mathbb{R}^T would be clearer than x_s \\in \\mathbb{R}^{C,T}\n2. The reference (Oord et al., 2017) should be rendered as (van den Oord et al., 2017) \n3. The Figure uses K, and S to denote Kernel Width and stride, but this isn\u2019t explicitly mentioned in the text. Perhaps it would be useful, in Section 3.1 to write: \u201c... convolution with kernel width, K=8, and stride, S=4, ...\u201d or something similar, and updating the caption to explain the notation.\n4. Section 3.2: \u201cThe decoder is almost the symmetric of the encoder\u201d --> \u201cThe decoder is almost the inverse of the encoder\u201d would perhaps be better?\n5. Section 3.2: \u201cThe final layer \u2026 S.C_0 ...\u201d --> \u201cThe final layer \u2026 S * C_0 ...\u201d\n6. The notation in Equation 1 and Equation 2 is slightly inconsistent (lower-case vs. upper-case L for the loss function)\n7. Section 4.3: \u201cadditionnaly\u201d --> \u201cadditionally\u201d; \u201c32 GB or RAM\u201d --> \u201c32 GB of RAM\u201d\n"}