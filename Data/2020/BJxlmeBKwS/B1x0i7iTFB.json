{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper describes a method for fricative phonemes boundary detection with zero delays. The authors suggest optimizing a convolutional based neural network with a binary cross loss function to detect such events. \nThe authors provide results on the TIMIT dataset and compare the proposed model to several baselines. \n\nThe task of phoneme boundary detection was well studies under different setups and is very important for various applications, including the one proposed in this paper.\n\nHowever, I have some major concerns regarding this paper, which I would like the authors to clarify. Without these, it is hard to understand the contribution in this paper.\n\n1) the authors chose to model the problem from the raw wave. Although it is getting popularity in several speech processing tasks, it is not clear why not using magnitude/MFCC, for example. In case the authors claim that learning from the waveform is better, I suggest providing a comparison to other features.  \nAdditionally, did the authors experience with simpler architectures Maybe more shallow models? Regarding supervision, did the authors tried comparing to the method proposed by [2] but with a unidirectional RNN? Similar to [3].\n\n2)  If I understand it correctly, the motivation for this task was: accurate detection of fricatives boundary can be used to shift into lower frequency bands in hearing aids. It seems like the boundaries are more important than other phoneme parts such as a mid phoneme, for example. \nIn that case, a better metric might be Presicion + Recall + F1 + R-val next to the boundaries (for instance, with a tolerance level of 10-20ms). Those metrics were suggested on several studies of phoneme segmentation, [1], [2]. \n\n3) The comparison in Table 3 is very strange. Results are reported on different datasets. Although the authors mentioned it in the caption, it is still misleading. I suggest the authors to compare either obtain results on the same benchmark or compare to other baselines.\n\nMinor comments: \n\"If for the majority of the samples in a phoneme our network\u2019s output is greater than the threshold we set\" -> not a clear sentence. \n\n[1] Franke, Joerg, et al. \"Phoneme boundary detection using deep bidirectional lstms.\" Speech Communication; 12. ITG Symposium. VDE, 2016.\n[2] Michel, Paul, et al. \"Blind phoneme segmentation with temporal prediction errors.\" arXiv preprint arXiv:1608.00508 (2016).\n[3] Adi, Yossi, et al. \"Automatic Measurement of Voice Onset Time and Prevoicing Using Recurrent Neural Networks.\" INTERSPEECH. 2016."}