{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an approach to detect fricative phoneme in speech with as little delay as possible, in the context of hearing aids improvement. The model is based on CNN and is trained to detect fricative given the past context. The model is evaluated in terms of recall and compared with recent published works. The results show that the proposed approach outperforms the baselines and yields state-of-the-art performance with no delay. The paper concludes with some analysis on the computational cost and draw possible future work.\n\nThis paper should be rejected for the following reasons:\n- The novelty is very limited: this work applied a well-known architecture (CNN) to a common problem, phoneme recognition. This only novelty is the zero-delay constraint, which is probably not sufficient for ICLR.\n- The significance is also limited given the very specialized application.  \n- Some references are missing (see below).\n- The presented results are not very clear.\n- The computational considerations section is interesting but is missing some important elements.\n\nDetailed comments:\n- The authors selected raw speech signal as input to the CNN, which is not trivial and should be motivated and discussed in the paper. For instance, using the standard features like Mel filterbanks or MFCC will introduce a delay as they are computed on an overlapping window of 25ms. Phoneme recognition using raw speech as input to a CNN has been presented before, the authors should cite [1] and [2] for instance.\n- Table 4 is confusing as the first four lines are not actually evaluated on TIMIT, so I don't see the point of adding these numbers to the table, as they cannot be compared anyway. I would remove these four lines from the Table.\n- In terms of previous works, phoneme recognition on TIMIT is a very popular task, and many others could be cited, such as [3-5].\n- On the computation consideration, the analysis is interesting, but a discussion on the size (i.e. number of parameter) of the network is missing: one way to decrease computation time is to have a smaller network, which is in line with the application: hearing aids probably do not have gigabytes of ram available. \n- Question about the network: the input segment seems to be of size 3072 samples, why ? any motivation for this particular input size ?\n\nMy review can seem to be a bit harsh, I actually enjoyed the paper, but I don't think ICLR is the right conference for it, and I would advise the authors to improve it and submit it to a speech conference.\n\nReferences:\n[1] Palaz, D., Magimai Doss, M. and Collobert, R.. \"Estimating Phoneme Class Conditional Probabilities from Raw Speech Signal using Convolutional Neural Networks.\" Proceedings of Interspeech 2013.\n[2] Zeghidour, N., Usunier, N., Kokkinos, I., Schaiz, T., Synnaeve, G., & Dupoux, E. \"Learning filterbanks from raw speech for phone recognition\". Proceedings of ICASSP 2018.\n[3] Zhang, Ying, Mohammad Pezeshki, Phil\u00e9mon Brakel, Saizheng Zhang, Cesar Laurent, Yoshua Bengio, and Aaron Courville. \"Towards end-to-end speech recognition with deep convolutional neural networks.\" arXiv preprint arXiv:1701.02720 (2017).\n[4] Chorowski, Jan K., et al. \"Attention-based models for speech recognition.\" Advances in neural information processing systems. 2015.\n[5] T\u00f3th, L\u00e1szl\u00f3. \"Phone recognition with hierarchical convolutional deep maxout networks.\" EURASIP Journal on Audio, Speech, and Music Processing 2015.1 (2015): 25."}