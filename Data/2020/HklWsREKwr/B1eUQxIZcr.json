{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new variation on adaptive learning rate algorithm that unifies SGD with momentum and Adam/Amsgrad. They provided convergence proof for this algorithm. The effectiveness of the proposed method was demonstrated through various domains and neural networks architecture. Though the empirical results are extensive, I am leaning towards reject because (1) The reason why the method works isn't clear. (2) The theory doesn't justify the practice. (3) The practical usefulness of the algorithm isn't clear. Here are my detailed comments:\n\n(1) The paper provides an observation which they call \"small learning rate dilema\": One often uses a smaller base learning rate for adaptive gradient methods than SGD with momentum. This makes the boost one can gain from applying learning rate decay to adaptive gradient methods not as significant as applying to SGD with momentum. Based on this observation, they propose to penalize the adaptiveness by adjusting the value p in their algorithm. However, the proposed adjustment seems like a trivial one, without giving too much insights into why learning rate decay is not compatible to adaptiveness. An insightful analysis should try to first answer the following questions:     \n      (a) Why shall one start with a large learning rate in the beginning?\n      (b) Why does learning rate decay gives a boost to performance?\n      (c) If one uses an adaptive method, how does it affect one's choice for the initial learning rate? \n      (d) and how does the adaptiveness changes the effect of learning rate decay?\nTo answer those questions, I suggest the author to read [1] where they gave partial answers to (a) (b). If one tries to do similar analysis performed in [1] for adaptive methods, one might be able to answer (c) and (d). \n\n(2) I have two criticisms to the theoretical analysis carried out in the paper. The most important issue is that the analysis is not useful to show the effectiveness of the proposed method:\n     (a) The rate of convergence matches with SGD + momentum. So it is not better than the baseline. \n     (b) It does not show the relationship of adaptiveness and decaying learning rate schedule. \nThe second criticism is related to the novelty of the theorems.  Please correct me on this because I did not go over the theorems carefully. But based on my crude assessment,  theorems are mostly mechanical applications of prior work to the current extended version. Hence it does not provide any further insights into the convergence of nonconvex optimization methods.\n\n(3) Though the empirical results are good, where the proposed method matched or outperformed all previous methods in The method introduces one extra hyperparameter, p, for tuning. It is then questionable whether the algorithm is efficient in terms of hyperparameter searches, i.e., how many hyparameter sweeps are needed for finding a good run, versus baseline methods like SGD+momentum. Since the performance of the methods are mostly the same, if the proposed method requires as many hyperparameter tuning as SGD+momentum, then it makes the proposed method less useful in practice.\n\n[1] Understanding short-horizon bias in stochastic meta-optimization. Wu et. al. ICLR 2018."}