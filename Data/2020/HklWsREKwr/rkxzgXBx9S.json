{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "It has been empirically observed that adaptive optimizers such as Adam/Amsgrad\nlead to worse generalization than SGD + momentum when used to train neural\nnetworks.\nMotivated by this observation the authors suggest Padam, a modification of Adam/Amsgrad.\nPadam contains a parameter p that, when set to 0 reduces their method to SGD + momentum\nand when set to 1/2 reduces their method to Adam/Amsgrad.\n\nThe Padam iteration is roughly $x^{k+1} = x^k - \\alpha * g / v^p$\nwhere $g$ is an exponential moving average of stochastic gradients and\n$v$ is an exponential moving average of the squared gradient.\n\nIn that way Padam is capable of interpolating between the two methods in order\nto find a good trade-off between the improved convergence of the one and the\nimproved generalization capability of the other.\n\nThe authors also suggest an explanation for why the generalization gap\nof Adam happens:\nThey claim that it is due to the \"small-learning rate dilemma\" that happens\nas follows.\nA small second moment of the stochastic gradients as approximated by $v^{1/2}$\n(think variance) in can lead to large effective steps in some components.\nTo balance this effect out, Adam needs to choose smaller step sizes than\nSGD + momentum.\nIf the same learning rate schedule is used based on a smaller base step size,\nAdam under-trains at the end of training.\n\nThe authors suggest that Padam with p < 1/2 can use larger learning rates\nbecause it does not have as large effective steps if second moments are small.\n\nThe authors also prove convergence rates for making gradients small with Padam.\nThey use the setting of nonconvex optimization and not online convex regret\nanalysis like the Adam/Amsgrad papers.\n\nFinally, the presented experiments using image data sets and standard neural\nnetwork architectures suggest that Padam indeed shares the advantages of both\nSGD + momentum and Adam and thus obtains a best of both worlds.\n\nI suggest to accept the paper.\nIt introduces an elegant generalization of Amsgrad that appears to be\nempirically useful in experiments.\nThe experiments seem to be fairly performed (in terms of hyperparam search).\nThe rigorous convergence analysis is laudable although perhaps not as relevant\nas the practical usefulness of the suggested approach.\n\nI do however suggest that some changes be made.\n1. The \"small learning rate dilemma\" phenomenon needs to be more clearly defined\n\tand explained.\n\tThe whole relationship of SGD step size schedules to generalization\n\t(e.g. simulated annealing analogy) is certainly nontrivial.\n\tI would rather the paper not make some nonrigorous claims if there is no\n\tproof.\n\tOr state clearly whether something is conjecture or proposition (with proof).\n\n2. Explain why the same learning rate schedule should or is (in practice) used\n\tfor Adam as for SGD + momentum, as this does not seem to make sense given\n\tthe aforementioned dilemma.\n\nSpecific notes / suggestions:\n- Page 2: \"We proposed a novel\" -> \"We propose a novel\"\n\n- Page 4: \"bridging this generalization gap\"\n\t\"this\" does not make sense to me in that context, rather use \"the\"\n\n- Page 5: Figure 1, p = 1/16 seems to still be the most attractive in terms of\n\tgeneralization in the long run. Why would I not want to wait till the model\n\tis fully trained?\n\n- Page 4: \"It is very likely that Adam/Amsgrad is \"over-adaptive\"\n\tThis seems to me a strong claim and the explanation that follows it to me\n\tis not rigorous enough.\n"}