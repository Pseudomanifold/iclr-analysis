{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper studies the hierarchical reinforcement learning (HRL) problem. It proposes a framework TAIC that learns temporal abstraction from past experience or expert demons without task-specific knowledge. The method is to formulate the problem by a temporal abstraction problem. That is, they assume that the action sequence is generated by a latent variable o. By regularizing the latent space by adding information-theoretic constraints, they are able to learn the representation. The paper later uses visualization to demonstrate the effectiveness of the learning. \n\nI would think this paper is slightly below the borderline. It is an interesting method of encoding the option sequence by a continuous variable. Therefore, the action space becomes continuous rather than discrete. However, I found it not convincing why continuous option space is better than discrete ones. It appears to me that the experiment section does not provide a comparison with previous discrete option based methods as well. \n\nComments:\n* 4th line of related work: Parr --> \"Parr & Russel\"\n* Page 2, problem formulation: in beta(s,o), s is not defined. Maybe you can denote it as beta_o(.).\n* It appears to be that the\"option\" is a sequence of actions? This can only happen in the deterministic environment. What will you do if applying pi does not give the same sequence of actions? For instance, from (s1,a1) -> (s2, a2), where s2 is generated from a random distribution, and a2 is based on s2.\n* the paper is overlength "}