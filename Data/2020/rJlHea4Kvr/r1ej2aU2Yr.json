{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\n\nThis paper deals with the posterior collapse issue, i.e., when the posterior is set equal to the prior, which may affect variational autoencoders (VAEs) in practice. Compared to existing literature on this topic, the focus here is on conditional VAE, where observations \u201cx\u201d are modeled conditional on a latent variable \u201cz\u201d and some auxiliary information \u201cc\u201d. The authors argue that when the decoder is conditioned on the concatenation of \u201cz\u201d and \u201cc\u201d, then posterior collapse is even more likely than in vanilla VAE. Indeed, the decoder under such configuration is free to ignore the latent variable \u201cz\u201d as long as \u201cc\u201d carries enough information to explain observations. To alleviate this issue, the authors propose an alternative, which consists in making the latent variable \u201cz\u201d explicitly dependent on the auxiliary signals \u2018\u2019c\u201d. In more details, they introduce a new VAE to represent the auxiliary data \u201cc\u201d, whose posterior q(z|c) is used as a prior in the original VAE over \u201cx\u201d. The posterior of the latter is defined by scaling and shifting the prior q(z|c). For fitting, the author alternate the optimization of the two VAEs and use annealing to down-weight the effect of the Kullback\u2013Leibler (KL) terms in early training.  The proposed method shows good performance compared to the retained baselines, on the tasks of conditional human motion prediction and image captioning.\n\nMain comments. \n\nStrengths.\n\nOverall, this paper is well written and easy to follow. Tackling posterior collapse in VAE and its variants is an important and current topic. The proposed idea, while simple, is interesting. Experiments show that, on the tasks of human motion prediction and image captioning, the proposed model CPP-VAE improves upon the performance of conditional VAE (CVAE), using concatenation, and some recent baselines introduced in the context of conditional human motion prediction. \n\nWeaknesses.\n\nThe idea of making the latent variable \u201cz\u201d explicitly dependent on the auxiliary information \u201cc\u201d is not new, and it has been already explored, e.g., in the context of collaborative filtering [1]. Although the approach of [1] and that of the current paper are technically different, the former work should be discussed and its CVAE variant (section 4.2 in [1]) should be included as a baseline. This would allow assessing the importance of the proposed CPP-VAE.\n \nExperiments do not focus on posterior collapse making it hard to assess and understand the impact of the proposed approach on this phenomenon. Moreover, it is not clear to what extent the proposed model can mitigate the above issue. In more details, \n- The authors use annealing to down-weight the effect of the KL terms in their objective, eq. 8, during the first training iterations. It is therefore not clear whether CPP-VAE can mitigate posterior collapse because of annealing or because of conditioning of \u201cz\u201d on \u201cc\u201d a priori. \n- Experiments show that the KL of CPP-VAE does not drop to zero as with CVAE when concatenation is used. However, this is not enough to conclude that posterior collapse does not occur in CPP-VAE. In fact it may still occur partially, i.e., some dimensions may well have collapsed. Therefore, further experiments are required in order to assess to what extent the proposed approach can alleviate posterior collapse. For example, one possibility is to follow previous related work [2,3,4] and report quantities such as the number of \u201cactive-units\u201d.\n- In addition to the CVAE variant explored in [1], comparisons with recent approaches to alleviate posterior collapse are lacking, e.g., KL-annealing, VampPrior [3], delta-VAE [5], etc. Although these have been introduced in the context of VAE, they can be easily considered/adapted to the conditional case, which would make it clear whether CVAE requires a different treatment.  \n\nTo sum up, I would strongly recommend revising the experiments according to the above comments and include more results regarding posterior collapse, which is the focus of the paper. Unfortunately, I cannot recommend acceptance with the current experiments. \n\nAdditional comments/questions.\n- The proposed approach uses VAE to represent \u201cc\u201d, referred to as CS-VAE, whose posterior is used as a prior over \u201cx\u201d. What if posterior collapse happens in CS-VAE? CPP-VAE would be reduced to vanilla VAE and thus may in turn suffer from an over regularized latent space. Have you observed such behavior in practice or tried to investigate this potential limitation of the proposed method?  \n- Please consider improving the readability of tables 3 and 4, the fonts are too small.\n\n\nReferences\n\n[1] Lee, Wonsung, Kyungwoo Song, and Il-Chul Moon. \"Augmented variational autoencoders for collaborative filtering with auxiliary information.\" Proceedings of the 2017 ACM CIKM, 2017.\n[2] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. \"Importance weighted autoencoders.\" ICLR (2016).\n[3] Tomczak, Jakub M., and Max Welling. \"VAE with a VampPrior.\" AISTATS (2018).\n[4] Dieng, Adji B., et al. \"Avoiding latent variable collapse with generative skip models.\" AISTATS (2019).  \n[5] Razavi, Ali, et al. \"Preventing posterior collapse with delta-vaes.\" ICLR (2019).\n\n"}