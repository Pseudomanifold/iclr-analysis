{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Mitigating Posterior Collapse in Strongly Conditioned Variational Autoencoders\n\n\n1. Summary\n\nThe paper considers strongly conditioned generative models -- a class of models in which the decoder can be powerful and the condition inputs are informative enough such that latent variables could be ignored. The paper proposes an objective function and a particular parameterisation of the variational distribution such that the latent variables explicitly depend on the input conditions. In particular, the paper proposes learning a vanilla generative model to explain the conditions, then using parameters of the variational distribution of this model to be in the prior for the latent variables of the data, as well as to constrain the variational distribution over the latent variables for the data. The proposed model + objective function are evaluated on a suite of tasks involving conditional motion generation and image captioning. The core evaluation metrics are to assess diversity of generated samples, as this indicates the latent variables are not switched off after training.\n\n2. Though the proposed approach is sound and the presentation/writing is clear, I\u2019m leaning toward *weak reject* because of the following reasons:\n\na. I like maths and thus in general like to take people\u2019s work and reinterpret/understand them by writing down the underlying probabilistic model, or the underlying variational distribution. \n\nFirst, the paper says that it considers conditional generative models -- I don\u2019t think this is 100% precise, as the final objective is a variational bound to the log joint p(x, c), not just p(x | c), where x is the data and c is the conditions. Now let\u2019s dig a bit deeper and write down the joint of everything, factorized as considered in this paper (where z_x and z_c are the latent variables for the data and conditions respectively):\n\np(x, c, z_x, z_c) = p(x | z_x) p(c | z_c) p(z_x | z_c) p(z_c)\n\nNow consider the following variational approximation, which is what the paper implies in the explanation and uses as listed in the algorithm in the appendix:\n\nq(z_x, z_c) = q(z_c) q(z_x | z_c)\n\nThe paper uses q(z_c) = N (z_c; \\mu_c, \\sigma_c^2) where \\mu_c and \\sigma_c are outputs of an encoder -- this is fine. The issue is that the paper uses a deterministic mapping z_x = \\mu + \\sigma z_c, or q(z_x | z_c) = \\delta (z_x = \\mu + \\sigma z_c). The paper uses this explicitly: \u201cz is sampled given z_c\u201d (to be precise, there is no sampling given z_c) and in algo 2 in the appendix. The paper correctly states the marginal density q(z_x) (which is another normal), but let\u2019s just focus on this delta density for now.\n\nNow let\u2019s us write down the variational objective (again, to deal with the joint, not the conditional):\n\nF = E_{q(z_c) q(z_x | z_c)} [ log p(x | z_x) ] + E_{q(z_c)} [ log p(c | z_c) ] - KL( q(z_c) || p(z_c) ) - E_{q(z_c)} [ KL( q(z_x | z_c) || p(z_x | z_c) ) ]\n\nNow the fourth terms is problematic as a density in the KL is a delta. So KL term presented in the paper is essentially approximating this by KL( q(z_x) || p(z_x | z_c) ) and ignoring the expectation wrt q(z_c). I don\u2019t think this is bad or incorrect, just that I wish the paper presents their approach this way to make things clearly, rather than from presenting heuristics + designing losses.\n\nNow, here is my derivation of what the paper probably means in the first place (which is unfortunately not what the paper ends up doing because of the computational graph presented in the paper). So instead of choosing the variational distribution like above, the joint variational distribution is a mean-field one: q(z_c, z_x) = q(z_c) q(z_x) where q(z_c) = N (z_c; \\mu_c, \\sigma_c^2) and q(z_x) = N (z_x; \\mu_x + \\sigma \\mu_c, \\sigma^2 \\sigma_c^2). Note that there is no explicit conditioning, just the parameters are tied. This also means that the sampling of z_c and z_x  should not be tied/sequential as in the paper. Now the paper also chooses p(z_c) = N (z_c; 0, I) and p(z_x) = N(z_x; \\mu_c, \\sigma_c^2). Now write down the variational bound, we will get the final bound in the paper in eq 8.\n\nSo in short, there is a discrepancy in what the paper presents and the actual computation graph, with a valid variational bound in a generative model. As I said above, I don\u2019t think this invalidates the results in the paper, just means that the theoretical foundation is not clean as I would like, in particular, as the paper seems to criticize previous methods: \u201cAny annealing weight that does not become, and remain equal to one at some point during training yields an improper statistical model; weakening the decoder tends to degrade the quality of the generated samples; changing the objective does not optimize the true variational lower bound\u201d.\n\nb. Having motivated the need of the proposed additional architecture and objective, the paper doesn\u2019t include a comparison to vanilla conditional VAE, or VAE for both data and conditions in which the latent variables collapse to the prior, especially in the motion prediction example. The ELBO/log-likelihood reported is also not comparable (because of some models the conditional while this paper models the joint, as detailed above). \n\nIt would also be interesting to see the performance of a conditional VAE with a variational distribution parameterized like the marginal considered here: q(z_x) = N (z_x; \\mu_x + \\sigma \\mu_c, \\sigma^2 \\sigma_c^2), i.e. the parameters of z_x explicitly depends on c and there is no concatenation of c and z_x to generate the data.\n\n\n3. Additional points:\n\njust to be pedantic, what is the goal of the latent variables here given the observation likelihood can be explained by the condition inputs and that the objective is to maximise p( x | c )? Does this mean posterior collapse is a good thing here? Or are we essentially overfitting? Would early stopping help with diversity?\n\nIt would be good to have some error bars for the numbers reported.\n\nWhy z = z_c in the testing phase in algorithm 2 in the appendix? This seems strange."}