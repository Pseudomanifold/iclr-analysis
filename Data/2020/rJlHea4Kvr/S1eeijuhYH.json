{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tries to address a problem they call 'posterior collapsed' in which conditional VAEs tend to ignore the latent variable and learn an almost-deterministic mapping from conditioning to samples. The paper poses a potentially algorithmic approach. However, the paper lacks evidence to validate most of its claims, in particular it fails at 1) Establishing that the problem actually exists, by theory or experiments (or carefully selected references), 2) Establishing the existence of a causal relationship between conditioning and latent variable being independent and posterior collapse, again either by theory and / or targeted experiments 3) Validating claims about the method's performance empirically.\n\nHere go my detailed comments in these regards:\n\nBefore going into the comments, I will use Q to denote the joint distribution (x, z, c) arising from the prior Q(z) ~ Gaussian, Q(c) ~ whatever the conditioning distribution is and independent of z, and Q(x |z, c) being the decoder. As well, I will use P to denote the joint (x, z, c) where P(x, c) is the joint of data and conditioning distributions, and P(z | x, c) is the encoder. I'm sorry this is different from the paper but I find it a lot easier to elaborate my review with this notation.\n\nI will refer to posterior collapse as in the abstract, namely Q(x | z, c) = Q(x | z', c) = Q(x | c), or alternatively Q(x | c) being deterministic.\n\n1) The authors state that Q(x | c) becomes deterministic in many tasks. However, this is not a priori a problem if there indeed P(x | c) is deterministic. The authors thus make a really bad job at motivating why this might be an important problem, and no experiments are provided show that this happens in tasks where P(x | c) is non-deterministic (aside from the unsubstantiated claim without a reference or experiment that this must happen in human motion prediction).\n\n2) The authors imply many times that posterior colapse (Q(x | c) deterministic) is somewhat equivalent to the encoder P(z | x, c) ignoring x, i.e. P(z | x, c) = q( z ) [i.e. the KL in equation 2 is zero when c and z are independent for the prior]. The authors never justify this claim in any way, and it is essential since the method indirectly tries to address posterior collapse by preventing the encoder from satisfying P(z | x, c) = q(z). This to me is the biggest flaw of the paper.\n\nIn the first paragraph when they discuss the non-conditional case and say \"Training such models however may often result in posterior colapse: the posterior distribution q(z | x) of the latent variable carrying no information about the input. In other words, the model learns to ignore the latent variable\". First of all, it is unclear wether q(z | x) is the *approximate* posterior given by the encoder or the true posterior.\nIf it is the approximate posterior given by the encoder, P(z | x) in my notation, then this claim seems false. This is a claim of equivalence between P(z | x) = P(z) for all x (the first sentence in this quote, i.e. the encoder ignoring x and matching the prior) and the second sentence in this paragraph, which according to the abstract should be Q(x | z) = Q(x) for all z, i.e. the decoder ignoring z. Surely this doesn't follow from Bayes' rule, or at least I cannot see how. Namely, Q(x | z) = Q(z | x) Q(x) / Q(z). If it was the case that Q(z | x) = Q(z) then this would be true because the denominator would cancel with Q(z | x). However, this would require the posterior of the encoder matching the prior, not the decoder matching the prior! Unless we assume Q(z | x) = P(z | x) which is false unless the approximate posterior matches the true posterior, the two things (encoder ignoring x, decoder ignoring z) are far from equivalent or causally connected. This makes most of the paper fundamentally flawed in my opinion.\n\nIn the same way, the link between the KL term in equation 2 being zero and Q(x | z, c) = Q(x | c) is false by the exact same argument. Q(x | z, c) = Q(x | c) would require Q(z | x, c) = Q(z | c), not P(z | x, c) = Q(z | c) which is the KL term being 0.\n\n3) The authors employ very ad-hoc techniques for measuring quality and diversity of the samples. For instance, they use expected distance between samples to assess diversity, which fails to capture multimodal behaviour, and for quality they train a discriminator after training the generative model. The discriminator training after the generative model has been finalized tends to be meaningless, since the accuracy of the discriminator doesn't depend on how close the generated samples are to the real data (or how close the fake and real data manifolds are), see https://arxiv.org/abs/1701.04862 .\n\nFurthermore, the authors fail to make experiments showing that these problems have indeed a non-deterministic P(x | c), thus the importance of posterior collapse is impossible to assess in these tasks. "}