{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the problem of span recovery for deep neural networks, that is\nrecovering the space of inputs that affect the output of the network.\nThe authors propose two algorithm, one for the case of ReLU activations and one\nfor the case of differentiable activations, and theoretically prove that they can\nrecover the span under certain assumptions. They complement these results with\nexperiments on a simple model on MNIST.\n\nAt a high level, the algorithms rely on computing gradient information through\nfinite differences in order to recover direction in input space that the model\nis sensitive to. The assumptions are reasonable, mostly requiring that when\npassing random inputs through the model, the output varies sufficiently while\nthe gradients are sufficiently large with some probability. Given these\nassumptions, the algorithms maintain a subset of the span and iteratively find\ndirections in the input space that are not captured by the current subspace.\n\nOverall, the paper is well-written and addresses a fairly fundamental problem.\nThe solution presented is motivated and the analysis intuitive. However, I am\nnot fully convinced about the importance of studying span recovery in this \nsetting. In a white-box setting, we can just run SVD on the first layer. The\napplication to input obfuscation is fairly unconvincing since we already know\nhow to perform query-only adversarial attacks. Finally, I have a few questions\nabout the experimental results (see below). \n\nI thus recommend weak rejection at the moment but would be willing to\nreconsider based on the author response.\n\nSpecific comments:\n-- Algorithm 2 relies on the value of sigma when only tau(sigma) is available.\nThis should be clarified and the specific lemmas that allow this referenced.\n-- The first sentence of the experimental section is confusing. When is this\ncomputation performed? Based on the plot, the matrix is always full rank (80).\n-- More generally, how is the evaluation performed? Is the span returned \ncompared to the ground truth (via SVD)?\n-- I find it somewhat odd that it is possible to recover a rank 80 subspace with\n100 samples. After all, the subspace is described by 784 * 80 reals (which is\nalso the theoretical complexity O(n * k)) and we are only allowed input-output\nqueries.  Can the authors provide some intuition/clarification? Are these a 100\n_gradient_ computations?\n-- Can the authors provide additional details about how they compute gradients\nexperimentally (using finite differences)?\n-- Why are the input obfuscation experiments not performed with the subspace\nrecovered by the proposed algorithms? This would be necessary to argue that\npartial recovery actually leads to adversarial vulnerability.\n -- Denoting a neural network by sigma in the abstract and first intro paragraph\n is confusing (since sigma denotes the activations later).\n-- Prop 4.1: gradient needs a norm."}