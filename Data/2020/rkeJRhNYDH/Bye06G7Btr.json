{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes a new dataset for table-based fact verification and introduces a couple of methods for the task. I think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ML and NLP are overstated in my opinion. In addition the paper needs proof reading as there are many typos, some of which make comprehension problematic. In detail:\n\n- The dataset is the main contribution of this paper. Its size is great. However I have some concerns on its construction. The guidelines described for constructing simple and complex claims are rathe vague, e.g. how does one define \"too much symbolic reasoning\" and explain it to crowdworkers? How was the linguistic complexity difference between the two channels measured? How were the claims sanity checked? In the beginning of section 2.3 it is stated that quality control filtered a substantial proportion of the statements. How was this done?\n\n- A troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the Entailed/Refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion. Thus it will always be possible for models to score highly without doing the right thing. Avoiding trivial re-writes helps, but it is unlikely to be enough as human crowd workers will try to optimize their earnings.\n\n- The two models discussed are OK as baselines, but not particularly interesting or appropriate. Both require substantial rule-based processing (named entity linking, latent program construction. templates) and eventually linearize structured data (the program or the table). I understand that this is not the main contribution of the paper, but given the substantial amount of work on semantic parsing and question answering I was expecting more appropriate baselines taking previous work into account. The performance of the model is not great either, especially if we consider that they are only evaluated on returning a binary label, not the correct evidence from the table.\n\n- While it is true that most of the fact verification work has focus on textual sources, the challenge of combining reasoning over continuous and discrete representations is not new. The various QA works mentioned in the related work section address this, as well as work on theorem proving: https://arxiv.org/abs/1705.11040 Furthermore, there has been at least one more previous work on table based verification against  FreeBase tables: https://www.aclweb.org/anthology/D15-1312/. Thus I believe the discussion of the challenges posed by this dataset should be re-framed, especially given that the kinds of programs that need to be constructed are of similar complexity to previous work like the WikiTableQuestions.\n\n- writing: \"the model is expected to excel... but to fall short\", \"we follow the human subject research protocols\" (which ones?), \"in case of obvious stylistic patterns\" (which ones). On the whole it is understandable, but the writing should be improved."}