{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. \n\nApart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I'm inclined towards accepting the paper. \n\nA few remarks/concerns are:\n1. Usefulness of the dataset:\nIt seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (\u2018there are five candidates in total\u2019). With a binary space for supervision, we don\u2019t really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to \u201cdo well\u201d without \u201clearning well\u201d, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014).\n2. Related work:\nThe paper talks about introducing a new \u2018format\u2019 of evidence (structured text) and talks about \u2018unstructured text\u2019 as the only \u2018other\u2019 format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with \u2018textual\u2019 evidence.\n3. The dataset statistics in Table 1 don\u2019t seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)).\n4. Page 3, Section 2.3: \u201cwe further perform quality control\u201d -> a line or two to explain quality control?\n5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study.\n\nMinor remarks:\n- Page 2: Section 2: \n1. overtly -> overly\n2. huge tables(e.g. -> huge tables (e.g.\n\n- Page 3: Section 2.3\n1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements\n\n- Page 4: \n1. candidate) . we need to  -> candidate), we need to"}