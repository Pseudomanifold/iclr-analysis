{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work presents a novel approach to extracting reusable motor primitives from task demonstrations.  The approach taken in this work involves learning a deep encoder network which translates an arbitrary length trajectory in a robot's configuration space (is this right?) into a sequence of vectors describing different motor primitives.  A second decoder network translates these vectors into a sequence of trajectory segments.  These networks are trained to minimize the distance between the original trajectory, and the trajectory generated by encoding and reconstructing the original as a sequence of primitives and reconstructing.  An additional regularization term discourages the network from learning trivial, one step primitives.  The decoder network is also initialized by training on a set of simple trajectories generated by a robotic planning algorithm.\n\nExperiments involved extracting motor programs from the MIME data set consisting of demonstrated trajectories for the Baxter robot.  In addition to qualitative visualizations of the learned primitives, quantitative results using a limited set of human-segmented trajectories demonstrate that the learned primitives roughly correspond to the segmentations that humans identify.  Further experiments show that reinforcement learning in the space if learned primitives is more sample efficient than RL in the low-level control space.\n\nThe work presents a novel and effective solution to the difficult task of learning reusable motor skills.  While the work focuses on robotic control, it is likely that similar approaches could be developed for more general reinforcement learning problems.  There is room for improvement.  In particular, ablations on the regularization and initialization mechanisms could help us better understand the importance of these elements in learning useful motor programs, and illustrate the robustness of this method to different methods of initialization."}