{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper offers through analysis on instability of NAS training that the system degenerates and reaches to trivial solutions, e.g., more skip-connect operators, as training goes longer. Motivated by this issue, this paper proposes an approach to stabilize NAS training.\n\nStrength:\n[1] Theoretical analysis\n[2] The paper is well written\n[3] This paper is trying to solve a very interesting and important problem\n\nWeakness:\n[1] Lack of ablation study. It would be better to show learning curve like Fig.1 to illustrate how the proposed approach helps to stabilize training and how about >500 epoch, any unstable?\n[2]The proposed approach offers comparable results with SOTA (early stopping). I agree that it opens a direction of stable NAS training, but the contribution so far is limited. I expect to see quality gains due to improved training technology\n"}