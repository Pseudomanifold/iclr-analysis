{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: Comparison of neural architecture search algorithms is hindered by the lack of a common measurement procedure. This paper describes a publicly available benchmark on which most recent types of NAS algorithms can be evaluated. It does so by exhaustive calculation of performance metrics on the full combinatorial space of select architectures, on two select datasets. NAS algorithms can then perform search without having to perform evaluation on each node, which shrinks the computational cost of experimentation and benchmarking drastically.\n\nI recommend acceptance, as the resource described in the paper has been created thoughtfully and is useful to the research community, as well as to users of NAS algorithms. The paper is clear about restrictions too, which doesn't hurt.\n\nThe technical details are laid out clearly especially in sec 2.1. It would be interesting to know the computational cost of producing the data. It is useful in practice to have access to different metrics (validation, training and test) for each node, as well as extra diagnostic information.  \n\nThe usefulness of the resources hinges on a few elements, which make its strength and also weakness:\n- choice of tasks and datasets\n- choice of skeleton architecture, fig 1\n- choice of hyperparameters, sec 2.3 (I note there is no regularisation, as discussed in the paper)\nAll of these seem reasonable to me. It is clearly a limitation that hyperparameter search is infeasible to conduct in parallel with architecture search, as pointed out sec 6.\n\nThe principal competitor NAS-Bench-101 is only applicable to specific NAS algorithms, which evidences the need for the present resource. The discussion and comparison in sec3 is fair.\n\nThe discussion of weaknesses, such as possible overfitting patterns, or technical choices, is balanced. \n\n# Minor\nEnglish proofreading is required. \n- Maybe you can attempt a pun on Ananas in the naming?\n- I'm not sure \"fairness\" as in the abstract is the exact core problem; I would call this comparability.\n- sec2 head: \"side information\", I usggest diagnostic information\n- sec2.2 \"and etc\" is a redundant: etc stands for \"and the others\"\n- sec2.4 almost involves almost; target on computation cost; stabability\n- sec 4: has impacts on, parameters keeps the same -> stays, which serves as testing -> to test\n- sec6 tricky ways-> insidious?"}