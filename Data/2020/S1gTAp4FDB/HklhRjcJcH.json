{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThe authors of this paper propose a novel approach for symbolic regression. The simulation results demonstrate that the proposed approach can find a better function g producing similar results as desired function f by providing the additional information \u2013 the leading power of function f.\n\nPaper strength:\n1.\tThe proposed NG-MCTS is elegant to solve the problem of symbolic regression.\n2.\tExperiment results illustrate the superiority of the proposed approach\n\nPaper weakness:\n\n1.\tI can follow most of the mathematics in the paper. But the most confusing part for me is why you feed the random partial sequence for training. Besides, how you do inference to generate a sequence of the production rules.\n2.\tWhat is the final objective function? If I do not misunderstand, it could be the cross-entropy loss between the output of GRU and the next target production rule, RMSE and the error on leading power. Then how you optimize it? Please describe more details about this.\n3.\tThe authors of this paper introduce more information \u2013 leading power of desired function for symbolic regression but they incorporate the additional information by introducing a simple loss function term. How about the performance of baseline approaches with those kinds of information?\n4.\tThe whole systems seem like very complicate and it would be more interesting if the authors provide sufficient ablations to decompose the complex algorithm.\n"}