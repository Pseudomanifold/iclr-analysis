{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors introduce an unsupervised text style transfer model that adds syntactic features (part-of-speech tags) to the word-level output targets and input. The authors use a fairly simple neural architecture (as far as text style transfer goes), and achieve state-of-the-art style transfer, compared to many other models on automatic metrics and human evaluations. The overall idea (adding syntax information to models) is an interesting one to explore.\n\nThe empirical results and evaluation show that the overall model is strong; however, I would like to see the authors' main claim (that adding syntax is helpful) better explored. The proposed model differs from other style transfer models is a number of ways (architecture and training losses), so it's unclear how much of the gains come from syntax. The authors include one (very helpful) ablation in Table 4 (row 2) that shows that removing syntax entirely from their model hurts performance. However, I think it'd be helpful to explore the role of syntax in more depth, i.e.:\n1) Is syntax helpful because it is an additional prediction target/auxiliary loss? Or it is helpful because it is an additional input feature to the model? Or both? If it is only one or the other, it may be possible to simplify the model and/or improve other model architectures by making a simple modification. (For example, what happens when you add syntax as an auxiliary prediction target to another model?)\n2) In what kinds of cases syntax is helpful/hurtful? What sorts of errors does having syntax help with?\n3) Do other kinds of syntax-related annotations help?\n4) How do humans rate the fluency of the no-syntax version of the proposed model (i.e., the evaluation in Table 2)? It would be interesting e.g. if style and content ratings stayed the same, but fluency ratings went down (or if something else happened)\n\nA second major concern that I have is that syntax may not be helpful for style transfer when using pre-trained language models (i.e., as in XLM from from \"Cross-Lingual Language Model Pretraining\"). XLM showed that the initialization is very important for strong unsupervised results with back-translation and denoting auto-encoding/self-reconstruction. Thus, in the long run, it seems that unsupervised style transfer will move towards using pre-trained language models, which already learn a great deal about syntax implicitly. So I am concerned that the gains that the authors report now will not hold in the future. Ideally, I would like to see a comparison of style transfer results between XLM and XLM+Syntax.\n\nQuestions/Other Notes:\n* Consistency Reward in Sec. 3.3.4: I might be missing something, but why not use supervised learning to train the label predictor to predict the labels from an external syntax annotation tool (rather than using REINFORCE)? In general I found this subsection a bit confusing and hard to follow\n* It would be nice to add XLM to the related work section, as the model architecture and losses seem similar (except without syntax). Other than that, the related work is written very clearly!\n* It would be helpful to add some discussion/citations about where syntax has been shown useful in related contexts (I felt that mostly unsupervised style transfer work was discussed)"}