{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a multitask learning-based model for unsupervised text style transfer by adding a syntactic prediction task to the existing framework of Lample et al. (ICLR 2018, 2019). The authors essentially modify the backtranslation part of the Lample training procedure by forcing the model to reconstruct both the input tokens and input syntactic labels (here, POS tags). They also propose a consistency constraint to align the generated syntactic labels with the words. Both human and automatic evaluations demonstrate the effectiveness of their model (called SAST). The overall idea of the paper (using not only surface forms of word but also syntactic information for style transfer without parallel data) is very interesting. However, the paper strikes me as a relatively minor modification over the Lample et al. framework, and the ablation study shows that removing the syntax information / consistency loss does not significantly affect the model. As such, I am a weak reject.\n\ncomments:\n- A more detailed comparison with the Lample papers would be nice. From what I've understood, it appears the same apart from (1) reconstructing POS tags along with words during autoencoding and backtranslation; (2) the encoder is pretrained here whereas Lample train it online; (3) a consistency loss is added to align words and POS tags. \n- Following the previous comment, why did the authors find it necessary to pretrain the encoder here? \n- There is a lack of detailed analysis of why the consistency constraint helps. What kind of errors are made without this constraint? Did you observe a lot of \"incompatible outputs\"? In general, explaining this section clearly with examples would make it more compelling; as is, it feels like an unnecessary addition (and the ablation study doesn't really help motivate it).\n- The authors mention \"our preliminary experiments did not show further improvements by feeding the syntactic label as input\". It would be nice to have more details about these experiments. How were the POS tag labels fed to the model? Similar results in NMT (e.g., Aharoni & Goldberg ACL 2017) have shown that this is an effective approach, so it would be useful to know if it doesn't work in style transfer. \n- Why just POS tags? There was potential here to use many different types of syntactic labels, such as linearized parses (the Aharoni & Goldberg paper above), chunked parses (Akoury et al., ACL 2019), or dependency relations, all of which seem more informative than POS tags. \n- there were minor typos / grammatical errors throughout. some examples:\n    - 4.2 last line \"we the\"    \n    - Table 4 downward arrow for perplexity\n"}