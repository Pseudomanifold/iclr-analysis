{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The article \"PairNorm: Tackling Oversmoothing in GNNs\" considers the interesting phenomenon of performance degradation of graph neural network when the depth of the network increases beyond the values of 2-4. The authors argue that one of the reasons for such behavior is so-called \"oversmoothing\", when intermediate representations become similar for all the nodes in the graph. The authors propose the special NN layer \"PairNorm\", which aims to battle with this issue.\n\nThe proposed PairNorm approach boils down to the recentering and normalization of all the representations after each graph-convolutional layer of the network. The authors consider 2 variants of choosing normalization constant:\n1. The one which multiplies all the embeddings for the layer by the same number. This operation allows to keep the average squared pairwise distance between node representations constant. \n2. The one which makes the norms of all the representations equal to pre-specified constant, i.e. just projection of all the embeddings on the sphere.\n\nI should note that the two proposed approaches are very different in nature, though the latter one is introduced without much additional discussion. The benefits of approach 1 are not entirely clear as it basically just scales the whole embedding population. Such a scaling doesn't affect the relative distances between points and thus should not have major effect on the performance. Approach 2 is completely different due to the projection on the sphere of each embedding independently. However, the reasons why it is a good idea or not are not discussed in the paper. \n\nThe experimental part of the paper considers several standard graph data sets. The authors report that the proposed normalization schemes do not improve the quality of classification in the standard semi-supervised learning setting. They additionally consider artificially created missing features and observe increasing quality in such a scenario.\n\nTo sum up, I think that while the motivation behind the paper is very natural, it doesn't look like the paper finds the solution both theoretically and experimentally."}