{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles the problem of learning an encoder and transition model of an environment, such that the representation learnt uses an object-centric representation which could favor compositionality and generalisation. This is trained using a contrastive max-margin loss, instead of a generative loss as previously explored. They do not consider RL or follow-up tasks leveraging these representations and transition models yet.\nThey perform an extensive assessment of their model, with many ablations, on 2 gridworld environments, one physical domain, and on Atari.\n\nThe paper is very well motivated, easy to follow, and most of its assumptions and decisions are sensible and well supported. They also provide interesting assessments and insights into the evaluation scheme of such transition models, which would be of interest to many practitioners of this field.\n\nApart from some issues presented below, I feel that this work is of good quality and would recommend it for acceptance.\n\n1.\tThe model is introduced in a very clear way, and most decisions seem particularly fair. I found the presentation of the contrastive loss with margin to be clear, and the GraphNet is also well supported (although see question below). \u2028However, two choices are surprising to me and would deserve some clarification and more space in the main text, instead of the Appendix:\n\ta.\tWhy does the object extractor only output a scalar mask? This was not extremely clear from reading the main text (and confused me when I first saw Figure 1 and 3a), but as explained in the Appendix, the CNN is forced to output a sigmoid logit between [0, 1] per object channel.\u2028\n\tThis seems overly constraining to me, as this restricts the network to only output \u201c1 bit\u201d of information per \u201cobject\u201d.\n\tHowever, maybe being able to represent other factors of these objects might be necessary to make better predictions? \u2028\n\tThis also requires the user to select the number of output channels precisely, or the model might fail. This is visible in the Atari results, where the \u201cobjectness\u201d is much less clear.\n\t\u2028Did you try allowing the encoder to output more features per objects? \n\tObviously this would be more complicated and would place you closer to a setting similar to MONet (Burgess et al. 2019) or IODINE (Greff et al. 2019), but this might help a lot.\n\tb.\tIt was hard to find the dimensionality D of the abstract representation $z_t$. It is only reported in the Appendix, and is set to $D=2$ for the 2D gridworld tasks and $D=4$ for Atari and the physics environments. \u2028These are quite small, and the fact that they exactly coincide with your assumed sufficient statistics is a bit unfortunate.\u2028 \n\tWhat happens if D is larger? Could you find the optimal D by some means?\n2.\tThe GraphNet makes sense to me, but I wondered why you did not provide $a_t^j$ to $e_t^{(i, j)}$ as well? I could imagine situations where one would need the action to know if an interaction between two slots is required.\n3.\tSimilarly, the fact that the action was directly partitioned per object (except in Atari where it was replicated), seemed slightly odd. Would it still work if it was not directly pre-aligned for the network? I.e. provide $a_t$ as conditioning for the global() module of the GraphNet, and let the network learn which nodes/edges it actually affects.\n4.\tIn your multi-object contrastive loss, how is the mapping between slot k in $z_t$ and $\\tilde{z}_t$ performed? Do you assume that a given object (say the red cube) is placed in the same $k$ slot across different scenes/timesteps?\u2028This may actually be harder to enforce by the network than expected (e.g. with MONet, there is no such \u201cslot stability\u201d, see [1] for a discussion).\n5.\tIt was unclear to me if the \u201cgrid\u201d shown in Figure 3 (b) and 5 is \u201creal\u201d? I.e. are you exactly plotting your $z_t$ embeddings, and they happen to lie precisely along this grid? If yes, I feel this is a slightly stronger result as you currently present, given this means that the latent space has mirrored the transition dynamics in a rather impressive fashion.\n6.\tRelated to that point, I found Figure 3 b) to be slightly too hard to understand and parse. The mapping of the colours of the arrows is not provided, and the correspondence between \u201cwhat 3D object is actually moving where\u201d and \u201cwhich of the coloured circles correspond to which other cubes in the image\u201d is hard to do (especially given the arbitrary rotation). \u2028Could you add arrows/annotations to make this clearer? \u2028Alternatively, presenting this as a sequence might help: e.g. show the sequence of real 3D images, along with the trajectory it traces on the 2D grid.\n7.\tFigure 4 a) was also hard to interpret. Seeing these learnt filters did not tell much, and I felt that you were trying too hard to impose meaning on these, or at least it wasn\u2019t clear to me what to take of them directly. I would have left this in the Appendix. Figure 4 b) on the other hand was great, and I would put more emphasis on it.\n8.\tThere are no details on how the actual test data used to generate Table 1 was created, and what \u201cunseen environment instances\u201d would correspond to. It would be good to add this to the Appendix, and point forward to it at the end of the first paragraph of Section 4.6, as if you are claiming that combinatorial generalization is being tested this should be made explicit. I found Table 1 to be great, complete, and easy to parse.\n9.\tIt would be quite interesting to discuss how your work relates to [1], as the principles and goals are quite similar. \u2028On a similar note, if you wanted to extend your 2D shape environment from a gridworld to a continuous one with more factors of variations, their Spriteworld environment [2] might be a good candidate.\n\n\nReferences:\n[1] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P. Burgess, Alexander Lerchner, \u201cCOBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration\u201d, 2019, https://arxiv.org/abs/1905.09275\n[2] Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, Alexander Lerchner, \u201cSpriteworld: A Flexible, Configurable Reinforcement Learning Environment\u201d, https://github.com/deepmind/spriteworld/ \n\n"}