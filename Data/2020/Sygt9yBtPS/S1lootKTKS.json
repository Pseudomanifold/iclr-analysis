{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# 1. Summary\nThe paper tackles the problem of generating long, paragraph-like, fine-grained captions from describing images. It proposed a text embedding bank (TEB) model to encode a paragraph into a fixed-length representation.   \n\nThe strengths of the paper are:\n* The tackled problem is relevant to the computer vision community\n\nThe rejection decision is supported by the following reasons:\n* Missing motivations: it is not clear how TEB is able to encode long paragraph-like sentences when compared to other standard decoders\n* Limited novelty: it seems that the only contribution is how the paragraph is encoded (TEB) which details are not clear (see point 3 below)\n* Writing need to be improved: Missing motivations, descriptions and details (see point 2 below)\n* Results are not consistent (see point 4 below)\n      \n\n# 2. Clarity and Motivation\nTEB is presented as novel model, however it is not clear how it overcomes issues from previous works on paragraph captioning (Sec. 2.2), i.e., what would be the main contributions with respect to the literature and what are the motivations of the proposed model. The authors try to explain that with the following sentence: \"all of these methods suffer from the fact that only a tiny partial scalar from ...\", which is not clear and very hard to understand without context.\nMoreover, the authors cited work on long-term dependency (Sec. 2.3) without explaining why this is relevant with respect to the current work. How is it related to the proposed method? How existing methods compare with the proposal TEB?\n\nUnfortunately there is not a high-level overview of the proposed method or the different components, and different sections of the paper are not consistent. This makes the paper and method really hard to read and understand. Going deeper into the explanation of the method (Sec. 3), the reader is right away confused by the used terminology. E.g. the paragraph \"The paragraph vector is based on word vectors. A word vector is the concept ...\" is not clear (What are a paragraph vector and word vectors?). The section continues with details about the method which are not very relevant for the proposed TEB model and can be summarised by citing encoder-decoder networks for image captioning, since it is pretty standard. The math is moreover incomplete with several gaps (e.g., Eq. 3: U and h not defined, how is the image encoded?, ...).     \n\nSec. 3.3, which seems to be the most important part of the proposed method, is only a sentence referring to Fig. 1. Although the text in Fig. 1 is verbose, it is eventually impossible to understand how TEB works in practice. To summarize, it would not be possible to implement TEB with the information provided in the paper.\n\n\n# 3. Novelty\nNovelty is not clear since there is not much information/details about how the TEB module is implemented (apart some references in the experiment section) or how this work positions itself compared to other existing techniques. Assuming that TEB is the only main novelty of the paper, it seems that the model is a minor extension of the method proposed by Melas-Kyriazi et al. (2018). \n\n\n# 4. Experimentation\nThe experiments and results are inconclusive and inconsistent:\n* It is not clear why the authors used the transformer as backbone network since the results are significantly lower (Table 1) than Melas-Kyriazi et al. (2018).\n* Adding TEB to the transformer do not have a significant improvement of the results (Table 1, still lower that Melas-Kyriazi et al. (2018)), meaning that the proposed TEB module is not important to have.\n* What it makes the method to surpass Melas-Kyriazi et al. (2018) is to add TEB to the diversity model presented by Melas-Kyriazi et al. (2018). However it is not clear why TEB is having such an improvement on such dataset but not having an improvement when coupled with the transformer net. The authors should have given an explanation or intuition why this is the case."}