{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors consider two problems: Overcomplete dictionary learning (ODL)\nand convolution dictionary learning (CDL).\nDictionary learning learns a matrix factorization of the data\nY = A X\nwhere A is the dictionary and X is the (known to be sparse) code.\nY consists of n rows (sample size) and p columns (dimension).\nIn the overcomplete version A is n x m where m > n, i.e. the number of learned\nfeatures is larger than the sample size.\nThe CDL problem is a special case of the ODL problem where the dictionary\nmatrix is known to consist of convolution filters instead of being unstructured.\n\nThe authors show that under a given set of assumptions local nonconvex\noptimization can be used to find globally relevant solutions.\nThe basic assumptions are:\n(i) unit norm tight frame\n(ii) mu-incoherence\n\t(relates the angles of the columns of a, e.g.\\ if columns are orthogonal,\n\tthey are incoherent / have small mu)\n(ii) stochastic model of the code X that says entries are Gaussian and sparse\n\taccording to a Bernoulli random variable\nThe authors present the idea of maximizing the l^4 norm of A^T q in order to\nfind q as rows of A.\nApparently l^4 norm maximization leads to \"spikiness\" which is exactly\ndesirable under mu-incoherence.\n\nThe authors show (assuming p \\to \\infty) that the optimization nonconvex\nlandscape (constrained to the sphere) does not contain any stationary points\nwithout negative curvature.\nA saddle avoiding optimizer therefore converges to local minimizers from\nrandom initialization.\n\nThe authors also show that the analysis extends to CDL via a preconditioned\ninitializer.\nFinally, they go on to briefly show some experiments that further validate\nthe theory presented in the paper.\n\nOverall, the authors present a rigorous technical analysis using powerful\nmathematical tools for nonconvex optimization (which is relevant to many\nmachine learning problems).\n\nI am recommending to accept based on the high quality of the work.\nBut I am not confident as to the accessibility of the paper to the wide\naudience of ICLR as it is rather technical.\nPerhaps, the complete contribution would be better suited as a journal article.\n\nNotes:\nIt would have been useful to give some more intuition about what \"spikiness\"\nof A^T q is, why spikiness exists under mu-incoherence and why l^4 norm\nmaximization improves spikiness.\n\nI am not sure that the inclusion of the CDL problem is beneficial for a\nconverence paper and would rather have more space allocated to the intuition on\nwhy the method works for ODL.\n"}