{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces few-shot graph classification problem and proposes super-class based graph neural network (GNN) to solve it. Experiments on two datasets demonstrate that the proposed model outperforms a number of baseline methods. Some ablation study and analysis are also provided. Followings are my detail review. \n\nIt is interesting for the authors to introduce few-shot graph classification problem which is meaningful. If I understood correctly, the authors use graph spectral distance to find prototype graph of each class, then employ prototype graph clustering to obtain super-classes, which are further fed to GNN as the joint optimization of super-class and regular class prediction. To me, the novelty is incremental. \n\nThe authors use two new datasets for experiments due to the requirement of numerous class labels. I concerned about performances of different GNN baseline methods in Letter data due to small graph size (with 4.6 nodes in average). The context neighbor information is important for multi-layer GNN. Thus I could not fully judge the effectiveness of proposed model in this data. \n\nIn Table 3, I found performance of proposed model with one super-class is still better than different GNN. I did get the point from this result. Why there is no performance decrease as all have the same super-class label? What is the model performance when removing super-class augmentation? I would like to see more discussion or experiment about this. "}