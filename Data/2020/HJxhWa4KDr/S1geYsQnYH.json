{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview: \nThe paper propose an MMD GAN extension via using Random forest Kernel.  Instead of using Gaussian kernel on the top of the learned embeddings from the discriminator, it combines existing deep forests kernels. The theory of being differentiable is carefully studied (to prove zero measure) and the experiments are well conducted.  \n\n1.  Some important  references are missing.  One very related paper is \n\n* Li et al., Implicit Kernel Learning,  AISTATS 2019. \n\nThat paper is using the same idea to learn to manipulate the random features on the top of the learned embedding.  The main difference between it and the proposed algorithm is they use MLP parameterization instead of the tree-based model.    Also, the deep forest model can be treated as a sparse neural network, does it have more advantage over Li et al., (2019)? given they use simple dense MLP.   Please at least discuss the similarity and difference in the rebuttal and update the draft correspondingly.  I would even encourage the author to empirically compare with it in the camera ready version.  It would be interesting to see which parameterization is better in this space. \n\nThere are also other recent MMD GAN extensions should be cited in the discussion, such as \n* On gradient regularizers for MMD GANs. \n\n2. For the theory part, based on Binkowski (2018), the gradients for the generator parameters should be biased. Could you discuss it with Theorem 2? \n\n3. For most MMD GAN results, one important property in Li et al., (2017),  Arbel et al., (2018) and Li et al., (2019) is weak* topology.  Does the proposed Random Forest MMD GAN also has that property? In Li et al., (2019), they need some condition to ensure that, how's case in the proposed algorithm?  "}