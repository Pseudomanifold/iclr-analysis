{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n*Summary* \nThis paper deals with the global convergence of deep linear ResNets. The author show that under some initialization conditions for the first and the last layer (that are not optimized !) GD and SGD does converge to a global minimum of the min squared error. The closed related work seems to be Bartlett et al. 2019 that study the convergence of GD in the case of linear networks.  \n \n*Decision* \nOn issue for Bartlett et al. 2019 was that they required a condition on the initial suboptimality to be small in order to insure convergence. This work shows that in the case of linear ResNets, with a well chosen initialization, a similar condition holds with high probability. \nI think this paper is interesting for the ICLR community and seems to provide good contributions (like for instance the analysis for SGD). However I have some question that I would like the authors to answer.\n*Questions*\n- In Proposition 3.3 you show an upperbound on $\\sigma_{\\min}(B)$ in order to show in Corollary 3.4  that the condition to apply Theorem 3.1 is true. However, it seems to me that you need a lower bound on  $\\sigma_{\\min}(B)$ to prove that (A.3) is true.\n- To what extent the proof of Theorem 3.1 uses the proof technique of Bartlett et al. 2019 ?\n- With you small enough conditions, are you in the lazy regime described by Chizat, Lenaic, Edouard Oyallon, and Francis Bach. \"On Lazy Training in Differentiable Programming.\" (2019). NeurIPS\n- In Theorem 3.1 What is $e$ ?\n- Could you prove the same result as Theorem  3.1 and 3.6 but with an inequality constraint on the step size? It seems very restrictive to me to ask a stepsize to be exactly equal to a quantity. If you cannot relax you equality constraint into an inequality constraint, can you at least show that your result hold for step size in an interval?\n"}