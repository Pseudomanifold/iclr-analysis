{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the convergence properties of GD and SGD on deep linear resnets. The authors prove that, under certain conditions on the input and output transformations and with zero initialization, GD and SGD converges to global minima. The results derived in this paper show that the condition on the width of a deep linear resnet is less strict (by a factor O(L kappa) where L is the depth and kappa the condition number of the training data) that a network without residual connection. Overall, the paper is well-written and the analysis is fairly standard and easy to follow (I checked most of the theorems except for Lemma A.2 and the results for stochastic gradients). The authors do not clearly contrast their results to prior work (especially Bartlett et al. (2019) and Arora et al. (2019a)) and I\u2019m therefore not convinced the final result brings any new insight. Please address this issue in your rebuttal. I will reconsider my score if the authors can provide a satisfactory answer.\n\nComparison to Du & Hu\nThe authors claim that their bound shows an improvement by a factor of O(kappa L) over the deep linear network (without residual connections) analyzed in Du & Hu.\n1) I don\u2019t think this is explicitly stated in the paper but are the initialization conditions and the assumed distance to the optimum the same in both papers?\n2) These results are obviously worst-case bounds and the analysis used in both papers is different to some extent. Couldn\u2019t you re-derived this result by adapting your own analysis?\n3) Is there any theoretical proof or empirical evidence showing that resnets do indeed scale better w.r.t. to the condition number of the data?\n\nComparison to prior work\nThe authors mention the work of Bartlett et al. (2019) and Arora et al. (2019a) but it is never very clear what the real differences are.\n1) Regarding Bartlett et al. (2019), you say that \u201cTheorem 3.1 can imply the convergence result in Bartlett et al. (2019).\u201d. Does the result of Theorem 3.1 provides a tighter bound in terms of L or kappa (when using the same initialization conditions)? \n2) Arora et al. (2019a): You explain that they showed that \u201cGD converges under substantially weaker conditions\u201d. How much weaker are these conditions? How do they compare to the \u201cmodified identity transformation\u201d that allows you to obtain a global convergence rate. How does your final result compare to Arora et al. (2019a)? The analysis of Arora et al. (2019a) requires a balanced-ness condition, is this substituted by a different condition in your analysis?\n3) Finally, Allen-Zhu et al. (2019) already has some results on deep resnets although their analysis is for a heavily over-parametrized regime. Can you still comment on how their results differ from yours?\n\nA & B are fixed matrices initialized from a Gaussian distribution. How essential is this condition? Assuming for simplicity that A and B are square matrices. Can I not set A=B=identity and still satisfy the condition in Theorem 1? The term on the RHS would be 1 so then the initialization would need to scale as a function of the spectrum of the data matrix X.\n\nResNet vs LinearNet\nWhere does your proof break down for a linear net without residual connections, i.e. where does the proof absolutely need the identify matrices. Looking at the proof, it seems to me, one could change the following in order to still obtain a similar result:\n1) If we consider a network B \\prod_l W_l AX, the proof of proposition 3.3 could be unchanged if the W matrices are initialized to identify instead of zero.\n2) The lower bound on \\sigma^2_min(I + \\tau W_l) would instead be replaced by a lower on \\sigma^2_min(W_l)=\\lambda_min(W_l). Is this where  one can see the benefit of having residual connections?\n\nProposition 3.3\nThe bound on the singular values of the matrices A and B is vacuous for square matrices, in which case I believe the theorem does not hold. Can you comment on this?\n\nExtension stochastic setting\nI only skimmed at the proof but the extension looks fairly straightforward. Can you comment on how difficult this derivation is compared to the deterministic setting?\n\nExtensions\nYou claim \"can potentially provide meaningful insights to the convergence analysis of deep non-linear ResNets.\" although this is not obvious as the landscape of such networks can be very different. Can you elaborate?\n"}