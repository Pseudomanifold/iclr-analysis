{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis work studies the instability problem of DDPG in the setting of a deterministic\nenvironment and sparse reward. This work designed a toy environment to showcase\nthe potential issues of leading to the instability of DDPG. The observations, such as the correlation between the early access of good trajectory leads to more stable performance later, the deadlock of training could be beneficial. \nIt is essential to analyze and understand the intrinsic properties of the classic algorithms, and it would benefit the research community a lot if the empirical study is appropriately designed and conducted. Overall, this paper studied an essential problem\nof the vulnerability of the classic DRL algorithm (DDPG), which should attract more attention and efforts\nfrom the research community. \n\n\nDetailed comments:\nMethodology:\nThe experiments conducted cannot support the conclusions in this paper.\nI can not fully understand the conclusion from the subsection \"Residual failure to converge using different noise processes\". The DDPG agent is finding the reward regularly while it couldn't converge to the 100% optimal performance. In my opinion, this is an observation,\ninstead of giving any useful conclusion. The convergent issues when using the combination of off-policy\nlearning, function approximation, and bootstrapping are known (Sutton and Barto, Chap 11, 2018). \n\nThe increasing of Q value seems natural to me due to the overestimation of Q learning,\neven with the zero reward setting, which is one of the motivations of Double DQN. \n\nSeveral potential solutions are discussed while no empirical evidence or theoretical\njustification is provided, even in the designed 1D-Toy example.\n\nIt would be more convincing that the conclusions can be validated on more challenging\ntasks such as regular continuous action benchmarks (mujoco, etc.)\n\nWriting:\nThe presentation of this work is not ready for publication, given its current form. \nWhat is the definition of reward function? The formula as shown in Eq 4e is not clear. \nWhat is the optimal performance of 1D-TOY example? "}