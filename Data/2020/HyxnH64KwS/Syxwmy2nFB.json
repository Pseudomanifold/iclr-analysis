{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview: This paper describes a shortfall with the DDPG algorithm on a continuous state action space with sparse rewards. To first prove the existence of this shortfall, the authors demonstrate its theoretical possibility by reviewing the behavior of DDPG actor critic equations and the \u201ctwo-regimes\u201d proofs in the appendices. They then demonstrate the occurrence of the critic being updated faster than the actor, leading to a sub-optimal convergence from which the model can never recover. In this demonstration, they use a very simple environment they created, \u201c1D-Toy\u201d. The 1D-Toy environment is a one-dimensional, discrete-time, continuous state and action problem. Moving to the left at all in 1D-Toy results in a reward and episode end. Episode length was set at 50, as the agent could move to the right forever and never stop the episode. The authors demonstrate how the failure of the agent to obtain 100% success in this simple environment was, in fact, due to the phenomenon mentioned earlier. If the agent managed to obtain a reward very early on in training, it was highly likely the agent would converge on an optimal solution. If not, the actor would drift to a state were it no longer updates, and the critic would similarly no longer update either, resulting in a deadlock and suboptimal policy. The authors then generalize their findings using a helpful figure (Figure 7) which describes the cyclical nature of the phenomenon and how it can happen in any environment. Finally, the authors mention potential solutions to prevent the training failure from occurring, such as avoiding sparse rewards, replacing the critic update to avoid loss, etc.\n\nContributions: the discovery and review of a potential training failure for DDPG due to the nature of the critic update being reliant on the policy, and the deterministic policy gradient update\n\nQuestions and Comments:\nI believe that this work is relevant to ICLR and to the field. The paper is well-written, the theory is sound, and the experiment is sufficient to describe the stated deadlock situation that DDPG can contain during training. I believe this paper should be accepted because of these reasons. I have a few comments/questions for the authors which I have written below.\n\nI\u2019m interested to see how likely this deadlock situation is on more complex environments. Did you run experiments on common benchmarks and analyze them?\nYou mention several potential solutions, one of them being the avoidance of sparse rewards. Of course this is problem-dependent which you stated yourself. The other two involve replacing two of the update functions. In the policy-based critic update, both of the mentioned solutions in this section have drawbacks mentioned. Would using a stochastic policy gradient update affect the networks ability to learn successfully in more complex environments? Would this make training less stable?\n\nI\u2019m curious to see in which directions you see this work being extended. You briefly mention in the conclusion that there would be more formal studies: how do you imagine these being?\n"}