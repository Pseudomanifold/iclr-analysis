{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n------------------------\n\nSummary:\n\nThis paper derives a new generalization bound for CNNs that includes a term, NN-mass, based on PAC-Bayesian setup and Network Science. Notably, NN-mass can be calculated from a CNN architecture *without training*. It plots some graphs showing that log(NN-mass) correlates strongly with the test accuracy of CNNs of varying depth. It also proposes new architectures for the CIFAR-10 dataset, and compares its results against DARTS.\n\n------------------------\n\nDecision: Reject.\n\n1) Theory contains a fatal flaw.\n\nMcAllester's Bound only applies to Gibbs classifiers -- every time a prediction is made, a new classifier is randomly sampled from the posterior distribution. (See: The Proof of McAllester's PAC-Bayesian Theorem, M. Seegar 2002.) In other words, Theorem 1 does not bound the generalization gap for individual classifiers drawn from Q; it only bounds the generalization gap for the expected accuracy of a Gibbs classifier defined by the distribution Q. I didn't check the proof for Theorem 2, but even if the steps are correct, Corollary 1 interprets f_Q^L and f_Q^S as individual models drawn from Q, which is incorrect.\n\n2) Experiments do not directly support the generalization bound in Corollary 1.\n\nCorollary 1 is not about the relationship between log(NN-mass) and test accuracy; it is about the relationship between (generalization gap of deep model) - (generalization gap of shallow model) and the square root term. Unfortunately, data augmentation complicates this formula. For a proper evaluation, the models should not be trained with cutout. Besides, figure 10 consists of trend lines with just 20 data points, which is not very convincing. It would be much more convincing if this paper had a Q-Q plot of generalization error against NN-mass over a dataset of ~100 data points.\n\n3) Empirical study does not verify whether different optimization and regularization settings would affect the results.\n\nRecent advancements in the performance of image classifiers came not only from architectures; it also comes from innovations in optimization (e.g., batch norm, learning rate scheduling) and regularization (e.g., cutout, drop path). A training-free architecture search procedure thus faces this concern: if we change these training hyperparameters, will the results still hold? Thus, experiments to confirm Corollary 1 should be repeated with/without cutout, with/without batch-norm, and with different learning rate schedules.\n\n------------------------\n\nNovelty: Applying Network science to the study of generalization of untrained CNNs seems novel to me.\n\n------------------------\n\nClarifications:\n- Would this theory predict that DenseNets with a large number of channels have superior generalization performance compared to ResNets? If so, why are current state-of-the-art ImageNet models more similar to ResNet than DenseNet?\n\n------------------------\n\nOther issues:\n- Why does Table 1 choose DARTS, an architecture for which the NN-mass cannot be computed? Why not compare against DenseNet?\n\n------------------------\n\nMinor issues:\n- page 1, \"In contrast, traditional wisdom suggests that models should overfit when the number of parameters is much larger than the dataset size (which is true in practice).\" It should be \"(which is *not* true in practice)\"?\n- page 2, s/Densenet/DenseNet\n- page 6, Remark 1: \"Theorem 2 guarantees that the test error should reduce as NN-Mass increases\". This overstates what can be deduced from a PAC upper bound; an upper bound alone can *suggest* a relationship, but it cannot guarantee anything because we don't know if the bound is looser at lower values of NN-Mass.\n- page 6, Corollary 1: f^S_Q and m_S correspond to the shallower network, and f^L_Q and m_L correspond to the deeper network. Why not use D instead of L to label the deeper network?\n\n------------------------"}