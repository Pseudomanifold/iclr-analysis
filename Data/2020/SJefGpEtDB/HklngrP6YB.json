{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose approaches to accelerate deep neural network training with mixed-precision arithmetic.\nObserved that relying purely on half-precision arithmetic results in lower accuracy, the authors developed a method \nto dynamically switch between mixed-precision arithmetic (MP) and half-precision arithmetic (BF16 FMA).\nEmpirical results show that the dynamic approach can achieve similar accuracy as MP and FP32 algorithms.\n\nAlthough this paper shows the possibility to accelerate DNN training without great loss in performance, there are many issues with the paper itself. First, the title is ambiguous. The dynamic approach could mean a lot of things while training \nDNNs and one cannot tell what the paper is about simply relying on the title.\nAlso, the dynamic algorithm itself is not well presented. For example, how to choose the hyperparameters? What is the\noverhead to switch between MP and BF16FMA? \n\nApart from the algorithm itself, I also have questions regarding the experimental results.\nIs there a reason why the performance of the half-precision arithmetic \nvaries across different neural networks (Inception > Resnet)?\nSpecifically, what is the key factor that influences the sensitivity of a neural network towards precision?\n\nOverall I think this paper should be improved in its experiments and presentation.\n\n"}