{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The author(s) propose to accelerate the training of deep neural networks while also maintain the performance of the trained model by switching between fully half-precision computation and mixed-precision computation. Compared to the commonly-used mixed-precision training strategy, the proposed method can accelerate the training speed. Besides, on an image classification task, models trained by the proposed method achieve comparable performance with those trained by mix-precision training or full-precision training.\n\nStrength:\n1.\tSection 3 provides useful information for the workloads of deep neural networks.\n\nWeakness:\n1.\tThe overall idea is not novel. The proposed method simply switches between two existing training strategies, i.e., the mixed-precision training and half-precision training. The claim that \"this paper is the first in demonstrating that  half-precision can be used for a very large portion of DNNs training and still reach state-of-the-art accuracy\" may not correct, in fact, Nvidia's apex has already supported using mixed-precision or entirely half-precision to train DNNs, and there is no clear evidence that the proposed method is better than theirs due to the lack of experiments on more tasks and datasets.\n\n\n2.\tFrom Table 1, the Dynamic strategy outperforms BF16 in terms of classification accuracy. However, from the experiments, it\u2019s unable to tell that the gains actually come from this Dynamic strategy. Maybe similar gains can be obtained once the same amount of MP iterations are executed at any period of the training process. For example, consider a simpler strategy: first train the model with BF16 for K% of the total training iterations, then for the last (100-K)% iterations, train it with MP. K can be tuned so that the proportion of BF16FMA is close to those in Table 1. Models trained with this strategy might achieve similar performance with the proposed Dynamic strategy. \n\n\n3.\tIt\u2019s hard to apply the proposed method in real applications, since BF16 is only supported by very few kinds of hardware.\n\n\n4.\tTo demonstrate the effectiveness of the proposed method more clearly, it could be better to provide the exact proportion of reductions in terms of memory/computation/bandwidth in the experiments. \n\n\n5.\tFrom Table 1, there still exists a large performance gap in terms of accuracy (1.56% for ResNet-50) between the model trained by the proposed method and the model trained by state-of-the-art MP.\n\n\n6.\tThe organization of this paper can be improved. For example, Section 5.2 spends too much space introducing the emulation of BF16, which I think is not very relevant to the topic of this paper. And Figure 3 takes too much space.\n"}