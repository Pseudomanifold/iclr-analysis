{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of learning from medical data that is separated both horiontally (across different practices and centers) and vertically (by data type). The contribution is a \"confederated\" machine learning method that learns across these divides. The particular application considered here as means of illustration is that of fall prediction in the elderly. Specifically the authors investigate an ML approach to risk-stratifying elderly patients with respect their likelihood of falling in the next two years.\n\nThe basic challenge addressed here is learning in the setting in which different data elements are available only at specific sites, and it is assumed that they do not share data. In addition, it is assumed there are multiple distinct sites that have data corresponding to the respective elementes. However, it is assumed that labels (target vectors) are shared across all sites. This setup is simulated using available data. A simple distributed training scheme is outlined. \n\nThis is a potentially important problem worthy of study. I some major concerns with the present work, however. First, I do not think that ICLR is really the best venue for this work. The machine learning component of this is quite straightforward; basically SGD is performed iteratively on parameters associated with the data types \"owned\" by the respective (simulated) sites. Updates are then averaged over these parameter subsets. This is perfectly reasonable, but not terribly novel. The presentation of this is also much longer than it needs to be for the ICLR audience. I think this paper, in its current form, would be better suited for an audience more interested in clinical applications specifically (and I say this as someone quite appreciative of work on applied ML; it's just that the audience here will be more interested in methodological innovations.)\n\nWith respect to clinical utility: Do we really need ML to tell us about risk of falls? I mean, if we were to ask the MD who had seen these patients to perform a simple stratification (perhaps on an ordinal scale), would they not likely be able to do so reasonably well? The authors mention something like this, discussing the 'clinical screening process' which involves asking about prior falls. This seems like a really strong baseline. The authors argue that this is time-consuming, \n\nIn any case, is AUCPR an appropriate or useful metric here? In practice one would need to pick a threshold on which to act; perhaps a simulation that investigated doing so would provide a more meaningful evaluation. Although again a strong baseline here would probably be to ask physicians to risk stratify patients for interventions direclty (I appreciate that this would be a non-trivial experiment to run, but still).\n\nI also have a question regarding the simulation. I *think* the authors have randomly assigned patients to the respective simulated sites; is that right? This seems problematic because in practice patients would not be IID distributed in this way; sites would have their own patient populations which would affect the losses. This should be somehow taken into account in the simulation. \n\n\nOther comments\n---\n- I think I am missing something in the notation here. $Y_{si}$ is a 'binary label' but seems to vary across 'states' for an individual, is that right? Shouldn't this be constant for an individual? The paper states below that \"The output of the classifier is a binary variable indicating whether the beneficiary had a fall during the follow up period.\"\n\n- Labels were derived from ICD codes; was there any effort to spot check these? I am always a bit concerned about deriving labels from ICD and trusting them. \n\n- As far as I understand from 2.1, the authors have not included features extracted from notes in the patient history; is that right? Why not?\n\nSmaller issues\n--- \n- I would strongly suggest numbering your equations. Also, suggest using \\text while in mathmode for superscripts like `diag'. \n\n- \"Step 1..\" --> \"Step 1.\" (p3)\"\n\n- \"The parameter \u0398of f is randomly initialized\" --> missing space before \"of\"\n\n- On page 4: L(Xdiag, Xmed, Xlab, \u0398) is written incorrectly.\n\n- page 4: \"Tstands\"  missing space.\n\n- page 5: \" fallst.\" --> \"falls.\"\n\n- Appendix Tables 2 and 3 both contain the typo \"varaible\" (should be \"variable\")\n\n- In Appendex Table 2, I suggest reporting results with a consistent amount of precision, e.g., 0.002 --> 0.0020 here."}