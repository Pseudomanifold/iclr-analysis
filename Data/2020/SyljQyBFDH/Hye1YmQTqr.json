{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes a new type of energy-based models, a class of non-normalized generative models that relies on an energy function to retrieve patterns that correspond to its minima. The goal that is tackled by the authors is to implement an associative memory system, i.e. a mechanism that is able to retrieve any one of a set of patterns, given a distorted copy of these patterns. This task is traditionally carried out using attractor neural networks like the Hopfield model, a recurrent neural network model endowed with a learning rule that allows it to quickly embed a given set of patterns in its weight matrix such that the patterns become stable fix points of its dynamics. As the authors point out though, models like the Hopfield model are limited in their capacity to assimilate attractor patterns and in term of their expressiveness. On the other hand, more complex models based on deep architectures trained with gradient descent are slow at updating their weights to create new attractors.\nThe authors propose a new method to make up for the weaknesses of these two approaches. Their method is based on meta-learning, and in short consists in meta-training an energy function parametrized as a neural network such that  executing a write dynamics on the weights results in a model whose read dynamics (a gradient descent on the energy function) is able to denoise distorted inputs and retrieve the original ones. In practice, the write dynamics is obtained as a gradient descent procedure on a writing loss (which is itself dependent on the energy function) as a function of the weights. The meta-learning procedure minimizes the discrepancy between the original patterns and the retrieved ones by optimizing end-to-end the learning schedule parameters and initial conditions of the weights, analogously to gradient-based meta-learning methods like MAML.\nThe authors then carry out a series of experiments to check that their model is competitive with Memory-Augmented Neural Network (MANN) and Memory Networks (MemNets) in retrieving samples from Omniglot, CIFAR and ImageNet, in terms of retrieving abilities for a given memory size. In the supplementary material section they in addition compare their model's performance against the Hopfield model and recurrent networks on the classical toy task of retrieving random binary patterns, also with good results for the new model.\n\nDecision:\nThis paper is very clearly and compactly written. The idea of training an energy-based model through gradient-based meta-learning seems novel and innovative. \nOne thing that the the paper is arguably missing, is a convincing motivation section for focusing on energy-based models. The panorama of generative models has radically changed since attractor neural networks and energy-based models were first introduced. At the time powerful methods like variational autoencoders, normalizing flows and GANs didn't exist. But nowadays, one could arguably expect that energy-based models should be contextualized and motivated in the perspective of comparing them with these new breeds of deep generative models. I am absolutely not suggesting that the authors should providing experimental comparisons between their models and GAN or VAE, but simply that they compare them to their style of generative modeling in terms of advantages, disadvantages, use cases, and potential for applications."}