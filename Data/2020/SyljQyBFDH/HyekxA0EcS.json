{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "My understanding of eq 1-5 is that the algorithm finds an energy landscape (by modifying \\theta) for each dataset (task) such that in this landscape, the inputs from the distribution are reachable by truncated gradient-descent initiating at a query (distorted input with respect to some distortion model).\n\n1- The connection to meta-learning is unclear in your experiments. Can you elaborate on that? \n\n2- The expectation in eq 5 is over different input patterns, which I assume that a set of input patterns belong to a task. What is that you write in memory?  For each experiment, what are the different input patterns (tasks) that you have written in the memory?\n\n3- What is the \\theta that is feed to the read function at the test time? \n\n4- Are you testing on the tasks that you already trained on?\n\n5- How this approach generalizes to unseen (or relatively close) task? \n\n6- Can it recover any query that is not constructed with respect to the distortion model that is trained on? or what happens if the distorted image at test times comes from a different distortion model? (image blocking, for example)\n\n7- How many distorted samples are used for training?\n\n\n8- For the chosen tasks, I am curious to see the experimental comparison to deep image prior (Ulyanov, 2018). Deep image prior would be very similar to the read operator (although the gradient descent is over the parameter of model) without having write operations when you define the energy as MSE.\n\n\n\nTypos and writing style:\n-- The expectation in eq 2 should independently show what the expectation is taken with respect to. \n-- input patters -> input patterns\n-- figure 3 -> Figure 3\n-- section 1 -> Section 1\n-- 4 random images -> four random images \n-- the Figure 5b -> Figure 5b\n-- Models such as (Ba et al., 2016; Miconi et al., 2018) enable ->  Models such as Ba et al. (2016) and Miconi et al. (2018) enable"}