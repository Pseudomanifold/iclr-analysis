{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method to learn language-independent cross-lingual contextual representations by mapping the representations of a monolingual model in one language to the representations of a monolingual model in another language. The proposed approach consists of three steps: 1. A transformation is learned that minimizes the distance between the contextual word representations of the two models of a sentence and its translation in a parallel corpus. 2. Each sentence is summarized as a sequence of key-point tokens based on phrase alignment. 3. The contextual word vectors are reordered based on an order prediction model.\nThe authors perform experiments on intrinsic tasks and on XNLI, mapping between an English and a Chinese BERT model. They outperform multilingual BERT (mBERT) on the latter.\n\nOverall, the approach is novel, but the steps seem overly complicated. The extrinsic evaluation is the weakest point of the paper. Because of this, I tend to a Weak Reject. I would be willing to increase my score if additional languages are added to the evaluation and if the steps are better motivated and compared to simpler alternatives.\n\nThe high-level steps of the approach (transform, align, reorder) make sense. They seem to be inspired by classical phrase-based MT pipelines, but this connection is not made clear. In particular, some of the steps seem unnecessarily complicated and I am wondering whether the authors tried or compared to simpler alternatives. As a parallel corpus is used in the first step, word alignment could be automatically obtained by FastAlign without the use of attention matching or using a phrase table that is learned in an unsupervised way as in recent work in NMT (https://arxiv.org/pdf/1804.07755.pdf). For the second step, embeddings of aligned phrases could be averaged or the head of a phrase could be used instead of predicting key-points with a separate network.\n\nThe data requirements between the steps seem somewhat inconsistent. A parallel corpus is used in the first step, but explicitly not used in the second step. While I agree that language-independent cross-lingual representations should not use a parallel corpus, it would have been good if the first step could have also been performed without one (or with a smaller size).\n\nThe extrinsic evaluation of the paper could be improved. The accuracy on each intrinsic step is evaluated. Without any reference or comparison, I found it hard to tell how good these numbers are so this evaluation did not add much for me. The main extrinsic evaluation of the paper is on XNLI but only employs an English and a Chinese model. When training cross-lingual models, the aim is to train approaches that not only work for one or two languages but for many. In light of this, I find it hard to tell from the results on a single language pair how well the approach will generalize to other languages, particularly as mBERT's performance on zh XNLI is comparatively weak (https://arxiv.org/pdf/1901.07291.pdf). There are other publicly available BERT models such as for German (https://deepset.ai/german-bert) on which the approach could be tried. If compute is a concern, then the approach could be applied to ELMo representations. This would also enable a comparison to Schuster et al. (2019; https://arxiv.org/abs/1902.09492).\n\nFinally, the data requirement of 250k parallel sentences is prohibitive for many language pairs where this approach would be valuable. It would be good to see a chart on how model performance develops with the number of parallel sentences to see if a smaller number of parallel sentences would be viable in practice."}