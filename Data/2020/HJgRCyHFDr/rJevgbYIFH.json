{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This work proposes an algorithm for handling the weight-sharing neural architecture search problem. It also derives generalization bound for this problem.\n\nThe reviewer has several concerns:\n\n1) the SBMD and ASCA algorithms are existing generic algorithms. The analysis in this work also looks very generic. There is a sense of disconnection with the considered training problems. The reviewer would like to see more discussions on how to connect the algorithms with specific NAS problems. For example, what is the beta parameter when training a NAS problem?\n\n2) The convergence rate improvement brought by using mirror descent has been long known. It is not easy to see what is the contribution of this work.\n\n3) The generalization part seems to be meaningful. But it may be much stronger if the NAS problem can also have a theoretical bound. It is less appealing to only discuss cases with strongly convex objectives."}