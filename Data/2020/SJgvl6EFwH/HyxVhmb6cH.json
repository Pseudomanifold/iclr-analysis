{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper suggests a methodology of partitioning latent code to a set of class-specific codes, to solve the inefficiency from the large size of the latent code in conditional normalizing flow. This inefficiency problem has been a weak point in the existing conditional normalizing flow models. Also, this work addresses the increase of the number of function evaluations which is caused by code partition, by optimizing the error tolerance with a reinforcement learning approach.\n\nThis paper has a novel contribution, outperforms the baseline (CCNF: CNF (Chen et al.) + GMM / auxiliary classifier (Kingma & Dhariwal, 2018)) on several evaluation metrics. I agree to accept this paper, but I vote for \u2018weak accept\u2019 because of the following weaknesses:\n\n1. The (three) contributions described by the authors seem to be somewhat exaggerated.\n- For the second contribution, the authors\u2019 way is quite similar to Wang et al. (SkipNet). For both studies, the purpose is the efficiency of certain neural architectures and training scheme is hybrid reinforcement learning with REINFORCE (but with different reward design).\n- For the last contribution, I think it is a minor contribution comparing to two first bullets and overlapping with the second one.\n\n2. There is a lack of explanation to support the efficiency of the proposed model.\n- The authors claim that InfoCNF needs fewer parameters than the baseline. Then, why didn't you show the actual number of parameters? The only line that I found was \u201c... InfoCNF requires 4% less parameters than CCNF. (Section 2)\u201d.\n- Also, it would be better if there were a direct computation and comparison between the size of InfoCNF and CCNF.\n- Finally, is there any constraint on the length of merged latent code z? Since InfoCNF is also an invertible model, it should have the same size as the input, but I cannot find descriptions about it.\n\n3. It is skeptical that automatic tuning of error tolerance has achieved its original purpose.\n- For me, it is questionable whether automatic tuning has achieved its original purpose: reducing NFEs for better speed (and performance).\n- In figure 3c and 3f, we can find NFE of InfoCNF learned tolerances eventually exceeds the NFE of InfoCNF fixed tolerance. It looks like learning tolerance increases NFE in large epochs, and the timing seems to depend on the batch size. If so, the batch size is a really important hyper-parameter in this framework, how can we determine the appropriate size?\n- In section 4.4 (Automatic Tuning vs. Manual Tuning), the authors state that \u201cour automatic approach learns the tolerances which outperform the manually-tuned ones in both classification and density estimation while being only slightly worse in term of NFEs.\u201d. But as abstract says, is reducing NFEs the original goal of automatic tuning?\n- Lastly, how can we know/confirm \u201cour automatic approach via reinforcement learning requires much less time and computational budget\u201d. I cannot see any explanation about this claim.\n\n4. Minor things.\n- What is the stage stated in Figure 1?\n- The abbreviation of the CCNF first appears in section 1 but its full name first appears in section 2.2.\n- In figure 3a and 3b (and table 1), why is test NLLs of CCNF lower than InfoCNF\u2019s where test error of CCNF is larger than InfoCNF\u2019s with high margin? Is there any possible explanation?"}