{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a weak supervision method to obtain labels from functions that are easily programmable, and propose to use this for learning policies that can be \"calibrated\" for specific style. The paper demonstrates some experiments on a basketball environment and a halfcheetah environment, showing that the agent will perform according to corresponding styles.\n\nMy main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels. In this case, we are not concerned with the latent variables whatsoever, therefore it seems that the CTVAE baselines are overkill for the task (learning latent variables that are not actually needed). Maybe more interesting baselines is to see how the two terms in (8) affect self-consistency performance, and not consider any methods that use unsupervised latent variables? \n\nMinor questions:\n\t- The method's name, CTVAE-style is a bit confusing, since the policy does not depend on any latent variable z? At least from how the policy is described pi(\\cdot |y) does not depend on unsupervised latent variables z.\n\t- Table 4, KL and NLL results do not seem to match? I wonder if the basketball kl should be multiplied by 10 and the cheetah ctval-style NLL is a typo?\n        - Is it possible to extend this to continuous labels? This seems technically viable but unclear empirically.\n"}