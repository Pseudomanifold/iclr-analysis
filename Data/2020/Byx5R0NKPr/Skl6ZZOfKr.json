{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose learning generative models for long-term sequences that can take a style argument and generate trajectories in that style. The motivating example used is reconstructing expert trajectories from basketball games - trajectories can be sampled based on whether we want fast movement (SPEED), or whether they end close to the basket (DESTINATION).\n\nIt follows a data programming paradigm. We do not directly have style labels, so to get around this, we define labeling functions, which take a trajectory and output some boolean value. (Real valued labels are allowed but are not considered in this work). We assume the desired style is defined by some combination of labels, and that we know this combination (i.e. a fast trajectory to the basket should have the \"speed above threshold c\" label and \"final location close to basket\" label, which we have labeling functions for.)\n\nOnce we have this labeling function, we learn a trajectory VAE with a few loss functions. Standard behavioral cloning loss, and a style consistency loss that encourages labels of the generated trajectory to match labels of the target style. To make the optimization fully differentiable, we approximate non-differentiable labels with a learned labeling function (i.e. learn a classifier and then use classifier probabilities as the label), and learn a model of the environment to allow backprops through the rolled-out dynamics model for the entire trajectory. It's argued that learning a model helps with credit assignment, from an RL perspective credit assignment seems like the wrong word. Nothing about learning the model makes it easier to assign credit, the main gain it gives is making the problem differentiable.\n\nOn the basketball dataset, and a dataset of episode collected from the HalfCheetah MuJoCo environment, they demonstrate better style consistency. I had trouble finding an exact definition of style-consistency here - I assume it's defined as \"given style c, how often does a trajectory sampled from pi(a|s,c) satisfy style c\". It would be good to define this.\n\nI would appreciate discussion on why style consistency classification is better than the mutual information baseline where MI between style labels and trajectory is maximized, it feels like they should be equivalent.\n\nOverall I think this is a reasonable paper. The domains considered are fairly simple, but the idea seems sounds and the results seem good. I am concerned at all the requirements though - the method assumes the dynamics of the environment are learnable, and that we can define labeling functions that cover the space of styles we want, both of which seem like strong requirements."}