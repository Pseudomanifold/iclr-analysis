{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an unsupervised method for learning node embeddings of directed graphs into statistical manifolds. Each node in the graph is mapped to a distribution in the space of k-variate power distributions, endowed with the KL divergence as asymetric similarity. The authors propose an optimization method based on a regularized KL divergence objective. They also propose an approximation of this objective based on finite neighborhoods, with a separate treatment of infinite distances based on a topological sorting. They also introduce a natural gradient correction to the gradient descent algorithm in this setting. They validate the fitness of their approach by showing that asymmetric distances in the graph translate into correlated asymetric distances between the node embeddings for various datasets.\n\nThe paper appears to bring a valuable method for directed graph embedding. However, a more thorough experimental study would help validating the improvements and hyperparameter setting of the method. Moreover, I suggest that the authors work on an improved version of the manuscript, as it contains many grammatical and spelling mistakes, some of which listed under.\n\n* Experimental questions *\n1. The hyperparameters and training time seems to have been set using the evaluation metric on the datasets. Could the authors provide a more principled validation approach to their experiments, e.g. using cross-validation?\n2. While the focus on preserving asymetric similarities is understandable, it would be interesting to know how the method performs for conventional evaluation tasks of network embedding, and to show that the gain in correlation can translate into gains for the end task in practice.\n\n* Spelling / grammar / layout *\n- Title: \u201cOn the geometry and learning low-dimensional embeddings\u2026\u201d does not make sense.\n- abstract: \u201cis better preserving the global geodesic\u201d\n- Fig 1: The sigma ellipse*s*\n- 2.1 Intuition.: \u201ccolor codded\u201d\n- Figure 2: \u201cwhich was reflected the highest mutual information\u201d\n- Figure 2 should visually identify the rows and the columns, rather than relying on the caption.\n"}