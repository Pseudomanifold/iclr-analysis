{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nIn this paper, the authors made a study on large-batch training for the BERT, and successfully trained a BERT model in 76 minutes. The results look quite exciting, however, after looking into the details of the paper, I would say that this is just a kind of RED AI \u2013 the results were mostly achieved by putting together a huge number of TPUs, without necessary technical innovation and fundamental contributions.\n\n1)\tThe work used 1024 TPUs to achieve 76-min training. If we compare this with the original BERT training (16 TPU for 81 hours), there is no algorithmic speed up at all (only system speedup). Not to mention that making a single BERT training faster by using more resources does not seem to be a big thing \u2013 one can do multiple BERT training experiments in parallel or in pipeline, which will correspond to similar innovation speed.\n\n2)\tThe technical highlight of the paper seems to be LAMB, which enables very large batch training for BERT. However, technically it is just a direct use of ADAM in the optimization process. No specific customization for BERT.\n\n3)\tThe theoretical analysis is not very impressive, and to certain degree, is not helpful. The theory just says that in certain conditions, both LARS and LAMB converge fasters than SGD. However, LAMB ha no advantage over LARS at all, which cannot well explain the experimental observations. Furthermore, when \\beta_2 > 0, the convergence rate of LAMB is even slower than LARS, which delivers some contradictory message. As we know, \\beta_2>0 is very important, otherwise the optimization algorithm will not be ADAM at all.\n\nOverall speaking, I do not believe such work would have a big contribution to the research community \u2013 there is no sufficient theoretical or algorithmic contributions and no deep insights, there are only brute-force approach based on huge computational resources. \n"}