{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The paper proposes to generate video frames from a single blurred image in an end-to-end manner. The neural network architecture comprises a feature transformer network and a spatial transformer network. The optimization includes a multi-scale photometric loss with a transformation consistency loss that ensures efficient training of neural network. The proposed approach is evaluated using two datasets: (1). Camera rotation blurs generated from the panorama scene (Xiao & Torralba); (2). High-Speed videos. \n\nStrengths: \n\n+ This work is technically sound, has an intuitive formulation, and nicely contrasted with prior work.\n\n+ demonstration of superior qualitative and quantitative results. \n\n\nConcerns: \n\n- This work is dealing with an ill-posed problem, and that proposed formulation can only work for a specific setup, and not in-the-wild videos. Here is a detailed reasoning for my concerns: \n\n(i). One of the most potent applications of this work is temporal super-resolution (see Mahajan et al. SIGGRAPH 2009). Is it possible to use the model trained on high-speed video dataset and be able to do temporal super-resolution for the kind of videos used in Mahajan et al.? More specifically, given a 30 fps video -> generate a 240 fps by inserting seven frames in between. \n\n(ii). The current approach is seemingly working on the panorama dataset because it is a static content. The texture around the surroundings do not change too rapidly in panoramas. It is, therefore, able to learn how to move around pixels. It is not clear to me how arbitrary motion can be handled using the proposed approach.\n\n(iii). The results demonstrated on high-speed videos are not convincing. The demonstration in Figure 4 and Figure 5 further reinforces confidence in my concerns that the proposed approach is learning to move laterally by doing a sort of texture-inpainting as it moves on either side.\n\n(iv). It is not clear to me how can the proposed approach learn about human motion in general? The plausibilities are too large that it is not just possible to show one set of outputs from a single blurred image. \n\n(v). Even more challenging is to learn about the generalization of scenario when both object and camera moves arbitrarily. How is it that a single blurred image provides suitable information that can enable reconstructing video frames?\n\n"}