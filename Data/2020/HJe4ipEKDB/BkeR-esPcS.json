{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "### Summary\nThis paper proposes a new framework to generate video frames from a single motion-blurred image. While prior work [1,2] has dealt with this problem, they can suffer from error compounding as they they first predict middle frame and then use it for other frames.\nTo this end, key insight in this paper is to learn a separate spatial transformer network to produce each output frame. This removes the dependence on middle frame reconstruction quality, but assumes a fixed number of frames due to lack of recurrence. Experiments show better results than [1], but lack any comparison with [2].\n\u200b\n\u200b\n### Strengths\n- The synthetic method to generate rotational blur from panaromic images can be reusable for future work in this domain.\n- The network architecture look intuitive for a fixed-size video prediction. The proposed losses also make sense at a high-level, and are shown to be helpful through ablation studies.\n- Ablations for various components of the proposed method are well done - in terms of network architecture and loss components proposed.\n\u200b\n### Weaknesses\n- **Inconsistent and hard to follow math in the paper**: e.g. k is used for temporal dimension in Fig. 2, and scale in Eq. 3; w_l in Eq. 3 for multi-scale photometric loss is not described at all. Similarly the dual subscripts in Eq. 3 are not well-defined at all, making an otherwise easy equation very hard to understand. The meaning of predicting different \"scales\" is also not clearly defined anywhere and is left to user to interpret - they are used interchangably with multi-level features (e.g., in Fig. 2) making things very confusing.\n\u200b\n- **Lack of Pairwise terms in losses**: It seems more intuitive to have pairwise losses for transformation consistency loss (i.e. temporal consistency across *all* scales) and penalty term (i.e. penalizing similar frames across all generated frames). Especially for the penalty term, can the authors explain the reason for choosing penalty in symmetric manner? It seems like an arbitrary design choice. It is also unclear how temporal consistency loss is making the model design symmetric around middle frame (as mentioned in first line on Page 7)?\n\u200b\n- **Lack of comparison with [2]**: This is the reviewer's primary complaint with the paper. The authors mention that [2] have no open source code, so they did not compare any results. However, the high-speed video dataset used is exactly the same in [2], and they provide many metrics in their paper. In fact, [2] also show many improvements over [1], which is the main comparison in this paper. So without comparing with [2], it is very hard to assume that the problem this paper tries to solve even exists - whether recurrent dependence on middle frame is bad for video restoration. Therefore, the reviewer recommends to compare with this more recent paper [2]'s results in the form it is present in that paper.\n\u200b\n- **Using 10 pages**: While the reviewer appreciates the ablation studies and qualitative results mentioned in the paper, there are many architectural and data-processing details that would be more suitable for Appendix section, hence reducing the main paper length to 8 or 9 pages for the interest of readers.\n\u200b\n\u200b\n\u200b\n#### Minor and nits:\n- From abstract and introduction, it seems that restoration of video frames from single blurred image is a new problem proposed by this paper. Correct word choice should be used to mention the precise way this work differs from prior work.  \n- Page 3, \"Later\" is misspelled.\n\u200b\n\u200b\n### References\n[1] Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to extract a video sequence from a single motion-blurred image. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.  \n[2] Kuldeep Purohit, Anshul Shah, and AN Rajagopalan. Bringing alive blurred moments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6830\u20136839, 2019."}