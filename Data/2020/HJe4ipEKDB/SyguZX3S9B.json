{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "OVERVIEW:\nThe authors present a framework to generate video frames from a single motion-blurred image. Their framework is based on an encoder-decoder architecture with Spatial Transformer and Image Warping layers. A main difference between their work and prior work is that they reduce dependency on predicted central frame and thus have lesser error accumulation which prior works have. They also predict the non-central frames using same decoder but with appropriate global and local transforms. They train their network using a combination of (i) photometric loss (predicted image is close to ground-truth), (ii) transformation consistency loss (transformations that lead to nearby frames are not very different) and (iii) penalty term (different predicted frames have different content). They demonstrate results on two datasets: (1) Synthetic dataset with rotation blur and (2) High-speed video dataset containing dynamic blur. \n\nCOMMENTS:\n1. I like the proposed approach of modeling motion via the STN and LW layers. However, I wonder if both are required simultaneously. Any arbitrary motion could in principle be modeled using the LW layers that estimate the motion flow with the caveat that predicted frames are close (to approximate a complex motion with linear estimates in delta time steps). Why is the STN then important and useful? Can it be avoided? Any empirical evidence to back up claim?\n2. Table 1 seems to indicate that you get better center frame prediction also compared to prior work. Would it make sense to run their algorithm with your predicted center frame and add it as a baseline for comparison. It would emphasize to the reader that even with your predicted center frame, prior work fails in the non-center frames.\n3. Will the synthetic images generated as part of the Rotation Blur dataset be made public for future evaluation?\n4. Please discuss the limitations of the proposed approach specifically how much motion can be deblurred and using how many frames (maybe relative to magnitude of blur). How is the number of predicted frames determined?\n\nDECISION:\nI think the proposed framework to generate multiple de-blurred video frames from a single motion-blurred image is very interesting and the ability to handle arbitrary motion makes it very appealing. However, I do not know enough about the area to make a strong decision. Hence, I give WEAK ACCEPT (subject to change based on discussion)."}