{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a novel way of compressing a CNN by selecting the subset of convolutional filters that maximises the IB criterion. Based on this idea the authors propose an automatic procedure to prune non-informative filters from a trained network and show empirically that, with minimal impact on performance.\n\nThe idea of the paper is interesting but I feel the relative novelty of the approach is not sufficient to make it eligible as a full conference paper. The idea of feature selection using information gain is a classical approach in standard applied ML literature and the only difference I see in the current approach that it is applied to convolutional filters and not to features themselves.  \n\nFurthermore, the writing of the paper needs to be revisited, there are many instances of sentences that are confusing or not fully finished such as:\n- So pruning these high informative filters affects  to predict the objects correctly \n- [\u2026] in which network are trained to regularize most of neurons approximately uninformative.\n- [\u2026] then prune the high-informative filters incrementally in the precise of not hurting expected performance. \n \nIn terms of suggestions, I would recommend a careful rewriting of the paper and its resubmission to a workshop on neural networks compression."}