{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper investigates the use of the information bottleneck principle in convolutional neural networks. It uses the principle to prune the filters of the convolutional network that shares the least mutual information with the label.\n\nThe paper has both technical shortcomings as well as shortcomings in presentation.\n\nTechnical shortcomings:\nThe way mutual information is calculated in this work, by binning the filter outputs, is an approximation that should be further justified. It ignores potential correlations between the filters as well as the fact that the network uses continuous values. This approximation lacks technical justification.\n\nPresentation shortcomings:\nThe paper does not do a good job at communicating its ideas. The introduction is very difficult to read and it does not give an insight into the content of the paper. Section 3 does not explain the notation and it does not clearly communicate the method used. Perhaps providing pseudocode for the algorithm would be helpful.\n\nSignificance:\nThe method performs worse than most recent approaches to pruning neural networks. (eg. Deep compression, Bayesian compression)"}