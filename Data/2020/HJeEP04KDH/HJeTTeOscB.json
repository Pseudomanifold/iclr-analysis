{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper studies the effect of quantization on training reinforcement learning tasks. Specifically, the paper applies post-training quantization and quantization aware learning to various tasks and record the effects on accuracy and training speed.\n\nOverall, the empirical evaluations suggest that quantization does not significantly hurt the performance of RL training among a wide range of tasks. On several tasks, the authors showed that quantization can significantly reduce memory usage and speed up the inference time. On the other hand, the improved efficiency comes at the cost of accuracy or lower rewards (2% - 5% error as shown in section 4) and (> 5% in terms of success rate as shown in Figure 5).\n\nWhile it is expected that quantization should decrease the accuracy of the trained model, it is not entirely clear how one should evaluate the trade-off presented in the work. Some natural questions that I believe deserve more discussions are:\n-- Are the kinds of accuracy cost the best one could hope for using these methods?\n--  Is there still room for improvement in terms of reducing the cost of accuracy?\n\nDetailed comments:\n-- In the definition of Q_n(W): isn't $\\delta$ equal to |W| / 2^n?\n-- In Figure 5: your results show that the \"int8\" method has a significantly lower success rate than \"fp32\". Could you provide some discussion as to why this is the case?\n-- Typos: Page 4, \"is a applied\"; Page 5, \"full connected weights\"; Page 8, \"of a accurate\"."}