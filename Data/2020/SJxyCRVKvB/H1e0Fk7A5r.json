{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper investigates the important problem of inferring Granger casual structures from multi-variate time-series data, and propose the Granger casual structure reconstruction (GASER) framework with prototypical Granger causal attention.  The paper is in general well written with clear notations and is easy to follow. The proposed attention mechanism is also  intuitive. Experiments on  both simulated/synthetic and real-world datasets show the proposed approach achieved both improved casual recovery and more accurate predictions.\n\nHere are some concerns, and the reviewer is willing to adjust the rating if these concerns are resolved.  \n\n- Baselines. Have enough strong baseline algorithms been included? \n (1) It seems that GASER outperforms state-of-the-art prediction algorithm IMV-LSTM by a large margin (Table 5) even if it is not designed for the prediction task. \n(2) In Table 1-3, the other two baselines usually perform similarly or worse than the simple linear Granger baseline. Does it indicate the compared baselines are not strong enough? \n(3) Why linear Granger, i.e, VAR with L1 regularization, is not included in Table 5 for prediction? \n\n- Model design and trade-off.  According to Table 1-5, it seems that by adding the proposed attention mechanism, we can achieve both improved results (better than SOTA IMV-LSTM) and better interpretability.\n(1) Is there any trade-off in the design?  \n(2) Is the proposed approach sensitive to different parameters, e.g., 1) the number of prototypes, $K$ (when it is not the same with the ground-truth), 2) $\\alpha$ in Equation 9 (is 0.5 is a generally good default choice?), 3) $\\lambda_1, \\lambda_2$ in Equation 13, 4) $\\gamma$ in Equation 12.\n\n- Prototype learning. It seems that the prototype learning is used to deal with the lack of data. However, adding prototype learning increases the number of parameters to be learnt, i.e., $[p_1, \u2026 p_K]$.  It will be helpful to provide more intuition. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}