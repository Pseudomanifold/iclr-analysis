{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes using more expressive priors in variational autoencoders (VAEs), such as the multivariate Student\u2019s t-distribution. The experiments demonstrate a small improvement of SSIM metric on Omniglot.\n\nExploring various design choices of the VAE, including the prior distribution, is important, However, I feel that the paper does not have a sufficient contribution. The theoretical novelty is small; the experimental evaluation is lacking and missing crucial baselines, such as normalizing flows. The text is also not very well-written. Because of these factors, I think the paper should be rejected.\n\nIn more detail, I see the following issues with the current manuscript:\n1. The manuscript is not properly anonymized as it includes acknowledgements.\n2. The title of the paper is confusing, as it suggests that only the prior is being changed. However, Table 1 shows that the posterior is changed jointly with the prior.\n3. The novelty compared to (Abiri & Ohlsson, 2019) is very small. The difference is replacing diagonal covariance multivariate Student\u2019s t-distribution with similar distributions, such as the full covariance Student\u2019s t-distribution or a batch of Student\u2019s t-distributions. The empirical improvement from these changes, based on Figure 5, is quite small.\n4. The text of the paper can be significantly improved. There are many typos, e.g. \u201clower bound log likelihood\u201d instead of \u201clower bound on the log-likelihood\u201d. There are also issues in the math, such as a missing negation in the KL divergence at the bottom of Page 2. Some things are not specified: how exactly the diagonal values of the Cholesky decomposition are enforced to be positive? what are the bias nodes mentioned in Figure 2? what is the multivariate Student\u2019s t-distribution density function?\n5. One thing I didn\u2019t understand is why the KL-divergence of a product of independent (\u201cbatch\u201d) t-distributions cannot be computed analytically using equation (3), given that the KL(prod_i q_i || prod_i p_i) factorizes as \\sum_i KL(q_i || p_i).\n6. The experimental protocol is lacking. A key missing comparison is normalizing flows (https://arxiv.org/abs/1606.04934) which are typically used to make the VAE prior distributions more expressive. There are also VampPrior (https://arxiv.org/abs/1705.07120) and LARS (https://arxiv.org/abs/1810.11428) which also augment VAE\u2019s prior in various ways. Another issue is that only the SSIM metric is reported, whereas the standard way of comparing generative models is the test log-likelihood."}