{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper attempts to address the problem of out-of-distribution detection with generative models. To do this they assume they are given batches of OOD examples or batches of in-distribution examples, and they detect whether the batch is in- or out-of-distribution. Normally we try to detect if an example is in- or out-of-distribution.\nUnfortunately, this is not an interesting assumption and makes the problem significantly easier. If we assume this, then averaging the anomaly scores of multi-class OOD detectors would result in performance near the ceiling.\nIt is not surprising that one can obtain a much better OOD detector given a batch of OOD samples. Hence they're making progress on a problem we have the community has not been interested in, and they are not making progress on the standard OOD detection problem.\n\nSmall notes:\n\n> given their successful generalizing on a test dataset.\nWhat does this mean? Does this mean their BPP is good? By what standard?\n\n> unerlying\nunderlying\n\n> CIFAR\nInclude CIFAR-10 vs CIFAR-100 results in the table.\n"}