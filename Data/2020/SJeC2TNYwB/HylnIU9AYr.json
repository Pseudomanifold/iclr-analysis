{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the out of distribution detection problem and utilizes the property that the calculation of batch-normalization is different between training and testing for detecting out-of-distribution data with generative models. The paper first empirically demonstrates that the likelihood of out-of-distribution data has a larger difference between training mode and testing mode, then provides a possible theoretical explanation for the phenomenon. The proposed scoring function utilizes such likelihood differences in a permutation test for detecting out-of-distribution data. The evaluation is performed on two small in-distribution image datasets and four out-of-distribution datasets with three types of generative models.\n\nWe recommend a weak accept. Its clarity is good, and the strength of the paper has three parts. The first is the thorough observation of the likelihood changes between different modes of batch-normalization. Second, the theoretical explanation for the observed phenomenon is sound. The example in Figure 2 gives a good intuition of how mis-specification can happen. The last is the strong performance on the out-of-distribution detection with generative models.\n\nHowever, I have some concerns about the design of the scoring function (Section 5), which looks like it is carefully tuned:\n1. Why do the authors use the permutation score ($T_{b,r1,r2}$) instead of likelihood difference ($\\delta_{b,r1,r2}$)? The likelihood difference itself seems like it could be a good indication for OoD detection.\n2. Why do the authors use interpolation between training and evaluation? It introduces extra hyperparameters (r1, r2) for the method. Is the performance sensitive to the choice of r1 and r2?\n\nOne minor issue:\n3. The sentence at the bottom of page 7 should be removed (\"related work still needs some work, but there seems to be some bug on overleaf right now\")\n"}