{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper reads well, with intriguing ideas that challenge the \u2018dense\u2019 approach to DNNs, excellent thought experiments and convincing experiments.\n\nThe reviewer is neither an expert in face verification and in K-NN retrieval algorithms but has a solid experience in sparse ML algorithms and group lasso algorithms.\n\nIn this respect, the algorithms proposed in this paper represent an excellent extension of existing sparse algorithms that go against the current trend of focusing on compact dense representations because this is what GPUs handle best.\n\nClarity: an excellent introduction (appreciated by a reviewer not up-to-date in the topic) introduces representation learning for retrieval, though says little about the sparse multiplication state-of-the-art or face verification.  The rest of the paper reads very well (I had to dig deep to find some clarifications in detailed comments). For lack of space, one has to through quite few references to fully understand the experiments.\n\nQuality: there are only few equations in this paper, but they rely on excellent notation. The algorithm is also well formulated. What strikes me as very good are the \u2018thought experiments\u2019 that suggest (rather than prove) that the approach is well grounded: the end of section 4 is excellent\n\nOriginality and significance: working on sparse representations is quite \u2018original\u2019 now, especially as they are so GPU-unfriendly (I note the sparse algorithm is implemented in C++, and am looking forward to the code release). I hope the excellent results reported in this paper will incite others to revisit them. 15 years ago, such as paper would have been less original. The sparse vector sparse matrix product algorithm is well known and has been used in ML publications before, for instance:\n-\tHaffner, ICML 2006, \u201cFast Transpose Methods for Kernel Learning on Sparse Data\u201d \n-\tKudo and Matsumoto 2003 \u201cFast methods for kernel-based text analysis\u201d\nWhat seems to be original is the algorithm to balance sparsity probabilities. The reviewer is well aware that a single non sparse column can kill the performance of the sparse matrix multiplication algorithm, and this is probably the main reason this algorithm has not found broad usage. The derivation and the connection to group and exclusive Lasso are excellent.\n\nDetailed comments:\n-\tpage 5 \u201cwhere we suppress the dependency of Mu and Sigma\u201d. How do you do that?\n-\tPage 7 \u201cThe analysis in figure 2 follows similarly for Stresh\u201d.  Probably more explanation is needed here. Replacing Relu with a sum of Relus over affine transforms of the Gaussian variable is a complex operation:  how does it keep the same curves?\n"}