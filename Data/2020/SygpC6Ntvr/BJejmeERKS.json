{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to learn sparse representation in neural networks for retrieval in large database of vectors. Such sparse representation, when the fraction of non-zeros is high, can be computed using sparse matrix multiplication, or variants of inverted index scoring and lead to potentially lower FLOPs needed. This paper proposes to induce sparsity by adding a regularization term, which counts the expected number of FLOPs needed for sparse scoring.\n\nThe final experiments were done on Megaface dataset, where there are 1M distrators and accuracy is measured in Recall@1. The sparse embedding approach is combined with a dense re-ranking stage, and the speed-recall trade is compared to the pure dense approaches (such as using baselines such as FAISS\u2019s IVF-PQ, LSH or SDH). The authors\u2019 evaluation showed that, the performance of FLOPs regularized embedding is better than L1 regularized sparse embedding, or applying IVF-PQ directly to dense embeddings.\n\nPros:\n- The topic of learning sparse embeddings is of great interests. As far as I know, many researchers attempted and there was not an agreement. For example, https://arxiv.org/pdf/1904.10631.pdf [1] claimed \"Using sparse operations reduces the number of FLOPs, as the zero entries can be ignored. However, in general, sparsity leads to irregular memory access patterns, so it may not actually result in a speedup;\" This has been my experience as well. The authors\u2019 embedding seems to be much more sparse (<5% non-zeros) than the sparse embeddings from other approaches and thus works better.\n\n- The formulation is simple and intuitive - it appears easy enough to plug into a variety of embedding architectures (potentially all embeddings, NLP, CV, Audio).\n\nCons:\n- The learned sparse embedding system still requires training a separate dense embedding for re-ranking, so essentially is a hybrid approach. One cannot simply \"get rid of the dense\". Ideally one would hope to not need two inference runs (for sparse and dense) and keep two database (sparse and dense). Maybe the author can report how sparse embedding performs on its own.\n\n- It is widely known that inverted index is not FLOPs bound, as its FLOPs utilization in inverted index is typically low. Inverted index is almost always dominated by random memory accesses and thus ideally the regularizer should be modelling after cache miss / memory access pattern instead of FLOPs. I\u2019d like to see authors gave more discussion on these topics instead of taking a FLOPs centric view (which is not true for inverted index).\n\n- For comparison dense ANN, Faiss\u2019s IVF-PQ is a relatively dated pipeline. It would be good to see how what the curve would look like for other dense technique such as HNSW [2] which performs better than IVF-PQ on http://ann-benchmarks.com/. Also dense ANN can also greatly benefit from the use of batching, which is not considered for this paper. \n\n- Finally, I recommend the authors perform additional experiments on other datasets. As the authors suggested, sometimes sparse embedding learning risks collapse to predicting the classes. The unseen face queries avoid this problem to some extent. But I still worry that the Megaface task somehow allow representation to be much more sparse than other NLP/CV tasks. For example, authors can try BERT tasks with sparse embedding. It would be much more convincing to see this approach generalizes across many tasks.\n\n===\nOverall, I think this is a nice paper which takes a good step to improve the effectiveness of sparse embeddings over existing methods such as simple L1 regularization. However, with the need of training separate dense embeddings, and the fact that experiments were conducted only on 1 tasks with relatively weak ANN baseline, I\u2019d lean towards rejection.\n\n[1] Low-Memory Neural Network Training: A Technical Report\n[2] Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs\n"}