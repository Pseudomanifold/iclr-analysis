{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a framework for evaluating the sensitivity of a QA model to perturbations in the input. The core of the idea is that one can replace content words (i.e. named entities and nouns) in questions in such a way that makes QA models more confident of their original answer (despite, presumably, the question now being unanswerable). Replacements are constructed by mining equivalence classes in Squad data (i.e. all words w/ pos = noun are one set).    Depending on how many such substitutions are searched over (and whether multiple are applied),  one can find at least one such failure in about 50% of cases, on a BERT model trained on Squad2.  The paper also proposes a simple mitigation technique: an objective that modifies a given QA example with all possible substitutions and trains for \"no answer\" (or alternatively substitutions which break the system).  Results demonstrate that performance on Squad2 is roughly unchanged while the success rate of the attack is significantly decreased.\n\nWhile the idea of forming such equivalence sets is very interesting, my concern with the paper is both in terms of impact and experimental methodology. \n\nImpact: the method is essentially a data augmentation approach over a fixed list of words. This isn't very different than what was proposed in https://arxiv.org/pdf/1804.06876.pdf and https://arxiv.org/pdf/1807.11714.pdf . While there are some nice nuggets in the analysis, in particular that model confidence is a factor for the attack, I'm not sure anything very novel is being proposed. \n\nExperimental Methodology: Other works in this vein explicitly create a split between counterfactual examples evaluated at train vs at test. The methodology proposed here requires a search where there isn't a clear split between what aspects of the search are allowed at train vs test. In doing counterfactual data augmentation, it is possible the model observes most elements of the search that will be evaluated at test time, making it almost inevitable that the search will be less successful after the model is trained. A simple solution would be splitting the equivalence sets into train/test. I was not able to confirm whether or not this happened from the paper.\n\nThat being said, the paper did evaluate on Lewis&Fan( https://openreview.net/pdf?id=Bkx0RjA9tX ) 's bias training simulation, which I appreciate, but I was disappointed that (a) the results from Lewis&Fan were not included for comparison, and when compared the augmentation method proposed here works much worse, in some settings, than generative based training. \n"}