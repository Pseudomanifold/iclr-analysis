{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Paper proposes a method for continual learning. The method is based on the learning of a metric space where classes are represented by prototypes in this space. To prevent forgetting the method proposes to perform prototype recall, aiming to keep prototypes in the same location in embedding space (Fig 1b). The method is compared with several recent methods and is shown to outperform them on two small datasets (MNIST permuted and CIFAR10). The idea of using prototypes for continual learning is interesting, as the authors point out, this does not require adding new neurons to the network for new tasks.\n\nI found the paper hard to follow (presentation needs improvement) and the experimental results are lacking. I, therefore, recommend weak reject at the moment. \n\nClarity method:\nI did an effort to understand the details of the paper but found the chosen notation hard to follow. Some remarks with respect to that. \n\n1. The delta loss is based on previous models (last term Eq. 1), which you do not want to store. It is important to immediately point out that you are not storing them (if I understand correctly) and that you are actually only storing the prototypes (Eq. 5). \n2. Could the loss in Eq 6 be seen as a distillation loss for an embedding space. You want to stay close to previous network ?\n3. It was unclear for me how often prototypes where updated, in every minibatch ? \n4.The algorithm is not very clear (what is a training episode?). Sampled m examples per dataset (should be per class per dataset). From the algorithm it seems that the loss in Eq 4 (first line after else) is first minimized separately from the loss in Eq. 7 . Should this not been done jointly ? \n5.What is the contribution of dynamic episodic memory allocation ? I am not sure if the Equation is very helpful. Is in practice M<<D ?\n6. Exemplars can only be chosen ones (when considering the task). I would like the authors to confirm  that they do that, because I was not sure from the text (they can be deleted later but not replaced with other exemplars from the same task). \n\nExperimental results:\n1. The authors should provide more experimental details. What network is used? Is it a pretrained network. \n2. Colors in Fig 3 preferably stay the same for same methods. The legend should only show results which are present in graph. (Fig 3c has 5 curves and 10 methods in legend). Why are there 21 tasks in Fig 3a and b, should it not be 20 ? Why are there 9 tasks in 3c,  should it not be 10 ? SGD results missing in Fig 3a and b, but mentioned in text. What happens with EWC, it has excellent results in Fig 3a but does bad in 3b, is there an explanation ?\n3. Ablation study should include ablation of the proposed losses, do they actually help in improving the results. The ablation is on memory usage which is not the main contribution of the paper. It would  be better to use CIFAR 10 then MNIST premuted for ablation (see for example \u2018A COMPREHENSIVE, APPLICATION-ORIENTED STUDY OF CATASTROPHIC FORGETTING IN DNNS\u2019 on the limitations of MNIST permuted). \n4. Figure 6 did not help me understnad the method. It would be nice if you could plot the prototypes, than we can see if they keep in the same location. \n\nMinor:\nI am not sure the authors are aware of difference \\cite and \\citep which would improve readability. \n"}