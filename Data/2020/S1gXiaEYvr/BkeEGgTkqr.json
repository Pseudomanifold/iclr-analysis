{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Paper summary:\nThis paper proposes a method for continual learning/few-shot learning which involves (1) putting class prototypes in a replay buffer, at test time computing the distance to these prototypes in order to classify the example (2) regularizing the embedding space used to compute prototypes by ensuring classification accuracy on a replay buffer of data examples does not degrade.\u00a0\n\nPaper contributions:\n\u00a0- The idea of putting prototypes in a replay buffer, and of the regularization for ensuring classification accuracy on earlier examples does not degrade (I'm not up-to-date enough in this area to know if these ideas are novel)\n\u00a0- Experiments in continual learning and few shot learning which compare the proposed method to alternatives\n\u00a0- Ablation study with different settings of the proposed method\u00a0- exploration of the embedding clusters learned by the method\n\nReview summary & decision:\u00a0\nThis paper introduces two ideas that I think seem quite good and could be valuable to the community. However, I found the presentation of material very confusing and poorly organized, some claims misleading, and some experiments missing. I recommend rejection of the paper in its current state.\n\nReasons for decision:\n1. The intro, \"proposed method\" section, and \"prototype recall\" section are repetitive of each other (especially proposed method and prototype recall). The latter two sections also have some information that should go in the related work (aka introduction). I actually like the merging of related work and introduction though; I think it could be nicely space-efficient if things weren't so repetitive later.\n2. Overall I found the paper is very disorganized and hard to follow; the description of what is done (in particular the fact that you evaluate on three different tasks) is very unclear. It's mentioned in the last paragraph of the intro that you do two experimental protocols, but these are not referred to (at least not by the same name) in all of the experiments. Also there are experiments on few shot learning which are only ambiguously referred to in the abstract. The relationship of \"classification\" and \"eliminating catastrophic forgetting\" to these tasks is not discussed, even though there is a whole section called \"classification\" which seems like a misleading title for this section - it seems more like this is further description of your proposed method, along with some related work. The terms \"single head\" and \"multi head\" evaluation are in common use in continual learning, but are not referred to here at all. The pseudocode algorithm with references to equations makes it reasonably clear what is going on, but it would be helpful if it appeared much earlier and the discussion followed this outline (e.g. have this be the first thing in the section, and then have sections called \"Prototype computation\", \"Episodic memory\" and \"Prototype recall\"), and if it were made more clear where the actual task comes in. Also there's a lot of related work in the Experiments section. It seems like the few-shot experiments are kind of tacked on as an afterthought; the intro doesn't mention these and there are methods (e.g. logit matching) used here that are not mentioned in the intro/related work.\n3. What is the motivation for averaging embeddings? As far as I can tell this is mentioned in the caption of Figure 1; it should be discussed elsewhere.\n4. Part of the motivation for your approach as given in the abstract and intro is not to have to save examples, but your method does require this for computing the regularization term. I would like to see an ablation study of the proposed method with varying amounts of data examples, including 0 (i.e. without this regularization); as far as I understand this is only done for 10 and 530 examples. It seems odd to refer to this as \"memory\"; this only makes sense if we know you're referring to episodic memory, which is not clear (or even mentioned) in the caption for Figure 5. Based on the fact that the method performs poorly with only 10 examples as the number of tasks goes up, I would say it's misleading to claim you use only a small number of samples, as well as to claim that you only need to store class-representative prototypes (you also need to store these examples).\n\nFeedback/suggestions/nits (not necessarily part of decision assessment):\n\u00a0- \"continual learning is a critical ability\" this kind of subjective assessment doesn't add to our understanding and is generally not recommended in scientific writing. This first sentence does not do a good job of explaining what is continual learning for someone who does not know.\n\u00a0- \"enabling CL\" strange word to use here.\n\u00a0\u00a0- Inconsistent capitalization of prototype recalls\n\u00a0- In the abstract, the 4 contributions seem repetitive (1 and 3 seem the same to me) and the 4th contribution makes it sound like an avenue for future work than an actual contribution.\n - Intro is repetitive of the abstract\n\u00a0- citations are frequently incorrectly formatted (should use \\citep, not \\citet unless you are referring to the authors by name)\n\u00a0- unclear what \"stationary batches\" means; do you mean in continual learning the data distribution is non-stationary?\n\u00a0- verb tense: \"have been currently proposed\" does not make sense\n\u00a0- I don't understand the criticism of \"Learning without forgetting\" (that it requires exhaustive hyperparameter tuning); your method also has hyperparameters to tune. Do they have more, and is their method more sensitive to hyperparameters? What's the evidence for this?\n\u00a0- If your criticism of replay-based methods is that they ignore topology among clusters in embedding space, you should explain why/how your method does not do this. Likewise for relying on \"small amounts of individual data\"; if seems to me that your method has the same structure, and for both your and these methods the size of the replay buffer is just a hyperparameter, unless I've misunderstood something.\n\u00a0- \"our method learns embedding functions... has also been verfied in works\" what exactly has been verified? What do those works do? This sentence is unclear.\n\u00a0- Hoffer & Ailon is incorrectly cited as Ailon (2015); in general the references are poorly formatted\n\u00a0- Snell et al is barely mentioned in the related work, but the two methods seem very similar to me; I think it merits more discussion (in particular, highlighting how what you do is different / an extension). The \"classification\" section seems to imply you compute prototypes exactly as in Snell et al., but elsewhere it sounds like you do something different.\u00a0\n -\u00a0 Figure 1 is not clear; one must have already read the full explanation of the two settings (which is spread over the intro and other sections) to understand it. Just mentioning that these two are different evaluation settings would be helpful. Also,\u00a0 I find the relationship between the tasks unclear.\n\u00a0\u00a0- \"yielded outstanding performance\" what is outstanding in this context?\n\u00a0- not sure it's accurate to call \"the metric space at task t can be underlined by class-representative prototypes\" an \"inductive bias\". Also what does \"underline\" mean here?\n\u00a0- \"without loss of generality\" I don't think this phrase is used appropriately here\n\u00a0- Algorithm 1's for loop is quite unclear to me, in particular it's unclear where you actually do the task you're supposed to be doing.\n\u00a0- EWC and yours are the same colour in Figure 3\n\u00a0- Figure captions should mention what the abbreviations mean (or mention where in the text these explanations can be found).\n\u00a0- What are we supposed to tak away from the TSNE plots? This explanation should be in the caption, and the tsne hyperparameters should be in appendix.\n\u00a0- \"we restrict ourselves to... discrete prototypes\" not obvious what discrete means here; you're using a continuous embedding space. I assume you mean discrete output tasks (i.e. classification), but this is unclear.\n\u00a0- It would be nice to see results about computation time; it seems like computing the prototypes might be very intensive.\n\nQuestions:\n1. Can you contrast your method with prototypical networks? Are prototypes computed in the same way, and then put in a replay buffer along with examples?\u00a0\n2. Why averaging the embeddings? Did you try anything else?\n3. How do your task protocols correspond to \"single head\" and \"multi head\" evaluation protocols? Why weren't these mentioned? Why did you choose these protocols?\n4. The approach of using the same architecture and same memory allowance for all the methods tested seems reasonable to me, but can you think of alternatives, or reasons that this could be unfair to any of the methods?"}