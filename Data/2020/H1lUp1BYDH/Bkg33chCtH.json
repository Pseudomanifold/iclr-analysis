{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper aims to ease network congestion and enable more efficient flow in systems like traffic, supply chains and electrical grids. They apply Multi-Agent Reinforcement Learning (MARL) to such problems by considering each vertex in the network as an agent. Instead of assuming the agents are independent like most MARL-based models, the authors propose a framework that induces cooperation and coordination amongst agents, connected via an underlying network. The network is learned via emergent communication in a MARL-based setup. Experimentally, they demonstrate the utility of communication in networks of traffic systems.\n\nStrengths:\n- This work is nicely written. I like how communication and RL can be used in this interesting setup.\n- The experimental results seem plausible and show some improvement over existing methods that do not explicitly model communication.\n\nWeaknesses:\n- In Figure 4, why does a DQN start off so much higher in rewards as compared to your method?\n- In Figure 5, why does having 1 blind agent make almost no difference and still achieve as strong performance as the full model?\n- Why does 'communication' have to be used in this setting, instead of other graph-based methods that take information from a node's neighbors and make a holistic prediction on the whole graph? It was not completely clear if there was imperfect information between agents, which makes communication crucial. If there was complete information about each agent then communication would not be needed.\n- Could the authors comment on how the complexity would increase with more agents? Would it still be feasible? "}