{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors propose a general form that covers most stochastic gradient methods so far, e.g. stochastic gradient descent, Adam,  or adaptive probabilities methods. Then, they also provide a convergence analysis of convex problems.  In the experiments, they compared the proposed DASGRAD method with Adam, AMSGrad or SGD. Experimental results show that the proposed method converges faster than compared methods.  Following are my concerns:\n\n1) It is important to cite properly. There are some related works missing in the paper. e.g.,  [1][2][3] for adaptive methods, and [4] adaptive sampling methods.\n2) In corollary 1.1, the rightmost term is in the order of \\sum_t=1^T \\sqrt{t}, which should be O(T\\sqrt(T)). If we divide both sides by T, the right side of the inequality is still upper bounded by \\sqrt{T}, it is not convergent. Otherwise, please correct me and show the convergence rate of the proposed method.\n3) The same question in Corollary 2.1 about the convergence result. \n4) Some variables are not defined clearly in the paper, e.g., \\bar g and  \\beta_{1_{t}}  in corollary 1.1.\n5) Algorithm 2 requires to compute p every J iterations. How about the time complexity of sampling? Please also show the convergence regarding time in the experiments.  \n\n[1] Zhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n[2] Chen, Xiangyi, et al. \"On the convergence of a class of adam-type algorithms for non-convex optimization.\" arXiv preprint arXiv:1808.02941 (2018).\n[3]Zaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018.\n [4]Csiba, Dominik, Zheng Qu, and Peter Richt\u00e1rik. \"Stochastic dual coordinate ascent with adaptive probabilities.\" International Conference on Machine Learning. 2015.\n\n"}