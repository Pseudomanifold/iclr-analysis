{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the doubly adaptive stochastic gradient method (DASGrad) is introduced via augmenting adaptive moment methods with adaptive (as opposed to uniform) probabilities for data sampling. The convergence of the proposed method is analyzed in terms of regret bound and is compared to similar results for ADAM. The method is validated with experiments on both convex and non-convex objectives as well as in applications to transfer learning.\n\nThe paper is well written and provides novel, to the best of my knowledge, theoretical results on the convergence of adaptive moment methods when augmented with non-uniform data sampling, which is quite interesting. However, my main concern is the proposed implementation of non-uniform sampling. In particular, the adaptive probability of every training sample is set proportional to its gradient and, in theory, this has to be updated at every iteration. This essentially means that the full batch gradient is available at every iteration and, since the full gradient is available, wouldn\u2019t it be better to use full batch gradient descent instead?\n\nThe proofs in the appendix repeat the known results in detail, while the definitions and proofs of the concepts related to the main contribution of this paper are rather short. In particular:\n1. The expectations are not formally defined. The expectation with respect to p_{1:T} must be with respect to the join of all distributions from every iteration (?); sometimes there is conditioning on parameters and sometimes it disappears (proof of Lemma 1). This should be clarified.\n2. It would also be helpful to see more detailed proof of Lemma 3 since it is not obvious and is the key ingredient of the presented results. \n3. As a sanity check, I would expect that the results of Th. 1 and Th. 2 (or Corollary 1.1 and Corollary 2.1) should coincide when p is set to the uniform distribution. Is this correct and, if yes, can one show this?\n\nLess important questions:\n1. In Algorithm 2, there is a parameter J which controls the frequency of computation of optimal probabilities. How is this parameter set and what are the tradeoffs? After the parameters are updated, does it actually help to use such probabilities from the previous step when the parameters are in fact different?\n2. Why parameter epsilon is introduced in Algorithm 2? How is it set?\n3. Gradient aggregation methods are mostly criticized for their linear runtime in the number of examples n (see, e.g., Bottou et. al. 2016), while adaptive moment methods are widely used in practice since their runtime doesn\u2019t depend on n. Since the proposed method is also linear in n, it would be interesting to see how it compares to aggregation methods in practice.\n4. Conceptually, what is the difference of the results in Theorem 1 and Corollary 1.1 from the previous results in Kingma & Ba (2014) and Reddi et. al. (2018)?\n5. In Lemma 4, there is unnecessary \\alpha_t in the denominator. Does it change the final result?\n6. To improve the runtime of the proposed algorithm, can one develop any approximation of optimal probabilities that wouldn\u2019t require full data pass?\n\nMinor:\n1. p. 3, \u201cDuchi et. al.\u201d is this a reference?\n2. functions phi and psi are called proximal; are they related to proximal operators?\n3. typos: p. 3, \u201cwe first we\u201d; p. 5, \u201can may\u201d\n4. p. 2, it is AMSGrad, not ADAM; betas aren\u2019t defined\n5. Inconsistent numbering: Theorems 1 and 2, but Corollaries 1.1 and 1.2\n"}