{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a new reading comprehension dataset for logical reasoning. It is a multi-choice problem where questions are mainly from GMAT and LSAT, containing 4139 data points. The analyses of the data demonstrate that questions require diverse types of reasoning such as finding necessary/sufficient assumptions, whether statements strengthen/weaken the argument or explain/resolve the situation. The paper includes comprehensive experiments with baselines to identify bias in the dataset, where the answer-options-only model achieves near half (random is 25%). Based on this result, the test set is split into the easy and hard set, which will help better evaluation of the future models. The paper also reports the numbers on the split data using competitive baselines where the models achieve low performance on the hard set.\n\nStrengths\n1) The introduced dataset is timely and is more challenging than existing reading comprehension datasets, based on the examples presented in the paper.\n2) Analyses of reasoning types (Table 2) are done in a very comprehensive way, especially because they are comparable with descriptions from previous literature. This analysis clearly demonstrates that this data requires diverse types of challenging reasoning.\n3) The experiments are comprehensive with many competitive baseline models, and their efforts to identify bias and use the result to split the test set are impressive.\n\nWeaknesses\n1) Although the main claim of this paper is about logical reasoning, I believe the term \u201clogical reasoning\u201d is somewhat subjective. What exactly is the definition of logical reasoning? The paper never defines it in any way; it only shows Table 1 as an example.\n2) (Continuing the point above) The paper mentions other datasets\u2019 statistics on logical reasoning (in Section 1, they mention \u201c0% in MCTest dataset and 1.2% SQuAD\u201d requires logical reasoning). How was this analysis done?\n3) (Continuing the point above) Are all types of reasoning in Table 2 logical reasoning? I agree that those reasoning are very challenging, but I am not convinced why some of them belong to logical reasoning (E.g. summary). I expect this problem can be resolved if authors bring a clearer definition of logical reasoning.\n4) Although I appreciate the experiments to identify bias, I believe that the bias seems to be somewhat significant if 440 out of 1000 examples are identified as biased examples. I believe that the authors should do more comprehensive analysis on biased examples and explain what are possible biases here, in order to justify the significant bias.\n\nMarginal comments\n1) The authors claim they collected the questions from open websites and books, but they didn\u2019t provide the URLs or names. More information about which source was used, how data points were filtered and so on would be necessary to assess the quality of the dataset. Specifically, is there any particular reason for removing one of wrong options to make it have four choices, instead of five choices?\n2) Why is the number of Chance model 0.39? Shouldn\u2019t it be 1000 * 0.39% = 3.9?\n3) In Table 5, it\u2019s interesting to see that different models identify different numbers of biased examples. Have you looked at how much do they overlap each other if you take a pair of models? (A number of the union kinda shows it, but I wonder if there is a more intuitive way to see how consistent the models\u2019 decisions are.)\n4) It would be also helpful to see the baseline results of \u201cquestion & answer options only model\u201d and \u201ccontext & answer options only model\u201d."}