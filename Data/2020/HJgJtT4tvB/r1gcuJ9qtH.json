{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nPaper Summary:\n\nThis paper presents a machine reading comprehension dataset called ReClor. It is different from existing datasets in that ReClor targets logical reasoning. The authors identified biased data points and separated the testing dataset into biased and non-biased sets. Experimental results show that state-of-the-art models such as XLNet and RoBERTa struggle on the non-biased HARD set with poor performance near that of random guess.\n\nStrengths:\n\n\u2014The dataset, which is extracted from standardized tests such as GMAT and LSAT, requires the ability to perform complex logical reasoning. It is difficult for crowdsourcing workers to generate such logical questions.\n\n\u2014The authors carefully analyzed the biases, which are often exploited by models to achieve high accuracy without truly understanding the text.\n\n\u2014The authors thoroughly investigated how well strong models such as RoBERTa can perform.\n\n\u2014The paper is well organized and well written.\n\nWeaknesses:\n\n\u2014The dataset seems small to acquire the ability to perform complex logical reasoning. The training, validation, and testing datasets consist of 4,651, 500, and 1,000 questions, respectively.\n\n\u2014The paper does not show statistics of the dataset such as question/passage length and question vocabulary size.\n\n\u2014The paper does not show results of models trained with other reading comprehension datasets such as RACE and fine-tuned with ReClor.\n\n\u2014Unlike other datasets, the questions themselves show their required logical reasoning types in ReClor. For example, the question \u201cWhich one of the following is an assumption required by the argument?\u201d shows the \u201cNecessary Assumptions\u201d type and has no information with respect to passages. This characteristic makes it difficult to use the ReClor dataset as an evaluation benchmark for models trained with large-scaled reading comprehension datasets such as RACE.\n\n\u2014The human performance with respect to different question types of logical reasoning is not analyzed in Figure 3.\n\nReview Summary:\n\nThe paper is well motivated. ReClor can be a useful dataset to evaluate the ability of logical reasoning, while the dataset seems small to acquire the ability to perform complex logical reasoning.   I think it can benefit a lot with a more comprehensive analysis of transfer learning with other reading comprehension datasets.\n"}