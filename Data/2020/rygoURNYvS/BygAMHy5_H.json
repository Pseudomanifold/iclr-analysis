{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes a BERT-based pre-training for source code related task. By pre-training on BERT-like models on source code and finetuning on a set of 5 tasks, the authors show good performance improvements over non-pretrained models. The authors make a series of ablation studies showing that pre-training is indeed useful.\n\nOverall, I find this work relevant and interesting, albeit somewhat unsurprising. Nevertheless, I see no reason to reject this paper. To make my \"weak accept\" to a \"strong accept\" I would like to see experiments on more tasks, preferably more complex tasks. For example, such tasks could include (a) variable naming (b) method naming (c) docstring prediction/summarization (d) language modeling/autocompletion. I believe it's unclear to the reader if pre-training is also helpful for any of those tasks too and adding such experiments would significantly strengthen the paper.\n\nSome clarifications comments/questions to the authors:\n\n* I would insist that the authors rename the \"Variable Misuse\" task to \"Variable Misuse Localization\". To my understanding the current model points to the misused variable (if any), but does not attempt to suggest a fix. This tackles only a part of the task discussed in Vasic et al. (2019), Allamanis et al (2018) and this might confuse readers who want to compare with those works.\n\n* For the Function-Docstring Mismatch task (Section 3.4):\n    * It's unclear to me which dataset is used. Is it the Py150 dataset or the Barone & Sennrich (2017)?\n    * I believe that the citations [a], [b] would be appropriate here.\n\n* Overall, for the all the tasks except from \"Exception Type\", there is a replicability issue: Since the authors manually mutate the code (e.g. introduce a variable misuse, swap an operand), for anyone to compare directly, they would need access to the mutated samples. I would strongly encourage the authors to provide more details on how they create mutated samples and (eventually) the source code that achieves that.\n\n* For the Variable Misuse, Wrong Binary Operator, Swapped Operand tasks. There are a few things that need to be clarified:\n   * How long is each code snippet? One would expect that the longer the code snippet the harder the task. Do the authors pass a whole function?\n   * What is the proportion of positive/negative examples in each task?\n\n\n\n[a] Cambronero, Jose, et al. \"When Deep Learning Met Code Search.\" arXiv preprint arXiv:1905.03813 (2019).\n[b] Louis, Annie, et al. \"Deep learning to detect redundant method comments.\" arXiv preprint arXiv:1806.04616 (2018)."}