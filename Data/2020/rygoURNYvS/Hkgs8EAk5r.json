{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Summary\n\nThis paper presents a BERT-inspired pretraining/finetuning setup for source code tasks. It collects a corpus of\nunlabeled Python files for BERT pretraining, designs or adopts 5 tasks on established smaller-scale Python corpora, and\nadjusts the BERT model to tokenize and encode source code snippets appropriately.\n\n# Strengths\n\n* The idea of applying the pretraining/finetuning paradigm to program analysis tasks makes sense, and has been\n  informally attempted by multiple groups in the community in 2019. This is the first high-quality submission to a\n  top-tier ML conference I've seen on the subject, though.\n* The authors exercised commendable care and diligence in preparing the training data, adopting BERT to source code\n  inputs, and ensuring correctness of the experimental setup. I appreciated all the provided details on tokenization\n  (Section 3.3), deduplication (Sections 3.1-3.2), and task setup (Section 3.5). This should become a technical standard\n  in the community.\n* The paper is written clearly and concisely, and is generally a pleasure to read.\n\n# Weaknesses\n\nI have a gripe with the authors' choice to ignore program structure (e.g. abstract syntax trees) or features (e.g.\ntypes) in their program representation. Without this extra information (easily available from a compiler/interpreter\nAPI) the pipeline is not substantially different from the original NLP pipeline of BERT et al. The main program-related\nrepresentation insight comes in tokenization (Section 3.3) and the definition of \"sentences\". To repeat, I appreciate\nthe effort the authors put in making tokenization appropriate for BERT processing of source code, but this is a drop in\nthe bucket compared to the all the other program-related features the work is leaving off the table. Programs are not\nnatural language.\nThe argument that source code analysis would \"pass on the burden ... to downstream tasks\" (Page 3) is odd. First, most\ndownstream tasks of interest occur in the settings where this analysis is already available: IDEs, code review\nassistants, linters, etc. Second, one often needs program analysis to even define downstream tasks in the first place --\nfor example, determining whether function arguments are swapped required detecting a function call and boundaries of its\narguments, thus parsing the program!\n\nThis work obtains (and nicely analyzes) impressive results obtained by applying CuBERT. However, it does not put the\nresults in context with prior work based on structured program representations. Without this, it is difficult to say\nwhether the improvement comes from pretraining or from the language model simply learning a better \"parsed\"\nrepresentation of an input program from all the unlabeled corpus. If it's the latter, one might argue that supplying the\nmodel with structured program features explicitly might eliminate much of the need for the unlabeled corpus.\nI personally think that there will still be a gap between pretraining and finetuning even with structured program\nfeatures simply due to the sheer volume of available data, which, as the authors showed, is crucial for good\ngeneralization of Transformer. However, this still needs to be shown empirically.\n\nThe \"Function-Docstring Mismatch\" task, as presented, seems too easy. If the distractors (negative examples) are truly\nchosen at random, most of them are going to use obviously different vocabulary from the original function signature (as\nFigure 4 demonstrates). A well designed task would somehow bias the sampling toward subtle distractors such as `get` vs.\n`set` docstrings, but this seems challenging.\nThis also explains why the task is not influenced as much by reduction of training data (Table 3).\n\nThe Next Sentence Prediction pretraining task, as adapted for CuBERT, seems too difficult, in contrast. If the paired\nsentences (i.e. code lines) are chosen at random, the model would lack most of the context required to make a decision\nabout the logical relationship between them, such as which variables are defined and available in context, which\nfunctionality is being implemented, etc. I wonder, can the authors experiment with pretraining CuBERT only with the\nMasked Language Model task? Will it worsen the results substantially or at all?\n\n# Questions\n\nSection 3.2: \"similar files according to the same similarity metric...\"\nWhat are these metrics?\n\nWhat is the fraction of positive/negative examples in the constructed finetuning datasets?\n\nWhat is the motivation for making Variable Misuse and Wrong Operator/Operand into a simple classification tasks instead\nof the original (more useful) correction task?\n"}