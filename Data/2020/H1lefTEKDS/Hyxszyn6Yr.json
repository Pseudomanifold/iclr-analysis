{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "Summary of the paper\nThe authors benchmark 11 model-based and 4 model-free RL algorithms on 18 environments, based on OpenAI Gym.\nThe performance in each case is averaged across 4 different seeds.  Computation time is also reported.\nFurthermore, the authors analyze the performance hit incurred from adding noise to the observations and to the actions\nFinally, the authors propose to characterize what hinders the performance of model-based methods, ie., what they call the dynamics bottlenecks, the planning horizon dilemma, and the early termination dilemma.\nAs a conclusion, it turns out no clear winner stands out, which motivates further development of model-based approaches.\n\nStrong and weak points\nThis is a very interesting empirical study, especially since\n- it includes a comparison with model-free algorithms,\n- it considers computational aspects and indicates what algorithms can be run in real-time,\n- the authors use open-source software (PyBullet) as simulators, which makes the study more reproducible (although the code has not been shared yet)\n\nBut,\n- 4 seeds averaged across is clearly low, given the well-known variance of RL algorithms. In fact, the std values in the tables prove this point. I understand the benchmark is heavy on computation, however, this would only have delayed the output of the numbers without requiring more work (and admittedly, been even more harmful for the ecology...).\n- Not as a criticism but rather a suggestion, it would have been useful to summarize the table 2 by comparing the algorithms using normalized values (mean = 0, std = 1) averaged across the environments. \n- One the (strongest) weak points for me remains the assumption of the given differentiable reward function, as learning the reward function might be challenging for the model, for instance when it is sparse.\n- It is also surprising that the authors did not benchmark against nor cite (Ha, David, and J\u00fcrgen Schmidhuber. \u201cRecurrent world models facilitate policy evolution.\u201d NeurIPS 2018), especially since the code is open-source. \n- I would have made the same remark for (Learning Latent Dynamics for Planning from Pixels,  Hafner et al.) but the paper does not seem to be not peer-reviewed. Edit: the paper is indeed published at ICML 2019. So the remark holds. \n\nQuestion:\n- could the author elaborate on the early termination? it is not precisely defined anywhere and yet, seems to be an important point.\n\nMinor\n- page 3, parentheses around Finn et al. (2017)\n- page 3, ILQG, \u201cis an model\u201d"}