{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses empirical study of generalization behavior in gradient-based meta-learning. To this end, authors evaluated: (1) flatness of minima in terms of the spectral norm of Hessian matrix; (2) coherence of adaptation trajectories; (3) the average inner product between meta-test gradient vectors. Finally a regularized MAML is proposed, adding a penalty on angles between inner loop updates. Experiments are shown, measuring three quantities mentioned above. \n\n---Strength---\n- Empirical analysis of various properties of the objective landscape in gradient-based meta-learning is interesting and new.\n\n---Weakness---\n- Recent theoretical work on meta-generalization bound or convergence properties of MAML is available. For instance,\n  [1] M. Khodak (2019), \"Provable guarantees for gradient-based meta-learning,\" ICML.\n  [2] N. Golmant (2018), \"On the convergence of MAML,\" NeurIPS.\n- While empirical study could be interesting, but not much insight was not provided. \n- A regularized MAML is proposed but its effectiveness is not well studied yet.\n\nTo sum up, the paper provides a few interesting empirical results but it is not clear what benefits are gained from these results. "}