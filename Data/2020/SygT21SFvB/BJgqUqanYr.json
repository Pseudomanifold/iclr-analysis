{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper empirically analyzed the generalization properties in gradient based meta-learning. The authors provided an experimental evidence against correlation between generalization and flat minima in the context of meta-learning. Then they showed the generalization is correlated with the coherence between the adaptation trajectories in the parameter space. Finally they proposed a new regularizer for MAML.\n\nMain comments:\n\n1. This paper aims at understanding the generalization in gradient based meta learning, which is the key to understand learn to learn problem. However, the significance of this paper is not clear, because there are several recent papers starting analyzing the theoretical aspects in gradient-based meta-learning (at least 3 months before ICLR deadline), for example:\n\n(a) Khodak, Mikhail, Maria-Florina Balcan, and Ameet Talwalkar. \"Provable guarantees for gradient-based meta-learning.\" ICML 2019. \nThis paper proved the convergence and transfer-risk in the first order gradient based meta-learning (e.g Reptile) for convex and non-convex loss. Similar to the paper, it also introduced and demonstrated the **importance of the similarity in the parametric space**(e.g Section 2.2) in meta-learning.\n\n(b) Finn, Chelsea, et al. \"Online meta-learning.\" ICML 2019.\nThis paper proved the regret in gradient based meta-learning and empirically validated in many real world problems.\n\n(c) Denevi, Giulia, et al. \"Learning-to-Learn Stochastic Gradient Descent with Biased Regularization.\" ICML 2019.\nThis paper proved the statistical advantage for adopting the meta initialization (reducing the variance in the statistical learning).\n\n(d) Khodak, Mikhail, Maria Florina-Balcan, and Ameet Talwalkar. \"Adaptive Gradient-Based Meta-Learning Methods.\" arXiv preprint arXiv:1906.02717 (2019).\nThis paper extended the theoretical conclusion in (1) into the non-stationary task-environment.\n\nUnfortunately, the paper does not discuss or even mention these related works. I would like to see more theoretical support.  Particularly in section 5.3.1., they proposed a simple regression problem for fitting sine function under l2 loss, implemented by two layer-MLP, I think it is feasible to derive some theoretical results in this paradigm (e,g. Denevi, Giulia, et al [2018]).\n\n2. As for the technical part, the against correlation between generalization and flat minima in the context of MAML should be theoretically verified, at least the author should propose some theoretical/analytical insights. It is not convincing that the paper only empirically verified on the Omniglot and Mini-ImageNet under cross-entropy loss. It will be much better to design more diverse empirical tests (e.g different network architectures and different tasks like RL /NLP and different loss). It is hard to arrive such an important conclusion by testing only two datasets.\n\n\nMinor comments:\n1. Fig. 3 seems over training (support loss keeps stable after several training epoch) and it is natural the target accuracy dropped. why not using the early-stopping strategy?\n2. The regularizer for MAML seems interesting but only tested in Omniglot dataset, I would like to see more empirical results. \n\nOverall, the main claim in the paper \u201cagainst correlation between generalization and flat minima\u201d is not convincing. I suggest the author think more theoretical supports and discuss the related works, to justify this important conclusion.\n\nReference: \nReptile: On First-Order Meta-Learning Algorithms.  Alex Nichol et.al 2018\nLearning To Learn Around A Common Mean.           Denevi, Giulia, et al  NeurIPS,  2018"}