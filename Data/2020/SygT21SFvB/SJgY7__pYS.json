{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper explores generalization of gradient based meta learning algorithms (MAML in this case). They first look to flatness of minimum as a measure of meta-generalization and find it is not correlated. They then look to similarity of adaption trajectories (3 different signals tested) and find all of these correlate to varying degrees. They then show a meta-regularization strategy designed.\nMotivation:\nI am unclear as to the motivation for all of the similarity metrics. Why do we expect meta-generalization to be correlated with these? If my understanding is correct, these are computed between 2 different tasks. As a result I would not expect there to be any correlation yet there is?! This is puzzling and intriguing but I don't feel this paper illuminates why this is.\nThe general flow of the paper presents a large number of things which imo are distracting. For example the discussion of flatness seems out of place in the context of these trajectory metrics (which occupies most of the paper). There are figures with more than one shot experiments -- e.g.  8ce which doesn't seem to be related to any of the points you are making. Improving the cohesiveness of the work will greatly improve this work.\nExperiments:\nMost of this paper is backed up by experimental evidence. Additionally most of the trends rely on the reader matching two curves -- e.g. figure 7 where the reader is expected to see similarities. As it stands now, these relations are certainly readable but could likely be presented in a much more concise way -- e.g. like figure 8e.\nSecond, considering how important meta-generalization is to this work there are no curves showing training performance anywhere in the main text (though there are a few in the appendix). Showing that a generalization gap exists seems quite important. Infact, showing this gap vs your measurements might be interesting as this seems to be something we care about.\nShowing the fact that a penalty targeting these metrics and that meta-test performance increases as a result is a strong piece of evidence in favor of this work. Expanding on this outside of the toy setting of omniglot would be highly valuable imo. \nRating:\nAs it stands now, I am borderline leaning towards reject. I am willing to change my score however. Improvements in motivation for why these generalization metrics make sense should be considered. I also think the paper can be unified to tell a more compelling story. E.g. distilling the points being made in figures and clarifying the exposition.\n \nOther Suggestions:\nConsider being more careful with the use of generalization. I believe in many places -- e.g. in contributions -- you mean meta-generalization. These two concepts are not the same but are related and this confused me while reading. For example, the flatness section discusses flatness of the minimum post adoption. This is a test of generalization in my mind. Sensitivity of the meta-parameters (learned initialization) involves computing flatness at the test solution as well but is not the same thing and will be computed differently.\nPlease include what the error bars are over in all plots.\nFigure 8 nit: the a, b, c, d labels are extremely small and in an odd place.\n"}