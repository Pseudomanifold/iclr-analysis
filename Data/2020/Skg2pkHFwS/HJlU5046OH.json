{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the settings where multiple agents each act independently in different copies of an environment, without interacting with each other. Each agent uses model-based learning, learning a representation of the world, then learning a controller against the learned world model (learned with an evolutionary strategy of CMA-ES). Different agents will naturally learn world models with different biases - this paper proposes a method that learns a collective policy from the biased models of the different agents, by letting each agent observe imagined rollouts from other agents' world models, training on that data. They find this improves performance over learning within each agent's individual model.\n\nFrom a style perspective, I found this paper hard to read. Many terms are immediately abbreviated into non-conventional single letter abbreviations (C for controller, T for Translator). This extends to the results table - it's quite difficult to understand the results in Table 1 and Table 2 without a careful reading to find what what the abbreviations mean, and I still don't understand what the Overlap (%) and Cov. (%) columns mean in Table 2, despite reading the paragraph explaining it several times. The analysis section also makes a distinction between delusional agents and cheating agents, which I also didn't understand. The GIFs provided demonstrate that the model moves towards the occluded region, and this is described as a novel delusion that hasn't been observed before, distinct from the cheating behavior from Ha and Schmidhuber's world models paper. I don't understand why these are different - in both cases, the agent travels to a region of space where no fireballs appear, and since reward is defined as not getting hit by a fireball, this is optimal behavior for the imperfect world model.\n\nTo created biased representations, the authors apply cutout or zoom in on specific parts of the state. I found this surprising - shouldn't different models naturally lead to biased representation by themselves? The VAEs used should naturally be inaccurate enough to lead to different imperfections.\n\nThere are very few details for the translator T, that translates latent z_i from world model M_i into the correspond latent x_j for world model M_j.  This seems like a very important detail of the method, I could see different translators T leading to very different results, and would have like to see ablations for how different Ts affected the training procedure.\n\nOverall, the question I found myself asking the entire paper was, \"what does this add on top of existing work on training against an ensemble of world models?\" This question has been studied in previous work (a quick search turned up https://arxiv.org/abs/1809.05214 and https://arxiv.org/abs/1802.10592). In these prior works, instead of learning a translator T between representations, we decode an imagined rollout from the VAE for each world model, in the original high-dimensional input state (images, in the case of this paper), and simply train on that data instead. This does not require learning O(N^2) translator models translating z_i to z_j for each pair of world models. I would have liked to see a comparison to this approach, because right now I don't see why you would want to learn translators T in the first place. Simply aggregating rollouts across models would still lead to individually biased world models that can be aggregated to learn a single controller that performs better than any single model."}