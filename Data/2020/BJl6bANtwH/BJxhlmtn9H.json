{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points. The authors define an extrapolation score which is used to estimate the standard deviation of predictions at test points. The extrapolation score is chosen to represent the variability in predictions that would be generated by models with similar training loss. By considering the eigenvectors of the Hessian that are associated with minimum eigenvalues the directions of the loss surface with minimal curvature are found, and perturbations of the parameters in the subspace of minimal curvature correspond to models with similar training loss. \n\nThe authors show that their extrapolation score is proportional to the first order approximation to the change in prediction under a perturbation of the parameters with minimal change in loss. In practice the minimum eigenvalue/eigenvector pairs are computationally challenging to compute for large matrices. For this reason the subspaces with minimal change in loss are computed by finding sets of vectors that are mutually orthogonal to the eigenvectors associated with dominant eigenvalues of the Hessian. \n\nThe method is validated experimentally first through out-of-distribution detection on synthetic data. The authors then test performance by constructing a \"blind spot\" by generating features that are a linear combination of existing features. Data can be generated as either within or out of distribution and the AUC metric can be applied to test model performance. In the final experiment the authors demonstrate the use of local ensembles in active learning. By determining which of the training samples are in the model's blind spots at each iteration and training based on these examples rather than randomly selecting training examples rates of convergence can be increased. \n\nI vote to accept this paper as the proposed local ensemble method builds on a growing body of literature regarding loss-surface inference, providing a new way to connect the shape of the loss surface to extrapolation detection. The theoretical result showing the first order relationship between the standard deviation of extrapolation predictions and perturbations in solutions is a useful insight. \n\nThere are some points that should be addressed for clarity however. Firstly the proof of proposition 1 should be made clearer. This is central to the work of the paper and a more full treatment of the proof here could help illuminate some intuition about the connection to perturbations and variance of predictions. \n\nThe other main point that is not addressed is that in principal we aim to find the subspace associated with minimal eigenvalues, but in practice this is computationally prohibitive. Therefore a space that has a basis that is mutually orthogonal to the dominant eigenvectors is sought (the found subspace), and this could have minimal relation to the subspace that is actually sought (the optimal subspace). Some experimentation showing how the found subspace relates to the optimal subspace would be informative, as well as how sensitive the results are to how much the found and optimal subspaces differ.\n\nSome minor points:\n- Many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved\n- Some explanation about the choice of AUC as a metric would be informative and could help connect to the initial motivation of the method\n- Experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a \"neural network\" is trained, the architecture should be specifically given alongside the discussion of the experiment\n"}