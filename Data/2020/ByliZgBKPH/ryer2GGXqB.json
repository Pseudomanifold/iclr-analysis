{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper considers the problem of entropy-regularized discounted Markov decision processes with discrete state space. Instead of working on the parameter space of policy (\\pi_ij), the paper has proposed to reparametrize with natural parameters (A_ij). The reparameterization trick helps to learn the natural parameters using the natural gradient method.\nThe writing is easy to follow. However, it is not clear what is the benefit of learning policy using path representation compared with other methods in the literature. The paper does not clearly state the motivation of the proposed method.\nThe experimental section presents the convergence of the proposed methods in 3 small problems including decision trees of four levels, the tower of Hanoi problem, and four-room grid worlds. The experiment setting is very simple with a small number of states in the policy. It is not clear how the proposed method is able to scale up the size of state space. Besides, there is no baseline method in the literature to be presented to compare with the proposed method. \n"}