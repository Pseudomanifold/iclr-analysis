{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a reinforcement learning method that exploits the full-depth backup. The policy update based on the full-depth backup is derived for entropy-regularized MDP. The state-action correlation function is introduced and the Fisher information matrix is computed with it. The proposed method is evaluated on tasks with discrete states and actions.\n\nI understand the concept of using the full path for updating the policy, but I do not see significant novelty of the proposed method from the current manuscript. The proposed method looks equivalent to the natural policy gradient with full-depth backup for entropy-regularized MDP, which is a special case of existing methods. \n\nMy concern is that the scalability of the proposed method. The use of the full-depth backup should suffer from the large variance, and I think the proposed method will not work on tasks with the high-dimensional state space. \nThe evaluation is limited to tasks of which the state space is small, and the proposed method is not compared with existing methods.\n\nDue to the unclear novelty and limited empirical results, I give weak reject to the paper in the current form.\n\nI request authors to answer the following questions to improve the clarity.\n\n- Is the proposed method equivalent to use the natural policy gradient with the full-depth backup for a softmax energy-based policy? If they are different, what it the crucial difference?\n\n- I think that the variance of the estimation of the gradient is large when using the full-depth back up. I'm curious about the performance of the proposed method in high-dimensional tasks. However, the evaluation is very limited to simple tasks in which the state space is relatively small compared with tasks commonly used in deep RL papers.\nDoes the proposed method scale to more complex tasks, such as Atari games?\n\n- When using the n-step TD learning, increasing n does not always improve the performance, and n should be set to an intermediate value\nWhat is the motivation of using the full paths for updating the policy? Does the proposed method outperform existing methods? Especially, the comparison with natural policy gradient methods is necessary to show the benefit of the proposed algorithm.\n\nMinor comments:\n\n- In page 2, \"A state-space \\mathcal{X} is composed of states x \\in \\mathcal{X}\" <- Authors may want to replace x with s in this sentence.\n - In page 6, \"0\\lambda < 1\" I think that \"<\" is missing between \"0\" and \"\\lambda\"\n- In page 6, some variables are explained after Equation (10). However, it took me a while to find \"lambda\" in Equation (10), since Equation (10) has 9 lines and many terms. I think the description in page 6 can be improved. For example, I recommend to use \"\\exp\" instead of \"e^\" for readability."}