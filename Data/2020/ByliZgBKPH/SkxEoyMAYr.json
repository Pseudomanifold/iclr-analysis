{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a policy iteration algorithm that implements full-depth, full-width backups in contrast to one-step, full-width methods. The authors go over existing algorithms and talks a bit how their proposal conceptually differs in how it performs said backups. They provide a bit of intuition to help explain their algorithm's derivation. Finally, they provide a few experiments showing that their algorithm works.\n\nMy personal issues are with these experiments. First, I would like to see better comparisons between this method and existing policy iteration methods. I don't have a good sense in which one would choose to use this algorithm over any baseline methods. Is it faster in any sense? Does it produce better policies during certain games? For the experiments themselves, I don't see much clarification of what the various graphs even show. More effort should have been spent analyzing these. \n\nI come away from this work not fully appreciating the impact it is trying to sell me on. I also think the discussion section should have been more fleshed out."}