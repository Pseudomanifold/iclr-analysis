{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Contributions:\n\tThis paper best fits in the literature that explores growing network depth.  The main framework here is to interleave training a shallower network and adding new layers.  This paper (their final algorithm) differs from existing methods in that they: 1) initialize the new layers using standard initialization as opposed to the commonly used zero-init in this literature, 2) grows at a fixed interval , and this interval is short (to avoid the shallower nets being  overly-trained)., 3) uses a large and constant learning rate during the growing phase.  \nEmpirically, they show competitive results on standard image benchmarks.  \nMore interestingly (to me), they provide interesting insights to this paradigm of \u2018growing networks\u2019.  \n\nComments/Questions:\nSection 2 of the paper describes the proposed method is good details.  \n\nSection 3 of the paper describes the experiments.  Since for now I see the contribution of this paper is mostly empirical, I will give my detailed feedback here.\n3.1 (Suboptimum of Network Morphism (NM) )\nTable 2 shows NM is worse than training from scratch, and this isn\u2019t fixed by AdamInit.\nTable 3 shows c-AutoGrow (in between p-AutoGrow and NM) still does worse than from scratch, pinpoint the problem to converged subnetworks.\n3.2  (p-AutoGrow)\nTable 3 shows +Constant LR helps, then +RandomInit helps. \nTable 4, 5 shows +Periodic gets the best performance.  \n*Suggestion* The found net is Table 4,5  are significantly deeper than those in Table 2,3, also there are no \\Delta.  Also, although within this write-up those are the highest numbers, in the broader literature of NAS this doesn\u2019t seem to be that good.  From a quick search, many methods in the Table 1 of [1] seems to give >96% accuracy on CIFAR10, some even close to 98%.  It might be good to at least discuss why this method is limited from achieving that.\nI do like the finding that ZeroInit is unnecessary, as reported in the rest of this subsection.  However, it is unsatisfying to me that many past works (as cited by the authors) required this ZeroInit without ever trying RandomInit.  \n*Suggestion* I would love to see a more thorough discussion on why GauInit is better than ZeroInit, not just more numbers.  For example, even just text description on why past works found ZeroInit useful, and countering some of those claims would be interesting.  A more controlled experiment rather than training 2 networks by swapping this would be interesting.  ZeroInit is used in more context than just NM.  For example, good flow models like Glow also uses such initialization, for likely a different reason, but I wonder if findings here have any implication for ZeroInit more generally.\n\n3.3 (Many datasets)\nTable 6 is a strong result.  One odd thing is how deep the found-net has to be for MNIST.  This actually suggest to me that AutoGrow does not have the ability to stop early when it can.  And in the discussion, the authors argue that by using a better sub-module like in NAS they can do better.  This raises the question why the authors did not choose to use it.  I would believe it if the proposed method has obvious reasons that it can transfer to different architecture, but for now I cannot jump to the conclusion that, say, p-AutoGrow with GauInit will necessarily work when using a different sub-module.  Perhaps, the reason past NM works didn\u2019t use a GauInit was also due to the fact that past sub-modules didn\u2019t work with GauInit.  \n\n3.4 (Scale to ImageNet) It\u2019d be good to add reference results from other papers.  \n\nMinor details:\n\n\nThere are some good contents in this work, but for it to be a strong *empirical* contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show state-of-the-art results.  For it to be a strong *analysis* paper, it should expanded, at least addressing some of the *suggestions* mentioned above. \nUnrelated to my evaluation of this work, reading this makes me think we should (and can) develop theoretical understanding to this paradigm of growing networks.\n\n\n\nReferences:\n[1] https://arxiv.org/pdf/1905.13360.pdf\n\n"}