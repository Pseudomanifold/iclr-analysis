{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \\hat{x} in a Kullback-Liebler divergence involved in the IB optimization criterion.\n\nInterestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution.\n\nGood properties of the exponential families (existence of non-trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.\n\nThe paper is globally well written and clear, and the maths are rigorously introduced and treated. Two minor comments would concern the definition of the Mutual Information (MI), which could be recalled to help the unfamiliar reader and improve self-containedness, and the notation of the expected value (\\langle \\rangle_p), unusual in Machine Learning where \\mathbb{E} is often preferred.\n\nAnother point that could be enhanced is the intuition behind the IB and dualIB criteria: a small comment on their meaning/relevance as well as the rationale/implication of the probabilities permutation would be valuable addition.\n\nExhibiting a link with variational autoencoders (VAEs), and expliciting the differences in a VAE framework between the two criteria could also represent an interesting parallel, especially for machine learning oriented readers.\n\nOne of the main drawback of the present paper is however the lack of convincing experiments. Graphs showing the good behavior of the introduced framework are ok, but the clear interest of using dualIB rather than IB could be emphasized more. In particular, it does not seem that the issues about IB raised in the abstract (curse of dimensionality, no closed form solution) are solved with dualIB. Furthermore, dualIB optimizes the MI of the predicted labels, which is claimed to be beneficial contrary to the MI on the actual labels. However, no empirical demonstration of this superiority is produced, which is a bit disappointing."}