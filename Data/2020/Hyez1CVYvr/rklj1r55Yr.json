{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes a new loss function to train the network with Outlier Exposure(OE) [1] which leads to better OOD detection compared to simple loss function that uses KL divergence as the regularizer for OOD detection. The new loss function is the cross entropy plus two more regularizers which are : 1) Average ECE (Expected Calibration Error) function to calibrate the model and 2) absolute difference of the network output to $1/K$ where $K$ is the number of tasks. The second regularizer keeps the softmax output of the network uniform for the OE samples. They show adding these new regularizers to the cross-entropy loss function will improve the Out-distribution detection capability of networks more than OE method proposed in [1] and the baseline proposed in [2].\n\n\nPros:\nThe paper is written clearly and the motivation of designed loss functions are explained well.\n\nCons:\n1- The level of contributions is limited.  \n\n2- The variety of comparison is not enough. The authors did not show how the approach is working in compared to the other OOD methods like ODIN[3] and the proposed method in [4].\n\n3- The experiments are not supporting the idea. First, the paper claims that the KL is not a good regularizer for OOD detection as it is not a distance metric. But there is no experiment or justification in the paper that supports why this claim is true. Then the second contribution claims that the calibration term that is added to the loss function improves the OOD detection as well as calibration in the network, but the experiments are not designed to show the impact of  each regularizer term separately in improving the OOD detection rate.  Figure 2 also does not depict any significant conclusion. It only shows that the new loss function makes the network more calibrated than the naive network. This phenomenon was reported before in [1]. It would be better if the paper investigated the relation between the calibration and OOD detection by designing more specific experiments for calibration section.   \n\nOverall, I think the paper should be rejected as the contributions are limited and are not aligned with the experiments.  \n\nReferences\n[1]Hendrycks, Dan, Mantas Mazeika, and Thomas G. Dietterich. \"Deep Anomaly Detection with Outlier Exposure.\" arXiv preprint arXiv:1812.04606 (2018).\n\n[2] A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks, ICLR2016.\n\n[3] Liang, Shiyu, Yixuan Li, and Rayadurgam Srikant. \"Enhancing the reliability of out-of-distribution image detection in neural networks.\" arXiv preprint arXiv:1706.02690 (2017).\n\n[4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018."}