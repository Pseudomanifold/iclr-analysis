{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a method for transferring knowledge from a complex, high-performing model to a small/lower-performing one. The approach is based on reweighting the train examples according to the ratio of confidences of the complex and simple models, and then retraining the simple model on the reweighed dataset. \n\nPros:\n* Some interesting ideas motivating the approach\n\nCons:\n* An important error in the main result and various other issues (imprecise statements, lack of details) raise doubts on the theoretical contribution of the paper.\n* Limited novelty. Contribution boils down to importance reweighting via a ratio of probabilities.\n* Limited awareness of related work.\n* Experimental framework lacks meaningful baselines and important details .\n\nBased on the points above, I have to recommend rejection of this paper to ICLR. \n\nDetailed Comments:\n* The claim that per-sample hardness reweighting is novel to this work is a bit of a stretch. Most reweighting-based methods for distillation use some form of this idea, though how each method estimates \u201chardness\u201d varies, ranging from raw confidence scores of the teacher model (e.g., classic distillation), to scaled versions of it, to a ratio of this and the student\u2019s score (this work).\n* More generally, there\u2019s quite a few very relevant related works which are not cited here (see list below for a non-exhaustive list).\n* I believe the inequality used to prove Lemma 3.2 is not true in general. It\u2019s easy to come up with counterexamples for which w>=1 and x>1 yet log(wx) <= w log(x) does not hold. For example, for w=2, any x in (1,2) will not satisfy this. In general, this inequality is only true if x >= w^{1/(w-1)}. I would be prepared to pass this off as a minor error, were it not for the fact that the w and x in the proof of Lemma 3.2 could quite conceivably be outside this range, i.e., rendering the inequality (and therefore the bound) invalid.\n* There are various other technical aspects of the paper that are either non-rigorous or lack details. For example, Lemma 3.1 requires more information. For which y does w>= hold? Is it just for the true class y*? The proof is equally obscure, and seems to use the same problematic inequality of Lemma 3.2 (for the cross-entropy loss). \n* The statement of Lemma 3.2 is confusing. The last sentence should read loss smaller than *that* of theta*. Also, I don\u2019t see the optimality of theta* being used anywhere. The statement could very well be phrased in terms of any two models, one of which has a lower loss than the other. \n* Assuming the complex \u201cteacher\u201d classifiers are very accurate (which is assumed several times throughout the paper), re-weighting training examples by max(1, p_c / p_theta) does not provide too much information beyond the ground truth labels. This is a core problem in knowledge distillation, already identified as early as Hinton et al (2015), and which is often alleviated by some form of temperature annealing or by operating on pre-softmax logits rather than final probabilities. That doesn\u2019t seem to be the case here, so I\u2019m quite puzzled by the \u201cintuitive justification\u201d provided in this work.\n* The discussion in the first paragraph of pg 4 is a well known dichotomy (namely, up-weighting easy or hard examples) in the distillation/boosting/instance-reweighting literature. This discussion would really benefit from awareness of previous work discussing this duality (e.g., (Bengio et al 2009; Ren et al 2019)\n* Where and how is the ordering of the \u201cgraded\u201d classifiers used? In step (3) of Algo 1 all \\zeta\u2019s seem to be weighted equally, so I\u2019m missing why the classifiers need to be delta-graded.\n* I find it quite surprising that in almost half of the datasets in Table 1, distillation yields the exact same performance as just training the Simple Model, and sometimes even degrades the performance. Section 4.1 points out that this is actually \u201can equivalent model to Distillation\u201d, but no further explanation is given, so it is hard to conjecture what might be the problem. Futhermore, as the authors point out in the introduction, Distillation alla Hinton 2015 usually assumes both teacher and student models are soft predictors (e.g., neural nets), but the models used in the UCI experiments trees and SVMs, so I wonder how exactly they did this. \n* For problems involving distillation into simple interpretable models (like trees), such as the UCI experiments, there are many reasonably strong baselines from previous work that could have been compared against (e.g. Frosst and Hinton 2017), which could have provided a more meaningful evaluation.\n\n\n[1] Learning to Reweight Examples for Robust Deep Learning, Ren et al. ICML 2018\n[2] Distilling a Neural Network Into a Soft Decision Tree, Frosst and Hinton, 2017\n[3] Born-Again Neural Networks, Furlanello et al., ICML 2018\n[4] Fidelity-Weighted Learning, Deghani et al., ICLR 2018"}