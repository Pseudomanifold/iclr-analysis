{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose using a recent method for adversarial inverse reinforcement learning (AIRL) for the task for generating high-quality image captions. Leveraging the GAN framework, a discriminator is trained to distinguish real captions from those produced by the generator, while the generator is optimized with policy-gradients (REINFORCE) to maximize the pseudo-reward from the discriminator. The main difference from prior work seems to be that the discriminator acts on a word-level, rather than sentence-level (as done, for instance in Dai et. al. 2017). Correspondingly, the generator policy is updated with the objective of 1-step reward maximization (more like contextual bandits), rather than with a long-term sequential decision-making objective (as done in Dai et. al. 2017). The evaluation is done using 2 data splits \u2013 standard and robust, with various metrics such as SPICE, CIDEr, BLEU, CHAIR. Diversity analysis and ablations are also performed to dissect the performance of the proposed approach. \n\nMy 2 main issues with the paper are confusing motivation (in section 1) and various imprecise parts (in section 3 and 4).  \n\n1.\tThe authors argue that current GAN-based captioning models provide ill-defined rewards due to the \u201creward ambiguity problem\u201d. This problem is not explained or motivated well in the paper, but instead the readers are referred to the AIRL paper. \u201cReward ambiguity\u201d in inverse-RL arises because there could be many reward functions that yield the same optimal policy. The AIRL algorithm recovers one of these possible reward functions, and since such a recovered reward could be shaped by the environment dynamics, AIRL attempts to disentangle the reward from the dynamics. The motivation there is to use the recovered reward on a new system with different transition dynamics. In the context of this paper though, I would like to understand the angle of reward ambiguity. The authors disentangle the sentence reward into word-wise rewards; however, I\u2019m not sure if there\u2019s any relation between this and the disentanglement done in AIRL for solving reward ambiguity.\n\n2.\tOne of the objectives is learning compact rewards. It is claimed that addition of a constant term to the reward provided to the generator policy results in this, but what\u2019s the intuition behind this? As for evaluation, it needs to be shown that words with similar semantics have similar discriminator score. How do we conclude this from Figure 2.? Also, please include Up-Down method results in Figure 2.\n\n3.\tSection 3 questions\n     a.\tHow is state s_t defined? It is very hard to follow sections 3 and 4 without a clear definition and example for this.\n     b.\t\u201cFinn et. al. 2016 proved that IRL is mathematically equivalent \u2026\u201d --- this is imprecise. Maximum-Entropy-IRL is equivalent to the GAN formulation, not general IRL. \n     c.\tp_theta(a,s) is referred to as \u201creward distribution\u201d. I don\u2019t think it\u2019s a distribution.\n     d.\tEquation 3. AIRL defines g only as a function of state (s_t) for the disentanglement, and not like what the authors have written.\n     e.\tEquation 4. How is s_t sampled?\n\n4.\tSection 4 questions\n     a.\t4.1 says discriminator \u201cmaximizes the divergence\u201d. This doesn\u2019t seem correct.\n     b.\tf is referred to as state-value. This doesn\u2019t seem correct.\n     c.\tShouldn\u2019t the -1 term in Equation 8 disappear under expectation?\n     d.\tDon\u2019t understand how second line of Equation 11 is arrived at.\n\nThere are quite a few other sources of mathematically imprecise writing that I noticed. I would recommend the authors to be more robust in their presentation.\n"}