{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of data-to-text generation so that the generated text stays truthful to the data source. The idea of the paper is use a learned confidence score as to how much the the encoder-decoder is paying attention to the source. The paper includes several components, 1. unconditioned language model to incorporate the confidence score, 2. use calibration techniques to adjust the output probability; 3. variational bayes objective to learn the confidence score. \n\nThe paper has good motivations and is quite well-written. The problem is of great pragmatic interest. In the experimental part, the authors demonstrate the effectiveness of the proposed algorithm.\n\n1. For training part, regarding the language model and  variational bayes objective being trained jointly, does it have convergence problem? What is the motivation of not training them jointly?\n2. Will the code be released and the human evaluation be published?\n3. There are some importance baseline missing, such as [1], [2], [3]\n\n[1] Marcheggiani, Diego, and Laura Perez-Beltrachini. \"Deep graph convolutional encoders for structured data to text generation.\" arXiv preprint arXiv:1810.09995 (2018).\n[2] Ratish Puduppully, Li Dong, and Mirella Lapata. \"Data-to-text Generation with Entity Modeling.\" arXiv preprint arXiv:1906.03221 (2019).\n[3] Ma, Shuming, et al. \"Key Fact as Pivot: A Two-Stage Model for Low Resource Table-to-Text Generation.\" arXiv preprint arXiv:1908.03067 (2019).\n"}