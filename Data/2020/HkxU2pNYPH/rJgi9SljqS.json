{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a method for conditional text generation that has higher factual precision, minimizing hallucination of facts. The method involves predicting confidence of generation at each time step and using this confidence measure to skip tokens during generation and calibrate output probabilities in test time. Their method achieves SoTA performance on automatically measured precision and human evaluated \"faithfulness.\" However their method does see a drop in recall (automatic metric and human evaluation).\n\nComments and issues,\n- The intuitive explanation for the confidence score is a little confusing. In Section 4, page 3, you say that \"If a token is likely a content word (i.e. when its generation probability by the encoder-decoder is much higher than the unconditioned language model), but the attention score is low, then the token might not be predicted based on the source, and could be hallucination.\" However, this doesn't seem like an airtight conclusion. Isn't it possible that the base-LM and enc-dec model have similar probabilities for a content word with the enc-dec attention being low? This seems possible given your observation that low attention to the source is what may be causing content hallucination. This same thing is essentially restated in section 4.1 \"we expect P(y_t |y_<t, x) to be higher than P(y_t | y_<t) for content words so the confidence score will largely depend on the attention score\", which seems more tangled up since P(y_t |y_<t, x) inherently depends on the attention score. This is all clarified when you explain the alteration made to the base-LM. I would recommend rewording/rearranging some of the earlier explanation for the efficacy of the confidence score since it seems that the alteration to the base-LM is an essential part of the explanation. \n- Need some explanation for Equation 6. I don't really get the intuition behind it.\n- The presented results are pretty good! However, I would like to see some numbers on average score across a few runs.\n- It would also be good to see results on one more dataset like E2E.\n- Provide a little more detail on human evaluation, you don't even mention if the evaluation was done with crowd-workers or another pool of people like grad students. How many annotators? What is the inter-annotator agreement? What was the prompt/structure? Human evaluation of models is notoriously difficult, more details would give some more weight to the results.\n\nI think this is a well written paper with thought out experiments. I recommend it be accepted to ICLR. I'd also be curious to see some future work that improves, or at least maintains recall, while keeping the higher precision.\n\nMinor requests/recommendations: \n- Include more examples of generations. Could be an appendix.\n"}