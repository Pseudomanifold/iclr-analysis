{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents a new gradient update rule for neural net weights, which is a result of relaxing the usual SGD to a regularized linear regression between the descent direction and the original gradient. The method is interpreted as some kind of input normalization similar to Batch Norm later in the paper.\n\nAlthough the method seems to work and the idea is interesting, I\u2019m not entirely convinced by the experimental evidence. There are some technical concerns:\n\n- In section 2.1, I\u2019m only familiar with the MNIST dataset. Although the proposed method achieves slightly better performance than vanilla SGD, it\u2019s pretty weird the accuracy is only 98.xx% as a very simple CNN could easily achieve >99% accuracy on MNIST. This might be due to the simple network architecture used. A more modern architecture would be preferable in those experiments. Also the improvement on CIFAR seems to close to tell.\n\n- The writing seems to need some improvement. It\u2019s sometimes hard to follow the text, and some result tables (Table A1,A2,A3) are in the appendix not the main text. It might be better to reorganize the text and present everything about the method (fast approximation, CNN, interpretation as normalization) together.\n\n- The authors name the proposed method \u201cdeep gradient boosting\u201d, but I\u2019m not entirely sure how it is related to gradient boosting. Eq. 3 concerns sum of weak predictors, but how is this related to Eq. 4? The neural net F(x) doesn\u2019t seem to be exactly a sum of weak learners. I don\u2019t quite follow Eq. 5 \u2013 There seems to be missing something.\n\nTypos:\n- Eq. 5, d L_t instead of d L_T, and should there be a d \\phi / d w_ij term?\n- Eq. 7, v_t => v on the right-hand side.\n- Eq. 12, should be \\sum_i=1^B\n"}