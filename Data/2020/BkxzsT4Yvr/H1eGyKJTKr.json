{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an a new idea of treating the back propagated gradients using chain rule as pseudo residual targets of a gradient boosting problem. Then the weight update is done by solving the boosting problem using a linear base learner. Furthermore, to reduce computational cost incurred by solving the boosting problem, an idea proposed is only keep the diagonal terms of the matrix inversion involved. \n\nIt is an interesting idea to look at gradients different and update weight accordingly. The experimental results shows some improvements. \n\nThe paper is written in a way that it looks like two papers and connected with the same idea DGB. It is a bit confusing during the first pass of reading. \n\nIn all experiments, can you also report running times? how much is the overhead? and  how much reduction in training time if only using diagonal terms? \n\nDoes only using diagonal terms hurt model performance?\n\nFor 2.1.1 MNIST, why is alpha the larger the better while the others are not this case?\n\nFor 2.1.5 AIR, why spit training and test equality?\n\nFor 4.3 why choose heavy model VGG? not ResNet? \n\nIn Figure 1, is the spike of BN normal?\n\n"}