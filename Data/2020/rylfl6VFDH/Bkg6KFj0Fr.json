{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new method for learning to make neural networks sparse by adopting a beta-Bernoulli prior with variational dropout. One motivation for the method is to make the dropout rate dependent on the input, by introducing the input to a layer as a factor in the computation of the Bernoulli parameter for the layer which controls whether some parts of it will get turned off. \n\nThe motivation for the goal of making neural networks sparse is clear, since it can potentially lead to significant memory and computation savings (although given that hardware architectures typically used for executing neural networks generally expect dense computations, it may be difficult to realize these savings in practice). Furthermore, it's satisfying to see that the underlying method has a strong probabilistic justification.\n\nHowever, while a significant amount of the paper was devoted to the input-dependent version, it was unclear from the empirical results whether there is much actual advantage to the additional complexity.\nEmpirical validation of the method with experiments on larger datasets such as ImageNet would lend further credence to the viability of the approach.\nI was also somewhat disappointed to see that in order to actually accomplish the pruning, the method involves a naive approximation (equations 14 and 19). I think an interesting experiment would be to see how the method performs when computing the expectation on equation 13 through empirical samples; given the savings in FLOPs and memory, we could run the network with several samples even without exceeding the original computation budget.\nSome other design choices (such as the factorization of q, and equation 17) seem somewhat arbitrary, so I would also prefer to see further justification or a sketch/empirical evidence of why alternative methods may not work as well.\n\nFor the above reasons, I am rating the paper as weak accept.\n\nSmall note: please use \\citep and \\citet (instead of \\cite, when using natbib) properly throughout the paper so that citations are formatted correctly."}