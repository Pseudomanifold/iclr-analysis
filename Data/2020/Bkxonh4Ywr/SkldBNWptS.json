{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a family of variational distributions in which the variational covariance matrix is parameterized as RR', with R_ij being nonzero only when j is a neighbor of i as defined by prior covariance. In other words, data points are only allowed to have nonzero posterior covariance if they are highly correlated a priori. This results in a sparse factor of the covariance matrix which can be used for efficient computation. Rather than being parameterized directly, the variational parameters \\mu_{i} and R_{i, n(i)} are parameterized using a GCN: the labels and prior covariance information are supplied to the GCN, which produces the mean for a data point x_i and the |n(i)| nonzero elements of the covariance factor R.\n\nOverall, the idea is interesting in the sense that a variational family with enforced sparsity likely leads to a reasonable probabilistic model. However, I have a few concerns about the execution.\n\nFirst, as a minor point, in my opinion, the approximations eqns. 3-7 are not sufficiently motivated, and are essential to the method as they allow for stochastic optimization. It would be useful to see an empirical analysis of the tightness of the additional approximations, as well as a generally expanded discussion in this section. Beyond this, the method is highly engineered but the only ablation study done of the various components is Figure 3, which merely offers an analysis of convergence speed, but not on final model performance. Both ideas introduced in the paper (localization and amortization) can be used independently of the other.\n\nMore importantly, I believe the experimental evaluation should be substantially broadened. At present, three datasets are considered and on one of them (MNIST) three methods considered are within the error bars of each other: the bolding in Table 3 is inappropriate. Many popularly used benchmark datasets for sparse GP methods are widely available, and it seems particularly essential to include datasets larger than the ones considered here, since exact GPs can trivially be trained on these datasets in a matter of seconds (at least for the regression tasks). Again ignoring the single classification task, Variational GP methods are usually only considered for regression for much larger datasets. I suspect that, with proper hardware (e.g. a GPU) and truly large datasets much of the speed advantage enjoyed by LAIN as reported in the paper will be lost to overhead."}