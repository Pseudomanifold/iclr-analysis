{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1) Summary\nThe manuscript proposes a k-nearest-neighbor (KNN) Gaussian process (GP) approximate inference scheme to render computations more scalable.\n\n2) Quality\nAlthough the application is clear and the methodology is well established, the quality of the submission can be improved by a more thourough empirical evaluation in particular a proper evaluation in terms of runtime, approximation accuracy and comparison to baseline methods.\n\n3) Clarity\nThe manuscript is reasonably well written and most of the technical and experimental content is accessible. There are some typos and some glitches in the notation. See \"Details\". There are some open issues regarding the KNN computations. See Questions.\n\n4) Originality\nThe use of a localized (in the KNN sense) set of inducing inputs to improve GP inference but the impact needs to be better quantified empirically.\n\n5) Significance\nThe proposed method is aiming at improving the established setting of GP inference. The modification is rather marginal and the empirical evaluation makes it hard to judge the relative merit of the proposal.\n\n6) Reproducibility\nThe data is from published sources (toy, ebirds, precipitation, digits) and the code for the baseline methods and for the LAIM method itself is available. However, there is no code for the experiments, which makes the results slightly tricky to exactly reproduce.\n\n7) Evaluation\nThe evaluation does not consider simple baselines like dense GPs or sparse approximations such as FITC and VFE. Also plain NN should be considered.\n\n8) Questions\n  A) How do you set the parameter delta?\n  B) How are the nearest-neighbors computed in the first place? Does it require computing the dense covariance matrix?\n  C) How accurate is the NN computation? How much of the computational effort (percentage) of the overall pipeline is required for the NN computation?\n  D) How much better is the proposed approach than directly using NN predictions?\n  E) Does \"most correlated\" in the footnote on page 2 really mean correlation or is it about covariance? The latter would involve a diagonal rescaling of the covariance matrix.\n\n9) Details\n  a) Abstract: \"Gaussian Processes\" -> \"Gaussian processes\"\n  b) Intro: \"GP poses a Gaussian prior\" -> funny sentence, \"with some special \"\n  c) Intro: \"with some special structures\" -> fix\n  d) Background: \"q(f)~N(mu,V)\" -> imprecise notation, rather \"q(f)=N(mu,V)\"\n  e) Background, footnote: \"distance metrics\" -> Are you talking about \"distance\" or \"metric\"?\n  f) \"Experiment\" -> \"Experiments\"\n  g) References: capitalization not correct e.g. Gaussian, Fourier, Bayes"}