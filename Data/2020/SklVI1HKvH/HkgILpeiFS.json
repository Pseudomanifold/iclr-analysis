{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper is about decoder networks for 3D point cloud data, i.e., given a latent vector that encodes shape information, the decoder network outputs a 3D point cloud. Previous approaches like PU-Net, or TopNet have used an MLP with a fixed number of output points for this task. The approach in this paper takes inspiration from the work of Li et al. and extends a sample based method for the decoder network. Specifically, the core contribution is to sample from a Gaussian distribution with the latent vector as mean and a variance computed by a small network conditioned on the latent vector. This sampled vector is then feed to an MLP to generate a 3D point estimate. The sampling approach is compared to the method of Li et al. (concatenating noise values to the latent value), and to directly adding Gaussian noise to the latent vector on auto-encoding shapes from the ModelNet40 dataset. Further studies on the impact of the noise on the reconstructed shapes are presented.\n\nI argue for a weak reject of this paper (in its current form) because (1) the presented method seems to be too incremental, and (2) I am missing experiments that compare to other state-of-the-art methods (e.g. on the task of 3D reconstruction from single images).\n\nRegarding the first point, let's start with the related work that is missing some very relevant paper. I would like to see the relation and comparison to implicit surface models (e.g., Park et al. \"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\", or Mescheder et al. \"Occupancy Networks: Learning 3D Reconstruction in Function Space\") that sample not based on a random vector, but on the 3D position itself. Further, I see Fan et al. \"A point set generation network for 3d object reconstruction from a single image\" in the references, but not where it is discussed in the paper. Further, Yang et al. \"FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation\" and Groueix et al. \"AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation\" seem to be quite relevant.\nFurther, the core contribution is to add a noise vector to the latent vector, instead of appending it to the latent vector as in Li et al. The presented evaluations in Fig. 3 and Fig. 4 show that this might be slightly beneficial, but I am not convinced how this performance increase would translate to a other tasks.\n\nThis is also my second major point of criticism: The presented evaluations are nice, but also seem like a toy problem. What would have been interesting is to compare the presented decoder network to state-of-the-art methods on other tasks presented in the literature. One example might be 3D reconstruction from a single image (cf. Fan et al.), or shape completion (cf. Park et al.). Even for the auto-encoding setting it would be possible to compare to other methods (cf. Yang et al., PointFlow).\n\nOne part that might lower the performance on the above mentioned task is the inferior encoder network (PointNet) that takes no neighbourhood information into account. There are quite a few stronger methods that can be considered, e.g. PointNet++, or any of the recently proposed point convolution methods.\n\nIn Fig. 3 and 4 the loss seems to be quite large, given that a Chamfer distance is used and the shapes are scaled to fit into a unit sphere. What function is used for dist? And are there severe outliers in the generated point clouds?\n\nOn page 5 it is stated that the Chamfer distance is used due to its relative speed. Relative to what?\n\nOn page 7 it is stated that the MLP results appear to be more evenly distributed. The last row in Fig. 5 suggests otherwise on the back of the chair. Also, the NoiseAdd and NoiseAppend seem to miss quite some details in the second row (or are just more noisy in general), which one is the proposed NoiseLearn?\n\nThings to improve the paper that dis not impact the score:\n- On the second pager, first line, there is an inconsistency in the citing style (use of parentheses)\n- What is meant with a \"parameterized ReLU\" on page 4?\n- The point cloud visualizations can be greatly improved. The grid is not necessary and the images are very small, making it hard to see details."}