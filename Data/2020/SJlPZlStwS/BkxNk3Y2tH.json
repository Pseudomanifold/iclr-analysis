{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Paper summary: The authors propose to improve the robustness of neural networks by encouraging them to rely more on shape information (as opposed to texture). Specifically, the approach involves training classifiers on processed versions of the input image---either just the output of a (robust) edge detector in EdgeNetRob, or a combination of this with GAN-based infilling in EdgeGANRob. The authors evaluate these approaches in terms of clean and robust accuracy under the threat models of l-infinity adversarial attacks, backdoor attacks, and specific forms of distribution shift on the Fashion MNIST and (binary) CelebA classification tasks.\n\n\nComments: I find the high-level motivation of the paper compelling. As discussed in the paper, it has been found that there is a mismatch in the kind of features that deep networks rely on for classification, when compared to humans [Geirhos et al, 2019; Ilyas et al., 2019]. Moreover, it has also been shown that encouraging networks to rely more on shape cues may boost robustness to certain corruptions [Geirhos et al., 2019]. This paper aims to put forth a general framework to encourage networks to rely more on global structure in the input images.\n \nMy main concerns with the paper are as follows:\n \nA. The individual components of the approach are poorly motivated. \n \n1. In particular, it is not obvious that preprocessing using edge detection is enough to suppress adversarial perturbations/watermarks/distribution shift in general (more on this in B). The authors claim that edges are robust features, but do not back this up with sufficient evidence.\n\n2. The images generated post GAN-based infilling seem to have similar texture (and thus local structure) to the original input images. The authors do not provide sufficient explanation for why classifiers trained on this generated data would be any more robust (or more broadly, reliant on global structure) than classifiers trained on the original inputs.\n \nB. The evaluation of the proposed approach is not sufficiently convincing. \n \n1. Datasets: Experiments are performed only on Fashion MNIST and *binary* CelebA, which are a) not standard datasets in robustness literature and b) simple tasks. In particular, the fact that images produced by EdgeNetRob (basically the output of an edge detector) get comparable clean accuracy to training on the actual data (cf. Table 2) suggests that the classification task is just not hard enough. For instance, on typical deep learning datasets such as ImageNet, it seems improbable that we can get good accuracy using just edge information. Thus, the authors need to evaluate their approach on standard (harder) tasks such as CIFAR-10 or ImageNet classification.\n\n2. Choice of watermarks/distribution shift: What was the motivation behind the specific attacks considered in the backdoor attack section, and in the distribution shift sections? To me, these seem like specific attacks under which the proposed approach is more likely to succeed. \n- For instance, in the backdoor attack literature, the pattern is often small but somewhat perceptible [Gu et al, 2017: arXiv:1708.06733] and such a pattern would easily bypass the edge detector in EdgeGANRob. Thus, the performance reported in the paper seems somewhat specific to the attack considered (an imperceptible watermark) and would probably not generalize to patterns that are typically considered in the literature. \n- Similarly, the kinds of distribution shift that are considered are such that the edge information is preserved and thus may not provide a clear picture of the performance of the proposed defense. I would be curious to see how this approach performs in the face of other corruptions such as JPEG compression, fog, snow, etc. from the benchmark in Hendrycks and Dietterich [2019].\n\n3. Adversarial evaluation: The authors should report accuracy under black-box attacks and also white-box adversarial accuracy as a function of epsilon (e.g., for the Fashion MNIST model, plot adversarial accuracy as a function of the attacker eps from 0/256-256/256). In the Appendix, the authors mention images are scaled to [-1, 1], whereas in the robustness literature it is usually [0, 1]. This matters when you consider the eps used for attack and compare to other work: are all the networks in the paper including the adversarially trained ones also trained with images in this range?\n\n\nThus, while the problem the paper tries to tackle is an important and interesting one, I am not yet convinced about the effectiveness of the proposed approach. To make the empirical evaluation more rigorous and convincing, I think the authors should: \n\n(1) Repeat the adversarial robustness experiments on more commonly-used datasets such as ImageNet and CIFAR, and also evaluate robustness of their models more thoroughly, using techniques like black-box attacks.\n(2) Repeat the poisoning experiments with watermark patterns which are more standard in the literature.\n(3) Evaluate their approach on other forms of distribution shift such as the common corruptions benchmark from Hendrycks and Dietterich.\n\nAs of now, I am recommending rejection, but I would be willing to reconsider my score if the authors performed the above-mentioned experiments. \n"}