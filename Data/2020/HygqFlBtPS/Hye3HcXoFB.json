{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Strengths:\nThis work proposed two regularizers that can be used to train neural networks that yield convex relaxations with tighter bounds.\nThe experiments display that the proposed regularizations result in tighter certification bounds than non-regularized baselines.\nThe problem is interesting, and this work seems to be useful for many NLP pair-wise works.\nweaknesses:\nSome presentation issues.\nThe dataset, MNIST, is not good enough for a serious research. \nMore datasets need to be added to the experiments in this paper.\n\n\nComments:\nThis paper proposes two regularizers to train neural networks that yield convex relaxations with tighter bounds. \n\nOverall, the paper solves an interesting problem. Though I did not check complete technical details, the extensive evaluation results seem promising. \n\n1. There are some presentation issues that can be addressed. For example, on page 8, the sentence of \u201cthe family of 10small\u201d misses a blank space.\n\n2. In the experiments, the dataset is not a good one for evaluating the performance of the proposed idea.\n\nIn conclusion,  at this stage, my opinion on this paper is Weak Accept. "}