{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an analysis of recurrent networks with random weights, showing that certain input phases can lead to more stable neuron trajectories over time. To explain the phenomenon, the author\u2019s present a mean field analysis, and experimentally show that certain input phases lead to more robust readouts.\n\nMajor comments:\n* I found the paper poorly motivated and hard to follow. I didn\u2019t understand the connection to disentangling, which focuses on learning nonlinear (not fixed) feature extractors. A more concise statement of the problem setting and why it is interesting would be useful.\n* Much of the analysis focuses on the canonical angle between the driven subspace and the spontaneous subspace, but this only makes sense when there\u2019s primarily one dimension of variance in each space. What are the dimensionalities of the subspaces you are analyzing? For chaotic activity, shouldn\u2019t it produce similar variance in all dimensions in which case the principal directions are not well defined? I\u2019m generally confused why there are directions with dominant chaotic activity, and how that depends on initial conditions / parameters of the network.\n* The mean field analysis is done in a regime which doesn\u2019t make sense, and the accuracy of the approximations made are never evaluated on any networks (especially the Taylor series expansions about g = 1 + delta isntead of g = delta).\n\nMinor comments:\n* Intro: I didn\u2019t understand the connection between your example of untangling images and the work you present here on entraining neural dynamics.\n* Please define \u201cspontaneous\u201d vs. \u201cchaotic\u201d activity. Use them many times but never define what you mean.\n* You say W is sparse, but if it\u2019s just drawn iid from a Gaussian it doesn\u2019t meet any of the typical definitions of sparsity\n* If the driven a\n* Eqn 2 doesn\u2019t make sense as written. The first principle angle would be the first element of arccos(SingularValuesOf(V1^T V2)), without the indices a and b and the min.\n* Canonical angles go from 0 to pi/2, but the y-axis in Fig 1 goes 0 to 180 for subspace orientation\n* The procedure you use to identify input phases that align with chaotic activity requires sweeping all phases and then computing the one that has the lowest principle angle. This doesn\u2019t seem like a scalable or useful learning procedure.\n* Are the <PC1 notation denoting perpendicular to PC1? Please note that in the text.\n* Fig 2: instaed of using trajectory distance, can you report results on a task where the different inputs are explicitly trained to produce different outputs, and then report MSE vs. the target outputs?\n* I didn\u2019t follow the discussion of attractor states or locally stable channels, and you don\u2019t define either of those concepts. \n* \u201cCan be expanded with Taylor series for small values of g\u201d: g should be near 0, not near 1. You need g > 1 for chaotic activity, but that\u2019s a regime in which the Taylor series approximation you make breaks down.\n* I don\u2019t get how Fig 3 and section 5 are connected to the phase-dependence you observed in previous sections.\n* Section 6: instead of just looking at two phases, could you plot accuracy of the output as a function of phase? If you paired that with a plot of canonical angle as a function of phase it may provide stronger evidence for your claims."}