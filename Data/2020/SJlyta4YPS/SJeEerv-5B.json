{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper applies Multi-Head Self-Attention (MHSA) to a CTR prediction model with some small changes. The empirical results on two public datasets show it improves performance over some baselines.\n\nFirst of all, the novelty of the proposed algorithm is limited in that it mainly applies existing mulit-head self-attention. The paper does include some small modifications to MHSA and achieves better performance, such as bi-linear similarity and max-pooling. However, the nature of these changes seems more incremental.\n\nThe experiment section is very detailed and the paper conducts several ablation studies to understand which components contribute the most, which is nice. However, the paper is missing several important baselines, for example, Deep & Cross [1], which makes the results less convincing.\n\nAnother issue with the paper is that it does not control the model capacity when comparing performance. It is usually the case that increasing model capacity leads to better performance. Given that MHSA and bi-linear similarity have increased a lot of model parameters, it is more fair to compare performance across models with similar capacity. In fact, in [1], they show the logloss on Criteo dataset can be as low as 0.4423 when using large enough parameters.\n\nMinor: in the ablation study, it shows head = 1 has the best performance. In this case, why max-pooling is needed? \n\nReference:\n[1] Wang, R., Fu, B., Fu, G. and Wang, M., 2017, August. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17 (p. 12). ACM."}