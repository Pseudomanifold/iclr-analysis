{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a modification to the ADAM optimizer by introducing an adjustment function, which consists of a square root function and an extra parameter delta. Although the modification is very simple and easy to implement, the theoretical analysis is weak and the empirical performances of the proposed method are similar to the previous adaptive methods.   \n\nHere are my main concerns of the current paper:\n1. The presentation is a little bit confusing in the motivation section. According to equation (2.1), the $L^p$ norm of $\\mathbf{g}_t$ is defined as a vector. However, it seems that the authors treat $\\|\\mathbf{g}_t\\|_p$ as a scalar when presenting the intuition of the proposed method in Figure 1.\n\n2. I do not understand why Figure 1 says the proposed method is better than Padam. It seems to me that if Padam choose a specific p, it can recover the proposed $\\Phi$ function, or even better than the proposed method. Therefore, I do not think the intuition of the proposed method is correct.\n\n3. For the convergence analysis, the authors only consider the convex setting, which I think is meaningless. Because the proposed method is designed for training neural networks, such convergence guarantee in convex setting is not enough. There exist some work such as [1] have proved the convergence guarantee of the adaptive algorithms including Padam in the nonconvex setting. \n\n4. There is one missing baseline Yogi [2] in the current paper.\n\n5. For experimental results, the performance of the proposed method is very similar to the Padam. Due to the close formulation of the proposed method and Padam, it seems to me that the proposed method is  just a more careful hyperparameter tuning process.\n\n6. In NMT experiments, why there is no Padam baseline? In addition, the authors should also report the test perplexity to validate the generalization performance of the proposed optimizer.\n\n7. To fully evaluate the performance of the proposed method, the authors should at least conduct an experiment on the task of language model.\n\nMinor comments:\nThere is an unknown citation in section 5.1.\n\nReference:\n[1]. Zhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n[2]. Zaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018."}