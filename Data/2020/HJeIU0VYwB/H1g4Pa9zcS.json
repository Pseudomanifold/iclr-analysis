{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new variation on adaptive learning rate algorithm that builds on a prior work Padam, which is also a concurrent submission to this conference. The method is empirically validated through through various domains and neural networks architecture. Though the empirical results are extensive, I am leaning towards reject because (1) The method is a very small variation; no insight is provided for why. (2) The theory is not very useful in justifying the method. (3) The empirical results are weak.\n\n(1) This work builds on the prior work Padam, which is not a well justified algorithm. Same criticisms hence can be made to this paper. I don't see any strong arguments from the paper that well justifies the methodology either.\nThe problems are listed below:\n\nHere's what I can find of the authors' interpretation of the prior work Padam: \n\n\"The internal cause is that a concave function is applied rather than the linear function in ADAM.\nOnce \u03b5 is extremely small and |g| \u2208 (0, \u03b5), the mapping value of \u03a6(\u00b7) would be much\nlarger in PADAM than in ADAM; therefore, PADAM can adapt to larger learning rate \u03b1,\nthus flexibly adapting to the variable learning rate scheme.\"\n\n(i) I don't think phrasing the cause as a result of concavity gives arise to any new insights. Simply, what this is trying to say is that \u03a6(\u00b7) is large when |g| is small, that's why PADAM can adapt to large learning rate. \n\nBuilding on Padam, the paper further justifies the proposed method by:\n\n\" This form of \u03a6(\u00b7) not only directly inherits advantages of PADAM,\nas is depicted in Figure 1, but also makes a better guarantee for larger learning rates. The\noffset \u2206 makes sure that \u03a6(\u00b7) can altogether avoid the extreme situation. Even when\n|g| \u2192 0, a more extensive learning rate \u03b1t is allowed. \"\n\n(ii) The paper inherits a serious problem from Padam, that is to assume large learning rate is important for learning rate decay. Padam didn't explain this, nor does this paper. So the justification is very weak. \n(iii) The introduction of the offset is not novel. Just as the author noted, this is almost the same term epsilon in original updates. It's not rare that one also tunes epsilon for some optimization problems.\n\n(2) Continuing the last point, the theoretical analysis focus on the convergence proof of the algorithm. The proof is not new. Theory also is not useful in justifying the method.\n\n(3) The method seems not to be able to beat the previous baseline Padam, which makes it questionable as a practical algorithm. "}