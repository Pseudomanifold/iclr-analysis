{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a general framework for adaptive algorithms, and presents a specific form: ADAPLUS. In the theory part, this work gives convergence analysis of ADAPLUS. For experiments, this work analyzes several algorithm's empirical performances including SGDM, ADAM, AMSGRAD, PADAM, ADAPLUS on CV and NLP tasks. \n\n1. This paper's analysis is not solid enough to support its claim. Since this paper gives a general framework and claims that offset term can achieve superior performance, it is better to give the convergence analysis of general algorithm in the framework and discuss the benefit of the offset term theoretically. Actually, the author gives almost the same theoretical result as ADAM type algorithms, from which I did not see the advantage of using ADAPLUS. \n\n2. And the experiment shows that ADAPLUS performs on par with PADAM on CV task. \nI wonder why the authors did not give the experimental result of PADAM on NLP task? \n\n3.The notation in this paper is quite confusing. In page 2 definition (2.1), v is a vector so the L^p norm of g is actually a vector. But in Figure on page 2, the paper treat it as a scalar and I cannot understand it without further explanation from the authors.\n\nTherefore, I think this work doesn't make enough contribution and the novelty is not enough for ICLR standard.\n"}