{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a hierarchical graph-to-graph translation method to modify compounds to improve the biochemical properties. The authors proposed to generate the new molecular in a autoregressive manner. To improve the performance of the model, the input molecular is encoded into different resolutions including atom, attachment, and substructure layer. The paper is well written and the figures in the paper also enable the paper easy to read. In the experiments, the authors compare the proposed method with serval state-of-the-art methods. The results well analyzed and the ablation study is provided. Overall, this is a good paper considering its technical contribution and writing.\n\nHowever, there are some small issues should be addressed:\n\n1. There are some typos in the paper. For example, in the topological prediction section in page 3,  \"the hidden representation of $S_k$ learned be the decoder \" -> learned be encoder.\n\n2. In the training, the authors apply teacher forcing to the generation process with depth-first order. Why do you use depth-first order not any other orders?  \n"}