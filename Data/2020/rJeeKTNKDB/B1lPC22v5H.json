{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The authors present a heirarchical graph-to-graph translation method for generating novel organic molecules.\nWorking from the model of Jin et al. (2019), the authors introduce a three step heirarchy - the model first determines where a substructure should be generated, what is the substructure, then the attachments to the existing molecule.\nAll steps of this uses embeddings generated from a message passing network - these embeddings are input into a few bilinear attention layers to obtain the heirarchical generation scheme.\nThe model is trained with molecular pairs (X, Y), and a VAE loss - a hidden z vector controls the way to modify X to improve its properties.\nThe encoder is just a MLP over the difference between sum of embeddings at a atom level and at the substructure level.\nThe model is evaluated on accuracy and diversity, in both conditional and unconditional settings.\nThe experiments show a small improvement over previous SOTA algorithms.\n\nThis is a borderline paper, and I'm leaning towards a weak reject, because I don't believe the model is well motivated enough:\n- Sec 3.1 it's unclear how the substructures are generated - they provide a lot of inductive bias for the algorithm. \n  Are they automatically generated or built from a database of substructures?\n- Variational decoding does not seem well motivated enough - would a stochastic decoding procedure not work as well as having a latent vector that essentially adds noise to the training?\n- The experiments seem interesting and comprehensive - it seems that the model learns to exploit the biases and increase logP, as well as showing the ability to conditionally turn off DRD2-active properties of the molecules.\n\nSome questions:\n- Why not use a Transformer instead of an LSTM or GRU? The cell naturally acts over sets of neighbors and transformers are a natural model to tackle this problem.\n- Sec 3.1 Topological Prediction, the attention is over c_{X}^{S} but the text claims it should be over c_{X}^{G}? Is ^G the attention substructure?\n- Sec 3.1 Attachment Layer MPN: the A_i seem to be a tuple (S_i, {v_j}). The set of attaching atoms is limited to 2 right? It might be more clear to simply enumerate them here if so.\n- Sec 3.1 Substructure Tree: Since tree decompositions are not unique, does this work use the different tree decompositions and DFS traversals as data augmentations?\n- Table 2b: What is a \"two-layer\" and \"one layer\" encoder? Is it the size of the MLP or the removal of the attachment MPNs?\n- Ablation study: Since the Attachment Layer has all the substructure information, this ablation should ideally make sure the models all have a similar number of parameters, and the decrease in performance isn't due to the decrease in parameters.\n\nNits:\n- Sec 3.1 \"bi-linear\" should not have a dash, bilinear is one word.\n"}