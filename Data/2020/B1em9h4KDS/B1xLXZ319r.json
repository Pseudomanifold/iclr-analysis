{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). \n\n\nSuggestions for improving the introduction & discussion: \n* The purpose of this paper is to model uncertainties about missing values \u2014 you really should say more about probabilistic methods than \"A few exceptions exist such as Bayesian models\u201d.  At least give some motivation for why certain imputations problems couldn\u2019t be feasibly solved by modeling the missing values in a probabilistic programming framework. \n* Other GAN methods for imputation (GAIN and MisGAN) are dismissed as \"often very complicated to be applied in practical setups by practitioners\u201d. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work?\n* \"This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.\u201d \u2014 the main difference between this approach and MI is that you\u2019re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand.\n\nSuggestions for improving the evaluation:\n* You\u2019re imputing missing rectangles from an image dataset \u2014 please show us the resulting images. I would greatly prefer this to Section 4.5 \u2014 which is a very low sample size, low dimensionality example and it\u2019s really unclear how well it generalizes to real data. \n* \"We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.\u201d \u2014 I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. \n* \"We run each experiment multiple times (at least 4)\u201d \u2014 please report how often each experiment was run, even better if you standardize this number. \n* For Table 2, please provide accuracy without missing values as a baseline.\n* Add MICE or some other \u201cstandard\u201d imputation method as a baseline. \n\nSuggestions for improving readability: \n* Many sentences start with \u201cin this\u201d (e.g. \u201cin this case\u201d, \u201cin this setting\u201d, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. \n* Not a complete sentence: \"For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.\"\n"}