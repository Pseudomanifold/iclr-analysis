{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance.\nOverall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns:\n1-\tThe predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too?\n2-\tYour method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN.\n3-\tI am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable  (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. \n4-\tIn justification for claim 1, it is said \u201cThis is equivalent to training models using noisy labels\u201d. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample.\n5-\tIn the implementation details, I cannot fully wrap my head around the part \u201cz vector of size 1/8\u201d; how did you choose this 1/8?\n"}