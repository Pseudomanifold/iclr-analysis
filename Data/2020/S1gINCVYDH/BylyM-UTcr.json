{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors, in this paper, study the problem of efficient exploration and exploitation in RL. They propose an algorithm, as referred to as a suite of solutions, based on the posterior sampling approach to tackle mainly continuous state-action space MDPs. Their method learns the model of the environment using deep neural networks while having the last layer trained using Bayesian linear regression. They utilize this model to design a policy for the exploration and exploitation trade-off.\n\n\nA few comments that did not affect my evaluation but might be helpful to the authors: I might suggest to the authors to reconsider the title of their paper. I can guess the authors mean they aim to design a more sample efficient algorithm and by RL in the title, they mean the RL algorithms. Please rephrase it. Also, the term \"again\" implies that the methods in this filed were sample efficient, then became sample inefficient, and then this paper proposes to make them sample efficient. \n\nMinor changes: In the middle of page 3, the authors define V_{mu,i}^M. I might have mistaken, but either \"i\" goes from 1 to tau-1 or V_{mu,tau+1}^M is zero instead of V_{mu,tau}^M to have general MDP setting. Please correct me if I am wrong. \n\nAlso, the sentence on the very same page, \"for every previous step the expected future value is the reward at the current step plus all future rewards.\" might require a bit more attention. I might guess the authors mean the expected reward.... is the value. \n\nIn theorem 1, borrowed from Osband & Van Roy 2014a, I think the term sigma_P is not defined. Also please reconsider calling sigma_R the variance, since in that case, the theorem might not hold. \n\nGenerally, I have a major concern about the presentation of this paper. While the authors spent the first 4 pages on the discussions that are useful, important, and provide intuition, I have a hard time to see the significance and importance of presenting them. Especially, when the authors spent a considerably small portion of their paper on their own contribution and provided very shallow representing of their algorithm. \n\nRegarding the references, I found this paper not satisfyingly covering the space of literature. Besides the issue of coverage, there are quite a few parts that require citation, but no citation is provided. I would be happy to see a citation related to AdamW, citations of the line of work by Ronald Ortner on continuous MDPs, and also literature on Thompson sampling in linear MDPs. I can extend this list if the authors are interested. \n\n\nRegarding the presentation of their method, first of all, I strongly recommend the authors to provide a self-contained, detailed and profound description of their contribution and their algorithm. I also strongly suggest the authors discuss the motivation and then the detail of each component used in their approach. \n\nI appreciate the fact that the authors provided the code used in their paper, but it would be more appreciated if they would have provided more details of their algorithm, in the paper, to the point that the algorithm could be reproduced by a reader to some extent, without the code.\n\nIn order to draw a conclusion on the performance of the proposed method, I would be happy to see a set of significantly more empirical studies. \n\n\nGenerally, I like the approach and appreciate the authors' effort. While I think the current paper has undoubtedly high potential to improve, I do not think the current version is ready to be published in this conference."}