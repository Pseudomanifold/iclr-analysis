{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Posterior Sampling: Make Reinforcement Learning Sample Efficient Again\n===============================================================\n\nThis paper proposes an implementation of PSRL via Neural-PSRL: a neural network is trained fit the transition dynamics, with a last-layer linear approximation used to give a posterior distribution.\nThis model can be used in conjunction with a policy gradient algorithm to give an exploratory policy.\nEvaluated on Mujoco benchmarks this leads to state of the art performance.\n\n\nThere are several things to like about this paper:\n- The problem of extending \"efficient exploration\" to Deep RL is an important one, and PSRL seems like one of the most suitable general approaches to base an algorithm on.\n- The proposed algorithm seems reasonable, and seems like it also performs well.\n- The approach to writing and accessible code release should be praised.\n\nHowever, there are some other places where this paper falls down:\n- Despite the focus on \"incorporating prior knowledge\" in the early part of the paper, it seems like the proposed method actually incorporates very little prior knowledge... for example, the \"Eluder dimension\" of the space represented by these neural networks is presumably huge! This seems like a bit of a bait and switch.\n- I don't think the experiments offer a huge amount of *insight* into what is really happening in the algorithm. It would be helpful to show *really good* performance even on a toy domain, so that you can really say that the exploration via PSRL is really working... maybe something like bsuite = https://github.com/deepmind/bsuite and particularly the \"deep sea\" experiments could help?\n- It's a problem that the actual main paper doesn't give a clear and concise description of the algorithm used. I think this is quite important and it would be much better to put the older results on PSRL to the appendix instead.\n\nMinor points:\n- I'm not sure the title is a great idea... it might put off some people, but also I don't think it's really warranted from the results... it seems like Neural-PSRL is a potentially interesting avenue, but hardly making anything great (again?)!\n- You might want to connect / contrast this work vs the line of research around \"Randomized Value Functions\" http://jmlr.org/papers/v20/18-339.html... which is another approach that tries to extend something like PSRL to Deep RL.\n\n\nOverall, I do think this is an interesting paper, and a promising avenue for research.\nHowever there are probably too many places where the paper could be tightened up: clarity of algorithm description, clarity of support/results, separation of actual claims vs claims of \"making great\" ;D."}