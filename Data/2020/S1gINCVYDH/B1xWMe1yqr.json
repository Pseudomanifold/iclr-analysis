{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper suggests a practical implementation of Posterior Sampling for Reinforcement Learning (PSLR) using deep networks. The PSLR framework assumes that the model of the environment is sampled from a relatively lower dimensional distribution (for example because of some inherent structure they posses), and exploits this to reduce the sampling complexity to learn an optimal policy, so that the number of samples required is bounded by the complexity of the model distribution, and not by the dimensionality of the state or action space.\n\nThe paper is well motivated and well written, and the research direction is indeed interesting. I would like to see the implementation section pushing further the limits of the theory by using a more complex posterior model \u2013 rather than a simple linear layer \u2013 and by testing on more challenging environments, where explicitly exploiting the structure of the non-linear model is essential to solve the task.\n\nRegarding the implementation, in Section 4.2 the model is parametrized as s' = W z + s, where W is sampled from the posterior and z is the output of a feed-forward network. Could you further comment on which data the feed-forward network is trained on? Does the theoretical framework remain valid if the feed-forward network is trained only on states from the environment on which the PSLR will later be tested? Wouldn't that imply that the prior model distribution is already biased toward the particular environment?"}