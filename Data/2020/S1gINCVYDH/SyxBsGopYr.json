{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors implement Thompson sampling for finite horizon MDPs in a continuous control setting where they learn the uncertainty over the dynamics using a Bayesian linear regression model on top of a neural network embedding. They use model-based RL to learn the optimal policy for a sample from the posterior. Specifically, they use a variant of SAC on unrolls from the learned model. The optimization is not entirely clear from Algorithm 2--it seems that they backpropagate through the learned model and a ground truth reward function. This is very reminiscent of Stochastic Value Gradients (Heess, 2015) yet they do not cite this paper. Important ingredient to make the model based RL to work is to use an ensemble of policies. The work lacks in theoretical novelty since the Thompson sampling approach is established and well-known. This would be perfectly fine if the experiments demonstrated clear improvements but that is not the case.\n\n1. The paper is too wordy and repetitive--there's no reason to spend 5 pages discussing Thompson sampling and the motivations of Bayesian RL.\n2. As also pointed out by the authors it is not clear if the observed improvements are not due to the ensemble of policies. This seems like it should be easy to check by running the other methods with ensembles of policies too.\n3. More generally, since the hypothesis of the paper is that incorporating the uncertainty is important, the experiments should focus on that, and e.g. validate that the proposed approach to learning the posterior actually learns the correct uncertainty. One possible ablation would be to simply add (annealed) noise to the weights and check if maybe such an unstructured distribution over dynamics would work too.\n\n"}