{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method for learning models for discrete events happening in continuous time by modelling the process as a temporal point process. Instead of learning the conditional intensity for the point process, as is usually the case, the authors instead propose an elegant method based on Normalizing Flows to directly learn the probability distribution of the next time step. To further increase the expressive power of the normalizing flow, they propose using a VAE to learn the underlying input to the \"Flow Module\". They show by means of extensive experiments on real as well as synthetic data that their approach is able to attain and often surpass state of the art predictive models which rely on parametric modelling of the intensity function. The writers have put their contributions in context well and the presentation of the paper itself is very clear.\n\nThough the final proof is in the pudding, and the addition of the VAE to model the base distribution yields promising results, the only justification for it in the paper is to create a more \"expressive\" model. There are multiple ways of increasing the expressiveness of the underlying distribution: moving from RNNs to GRU or LSTMs, increasing the hierarchical depth of the recurrence by stacking the layers, increasing the size of the hidden state, more layers before the output layer, etc. A convincing justification behind using a VAE for the task seems to be missing. Also, using the VAE for a predictive task is a little unusual.\n  \nAnother, relatively small point which the authors glance over is the matter of efficient training. The Neural Hawkes model suffers from slow training because of the inclusion of a sampling step in the likelihood calculation. I believe that since the model proposed by the authors allows easy back-propagation, their model ought to be easy and fast to train as well. Including the training time for the baselines, as well as the method proposed by the authors, will help settle the point. \n\nMinor point:\n\n - The extension of the method to Marked Temporal Point Processes in the Evaluation section seems out of place, esp. after setting up the expectation that the marks will not be modelled initially, up till footnote 2 on page 7."}