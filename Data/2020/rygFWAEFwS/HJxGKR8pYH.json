{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a 2-stage SGD variant that improves the generalization. The experiments show good performance.\nHowever, there are some weakness in this paper:\n\n1. (Minor issue) The Update() function in Algorithm 1 seems to be something very general. However, it seems that Update() is simply SGD or SGD with (Nesterov) momentum, according to Section 5.1. Furthermore, the authors never explicitly explain what exact Update() function is used, which is very unfriendly to the readers.\n\n2. The major issue is that the proposed algorithm and the contribution (improvement of generalization) is not novel. Phase 2 of Algorithm 2 is called local SGD, proposed in [1,2]. Local SGD also has variants with Polyak momentum and Nesterov momentum [3]. Furthermore, [4] has already proposed an algorithm, post-local SGD, which is basically the same as Algorithm 1 in this paper (run fully synchronous SGD first and then local SGD). Note that [4] also shows that post-local SGD converges to flatter minima, and results in better generalization. Please correct me if I'm wrong, and explain the difference between Algorithm 1 and (post-)local SGD in details.\n\n\n----------\nReference\n\n[1] Stich, Sebastian U.. \u201cLocal SGD Converges Fast and Communicates Little.\u201d ArXiv abs/1805.09767 (2018).\n[2] Yu, Hao et al. \u201cParallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning.\u201d AAAI (2018).\n[3] Yu, Hao et al. \u201cOn the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization.\u201d ICML (2019).\n[4] Lin, Tao et al. \u201cDon't Use Large Mini-Batches, Use Local SGD.\u201d ArXiv abs/1808.07217 (2018).\n"}