{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a parallel version of the stochastic weight averaging method. It utilizes to phases to train the DNN. The first phase consists of distributed large-batch training, where the learning rate is scaled linearly with respect to the scale of the batch size. The second phase consists of using small batches with SWA to obtain the final model. Experiments verify that this method is able to achieve similar generalization performance as small-batch methods in less training time. A comparison against small-batch SWA, large-batch SWA, etc. \n\nStrengths:\n\nThe proposed algorithm is a natural extension to SWA, and appears to give good generalization performance. It is able to utilize large-batch training effectively, which is perhaps surprising given the amount of tuning necessary in Shallue, et al. (2018) in order to achieve good performance. The experiments are well-detailed, and some interesting visualizations, graphs, and empirical analyses are provided.\n\nWeaknesses:\n\nI think that this paper is fairly complete; the only question is whether or not it contains enough novelty, as it is a natural extension of SWA to the parallelized setting. No theoretical analysis of the algorithm is given.\n\nSome questions and comments:\n\n- How sensitive is the algorithm to the choice of the transition point? How was the transition point tuned?\n- How large of a batch size can one use before this approach breaks down or is no longer efficient?\n- In Figure 2, LB is shown to obtain worse training error than SGD. What is the reason for this? This seems contrary to theory (assuming one has already converged to a neighborhood of the solution).\n- The authors comment in the conclusion that \"it is possible that regions with bad and good generalization performance are connected through regions of low training loss\". Can one check if this is related to the invariances described in Dinh, et al. (2017)? \n- What is the relationship between SWAP and methods on local SGD?\n\nThis paper is missing some classical optimization references on increasing batch sizes, which has been well-studied within that literature:\n\n[1] Byrd, Richard H., et al. \"Sample size selection in optimization methods for machine learning.\" Mathematical programming 134.1 (2012): 127-155.\n[2] Bollapragada, Raghu, Richard Byrd, and Jorge Nocedal. \"Adaptive sampling strategies for stochastic optimization.\" SIAM Journal on Optimization 28.4 (2018): 3312-3343.\n[3] Bollapragada, Raghu, et al. \"A progressive batching L-BFGS method for machine learning.\" arXiv preprint arXiv:1802.05374 (2018). \n[4] Friedlander, Michael P., and Mark Schmidt. \"Hybrid deterministic-stochastic methods for data fitting.\" SIAM Journal on Scientific Computing 34.3 (2012): A1380-A1405.\n\nAlthough developing some theory for this algorithm would be beneficial, this paper performs a comprehensive set of experiments and is well-written. For these reasons, I'm inclined to accept the paper."}