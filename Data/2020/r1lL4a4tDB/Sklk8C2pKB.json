{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes that for partially observable reinforcement learning tasks it might be simpler to decompose the problem in two parts: a recurrent world model and a feedforward agent, as opposed to using just a recurrent agent. Intuitively the decomposition makes sense, though it's not very clear to me that the problems encountered training a recurrent agent should be dramatically simpler when training a recurrent world model instead.\n\nFor the world model the paper proposes using a variational recurrent state-transition model, which is essentially a VRNN which also conditions on the actions. It is not clear to me that this is easier to learn than a recurrent agent because the probability distribution of the data used to train the modified VRNN is highly dependent on the current policy in the same way that the non-iid training data makes it tricky for the RNN policy models to converge.\n\nWhile the experimental results seem to show the new algorithm outperforming the existing ones over a large set of random seeds the paper does not specify how the fixed hyperparameters for all models were chosen, leaving open the possibility that different hyperparameter settings would have shown reversals in the experimental results. It's also not clear that the complexity of the alternate models was adequately accounted for (specially since two VRNNs are required to match the performance of a single RNN agent). That said the incompleteness of the experimental results is my only reservation against accepting this paper."}