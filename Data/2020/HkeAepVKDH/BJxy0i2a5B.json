{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduces a fairly simple yet seemly effective method for quantizing GAN. Existing quantization methods (namely minmax, log, and tanh quantization) for CNN/RNN fail brutally under GAN setting. From empirical observation of the distribution of quantized weights, the authors conjecture the reason being under-utilization of the low-bit representation, called under-representation in the paper. Based on such observation, linear scaling with EM is proposed and experimental results seem to be effective. \n\n[Advantage]\nThe paper is clearly written and easy to follow. The proposed method is well-motivated from the empirical observation presented in Sec 3, and seems to mitigate the difficulties from the discussion in Sec 5.\n\n[Disadvantage & Improvement]\nWhile I am not a direct expert in this area, I do have some concerns regarding the novelty of the method and comparison to previous works. Linear quantization seems to be a common/intuitive method and there are various improvement techniques built upon it (e.g. cliping, ocs,... etc [1,2]). These are related works, yet neither included nor discussed in this paper. How is the presented linear+EM method comparing with these variants, in term of effectiveness on training GAN and the reported test-time metrics? In short, the comparison to previous works seems insufficient in my point of view.\n\nAlso, can you comment on Defensive Quantization (DQ) [3]? The quantization method is specifically designed for adversarial attack/perturbation setting and seems applicable under GAN setting. \n\nLast, there is a typo at the end of Sec 4.2: should it be f_{em}(x) instead of f_{e}m(x)?\n\n[1] Low-bit Quantization of Neural Networks for Efficient Inference\n[2] Improving Neural Network Quantization using Outlier Channel Splitting\n[3] Defensive Quantization: When Efficiency Meets Robustness\n\n"}