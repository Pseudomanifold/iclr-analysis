{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper propose to study the quantization of GANs parameters. They show that standard methods to quantize the weights of neural networks fails when doing extreme quantization (1 or 2-bit quantization). They show that when using low-bit representation, some of the bits are used to model extremal values of the weights which are irrelevant and lead to numerical instability. To fix this issue they propose a new method based on Expectation-Maximization to quantify the weights of the neural networks. They then show experimentally that this enables them to quantize the weights of neural networks to low bit representation without a complete drop of performance and remaining stable.\n\nI'm overall in favour of accepting this work. The paper is well motivated, the authors clearly show the benefits of the proposed approach compared to other approach when using extreme quantization.\n\nMain argument:\n+ Great overview of previous methods and why they fail when applying extreme quantization\n+ Great study of the influence of the sensitivity to the number of bits used for quantization\n- It would have been nice if the author had provided standard deviation for the results by running each method several times. In particular figure 2.c seem to show that they might be a lot of variance in the results when using low bit quantization.\n- I feel some details are missing or at least lack some precision. For example are the networks pre-trained with full precision in all experiments ? if so can you precise it in section 3.1 also ? \n- The proposed approach seem very similar in spirit to vector quantization, can the author contrast their method to vector quantization ?\n- In equation (7) doesn't the constant C also depend on alpha and beta ?\n- In section 5.1 do you also use the two phase training described in section 4.2 ?\n- Figure 4.c seems to indicate that quantize the generator only is no more a problem ? Can you explain why this figure is very different from figure 2.c \n- In table 3 how is the number of bits chosen, did you try several different values and report the best performance ?\n\nMinor:\n- Some of the notations are a bit confusing. You call X the tensor of x, I think it would be more clear to say that X is the domain of x.\n- I'm surprised by the results in section 3.1, wouldn't the issue described in this section when training standard neural networks ? wasn't this known before ?\n- There is some typos in the text"}