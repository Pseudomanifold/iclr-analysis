{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThe authors address the quantization of Generative Adversarial Networks (GANs). The paper first performs a sensitivity study for both the generator and the discriminator to quantization methods. Building upon the conclusion of this study, the authors propose a scalar quantization method (QGAN) and compress models to 1 bit of 2 bits weights and show generated images and metrics by the compressed models. \n\nStrengths of the paper:\n- As well stated in the introduction, the compression of GANs (in particular the generator, which is used at inference time) is of practical interest and, to the best of my knowledge, novel. This novelty can be explained by (1) the fact that it takes tome for quantization methods to percolate the entire deep learning field and/or (2) the fact that quantizing GANs has its specificities and own challenges that have not been yet addressed (this is the claim of the authors).\n- The sensitivity study is of interest for the community that can build upon this work. The conclusions (discriminator more sensitive than generator to quantization, quantizing both generator and discriminator helps) are sensible and interesting. \n\nWeaknesses of the paper:\n- The related work section could be greatly improved, thereby showing the limited novelty of the proposed method (QGAN). Indeed, the authors propose to learn the optimal scaling parameters alpha and beta. Many works perform this already and are currently missing in this section, see for instance the two recent surveys: \"A Survey on Methods and Theories of Quantized Neural Networks\", Guo, \"A Survey of Model Compression and Acceleration for Deep Neural Networks\", Cheng et al. \n- Results. The results are not sufficient to justify the performance of the method for two reasons. (1) First, the scale is crucial in assessing the performance of a quantization method. As an example, it is easier to quantize small ResNets on CIFAR-10 than large ResNets on ImageNet. Thus, scales enables to better discriminate between various approaches. I acknowledge that this requires large computational resources but this would greatly strengthen the paper (2) Second, GAN metrics have known shortcomings (see for instance \"How good is my GAN?\", Shmelkov et al.), so the strength of Table 2 is limited. This is in part alleviated by the authors by showing generated images (which is a good practice), but again echoing point (1), larger images would have helped assess better the quality of the quantization.\n\nJustification of rating: \nThe authors propose a sensitivity study that is interesting for the community. However, the proposed method lacks novelty and the results are not convincing enough. I encourage the authors to pursue in this interesting direction which has important practical implications."}