{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to determine explanations for predictions using first-order logic (FOL). This requires being able to learn FOL rules. The authors propose to divide the search for FOL rules into 3 parts, each being hierarchical. The first level is a search for operator, followed by primitive statement search, followed by search for the Boolean formula. The authors also propose to implement logical variables that appear only in the body of the rule (and thus are existentially quantified) using skolem functions which reduces to a search operation and is an interesting idea which I haven't seen in recent works combining logic and neural networks. The paper then proposes a parameterized logical operator and describes their architecture for training these using attention and transformers.\n\nI found the paper to be very difficult to read. For instance,  Equation (5) doesn't mention parameters \\alpha on the RHS. I can't make out what the parameterization of the logical operator is. I can't also connect section 4 to the parameterized logical operator in section 3. Then Section 4.2 presents a score formulation (Equation 6) and refers the reader to the NeuralLP paper. This is not my favorite way to write a paper. So many indirections make it very difficult to appreciate the contributions. I hope the authors take this feedback and try re-writing their paper with the reader in mind.\n\nIs there a specific reason why Neural Logic Machines (Dong et al, ICLR 2019) is not referenced? It also claims to be more efficient than \\partial-ILP and to be able to learn rules. This is an important question. From what I can make out, not only should this paper cite Neural Logic Machines but they should in fact be comparing against it via experiments. It would also be helpful if the author present experiments against ILP systems (e.g. AMIE+). While ILP cannot deal with noisy labels, these full fledged systems do have some bells and whistles and it would be interesting to find out exactly what results they return. I couldn't make out exactly what the authors meant with this statement from the Appendix:\n\"The main drawback of NeuralLP is that the rule generation dependents on the specific query, i.e. it\u2019s data-dependent. Thus making it difficult to extract FOL rules ...\"\nNeuralLP does learn FOL rules (of a particular form). I don't understand what the above statement means. I think the authors need to reference NeuralLP more carefully lest their statements come off as being too strong.\n\nTwo questions requiring further clarification:\n\n- Since your logical operator is parameterized, how do you take a learned operator and identify which logical operator it corresponds to? More generally, how do you derive the crisp rules shown in Table 4 in the appendix?\n\n- Since your network is fairly deep (e.g., I don't see a direct edge from the output layer to the operator learning layers in Fig 2), how do you ensure that gradients do not vanish? For instance, Neural Logic Machines use residual connections to (partially) address this. Is this not a problem for you?\n"}