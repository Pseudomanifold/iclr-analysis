{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a novel architecture of integrating neural models with logic inference capabilities to achieve the goal of scalable predictions with explanatory decisions, which are of significant importance in the real deployment. In general, the article is well-written and nicely-structured with clear motivations, i.e., make the model predictions interpretable, make the logic rules differentiable, and make the algorithm scalable with longer rules and faster inference time. Both the methodology and experimental results make the idea promising and interesting. The contributions could be summarized in the following which make this piece of work worth of publication to my point of view:\n\n1. I like the idea of introducing the operator concepts to save much time in the variable binding problems in the common logic inference domain. \n\n2. The authors proposes a hierarchical attention mechanism to transform the rule composing problem into 3 stages with 3 different subspaces, namely operator composition, primitive statement composition and formula generation. This hierarchy structure solves the inductive problem efficiently by decomposing it into subproblems, which seems novel to me.\n\n3. The proposal of a general logic operator defined in eq5 is crucial for formula generation in a differentiable way. \n\nDespite the above contributions, there are a few limitations and questions to the author:\n\n1. The paper states that eq(5) is able to represent various logic operators with proper parameters. Can you provide some examples of how this general formula represent simple operators such as \"p \\vee q\"? It also mentions the case to avoid trivial expressions, but it's not clear how this is solved.\n\n2. For operator search, I assume \"e_X\" indicates the representation for the head entity, then what does \"e_Y\" represent? If each operator at most takes the head entity as input, where does \"e_Y\" come from? does the process for operator search repeat for each different operator indicated by \"e_\\phi\"? If this is the case, what's the effect of adding extra predicate embeddings \"H_b\"? Furthermore, the formula search is not clearly illustrated as of how eq(5) is softly picked using the defined process?\n\n3. Section 4.2 introduces a use case for end-to-end evaluation through relational knowledge base.However, it is unclear to me how those score functions and \"f_i^{(t)}\" contribute to the search model, i.e., how those formulas in section 4.2 map back to the search functions introduced earlier? This is crucial to understand the gradient backpropagation. And it could be better to provide an illustrative algorithm on generating the actual rules from the search modules.\n\n4. For the experimental section and related work, another existing work is missing, i.e., \"Neural Logic Machines\". More discussions and comparisons (experimental comparisons if possible) are helpful. A further question to ask is whether the proposed architecture could be used in the case when domain knowledge is not that explicit, e.g., the predicates are unknown or some of them are unknown? "}