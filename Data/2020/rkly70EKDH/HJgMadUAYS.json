{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper studies the amount of over-parameterization needed for a quadratic two (three) layer neural network to memorize a separable training data set with arbitrary labels. The main result of this paper shows that as long as the number of data in dimension d is smaller than d^2/2 and the data is separable by a quadratic function, then a fully connected two-layer neural network with quadratic activation function and 2d hidden neurons can memorize the training data set efficiently. \n\n\nThe intuition behind this result is quite simple: Given data x_1, ..., x_N with a matrix A in R^{d x d} such that x_i^T A^T A x_i = y_i, given the current weight matrix W one can always try to construct an Hessian update direction A' = U A for a column orthonormal matrix U such that x_i^T W^T U A x_i = 0 for every i and (by the property of U) x_i^T A'^TA'x_i = y_i. Note that U has more than d^2/2 degree of flexibility, so having x_i^T W^T U A x_i = 0 is in principle always possible. \n\n\nThe paper studies a very important question: How can neural network memorize data in the mildly over-parameterized regime (number of parameters is linear in the number of data). There is experimental evidence that in this regime, the neural network does memorize much better than its counterpart neural tangent kernel (NTK).\n\n\n\nMy main concern about this paper is the assumption that the data is separable by a quadratic function, which seems to be very restricted (although this is indeed a progress from those who assume linear separability of the data, e.g.  Brutzkus et al. arxiv:1710.10174,  Wang et al. arXiv:1808.04685 and Oymak & Soltanolkotabi 2019 which has a sub-optimal dependency on the condition number of the training data set). However, in the result such as Allen-Zhu et all, the only requirement is that every data has distance at least \\delta, and the final dependency is poly(1/\\delta), the assumption is necessary to memorize arbitrary labels over the data set. However, quadratic separable seems to be too strong for a general data set.\n\nIndeed, the authors argued that if the data set is noisy with N < d^2/2, then w.h.p. it is quadratic separable. However, if the network is really using those noise in the data to fit the labels, then one should expect NO GENERALIZATION GUARANTEE of the learnt network at all -- a result that is meaningless for the theory of deep learning.\n\nMissing citation:\nLearning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"}