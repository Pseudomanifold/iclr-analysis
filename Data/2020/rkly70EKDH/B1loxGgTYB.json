{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proves that one can design a (shallow)neural network that with a mild amount of overparametrization (e.g. the number of datapoints n is roughly less than d^2 in Theorem 1), a second-order method can reach a global minimum. In general, I think this is an interesting direction. However, I have some doubts regarding the comparison to prior work (especially regarding the results on the two-layer network), as well as some technical details that need some clarification. Regarding the quality of the writing, it\u2019s in general ok but there are lots of grammatical mistakes, the authors should pay more attention to this. I\u2019m not giving a high score for now but I will reconsider my review once I hear back from the authors.\n\nComparison to Oymak & Soltanolkotabi 2019: my understanding is that this paper prove convergence to a global minimum for a neural network where the number of parameters is only twice the number of datapoints so aren\u2019t your results \u201cworse\u201d in that sense? The text in the related work seem to say the opposite, so I\u2019m rather confused by your statement, please clarify.\n\nLandscape: I think this discussion is largely missing in the paper but another way to prove the same result would be to focus on showing that the loss surface is \u201cwell-behaved\u201d. In fact the paper by Soltanolkotabi et al (see reference below) proves this for a similar network with a quadratic activation function and arbitrary data. Essentially, they show that such network obeys the following two properties:\nThere are no spurious local minima, i.e. all local minima are global.\nAll saddle points have a direction of strictly negative curvature\nMy understanding is that they prove this for a parametrization regime where n=d^2, isn\u2019t this the same regime as proved in your results?\nMahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks.\n\nThree-layer neural net\n1) This network uses activation function of the form x^p. You wrote \u201cfor some constant p\u201d, is there any specific lower bound on p? I\u2019m also surprised that one would allow large values of p as such functions have a saddle at x=0 with a large region with low gradient magnitudes around it.\n2) Limitation quadratic activation function. You say \u201cTo address this problem, the three-layer neural net in this section uses the first-layer as a random mapping of the input\u201c. How is this helping with changing the activation function?\n3) You have to fix part of the weights of the network, this seems to be a limitation of the analysis that should be more clearly highlighted and better contrasted to what has been done in prior work.\n4) I think it would be interesting for the reader to focus more on the three layer network (instead of the two-layer one whose analysis is rather simple) and provide a more detailed proof sketch.\n\nPaper organization\nThe most interesting result of the paper is the one about the three-layer network but the entire analysis is relegated to the appendix. I feel it would be worth trying to provide a rough proof sketch in the main paper to highlight the difficulty of the analysis.\n\nProof Lemma 2\nAlternatively to the current proof, couldn\u2019t you differentiate the second term (2/n \\sum_j (\\sum \u2026)^2 ) w.r.t. z and show it is zero at the argmax_z? Isn\u2019t this what your result says?\n\nExperiments\nConsider repeating the experiments a few times and showing the average.\n\nMinor: Low loss vs perfect fitting: I couldn\u2019t find any discussion about this but the authors seem to assume that a zero loss directly implies that the training data is fit perfectly. In the case n=d, there exists a function that yields a perfect fit of the data and therefore a larger network should be able to represent this function. Perhaps it would be worth writing this down.\n\nMore minor comments\n- Figure experiments: please use a log scale\n- Formula top of page 16: should be z_k on the left and right of the term inside the bracket\n- missing citation top of page 24: \u201ce.g. in\u201d\n"}