{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper is well written, but I am not entirely sure of the interest of the results. \nI might accept, but would not be too disappointed if it didn't pass.\n\nA first comment is that the  alignment between student and teacher nodes is a very old problem, discussed at length in, for instance Saad&Solla, under the name \"specialisation\". Since the phenomenon is known, and already has a name, it should at least be also refereed to as such.\n\nThe result on the overlap and the \"specialisation\" of the teacher to the student presented in the paper is rigorous (though I did not completely checked the proof), and seems general enough, but it seems a bit trivial: of course if I have no or little error on all my data-points, I have overlap with the teacher, and since I'm over-parameterised and it's a ReLU network, then the alignment will be many-to-one.  More interesting would be to study alignment in the deeper case, but the authors prove it only for the lowest layer of a deep network.\n\nThe paper is mainly mathematical, and they are number of things I would find more interesting than the proof (though of course, this is a personal bias):\n- plot the overlaps layer-wise (i.e. student layer 1 vs teacher layer 1, student layer 2 vs teacher layer 2, etc.) What do they look like? That's something I would actually quite like to know!\n- the result on larger nodes being learnt first is known for online learning already in the 1990s (This is a celebrated results of Saad&Solla, though not an entirely rigorous one, and only for model data), so here the contribution is to show this for ReLU networks in particular. \n- Since ReLU networks are somewhat linear, it would be interesting to compare the results on the dynamics to plain linear networks, as in Saxe et al (e.g. https://arxiv.org/abs/1710.03667 ). Discuss similarities / differences?\n- The absence of \"specialisation\" in linear model is also a well known feature, see for instance https://arxiv.org/abs/1312.6120 https://arxiv.org/abs/1710.03667.\n\nFinally, I am a bit confused by the experiments : I did not understand which experiment is done for which data in fig. 5.6.7 (8 is for CIFAR of course) and 11. \n\n"}