{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work uses imitations learning (from synthetic data) to train a deep model which takes a natural language instruction, and a visual representation of a robot's environment, and outputs a trajectory for the robot to follow which executes this instruction.  The work focuses on a robotic pick-and-place task, where the instruction indicates which of the available bins an item should be placed in.  In addition to the trajectory model, a second model is trained which allows the agent to predict whether a given command is actually feasible (i.e. whether the target bin exists).  Empirical results show a reasonably high success rate in placing objects in the bin specified by the instruction, though there is still room for improvement in cases where the shape o a combination of features is important to the selection of the correct bin. \n\nRather than mapping directly from instructions and observations to control signals, the model trained in this work translates from an instruction, and an image of the agent's environment, to the parameters of a DMP controller.  The network therefore outputs the entire motion for the task in a single inference pass.  This approach would have advantages and disadvantages.  The DMP formulation ensures that the resulting trajectory is relatively smooth.  It also means that the network outputs a distinct goal configuration, which the DMP should reach (assuming the goal is feasible) regardless of the other motion parameters.  The use of a DMP output space, however, limits the model to generating relatively simple, goal-directed motions, and does not allow the agent to adapt to changes in the layout of the environment (which would only be observed in the static visual input).\n\nAs other work has considered visual instruction following (e.g. Misra et. al. \"Mapping Instructions and Visual Observations to Actions with Reinforcement Learning\") it would strengthen this work considerably to see a direct comparison between this method and existing approaches.  It is likely that the approach presented in this work is better suited to the specific problem of robot control, but it would be helpful to see if learning a low-level control policy directly can be successful in this context.\n\nThe work needs to expand on the discussion in the second paragraph of section 4, where human annotators were used to generate natural language instructions for different tasks.  The paper suggests that this data was not used directly to train the model, but was instead used to build a template for generating natural language instructions.  What this template looks like, and how it was constructed based on the human-generated data, remains unclear, and needs to be described in much more detail."}