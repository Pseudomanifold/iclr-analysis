{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper claims to propose a method to train q-based agents that use \u201calternating\u201d Q-learning. However, the alternating approach given in the paper appears to be the normal Bellman update implemented in most versions of DQN. Furthermore, the citation given for AltQ (Mnih et al. 2016) makes no mention of the term \u201cAlternating Q learning\u201d.\n\nThe novelty here would be that the authors propose incorporating an Adam-like optimizer and periodically resetting the ADAM parameters. I would not consider using Adam to be sufficiently novel for publication in this venue, and the results from using parameter resetting are not so spectacular or convincing that they qualify, either. Since no ablations are given, I suspect some of the improvement could have come from just using Adam.\n\nFinally, the convergence proofs given seem to hold only in the tabular case--not in the case when the Q function is an approximation. Generally, proofs only show that Q-learning converges in the tabular case. If these proofs held in the function approximation case, this would be a surprising breakthrough."}