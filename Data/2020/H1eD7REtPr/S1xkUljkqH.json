{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper describes a method to improve the AltQ algorithm (which is typically unstable and inefficient) by using a combination of an Adam optimizer and regularly restarting the internal parameters of the Adam optimizer. The approach is evaluated on both a synthetic problem and on Atari games.\n\nThe core of the approach (simply replacing the optimizer with Adam) is relatively simple and the restarts seem mostly to improve variance rather than return over the vanilla Adam approach. It's hard to see what additional value the convergence analysis provides over the AMSGrad convergence analysis. Especially when the convergence rate appears to be the same for AltQ-AMSGrad and AltQ-AMSGradR for large r. Overall, it seems the approach of using Adam for the optimizer in AltQ seems to be too trivial an improvement and the difference between the Adam with restarts and Adam without restarts also seems to be relatively insignificant when looking at normalized return on Atari. They also both have increasing variance near the end of training in figure 2, much larger than that of DQN. Another downside of this work is that the convergence analysis is done on AMSGrad instead of Adam which the experimental results are based on, why were experiments not done with AltQ-AMSGrad?\n\nOther comments:\nIn section 4.1: \"to help preventing\" -> \"to help prevent\"\nBottom of page 5: \"most well-performed\" -> \"most well-performing\"\nAbove eq 8: \"step stone\" -> \"stepping stone\""}