{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a method All SMILES VAE that\u2019s used for predicting chemical properties of small molecules and also for optimizing the structures of these molecules. The authors evaluate their model on the Zinc250K and Tox21 dataset and report that they are able to exceed the previous SOTA. This paper was well written, and did a good job with explanations and illustrations. \n\nThe central idea is to use a RNN to learn representations of strings encoding molecular structures (SMILE strings). SMILE strings are constructed by a DFS traversal over molecular graph structures. The authors choose to feed in distinct SMILE encodings of the same molecule in parallel, resulting in a stacked RNN architecture. They observe that since SMILE strings are DFS traversals of the molecular graph, propagation in the RNN corresponds to sequential message passing steps between nodes in the graph. This is in contrast to a graph neural network wherein all nodes simultaneously broadcast messages to their neighbors at every propagation step.\n\nWhile it\u2019s interesting to be able to optimize molecules in the space of SMILE strings, the impact is less clear. The authors mention in Sec 3.1, that graph models e.g. GCNs have higher overall compute complexity O(b^2), compared to their method which has O(Mb); however this does not do complete justice. Most graph propagation operations benefit from heavily parallelizable sparse matrix operations on GPUs. Infact, they can actually be much faster than RNNs in practice since GCNs need only a fixed number of propagation steps (independent of the number of bonds).\n\nIn Sec 3.2, the authors describe their approach for constraining the space of molecular optimization. While this may lead to directed searches, it will prevent truly novel molecules from being synthesized. The authors will need to address and provide experiments with unconstrained search.\n\nIt was not clear if the authors implemented any of the baselines? It seems from the text of figure 5, that the SSVAE and GraphConv results have been taken directly from the paper. The authors can strengthen their claim by replicating the results of baselines and making sure that they agree.\n\nSome clarification questions:\n - For optimized/predicted strings, which are presumably novel molecules not in the dataset, how is the true chemical property  e.g. logP, determined?\n - In Figure 6, from steps 30 - 40, it seems that the predicted logP goes up but the true logP stays the same in a few cases. Why is this the case?\n\nOverall, I think that this paper has some interesting ideas and is well written. However, the novelty and impact of the model is somewhat lacking. If this were introducing a new application area to this field, then I think the case for acceptance could have been stronger, however there has already been a lot of work related to molecular property prediction/design. \n\nAlso, AC please note, I would have given the paper a score of 5, but Openreview only gave me a choice between 3 and 6. So, I went with 3, but please consider this to be a 5."}