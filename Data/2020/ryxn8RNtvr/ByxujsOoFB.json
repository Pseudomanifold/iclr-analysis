{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThis paper proposes a novel scheme to aggregate local explanations\nfrom LIME to generate global and/or class-level interpretations in the\nform of feature importances. The technical novelty lies in the\nnormalization of the feature weights across local explanations where\nthe proposed scheme utilizes a well-motivated L0 normalization in\nplace of the existing L2 normalization in Ribeiro et al., 2016. The\nsparsity induced by the L0 normalization leads to more \"easy-to-read\"\nglobal (or class level) \"explanations\" since the number of features\nwith non-zero importances is reduced.\n\n\nThe simple idea of generating sparse aggregations of local\nexplanations to obtain global (or class-level) interpretations is well\nmotivated and seems fairly intuitive (and I consider simplicity a\nstrength) and the extremely favourable empirical results, I am leaning\ntowards a reject for the current version of the paper. The main\nreasons for this decision are as follows: \n\n- The proposed scheme is presented as closely tied to LIME, which\n  severely limits the scope of the proposed solution. However, I can\n  imagine that the high level idea of sparse aggregation of local\n  explanations can easily generalize to many local explanation\n  schemes. \n- The empirical evaluation needs to be improved (or better clarified)\n  in my opinion. There are many potential baselines that are also not\n  considered (see details below). Moreover, the evaluation is\n  completely limited to a single data set (which is good as a strawman\n  but not sufficient to make significant claims).\n\n\nClarifications re: baselines and empirical evaluations:\n\n- No comparison to Ibrahim, et al., 2019 -- you can cluster in\n  unsupervised setting but you can also aggregate within class in\n  supervised setting. Generating class-level interpretations is an\n  interesting idea but its novelty is somewhat unclear.\n- In terms of explanations, there are contrastive explanations [1]\n  which are not really discussed in this paper. This scheme could be\n  more useful for generating class level interpretations.\n- It is not clear why the negative correlations were removed in the\n  user study. In certain situations, such as when distinguishing\n  between digits 3 & 8, the absence of weights (potentially\n  corresponding to negative weights) can be crucial explanations.\n- It is not clear why in the SmoothGrad and VarGrad baselines, the\n  class level interpretations are generated with just 10 images per\n  class. Are the class level interpretations of LIME and NormLIME also\n  generated from just 10 images per class. If that is the case, 10\n  seems like a very small number and can potentially create noisy\n  explanations. If LIME and NormLIME used more than 10 images to\n  generate class level interpretations, the comparison does not seem\n  fair. Please explain this discrepancy.\n- For the evaluation in Sec. 5, it is not clear how the features are\n  removed. Are they removed based on the global explanations (feature\n  importances) of each baseline? If this is the case, are the global\n  explanations of each baseline generated using the same amount of\n  local explanations? Please clarify.\n- The KAR analysis results in Fig. 4(b) need to be further\n  investigated. First, it is not clear if this difference between the\n  different baselines something that happens in multiple datasets or\n  just this one. Moreover, it seems natural that something like\n  SmoothGrad and VarGrad would be able to capture the feature\n  redundancies if the redundancies were the actual cause for this\n  marked difference between the relative performances in Fig. 4(a) and\n  4(b). This could definitely use a better discussion.\n\n\n[1] Dhurandhar, Amit, et al. \"Explanations based on the missing:\nTowards contrastive explanations with pertinent negatives.\" Advances\nin Neural Information Processing Systems. 2018. "}