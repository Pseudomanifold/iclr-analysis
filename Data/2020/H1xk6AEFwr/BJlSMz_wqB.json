{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to use a fixed set of response classes and build a classification model leveraging advances in pre-training strategies to pick a response from the bank to return rather than using a generative model. The main idea is to have  a stronger quality control over the generated responses while trading from the flexibility. Authors evaluate their proposed approach compared to generative models through expert human evaluations, where they show the responses generated by the proposed approach is worse than the doctor\u2019s 12% of the time, whereas this number is 18% for generative models. While the paper\u2019s objective of getting more control on the quality is an interesting direction, it is not clear how the proposed solution is particularly novel or significant because it seems to execute a pipeline of existing methods to first cluster the responses that appear in training data, and then train a classifier on the created response classes. More precisely, it is not clear what exactly is claimed as the central contribution of the paper. Overall, this paper needs further work and improvements to support that the proposed method is applicable in the general multi-domain dialog scenario and superior to generative models.\n\nHere are some of my questions and concerns about the paper:\n\n# The applicability of this approach seems limited to be considered as a general approach for dialogue response generation especially because it relies on several hand-crafted steps and human experts (for Step 5) to finalize response classes that can lead to a better performance than generative models. So, it not clear how applicable/costly the proposed method would be for open-domain or multi-domain dialogue. It would be useful to include experiments and comparisons on some other dialogue datasets.\n\n# My another major concern regarding applicability is that it is hard to imagine many realistic scenarios where a few hundred predefined responses would be enough to cover the complexity and diversity of person-to-person dialogues (even in patient-doctor conversations). One interesting study to report would be to do an additional analysis over the test data by cheating to compute an upper bound on the BLEU score: Compare the ground-truth response with the response classes to find the best matching class, and compute the BLEU score by using the best matching response class for the ground-truth response in test data. This would give some idea on the applicability of the proposed method and its generated response classes in the scenario where the classifier is assumed to be perfect.\n\n# I am also curious about the development cycle of this approach. What is the criterion to decide on the final set of response classes and their quality? More precisely, is human evaluation the only way of testing the quality of response classes and hence the proposed model? And if so, how would one be able to adapt this approach to their own problem or domain without intermediate automatic evaluations? It would be useful to include a brief discussion on this.\n\n# Including a discussion on the final response classes as well as some examples of these in the form of (centroid, a few other responses from this class) would be helpful. It would also be helpful to see some qualitative examples of actual response generations comparing the proposed approach and the generative baseline.\n\n# Details of the human evaluation (#raters, rater agreement, etc.) along with its design choices should be discussed in more detail. In terms of its design for example, why are categories (a) and (c) treated differently? Or, is there a reason why the authors did not ask raters to select between discriminative response and generative response, comparing them head-to-head?  Besides, it would still be useful to include automatic evaluation metrics (e.g., BLEU, ROUGE, etc.) for the evaluated models. If nothing, it would help us understand the correlation between BLEU score and human eval in this domain. Can the authors include common automatic evaluation metrics like BLEU and ROUGE?\n\n# A few relevant works like [1*, 2*] are missing from the discussion.\n\n\nMinor comments and questions:\n\n# I am wondering why the patient-doctor data experimented with in the paper is considered multi-domain.\n\n# It is not entirely clear how the discriminative model can generate a higher quality response than the gold response. It would be nice to see an example or a more general scenario when this happens.\n\n# If step 5 for response class generation is not skipped, after the manual merging, which text becomes the centroid? And which text from the response class is used to return in the inference time?\n\n\nRelated work:\n\n* [1*] Exemplar Encoder-Decoder for Neural Conversation Generation, Pandey et al.\n* [2*] Search Engine Guided Neural Machine Translation, Gu et al."}