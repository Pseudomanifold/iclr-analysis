{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Adversarial neural pruning\n\nThis paper proposes \u201cadversarial neural pruning\u201d method and vulnerability suppression loss to defend against adversarial attacks. For adversarial neural pruning, the authors train a pruning mask in an adversarial way that benefits both the clean accuracy and the adversarial robustness. The authors also propose a new loss function \u201cvulnerability loss\u201d that measures the robustness of intermediate features in neural networks. By using both techniques, the authors demonstrate through experiments that it is an effective method to defend against adversarial attacks.\n\nI have the following questions:\nQ1: Don\u2019t understand definition 1 and definition 2?\nI don\u2019t quite understand the meaning of definition 1 and 2. It seems useless for the presentation of the paper to me. Can the authors illustrate why you define these two notions here? Is there any usage in this paper for defining (\\epsilon, \\delta)-robust and (\\epsilon, \\delta)-vulnerable?\nQ2: How do you compute the vulnerability of a network?\nThe authors define the vulnerability of a feature in equation (1). However, I can not find how such vulnerability measure is computed exactly. Can the authors specify how they compute this  vulnerability measure?\nQ3: In page 4 \u201cWe emphasize that our proposed method can be extended to any existing or new sparsification method in a similar way\u201d\nThis claim is not carefully discussed in this paper. Can your approach be applied to [1]? \nQ4: Title does not properly cover the content of this paper?\nThe author proposed two approaches: adversarial neural pruning and vulnerability loss. However, the title seems to only cover one of them. It is misleading since vulnerability loss seems also to be a major part of the proposed method. Therefore, either the title needs to be changed or the structure of this paper should be altered to highlight adversarial neural pruning approach.\n\n[1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015."}