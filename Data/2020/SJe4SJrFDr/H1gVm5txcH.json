{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an approach to improve DNN robustness to adversarial attacks. The main novelty relies on defining vulnerability in the latent feature space. The algorithm trains with an additional term to promote similar features between an input and its adversarial counterpart.\n\nI like the paper and the idea behind. Here are some comments that would be nice to address.\nMethod:\n- Efficiency-wise, how is this implemented?  If I understand correctly, the training process is less efficient as the features need to be stored and compared, right?\n- Any guarantees to make sure the vulnerability does not reduce the capacity of the model?\n\n- I like the idea of moving the robustness to hidden layers. The paper proposes a metric (the regularizer) to get features closer. While that sounds interesting (I assume resulting features would be agnostic to the input), the evaluation shows that the metric does not correlate with the robustness. \n- I would rephrase all around the objectives for training the model J(\\theta...). \n- I find difficult to follow around Eq. 7\nExperiments:\n\n- The way it is written, is very confusing: ANP leads to 88% reduction in memory while maintaining similar robustness... what about clean accuracy? If clean accuracy is not valid, the entire system is irrelevant.\n- I am not sure how to read the numbers. I can understand some measures are relevant (accuracy to attacks) but at the expense of a huge clean accuracy drop (69% to 57%). I rather would tune the proposal to achieve similar accuracy and then be able to compare just the robustness. \n- In the experimental section, I am missing the size of the resulting network. As there are neurons set to zero, that value is relevant. \n- The contribution of VS seems relevant over AT but marginal on ANP. Please, elaborate. \n- Lack of correlation between vulnerability and robustness in the experiments. Please, elaborate. \n\n\nOther comments:\n\n- I think the overall organization needs improvement. The experimental section has plenty of numbers repeated in different tables and that consumes space and is confusing. "}