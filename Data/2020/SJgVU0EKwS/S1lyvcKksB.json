{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper outlines a new method that allows using a variety of precision in the numerical representation of the network to increase performance (both in terms of accuracy and speed). They learn a threshold value for which all activation values above the threshold are learned at full precision, while all below are learned at reduced precision. This enables substantial performance gains. \n\nThe authors summary of the contributions made in the paper is accurate.\n\nThe paper is well written and clearly articulates a contribution to the literature. As such, I think it should be accepted. However, the major question I had as I read the paper was the efficacy on GPU, which the paper discusses, but does not implement, nor show any empirical results for, which weakens the paper. Most deep learning happens on GPUs (or similar accelerators), so until this technique is implemented there, it is of limited use. It is still a contribution to the literature, but the paper would be significantly strengthened with a GPU implementation. Additionally, the experimental evidence is lacking. More experiments would also strengthen the paper.\n\nIf these changes were made I would change my score to 8 (accept). I do think that the work is slightly premature, and would benefit significantly from adding GPU results and additional experiments. The contribution is strong, however, and should be published in some form, either now, or at a future date.\n\nFor the experimental setup, I had a few questions: \n\n1) What happens with fixed thresholds? E.g. doing a sweep over fixed values. \n\n2) How do the results vary for different initialization schemes? \n\n3) How do the results vary with the 5 hyperparameters listed? How were they chosen?\n\n4) How consistent are the results? i.e. what happens if the experiments are repeated N times? Do we see the same values?\n\nIn short, I would like to see more experiments. The results are encouraging, but brief. Evaluating on more architectures would strengthen the paper.\n\nOverall questions:\n\n- How well does the 1.2% improvement in perplexity compare to SOTA? Please add context for the numbers reported. It's not at all clear how good of an improvement is seen. \n- How do the results change with top-5 accuracy vs top-1 accuracy? \n\n\nNotes which did not affect the review score:\n\n- There are some typos, e.g. \u201cPG computes most features in a low precision and only a small proportion of important features in a higher precision.\u201d Saying \u201cPG computes most features using reduced precision and only a small proportion of important features using high precision\u201d would be more correct. There are similar typos throughout that I have not listed.\n- Tables 3 & 4, and Figure 4, are very cramped and hard to read.\nTable 1 and 2 are quite crowded; can you rearrange them so they\u2019re easier to read?\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}