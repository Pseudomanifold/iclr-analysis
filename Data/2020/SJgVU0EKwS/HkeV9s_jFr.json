{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an interesting quantization technique that is, unusually, end-to-end trainable and not just an inference technique. According to the experiments, the method achieves better performance and computational savings as compared to other quantization method baselines. The results are admirably demonstrated on a variety of models, including CNN and RNN-based neural nets, as well as on several datasets in different domains, including ImageNet, CIFAR10, and PTB. We see the method seems to generalize across all of these.\n\nNevertheless, while I found this is very interesting work, I have a number of issues with the experiments, which I'll go into below. I feel this work is being released prematurely and could use some more polish to help sell the method better. Below are a few remarks and questions for the authors that would be helpful to be answered.\n\n* Why only report on ResNet-18? It would be far more useful to show numbers against ResNet-50. It would also be useful to show the non-quantized best results on these models and datasets.\n* I wish more effort had been spent to analyze the experiments. For example, I am not sure I understand the effects of the threshold on this method. What happens when it's set manually?\n* How exactly is computation cost savings calculated so crudely? If it uses B_avg, why not calculate the bitwidth per layer and sum things up? Using B_avg strikes me as being quite crude.\n* When the authors address runtime changes except in table 6, they changed their baseline to a vanilla ResNet-18 with dense weights. What are the runtime effects relative to ShuffleNet and ShiftNet?"}