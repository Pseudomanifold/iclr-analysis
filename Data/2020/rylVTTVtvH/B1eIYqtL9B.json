{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a new type of graph embedding technique for dynamic graphs based on tensor representation (node x feature x time).  Experiments on edge classification demonstrate improved prediction accuracy. \n\n+ Clear writing with tensor notations and explanation is well-structured \n+ Improved prediction results on 3/4 real-world dynamic graph datasets\n\n- The theoretical results are a bit artificial. The tensor eigendecomposition used in this paper and [Kilmer and Martin] is for slices of the tensor, similarly for FFT and convolution. The technique is a trivial generalization from matrix results. \n- The paper is missing a large body of baselines, both from the network science community (non-deep learning methods) and from this community (diffusion convolutional RNNs, graph attention networks, etc). \n- The method doesn't scale well, especially for graphs with long-term dynamics. It would be good to show the scaling behavior of the proposed model. "}