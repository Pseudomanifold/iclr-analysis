{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: this work uses tensor methods to improve graph convolution for dynamic graph, where the nodes are fixed and the edges are changing. Specifically, it uses the M-product technique to develop the operations of sequence of matrices that analog to these operations of matrices. In the M-product notations, everything seems to be as neat as matrix operations. The works also shows decent supremacy on edge classification tasks.\n\n\nComments: this paper is mathematically interesting. It is well-written in general, but the definitions are dense and hard to follow.\n\nIt will be better to give some examples of M-product. For example, what these operations will be if we choose M to be the identity matrix?\n\nM-transfer is a tensor contraction, right?\n\nIt seems if you do the operations of the sequence of matrix, there is no need to do iterations like RNN. I am interested in how this will influence the runtime and memory cost.\n\nThe M matrix is defined as a lower triangle matrix such as (A \\times M)_::t depends on A^(1:t). Is it possible to formulate M such that (A \\times M)_::t will depend heavily on A^(t), and less on the farther matices? such that we encode some Markov property?\n\nDoes there exist some condition when this method will be equivalent to RNN?\n\n\nDecision: I feel this work novel and interesting in general. I would like to weakly accept it.\n"}