{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a M-product based temporal GCNs to handle dynamic graphs. Experiments on four real datasets are performed to verify the effectiveness of the proposed model.\n\nOverall, I think this paper make a few contributions to advocate tensor M-product. However, there are several big issues as listed below. Given the current status, I could not accept the paper.\n\nPros:\n\n1, The generalization brought by M-product seems to be general as it includes quite a few graph convolution elements for 3D tensors in a natural way.\n\n2, The experimental setup is reasonable. Datasets are collected from practical problems and of moderately large scale.\n\n3, The paper is clearly written and easy to follow.\n\nCons & Questions: \n\n1, My first concern is that M-product formulation does not bring any new insights as people have already used some of the key elements in practice for a long time. For example, the M-transform is just applying 1x1 convolution to multi-channel image. Slice-wise matrix multiplication is also common in practice.\n\n2, Moreover, I think there are several challenges in the M-product formulation which prevent the technique from being practical.\n\n(1) Sharing M such that frontal slices of the transformed signal are the same, i.e., each row of M share the same vector, limits the model capacity significantly. If there is no sharing mechanism, then the model learned on one sequence of graphs could not be applied to another sequence of graphs given two sequences have different lengths. \n\n(2) If you learn M from data, how could you ensure that M is invertible? In the paragraph before section 4.1, an edge classification formulation is proposed where the inverse M-transform is abandoned. However, if in practice, you do not need the inverse transform, then do those theoretical properties still hold and what is the meaning of introducing such M-product formulation?\n\n3, A few temporal GCN baselines are neither compared or discussed, e.g., [1]. \n\n4, Could you explain why all the other GCN variants performs significantly worse with a symmetrized adjacency matrix compared to using the asymmetric one? \n\n[1] Li, Y., Yu, R., Shahabi, C. and Liu, Y., 2017. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926.\n"}