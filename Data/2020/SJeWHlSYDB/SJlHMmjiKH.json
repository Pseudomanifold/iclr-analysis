{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduced a way to modify densities such that their support agrees and that the Kullback-Leibler divergence can be computed without diverging. Proof of concept of using the spread KL divergence to ICA and Deep Generative Models ($\\delta$-VAE) are reported based on the study of spread MLE.\n\nComments:\n\nIn Sec 1, mention that f should be strictly convex at 1. Also mention \nJensen-Shannon divergence, a KL symmetrization, which is always finite \nand used in GAN analysis.\n\nIn Sec 2, you can also choose to dilute the densities with a mixture: \n(1-\\epsilon)p+\\epsilon noise.\nExplain why spread is better than that? Does spreading introduce \nspurious modes?, does it change distribution sufficiency? \n(Fisher-Neymann thm)\n\nIn Formula 4, there is an error: missing denominator of \\sigma^2. See \nAppendix D too.\n\nIn footnote 4, page 8, missing a 1/2 factor in from of TV (that is upper \nbounded by 1 and not 2)\n\nKL is relative entropy= cross-entropy minus entropy. What about spread KL?\nIn general, what statistical properties are kept by using the spread? \n(or its convolution subcase?)\n\nIs spreading a trick that introduces a hyperparameter that can then be \noptimized for retaining discriminatory power, or is there\nsome deeper statistical theory to motivate it. I think spread MLE should \nbe further explored and detailed to other scenarii.\n\nSpreading can be done with convolution and in general by Eq.3:\n\nThen what is the theoretical interpretation of doing non-convolutional \nspreading?\n\n\nA drawback is that optimization on the spread noise hyperparameter is \nnecessary (Fig 3b is indeed much better than Fig 3a).\nIs there any first principles that can guide this optimization rather \nthan black-box optimization?\n\nOverall, it is a nice work but further statistical guiding principles \nor/and new ML applications of spread divergences/MLE will strengthen the \nwork.\nThe connection, if any, with Jensen-Shannon divergence shall be stated \nand explored.\n\nMinor comments:\n\nIn the abstract, state KL divergence instead of divergence because \nJensen-Shannon divergence exists always.\n\n\nTypos:\np. 6 boumd->bound\nBibliography : Cramir->Cramer, and various upper cases missing (eg. \nwasserstein ->Wasserstein)\n"}