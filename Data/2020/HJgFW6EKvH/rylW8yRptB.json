{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a new method for generating adversarial examples for the task of automatic speech recognition. The authors suggest using their method called Iterative Proportional Clipping, which limits the amount of change we allow at every time-step. \n\nOverall this paper is an incremental research work. The paper is clearly written. The idea is intuitive and well presented.\n\nI have several questions to the authors: \n1) Can the authors provide more details regarding the attack setup? Why exactly did you run the attack in two stages? What makes the difference between the train and eval modes? Only on the batch-norm layers? \n\n2) All experiments were conducted in a white-box settings, did the authors try to explore gray/black box settings as well?\n\n3) It seems like there are phase mismatch issues in the generated adversarial examples. Did the authors try to generate adversarial examples using other approximations besides the MFCC to wav approximator? Maybe working at the spectrogram magnitude level? \n\n4) Regarding comment (3). the underlying assumption of adversarial examples is: \"it will not be distinguishable from human ears.\" However, the changes to the signal are highly noticeable. Did the authors try to analyze when is it more or less noticeable, under which settings? Does it depend on the target text? "}