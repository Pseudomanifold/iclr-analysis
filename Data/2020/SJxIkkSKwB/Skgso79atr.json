{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a solution for batch active learning with noisy oracles in deep neural networks. Their algorithm suffers less from the well-known cold-start issue in active learning. They also improve the robustness by adding an extra denoising layer to the network. \n\nThe main concern is that the two contributions are rather orthogonal to each other and each of them is not that significant. \nThe first contribution, which alleviates the cold-start problem, is not very surprising, since it is a soft version of previous method BALD. \nThe second contribution, a de-noising layer, is relatively orthogonal to batch active learning. \n\nIn the experiments, the authors compared Proposed\u2028+noise with Proposed, Random, BALD, Coreset, and Entropy, but I think the only fair comparison here is between Proposed+noise and Proposed. \n\n"}