{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a self-distillation algorithm for training an over-parameterized neural network with noisily labelled data. It is shown that for a binary classification task on clustered data (same as [Li et al. 2019]), even if the labels are corrupted, the self-distillation algorithm applied on a sufficiently wide two-layer network can recover the correct labels (in l_2 loss). Experiments on CIFAR-10 and Fashion MNIST are provided, which show that the self-distillation algorithm is effective for label noise with relatively little tuning.\n\nAlthough the theoretical part of the paper has a large overlap with [Li et al. 2019], I find the self-distillation algorithm very interesting and it's nice that it can achieve zero l_2 loss w.r.t the correct labels. However, I think the paper could still use some improvement.\n\n1. The theorem in Section 3.3 is only for binary classification. Can it be generalized to multi-class classification?\n\n2. The theoretical guarantee is only for training data. Is it possible to prove a generalization bound? There's a remark on top of page 8 about margin. It would be nice to elaborate on this and maybe make it formal.\n\n3. Section 2 is not very satisfying. I don't quite see the point of this section. In particular, the concept of AIR is nothing new and its connection to NTK top eigenspace has already been written in previous work (e.g. [Arora et al. 2019]). I'd suggest to not have this section and to make Section 3 the main contribution of this paper."}