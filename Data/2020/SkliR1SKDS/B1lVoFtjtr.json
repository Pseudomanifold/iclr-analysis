{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper proposes a method for performing data augmentation using self-supervision and self-distillation. One of the main problems of data augmentation is that the data transformations have to be tunned for each dataset. E.g., in MNIST a \"9\" rotated 180 degrees would be a \"6\". This paper proposes a method using self-supervision (i.e., predict the rotation and color channel flips). Instead of having a multitask loss (i.e., one loss for predicting the object class and another for the self-supervision transformation), this paper proposes to frame the problem as a single classification problem where the new label is obtained by the permutation of object labels and self-supervised ones.\n\nThe method is tested in 3 scenarios: image classification, few-shot image classification, and imbalanced data image classification. In the first case, the proposed method is compared with performing data augmentation (without hyperparameter tunning in the best setting, I assume it is just the same transformations used for the self-supervised case) and with the multi-task setting (where the self-supervised and object classification tasks are in different losses). The results show that DA performs worse than the baseline (Obviously because the parameters used does not make sense) and the same for multitask learning. Then several ablation studies are done and also experiments on a few-shot and imbalanced classification.\n\nMain problem:\n\nThe evaluation is not fair. In the case of image classification, the method should be compared to standard data augmentation with correct parameters and with SOTA methods of automatic data augmentation which do not require tunning the DA parameters. It should be compared at leat with:\n - AutoAugment: Learning Augmentation Policies from Data\n - Fast AutoAugment\n - Adversarial Learning of General Transformations for Data Augmentation\n - DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification\n - TANDA: Learning to Compose Domain-Specific Transformations for Data Augmentation\n \nThe evaluation for few-shot and imbalanced is compared with other methods of the literature. However, no-one of these methods use data augmentation and then the comparison is not relevant."}