{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "To solve the distributional discrepancy when performing data augmentation, this paper proposes to learn the joint distribution of the original and self-supervised labels of augmented examples. The idea is easy to understand and implement. The authors then perform an empirical study on a standard setting (e.g. CIFAR 10/100) and a limited data setting (e.g. few-shot). \n\n\nI have some concerns as follows:\n\n1. The proposed method is not scalable. For a classification task with 1000 classes and 10 rotations, there will be 10000 labels to predict. \n2. AutoAugment (https://arxiv.org/abs/1805.09501) can learn a transform function to avoid the distributional discrepancy, but the authors did not compare theirs with this strong baseline. \n3. The experiments done on MIT-67 seems not meaningful to me, as the dataset is old and small (published in 2009). "}