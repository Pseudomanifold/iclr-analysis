{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "GEOM-GCN: GEOMETRIC GRAPH CONVOLUTIONAL NETWORKS \n\nThe paper introduces a novel GCN framework, whose purpose is to overcome weaknesses of existing GCN approaches, namely loss of structural neighbor information and failure to capture important dependencies between distant nodes. The paper uses a mapping from nodes to an embedded space and introduces a second type of a neighborhood: a proximity in the embedded space. In the embedded space, a set of relations of nodes is defined. For each node v, the paper uses a 2-stage convolution scheme: 1) for each neighborhood type, the nodes in the same relation with v are combined; 2) the resulting nodes are again combined into a new feature vector. This approach allows one to overcome the issues described above. The experiments show that in most cases the approach outperforms the existing GCN solutions, sometimes with a large gap.\n\nI have the following concerns about the paper:\n-- My main concern is the learning time, which is an issue for a straightforward GCN implementation. There were multiple attempts to decrease it (GraphSAGE, FastGCN, etc.). Therefore, I would like to see running times on the presented graphs as well as on relatively large graphs (see e.g. https://arxiv.org/pdf/1902.07153.pdf for candidates). If some techniques were used to make the implementation faster, I would be good to include them in the paper (or, if they are standard, they should be referenced). At the very least, I believe it should be prioritized as a future direction.\n-- It\u2019s unclear why we should use the same latent space and the same \u03c4 for both N_g and N_s. I would expect that mapping into different spaces could provide better results: the two neighborhood types seem very different, and I don\u2019t see why the neighbors should be aggregated in the same way. If using different spaces doesn\u2019t provide an improvement, an explanation for this would be very useful.\n-- \u03b1 and \u03b2 are defined and shown in Table 2, but they are never used (as it stands now, \u03b1 and \u03b2 can simply be removed). If the results in Table 3 correlate with them, then this dependence should be highlighted. In such case, it would also be better to move \u03b1 and \u03b2 to Table 3.\n-- The paper uses 3 different node embedding strategies. These strategies can be combined in q with different weights (which can be learned as hyperparameters). Will it produce the best of 3 (or better) result?\n-- \u201cWe use an embedding space of dimension 2 for ease of explanation\u201d But what \u03c4 is used in the real implementation?\n-- There are various GCN implementations; however, the comparison is performed with only 2 of them. I would like to see either comparison with more implementations, or the explanation why the comparison with the given two suffices.\n-- Is it possible to make the implementation available?\n\nWhile there are a lot of possible improvements, I believe that some of them can be addressed in a future research, and the paper\u2019s novel approach is noteworthy in itself. My current verdict is 5/10, and I\u2019ll be happy to improve it if the above issues are fixed.\n\nPresentation issues:\n-- The notation used in definition of m_v^l is unclear.\n-- Why \u03c4 is a part of each node\u2019s structural neighborhood? It\u2019s a global function, isn\u2019t it?\n-- Introduction: I believe that the exact problems which GCNs solve (e.g. node classification) should be mentioned.\n-- The flow in Section 2.1 is a bit weird. Namely, it says \u201cTo overcome the first weakness\u201d, but the first wickness wasn\u2019t stated in the previous paragraph (of course, one can deduce it, and it also was defined long ago, but it\u2019s disturbing for a reader).\n-- Figure 1B is confusing: it looks like the nodes from N_g(v) lie in a small region around v.\n-- I think that splitting Figure 1C into 2 figures would make it clearer.\n"}