{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the challenging problem of learning to generate programs from natural language descriptions: the inputs are sequences of words describing a task and the outputs are corresponding programs solving the task. The proposed approach elegantly relies on tensor product representations. Inference with the proposed model is done in 3 steps: (1) encode the symbolic information present in the text data as a TPR, (ii) maps the input TPR to an output TPR encoding the symbolic relations of the output programs (here the authors use a simple MLP), and (iii) decode the output TPR into an actual program. The parameters of the models used in the 3 steps are learned jointly. For step (iii), the authors proposes a novel way of encoding an n-ary relation into a TPR which facilitates the recovery of the relation's arguments using unbinding operations: this is a neat trick (though I think it increases the number of parameters and may limit the expressiveness of the TPR, since reaching \"full-rank\" of the TPR will occur faster than with the encoding used in [Smolensky et al., 2016]). Experiments on two datasets demonstrate the validity of the approach.\n\nThe paper is very well written and easy to follow. The idea seems original and well executed but I think the experimental section could be improved. In particular, adding/reporting stronger baselines to the comparison would straighten the paper. I also feel some relevant literature may be missing from the related work. Nonetheless, I think it is a good paper which will be relevant to the community, I thus recommend acceptance.\n\n* Comments / Questions *\n\n- Section 3.1.1: if I understand correctly, the length of the sequence affects the rank of the TPR. Could that be a problem in practice? E.g., the capacity of the TPR could likely be saturated quickly for long sequences?\n\n- Section 3.2.1: the filler vector f_t = Fu is computed as a convex combination of the learned filler vectors. Is it a design choice to choose a convex combination rather than taking the column corresponding to the argmax of the vector u? Or is it because otherwise the model can not be trained using the classical backprop approach? \n\n- The results of the Seq2Tree+Search model from (Bednarek et al. (2019)) is not reported in Table 2. Why? I believe it should be included (it is ok that it outperforms the proposed method. In addition you can maybe identify clear advantages of your method illustrating a trade-off, e.g., running time, end-to-end, scalability ...). \n\n- A more thorough ablation study could also improve the strength of the experiments. For example, do you know to which extent the attention model in the decoder is necessary to achieve good performances?\n\n- I am not very familiar with the literature but it seems some relevant work may be missing from the review. In particular, I believe there are many papers tackling the problem of learning programs from input output examples or execution traces, e.g. \"DeepCoder: Learning to Write Programs\", \"Neural Turing machines\",  \"Inferring algorithmic patterns with stack-augmented recurrent nets\", \"Inferring and Executing Programs for Visual Reasoning\", \"Learning to infer graphics programs from hand-drawn images\"... This list is by no means meant to be exhaustive in any way, just to illustrate a large body of work that seems relevant to the present paper (even though I understand that those papers do not consider natural language description as inputs).\n\n* Typos *\n\n- Eq. (5) Should be r' instead of r_i' (?)\n"}