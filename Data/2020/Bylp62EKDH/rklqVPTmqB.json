{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors analyze the gradient of easy positive and hard negative pairs and propose a second-order loss for metric learning. Here are my concerns.\n1.\tThe idea of weighting different pairs is not new and can be found in [1].\n2.\tThe comparison is not convincing. Most of existing methods apply Inception as the backbone while this work adopts ResNet50, which has a better performance than Inception. Authors should adopt the same backbone for the fair comparison.\n3.\tThe results in comparison are outdated. Please include the SOTA results, e.g., those in [1].\n4.\tThe improvement from the second order term seems not significant. Besides, the second order term works as a data-dependent margin. Authors should compare it to a fixed margin to illustrate the effectiveness.\n\n\n[1] CVPR\u201919: Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning\n"}