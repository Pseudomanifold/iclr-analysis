{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows how to use the Discrete-time Algebraic Riccati Equation (DARE) to provide infinite horizon stability & optimality to differentiable MPC learning.  The paper also shows how to use DARE to derive a pre-stabilizing (linear state-feedback) controller.  The paper provides a theoretical characterization of the problem setting, which shows that prior work on differentiable MPC learning may lead to unstable controllers without the proposed augmentations using DARE. \n\nI'm not sure I understand the implications of imitating \"from an expert of the same class\".  Can the authors elaborate?\n\nCan the authors compare & contrast with this paper? \nhttps://arxiv.org/abs/1709.07174 \n(I have my own views, but I'd like hear the authors' thoughts first)\n\nMy biggest complaint is with regards to the experiments.  Unless I'm mistaken, it seems there isn't a thorough empirical study of the theoretical claims, especially as it relates to previous work.  E.g., can one construct scenarios where the baseline approach (Amos et al., 2018) fails, and compare with the proposed approach?\n\nThe idea of pre-stabilization is interesting, and seems related to this paper: https://arxiv.org/abs/1905.05380"}