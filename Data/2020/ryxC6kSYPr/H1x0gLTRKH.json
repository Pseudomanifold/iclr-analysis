{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows how to make infinite-horizon MPC differentiable\nby differentiating through the terminal cost function and controller.\nRecent work in non-convex finite-horizon continuous control [1,2,3] face\na huge issue in selecting the controller's horizon length and\nbetter-understanding differentiable infinite horizon\ncontrol has potentially strong applications in these domains.\nAs a step in this non-convex direction, this paper provides a nice\ninvestigation in the convex LTI case.\nThe imitation learning experiments on a small spring dynamical\nsystem are a necessary sanity check for further work, but\nmany other more complex systems could be empirically studied\nand would have made this paper stronger.\n\nOne point that would be useful to clarify: the DARE solution in (7,8) is\nderived to optimally control a LTI system *without* control/state bounds but\nis then used to control the LTI system *with* control/state bounds in (4).\nDoes this lead to suboptimal solutions to the true infinite-horizon problem?\n\n[1] Chua, K., Calandra, R., McAllister, R., & Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. NeurIPS 2018.\n[2] Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. Learning latent dynamics for planning from pixels. ICML 2019.\n[3] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba. Benchmarking Model-Based Reinforcement Learning. arXiv 2019."}