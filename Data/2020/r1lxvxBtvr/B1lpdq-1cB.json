{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a way to enforce a partial ordering in the relation representations learned by a knowledge graph embedding model and outperforms KALE on FB122. However, it misses a larger set of related works in the knowledge graph learning literature and only compares on a subset of knowledge graph embedding tasks.\n\nMajor \nThere is a number of related works missing in this paper:\nOne of the early papers incorporating logical constraints into knowledge graph embedding models is: Rockt\u00e4schel et al. Injecting Logical Background Knowledge into Embeddings for Relation Extraction. in: Proceedings of NAACL-HLT. 2015.\nAn approach that does this for relation embeddings by enforcing a partial ordering is: Demeester et al. Lifted Rule Injection for Relation Embeddings. in: Empirical Methods in Natural Language Processing (EMNLP). 2016\nA recent method for incorporating logical constraints (such as partial ordering of relations) is: Minervini et al. Adversarial Sets for Regularised Neural Link Predictors. in: Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI). 2017 \u2013 a comparison to their numbers for FB122 should be added.\nMost state-of-the-art knowledge graph embedding models are also evaluated on FB15K237 and WN18RR that is missing from this paper and makes comparison to prior work hard \u2013 see Lacroix, Timoth\u00e9e, Nicolas Usunier, and Guillaume Obozinski. \"Canonical tensor decomposition for knowledge base completion.\" ICML 2018.\nThis is unrelated to the content of the paper, but in terms of style my suspicion is that the authors tinkered a lot with the LaTeX spacing in this submission. There are many places that look suspiciously crammed and squeezed together. This is not only violating the formatting instructions, but also makes the paper harder to read. Moreover, it seems also unnecessary as you could have gone up to 10 pages of main text. Moreover, citations are missing the year of the publication.\n\nMinor\np1: The argument \"cannot embed n-ary relations in KG's\" is a) not true (see Minervini et al. 2017) above and b) your experiments are also only on binary relations in FB122.\np2: \"such interpretable meta-learning opens up\" \u2013 in what way is this meta-learning?"}