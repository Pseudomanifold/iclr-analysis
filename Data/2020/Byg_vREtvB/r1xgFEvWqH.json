{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper introduces a general framework for distilling expectations of the Bayesian posterior distribution of a deep neural network, aiming to extend the original Bayesian Dark Knowledge approach [1]. More concretely, the generalized framework takes as input a teacher network, a general posterior expectation of interest, a student network, and thus performs an online compression of the selected posterior expectation using iteratively generated Monte Carlo samples from the parameter posterior of the teacher model. The proposed framework is applied to the case of classification models and empirical results demonstrate that distilling into a student model with an architecture that matches the teacher, as is done in Bayesian Dark Knowledge, can lead to sub-optimal performance. It is also shown that student architecture search methods can identify student models with significantly improved speed-storage-accuracy trade-offs.\n\nStrengths:\nOverall, the paper is well written and the relationship to previous works is well described. I personally like the Bayesian Dark Knowledge approach, which combines SGLD and knowledge distillation or dark knowledge, and very happy to see its generalization. Unlike the previous work, it is clearly shown that restricting the student architecture to match the teacher can sometimes lead to a significant performance drop, which provides a basis for guiding future developments.\n\nWeaknesses:\n- I think it is a valuable contribution, but my major concern is that the authors only conduct experiments for the classification task, whereas the original Bayesian Dark Knowledge approach also deals with the regression task and shows some interesting results (see Sect. 3.2 and 3.3 in Ref. [1]). I would recommend the authors to extend the experimental evaluation and provide some insight on how to extend the proposed framework to cover the regression task.\n- On page 5, the choice of loss function does not seem to be discussed. I would like the authors to clarify why cross entropy loss is replaced with l(h, h\u2019)=|h-h\u2019| in the classification case.\n- The size of some figures appears too small, for example Fig. 1 and Fig. 2, which may hinder readability.\n\nAt the moment, I recommend a weak reject as the main weakness is the experimental evaluation, but I could be open to increasing my score if my concerns are addressed.\n\nReferences:\n[1] Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling. Bayesian dark knowledge. In Advances in Neural Information Processing Systems, pp. 3438\u20133446, 2015."}