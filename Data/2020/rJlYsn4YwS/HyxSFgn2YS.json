{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes Deep Learning Alternating Minimization (DLAM) algorithm. First, deep learning optimization problems are formulated as multi-convex Problem 2, by introducing additional variables, constraints and relaxations. Second, alternating update is then used to solve Problem 2, the analysis shows that weights of update converge to a critical point with O(1/k) rate. Finally, some experiments are conducted on MNIST and Fashion-MNIST to show that DLAM is better than SGD, Adagrad, Adadelta and ADMM.\n\n1. DLAM follows dlADMM in Wang et al. (2019), and the only difference between Problem 2 and Problem 2 in Wang et al. (2019) is the indicator function rather than squared l2 loss for a_l. The update is quite similar with dlADMM, with the same convergence result. The technical contribution is incremental.\n\n2. The author claim that Problem 2 is multi-convex. However, I did not see why this is a good point. First, multi-convexity does not imply that gradient update can find global optimum. Second, the results presented here is standard, i.e., O(1/k) convergence to a critical point, which does not show any advantage over Problem 2 in Wang et al. (2019).\n\n3. The experiments are also questionable.\na) The most related dlADMM (Wang et al. (2019)) is not compared here, which can not empirically show why Problem 2 here is a better choice than dlADMM.\nb) The accuracy on MNIST is pretty low. In Wang et al. (2019), the results are about 0.9x, here only 0.7x. Why? And such low accuracy on MNIST seems not convincing to claim \"convergence\".\nc) Please compare with Adam.\nd) What is the ADMM update in the experiments? Directly use ADMM on Problem 1 or 2?\n\n4. The term \"global convergence\" seems misleading. It is not convergence to global optima.\n\nOverall, I found the formulation quite similar with dlADMM (Wang et al. (2019)), the contribution is incremental, the analysis did not show why multi-convex formulation Problem 2 is good (the rate is the same as standard results), and the experiments are questionable and also did not show advantages of the formulation and proposed method."}