{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The motivation for this paper is as follows: \n\nWhy SGD:\n1. Easy\n2. applied in online settings \n\nHowever:\n1. Convergence proof has been done by others, but the assumptions of their proofs cannot be applied to problems involving deep neural networks, which are highly nonsmooth and nonconvex\n2. Suffer from gradient vanish\n3. sensitive to the input\n\nThe paper is based on new ideas  on alternative minimization method. In particular the  loss function is reformulated as nested function associated with multiple linear and nonlinear transformations. This nested structure is then decomposed into a series of linear and nonlinear equality constraints by introducing auxiliary variables and penalty hyperparameters. Then  multiple subproblems are generated which can be minimized alternatively. (using ADMM, BCD)\nHowever, these methods suffer from problems:\n1. Convergence properties are sensitive to penalty parameters.\n2. Lack of unified theoretical frameworks with general conditions. (Not fully proved the\nconvergence. Most of the work is based on some assumptions)\n\n\nMain Assumption: Activation functions are quasilinear functions. \n\n\npros (+):\n1. Generic, easy to extend it to convolutional layers and recurrent layers.\n\n2. Transform the Neural network optimization problem into inequality constrained\nproblem (new point of view)\n\n3. Ensure convexity of subproblems. (quadractic approximation of activation function).\n4. Ensure global minima, and converges to the critical point (where 0 is in the set of\npartial differential of F(F(W,b,,z,a) is the augment loss function) with respect to W*,\nand set of partial differential of F with respect to b*).\n5. The convergence rate is O(1/k), k is the number of iterations of updating (W,b,z,a).\n6. inequality-constraint prevents the output of a nonlinear function from changing much\nand reduces sensitivity to the input.\n7. Matrix inversion is avoided by quadratic approximation\n8. No need strict and complex condition, such as KL properties to prove convergence.\nInstead, this algorithm needs simple and mild conditions to guarantee convergence,\nand cover most of the loss function and activation functions.\n9. The choice of hyperparameters has no effect on convergence.\n10. Much better performance on MNIST\n\ncons(-):\n1. penalty parameters need to be tuned as ADMM does.\n\n2. The regularizations $\\omega$ is l1 and l2, otherwise, it will not be closed-form\nsolutions. The regularization like dropout, batch-norm can not be added into this algorithm.\n\n3. The dataset is relatively naive. Only MNIST and Fashion MNIST. It would be better to include little more sophisticate dataset like ImageNet or Cifar10. Since the computation complexity is O(d^2) where d is the dimension of the features, large scale of image like ImageNet could cause problem. (Comparison: features of MNIST: 196, features of Fashion MNIST:784, features of ImageNet: 65536).\n\n4. The Adagrad has better performance than DLAM on 500*500 on Fashion MNIST and close performance on MNIST. As the authors suggest: DLAM performed competitively for the Fashion MNIST dataset. This can be interpreted as follows:  the proposed algorithm performs well on easy datasets, however the performance is not much better than other algorithms  on Fashion MNIST, which is little more challenging. What would be your comment about this criticism? Can you provide any theoretical insight?  Certainly  it would strengthen the paper if there were more comparisons of  DLAM on more complex dataset. "}