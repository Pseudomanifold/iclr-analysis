{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tackles the problem of bias in target Q-values when performing Q-learning.  The paper proposes a technique for computing target Q-values, by first taking the min over an ensemble of learned Q-values and then taking the max over actions.  The paper provides some theoretical properties of this technique: (1) the bias of the estimator can be somewhat controlled by the size of the ensemble; (2) performing Q-learning with these target values is convergent. Experimental results show that the proposed technique can provide performance improvement on a number of tasks.\n\nOverall, this paper is a modest contribution to the field, since variants of this technique are known and the theoretical arguments are derivatives of known arguments, which places it roughly borderline for an ICLR conference paper.\n\nMy comments:\n-- The paper is very well-written. Thank you for putting the effort to provide clear writing.\n-- The idea of computing a target value as the minimum of an ensemble is well-known in continuous control.  See https://arxiv.org/abs/1802.09477 as well as a number of works which follow it.\n-- The method is motivated as a way to control over/under-estimation.  However, the theoretical arguments show that this depends on N, M, and tau (unknown). Are there any ways to choose N other than hyperparameter tuning?"}