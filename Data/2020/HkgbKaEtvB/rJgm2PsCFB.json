{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an approach to discrete input selection for NNs, using the Gumbel-Softmax trick at its core. It motivates this problem in the context of communicating data over a network with limited bandwidth budget. It proposes constructing different kinds of masks that can be applied over channels or pixels in the input, grounding the discussion in the image domain. This can be seen as a special case of Feature Selection, with image specific substructures motivating the choice of mask types. \n\nThere is very little novelty in this work over that presented in Abubakar Abid et. al. [1], where the idea of using Gumbel-Softmax as a differentiable Feature Selection algorithm has already been expounded at depth, both in unsupervised as well as supervised settings. The current work draws directly from the supervised form in [1]. The only incremental contribution in this work is the specific mask types and mask-specific losses.\n\nPros\n\u2022\tInteresting approach to extend the framework in [1] to CNNs, with use of masks and mask-specific loss\n\u2022\tClear motivation for the network bandwidth limited use case\n\nCons\n\u2022\tHardly any technical novelty because the core ideas are already presented as well as applied to the same task in [1]\n\u2022\tIt is very surprising that the authors do not even cite [1] in their paper, despite their work being extremely closely related to it\n\u2022\tMost of the discussion in the Related Work section is unrelated to the specific task they tackled in the paper (i.e., input/feature selection). The second paragraph in this section talks about \u2018gradient-driven search\u2019 for discrete selection, which has been recently explored not only in [1] but also related G-S applications like [2], [3], but the authors seem unaware of this line of works\n\u2022\tThe authors do not compare their approach against any existing baselines from literature for this task, again with the most apt being [1] and baselines therein. This makes it hard to understand the true value of their proposals such as mask types, schedule that adjusts both \u2018tau\u2019 and \u2018lambda\u2019 during training etc.\n\n[1] Abubakar Abid et al., \u201cConcrete Autoencoders for Differentiable Feature Selection and Reconstruction\u201d, ICML 2019, (https://arxiv.org/abs/1901.09346)\n[2] Hanxiao Liu et al., \u201cDARTS: Differentiable Architecture Search\u201d, ICLR 2019\n[3] Bichen Wu et al., \u201cFBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search\u201d CVPR 2019\n"}