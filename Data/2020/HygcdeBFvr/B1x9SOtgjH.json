{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper tries to addresses an interesting problem of generating singing voice track under three different circumstances. Some of the problems that this paper deals with is a new problem and introduced first in this paper, which could be a contribution as well.\n\nAlthough the paper is fairly well-structured and written, especially in the early sections, I am giving a weak reject due to its weak evaluation. Evaluation is almost always difficult when it comes to generative art, but I am slightly more concerned than that. The literature review can be, although it is nicely done overall, improved, especially on the neural network based singing voice synthesis.\n\nI appreciate the authors tried to find a great problem and provided a good summary of the literature. Successfully training this kind of network itself is already tricky. It is also nice to see some interesting approaches towards objective evaluation. \n\nBelow are my comments. \n\n> Our conjecture is that, as the output of G(\u00b7) is a sequence of vectors of variable length (rather than a fixed-size image), compressing the output of G(\u00b7) may have lost information important to the task.\n\nI am rather not convinced. The difficulty to discriminate them doesn't seem to be (strongly) related to their variable length for me because a lot of papers have, at least indirectly, dealt with such a case successfully. \n\n> For data augmentation, we transpose the chord progressions found in Wikifonia to 24 possible keys\n\nWhat do you mean by 24 keys? I think there should be only 12 keys. \n\n> Vocalness measures whether a generated singing voice audio sounds like vocals. We use the singing voice detection tool proposed by Leglaive et al. (2015) and made available by Lee et al.(2018).\n\nActually, the paper (Lee et al. 2018) suggested that vocal activity detection in the spectrum domain is easily affected by some features such as frequency modulation. I am not sure if this feature is suitable as a measure of the proposed vocalness. The computed vocalness may provide more information if they are computed on other tracks (e.g., guitar, cello, drums, etc).\n\n> Average pitch: We use the state-of-the-art monophonic pitch tracker CREPE (Kim et al., 2018)8 to compute the pitch (in Hz) for each frame. The average pitch is computed by averaging the pitches of all the frames.\n\nI am pretty sure that this is not the right way to evaluate as a metric of generated vocal track. CREPE is a neural network based pitch tracker, which means it is probably biased by the training data, where the pitch values mostly range in that of common musical instruments/voices. This means, when the F0 of input is not really in the right range, CREPE might incorrectly predict somewhat random F0 within THAT range anyway. I'd say the distribution of pitch values can be interesting metric to show and discuss, but not as a metric of vocal track generation.\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}