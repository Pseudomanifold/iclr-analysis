{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach to generate explanations for recommendation\nsystems based on decision trees. The authors describe their approach and\nevaluate it empirically, comparing the achieved accuracy to other approaches.\n\nThe main drawback of the paper is that the approach is not evaluated in terms of\nexplainability of the generated trees. I realize that this is difficult, but\nsome proxy measure like the average height of a tree could have been computed to\ngive some evidence that the trees are human-understandable.\n\nThe authors do present samples of trees in Figure 4, but it is unclear how\nrepresentative those trees are, and some of the trees (e.g. a) purely bin\nratings, which presumably makes for explanations that are not very satisfying\nand counter-intuitive (e.g. \"the average rating is 10, therefore you will rate\nit 3.075\" but \"the average rating is 3.9, therefore you will rate it 3.836\").\n\nThe learned embedding function h seems counter-intuitive for providing\nexplanations as it is a complete black box that doesn't easily reveal the\nlearned relationships. The authors do not motivate why it is necessary.\n\nSection 3.3 describes how a partly-explainable tree is turned into a tree by\nreplacing nodes with decision stumps. It is unclear why this step is necessary\nand an explainable tree is not constructed in the first place.\n"}