{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Very well written paper.  A very easy read. Usually authors leave the references riddled with minor formatting errors, which you didn't.\n\nVery nice method.  Well constructed cost function (Sct 3.2), clear, simple approach (Alg. 1), and a technique for converting regression trees to decision trees (with tests on single variables) that surprised me with its simplicity and effectiveness.\nNote in a sense you are using amortised inference methods (the functions g() and h()), which makes the local trees work.  Suggest you make this connection in the Related Work.  Its definitely an important characteristic of recent algorithms.\n\nExperimental work is convincing enough.  Thanks for doing the ablation studies.  I think this should be an essential part of all machine learning algorithm papers.  The most important part of the paper is the discussion on white-box introspection.  This is what makes the algorithm worth something.  Without this discussion, you just have yet another reasonable RS method.  I wonder how this sort of evaluation can be made more \"scientific\", more \"objected\" and \"measured\".  Please think about this and read papers from your colleagues doing explainable systems to see what they have done.  I'd think a user study would help, though this adds a level of complexity machine learning folks have not been trained to do.\n\nNot much to say because, well, its surprisingly simple but effective, and well written.\n"}