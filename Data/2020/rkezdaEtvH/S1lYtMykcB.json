{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper argues that hyperbolic and other non-exponential discounting mechanisms have been more utilized by humans and animals for value preferences than exponential discounting as widely used in RL literature. The authors claim that hyperbolic discounting mechanisms are especially preferred in the setting of maintaining uncertainty over the prior belief of the hazard rate in the environment and propose an efficient approximation of the Q function with hyperbolic and other non-exponential discounting mechanisms as a weighted sum of Q-functions with the standard exponential discounting factor. The paper shows empirical evidence that hyperbolic discounting function can more accurately estimate the value in a vanilla Pathworld environment and also demonstrate that the approximated multi-horizon Q functions can improve performance on ALE, which is largely attributed to learning over multi-horizons as an auxiliary task.\n\nOverall, this paper is an extension of the prior work Kurth-Nelson & Redish (2009). It seems to me that the difference between this paper and Kurth-Nelson & Redish (2009) is that in this paper, the approximated Q-value with hyperbolic discounting function is a weighted sum over each Q-values using exponential discounting factor gamma, while in Kurth-Nelson & Redish (2009), the Q-value is estimated by sampling one Q-value based on the distribution of the gamma. Another difference as claimed in the paper is that the authors also present an approximation of other non-hyperbolic discounting functions. Those extensions seem a bit incremental and are not well supported by the experimental results in the paper. The authors didn\u2019t compare their method to Kurth-Nelson & Redish (2009) in both Pathworld and Atari 2600, which seems insufficient to demonstrate the point that the extension is useful. Moreover, the authors didn\u2019t conduct experiments on non-hyperbolic discounting functions, which makes the claim that approximating non-hyperbolic discounting functions is one of the paper\u2019s contributions unsubstantiated empirically.\n\nIn Section 6, the experiments in Atari 2600 show that Hyper-Rainbow and Multi-Rainbow have similar performance, and the authors claim that such results imply that learning over multi-horizons as an auxiliary task is more useful than choosing different discounting schemes. This seems to contradict the whole point of the paper that the hyperbolic discounting mechanism is biologically more reasonable than exponential discounting functions. The authors then claim that learning Q-values with multiple discounting factors is also one of the paper\u2019s contributions, which makes the paper look like a list of tricks to get RL to work in the multi-task setting rather than a unifying framework of the hyperbolic discounting scheme. I think the authors might need to pick a deep RL domain similar to Pathworld, i.e. an environment with risk increasing as the length of the trajectory increases, but in high-dimensional state space and continuous action space, to better demonstrate the effectiveness of the method."}