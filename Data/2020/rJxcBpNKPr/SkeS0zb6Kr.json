{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The central idea of this paper is to converts the classification problem into conditional generative modeling, it trains one normalizing flow (NICE) for each class of the data to perform out of distribution detection (one versus all binary classification). The data are presented one class at a time without being able to look back.\n\nThis paper proves a good improvement over the baseline methods compared to. Especially on CIFAR100, which is considered a difficult task in the regime of continual learning. It is quite evident to me that this approach has a big advantage over the other incremental classification methods.\n\nI give a weak reject to this paper mainly because,\n\n1. For invertible network, the NICE [1] coupling structure is used in this paper, although it is described in this paper as minimizing the distance from zero vector, the loss is basically equivalent to NICE with a Gaussian distribution as the target distribution. This is because normalizing flow maximizes the log p(z) + log(|det(z/x)|), in the volume preserving case the log determinant is a constant, it reduces to maximizing log p(z), which is equal to L2 loss if Gaussian distribution is chosen for p(z).\n\nIn my opinion, the paper could be better presented from the angle of generative modeling / out of distribution (OOD) detection. From this point of view, it would be desirable to include comparisons to the other OOD detection literature. It has been reported that the log probability is not a very good measure for OOD detection [2] because if an input has \"simpler\" patterns it would be classified as in distribution although it is actually OOD, and it has been reported that volume preserving flow (as used in this paper), also suffer from the same problem. There're several works proposing alternatives [3,4]. The good results of this paper may be because of the pretrained feature extractor, using a pretrained feature extractor could be practically useful but has limited insight for future research. Also, using a fixed feature extractor is avoiding the forgetting problem instead of solving it, it wouldn't generalize to more general / realistic continual learning setting.\n\n2. The incremental classification setting in this paper is not a very practical assumption for continual learning. i.e. the task boundary are assumed to be available during training, and within each task the data are i.i.d shuffled. One of the goal of continual learning is to prevent catastrophic interference, training separate network for each task, in my opinion, is avoiding the problem rather than solving it. Although there're many papers in the continual learning regime assumes availability of task boundary, I think this setting doesn't bring too much insight to how we can eventually solve the general case continual learning. This been said, I think this paper could be better presented from the OOD detection point of view.\n\n\n[1] NICE: Non-linear Independent Components Estimation https://arxiv.org/abs/1410.8516\n[2] Do Deep Generative Models Know What They Don't Know? https://arxiv.org/abs/1810.09136 \n[3] WAIC, but Why? Generative Ensembles for Robust Anomaly Detection https://arxiv.org/abs/1810.01392 \n[4] DETECTING OUT-OF-DISTRIBUTION INPUTS TO DEEP GENERATIVE MODELS USING TYPICALITY https://arxiv.org/abs/1906.02994 "}