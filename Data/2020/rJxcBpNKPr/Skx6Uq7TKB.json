{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles continual learning problem with stacks of invertible network blocks. Similar as the ensemble idea, the proposed method learns an invertible network for each new object class. During test, for each class, each learnt network outputs a norm and the one with the smallest norm is the predicted class. The proposed model has shown performance improvements in classification accuracy on MNIST and CIFAR100 datasets in incremental class tasks.\n\nDespite the linear increase of memory usage over number of tasks as authors pointed out in the conclusion, this is an elegant method for continual learning problems in incremental class tasks. The paper is very-well written and easy to follow. To my best knowledge, the formulations in the paper are correct and clear. It seems that there are sufficient details for reproduction. However, I have the following concerns which, I think, may lower the contribution of the paper, unless authors can help clarify.\n\n1. Since authors propose an invertible neural networks-based method for continual learning, instead of only focusing on incremental class, authors should also evaluate the proposed method in other continual learning tasks in object classification, for example, incremental domain and task, as defined in this review paper (https://arxiv.org/pdf/1810.12488.pdf).\n\n2. Though I would assume OVA-INN would outperform state-of-the-art regularization-based methods, it would be convincing to show quantitative results of these methods (e.g. inclusion of Elastic Weight Consolidation in Figure 2).\n\n3. I do not understand the term \"learning type\" in Column 4, Table 1. Please give the definition of this term.\n\n4. Figure 3 is an interesting visualization of the latent representation learnt by the model. What about prototypical networks? It would be great to provide side-by-side comparison with such visualizations for the clusters learnt by prototypical networks the few-shot continual learning settings.\n\n5. In Table 1, how much is the memory cost for nearest prototype method?\n\n6. How much training data has been used over multiple classes?\n\n7. Side note: it is unclear to me how this method can generalize to other tasks, e.g. regression problems and reinforcement learning problems. Authors can briefly discuss how the proposed method can be applied in these scenarios in the future work section."}