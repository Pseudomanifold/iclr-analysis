{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Paper summary:\nThis paper does continual learning using an invertible neural network (INN) trained to recognize each class. At test time a new example is presented to all of the INNs and the INN whose prediction has the smallest norm is used to predict the class of the test example. They do experiments on MNIST and CIFAR-100\n\nPaper contributions:\u00a0\n- Review of methods and evaluation settings for continual learning\n- Review of invertible neural networks\u00a0\n- Experiments comparing the proposed method to several other continual learning methods\u00a0\n- Examination of memory cost\n- Exploration of feature space of trained INNs\n\nReview summary & decision:\u00a0\nThe proposed idea of using invertible networks for continual learning sounds interesting, and I think that this could be a good paper. The experimental evaluation is not consistent or complete enough for me to to tell, however, and the core motivation for the idea is not clearly explained. There are also some less critical, but still important aspects of the paper (related work, clarity of explanations, repetitiveness) which lead me to decide this paper is not currently ready for publication.\n\nReasons for decision:\u00a0\n1. A lot of statements and decisions are made without being explained, or are unclear / innacurate\n\u00a0 \u00a0 \u00a0 - In CL, the objective is to learn several tasks one after the other\" this is not the objective, it's the problem setting. The objective is classification accuracy (or some other metric) in this setting.\n\u00a0 \u00a0 \u00a0 - The choice of datasets seems a little odd; why not CIFAR-10 as well?\n\u00a0 \u00a0 \u00a0 - It's very misleading to say your method is able to learn one class at a time without storing data; unless I've misunderstood this is only after pretraining all of the invertible networks with labelled data.\n\u00a0\u00a0 \u00a0 \u00a0 - Why 500 examples per class for CIFAR-100, and how many for MNIST? What criteria was used to arrive at this number?\n\u00a0 \u00a0 \u00a0 - in the abstract you state the definition of continual learning says you can't have access to the data from previous tasks, but then mention that previous works use samples from previous data. This seems contradictory\n\u00a0 \u00a0 \u00a0 - I'm not extremely familiar with Li & Hoeim (learning without forgetting), but I don't think it's very accurate to call it distillation regularization\n\u00a0 \u00a0 \u00a0 - In 4.1 I don't think it's accurate to call this compression; it's just having smaller size layers in between (unless I misunderstood).\n\u00a0 \u00a0 \u00a0 - I'm not familiar with model superposition; it should be more explained since your claims of having lower memory cost rely on this, as far as I can tell.\n\u00a0 \u00a0 \u00a0 - The experimental baselines seem inconsistent / not comparable, making it difficult to evaluate what is going on. E.g. some update pretrained features and some do not. Some (including yours) do offline pretraining on a subset of examples, with inconsistent amounts of data, and sometimes this is considered part of training (Fearnet) and sometimes not (yours).\u00a0\n \u00a0 \u00a0\u00a0\u00a0 - I don't understand the central motivation for using INNs. The only sentence that seems to talk about it is \"this way ... network won't be able to have an output similar to the outputs on data from its training set\", but this is not clear, not enough explanation, and seems somewhat innaccurate. It's not that it's not able, it's just not likely (I think; please correct me if I'm wrong!), and I don't see why this isn't also true of e.g. MLPs (i.e. for MNIST, train 10 MLPs each to classify [class i] vs [all classes except i]\u00a0- In order to justify the proposed approach, the baseline described above (using MLPs instead of invertible NNs) would be very useful\n\u00a02. The training regime for the INNs is not clear (are they shown negative (other class) examples? How many, how are they sampled? I didn't find any hyperparameters for the INN training in the appendix, even though it says they should be there.\u00a0 \u00a0\u00a0\n\u00a03. The single head setting seems very similar to (or maybe the same as?) open set learning; I think this should be mentioned and works in this area should be reviewed.\n\u00a04. Section 3 is repetitive and unclear. It could be greatly shortened to make space for more experiments.\n 5. It would be nice to see computational cost as well as memory cost; this is important in many settings where continual learning would be deployed.\n\nFeedback/suggestions/nits (not necessarily part of decision assessment):\u00a0\n1. Cite the definition of continual learning (e.g. with a reference to a textbook or review)\u00a0\n2. A lot of the writing is unclear, wordy, and/or grammatically incorrect\n\u00a0 \u00a0 - inconsistent verb tense\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0-\u00a0 e.g. \"if one would need .... it is required\" should be \"if one would need .... it would require\" I'd suggest rewording this sentence entirely, because it's misleading - it says \"retrain on this new dataset (which sounds like train just on the new data), but I guess you mean retrain on all data including the new data. e.g. \"Updating the model with new data requires retraining on the full dataset (old\u00a0+ new data). However, there are.... method may not be applicable.\"\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- \"we are reaching\" -> we reach\u00a0 \u00a0 - frequent use of \"indeed\" when it doesn't make sense\n\u00a0 \u00a0 - Section 3 repeats the intro, 3.1 and 3.2 are sort of saying the same thing, and are also sort of repetitive of the related work. The related work should all be in the related work section, and this section should just be about what _you_ are doing. Invertible neural networks should be reviewed in something like a 'background' section, which possibly could be in appendix. Right now that section has even more related work in it\n3. Include tsne hyperparameters in appendix\u00a0\n4. \"appendix\" not \"annexe\" (annexes are on buildings :) )\u00a0\n5. when you define continual learning, bold it (not italicize)\u00a0\n6. \"Y is then Y_i\" -> Y = Y_i\"\u00a0\n7. Caption for Figure 3 is unclear; I don't understand what I'm supposed to take away from these images. I also don't see a black cross.\n\nQuestions:\n1. What is the motivation for using INNs specifically? (rather than e.g. normal MLPs).\u00a0\n2. What is the training regime for the INNs?\n3. What is model superposition, is it expensive in computation or something other than memory, and if you don't do that what is the memory cost of your method?"}