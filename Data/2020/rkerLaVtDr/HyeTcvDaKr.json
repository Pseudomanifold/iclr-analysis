{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose an approach based on an upper bound on target domain error for the task\nof unsupervised domain adaptation (where one does not have access to\nany labels in the target domain). The upper bound makes possible the\npenalization of mixing samples from different classes together during\ndistribution matching.  Empirical results on image classification in a\nfew benchmarks show the significant promise of the approach.\n\n\nThe paper makes a good attempt at explaining the ideas/concepts, but\nstill remains very  unpolished (including English usage issues), and it is\nhard to follow in a number of places. A few such, a sampling, are mentioned below with\nsome suggestions. In my opinion, \nnot ready for publication.\n\n* abstract: '.. to address the problem for unsupervised domain\n  adaptation.' >> 'to address a major problem facing many unsupervised\n  domain adaptation techniques'.\n\n* page 2, several rewordings in: \"The reason is obvious, as marginal\n  distributions being matched for source and target, it is possible\n  that samples from different classes are aligned together, where the\n  joint error becomes non-negligible since no hypothesis can classify\n  source and target at the same time.\" For instance, a partial\n  rewording: '.. when an attempt is made to match marginal\n  distributions of source and target domains, samples from different\n  classes can be mixed together'. Also, I wouldn't use \"The reason is\n  obvious\"...\n\nFinally, here it is a good place to refer to Figure 1.\n\n\n\n* page 2: 'our proposal can degrade to some other methods' >> '.. can reduce\n  to several other methods ..'\n\n\n\n* in lines 1 and 2 on pg 3, the simplification from line 1 to 2,\n  explain the major reasoning (perhaps in the appendix if you don't\n  have space): there are number of terms added and subtracted (which\n  is understandable), but hard to keep track of what simplifications\n  are being carried and where the terms move (too many terms)...\n\n* 'the following theorem holds' >> 'the following bound holds' (or\n  inequality, etc.)\n\n* Is derivation 3 from triangle inequality?  (add that explanation to\n  line 3)\n\n* hard to parse (missing pronoun): 'the above upper bound in minimized when h=f_s thus\n  equivalent to ..'\n\n* 'is capable of' >> 'is capable to do so' (and at this point, is not\n  yet clear how the bound helps avoid the problems with distribution\n  matching.. )\n\n* Figure 1 (and subsequent figures): suggest put A, B and A' and B'\n  for the two classes and domains, inside the circles, so it's easier to see what\n  the source and target domain classes are (dotted boundary is hard to discern).\n\n\n* on top of pg 4: couldn't the feature extractor make the max-player\n  stronger too? ( In \"... since the max-player taking two parameters\n  f1 , f2 is too strong, we introduce a feature extractor g to make\n  the min-player stronger.. \" ..   )\n\n"}