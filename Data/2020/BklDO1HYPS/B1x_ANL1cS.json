{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a new stochastic algorithm: AVR-ExtraGD. AVR-ExtraGD combines the extended extragradient method proposed in [3] and the accelerated SVRG method in [1][2]. In their experiments, AVR-ExtraGD outperforms [2] in running time for sparse linear regression.\n\nThis paper presents their convergence analysis using results in [1][2]. They showed that the proposed algorithm can achieve O(sqrt{kappa n} log (1 / \\epsilon)) complexity for strongly convex problem and O(1/sqrt{epsilon}) for convex problem, which are the best results for both cases.\n\nThe idea of an accelerated version of variance reduced stochastic extragradient method is novel. However, there are some issues that the authors should address in order for the paper to match the quality of ICLR.\n\nEach step of extragradient approximates the proximal operator x_{k+1} = argmin_x P(x) + 1/(2 eta_k)\\|x \u2013 x_k\\|_2^2, therefore we would expect a faster and more stable convergence from this method. This paper claims that extragradient reduces the gap between the obtained optimal value and the real optimal value, which is confusing. The update of extragradient is actually biased towards x_k. The claim is then discussed in section 3.1 and 3.2 but is not clearly explained. Besides, how this claim is reflected in the convergence result is not discussed. I encourage the authors to clearly elaborate this claim and make relevant remarks after the main theorems.\n\nTo better understand the convergence result, it is important to know how extragradient affects the complexity and choice of hyperparameters such as K, eta_1, eta_2, and beta. Such discussion is not in this paper. I suggest the authors to make these aspects clear.\n\nThe experiments compare the proposed algorithm with other algorithms by their running time for lasso and elastic-net. The comparisons show the efficiency of the proposed algorithm. However, a more careful experimental design is required to better demonstrate the performance:\n1.\tFor the choice of inner iterations, choosing m=2n for Katyusha actually requires calculating 5n stochastic gradient because each iteration of Katyusha does gradient updates twice.\n2.\tThis paper only presents comparisons of running time. I encourage the authors to also plots comparisons on number of iterations, which will help revealing where the speed up of AVR-ExtraG comes from.\n3.\tIt is also preferable that the author compare with MiG [1], since the proposed algorithm is an extragradient version of [1].\n4.\tPlease at least solve two different optimization programs (e.g. logistic regression, neural network) so any conclusions are not specific to the oddities of a particular program.\n\nThe presentation and structure of this paper need to be improved. Here are some suggestions:\n1.\t In Section 1, only provide a high-level literature review and then motivate the work. A comprehensive review can come after the introduction.\n2.\tIn Section 4, put all the lemmas into the appendix while giving more intuitions and remarks. \n3.\tIssues including notions without pre-definition or reference, typos, and incorrect gramma need to be fixed.\n\n\nDetailed comments:\n1.\tFrom the title, the main application of this work is sparse learning problem. However, how the proposed algorithm benefits sparsity is not discussed. Besides, I suggest the authors to move the asynchronous algorithm in the appendix to the main paper.\n2.\tThe paragraph before section 1.1: lasso and elastic-net are used without citation.\n3.\tSection 1.1: PGD and SGD are used without citation.\n4.\tBeginning of page 2: \u201cAnd\u201d should be \u201cBesides\u201d\n5.\t\u201cBesides, for accelerating the algorithm and \u2026\u201d: \u201cfor accelerating\u201d should be \u201cto accelerate\u201d\n6.\tSection 1.2: \u201cNguyen et al. (2017) proposed the idea of extragradient which can be seen as a guide during the process, and introduced it into the optimization problems.\u201d What does \u201cthe process\u201d and \u201cit\u201d refers to is unclear.\n7.\tSection 1.2: the claim extragradient examines the geometry and curvature of the problem is confusing. The geometry of the problem is inspected through a line search step in [3]. However, line search is not discussed in this paper.\n8.\tSection 1.2: \u201creduce the gap between the optimal value we get and the real optimal value\u201d, these two kinds optimal values are important notions of this paper but they are not defined.\n9.\tIn Assumption 2, you can refer to Part 2, Section 7 of [5] for the definition of semi-continuity.\n10.\t\u201cdw is the gradient of the function at w\u201d, what does \u201cthe function\u201d refers to?\n11.\t\u201cAPG and Acc-Prox-SVRG\u201d needs citation.\n12.\t\u201cwas proposed to simply the structure of Katyusha\u201d, \u201csimply\u201d should be \u201csimplify\u201d\n13.\tSection 3.1: \u201cupdated with the update rules of MiG\u201d: \u201cwith\u201d should be \u201cby\u201d\n14.\tIn the equations of Section 3.2, the equivalent of gradient norm square and function f is incorrect, and the purpose of this equation is unclear.\n15.\tSection 4.1 Theorem 1: The inequality in theorem 1 is not intuitively related to the convergence rate. I suggest the author to simplify the inequality (For example, Theorem 2.1 in [2]).\n16.\tThe references are not in a uniform format. Conference/Journal names are missing for some references.\n17.\tOne useful reference for this paper is [4], it discussed extragradient for online convex learning.\n\nAdditional question:\n[5] update the extragradient step by sampling a new stochastic gradient while in this paper the same sample is used twice. How you compare these two approaches in terms of their performance and convergence?\n\n[1] A simple stochastic variance reduced algorithm with fast convergence rates, Zhou et al., 2018.\n[2] Katyusha: the first direct acceleration of stochastic gradient methods, Z. Allen-Zhu, 2017\n[3] Extragradient method in optimization: Convergence and complexity, T. Nguyen et al., 2017\n[4] Online Optimization with Gradual Variations, Chiang et al., 2012\n[5] Convex Analysis, R. Rockafella, 1970\n[6] Reducing Noise in GAN Training with Variance Reduced Extragradient, Chavdarova et al.  2019\n"}