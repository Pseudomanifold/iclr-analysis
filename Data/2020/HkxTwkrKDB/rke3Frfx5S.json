{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks. Authors should scope the paper to the specific function family these networks can approximate. No baseline comparison with GraphNets.\n\n\n\nThe paper proposes theoretical analysis on a set of networks that process features independently through MLPs + global aggregation operations. However, the function of interest is limited to a small family of affine equivariant transformations.\n\nA more general function is\n\n\\begin{equation}\nP(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} B_{(x_j, x_i)} x_j + c\n\\end{equation}\n\nwhere $N(x_i, X)$ is the set of index of neighbors within the set $X$. It is trivial to show that this function is permutation equivariant.\n\nThen, can the function family the authors used in the paper approximate this function? No.\nCan the proposed permutation equivariant function represent all function the authors used in the paper? Yes.\n\n1) If $B=0$, then the proposed function becomes MLP.\n2) If $A=0, N(x_i, X) = [n]$ and $B_{(x_j, x_i)} \\leftarrow B$, then this is $\\mathbf{1}\\mathbf{1}^TXB$, the global aggregation function.\n\nAlso, this is the actual function that a lot of people are interested in. Let me go over few more examples.\n\n3) If $N(x_i, X) = $adjacency on a graph and $B_{(x_j, x_i)} \\leftarrow B$, then this is a graph neural network \"convolution\" (it is not a convolution)\nExample adjacency $N(x_i, X) = \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}$.\n\\begin{equation}\n\\text{GraphOp}(X)_i = Ax_i + \\sum_{j \\in \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}} Bx_j + c\n\\end{equation}\n\n4) If $x_i = [r,g,b,u,v]$ where $[r,g,b]$ is the color, $[u,v]$ is the pixel coordinate and $N(x_i, X) =$ pixel neighbors within some kernel size, $B(x_j, x_i)$ to be the block diagonal matrix only for the first three dimensions and 0 for the rest, then this is the 2D convolution.\n\n\nAgain, the above function is a more general permutation equivariant function that can represent: a graph neural network layer, a convolution, MLP, global pooling and is one of the most widely used functions in the ML community, not MLP + global aggregation.\n\n\nRegarding the experiment metrics and plots:\n\nOn the Knapsack test, the metric of interest is not the accuracy of individual prediction. Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.\nFor example: success rate within the epsilon radius of the optimal solution while satisfying all the constraints. Fail otherwise. If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.\n\nAlso, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\\mathbf{1}\\mathbf{1}^TXB$ in PointNetST.\nPointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.\n\nAlso experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.\n\n\n\n\n\n\nMinor\n\nI am quite confused with the name PointNetST. Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer. The convention is B -> B', not A + B -> A'. In this case, A: PointNet, B: DeepSet\n\nLemma 3 is too trivial.\n\nThe paper is not very self contained. Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.\n\nP.2 power sum multi-symmetric polynomials. \"For a vector $x \\in R^K$ and a multi-index ...\" I think it was moved out of the next paragraph since  the same $x$ is defined again as $x \\in R^n$ again in the next sentence.\nAlso, try using the consistent dimension for x throughout the paper, it confuses the reader.\n\n"}