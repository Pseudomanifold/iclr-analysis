{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper provides a mechanism of building multi-task shared layer model, by computing Representation similarity between different layers which in turn computes the task affinity. Tasks that have a higher similarity have more shared layers, while tasks with lesser similarity are branched earlier.\n\nPros:\n1. The question in consideration is very important\n2. Yes, it is true that there are no enough studies that has studied this problem in a principled way.\n3. The results are shown in some very recent datasets including Taskonomy\n\nCons:\n1. There is not much novelty in this work. The important/ key aspect of this paper is the RSA (Representation Similarity Analysis) or the RDM (Representation Dissimilarity Matrices). This is already proposed. The rest of the paper is mere brute force. They compute the RSA between two tasks, at multiple predetermined layers of a model and whichever layer shows a lesser RSA, they start branching there. This is mostly brute force and part heuristics\n\n2. One of the key challenge in the literature was that NAS/. Zamir et al.(2018) was computationally expensive. Very surprising that the authors did not spend a good analysis over the computation time of the proposed approach. As far I can understand, the computation of the proposed approach is going to be really expensive.\na. Train a task individual DL model on each of the task\nb. For every pair of tasks, both the pretrained individual models, at multiple pre-determined layers, we have to compute the RSA\nc. Once the big RSA matrix is computed, then we need to compute the correlation. \nd. Thus, compute, at which point to share between these two tasks\ne. Repeat this for every pair of tasks.\n\nThis is really computationally expensive, and brute force - depending on the sampling rate of the layers\n\n3. I really dont like the comparison made in this paper with NAS - NAS really is a search optimization problem, finding new architectures. However, the technique proposed in this paper is more of a brute force comparison at pre-determined layers to compute which is the better of determined subset of n layers. This is not like a NAS problem. \n\n4. There is no generalization in this paper- we cannot say that for ResNet always share at this layer. For every pair of tasks, and for every given model, we need to train the individual model and find the correlation for every pair of layers and perform the whole computation again. There is no takeaway for me a researcher from this paper.\n"}