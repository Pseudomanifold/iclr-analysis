{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Review\nIn this paper, the authors proposed a new framework to jointly learn multiple tasks from heterogeneous sources. Most existing multitask learning frameworks require the input data to come from the same source. To overcome the limitation of those multi-task learning frameworks, the authors proposed to use tensor ring decomposition to decompose the weights of each layer into a sequence of 3rd-order latent cores. Then, some cores were selected to be shared for different tasks. The other cores were the task-specific cores that capture the task-specific features. The number of shared cores could be different for each layer. Experiments were done on multiple datasets.\n\nDetail comments:\n\n1. The writing of the paper is not good. The methodology needs to be more detailed. It is hard to follow. \n\n2. The novelty of the paper is not enough for an ICLR paper. It just applied an existing tensor decomposition method to multitask deep neural networks.\n\n2. In the first experiment, the authors divided the MNIST into two tasks. The first one was to classify the odd digits. The second one was to classify the even ones. I didn't see the reason for splitting MNIST into those two tasks. \n \n3. In figure 5, the authors only provided the performance of the difficult task and the average performance. Please add the performance of the easy task to show how much the performance drops for the easy task. Also, please report the performance on task C for monoglot dataset for 4.2.3.\n\n4. The setting can also be seen as transfer learning between different domains. In the experiments, the authors addressed the difficulty tasks' performance was enhanced. This is the contribution of the proposed method. However, we could see from the experiments that the performance of the easy task dropped. Why not compare with the transfer learning methods that do not affect the performance of the source domain? If the proposed method cannot get better performance compared with those transfer learning methods for difficult tasks, the existing transfer learning methods would be good enough to deal with the settings in this paper.\n\n5. Some baselines are missing. There is no single task performance shown in Figure 4. \n\n6. For 4.2.4, the authors just compared the results of their proposed framework under different settings. Please also provide the performance of other baselines to prove the proposed method is better than others."}