{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper applies tensor ring factorization to deep multi-task learning (DMTL) and shows that it leads to improved performance over existing DMTL factorization methods, more compressed representations, and the ability to share structure over more heterogeneous settings.\n\nHowever, the experiments do not clearly demonstrate the advantage of heterogeneity and consider mainly non-standard MTL settings. If these drawbacks were addressed, the paper could be quite strong. \n\nThe opening of the abstract discusses applications to diverse architectures and data modalities, but the experiments are limited to very similar image classification models. Could simply resizing the smaller image to the size of the larger images and using the same model work just as well as having heterogeneous models? Demonstrating sharing across more diverse settings, e.g., along the lines of the framing in [1], would make the contribution more significant. Can TRMTL be applied to share across any set of architectures?\n\nFor the experiments in the paper, it is also not clear why each architecture is suitable for each experiment. Some of them appear to be oversized models TRMTL can most effectively shrink.  E.g., the layer size of (23,328x256 FC) seems quite large for the Omniglot CNN, and the comparison methods in Table 1 look overparameterized. In each experiment, are the architectures standard architectures? If not, can we expect the results to translate to standard architectures?\n\nSimilarly, is there a reason MRN is missing from the Office-Home experiment? Also, Office-Home is usually validated on 5%, 10% and 20% training data. Is there a reason it is run on 50% to 80% here? Since Office-Home is the most standard benchmark used in the paper, it would be useful to have a table of detailed results (similar to the MRN paper), at least in the Appendix. \n\nOverall, the approach seems promising, but is missing some experiments and explanation that would validate its innovations.\n\n[1] Meyerson, E. & Miikkulainen R. \u201cModular Universal Reparameterization: Deep Multi-task Learning across Diverse Domains\u201d, NeurIPS 2019.\n"}