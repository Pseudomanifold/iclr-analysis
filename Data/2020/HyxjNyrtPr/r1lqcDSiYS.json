{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "# Review ICLR20, RGBD-GAN\n\nThis review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper.\n\n## Overall\n\nThe article proposes a method of modifying image-generating networks to also produce depth maps in an unsupervised way by enforcing rotational consistency.\n\nI enjoyed reading this work and I'm recommending it to be accepted. However, first there are some (in my opinion straight-forward) changes that need to be made to this work before I can recommend its publication: \n\n- The common \"Related Works\" section is missing and some of the literature is taking place in the introduction. I find this unorganized and I'd recommend keeping the intro shorter and just moving the literature either behind the intro or to the end of the paper.\n- Most figures and especially your headline figure (1) suffer from not having the depth normalized and not having a scale to it. The fix for this is simple and two-fold: for each depth image, subtract the minimum value and divide by the range (to normalize it and increase contrast), then write in the caption or as a legend that white is closer to the camera and black is further back.\n- 3D vs. 2.5D - If the common geometric definition of \"3D\" was applied here, the article's title was correct. However, in computer vision and especially 3D vision, the term is commonly used to refer only to models that include full scene geometry, including the occluded backs of objects and the term 2.5D is used to describe assigning depth values to pixels in an RGB image (and therefore only covering the view-dependent front of the object), which I think is the case here. However, this is not a hill that I'll die on so if you insist on that terminology, I won't block acceptance.\n- When you first discuss HoloGAN, you mention one of its main downsides being scalability and then proceed to not only explain that but also use a HoloGAN-like architecture in one of your experiments. I'd either remove the scalability argument or justify not just that but also how that's not relevant to your experiments.\n- The following phrase occurs multiple times throughout: \"camera parameter conditional image generation\". I _think_ you're missing a dash between \"parameter\" and \"conditional\".\n\n\n## Specific comments and questions\n\n### Abstract\n\nAll good.\n\n### Intro\n\n- Fig.1 normalize image \n- The literature section in intro mentions \"For all methods, 3D annotations must be used...\" - that's not true. See [Rezende, 2016][1] and [Rajeswar, 2018][2]\n- I understand how some literature is required to position your method, but I think it's better to not have the entire literature section in the center of the introduction\n\n[1]: https://arxiv.org/abs/1607.00662\n[2]: https://openreview.net/forum?id=BJeem3C9F7\n\n### Method\n\n- 2.1 clear + nicely written\n- Figure 2 good, caption a bit too short - figure+caption should be able to stand on their own\n- Illustration of Figure 3 nice, except for unclear DeepVoxel part: what's the wavy orange flag stand for?\n\n### Experiments\n\n- You mention K is fixed, but where does the initial K come from? I assume it's just neglected (since it's not important for StyleGAN/PGGAN), but then this needs to be mentioned in the methods sections closer to the formulas dealing with K.\n- Figure 4 - the depth maps need to be normalized. All we see here is a grey mush, even worse in Fig. 7\n- For ShapeNet cars, the model seems to suffer from not having a reference for the top and bottom of the image - have you tried adding floor/sky?\n- Figure 6, the tire marker is a good idea but image still unclear - I recommend slightly less rotation or an intermediate step between generated image and e.g. front view\n- For quantitative results/FID: try using Hausdorff or Chamfer distance on the rendered scenes' pixels. We don't care about the goodness of the RGB generation but the depth.\n\n### Conclusion\n\nAll good, albeit a bit short.\n\n### Appendix\n\nI don't think I saw any references to the appendix in the main paper."}