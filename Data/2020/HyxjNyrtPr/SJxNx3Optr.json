{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "SUMMARY: Unsupervised/Self-supervised generative model for image synthesis using 3D depth and RGB consistency across camera views\n\nCLAIMS:\n- New technique for RGBD synthesis using loss in 3D space\n- Can disentangle camera parameters from content (I disagree slightly with \"disentangle\" since you are conditioning on camera parameters in the first place)\n- Different generator architectures can be used\n\nMETHOD:\nGenerate RGBD images of 2 different views, have an adversarial loss on the RGB image, have a content loss between RGB1 and warp(RGB2), have a depth loss between D1 and warp(D2)\nEquation 5:\n- Possibly either \"c_{1->2}\" needs to be replaced by \"c_{2->1}\", or \"G_{RGB}(z, c_1) - warp(G_{RGB}(z, c_2), c_{1->2})\" needs to be replaced by \"warp(G_{RGB}(z, c_1), c_{1->2}) - G_{RGB}(z, c_2)\" (or am I missing something?)\n- Not entirely sure why there is a different \"projection\" operation, since both \"warp\" and \"projection\" are calculated from Equation 3. I understand that \"warp\" is the combined Rt matrix that is estimated using the two views and Equation 3, assuming that the \"d\"s are correct. Not sure what \"projection\" does though, possibly explain it better?\n\nDECISION: Very clearly written paper, simple idea executed well\n\nThe paper is clearly written and well organized. It uses a simple idea, and performs sufficient number of experiments to explore the idea. It is not very novel, but the paper shows its applicability with multiple architectures as a bonus.\n\nThe figures showed results almost only from their method. It would be great to pick one generator architecture, and elucidate more on the differences between not using their 3D loss and using it. Good attempt though.\n\nADDITIONAL FEEDBACK:\n- Might not be \"representation learning\", instead it is learning a generative model.\n- \"3 EXPERIMETNS\" -> \"3 EXPERIMENTS\"\n- The appendix should have more details on the equations and the specific formulations of warp  and projection operations"}