{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper tackles the problem of NER, POS and chunking in informal text data. The work attempts to leverage the much better performance that is achievable in formal text data by using an annealing technique to accomplish transfer to informal language tasks. The proposed method works by feeding in formal language data, and then slowly adjusting the proportion of informal language data that is fed to the BERT or LSTM network in order to allow it to initially learn parameters based on the source task and then update to the target task.\n\nThe authors show that the data annealing procedure results in slightly better performance than competing transfer learning methods. However, the increment seems very small. Looking at the results, it appears that the chosen domain may already be close to some theoretical maximum performance as the gains achieved are quite small.\n\nTo me, the method seems to be incremental. While there is a performance increase, the method does not seem to be particularly well motivated theoretically, and it is also not completely clear to me why DA would perform better than MULT. As such, I do not find the approach convincing. Is there some strong argument for why reducing the proportions is useful?\n\nMinor issue:\non pg. 7 - RiiterPOS should be RitterPOS\n\n"}