{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a data annealing strategy for transfer learning. This strategy controls the proportion of source data and target data, using more source data during the early training stage and exponentially decrease the amount of source data. Unlike the conventional transfer learning methods which use fixed amount data from source and target domain, the proposed data annealing strategy adjust the proportion of data to utilize knowledge from the source domain knowledge while focusing the target domain.\nThe reviewer has following arguments:\n1. The novelty of the paper is very limited. The presented data annealing procedure is similar to the fine-tuning strategy on motivation and behavior. The difference is that the fine-tuning uses all source samples to pre-train the model parameters and apply it for the initialization of target training, while the presented annealing strategy cuts down the proportion of source data progressively. Another refining strategy which iteratively moves more pseudo-labeled target instances into the training batches equivalently decreasing source data proportion. These strategies are commonly seen in transfer learning methods. Based on these considerations, the novelty of the proposed data annealing procedure is limited.\n2. The presented annealing strategy brings a very empirical hyper-parameter lambda. According to the derivation, the initial source proportion and the size of source dataset are all critically dependent on this parameter. While there is little description about choosing the hyper-parameter in Section 4.\n3. The experiment scenarios are limited in NLP, while the proposed data annealing procedure is claimed to have comprehensive ability in utilizing source domain knowledge.\n4. In the second term in equation (3), the subscript of the summation should be t=1.\n5. In the last paragraph of Section 3, it is the source data proportion that will decay to 0 in a short time rather than alpha, the fixed initial proportion."}