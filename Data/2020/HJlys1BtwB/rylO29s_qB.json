{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "- Summary \n\nThis paper proposes \u201cdata annealing\u201d, a simple and intuitive way to leverage data in the source and target domains, which gradually increases the proportion of training data from the target domain. Such an approach helps achieve state-of-the-art results on three informal language processing tasks (named entity recognition, part-of-speech tagging and text chunking) with BERT-large as the base model. \nHowever, the scientific contribution of this paper is relatively minor. The experiments are lack of details and therefore not clear enough to support the arguments: i. the author(s) did not mention how they chose or tuned hyperparameters, for both their method and baselines---the comparison is not guaranteed to be completely fair; ii. the author(s) made some vague arguments that are not supported or fully supported by experiments; iii. it is hard to follow the error analysis section without knowledge of the dataset. Besides, a lot of relevant references are missing. \nI generally suggest that this paper should be rejected, even if it achieves new state-of-the-art results. \n\t\n\n- Detailed comments and questions\n\nAlthough the general idea is simple and easy to follow, there are a few unclear points:\n1. In the main text, D_S \u201crepresent the size of source data\u201d. Does this mean \u201cthe number of instances in the source-domain training set\u201d or \u201cthe number of instances in the source-domain training set times the number of epochs\u201d? Note that it should be interpreted as the latter to ensure the correctness of Equation (3) when the model is trained for multiple epochs (at the transfer learning stage), though such an interpretation is definitely not natural. Also, how did you determine the value of D_S in Equation (5)? \n2. While alpha is chosen w.r.t. source data size (Eq. (4)), how was lambda chosen? Similarly, how did you choose about T_0 and lambda_0 for INIT and MULT? Did you tune these hyperparameters? \n3. Besides INIT and MULT, DA should outperform the following baseline (say, shuffle) before being claimed effective: (i) combine the source training set and the target one; (ii) at each step, randomly pick a batch from the combined training set. This is a strategy that looks like MULT, but the proportion of data from source/target domain could vary across batches. \n4. Sec. 3: \u201cLinear decay is popular\u2026 initialization from source data.\u201d\nThere is no experiment supporting the reason why linear annealing is not chosen . Have you tried linear data annealing scheme? \n5. Table 2 reports a single number for each model on each task---can the model be affected by any random factor (e.g., random initialization, random shuffle of training instances)? If so, how stable is it across different random seeds (in terms of standard deviation across several runs with different random seeds)? \n6. Sec. 4.2.2: \u201cin order to have a fair comparison with LSTM, it is necessary that we also use the CRF classifiers in BERT baselines.\u201d \nSuch an argument is not rigorous. It is hard to say BERT can have a really fair comparison with LSTM---there are a lot of other factors that affect the fairness (e.g., BERT has seen much more data than LSTMs at the pretraining stage).\n7. Sec. 5.2:  \u201cWe notice that the performance improvement is not as large as in the NER task \u2026 This suggests that the similarity between source data and target data is an important factor to consider in the transfer learning process. \u201c \nNER and POS tagging are completely different tasks---it is unfair to compare the absolute F1 scores across different tasks and draw any conclusions based on that comparison. \n8. Sec. 6: \u201cwe randomly sampled 30 sentences\u2026\u201d \nIt would be good to show the randomly sampled sentence in appendix. \n9. Sec. 7: \u201cwe show that when data annealing is applied with LSTM or BERT, it outperforms different state-of-the-art models on different informal language understanding tasks\u201d\nDA LSTM did not outperform state-of-the-art model on POS tagging. \n\n\n- Missing Reference \n\nThe paper proposes a transfer learning/domain adaptation technique, but few work on the topic was cited. To name a few in NLP, https://www.aclweb.org/anthology/W19-4302.pdf\nhttps://www.aclweb.org/anthology/D17-1038.pdf\nhttps://www.aclweb.org/anthology/P07-1033.pdf \nI strongly recommend the author(s) to also take a look at the papers cited by them and those cite them. \n\nIt would be good to state the relation and difference between the proposed approach and curriculum learning (Bengio et al., 2009). \n\n\n- Minor comments\nTitle: \u201clearning\u201d -> \u201cLearning\u201d. \nEquations: * -> $\\cdot$, to avoid confusion with convolution. \nJust introduce the abbreviation at the first mention. \nTable 2: \u201cBERT_{BASE}\u201d -> \u201cVanilla BERT_{BASE}\u201d. \n"}