{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the solution of neural network training in the NTK regime. The trained network can be written as the sum of two terms --- the first is the minimum RKHS norm interpolating solution, and the second term depends on the initialization. When the initialization scale is small, the second term almost vanishes, but when the initialization scale is large, it's likely that the second term becomes very large, leading to worse generalization.\n\nThe technical contribution of this paper is pretty low. The most important formula is (14), which only appears in the second half of the paper (the first half of the paper is almost all known results). The bounds in later part of the paper are also straightforward. Moreover, another paper https://arxiv.org/abs/1905.07777 already studied the same question and showed that non-zero output can increase the generalization error."}