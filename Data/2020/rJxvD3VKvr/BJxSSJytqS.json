{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies overparameterized fully-connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Based on this, the authors also derive a generalization bound of deep neural networks by transferring it to a kernel method. My major concern about this paper is the novelty and significance of its results:\n\nIn terms of connection to NTK, It seems that the connection between neural networks trained with squared loss and the result of NTK-based kernel regression has already been well-studied by \n\nArora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. \"On exact computation with an infinitely wide neural net.\" arXiv preprint arXiv:1904.11955 (2019).\n\nwhich is a missed citation. Without a clear explanation on the difference between the submission and this paper above, I don\u2019t think this paper is ready for publication.\n\nIn terms of generalization, it is also very difficult to judge whether this paper's result is novel. In fact this paper misses almost all citations on generalization bounds for neural networks. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small. To demonstrate the novelty and significance of the result, the authors should at least compare their generalization result with the following generalization bounds for over-parameterized neural networks in Section 4: \n\nAllen-Zhu, Zeyuan, Yuanzhi Li, and Yingyu Liang. \"Learning and generalization in overparameterized neural networks, going beyond two layers.\" arXiv preprint arXiv:1811.04918 (2018).\nCao, Yuan, and Quanquan Gu. \"A generalization theory of gradient descent for learning over-parameterized deep relu networks.\" arXiv preprint arXiv:1902.01384 (2019).\nArora, Sanjeev, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. \"Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.\" arXiv preprint arXiv:1901.08584 (2019).\nCao, Yuan, and Quanquan Gu. \"Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks.\" arXiv preprint arXiv:1905.13210 (2019).\n\nOverall, I suggest that the authors should make a clear discussion on the relation of this paper to many existing works mentioned above. As long as the authors can give a convincing demonstration of the novelty and significance of their results, I will be happy to increase my score.\n\nA minor comment: how can the bound in Theorem 3 be derived based on Theorem 2? Should there be a constant factor in the bound?\n\n"}