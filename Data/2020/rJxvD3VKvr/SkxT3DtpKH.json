{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n[Summary]\nThis paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(\\sigma^L + 1/\\sqrt{m}), where \\sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound.\n\n[Pros]\nThe general message that this paper conveys is interesting -- the initial network f_{\\theta_0}(x), which is typically omitted (or made small by making \\sigma small) in NTK analyses, can deviate the converged NN from the kernel interpolator in terms of generalization error.\n\n[Cons]\nThere are fundamental mistakes in the statements/proofs of Theorem 2, 3, 4:\n-- Theorem 2: the statement is \u201cwhp over W, the bound \u2026 holds uniformly for x\u201d. The proof relies on Lemma 3, whose statement is also uniform over x, but the proof applies the Markov inequality *for a single x* and is thus valid only for a single x. (As it\u2019s Markov, it seems not sensible to apply the union bound upon it.)\n\n-- Theorem 3: the difference between L^NN_test and L^int_test should be on the order of (\\sigma^L + 1/\\sqrt{m}) rather than it squared. To bound the difference in squared loss we have a^2 - b^2 <= O(1) * |a-b| (if a, b are bounded by O(1)). We don\u2019t have a^2 - b^2 <= |a - b|^2.\n\n-- Theorem 4: J(X_test) as defined is a vector whose dimension grows with the number of test data points, where the theorem requires it to be a scalar. Indeed the treatment of test data as a fixed matrix (rather than samples from a distribution) is already a bit atypical.\n"}