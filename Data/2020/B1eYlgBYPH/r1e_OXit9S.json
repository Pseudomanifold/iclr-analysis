{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Authors proposed a deep RNN via unfolding reweighted l1-l1 minimization, where reweighted l1-l1 minimization algorithms are applied to a video task. \nOverall, the paper is well explained in a theoretical part and exhibits a good result compared with other conventional RNN methods in the experiment. In Section 3, authors formulate Rademacher complexities for both conventional and proposed method, which shows the generalization performance of the proposed method when d increases. And this is empirically highlighted in Table 3 in Section 4.\n\nMajor points:\nSection 1:\n-\tFirst part of the introduction can be confusing because Eq. (1) sounds like representing dictionary learning framework (plus DNN is immediately described after Eq. (1) instead of RNN) and RNN is not explicitly written. It should be clearly written and flow should be considered.\nSection 2:\n-\tIt is hard to get how parameter g in Eq. (3) derives. \nSection 3:\n-\tHow to build network depth d for the network? A figure should be required.\nSection 4:\n-\tEven though previous papers (e.g., Wisdom et al. and Le et al.) just focus on single dataset like moving MNIST, I believe testing on language data is also quite important (this is a full paper and exhaustive experiments should be mandatory). For example, it may be good to use Penn TreeBank dataset to make a comparison. \n-\tIn Table 3, how did you set LSTM deeper? Is it a stacked LSTM?\n-\tExisting RNN methods should include other variations of LSTM (in particular, SOTA methods are welcomed) such as bidirectional LSTM and LSTM with attention mechanism. It should be better to compare with these methods.\n\nAppendix:\n-\tIt would be helpful for readers to show interpretabilities of the model additionally. For example, visualizing features from each RNN model would be beneficial.\n\nMinor points:\n-\tAfter introduction of unfolding reweighted l1-l1 minimization, how did the computational cost increase compared to previous l1-l1 minimization?\n-\tIn Section3, for easiness to readers, it may be good to briefly summarize how does the predictor\u2019s generalizability and Rademacher complexities relate."}