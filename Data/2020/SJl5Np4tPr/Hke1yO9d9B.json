{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors argue that existing metric learning approaches, for few-shot learning, may cause overfitting to the feature distributions encoded only from the seen domains and thus fail to generalize to unseen domains. This is a domain generaliation task under a few-shot learning setting. The authors proposed a so-called feature-wise transformation layer and integrate it into some existing metric-based few-shot learning methods, which helps learn more diverse feature distributions for better generalization ability.\n\nI think this is a pioneer work on few-shot learning for domain generalization, which is quite interesting.\n\nIn terms of experiments, good performance has been shown by using three existing metric-based few-shot learning methods on a number of settings using five image datasets.\n\nBelow list my major concerns:\n1. The authors mentioned that it is intuitive to have feature-wise transformation layers produce more diverse feature distributions, which would lead to better generaliztion ability for domain generalization. However, it is not obvious to me. Why the generalization ability would be improved with more diverse features?\n\n2. In the experiments, feature-wise transformation layers with hyper-parameters empirically (\\theta_\\gamma, \\theta_\\beta) set as (0.3, 0.5) are compared against learning-to-learn version of the proposed method. Why (0.3, 0.5) was chosen? Is there any chance that another set of (\\theta_\\gamma, \\theta_\\beta) may lead to better performance than the learning-to-learn version? It would be much clearer if the authors can provide detailed parameter analysis (e.g. grid search).\n\n3. Algorithm 1 should be quite expensive in time. It is not clear how the algorithm will be terminated. And in general, how many iterations in the algorithm will be until it achieves some convergence?"}