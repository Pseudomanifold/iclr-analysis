{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a feature-wise transformation layer for the cross-domain few-shot classification task. Besides, they apply the learning-to-learn procedure to tune the hyperparameters automatically. The primary motivation behind this method does make a lot of sense for me. The reduction of significant shifts in the feature norm could be crucial for successfully transferring the source domain model to the target domain. Also, such a method is method-agnostic, which makes the proposed layer more accessible for all kinds of metric-based query&support pipeline. The extensive experiments for three baselines, MatchingNet, RelationNet, and GNN, indicate the generalizable effectiveness of the proposed layer. Specifically, the visualization and analysis of learned feature space and layer parameters look interesting to me. Remember that the proposed method is designed to minimize the large discrepancy of the feature distribution. Such a study could be beneficial for the reader to understand this paper in depth.\n\nMeanwhile, I have some questions and suggestions for the authors:\n1. I am suggesting the authors include more recent state-of-the-art as baselines and comparisons, which could make such submission much stronger.\n2. This paper delivers an extensive study of the classification problem, how about the other tasks, which heavily rely on classification head, like detection or segmentation. I think it could be more attractive if the author could also show some improvement for these tasks in either a qualitative or quantitive way.\n3. This paper mainly focuses on metric-based few-shot frameworks. At the same time, there are also other two groups, recurrent-based and optimization-based schemes. It could be much better if the authors could also mention the potential of the proposed layer.\n4. This paper leverages the learning-to-learn mechanism to tune the hyper-parameters. How about considering the adaptive or dynamic approach. In other words, build the connection between the theta parameter and image feature."}