{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper described a multi-input model for text classification. This paper is a bit hard to read compared to other submissions. I highly recommend authors to have native speaker to proofread this before submission. The paper mentioned \"labels\" as input to embedding layer as well as attention mechanism. However, it will be more helpful if authors can provide examples of labels. Moreover, the experiment results (e.g., Table 3 and 5) did not include label. I wonder if the methods used as baselines are still the state-of-the-art as language model pre-training might have better results. It will be more convincing if authors include additional baselines such as fine-tuning BERT.\n\nMinor issue: the references in Table 3 (in brackets) are hard to find. \n\nBased on above reason, I would reject this paper."}