{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work proposes using a randomly parameterized convolutional layer as additional processing of the input observation to provide data augmentation to make policies more robust to environments with different observation spaces. The empirical results are thorough, comparing with other regularization techniques, including dropout, L2 regularization, and batch normalization with the same policy gradient method, PPO on a variety of generalization in RL benchmarks. There are additional experiments of this method to check that it actually removes visual bias in a computer vision problem better than other methods. \n\nThey also incorporate a feature matching loss that explicitly forces the learned representations of equivalent states to be close in L2 distance. While the empirical results are impressive, it is hard to feel excited about this work, which relies on the inductive bias of a randomly parameterized convolution layer to modify the texture of the observation and show it works in certain settings. I'd like more discussion and showcasing of failure modes, it seems that this wouldn't work for settings where the train and test environments are different in ways beyond texture and changes in small objects, and additional analysis in terms of the dogs and cats database about why it performs so much better than other methods. What exactly is the desired and meaningful information in images that a random convolution layer can keep while removing something that is able to generalize to different shades of cats and dogs? Why would this perform better than grayscaling?  What about grayscaling and additive Gaussian noise?\n\nThe comparison of PPO is also unfair in that the author's method uses an ensemble of policies to act, which other methods do not. A more fair comparison would use ensembles in all other baselines as well or results showing how their method performs without this ensemble.\n\nOverall, the presentation, analysis, and writing can all be improved to match the strong empirical results produced. "}