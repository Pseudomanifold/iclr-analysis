{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a method inspired by the categorical cross entropy (CCE). In a similar way as many metric learning approaches, a softmax approach based on cross entropy is used to enforce similar examples to have smaller distances than dissimilar distances. \nNonetheless, instead of considering a centroid-based deep metric learning approach (e.g. like Snell et al. (2017)), only one anchor is considered for each training example, and the distance of an anchor with one of its positive is learned that its distance is smaller than the distance with examples belonging to different categories.\n\nI vote for reject for the following reasons:\n- The paper is too similar to NCA and S-NCA (introduced in Section 2.2). In the same way as ICE, S-NCA also considers learning l2-regularized representations. The main difference with S-NCA is the way negative examples are sampled. \nS-NCA proposes a framework based on augmented memory, ICE proposes a sampling strategy similar to [A] and used in Nickel et al. (2018) to subsample negative pairs.\n- Another contribution of ICE is the use of some hyperparameter s in Equations (11) and (12). This hyperparameter s plays the role as the inverse of the temperature and is in fact learned in ICE. S-NCA also plays with the temperature but does not learn it. \nOn the other hand, learning the temperature in a similar way has been proposed in the deep metric learning literature (e.g. TADAM).\n\nOn the other hand, the paper has some nice contributions:\n- The main \"novelty\" of ICE seems to be the analysis in Section 3.4 of the partial derivatives and their impact on the sample reweighting. Although the explanation is simple, it helps understand what's happening during optimization.\n- The experimental results on different transfer learning benchmarks seem convincing.\n\n\n[A] Jean et al. On usingvery large target vocabulary for neural machine translation. ACL 2015"}