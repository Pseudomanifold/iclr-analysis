{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper claims two contribution towards ML. Recurrent networks that can grow its memory by creating new modules.  The second one is a language character and word-based task language modelling task. The main contribution is in my opinion the first contribution. If this would work very well this would be quite useful. The paper seems to provide a step in this direction. \n\nWhile the paper claim quite general usefulness of the task the test application is more narrow / only one has been tested. I would have loved to see more applications. Why not apply it to Machine Translation in the same fashion as for language modelling.   \n\nThe paper could have applied the method as well to another architecture for instance transformers to prove their claim that it would fit to other frameworks too. For language modelling this has become a standard now. Thinking on this the method might not be easily transferable as mentioned.\n\nThe paper could also state how much modules cells the final network uses. Does it use up all 30 at the end?\n\nThe proposed Growing Long-Term Memory Network are well investigated in a multilingual setting which is nicely able to showcase benefits of such a model to be potentially be more memory efficient and faster for the language needed. However, I guess it was finally not smaller as the largest LSTM model compared with and longer to train. The authors should made this clear.  The authors should have assessed more metrics like training times, steps, units.  Overall, I found the paper interesting while a bit weak on the empirical side, I would have preferred deeper analysis. It remained a bit difficult to judge efficiency of the final model and training time. \n\nThe captions of the tables could explain the tables better (some of the abbreviations are not explained - informed helps but make it more explicit.)   \n\n"}