{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper tackles the problem of continual (online) learning. Their approach is based on training subcomponents that can be specialized for each different task while the prediction of each subcomponent is getting mixed with a linear weight vector, which allows fast adaptation when the task is changed. They tested the suggested method on two new online learning datasets based on language modeling. The results show that the proposed method is effective in online learning, especially when tasks share transferrable knowledge due to the modular nature of the proposed method.\n\nThe approach is straightforward and general, so it has much potential to be applied to general continual learning problems. Also, the presented online learning scenarios and datasets are beneficial to the community in that the scenario is more realistic. \n\nHowever, I am not fully convinced of the contribution of this paper. My concerns are the following:\n\n- The experiment results show that the suggested method does not suffer from catastrophic forgetting by recalling the appropriate module through the weight vector. However, these results are not surprising since the whole parameters of a module are kept as LTM. While working well on the domain tested, the approach has a clear limitation in scaling due to the computation and hardware limit of a system. Ad-hoc method to address the problem is introduced and works empirically, but it is not justified clearly. More fundamental questions should be answered, such as, when LTM should be created and removed, how to reuse, consolidate, or merge prebuilt LTM modules.\n\n- The experiment details are missing. How exactly is LSTM trained?; when do you initialize the starting state of LSTM? (Or, when do you truncate the gradient signal in performing BPTT?) How do you define \u201cbatch\u201d? Since the proposed method is targeted to online-learning, a precise description of the experiment is very important, especially when others want to reproduce or compare with the results.\n\n- There is previous work that tries to solve a similar online setup, which does not contain an explicit task boundary [1,2,3]. As it is noted in the paper, some of the settings are somewhat contrived and artificial. However, to clearly show the advantage of the proposed method, it would be necessary to have an experiment on a more widely accepted training scenario. The dataset proposed in [4] could be used to show the efficacy and flexibility of the proposed method.\n\nThe following are general questions:\n\n- It seems like the training procedure for the mixture weights will result in selecting memory modules that have a relatively low loss, especially when the weight vector is trained with more iteration. Then, don\u2019t this become a winner-takes-all type of rules?\n\n- Why fixing up the parameters of some module is beneficial, i.e. 20/10 STM/LTM split is better than 30 STMs? Do you have an intuitive explanation about this?\n\n- Doesn\u2019t the post-switch confusion penalize the network having a low average loss? I understand the intention of the measure, but it might be harsh for a good network. \n\nHere are minor modifications to improve the paper.\n\n- page 2, section 3, \u201cOr\u201d -> \u201cOur\u201d\n- page 3, \u201cMoE\u201d is used, and I am assuming this is Mixture-of-Experts, but the acronym is never introduced.\n- page 3, \u201cSTEM\u201d -> \u201cSTM\u201d\n- page 5, \u201corpus\u201d -> \u201ccorpus\u201d\n- I am unsure that \u201cmemory\u201d is the right term to use; it is understandable conceptually, but \u201ca parameterized module\u201d or something similar to that is more precisely describing the proposed architecture.\n- page 5, \u201c10 for each language\u201d. Is this correct? There are 5 languages and if the training procedure alternates over each task 10 times in a random order, then there will be only 50 sequences, not 100. Is this typo, or am I misunderstanding something?\n\n-----\n\nReferences\n\n[1] van de Ven GM, Tolias AS. \"Three scenarios for continual learning.\" arXiv preprint. 2019:arXiv:1904.07734\n[2] Sprechmann et al., \u201cMemory-based Parameter Adaptation\u201d, ICLR 2018\n[3] Rolnick et al., \u201cExperience replay for continual learning\u201d arXiv preprint. 2018:arXiv:1811.11682\n[4] Lomonaco et al., \u201cCORe50: a New Dataset and Benchmark for Continuous Object Recognition.\u201d CoRL 2017"}