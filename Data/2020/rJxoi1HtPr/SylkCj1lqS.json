{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThe paper introduces Growing Long-Term Memory Networks  (GLTMNs) which has both long-term memory (LTM) and short-term memory (STM). STMs are the main modules used for computations and  LTMs are used as long-term storage which can be swapped with STMs at any point in time. The authors also propose 2 lifelong language learning tasks and show that GLTMNs perform better than vanilla LSTMs in these tasks.\n\nMy comments:\n\nThe paper proposes an interesting way of using long-term memory modules. This helps in growing the capacity of the network as and when needed. I have no issues with the experiments.\n\n1. It is not very clear if only  STMs are used while making predictions or LTMs are also used.  If LTMs are also used, then is it correct they are used with frozen weights (i.e. no backprop through LTMs? This is very crucial information. Please clarify.\n2. The model description is not very clear. It would be beneficial if the authors can describe the model in a precise way and also include pseudocode for the learning algorithm. I cannot reimplement this paper by just reading the current model description.\n3. I do not buy the argument that having a task id is not realistic. In fact, it is more realistic. Humans do not do an unknown task. They are told what task they are doing.\n4. Growing the model dynamically to avoid catastrophic forgetting is also explored in Sodhani et al. 2018 (Towards Training Recurrent Neural Networks for Lifelong Learning) who use Net2Net to do zero-shot expansion of the model parameters.\n5. What does p/lang mean in Figure 2? It was never introduced.\n6. I assume that the authors will release the code and data upon acceptance of the paper.\n7. I love the first and last line of the paper!\n\n\nMinor comments:\n\n1. Section 2.3: Fix the brackets for Joulin & Mikolov 2015 citation.\n2. Page 2: second last line: \u201cOr\u201d should be \u201cOur\u201d.\n3. Page 3: STEM should be STM.\n4. Fix the grammar of the last line before section 4.2\n"}