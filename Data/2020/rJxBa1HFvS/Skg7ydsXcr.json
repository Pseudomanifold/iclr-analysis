{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Value-Driven Hindsight Modelling proposes a method to improve value function learning. The paper introduces the hindsight value function which estimates the expected return at a state conditioned on the future trajectory of the agent. How use this hindsight value function is not obvious, since an agent does not have access to the future states needed in order to take actions (for Q-Learning) and the hindsight value function is a biased gradient estimator for training policy gradient methods. \n\nThe authors train the standard value function (which does not have access to future information) to predict the features which the highsight value function learns to summarize the value relevant parts of the future trajectory. These predicted features can then be used in place of the actual hindsight value function, circumventing the issues discussed above. The authors argue that this auxiliary objective provides a richer training signal to the normal value function, helping it to better learn what information in a given state is relevant to predicting future rewards.\n\nThe paper is well structured and written, flowing from high level motivation and review into the core of the method, followed by analysis of the approach, and then proceeds through three experiments. The first two are toy / crafted experiments which build intuition and probe the behavior of the method and finally a large scale test on the Atari 57 benchmark demonstrating improvements when augmenting a state-of-the-art method with HiMo.\n\nThis reviewer recommends acceptance (I would give a 7 given more granularity) based on the contribution of a new auxiliary objective for value functions and the strength of the experimental suite. The Portal Choice environment is well crafted and instrumented with the graphs of figure 5b and 5c to show the behavior of the approach and the clean demonstration of an improvement over a previously SOTA method for Atari 57 is encouraging (the same architecture and the ablation simply sets the auxiliary objective\u2019s weight to 0). However, the reviewer has some caution and concerns as follows:\n\n1) The lack of a large scale experiment demonstrating improvement with an actor-critic method. While the Portal Choice experiments are informative and use Impala, it is a bit toy, and it would increase the reviewer\u2019s confidence in the generality and robustness of the approach if improvements were also demonstrated for an actor-critic method on a large environment suite. Atair 57 could work but ideally a different setting such as DMLab 30 or continuous control from pixels. Demonstrating improvements in one of these additional settings would raise the reviewer to a strong acceptance.\n\n2) The potential sensitivity of the approach to the two important hyperparameters that the authors mention, the dimensionality of the hindsight feature space (to reduce approximation error) and the # of future states it conditions on (to avoid just observing the full return directly). The very low dimensionality of the hindsight feature space (d=3 for Atari) seems a bit at odds with the explanation that the hindsight features provide a strong training signal for learning to better extract value relevant information from the state. Experiments that studied sensitivity to these would provide better perspective on the robustness of HiMo.\n\nQuestions and suggestions for improving the paper:\n\nFor Figure 6 the dynamic range gets squashed by a few games with relatively large performance improvements or regressions. Changing to a log-scale on the y-axis could be more informative? For instance, I find it pretty difficult to eyeball the ~1 human normalized score median improvement according to Table 1 from the chart.\n\nFigure 3 could also be improved. It requires significant context from definitions in the paper in order to understand. It could be reworked into a stand alone expository overview of HiMo that helps readers quickly grok the idea of the paper such that abstract + figure is enough.\n\nCould the authors consider showing / adding full learning curves (median human normalized score?) for HiMO vs the baseline on Atari 57? This would help readers get a qualitative feel for the learning dynamics of the algorithm instead of only having a final scalar measure at the end of training."}