{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n*Summary \n\nThis paper describes an effective method that induces constituency trees from pre-trained language models, which are attracting great attention recently. The authors demonstrated that the pre-trained language models have some properties that are similar to constituency grammar by showing some interesting features of the extracted trees. \nThis study is based on the motivation to unveil the reason why such pre-trained language models work and the extent to which pre-trained language models capture the syntactic notion of the constituency.\nThey also show that the method can become a reasonable baseline method for English grammar induction.\n\n*Decision and supporting arguments\n\nI'm leaning toward accepting the paper as a conference paper.\nAs the author describes, pre-training LMs are attracting attention, and many people want to understand their inner workings. The paper provides a systematic analysis of the pre-trained LMs from the viewpoint of grammar induction. Also, this paper is well-structured and offers a good literature review.  I think the paper has enough value to be published from the scientific viewpoint. \n\n\n*Additional feedback\n\nFigure 1 & 5-10 look rasterized. It's better to use vector images.\n"}