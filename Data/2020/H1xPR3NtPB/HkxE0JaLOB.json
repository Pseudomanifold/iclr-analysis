{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the representations learned by large pre-trained models trained on language modeling objective (or language modeling-like objective, in the case of masked models). In particular, the authors investigate whether constituency information is present in the hidden layers. In contrast to much existing work that probe for this information with (for example) linear models on top of hidden representations, the authors propose to directly extract binary trees from the model using \"syntactic-distance\" measure that is calculated in various ways (e.g. dot product of hidden representations, distance in attention distributions). Across various models, the authors find that it is indeed possible to induce linguistically meaningful trees, in particular outperforming a right-branching baseline that is strong for English\n\nI found this to be a creative alternative to the existing \"BERTology\" type papers that rely mostly on linear probes, and the experiments are done across a wide number of setups (e.g. across models/datasets).  However, I have several questions/issues with the paper if addressed, would make the paper much stronger:\n\n- The degrees of freedom afforded by the choice in number of layers, heads, similarity measure, etc. is quite high. For example, with 24 layers and 16 attention heads I count (24*(16+1)*2 + 24*3) = 888 different measures of distance that could be used to induce trees. I think the authors should make sure that they are not \"overfitting\" to the test set with the following set of tests:\n\n1. Only use the training set to select the layer/metric combination.\n2. See if the selected combination generalizes to other languages (from comparing Table 1 against Table 2, it seems like this doesn't even generalized within the same language across different domains? This is worrying).\n3. See what the performance is with random initialization. In particular, I would like to see two setups: (a) the network is randomly initialized, (b) the *representations* (i.e. attention/hidden states) are randomly initialized. The random initialization should have variance such that it's not just uniform distributions.\n\n- I found the right branching bias baseline experiment not very informative. As the authors allude to, performance on unsupervised parsing itself is not so interesting, and the point of the paper (in my view) is not to get the best unsupervised parsing performance.\n\n- Related to the above, it is known that the decoding algorithm of Shen et al. 2018 is itself heavily biased towards right branching trees. See in particular: https://arxiv.org/pdf/1909.09428.pdf. Given this, I am not sure if the supposedly good performance in parsing is due to the model actually learning constituency or just the bias.\n\n- It would have been interesting to see how the performance changes as:\n1. The model is fine-tuned on the PTB-training set.\n2. The model is trained from scratch on PTB (obviously with a much smaller model since PTB is smaller).\n\n\n"}