{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new algorithm to learn latent disentangled representation in the framework on variational autoencoder (VAE).  Keeping the backbone architecture, an unsupervised approach and a supervised approach were proposed.  For the former, a PCA based decoder was introduced to the latent layer which concurrently models geometric transform and principal components.  The supervised approach, on the other hand, resorts to an adversarial excitation and inhibition mechanism.  The experimental results demonstrate better disentangled representation learned for classification, meta-learning, and synthesis/sampling.\n\nThe paper is clearly motivated and easy to follow.  Disentangled representation learning is an important task which has been addressed by a large of papers recently.  The approaches and models leverage existing methods such as RASL (Peng et al., 2012), the excitation-inhibition mechanism (Yizhar et al., 2011), domain-adversarial neural networks (Ganin et al., 2016), etc.  The introduction tried hard to delineate their difference to the proposed approach.  I think the novelty in methodology meets the minimum requirement for ICLR, but as a result, the bar for the experiment part needs to be high.\n\nIn experiment, a variety of datasets are used for supervised, unsupervised, and few-shot learning, using reconstruction error, classification error, and visualization.  Ablation study was also conducted on the impact of geometric transformation and adversarial excitation and inhibition.  My major concern is that the comparisons have been made only against weak baselines for disentangled representation learning, such as VAE and \\beta-VAE. The paper did include CC\\beta-VAE and JointVAE, but only for qualitative evaluation in unsupervised learning.  Given that there are so many variations of disentangled learning algorithms for supervised and unsupervised learning in the recent two years, why not compare with them?  Many of them are also based on VAE, and use mutual information.\n\nTo summarize, the paper presents an incremental modification to VAE to learn disentangled representation, and experiments lack comparison with strong baselines.\n"}