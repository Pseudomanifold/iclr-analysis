{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary: The authors propose a new strategy for fine-tuning natural language generation model called mix-review. The main idea is to mix the target dataset with a sample of the pretraining dataset to fine-tune parameters. This strategy can alleviate the forgetting and overfitting problems existing in traditional fine-tuning strategy. The authors conducted both automatic evaluation and human evaluation to prove the effectiveness of their idea. Meanwhile, the authors also did several experiments to reveal the effect of pretraining on natural language generation.\n2. Overall assessment: The problem studies in this paper is an important issue that has not been studied well. It shed some light on how to alleviate the forgetting problem in the pretraining and fine-tuning framework and analyzes how pretraining affects language generation models. I think it delivers some valuable ideas to the NLP community. Meanwhile, there still exist some problems for this paper to improve on. All in all, I'm leaning towards acceptance.\n3. Comments:\n3.1 In Figure 1(a), it looks the line of mixreview_ccnews_valid does not change at all. Does this indicate the fine-tuning on a dialogue dataset, which should be different from the CCNEWS data, does not change the model's ability in generating CCNEWS at all? Can the authors give some more explanations here?\n3.2 Raters are asked to give overall ratings on generated text in Table 2. While this human evaluation is great plus, readers may also like to see some finer-grained human anlaysis, such as human evaluations from different angles, such as cohere, fluency, engageness and so on. Such detailed evaluations can also better reveal why the mix-review strategy is working better.\n3.3 In Table 4 and Table 6, the NS and MASS pretraining tasks are utilized separately? Why not combine these tasks together? Will there be more gain of doing so?\n3.4 Table 3 is a great analysis. I'm wondering if it is possible to evaluate the context-sensitivity quantitatively?\n3.5 What do you mean by \"trigger\" in Sec. 5.3? Using the triggers as the input context to the generation model? What's the relationship between the reference description and the triggers? It seems the reference descriptions are some factual statements that are crawled independently with the triggers. "}