{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis work proposed an off-policy framework for policy gradient approach called\nguided adaptive credit assignment (GACA) in a simplified setting of goal-oriented\nentropy regularized RL.\nGACA optimizes the policy via fitting a learnable prior distribution that using\nboth high reward trajectory and zero reward trajectory to improve the sample efficiency.  \nThe experiments on sparse reward tasks such as WikiTableQuestions and WikiSQL\ndemonstrate the effectiveness of the GACA, comparing with a set of advanced baselines.\n\n\nDetailed comments:\n\nOff-policy learning:\nThe Environment dynamic is not considered. The trajectory\nreward is determined by the initial state, goal, and the sequence of actions taken thereafter. The off-policy learning can be applied since the distribution of\ninitial state, and goal is not affected by the policy. This reduces to a\nweighted maximum likelihood problem. \n\nResolving the sparse reward issue:\nIn sparse reward tasks, many of trajectories have zero rewards, in order to utilize\nthe zero reward trajectory (since in the weighted problem those samples have no\ncontribution to the gradient). This work proposed to store the trajectories\ninto two replay buffers and samples from both of them separately. \nIntuitively, it is not clear to me why minimizing mutual information between z and\nreward would help the learning. I am suspecting the reason is that mutual information brings non-zero gradient for zero reward trajectories (given zero-reward trajectories indeed helps the learning). \nThe authors also claimed that KL divergence performs worse than f-divergence due to the mode seeking issue. Do the experiments in GACA w/o AG support this claim?\n\nAblation study:\nThe authors claimed that using zero reward trajectory can help with sample efficiency.\nI wonder what the performance would be if we drop the zero reward trajectory buffer if we have a reasonable high frequency to reach the high trajectory reward sequence. \nIs it necessary to incorporate the zero reward trajectory? \n\nWhat is the exact formula of GACA w/o GP and GACA w/o AG? \n\nThe proposed method consists of three parts (GP, AG, and separate buffer. ) \nTwo variants (w/o GP, w/o AG) of GACA is conducted in the ablation study. \nHow does the GACA perform if we drop the separate buffer? What if we incorporate separate buffer for baselines. Does GP/AG play an essential role in performance improvement,\ncomparing to a separate buffer? \n\n\nOther questions:\nSince the sequence of actions is considered as a group, the performance\nmay highly depend on the size of action space and horizon. \nWhat is the size of the horizon of the tested problems? \nWhat is the value of WB and WC in each experiment?\n\n\nMinor:\nThere are many typos or grammar issues in this version. e.g.,\nL 3, Page 4, learn-able prior\nLast paragraph, page 3, \" as as a combination of expectations\", \nPage, 15 \"is actually equals mutual\"\nEq 23 -> 24"}