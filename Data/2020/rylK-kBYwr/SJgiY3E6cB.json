{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "[Contribution summary]\nAuthors propose a new E2E model for the DST task that (1) implements a response generator that leverages DB query results as well as other dialog contexts with a late fusion approach for joining domain and slot pair representations, and (2) obtains the competitive results (50.91% for the joint task of DST & context-to-text generation, and 49.55% joint accuracy for the DST task, both on MultiWoZ 2.1).\n\n[Comments]\nThe proposed model is reasonably structured. Empirical results show improvement over other baselines, with the main gain (among ablations) coming from incorporating the dialog states from previous turns, and the joint domain-slot module, etc. The empirical analysis section could improve and the novelty of the system over the previous literature be clarified -- some of the components used in this work are not entirely novel and previously utilized in other work, hence it is hard to discern where the main gain is really coming from, compared to the previous work (especially for the DST task). Empirical comparison of the joint DST & generation, which is one of the main results, is limited to Lei et al.\n\nThere have been recent work on DST with new SOTA results (e.g. \u201cTowards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset\u201d by Rastogi et al.) -- please consider comparing the approaches.\n"}