{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper makes an empirical claim that CNNs for object recognition do not contain hidden neuron which is highly selective to each class, mainly based on three aspects: (a) metrics related to the maximum informedness, (b) jitterplots of activation data, and (c) a user study assessing whether generated images maximizing a given unit is perceptible to the user. The paper point out these results are in contrast to the case of RNN, where it has been reported that many localist hidden units emerge. It is also noticed that the existing metrics for selectivity do not adequately discriminate highly selective units in CNN.  \n\nIn overall, the manuscript is well-written and easy-to-follow. I particularly appreciated a kindly presented overview on the literature and thoroughly conducted experiments including a complete user study. One of my key concerns, however, is that I am still not fully convinced whether the key finding in this paper - the lack of highly selective units in CNNs - is an indeed important problem for ICLR community: Personally, I feel the \"existence\" of selective units in RNN could be interesting, but the \"non-existence\" in the case of CNN is not that surprising for some readers, as it seems much likely (at least to me): The final layer of CNN would be surely selective across classes, but it may be not the case for the hidden layers - Nevertheless, some of the units may act selectively, not across classes but in some other concepts: e.g. stripes, orientations, etc. Therefore, I wonder if the paper could further provide a discussion on the importance of the key finding. \n\n- In Section 2 - Network and Dataset - \"... We selected 233 ...\" : Which criteria is actually used to choose the candidate units for the analysis?\n- Do the overall results mean that the \"maximum informedness\"-based metrics are superior to the others for assessing selectivity of a unit? Also, are those metrics original to this paper?\n- Currently, I feel the concept of \"selectivity\" is presented in somewhat subjectively: the definition could vary across the context. It would be nice if this could be more formalized to support the claims in the paper, e.g. the superiority of the proposed metrics."}