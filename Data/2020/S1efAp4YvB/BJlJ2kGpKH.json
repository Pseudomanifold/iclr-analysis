{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work attempts to experimentally investigate the difference between Conv3Ds and ConvLSTMs with the aid of visualization. The visualization focuses on two aspects: (1) Temporal masking for identifying key frames and key segments and (2) GradCAM for spatial saliency. Both visualization techniques are illustrated on two public available video classification datasets.\n\nThis work has some major issues:\n1. The quantitative results are not very relevant. It does not validate any of the following cases: (1) if Conv3Ds are more powerful, one should use the same number of parameters and compare classification results, or (2), in order to achieve the same level of accuracy, one of the two models is more parameter efficient. It is not clear which argument Table 1 is validating against if there is any as both parameters and accuracies vary. \n2. Qualitative results from Section 5.2 is not substantiated and underwhelming. For instance, it is hard to see why \"Conv3Ds has a bias around the center while ConvLSTMs find relevant spatial features in multiple smaller areas\". This is most likely due to visualization techniques other than the choice of models. \n3. The fundamental issue of this work is that it does not establish a hypothesis from the beginning and design experiments around it. Plenty of hand-wavy observations are made without further investigating the root of them, leaving readers unsatisfied, and quite often confused."}