{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper addresses the problem of learning segmentation models in the presence of weak and strong labels of different types. This is an important problem that arises while learning in data-scarce settings. The presented approach optimizes jointly over the various types of labels by treating each one as a task. They extend the U-net architecture to incorporate the different tasks.\n\nPrior work:\nThere has been other work on incorporating multi-resolution or different types of labels. Here is one that can be cited:\nLabel super-resolution networks (https://openreview.net/forum?id=rkxwShA9Ym)\n\nMajor comments:\n- The motivation for the specific structure of the multi-task blocks is not clear\n- The object boundaries labels can be noisy (i.e s(2) can have noise). How does model deal with this?\n- Is it the case that every image in I_3 is completely labeled - i.e all segments/classes marked?\n- The assumption that s(3) is independent of s(1) and s(2) is not true. Instead of constraining the model to learn masks that respect the various types of labels, it seems they learn from each source independently. It is not clear how the sharing of parameters in the multitask block helps.\n- Can they comment on the applicability of the prior work suggested above?\n\nMinor comments:\n- How do the rough labeling tools work on biomedical data where the objects are more heterogenous patterns where different labels can have very different distribution of pixels. How well will their method generalize in such settings?\n- Can this work be used for segmentation and prediction on crop data?\n\nResults:\n- It seems as if the improvement over the PL baseline (pseudo labels) is incremental? Can the authors provide error bars so the reader knows what the significance of the results is?\n- Can they give a more thorough comparison in terms of human effort? It is interesting to note that only 2 images give 0.82. Would 3 images give 0.94? They need to show the trade-off between additional effort vs gains in performance.\n- What is the performance of MT U-net without the SL images (i.e without task-3)? Table-2 does give some intuition, but authors should add another row with multitask WL\n- Table-3: How well does MDUnet do with 9.4% SL data?\n\n\n"}