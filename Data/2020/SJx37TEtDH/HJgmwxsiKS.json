{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper gives theoretical and empirical results for a gradient clipping variant of Adam they call ACClip.  While the theoretical analysis is rather  sophisticated and nontrivial, I personally do not believe that analyses of this form are of any value in guiding practice.  But that is a long discussion that is not specific to this paper.  The bottom line is that for me it is mainly the experimental results that matter.\n\nThe experimental results are not compelling.  The experiments focus on BERT. There is no mention of RoBERTa and her descendents.  RoBERTa still uses Adam.  It is now clear that careful hyperparameter search is critical to drawing experimental conclusions about optimizers.  This paper simply states the hyperparameters used with no discussion of hyperparameter search.  Very careful and convincing experiments would presumably cause the GLUE leaders to convert from Adam to ACClip.  In any case, this paper should be examined by members of the teams now leading the GLUE leader board simply because they have enormous experience in training BERT."}