{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper considers random projection forests for similarity measurements (which have been proposed earlier) and proposes to accelerate them by reusing projections. Tree levels up-to level-X use distinct random transformations, and subsequent levels cycle through existing projections (X of them). As this kind of reuse reduces the quality of trees, the paper proposes to (greatly) increase the number of trees in the forest. The paper also introduces a sensible \"beta-similarity\" which is based on average tree-distance between leafs into which the two data-points fall, rather than fraction of trees in which they fall into the same leaf-node. \n\nI recommend to reject the paper -- as the contribution in my opinion is incremental, not principled (more of an engineering trick) and not very convincing. Furthermore, the paper has a number of issues with presentation, language, and experimental results. RP trees have been introduced over a decade ago (Dasgupta and Freund, 2008), and the authors cite a reference to RP forests from (Yan et al, 2019, IEEE Big data). Level-wise use of the same projection within a tree (that the authors call \"layer-by-layer RP Trees\") has been discussed (and shown to be effective) in the original Dasgupta and Freund paper.  The paper makes it very hard to understand what is a contribution, and what is borrowed from existing papers -- as the authors say \"we introduce\" both for well-known concepts (like RP trees), and for what I understand to be their contribution. I would ask to clearly state what is existing work, and what is new, and what are the key contributions.\n\nOther comments. \n1. Experiments:  you cite a paper on RP-forests (Yan et al, 2019) -- so ensembles of RP trees have already been proposed.  Why in the experiments you still compare only to a single RP-tree?  You report speed gains ~ 2x or 3x  w.r.t standard RP Trees -- even when you use very many (hundreds or thousands of trees in an X-forest).  Are these layer-by-layer RP trees, or do you use new random projection for each node? Are these your implementations of RP trees, or did you use existing code? The experiments do not provide enough details on the implementation to judge their significance -- e.g. 2x gain of speed could be achieved by better software implementation of the same algorithm.  Is it standard to measure clustering quality by  taking a classification problem and assuming that clusters should correspond to class labels? There are various other measures of cluster quality that do not rely on labels (e.g. rand-index, homogeneity, ratio of within-cluster to inter-cluster distances e.t.c.).\n\n2. In figure 8, (a),(b),(c) -- it looks like choice of beta can have a dramatic impact on accuracy, and can either be monotone, or peak at intermediate values.  How do you propose to chose beta in practice -- don't you need supervision?  What is the cost to compute beta-similarity compared to naive 0-1 similarity?\n\n3. The claim that similarity measures should be independent from prior knowledge -- can be controversial -- and is not a widely accepted truth. The field of metric learning tries to find better similarity matrices based on additional prior information. It's hard to imagine that a single similarity matrix will be suitable for all problem domains. I agree that it's useful to have a generic default similarity measure to try first, but clearly if prior data is available it should be used. \n\n4. In addition to \"mathematical distance-based similarity\" and tree-based similarities -- there is also considerable work on similarity based on non-linear embeddings -- e.g. T-SNE or UMAP, provide an embedding, and then a simple cosine similarity can be used after the embedding. Overall -- this is a complex nonlinear similarity measure, not captured by your two classes. \n\n5. A well known paper \"Extremely randomized trees\" also proposes kernels (similarity measurements) based on a forest created with (nearly) random splits, and should be cited, and used in comparisons. There is even a scikit-learn implementation -- called random trees embedding.\n\n6. The introduction of the paper:  \"Similarity measurement is to measure similarity...\" conveys no information. The entire first paragraph does not have much content, and should be rewritten or skipped.  Bad word choice:  Unsupervised clustering is \"to classify\", \"Exalted speed\", dimension of original dataset is \"degraded\"...   If you say \"we introduce RP Trees\" -- it sounds like you propose them in this paper -- which is clearly not the case, as you cite Dasgupta and Freund's paper.   You can instead say -- we consider RP trees introduced by (ABC)... \n"}