{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper sets to solve the problem that many language models such as GPT-2 despite having achieved great success in their language tasks are too big to deploy in applications. They propose LightPAFF, a framework that allows to transfer knowledge from a big teacher model to a lightweight student model, thus solving the deployability issue at hand. They conduct experiments showing LightPAFF achieves a 5x reduction in the number of parameters and a 5x-7x improvement on the inference speed, while roughly preserving performance. \n\nThe distillation framework the authors use in their methods is not new. It has been proposed in previous work, as the authors noted. As opposed to previous works, where distillation is performed in the fine tuning stage, the authors of LightPAFF propose a two stage distillation procedure instead that performs distillation at the pre training phase and fine tunes this distilled model for use in a downstream task using a big fine tuned teacher model and the dataset of the task. The experimental results show these results to be meaningful. They achieve better accuracy and similar compression than previous approaches. It's main weakness is that the method doesn't seem to be particularly new. "}