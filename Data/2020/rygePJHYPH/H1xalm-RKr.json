{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a new loss function for training deep neural networks, which show good performance with respect to well-calibrated, trustworthy probabilities for samples after a domain shift. The authors conduct experiments with multiple datasets and multiple forms of perturbations, where the proposed method achieve superior performance. \n\nI have the following major concerns with the paper:\n\n1. Presentation: The paper's presentation is very weak. It contains repetitive, long, convoluted statements and paragraphs all throughout making it difficult for the reader to understand anything. The first time I read this paper, I couldn't process what is happening. The introduction is more like realted work with less focus on what they are trying to pitch in the paper. However, the latter is the less severe concern.\n\n2. While I appreciate the use of reliability diagrams and the loss function inspired from it, I do not completely understand what are the entities in equations 1 and 2 or how they are used later. Please give them a name, and elaborate on them.\n\n3. While I was reading the paragraph before section 2.2 the first time, it seems to me that ECE is defined for those 10 pertubations specifically. I believe this is not the case; hence, the authors should make it more general and divert the specific details to experiments. \n\n4. I am not sure what the authors mean by, \"... while the loss surface remains largely unchanged\", in paragraph above section 2.2.2. I believe the authors have constructed a new loss function by adding a new term to it. That makes it difficult to understand the advantage the authors are talking about after this statement. Overall, I am not really convinced how in contrast to Bayesian Deep Learning, their approach can be EASILY applied to LSTM's and GRU's.\n\n5. What is L_{adv}? There is no equation number, no discussion around where this is defined. Again a presentation issue.\n\nMinor: Please clarify the reference for supplementary materials. For example, it is not clear that Table S1 is in supplementary material.\n\nOverall, this paper is good and has an interesting idea. The experiments are also extensive useful. However, I have reservations regarding the presentation of this paper at this moment. "}