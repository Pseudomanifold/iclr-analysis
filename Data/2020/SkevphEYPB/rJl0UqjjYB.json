{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Paper summary: This paper proposes an alternative approach to batch normalization called POP-Norm. The authors motivate their approach theoretically and suggest that this also helps explain BatchNorm\u2019s success due to the similarity between the two approaches. They then empirically examine their method on CIFAR10, CIFAR100 and ImageNet classification.\n\nComments: I find the paper hard to read and and plagued with major technical flaws. The paper also shows unfamiliarity with standard literature on optimization. Some of my major concerns are discussed below:\n\nA. Theoretical Results (Section 2):\n\nThese results (the connections between convergence and gradient Lipshictzness (beta-smoothness) and gradient variance) are extremely well-known (an example reference is Section 3 in arXiv:1909.03550). In fact, the effects of gradient variance on convergence in optimization have prompted a whole sub-field on variance reduced gradient descent methods such as SVRG. Moreover, prior work by Santurkar et al. identifies precisely the same effect (better beta-smoothness) with BatchNorm as one of the factors responsible for its success. \n\nB. Proposed approach:\n\nThe authors claim that one of the key differences between their approach and BatchNorm is dividing by the l2 norm instead of the gradient variance. However, it seems that they take the l2 norm of the random variable after mean subtraction not before (Equations 10 and 12). This is actually the same as the variance of the variable before mean subtraction, which makes this part of their approach identical to BatchNorm. Then the only difference in this aspect of their approach is the way the learned scale and shift is introduced, which seems somewhat arbitrary. \n\nEven more concerning are the claims in Theorems 2 and 3 which justify why mean subtraction and dividing by the l2 norm (variance) help reduce gradient Lipschitz constant/variance. The authors are comparing two upper (or two lower) bounds which is insufficient to draw any meaningful conclusions. In particular, given two random variables X1 and X2, one cannot claim that X1 < X2 based on (arbitrary) upper bounds; especially since there is no evidence that these bounds are tight. \n\nMoreover, all the theoretical analysis in this Section is performed for a single layer of the network assuming other layers are fixed. Specifically, as per their proofs, the gradient of the weights at a particular layer is not dependent by any of the previous/layer layers, thus completely ignoring the effects that come from simultaneously updating all the layers. Thus, the theoretical analysis in this Section does not really tell us about the effects of BatchNorm in a deep network. Their setting is actually identical to studying the effects of input normalization on a single network layer. In this setting, it is well-known that input normalization improves the conditioning of the problem and thus helps optimization ([LeCun et al., 1998 and Wiesler et al., 2011])---in fact, this was actually one of the motivations of the original BatchNorm paper. \n\nC. Experimental Results:\n\nBased on this, it seems to me that the proposed approach is essentially the same as BatchNormalization except for (1) the positioning of the layer (before the weights instead of after), (2) the exact way the learned scale and shift is done and (3) a hyperparameter kappa that is used to scale down the variance of all the layers and hence essentially scales down the learning rate by a constant. \n\nFrom an experimental point of view, I find (3) particularly concerning because it just means that in their learning rate for the POP-Norm experiments are 1/kappa times those for the BatchNorm baseline. Since the authors do not really tune over hyperparameters (they use a fixed learning rate in all their experiments) when comparing the different approaches, it is not clear whether the chosen learning rate is optimal for BatchNorm. To perform a fair comparison of their approach to BatchNorm, the authors should:\n1. Introduce the kappa scaling into BatchNorm layers and see the effect on performance.\n2. Perform a grid search for both methods and report the best performance over hyperparameters.\n3. Also include comparison to other SOTA normalization alternatives (including those that the authors mention in the introduction)\n\nD. Other comments:\ni. The discussion in Section 4 is hard to parse and I am not convinced of its correctness (I am not sure how the authors arrived at (16)).\nii. The paper writing is poor with several typos and spelling mistakes. In addition, the inline math makes the paper very hard to read at times.\n\nOverall, I think there are significant concerns with the paper, both in terms of writing and the soundness/novelty of the technical results and experiments. Thus, I recommend rejection.\n\n------------------------------------------\nReferences:\n\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Orr, G. and K., Muller (eds.), Neural Networks: Tricks of the trade. Springer, 1998.\n\nWiesler, Simon and Ney, Hermann. A convergence analysis of log-linear training. In Shawe-Taylor, J., Zemel, R.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 24, pp. 657\u2013665, Granada, Spain, December 2011.\n"}