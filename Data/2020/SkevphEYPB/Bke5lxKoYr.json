{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a new normalization method for accelerating the training of deep neural networks. Specifically, the proposed method put the normalization step before the linear transformation step. Then, the authors show that the normalization step can reduce the variance of gradients. The experimental results show some improvement. \n\n1. The proof of Theorem 1 is the regular convergence analysis of SGD. No new contributions.\n\n2. If my understanding is right, Theorem 2 and 3 actually can also show that other normalization techniques have these effects. So, what's the difference between POP-Norm and other normalization techniques in reducing gradient variance and  Lipschitz  constant?\n\n3. This paper claims that the effect of batch normalization will be destroyed by Tanh and Sigmoid activation function. How about POP-Norm? It's better to show the performance of POP-Norm for the network with Tanh or Sigmoid.\n\n4. How about its performance for large networks, such as ResNet-50?"}