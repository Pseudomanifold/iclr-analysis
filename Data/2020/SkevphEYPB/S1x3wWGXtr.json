{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes Pre-OPeration Normalization (POP-norm) as a theoretically grounded modification of batch normalization (Batch-norm). The authors provide an analysis of stochastic gradient descent (SGD) showing that the convergence can be accelerated by lowering the gradient Lipschitz constant and gradient variances. Based on this observation, the authors show that by applying POP-norm which is quite similar to the Batch-norm applied before the activation functions, the gradient Lipschitz constants and the gradient variances can be lowered. \n\nI must first confess that I\u2019m not an expert in the relevant area. I\u2019m don\u2019t know why the reviewing system assigned this paper to me, but I will try my best to judge the value of the paper. \n\n Having said that, I don\u2019t have enough background knowledge to evaluate the novelty of the paper. For instance, the Theorem 1 is stated as if it was originally introduced in this paper, without any citation. But I doubt that because I could easily find several results relating the convergence rate of SGD to the gradient Lipschitz constant. Are there any papers or standard textbooks to be noted? If so, please cite them properly.\n\nThe theorem 2 and 3 are based on the full-dataset statistics, but because they are infeasible in practice, the authors propose to use the batch statistics. How may this affect the theory? Intuitively, one could easily imagine that the smaller batch-size would lead to higher variances for batch statistics estimators, and thus impeding the regularization effect of the normalization. Also, the theory just states that the upper bound of (Lipschitz constant, gradient variance) after normalization will be lower than the lower bound without normalization, so it is not clear how much the normalization would reduce them. If possible please state the detailed results in the main text. \n\nThe differences between POP-norm and Batch-norm are, 1) whether the normalization is applied before or after the operations, and 2) minor difference in the normalization procedure, such as the additional scaling parameter kappa. The experiments provide results to demonstrate the effectiveness of 1), but not clear about 2). It would be interesting to compare POP-norm to the Batch-norm applied before the operations to see the net effect of 2). The very idea of applying Batch-norm before the convolution operation has already been demonstrated and widely used in the case of ResNet with ReLU activation functions, so if the contribution from 2) is insignificant, I think one may ask the novelty of the proposed normalization scheme.\n\nOverall, despite the lack of background knowledge, I find this paper to be borderline, but more close to the reject, because of some unclear points. I would like to hear back from the authors and other reviewers.\n\nI don\u2019t understand why the matrix B in eq.8 gathering the biases has additional indices 1_k, \u2026, m_k. Shouldn\u2019t it be the repetition of b_k for m times?"}