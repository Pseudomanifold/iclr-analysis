{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper studies a theoretical aspect of the expressivity of policy, Q functions and dynamics. Based on theoretical and empirical analysis, the authors propose a new model-based RL algorithm that said to improve task performance. Final evaluations are demonstrated on MuJoCo benchmark tasks. \n\nOverall, the paper pursues an interesting and ambitious problem on the interplay between model-based and model-free approaches, and the expressivity of the representation of dynamics, policy and value functions. However, the results in the theory part is drawn based on analysis on a very simple and special task. Therefore the theoretical results can not be considered general for all MDP cases. In addition, these results are not surprising.\n\n- Theorem 4.3 states a general theoretical result that holds for neural networks, the proof does not look like it can hold with a universal representation power of a neural network. \n\n- The idea of using Q-functions estimate as Boostrapping is just an idea of using on-planning to improve action selection at every decision step. This is just a recurring idea of many model-based RL approaches. BOOTS consumes more computations as planning, hence would perform better than the baselines. BOOTS should be compared to other model-based approaches that also use planning at Testing, assume all are given the same budget of testing time."}