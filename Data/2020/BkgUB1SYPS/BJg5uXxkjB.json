{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper derives lower bounds on the separation rank of a wider class of recurrent NLP models in terms of its depth and number of hidden layers, demonstrating that both the number of hidden units as well as the number of layers improves the ability of NLP networks to model context dependency. It then introduces a novel bidirectional NLP variant that is supposed to capture a good trade-off between computational cost and performance.\n\nThe manuscript is very dense and does not follow a straight and easy-to-follow story line. In particular, the introduction of the bidirectional variant seems to substantially distract from the main story line of the paper (there is also no connection between the theoretical results to the bidirectional network). The improvements of the bidirectional models also seem to be minor, but no standard deviations for the performance results are reported.\n\nA clear description as to which language models are captured by the TSLM model is missing. Also, it is unclear how tight the bounds actually are given that no value for m (the word length) is given. Finally, the title does not reflect the content of the paper (there is nothing interpretable about the network structure)."}