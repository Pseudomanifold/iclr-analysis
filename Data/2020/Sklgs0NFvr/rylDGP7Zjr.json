{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Summary:\n       The authors take two tasks,sentiment analysis and natural language inference, and identify datasets for them which they counterfactually augment it by asking people over the Amazon Mechanical Turk Platform to change either the sentiment (in the case of sentiment analysis) or the nature of relationship in the NLI task by making minimal changes to the text that produce the targeted changes. \n\nAuthors find that popular models trained on either fail on the other dataset while the models trained on both actually generalize much better. This is because the original sample and its counterfactual pair the label changed , has the difference in the text that matters to the change and this pair could reduce spurious correlations that models might find in the data distribution. \n\nPros: \n This is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community. The one part (I dont have much NLP background but I do have a causality background) that I like most is that the new text generated are counterfactual in some real sense with respect to a real world generating process - that is people modifying text with changed targets.\n\n A lot of existing work that claim to do counterfactual changes do not specify assumptions about the generating mechanism. For counterfactuals to be valid they have to be intervention on the actual generating mechanism (or an assumed one) acting on a given unit (latent) that produced the current sample. The paper in that respect (even if it does not explicitly specify relationship between counterfactuals and generating mechanisms) tries to be faithful to a \"strict causal notion\" by actually asking people to modify the text. \n\nCons:\n    - I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the \"people\" in amazon turk were substituting. \n\n   -  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ? \n  -  In figure 6, it appears that BERT is sensitive to the domain - does it mean that it is bad ? - Authors indicate that ideally it must not be so. Because Table 3 results seem to indicate that BERT performs the best in almost all the cases .\n -  Can the authors highlight the best performances in each case in the Tables by a bold face.  It helps easily eye ball the best performing model.\n ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}