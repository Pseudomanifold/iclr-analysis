{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper combines generative modelling with few-shot learning, by distilling the information of a teacher classifier (which uses all of the data available) into a student generative model to produce samples with a similar decision boundary when only given a few examples as context. This generator can then be used to produce additional samples for an unseen class at test time. To improve training of the generator and ensure the teacher and student are not too far apart, the teacher is progressively strengthened, or the student progressively weakened.\n\nThe paper is clearly written and easy to follow. I think it could be a good contribution, but I have a number of concerns that I feel should be addressed first before it can be published.\n\nFirst, while the title and motivation are from the perspective of generative models, this is really a few-shot learning paper. The model only generates features from noise+other features (rather than a generative model of pixels), and the entire evaluation is from the perspective of few shot classification. I think the title could be changed to something like \"Few-shot learning via knowledge distillation and generative modelling\", and the introduction and motivation should focus more on few shot learning. As it stands, I don't think there is any evidence that this method leads to better generative models (as it doesn't generate pixels and the evaluation doesn't measure this).\n\nSecond, I think there are a few claims that need to be toned down in the paper:\na) In the context of the above, figure 4 should make it clear upfront that the images aren't *samples* but nearest neighbours of generated embeddings. Using \"synthesized embeddings\" for example, would be better.\nb) The approach is not model-agnostic, as it's specific to classification for this paper. I think it's necessary to demonstrate the generator can be used in other meta-learning contexts (eg. RL) for this claim to be validated.\nc) \"concurrent work\" should be \"prior/previous work\" in the results tables.\nd)  It's not clear to me why the progressive aspect is needed; why is it problematic if the teacher is far different from the student? The results seem to suggest that the progressive aspect doesn't improve performance much, and in the ensemble learning section, \"we found that starting with ktrain = kmax works as well as starting with lower values\". This seems to negate the need for progressive distillation in the first place if low values are fine.\ne) The paper states a few times that existing knowledge distillation approaches focus on distilling from large to small models, but there is plenty of work on distilling knowledge for other purposes (eg. See Teh et al, 2017; Schwarz et al, 2018), so this particular aspect is not novel.\n\n\nFinally, there are also some other questions and comments I have:\n\n1) The notation is confusing with uppercase for both sets and integers (eg. \"a subset L of M categories\"). I strongly suggest using \\mathcal for all sets in order to avoid confusion.\n2) From my understanding, the generator needs features as additional input (compared to more standard generators, eg. GANs) to act as context for unseen classes in the few shot regime. However, the intuition for this seems unclear given that the mean vector for a class is also introduced (this alone could ground the samples, with the noise capturing the intra-class diversity).\n3) The generator is trained to augment a class with samples that preserve its decision boundary (so perhaps it finds samples near the boundary). However, it's not clear to me how this captures information that transfers to unseen classes. Does it rely on shared \"decision boundary structure\" between classes (eg. The difference between cat and dog (in meta-training) might be similar between that of fox and wolf (in meta-testing)?) What sort of held out classes is this likely to generalise well or poorly to? Some intuition and discussion on this would be great.\n4) Typo in bottom of page 5: \"ditillation\"\n5) I think this work also shares similarity with virtual adversarial training (a competitive approach to semi-supervised learning) which should be referenced.\n\n\nMiyato, Takeru, et al. \"Virtual adversarial training: a regularization method for supervised and semi-supervised learning.\"\u00a0IEEE transactions on pattern analysis and machine intelligence\u00a041.8 (2018).\n\nSchwarz, Jonathan, et al. \"Progress & compress: A scalable framework for continual learning.\"\u00a0arXiv preprint arXiv:1805.06370\u00a0(2018).\n\nTeh, Yee, et al. \"Distral: Robust multitask reinforcement learning.\"\u00a0Advances in Neural Information Processing Systems. 2017."}