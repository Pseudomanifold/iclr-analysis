{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes using progressive knowledge distillation from teacher classifiers as a signal for training conditional generative models for the purpose of augmenting the classification performance of few-shot learners on novel problems.\n\nI would like to recommend acceptance for the following reasons:\n1) The proposed method clearly performs close to SOTA, yet slightly below. The paper demonstrates top performance in the generative category.\n2) The approach could potentially help augment the current SOTA (e.g. for miniImageNet: Lee et al. CVPR 2019), although this is not shown. Some results along these lines would definitely be impressive!\n3) The paper is clear and well written, which is a nice achievement considering that the approach is quite involved.\n\nI have some reservations because:\n1) The current state-of-the-art on miniImageNet (Lee et al. CVPR 2019) reports slightly better results with a conceptually cleaner approach. While results on the larger dataset are interesting, the improvements are fairly small.\n2) The resulting approach is quite complex technically and conceptually. While this is also true for competing methods, this paper doesn't manage to simplify the path to such level of performance, which I would have found valuable even if not SOTA.\n\nOverall, I found the work thorough and informative so I lean towards acceptance.\n"}