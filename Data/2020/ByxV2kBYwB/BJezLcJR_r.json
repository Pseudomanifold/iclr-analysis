{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes to train a generative model to improve the few-shot classification. Based on the prior work [1], the authors progressively transfer the knowledge of the teacher (trained by real samples) to the student (trained by generated samples). By gradually removing the real samples for the student, the generator produces better samples that retain the decision boundary.\n\nPros:\n- The authors propose a curriculum learning [2] approach to train a generative model.\n- The authors demonstrate that easy-to-hard curriculum improves the performance, in terms of the few-shot classification.\n\nCons:\n\n1. The method is a straightforward extension of prior work.\n\nThe idea of training generative models for a few-shot classification is proposed in [1]. The authors newly propose to apply curriculum learning [2] to improve the generative model. The proposed method is interesting, but the authors need new ideas to meet the high standard of ICLR. For example, the authors may investigate if the progressive distillation improves the generation itself, i.e., the standard knowledge distillation setting for the generative models [3].\n\n2. The performance gain is marginal.\n\nAlthough the proposed method requires an additional computation burden to train a generative model, the performance gain over the prior work is marginal. Also, while the authors denote other methods as \"concurrent work\", most of them are published 2-3 years ago. The most recent one, LEO [4] shows better results than the proposed method for 1-shot learning. Other recent work, e.g., MetaOpeNet [5], are not reported.\n\n3. The presentation could be improved.\n\nFirst, the title is misleading. It would be better to clarify that the goal of the paper is a few-shot classification. Second, the authors should specify what is already known and what is newly proposed. For example, the authors should clarify that Section 3 is preliminary.\n\n\n[1] Wang et al. Low-Shot Learning from Imaginary Data. CVPR 2018.\n[2] Bengio et al. Curriculum Learning. ICML 2009.\n[3] Aguinaldo et al. Compressing GANs using Knowledge Distillation. arXiv 2019.\n[4] Rusu et al. Meta-Learning with Latent Embedding Optimization. ICLR 2019.\n[5] Lee et al. Meta-Learning with Differentiable Convex Optimization. CVPR 2019."}