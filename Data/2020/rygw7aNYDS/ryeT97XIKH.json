{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Efficient Inference and Exploration for Reinforcement Learning\n================================================================\n\nThis paper presents a pure-exploration algorithm for reinforcement learning.\nThe approach is based on an assymptotic analysis of the Q-values, and their convergence to central limit distribution.\nUsing this analysis, and under specific assumptions, the algorithm outperforms existing algorithms for exploration.\n\n\nThere are several things to like about this paper:\n- Many existing analyses for efficient exploration are all based around more-of-the-same concentration bounds, which end up being quite messy and un-elegant. This approach based on central limit theorem appears to be more insightful and cleaner and I think there is something nice about that!\n- The proposed algorithm is reasonable, for the setting in question, and the experimental results show that it outperforms benchmark exploration algorithms in this setting.\n- The general structure of the paper, quality of writing and technical rigour appears to be of good standard... although I did not check all technical details carefully. [The paper is 24 pages long and we have many reviews]\n\n\nHowever, there are also some significant places where this paper falls short:\n- The problem setting that the authors consider is really not typical of the \"exploration\" problem in RL... I'm not talking about the fact that this is a \"pure exploration\" algorithm (that's fine), but instead that Assumption 3 is really not a good model for the types of problems that are \"hard\" for exploration in RL! For example, in the RiverSwim problem choosing an exploration policy = 0.8 right is essentially saying that you've already solved the hard part of the problem. Note - I am a little bit confused about the experiments in Tables 3 and 4, here it seems that you start with a pi(1|s)=0.6 which again feels like a cheat...\n- Would it be possible to compare this algorithm in a more like-for-like standard RL setting, perhaps using the standardised bsuite https://github.com/deepmind/bsuite (the \"deep sea\" problems might be of particular interest here.)\n- Alternatively, I can imagine a future version of the paper being more upfront about this deviation from the \"standard\" setting and highlighting that this is a special-type of result quite different from typical exploration in RL.\n- I'm not sure that this sort of paper is well-served by a conference like ICLR... certainly there seems very little of \"learning representation\" in this discussion of Tabular RL. That would sort of be OK if the paper made nods to how these *insights* could carry over the deep learning or at least RL with (linear) function approximation... I don't see much of that.\n\n\nOverall I do think this is an interesting paper, with a novel approach to pure exploration in tabular MDPs under specific assumptions.\nHowever, I'm not sure that this paper is well-suited to ICLR and I have some concerns about whether it really does address the sort of \"exploration\" problem in RL that one might expect."}