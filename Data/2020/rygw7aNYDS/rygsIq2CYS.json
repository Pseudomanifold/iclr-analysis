{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the inference problem of reinforcement learning. With a given exploration policy that satisfies some strong property to collect n data, the paper studies the distribution of the estimated optimal value function and Q-function when n goes to infinity. Both unique and non-unique optimal policy cases are studied. The non-unique case has a very different behavior as it is no longer Gaussian. The paper then uses these estimations to design a method that better explores and proposes a method Q-OCBA. Experiments were performed to compare this method with previous algorithms, e.g., UCRL.\n\nThe inference results for the Q-function is definitely important and will be useful for the community. But the authors should not ignore all the finite-sample bound results in their related work section, especially for those who achieving minimax rates. For instance: \nMG Azar, I Osband, R Munos, Minimax regret bounds for reinforcement learning, 2017\nMG Azar, R Munos, H Kappen, Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model, 2013\nA Sidford, M Wang, X Wu, L Yang, Y Ye, Near-optimal time and sample complexities for solving Markov decision processes with a generative model, 2018\nA Agarwal, S Kakade, L Yang, On the Optimality of Sparse Model-Based Planning for Markov Decision Processes\n\nPlease also see references therein. You can derive an inference result for the generative model as well. \n\nMore comments:\n* The assumption on exploration policy is quite restrictive. It essentially says the bound does not hold if the MDP is not communicating. However, the Q-function is still well defined.\n* The comparison to UCRL and other exploration-based method is not fair. In these results, the probability of picking \"the\" optimal policy is not important. Also, picking the correct action is not feasible if the problem is more complex. It is my understanding that your algorithm has a much larger regret due to that it requires to sample from every state.\n* What about finite horizon MDP?\n\n"}