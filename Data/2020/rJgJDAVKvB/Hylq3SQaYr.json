{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an approach to learn how to plan in continuous spaces using neural\nnets to learn a value function and a policy for scoring and sampling next-step candidates\nin a stochastic tree search.  The networks are updated as more planning tasks\nare executed, producing more data for the policy and value function, leading to gradually\nbetter plans compared to a number of baselines on benchmarks introduced by the authors.\n\nThis is a very interesting paper, although I did not always found it easy to read,\nmaybe too densely packed for comfort. My main concerns are clarity of the exposition (especially\nof the neural net architecture (sec 4.2) and that the comparisons are exclusively done\non benchmarks introduced by the authors rather than on benchmarks on which the baseline\nmethods had been previously been optimized, which may introduce a bias in favour of the\nproposed approach.\n\nClarifications\n\nBefore eqn 2, I don't understand why U includes both S_free and map, although the map specifies the free space and thus S_free seems redundant.\n\nIn sec 3 (page 3), the authors introduce a new notation s_init which seems to be the same as s_0 in the previous sections (or is it?).\n\nSection 4.2 was really difficult for me to parse and is too compressed (so is the rest of the paper but this one was worse).\n\nFigures were too small (esp. fig 4 and fig 7) for me to read from the printed paper.\n\nThe term 'meta self-improving learning' seems inappropriate. I did not see  how this was a form of meta-learning. Unless I missed something I suggest to change the terminology.\n\nOther Concerns\n\nI have a concern regarding the way r_t(s) is estimated (page 4) by kernel interpolation of the rewards. I fear that it will not generalize properly when trying to extrapolate, especially in high dimensions (since the claim of the paper is that the proposed algorithms is meant for 'high dimensional' states).\n\nIn addition, the experiments are actually performed in  rather low-dimensional settings (compared to working on problems with perceptual inputs, for example).\n\n"}