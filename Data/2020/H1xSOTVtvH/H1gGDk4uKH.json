{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nTo improve the generalization ability of deep RL agents across the tasks with different visual patterns, this paper proposed a simple regularization technique for domain randomization. By regularizing the outputs from normal and randomized states, the trained agents are forced to learn invariant representations. The authors showed that the proposed method can be useful to improve the generalization ability using CartPole and Car Racing environments. \n\nDetailed comments:\n\nI'd like to recommend \"weak reject\" due to the following reasons:\n\n1. Lack of novelty: The main idea of this paper (i.e. regularizing the outputs from normal and randomized states) is not really new because it has been explored before [Aractingi' 19]. Even though this paper provides more justification and analysis for this part (Proposition 1 in the draft), the contributions are not enough as the ICLR publications.\n\n2. As shown in [Cobbe' 19], various regularization and data augmentation techniques have been studied for improving the generalization ability of deep RL agents. Therefore, the comparisons with such baselines are required to verify the effectiveness of the proposed methods.\n\n3. For domain randomization, it has been observed that finding a good distribution of simulation parameters is a key component [Ramos' 19, Mozifian' 19, Chebotar' 19], but the authors did not consider training the distribution of simulation parameters in the paper. \n\n[Ramos' 19] Ramos, F., Possas, R.C. and Fox, D., BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators. In RSS, 2019.\n\n[Cobbe' 19] Cobbe, K., Klimov, O., Hesse, C., Kim, T. and Schulman, J., Quantifying generalization in reinforcement learning. In ICML, 2019.\n\n[Mozifian' 19] Mozifian, M., Higuera, J.C.G., Meger, D. and Dudek, G., Learning Domain Randomization Distributions for Transfer of Locomotion Policies. arXiv preprint arXiv:1906.00410, 2019. \n\n[Chebotar' 19] Chebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff, N. and Fox, D., May. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA) (pp. 8973-8979). 2019\n\n[Aractingi' 19] Michel Aractingi, Christopher Dance,  Julien Perez, Tomi Silander,  Improving the Generalization of Visual Navigation Policies using Invariance Regularization, ICML workshop 2019."}