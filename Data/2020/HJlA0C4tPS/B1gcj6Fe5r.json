{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper introduces a probabilistic generative model for\nunsupervised style transfer of text. The approach introduced in\nthe paper does not require paired training data. An\nencoder-decoder model is trained to transfer text from one style\nto another and back.\n\nReview:\n\nThis work is very well-written and easy to follow. The\ncontribution is clearly articulated as while there are\nprobabilistic generative models for transfer in the\nliterature (Shen et al does include one) they don't perform as\nwell. Ablation studies further confirm the need for the\nparticular kind of parameter sharing used in the model in the\npaper. Great results are shown on 5 text transfer problems.\n\nClarifications and improvements:\n\nJust for clarity, in the last paragraph on page 4. It says two encoder-decoder\nmodels are learnt, but isn't the idea that there is effectively only one\nencoder and one decoder learned that just put together in different ways\nduring training? I'm also curious why the baseline of BT+NLL was so strong? Is\nhaving the loss of a language model work that much better than the regular\nentropy term?\n\nI would also like if possible if you could share some of the repetitive examples\ncreated by BT-NLL which explain its low PPL."}