{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper aims to answer the question proposed in the title. The authors conduct a series of experiments using well known hierarchical and non-hierarchical algorithms in order to extract the what is it that makes hierarchical reinforcement learning (HRL) efficient. The conclusion is that the main benefits of HRL are due to 1) temporally extended training enabling multi-step rewards, 2) temporally extended and semantically meaningful exploration.\n\nI really enjoyed reading the paper and I believe it is an important contribution to the community. I believe the experiments are sufficient to support the hypothesis stated in the paper. Also, the authors state clearly the potential weak points of the paper in the discussion.  For the previous reasons I suggest the acceptance of this paper, although some improvements could be made. \n\nSupporting arguments:\n\nThe paper is clearly well written. There is a clear exposition of the ideas, hypothesis and experiments. \n\nThe sequence of experiments makes sense to me. For example, section 5.1 tries to disentangle H1 and H2 (concluding both are important to different degrees), then in 5.2 H1 and H3 are evaluated (concluding that H3 is non-beneficial) and 5.3 focuses on the most important effect (exploration) through H2 and H4. \n\nAs mentioned on the discussion, other hierarchical designs, environments and tasks might lead to different conclusions. Even though this is true, I think the paper correctly balanced breadth vs depth of experiments and conditions. I think this paper serves as a baseline and might inspire further similar research that may cover wider settings (more hierarchical designs, environments, tasks etc.) to see if the results still hold. \n\nThings to improve:\n\n\nThe data from all the figures presented in the paper could  be used to perform statistical tests to support the authors\u2019 statements. For example, In Figure 2 (top) there is a varying parameter (c_train) and the authors claim that there is a noticeable effect on performance for c_train > 1. One can clearly see this in AntMaze and AntPush but it would be nice that when authors make such statements that they are backed by a significance test (e.g. analysis of variance, two-way anovas, t-tests, or any kind of test that the authors might deem suitable).  Similarly, one can do this for Figure 2(bottom),  and the remaining figures with their corresponding statements (e.g. c_switch >1 vs c_switch =1 in Figure 4).\n\nPerforming such statistical tests might require to run more than 5 seeds tough, but I believe the paper would be stronger with such statistical tests and I would increase the score accordignly.\n"}