{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper investigates the question of generalization in deep networks. The main contribution is to that show train loss can be predictive of the test loss when you normalize the networks, for over-parameterized network using ReLu activation functions. Authors perform a set of experiments on CIFAR10/CIFAR100 and MNIST to validate their claim.\n\nComments:\nOverall the paper main message is clear, however some points/formatting in the paper could be improved: \n-\tAuthors state in section 2 that they use CIFAR10, MNIST, CIFAR100. While the results on CIFAR100/MNIST are in the appendix, I did not find references/descriptions of those results in the paper main body.\n-\tSection 2 refers to equation 7,8 several times. Those equations are only defined in the appendix. It seems that authors should refer to equation 3 and 4 instead? \n-\tAuthors refers to Figure 8 in section 2 without describing it. It is unclear why Figure 8 is needed here?\n-\tSection 2 claims that the Rademacher complexity in the experiment is found to be small for the normalized network. But it is unclear which data/figure support this claim?\n-\tSection 3 refers to figure 9 which appears unrelated (and figure 9 caption refers to figure 10). I think section 3 should refers to figure 4 instead.\n-\tI did not find any reference to figure 3 in the main text\n-\tIn section 4, I find the following part \u2018generalization for the classification error and for the unnormalized cross-entropy is attained for much larger N [\u2026] N must be significantly larger than the number of parameters...\u2019 a bit unclear. Could you elaborate given that overparameterized networks can achieve good validation test \n-\tAbstract claims that \u2018we show that when the performance is measured appropriately [\u2026] training performance is predictive of expected performance\u2019. In practice, the paper assumes that the networks are overparameterized and uses non-linearity which satisfies the positive homogeneity property. While those limitation are clearly stated in the section, it would be nice to make that clear in the abstract/introduction as well\n-\tIn section 2, authors say that \u2018most datasets are separable by overparameterization\u2019. It would be nice to elaborate on that or add a citation justifying this claim.\n\nIn addition, while the paper clearly show that the normalized train cross-entropy can predict the performance of the of the normalized test cross-entropy, it is unclear if the normalized train cross-entropy is predictive of the test classification error ?\n\nWhile the paper writing and clarity could be improved, I think the paper makes interesting observation, I therefore lean toward acceptance.\n\n"}