{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\n\nThis paper shows that the generalization puzzles introduced by Zhang et al. 2016 are in fact not puzzles at all. The difference in generalization performance from training performance is well explained by the normalized loss on the training set. This paper shows that for networks using the cross-entropy loss with ReLu activations, normalizing the weight matrices with the product of their Frobenius norms scales the training loss so that it more appropriately approximates the test loss. The paper concludes with a recommendation that practitioners use this normalized loss function, giving more accurate assessments of generalization error for early stopping. The paper additionally concludes for theorists that these experiments demonstrate that the training performance can still be considered a proxy for testing performance.\n\nReview\n\nI think that this papers approach to understanding the generalization behavior seen in deep learning is important. By understanding the relationship between the test-time performance and train-time performance, we can apply the classical statistical learning theory results to deep neural networks reliably. I think that the investigation of the training performance and generalization performance as the number of parameters increases is a fine demonstration of the utility of the proposed scaling.\n\nI did not fully understand the motivation to scale the weights specifically by their respective Frobenius norm. The paper mentions that \"Later we will explain the precise motivation for these choices\", but I cannot find any such explanation. It seems that we could choose to scale each weight matrix arbitrarily with positive constants and observe similar results, and perhaps scaling by even larger values (perhaps the squared Frobenius norm) we could asymptotically cause the training loss and testing loss to match. My question to the author is: \"Why use the Frobenius norm to normalize the weights as opposed to some other value?\"\n\nI found that the number of insights contained within the paper were too few and too inconsequential to constitute a full ICLR conference paper. This could simply be due to poor vertical space management, especially with several of the plots in the main paper. The paper additionally heavily referenced plots in the supplementary material in a way that made many of the claims unable to stand alone from the appendix. Figure 4 also does not appear to be referenced in the main body of the paper and the caption for this figure leaves much to be desired. It is unclear to me what this nearly full-page figure is trying to communicate, as the message is not immediately clear from the figure itself.\n\nThe paper directly responds to Zhang et al. 2016, however many of the empirical results are in no way comparable making it difficult to understand how this paper builds upon or further extends the work by Zhang et al. 2016. For instance, using the loss function as a surrogate for classification error while discussing the generalization abilities of the classifiers makes it difficult to understand how well each classifier is actually performing in the empirical results. For instance, the caption of Figure 2 says \"Notice that all points have zero classification error at training\", then continues to make additional claims about classification error in the empirical results; yet I see nothing in Figure 2 demonstrating the classification error of the networks. Because of this, the only conclusions I could draw from Figure 2 are that the suggested normalized loss function better matches in training and testing than the unnormalized loss function; however, this does not allow me to draw any conclusions about the generalization abilities of these networks for classification.\n\nAdditional Comments (do not affect score)\n\nThe captions throughout the paper do not well explain what is shown in the figures. For instance, I did not originally understand the dashed pink lines in Figure 1 until noticing them again in Figure 3 on my first pass.\n\nI generally disagree with this direction of research and reasoning as I find that the assumption that the number of parameters in a deep network is much greater than the number of samples is poor. While this is certainly true when testing deep neural networks on small datasets like MNIST and CIFAR-10, it is not sustainably true for networks deployed in data centers or deep networks used for any sort of lifelong learning setting.\n"}