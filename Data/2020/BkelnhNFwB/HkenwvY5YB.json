{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Many papers over the last few years have studied the \u201cgeneralization puzzle\u201d, in the words of this paper, of how overparameterized deep neural networks have the capacity to completely fit the training data yet still generalize well to a test set. This paper proposes a resolution to this puzzle: assuming a model $f: \\mathcal{X} \\to \\mathbf{R}$ and a classification rule $\\hat{y} = \\mathbf{1}\\{f(x) \\geq 0 \\}$, we can divide $f$ by a large number while maintaining the same classifications (and therefore leaving the 0-1 loss the same). However, dividing $f$ by a large number compresses, say, the logistic loss, with the end result that the train and test logistic losses of the resulting \u201cnormalized\u201d networks are quite similar. Therefore, the paper claims that there is no generalization puzzle.\n\nIn my opinion, the paper does not accurately capture the nature of this generalization puzzle. It is easy to train a model that achieves 0 training error but high test error (this is not specific to neural networks; any overparameterized linear model would do, or even consider a model that simply memorizes the training data and predicts a random number otherwise). The central question is why training a deep neural network on real data often results in a model with 0 training error but low test error. Normalizing the neural network by dividing it by a large constant and then showing that the resulting (high) logistic loss on the training set is close to the logistic loss on the test set is expected behavior, and does not address this question."}