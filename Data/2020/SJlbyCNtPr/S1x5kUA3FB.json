{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method for combining continuous Q learning and linear MPC. They propose learning a continuous control policy for long term behavior trained with standard model free reinforcement learning and a short term linear control model implemented as a linear dynamical system. They propose learning them separately each with its own loss function. At test time a combined controller can be used to infer actions by solving small optimization problem. The linearity makes inference very efficient and even when additional constraints are added only at test time. The model should therefore me more flexible and allows for adapting 0 shot when the constraints are known. The paper is well written and mostly clear. \n\nThe equations seem correct but I am not an expert in continuous control. The examples considered seem a bit too simple to be insightful. \n\n* the algorithm seems to be be setup to with the dynamics model being learnt on the transitions coming from an epsilon greedy with respect to the optimal policy. Does this mean that generalization may be affected if the model strays off the optimal trajectories ?\n* There would be some benefit in trying the model in some other domain maybe reacher with some constraints ?\n\n"}