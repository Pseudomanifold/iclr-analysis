{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nIn this paper, the author proposes LLQL (Locally Linear Q-Learning), which separates the action design from prediction models, such that the model can have short-term and long-term goals. The short-term network takes the current state and the action, and predicts the next state (to be specific, the status change) under the assumption that the output is linear to the action. The long-term network takes the current state as input, controls the controller to take action, and also optimizes a Q-function. I lean to vote for accepting this paper, though the experiment part can be further improved.\nStrengths\n- The proposed model is novel, which can take a trajectory or a constraint as a short-term goal. As a result, we can control the behavior of the model easily. Otherwise, we have to design a new reward function to guide our model indirectly. As the authors showed in Tables 1 & 2, LLQL outperforms this approach.\n- The paper is clearly written. The author provides clean formulas throughout the paper, which makes the paper easy to understand.\nWeaknesses\nMore experiments can be conducted to demonstrate the effectiveness of LLQL.\n- Only tested in one scenario. In the main text, though it is already very long, there are still only experiments in one scenario (i.e., Mountain Car).\n- Only compared to one baseline. In order to demonstrate the effectiveness of LLQL taking a short-term trajectory or constraint, the authors compared it to only one method (i.e., DDPG) with a few manually designed reward function.\nMinor\nEquation 6 looks incorrect to me.\nPossible Improvements\nAs mentioned before, it would be great to include more experiments in the main text, comparing LLQL to more baselines, testing LLQL in more scenarios, or doing some ablation studies (removing one part of LLQL to demonstrates the effectiveness of that part)."}