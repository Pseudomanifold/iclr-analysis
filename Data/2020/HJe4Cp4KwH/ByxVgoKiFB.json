{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper performs a concise experimental survey of graph neural network method, and introduce GNN-FiLM as a new approach. They reproduce all the baseline and show that GNN-FiLM is in line with SOTA models. \n\nThe paper is pretty easy to follow, and the intuition is clearly explained. The idea is pretty simple, and it is a natural extension of FiLM to the graph setting. The results are tested on 4 different datasets, and the experimental protocol is clearly explained, and the results are convincing.) \n\nHowever, I am missing a discussion of the proposed methods. For instance, what are the FiLM clusters? What are the key errors with GNN-FiLM, GNN-MLP0? Are the models complementary (e.g., by using a mixture of experts)? Can we combine GNN-MLP0 and GNN-FiLM? \n\nRemarks:\n - The bibliography work is incomplete. For instance, the authors do not cite the Hypernetwork paper [0], when stating that several GNN variant exists, we expect to have some references, idem when the authors mention that [FiLM] has been very effective in several domain (GAN [1], Meta-learning [2] Sound [3], language-vision task [4]). As a hint, it is pretty rare to have empirical papers with only 16 citations. I would say that 20-25 is the bare minimum. \n - Adding a visual sketch of the different models may provide additional intuition \n - I would encourage the author to define the dimension of W_l, h_u, etc. clearly.\n - On a personal note, I like informal writing and paper honesty, but I would not always recommend it for submission.\n - As the authors reproduce the experiments, It would have been useful to add the original results in the table whenever it is possible\n - The paper gives a feeling that it lacks some rigor (missing ref, detected bug, mathematical formalism is light); it makes me a bit skeptic regarding some experimental conclusion. It would have been helpful to look at the code (at least to see if it can be easily parsed and analyzed.)\n\nI think the paper is valuable for the community as it provides a simple survey of the current methods, the code is available, and the authors provide a lot of intuition. However, the overall level is slightly below the ICLR level for the following reason: incomplete bibliography, lack of complementary experiments, lack of discussion. The overall writing style is quite unusual, but It is not a negative point from my perspective. In its current state, it is a strong workshop submission but a not-good-enough paper for ICLR.\n\nHowever, I am open to discussion, especially if the authors have complementary discussions and results.\n\nPS: the reviewer is very familiar with the modulation and multi-modal literature, but he has only basic knowledge in graph networks.\n\n \n[0] Ha, David, Andrew Dai, and Quoc V. Le. ICLR 2017.\n[1] Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" ICLR 2019 \n[3] Jiang, X., Havaei, M., Varno, F., Chartrand, G., Chapados, N., & Matwin, S.. Learning to learn with conditional class dependencies. ICLR 2019\n[3] Abdelnour, Jerome, Giampiero Salvi, and Jean Rouat. \"From Visual to Acoustic Question Answering.\" arXiv preprint arXiv:1902.11280 (2019).\n[4] Strub, F., Seurin, M., Perez, E., De Vries, H., Mary, J., Preux, P., & CourvilleOlivier Pietquin, A.. Visual reasoning with multi-hop feature modulation. ECCV 2018"}