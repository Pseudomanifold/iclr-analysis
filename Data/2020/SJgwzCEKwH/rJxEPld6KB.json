{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies leveraging mode connectivity to defend against different types of attacks, including backdoor attacks, adversarial examples, and error-injection attacks. They perform a comprehensive evaluation to show the benign test accuracy and attack success rate over the models in the connected path between pairs of models with the same or different properties, e.g., both are attacked, both are benign, one attacked and one benign, etc., where the connected path is learned using existing algorithms to find the high-accuracy path over two different models. Their evaluation suggests that in certain attack scenarios, exploring the mode connectivity could help find a model that has a high benign accuracy, while with a significantly lower attack success rate than the models at the end points.\n\nIn general I like this paper. Although mode connectivity has been studied in the literature, to my knowledge, this is the first work to extensively study this topic in the context of attacks. An interesting part of the paper is their evaluation on backdoor attacks, where they show that with a very small number of training samples for fine-tuning, they are able to find a model with decent test accuracy, while the watermarks are removed. Meanwhile, the attacks studied in this work include various settings, i.e., poisoning attacks, error-injection attacks, evasion attacks, and also adaptive attacks where the adversary knows that the defender will use path connection to improve the robustness. These make this paper a good reference as a systematic study of their proposed topic.\n\nHowever, my main question is that while the algorithm is pretty effective to defend against backdoor attacks and error-injection attacks, the results of evasion attacks are somehow negative. While it is helpful to show negative results if this is indeed the case, do you have some possible explanation why the models on the path are less robust than the two end models?\n\nAnother concern is that while the benign test accuracy of the model found by their algorithms looks good, both CIFAR-10 and SVHN evaluated in this paper have a small label set and may not be challenging enough. It would be great if the authors can provide some results on a more complicated image recognition benchmark, e.g., some intermediate-level datasets studied in previous work on attacks such as CIFAR-100."}