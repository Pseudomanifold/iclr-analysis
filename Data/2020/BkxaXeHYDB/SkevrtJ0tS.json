{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a modification to ResNet architecture, motivated by Gauss-Newton optimization method. They change the residual block to include a weighted product of residual input with itself. I did not understand how exactly this product operation is defined, it is not clearly explained. In figure 2b there is a sign \"*\", not defined in text, which looks like Hadamard product of input convolved with weight and input itself. I do not see a link between Gauss-Newton and this operation, though, and will let the authors to correct me. The proposed residual block is compared to the original ResNet on a number of image classification datasets. The authors also compare their quadratic residual block to a residual block without non-linearities.\n\nI propose reject mainly because experimental validation is not aligned with paper claims.\n\nThe authors claim that their block aims to \"accelerate convergence of ResNet\", meaning to decrease the number of residual blocks in the network, and test this on CIFAR-10 and CIFAR-100 with 18 to 34 layer networks, claiming that Newton-ResNet is achieving the same performance with fewer blocks. However, state-of-the-art results on these simple datasets can be achieved with even 10-16 layers. On the difficult ImageNet dataset they show no improvement over ResNet, since the proposed network has more parameters than ResNet-50.\n\nIn figure 1 the authors visualize loss surface of ResNet with their modification with original ResNet. It is not evident from the figure why ResNet takes more steps to reach the minimum, and how this it related to the number of blocks. Specific details of this experiment are not provided.\n\nI would also disagree that Gauss-Newton was dominant method for solving optimization problems before deep learning, this is arguable.\n\nThe authors also claim that the proposed residual block eliminates the need of nonlinearity, which is false, because the weighted product is a nonlinear quadratic operation.\n\nAnother remark, CIFAR and ImageNet datasets are so well known that their description could be removed.\n\nExperiments with generative models are a nice addition, but do not support the claims.\n\nPyTorch or tensorflow code defining the proposed modification to residual block would be very helpful."}