{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This manuscript describes a new extension of the very popular and powerful deep residual neural network, mainly inspired by the Gauss-Newton optimization method. The main idea is to treat the original deep residual network as a difference equation and then add a second-order term to this equation. The idea of treating the residual network as a difference equation is not new, but to the best of my knowledge adding the second-order term seems to be novel. The authors have also presented a few experimental results to validate their idea. The major advantage of this idea is that by using the second-order term, a residual network with fewer layers can achieve similar performance as the original residual network with more layers. As such, using such a network architecture can potentially reduce the GPU memory consumption, which is important since most GPUs have a limited amount of memory which is not enough for some specific applications. Overall, I like the idea presented in this paper, but it also needs some improvement. \n\n1) There is no theoretical analysis of time complexity at each training iteration.  How expensive is it to calculate the second-order term and its corresponding gradients?\n\n2) In addition to the basic residual network, I suggest the authors compare their network with some variants that also treat residual network as a difference equation.\n\n3) I do not think that Fig. 1 is needed. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}