{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method, which is named T(Transferable)-NAS, for neural network architecture search (NAS) by leveraging the gradient based meta learning approach. The state of the art NAS algorithms search an architecture for a single task and a new architecture is searched from scratch for each new tasks. T-NAS learns a meta-architecture in order to adapt to the new tasks through a few gradient steps. This works uses MAML (Finn et al 2017) and some of its variants for gradient based meta learning. But instead of learning the weights (w), they learn a task sensitive meta architecture (\\theta). To decode the learned \\theta to task-specific architectures, they follow the method proposed by Liu et al 2018b. In that case, different architectures is used for different tasks as opposed to the baseline algorithms.\n \nThis paper proposes an incremental approach which is a combination of the existing algorithms. The most important contribution of this paper is providing Equation (5) that is used to update w and \\theta parameters together by only backpropagating once. In this way, you don\u2019t need the high-order derivatives and so you need less memory and search time. They compare T-NAS with state-of-the-art few-shot learning methods. I like the extensive empirical work of this paper. The prediction accuracy is mostly comparable with the baselines even some of the baselines are needed to be trained on many-shot classification tasks or use more complex architectures.\n \nI think it would be good to add the method of Liu et al 2018b as a background. Because it is used as a part of the proposed algorithm and it is important to know the method to understand how the learned parameter \\theta is adapted to the task-specific architectures. Besides that it is only mentioned once in the experiments section that T-NAS++ is based on MAML++ (which is an improved version of MAML, that investigates how to train MAML to promote the performance). If T-NAS++ is superior to the other baselines, including T-NAS, the difference should be clarified and emphasized more. \n \nTable-3 presents the experiment results in a confusing way. In the current version, it is not very clear which method performs the best. For example I prefer to see the best results in bold. In addition, maybe it could be better to present the results separately for methods that uses pretrained models and do not use pretrained models. The reason why some methods perform better than the proposed method could be mentioned in the text instead of mentioning it as a footnote. Finally, it would be better for the readers to see the time comparisons explicitly given that one of the most important contribution of this work is to increase the efficiency."}