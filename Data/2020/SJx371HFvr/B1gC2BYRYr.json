{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the distribution of the activations of the last layer before softmax in classification networks. They argue that cross-entropy minimization is performing posterior inference over the distribution of class labels given the activations. They use this intuition to propose a new loss to train a student network in knowledge distillation and class conditional image generation.\n\nCons:\n- Dropping the norm of w and a is not well-justified and the form of the vMF distribution is based on this.\n- The proposed L_CKD loss doesn't seem particularly interesting. Basically, instead of training the student network with the distribution p(pred label | input), they train it to match the distribution p(pred label | gt label). This is a simpler distribution that provides less training signal to the student. There is no reason why this loss should work better than the knowledge distillation loss.\n- Results for knowledge distillation in Table 2 and 3 are misleading. Usually, the knowledge distillation loss combines the classification loss with KD loss with a regularization term and the regularization term is tuned. So with properly tuned hyperparameters, KD should not perform worse than classification loss alone.\n- Figure 3 suggests that there is no difference between training a student network with a weak teacher network like ResNet-18 vs a stronger one like ResNet-101. This is expected based on the explanation above and not in favor of the proposed loss.\n- The gap between label accuracy in table 2 and the accuracies with wrn models from Anh et al 2019 is big. That suggests a problem in training.\n- Section 2.3: One needs to consider normalizing during training for the intuition about posterior inference to hold.\n- Eq 6 is ignoring the normalization by qj, why?\n- Eq 6: the expectation on the right-hand-side should only over i. What is the expectation inside D_KL? Based on the next paragraph it should be over a."}