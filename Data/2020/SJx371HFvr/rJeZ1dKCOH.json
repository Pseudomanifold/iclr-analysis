{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors observe that training with the softmax loss is (approximately) maximum log-likelihood for a model in which each class-conditional distribution on the last hidden layer is a von Mises-Fisher (vMF) distribution.  They claim that this shows that the class-conditional distribution is a vMF distribution.  I do not see this -- very roughly, they have shown that training with soft-max behaves as if this is the case, not that it is the case.  They then propose a distillation method based on this model, and also use the model to motivate a distribution of the seeds for a generative model (specifically a VAE model).\n\nThe experiments whose results are plotted Figure 1 are not described in\nenough accuracy to be reproduced.  I don't see strong evidence from\nthese plots that the class-conditional distributions are vMF distributions.\n\nIt is folklore that the soft-max loss performs multinomial\nlogistic regression with respect to the last hidden layer,\nand back-propagates the gradients to earlier layers.\nSee https://chrisyeh96.github.io/2018/06/11/logistic-regression.html.\nTheir model is like this, except they abstract away the lengths of the\nvectors.\n\nIn Section 4, the propose to distill a model by using a vMF\nmodel based on the last layer to probabilistic model of\nthe distribution of class predictions for a randomly\nchosen member of the class of an example sampled during\nthe student phases.  In constant, the original\ndistillation algorithm uses the teacher model to\nestimate the probability of different classes, for\nthe particular member of the class of a training example.\nThey make a case that using their method improves robustness\nto domain shift.  These results are believable, but\nunderwhelming.\n\nFinally, the use the vMF model based on the last hidden\nlayer as a prior for a VAE that chooses uses a prior of\nthe hidden variables that generates unit-length vectors.\nThis VAE trains on data that includes class designations.\nTaking account of the structure of the classes in this way\nsounds like a good idea to me, and they get slightly better\nresults in this way.\n\nThe paper is written in a clear and engaging way.\n"}