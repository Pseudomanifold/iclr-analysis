{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new variant of invertible flow-based model for graph structured data. \nSpecifically, the authors proposed a continuous normalizing flow model for graph generation, first in the graph generation literature. \nThe authors claim that the free-form model architecture of neural ODE formulation of continuous flow is advantageous against standard discrete flow models. Experimental results show that the proposed model is superior to recent models in image puzzling and layout generation datasets. \n\nOverall, manuscript is well organized. \nThe combination of continuous flows and graph structured data is new in the literature (as far as I know). Proposed formulation seems natural and reasonable. I find no fatal flows in the formulation. In experiments, the proposed model achieved good scores against recent GNN works. \n\nConcerning the existing invertible flow-based models for graph structured data, Madhawa\u2019s work is one of the first attempts in the literature. I think the paper below should be refereed appropriately:  \nMadhawa+, \u201cGraph NVP\u201d, arXiv: 1905.11600, 2019. \n\nThe way of incorporating relational structure into flows are very similar to the GraphNVP and Graph Normalizing Flow: using neighboring nodes\u2019 hidden vectors as parameters (or input to parameter inference networks). \nIn addition, I found no special tricks or theoretical considerations to achieve the continuous flow for graphs. Based on these points, I think technical contribution of this paper is somewhat limited. \n\nI cannot find information about the specific chosen forms of f-hat and g in Eq.(10) within the manuscript. Are the choices of f-hat and g are crucial for performance? It is preferable if the authors can present any experimental validations concerning this issue. \n\nMy main concerns are in the experimental section. \n\nI\u2019m not fully convinced in the necessity of the continuous normalizing flows for the experimental tasks. None of the experimental tasks have `````'' intrinsic continuous time dynamics over graph-structured data (Sec. 2)''. Then, what is the rationale to adopt Continuous graph flow for these tasks? \n\nOne reason to adopt continuous flows is that the ODE formulation allows users to choose free-form model architecture, yielding more complicated mappings to capture delicate variable distributions. \nI expect some assessments are made concerning this issue. My suggestion is (i) to test the discretized model of the proposed Continuous Graph Flow and see how the discretization deteriorate the performance, and (ii) to test several choices of f-hat and g (Eq.10) to show the necessity of ODE formulation, accommodating free-form model architecture.  \n\nIn the current manuscript, Graph Normalizing Flow (GNF) is the closest competitor. However, GNF is not tested in the puzzle and the scene graph experiments. Why is that?\n\nI\u2019d like to hear opinions of the authors concerning these issues, and hope some discussions are included in the manuscript. \n\n\nSummary\n+ continuous normalizing flow is first applied to graph structure data\n+ manuscript is well organized\n+- natural and reasonable formulation. But at the same time, technological advancement is limited. \n- Less convinced to adopt continuous flow for graph-structured data without no intrinsic continuous dynamics. \n- Necessity or advantages of ``continuous\u2019\u2019 flows are not well assessed in the experiments. Please consider some additional assessments suggested in the review comment. \n- GNF, the closest competitor, is omitted in the 2d and the 3rd experiments. No explanations about this. "}