{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes continuous graph flow (CGF), a flow-based generative model for graphs.  Based on the idea to build continuous-time flows with neural ODEs, the node features are transformed into Gaussian random variables via message passing. The log-likelihood can be approximated stochastically.\n\nI find it hard to assess the novelty of this work because 1) the algorithm looks like a trivial application of continuous-time normalizing flow to graph data using message passing algorithm, and 2) the concurrent work graph normalizing flow (GNF, Liu et al., 2019).  I failed to find any algorithmic innovation more than a mere application of continuous-time flow to graph data. The very idea of a flow-based model for graphs using the message passing algorithm may be considered as a contribution, but this is also blurred because of the concurrent work GNF. \n\nThe experiments look interesting, but there are some minor concerns. For the unsupervised graph generation task, CGF is shown to perform better than GNF, but I think the comparison here may not be fair because the results for GNF seem to be obtained from the paper uses a different way to construct initial node features to be fed into message passing. Specifically, according to the GNF paper, they inject an i.i.d. Gaussian noise as initial node features, but this work uses different schemes based on the dual graph (btw I think the term dual graph may be misleading. There already exists a common term called dual-graph with different definition). So I think to compare the expressive power of flows, it would be fair to start from a common scheme to build initial node features. GNF should also be compared to CGF for the other experiments. Seems like the baselines presented in Table 2 and Table 3 are quite dumb baselines not well suited for the tasks considered here. \n\nIn page 2, the authors stated that \"GNF involves partitioning of data dimensions into two halves and employs coupling layers to couple them back. This leads to several constraints on function forms and model architectures...\". I think GNF can be improved using more advanced transformation other than affine coupling, such as recently proposed mixture logistic CDF [1] or neural spline flow [2].  Continuous-time flow may still have an advantage in memory usage, but at least in terms of expressive power, I think it is not clear whether CGF is particularly better.\n\n[1] Ho et al., Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design, 2019\n[2] Durkan et al., Neural Spline Flows, 2019"}