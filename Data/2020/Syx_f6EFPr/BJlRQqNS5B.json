{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method for simultaneously imputing data and training a classifier. Here are my detailed comments:\n\n1. The proposed method is a simple combination of dictionary learning and classification. As can be seen from (5), J_1 and J_2 are the loss function for dictionary learning, and J_0 is the empirical risk for classification (J_0). I did not see much novelty here.\n\n2. There exists a capacity mismatch between data imputation and classifier. The paper claims that the method is proposed for training deep neural networks. However, deep neural networks are capable of modeling very complex distributions (e.g., images), and the dictionary learning only considers a very simple bilinear factorization, which is not of the same modeling capability as neural networks. \n\n3. The experimental evaluations are very weak. Table 1 even does not provide any meaningful baseline methods, e.g., the so-called \"sequential method\".\n\n4. Theorem 3.2 requires the dictionary to be RIP, which is a very strong assumption and unlikely to hold in practice, especially when considering D is actually trained by minimizing (5). The proof of Theorem 3.2 is very elementary, ad-hoc, and does not provide any insight to the problem."}