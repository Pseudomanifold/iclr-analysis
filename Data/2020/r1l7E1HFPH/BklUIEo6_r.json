{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on the implementation and some empirical evaluations of a class of algorithms designed to find optimal strategies/values of large MDP.\n\nThe basic idea of these algorithm (called \\kappa-PI or \\kappa-VI) is to combine two type of classical approaches:\n- policy/value iteration \n- k-step ahead computation (instead of just 1-ahead, and actually, k should be quite big or even infinite with an auxiliary appropriate discount rate).\n\nThe theoretical formulation of \\kappa-PI and \\kappa-VI involves solving, at each iteration, another auxiliary MDP problem (where the discount rate is of order \\kappa\\gamma). This is basically what this paper suggests to do, and implements. \n\nThe experiments are a bit difficult for me to read, as the baselines (\\kappa=0 and =1, say) are compared with \"the best \\kappa\" which seems to be problem dependent, so I do not know if there is a clear message."}