{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The main contributions of this paper are k-PI-DQN and k-VI-DQN, which are model-free versions of dynamic programming (DP) methods k-PI and k-VI from another paper (Efroni et al., 2018).  The deep architecture of the two algorithms follows that of DQN.  Efroni et al. (2018b) already gave a stochastic online (model-free) version of k-PI in the tabular setting.  Although this paper is going one step further extending from tabular to function approximation, I feel that the paper just combined known results, the shaped reward from Efroni et al (2018a) and DQN.  The extension seems straightforward.  Mentioning previous results from Efroni et al (2018a) and (2018b) does not justify the extension would possess the same property or behaviour.   The experiments were only comparing their methods with different hyperparameters, with only a brief comparison to DQN.  "}