{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper investigates the question of identifying concise equations from data to understand the functional relations. In particular, a set of base functions are given in hand and the goal is to obtain the right composition of these functions which fits the target function. The main contribution of the paper is to introduce a selection layer, which enhances sparse connections in the network. Several experiments are conducted to show the effectiveness of the method. \n\nMy main concern of the paper is about the novelty and the lack of comparison of existing methods. The framework of finding functional relations is set up in [1,2], the main contribution of the paper is a refine architecture with the introduction of the selection layer. However, this selection layer is nothing but incorporating a softmax function. The idea of combining softmax functions in the hidden layers is not novel neither, which could be found in [3,4]. As a result, I find the contribution of the paper very limited, which could be summarized as applying an existing technique on a specific problem. Moreover, in the experimental section, there is a lack of comparison with existing methods such as EQL[1,2] and I consider it a major omission. \n\nOverall, due to the novelty concern and the lack of comparison, I do not support publication of the paper.\n\n[1] Sahoo et al.  Learning Equations for Extrapolation and Control\n[2] Martius et al. Extrapolation and learning equations\n[3] Graves et al.  Neural turing machines\n[4] Graves et al.  Hybrid computing using a neural network with dynamic external memory"}