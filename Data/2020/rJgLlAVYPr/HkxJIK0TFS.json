{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "My understanding of this paper is that it proposes a combination of simple logical blocks that can efficiently learn logic rules implemented by a logic function. I think the authors define interpretability as the possibility to exactly express an unknown function in a composition of blocks, but despite several reading of this paper, I am not sure. A strong assumption in this paper seems to be that the target function of a supervised task can be exactly expressed via some compositional blocks. I would suggest a significant revision to clearly explain why WBNs are more interpretable and avoid any vague terminology.\n\nI will list below some of my concerns:\n\n- After reading several times this paper, I do not understand why this method is \"more interpretable\". The explanation of the papers are quite verbose. I tried to phrase my concern as this: could a standard CNN be more interpretable than this WBN because it simply uses linear operation? I'd like to see the reaction of the authors to this questions\n\nFor instance:\n\"This differs from the normal neural network because the WBN reveals the exact functions with the correct inputs and their ordering to construct the target function, instead of merely approximating them.\"\nIf the target function is precisely a cascade of linear operators and ReLU, then the objective of learning would be to recover exactly the linear operators and wouldn't consist in an approximation. Are the authors trying to tackle the nature of the objective functions? It is very unclear to me.\n\n\"It is different from other neural networks as it not only approximates a target function, but also constructs and reveals its structure.\"\nI do not understand why the structure is less opaque than in standard CNNs or how it is revealed.\n\nThe authors must clearly define the notion of interpretability in the text, in an explicit and simple manner. As an active researcher in this field, I believe that this is quite difficult because everybody has its own interpretation of interpretability. Here, I would suggest to significantly rephrase this.\n\n- I am quite confused in the notation.. For instance, sometimes \\hat W is used, sometimes W...\n\n- It is claimed that $\\ell^1$ minimisation will allow to avoid... sparse and sharp operators:\u2028\"This causes W\u02c6 (l) to be extremely sparse and sharp, and it can be an obstacle for shifting the function blocks from one ordering to another.\" This goes against my intuitions/knowledge, could the authors point me to a reference?\n\n- Table 1: Do the mean square errors indicate an exact learning? If yes, this should be commented. Also, the Table is not discussed in the text...\n\n- I do not understand why the CIFAR and MNIST experiments are relevant to this paper. Furthermore, the accuracy are very low."}