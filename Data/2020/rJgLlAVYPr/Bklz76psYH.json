{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents White Box Network (WBN), which allows for composing function blocks from a given set of functions to construct a target function. The main idea is to introduce a selection layer that only selects one element of the previous layer as an input to a function block. This allows for both introducing function priors as well as interpreting the learned function. The paper also presents a setting where each function block is a neural network that can be learned end-to-end using a PathNet style setting and shows positive transfer across MNIST and CIFAR classification tasks.\n\nThis presents an interesting technique to enforce learning composition of function blocks while learning the target function. This is important for both interpretability of the learned function as well as for introducing prior information about useful functions in a given domain. The extension to learnable functions (in the form of neural networks) for learning pathways for different tasks is also promising.\n\nThe biggest weakness of the paper is that it does not compare both theoretically as well as empirically with several closely related techniques such as RoutingNetworks[1], Modular Networks[2], Neural RAM[3], Compositional Recursive Learner[4], etc. (please find the references below). Routing Networks allow for selecting among a set of function blocks given some inputs. Module Networks similarly introduce modular layer that determines the appropriate modules given the inputs from the previous layers. Neural RAM learns to compose differentiable functions to learn a target function (similar to learning the LLD programs).\n\nIt would be good to describe the differences between the proposed approach in WBN and these approaches, as they all seem to propose a similar solution of learning target functions by learning to compose function blocks and reusing the learnt computations for transfer learning. It would also be important to empirically evaluate the related approaches to better understand the pros/cons of WBN compared to these approaches.\n\nWhat is the biggest size LLD programs that can be learned by WBNs? It would be interesting to evaluate the scalability of the approach to understand the limits of learning complex target functions.\n\nI was also curious if instead of providing the four pre-defined function block (Identity, not, and, or), what would the behavior be if they were all neural networks and also learnt in an end-to-end fashion somewhat similar to [4].\n\nFor the MNIST and CIFAR classification tasks, the function blocks are neural networks themselves. After training them using the PathNet like training, is the learned network more interpretable? It might be interesting to see if the selection layer and the function blocks learned some semantic information that might be easier to distill.\n\nFor a better comparison with Neural RAM, it might also be interesting to empirically evaluate the performance of WBNs on algorithm induction tasks such as the one used in [3].\n\n[1] Clemens Rosenbaum, Tim Klinger, Matthew Riemer. Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning. ICLR 2018\n\n[2] Louis Kirsch, Julius Kunze, David Barber. Modular Networks: Learning to Decompose Neural Computation. NeurIPS 2018\n\n[3] Karol Kurach, Marcin Andrychowicz, Ilya Sutskever. Neural Random-Access Machines. ICLR 2016\n\n[4] Michael Chang, Abhishek Gupta, Sergey Levine, Thomas Griffiths. Automatically Composing Representation Transformations as a Means for Generalization. ICLR 2019\n\nMinor:\n\npage1: ?. -> ?\npage 2: questions and answering --> question answering\npage 3: y^(l) represents (m(l)+n(l))-dimensional vectors --> should this be m(l) + 2*n(l)?"}