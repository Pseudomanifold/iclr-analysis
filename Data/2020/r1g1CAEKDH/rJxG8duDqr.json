{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for learning latent-variable models of paired data that decomposes the latent representation into common and local representations. The approach is motivated by Wyner\u2019s common information, and is made tractable through a variational formulation. On experiments with MoG, MNIST, SVHN, and CelebA, the paper shows that penalizing the complexity of the common representation can improve style-content disentanglement and conditional sampling.\n\nWhile I found the formulation and motivation from Wyner\u2019s common information and Cuff\u2019s channel synthesis interesting, the resulting model and experiments were unconvincing. There are many terms in the Wyner model proposed, and there are no ablations demonstrating which terms are important and which are not (e.g. do you need both L_xx and L_xy?). On the toy MoG experiment, the common information is known but trained models do not recover the right amount of information. For representation learning, accuracy decreases as the penalty on the common information increases, and for NLL, the joint and conditional NLL is often similar to existing work (CVAE and JVAE). The main win appears to be for style-content disentanglement, but the results there are qualitative and often change the content when only style is changed. It\u2019s also puzzling as to why CVAE overfits so severely in a subset of the experiments. Without a more thorough evaluation of what terms in the loss matter, and showing that the technique recovers something like Wyner\u2019s common information (by extracting the right information on a toy model), I cannot recommend this paper for acceptance.\n\nMinor comments:\n* Eqn 1: the constraint \u201cSubject to X-Z-Y\u201d is confusing. Do you mean X-Z-Y is a Markov network (undirected) and not a Markov chain (directed), in which case X -> Z <- Y does not correspond to X-Z-Y? (This is addressed in Eqn 3-4, but should be fixed here)\n* How is Wyner\u2019s common information related to the multivaraite mutual information I(X; Z; Y)?\n* When describing distributed simulation, please include the variables and objective like you do for common information\n* s/Markovity condition/independence assumption?\n* Why are both losses in Eqn 4 and Eqn 5 needed? Couldn\u2019t you use either to train q(z|x)?\n* Eqn 10 (and most of your objectives) are still intractable due to the q(x)/q(x,y) terms, you should note that it is constant and dropped from the objective\n* \u201cStyle control\u201d: could you define what you mean by style in this context? Prior work could likely also do \u201cstyle control\u201d by e.g. interpolating subsets of dimensions.\n* Related work: https://openreview.net/forum?id=rkVOXhAqY7 (CEB) that may result in a similar objective as Wyner\u2019s common info\n* When comparing to JVAE/JMVAE, it seems like the main difference is suing a latent-variable in the decoder, but the framework is still the same. It\u2019d be useful to spend more time comparing/contrasting with this prior work.\n* Fig 4: would be useful to have a picture of the samples (right now they\u2019re just in appendix)\n* Fig 4: in the data generating process, I believe I((X, Y); Z) = ln(5) = 1.6 nats, but none of your models converge to rates around there. Why not?\n* Unlike other approaches (JVAE, CVAE) you have additional terms in your loss for conditional prediction at training time. It seems like these may be giving you gains, and it\u2019d be useful to perform ablations over the terms in Eqn 15 to figure out what is impacting performance in Fig 4. E.g. if you set \\alpha_{x -> y} and \\alpha{y -> x} to 0\n* Why do the CVAE models overfit? Have you tried optimizing the variational parameters on the test set if it\u2019s due to amortization gap?\n* Fig 5b: what\u2019s the variance you\u2019re plotting? In representation learning, we often just care about downstream accuracy, and it looks like \\lam = 0  performs best there.\n* Table 3: isn\u2019t this showing that \\lam = 0 works the best for joint NLL and close to best for conditional NLL on the MoG task? What\u2019s the discrepancy with Fig 5 where conditional NLL is highly dependent on \\lambda for MNIST?\n* Fig 6 (a1-f1): for this MNIST add 1 dataset, the only common info should be the label. But it looks like the labels do change for non-zero values of lambda (b1 there\u2019s a 2 -> 7), c1 1 -> 9)\n* Table 4: would be useful to train CVAE yourself, as the small differences in numbers could just be due to tuning / experimental setup. You also should bold everything that\u2019s within stderr (i.e. \\lambda = 0 and \\lambda = 0.15 are equally good)\n* Fig 9: would be useful to include CVAE, and Wyner VAE in this plot, i.e. is it only using the mode information in the latent variable when doing conditional sampling?\n* CelebA results look interesting, but there\u2019s been other work on generation from attributes that presents more visually compelling results, e.g. https://arxiv.org/abs/1706.00409"}