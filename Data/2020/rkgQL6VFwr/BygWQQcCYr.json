{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a model that takes an image and a sentence as input, where the sentence is an instruction to manipulate objects in the scene, and outputs another image which shows the scene after manipulation. The model is an integration of CNN, RNN, Relation Nets, and GAN. The results are mostly on synthetic data, though the authors also included some results on real images toward the end.\n\nThis paper, despite studying an interesting problem, is limited in terms of its technical innovations and experimental results. My recommendation is a clear reject.\n\nThe model is simply an integration of multiple standard neural nets. To me, it's unclear how the system can inspire future research. Such an integration of neural nets won't generalize well. The authors have to pre-process real images in a very specific way for limited sim-to-real transfer. It's unclear how the model can work on more complex images, nor to mention scenes or sentences (or actions) beyond those available during training. \n\nThe experimental setup is very simple. The model is tested on scenes with a clean background and a few geometric primitives. There are only four actions involved. There are no comparisons with published, SOTA methods. All experiments are with the ablated model itself. Considering all this, I believe this paper cannot be accepted to a top conference such as ICLR.\n\n"}