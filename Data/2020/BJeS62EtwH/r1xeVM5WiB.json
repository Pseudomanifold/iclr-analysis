{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper presents a method to disentangle intermediate features between two different deep neural networks. More specifically, given two networks, the proposed approach aims to find consistent and inconsistent feature components for a certain layer in each network. If one network is more powerful to the other (e.g., ResNet and AlexNet), the method can figure out which components are weak or strong (i.e., helpful to the performance) for the given task. The authors design a simple yet effective algorithm for extracting knowledge consistency. In addition, they provide a variety of practical experiments including network diagnosis, feature refinement, and network compression. Most of the experimental results support that the proposed method can practically extract consistent feature components with promising improvements.\n\nMain concerns:\n\n1. In section 4.1, the authors provide both the unreliable features and blind spots with their visualization (Figure 3). It is expected that blind spots are more informative than the unreliable features since they are part of a more powerful DNN. However, in Figure 3, the Blind spots do not seem to be important features. This makes the lack of the motivation of visualizing the effectiveness of features. Could you provide more promising visualizing results?\n\n2. In equation (1), it seems that the scaling factor p^(k+1) can be unnecessary because ReLU is scale-invariant and \\Sigma_(k+1) can scale the feature h^(k+1) with p_(k+1). Does p_(k+1) significantly improve the performance or negligible?\n\n3. In Table 3, using x^{(0)} to the refined feature shows the best result under VGG-16. Are the result in Table 4 improved when changing the refined features? E.g., only x^{(0)} or x^{(0)} + x^{(1)}?\n\nIn short, the proposed disentangle method is simple, novel and very useful in many practical applications as provided in the experimental section. But, some experiments and intuitions of network construction are not fully promising. Thus, I vote for weak acceptance.\n\nMinor concerns:\n\n1. Does the proposed method show meaningful results when the tasks are different but the datasets are the same? Or the same task but different datasets.\n\n2. The disentanglement network can be constructed with other operations, e.g., h^{(k)} = W^{(k)} concat(x , x^{higher}). Did the author try to design the network with other operations?\n\n3. In the fourth paragraph on the first page, the reference of \u201cWolchover (2017)\u201d need to change \u201c(Wolchover, 2017)\u201d.\n\n4. The proposed method can be applied in other tasks, not with image datasets. Is the algorithm promising for other tasks?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}