{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a method to approximate the \"fairness-accuracy landscape\" of classifiers based on neural networks.\nThe key idea is to set up a multi-dimensional objective, where one dimension is about prediction accuracy and another \nabout fairness. The fairness component relies on a  definition of fairness based on causal inference, relying on the \nidea that a sensitive attribute should not causally affect model predictions.\n\nI found the causal idea intriguing, since it makes sense that we don't want a sensitive attribute to have a causal effect.\nHowever, there may be several problems with this approach:\n\n1) For a causal estimate to be valid we need several assumptions. For example, we need A (the sensitive attribute) \nto be independent of potential outcomes conditional on X --- the so-called \"unconfoundedness assumption\" in causal inference.\nWe also need \"positivity\", i.e., that 0< P(A=1|X) <1. \nThese assumptions are not discussed in the paper. Furthermore, the particular context of the paper, where the treatment is actually an immutable characteristic, makes such discussion much more subtle. \nWhat will we do, for instance, if there are no A=1 in the sample when X = ...?\n\n\n2) The authors seem to assume that the propensity score model is well specified. This can be tested, e.g., using [1].\nWhat do we do when this fails? \n\n\n3) Why do we want U to be small, i.e., why do we want the causal effect of A to be small, is never justified.\nIn particular, its relation to \"fairness\" is never fleshed out, but just assumed to be so.\nThis can be problematic when, say, we are missing certain important X that are important for A. \nThen, there will be a measurable causal effect of A on h(). \n\nSome other problems:\n- What is the reason for focusing on 'neural classifiers'? There is nothing specific in the method or analysis \nthat relates to neural networks, except for the use of the causal estimand in a 'hidden layer'.\n\n- In the Introduction, the authors could cite the works of Amartya Sea, etc., on fairness. \nCertainly the study of fairness problems did not start in 2016.\n\n- What exactly is a \"sensitive attribute\"? If we don't want to bias our predictions, then why include it in the analysis?\n\n- It is unclear what is new and what is related work in page 3.\n\n- Sec. 4: The claim that \"causal inference is about situations where manipulation is impossible\" discards \nvoluminous work in causal inference through randomized experiments. In fact, many scientists would \nagree that causal inference is impossible without manipulation.\n\n- As mentioned above, why this particular estimand leads to more \"fairness\" is never explained.\n\n- Do we need a square or abs value in Eq (5)?\n\n- The experimental section is weak and does not illustrate the claims well. \n   It would be important to explain the choice of the particular causal estimand, the choice of the hidden layer to put the estimand in, to explore the choice of the objective, and so on. Currently, none of these choices/design aspects are being investigated.\n\n\n\n[1] \"A specification test for the propensity score using its distribution conditional\non participation\" (Shaikh et al, 2009)"}