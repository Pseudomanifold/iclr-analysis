{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work all starts with the observation, made by many, that deeper (>=2 layers) networks generalise differently to the simple models we used to use, and that the good old bias-variance trade-off doesn't really apply.\n\nThis paper is a natural successor to a long line of papers, most recently Zhang at el (best paper at ICLR 2017, though some like Tom Dietterich weren't impressed), Neyshabur et al (NIPS 2017) and Jiang et al (ICLR 2019).  Whereas Jiang etal and Neyshabur etal had a few metrics, a few data sets and a few models, this paper instead has loads and loads of metrics and just one data set and one model class.  You have, however, been clearly strongly influenced by the prior work and attempted to fix what you saw as their empirical shortcomings.\n\nI agree with the choice of stopping according to cross entropy.  One thing not done in Section 5 is a discussion of\nvariation:  choice of initialisation and the impact of stochasticity in the optimisation. \n\nAppendix C lists your copious metrics.  I am not deeply embedded in the theory, though I understand the major concepts. I was impressed with the coverage and variations done.  I expect Appendix C may raise copius matching arguments amongst the theory community, with some valid and some invalid complaints.  For me, I wonder why you make P & Q have the same variance (see before equation 43 in App. C).  You credit Neyshabur, but I'm left wondering.\n\nSo, the short version of my review is that I believe this paper is imcomplete, in that not enough different data sets were investigated.  With this, I fear you could well be uncovering perculiarities pertinant to CIFAR-10 and the ease of getting near perfect training error on it.  However, I am impressed with what you have done, and think you made a great starting point.  So I say, publish and let the real battle begin.  Let the theoreticians argue (about metrics) and the practicioners implement (with more data), and let's see what happens.  You have laid a great ground work for folks, bulding on the earlier work.  Perhaps Google can be encouraged to support this in that Jiang's paper is a precursor.\n\nInteresting observations in Appendix A.1, as are some of the discussions in Sections 7, 8, 9.\n\nGiven the best metric seems to be sharpness, which exceeds PAC-Bayesian, shouldn't you relate this to grad noise (metrics 61 and 62) which measure the flatness.  Are these related?  Note, also, a Bayesian always wants broad peaks for their parameter surfaces.\n\nMINOR ISSUES: (1) some repeated words: \"an an\", \"the the\" (2) some strange grammar \"each canonical ordering that only be predictive for its own category\", \"VC-dimension bounds or and parameter counting\", \"possible to find to calculate\", \"in the Equation equation C.3 (3)\" Generalisation gap is poorly introduced.  Its officially defined in footnote 4 but used way earlier!  Put in the main text:  its an important clarification. (4) Table 1 legend should mention the \"numerical results\" are for equations 2, 4 and 5.  (5)  Somewhere you need to mention that numbers in red in tables refer to equations in appendix C.\n"}