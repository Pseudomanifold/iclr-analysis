{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes SAVE that combines Q learning with MCTS. In particular, the estimated Q values are used as a prior in the selection and backup phase of MCTS, while the Q values estimated during MCTS are later used, together with the real experience, to train the Q function. The authors made several modifications to \u2018standard\u2019 setting in both Q learning and MCTS. Experimental results are provided to show that SAVE outperforms generic UCT, PUCT, and Q learning.\n\nOverall the paper is easy to follow. The idea of the paper is interesting in the sense that it tries to leverage the computation spent during search as much as possible to help the learning. I am not an expert in the of hybrid approach, so I can not make confident judgement on the novelty of the paper.\n\n The only concern I have is that the significance of the result in the paper:\n1. The proposed method, including the modifications to MCTS and Q learning (section 3.2 and 3.3), is still a bit ad-hoc. The paper has not really justified why the proposed modification is a better choice except a final experimental result. Some hypotheses are made to explain the experimental results. But the authors have not verified those hypotheses. Just to list a few here: (a). The argument made in section 2.2 about count based prior; (b). the statement of noisy Q_MCTS to support the worse performance of L2 loss in section 4.2; (c). In the last paragraph of section 4.3, why would a model free agent with more episodes results in worse performance?\n2. The baselines used in this paper are only PUCT and a generic Q learning. What are the performances of other methods that are mentioned in section 2.1, like Gu 2016, Azizzadenesheli 2018, Bapst 2019? \n\nOther comments:\n1. What is the performance of tabular Q-learning in Figure 2 (a-c)?\n"}