{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper studies the loss landscape of two-layer neural networks on global- and self-attention models.  It shows that attention helps reduce sample complexity.  I went through the all the theorem part of the paper, but only checked the intuition and did not dig into the detailed proof.   I am less familiar with reviewing papers that are theorem-oriented, which is hard for me to justify the contributions of these bounds.  My concerns and suggestions are mainly focused on the empirical part, which I think authors need to improve.  \n\nFirst, the authors need to provide more detail about the baseline model, especially for the experiment on 5.1.1.  Also, please list the number of parameters for different models.  And please justify whether the reported results (sample complexity in table 1 & 2) are affected by the difference of number parameters if they are not the same.  The authors also need to clarify the meaning of the \"number of training samples\" in the tables.  Does it mean the number of unique training examples seen by the model or the total number of training samples?  If the authors mean the former one, then, what is the training procedure?  What are optimizers used between different models?  Are these will affect the test loss difference.  How does the author make sure the comparisons are fair?  Also, in the main text of section 5.1.1, the authors said they construct a dataset of 200,000 examples.  How are the 200,000 samples distributed?  What is the size of the testing set?   Do the reported results average over multiple runs? Similarly, in section 5.1.2 authors need to clarify these questions and the experiments are fair to support their claim.  \n\nGiven the theoretical part is a contribution, the experiments at the current version do not support their claim fully.  I will consider raising my scores if authors can clarify my questions.  "}