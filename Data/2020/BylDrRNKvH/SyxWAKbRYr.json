{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies attention and self-attention networks from the theoretical perspective, giving the first (as far as this reviewer knows) results proving that attention networks can generalize better than non-attention baselines. This has been observed empirically before and it is very good to start the analysis of foundations of this phenomenon.\n\nThe results cover fixed-attention (as we understand both single-layer and multi-layer) and self-attention in the single layer setting. One interesting part that is not covered (and may be very hard) though is multi-layer self-attention networks. What is the equivalent of condition (A9) there? In other words: can we prove that the attention will learn correct attention masks if they exist, or do we need to assume that?\n\nOn the experimental side, the paper introduces L1 loss on the attention mask. This is (to the knowledge of this reviewer) a new idea and it would be interesting to see larger models (e.g., a Transformer on a translation task) trained with this additional loss. Does the analysis suggest L1 loss in any way, more than say L2?"}