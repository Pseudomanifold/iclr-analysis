{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents an approach for learning density estimates of a data distribution convolved with noise through \u201cdenoising density estimators\u201d and shows how to leverage these density estimates to train generative samplers. The methods are evaluated on toy low-d datasets, MNIST, and fashion MNIST where they show reasonable density estimates and OK sample quality. \n\nMy major concern with this paper is that the \u201cdenoising density estimator\u201d proposed here is identical to denoising score matching (which is not cited or discussed). The second contribution of learning a sampler given a density estimate is interesting but likely suffers from all the instabilities of GAN training, and does not compare to related work on distilling energy-based models. Unless both these concerns are addressed, I cannot recommend this paper for acceptance.\n\nMajor comments:\n* The idea of learning a generative model whose energy gradient (score) matches the gradient of the data distribution has been explored extensively under the name \u201cscore matching\u201d. The proposed approach of \u201cDeep Denoising Density Estimation\u201d is *identical* to denoising score matching (Vincent et al., 2011, http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf). There\u2019s no discussion of any prior work on score matching in this paper, or comparison with recent approaches in this space (e.g. sliced score matching https://arxiv.org/abs/1905.07088, and https://arxiv.org/abs/1907.05600 that uses denoising score matching).\n* The algorithm presented for learning a generator given an energy function is not compared to other implicit sampling approaches like MCMC. Additionally, the algorithm requires alternating density estimation with updating the generative model, which is quite similar to GAN-training alternating density ratio estimation with updating the generative model. Thus the proposed algorithm likely experiences similar instabilities and challenges in how to partially solve the density estimation step. There are no comparisons to any GAN-based approaches anywhere in the paper. That said, mixing DSMs and explicit generators is a neat area of research.\n* The experiments are too limited, and do not compute any quantitative metrics on the image datasets.\n\nMinor comments:\n* \u201cDefining property of PGMs is that they provide functionality to sample\u201d ->  what about density estimates? Likelihood ratios? etc.\n* Boltzmann machines don\u2019t allow for efficient inference\n* Intro could should spend more time discussing relation to energy-based models, score matching, noise-contrastive estimation\n* Eqn 2 only holds for sigma^2 -> 0, please add and discuss this limitation\n* Divergenc -> divergence, above eqn 12\n* I found the math in section 4 very confusing. What\u2019s the <> notation in this context? Do you need to introduce \\Delta? Alternating density (ratio) estimation and generative model updates is super common in GAN literature and is not discussed here.\n* Table 1: are the estimates of log-likelihood upper or lower bounds? Unlike other approaches, you don\u2019t have exact likelihoods so I\u2019m not sure your #s are comparable.\n* 2048 samples per iteration -> batch size?\n* \u201cFor faster convergence, we take 10 DDE gradient steps\u2026\u201d  -> please add an experiment showing how results are impacted by # of gradient descent steps. Is larger # steps always better?\n* MNIST/Fashion MNIST samples aren\u2019t that good to my eye and there are no quantitative metrics (e.g. KID, FID, IS)"}