{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a method for density estimation in high-dimensional domains. The proposed algorithm combines ideas from denoising autoencoders The noise estimator procedure developed in this paper estimates the logarithm of the unnormalized density. The inference procedure is stochastic gradient descent w.r.t. the neural net parameters that minimize the KL divergence between the Gaussian smoothed version of the generator and the Gaussian smoothed version of the data.  Alternatively, seen the procedure tries to imitate a kernel density estimator using a parametrized network. Experimental results are demonstrated on a toy 2d datasets and a few real datasets. I like the ideas of the paper and the exposition in this paper.  Here are a few comments and questions\n\n1. My only concern is how scalable is this procedure to high-dimensional data. The very fact that the estimator is trying to imitate a kernel density estimator implies that the assumptions made on the underlying data are that the data comes from a smooth distribution. Modulo this assumption, non-parametric estimators do not make any further assumptions on the data and therefore such non-parametric estimators are only useful for low-dimensional data. So, it is not clear to me if the estimator proposed in this paper can scale to structured high-dimensional data distributions.  \n\n2.  Can you estimate the log-likelihood for the 2d datasets in Figure 1?"}