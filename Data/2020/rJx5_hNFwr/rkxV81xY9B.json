{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, a domain adaptive object detection model is proposed. It mainly includes two additional components : 1) stacked complementary loss, which includes multiple adversarial losses on intermediate layers, and a instance-context alignment loss at the final layer; 2) a gradient detach strategy to stop the backpropagation to  intermediate layers through the context features. Experiments are conducted on several benchmarks to valid their claims.\n\nThe paper is well written, and easy to follow. It shows good performance on a number of benchmark datasets. Overall, this is an incremental work. While the tricks discussed in the paper sounds reasonable, it provides very limited insights into the domain adaptive object detection problem,\n1) complementary loss: actually, applying adversarial training on multiple levels have been discussed in the literature. For example, in the CAN model [Zhang et al., 2018], they append domain discriminators on different layers, and automatically learn if GRL layer should be activated or not. In DA-Faster-RCNN and SWDA, the authors used two levels of adversarial training. And in the recent MAF work [He and Zhang 2019], they also applied adversarial trainings on multiple levels. From those works, it can be seen that applying multiple adversarial trainings help to improve the adaptation, and carefully tuning the strengths of adversarial trainings would gain further improvements. For example, in CAN, they found the lower layers favors non-adversarial trainings or smaller weights for adversarial loss. Similarly, in SWDA, they observed that a weak alignment at higher layer with FL, and a strong alignment at lower layer with LS gives improvement. This paper also discussed different configurations for loss types on different layers, however, the designs basically followed the above existing works, and the observations are also similar.  \n[Zhang et al. 2018] Zhang et al. Collaborative and Adversarial Network for Unsupervised Domain Adaptation. In CVPR  2018. \n[He and Zhang 2019] He and Zhang. Multi-adversarial Faster-RCNN for Unrestricted Object Detection. In ICCV 2019. \n2) context loss has also been discussed in SWDA, where they augmented the features from discriminators to instance features for a more stable training. In this paper, they used a different way to use the features from different conv-layers, however, this looks quite arbitrary. It is desirable to give more explanations on what motivates authors, and discussion and comparison to SWDA\u2019s strategy are necessary.   \n3) gradient detech has also been exploited. In SWDA, the gradient w.r.t. context features are also detached in the backpropagation for a stable training. However, neither SWDA nor this work present a convincing explanation on why gradient detach should help the adaptation. I noted that authors provided discussions in Section 4, but those observations are trivial. Could you explain how those gradients would demagage the model if no gradient detach is applied, and why? \n\nFurther questions: \n1) Moreover, the experimental results are also inconsistent in some cases. For example, \nIn table 1, ILoss(FL)+detach does not always give improvements. In the first group,  LS+LS+FL+X+X gives 35.0, while LS+LS+FL+FL+detach gives 34.9; in the second group, LS+FL+FL+X+X gives 35.5, while LS+FL+FL+FL+detach gives 35.5; in the last group,  LS+CE+FL+X+X gives 35.3, while LS+CE+FL+FL+detach gives 37.9. It seems the proposed ILoss+detach is quite tricky technique, it works only when they use LS+CE+FL, but performs in an opposite way for other settings. This means people should carefully design their models for using this trick. There are also a few cases of ablation studies missed in Table 1, for example, there is no corresponding \u201cw/o detach version\u201d of LS+FL+FL+FL+detach, making it difficult to valid if the detach generally helps.  I suggest the authors reorganize Table 1, and complete the missing cases, to verify their claims on complementary loss, context loss, and gradient detach separately and thoroughly. \n\n2) Similar ablation studies on other settings like PASCAL VOC->Clip Arts, Kitti->Cityscapes and INIT are needed. Cityscapes -> Foggy Cityscapes is a relatively speicial case, since the foggy images are synthesized images. \n\n3) There are a few works in CVPR 2019 and ICCV 2019, which provides state-of-the-art domain adaptation object detection performance. Those works should be included in experimental comparison, and carefully discussed. "}