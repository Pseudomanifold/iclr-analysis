{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a fine-tune technique to help BERT models to learn & capture form and content information on textual data (without any form of structural parsing needed). They key addition to the classic BERT model is the introduction of the R and S embeddings. R &S are supposed to learn the information in text that is traditionally represented as the structural positions and the content-bearing  symbols in those positions. \n\nIn order to effectively learn R and S embeddings, the authors propose two possible ways to do so: LSTM (Fig 2) and 1-layer Transformer (Fig 3). The main experiments are based on 1-layer transformer HUBERT b/c from a single test in Table 1, the transformer variant appears to be working better than the LSTM variant.\n\nMy main concern regarding this paper is two-fold: limited novelty and insignificant performance gain.\nThe authors did a great job motivating the need for separating role and filler in the intro. However, in neither implementation of HUBERT, I do not see how the structural information (e.g., a parse tree) is directly incorporated into the learning of HUBERT.\n\nRegarding the performance, it seems HUBERT is gaining very little over the BERT baseline. please refer to my specific question below.\n\n\nQuestions:\nWhat are the numeric values for d_S, d_R, n_S, n_R (defined under Section 3 on page 2) in experiment ? I think d_S, d_R are determined at author's discretion (just like the dimensionality of, say, the LSTM hidden layer). But how are n_S and n_R determined?\n\nPage 7, first paragraph: what is Filler embeddings F? F is not defined in either version the proposed HUBERT( Figure 2 or Figure 3). Did the authors mean S?\n\nTable 2. Why do the first 5 rows and the bottom 5 rows have different baseline Acc. ? Shouldn't we always use the best accuracy as baseline for comparison? If we look at the HUBERT Fine-tuned Acc., in many cases, they are actually worse than the best baseline acc. available. (i.e., QNLI , QQP, and SST).\n\nOther comments:\nTypo on page one: \u201c[] To strengthen the generality of \u2026.\u201d\nFigure 1 is never referred in main text.\n"}