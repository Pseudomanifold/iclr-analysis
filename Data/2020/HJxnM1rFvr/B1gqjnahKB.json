{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a layer on top of BERT which is motivated by a desire to disentangle content (meaning of the tokens) and form (structural roles of the tokens).  Figure 1 shows this clearly. The paper considers two variants of the disentangling layer (TPR), one with LSTMs (figure 2) and the other with attention (figure 3). The aim in both is to obtain a decomposition of the form x(t) = S a_s(v_t) a_r(v_t) R where S and R are shared matrices of parameters and v is the output of BERT. \n\nThe model is well motivated and includes clear reasonable design ideas, including choosing hyper-parameters so that the number of symbols (s) is greater than the number of roles (r), and forcing only the roles to be independent (eqn 6). \n\nMinor: I would have preferred that figure 1 appeared earlier in page 3. This would help as the authors forgot to define v in eqn 2. One has to wait for the figure. Having said this, the paper is extremely clear in the notation and does an excellent job at defining dimensions for all the quantities of interest.\n\nI read the paper eagerly and with excitement until I got to the results. First, it wasn't clear to me how well motivated is the idea of fine-tuning on intermediate tasks. I understand the authors are just trying to make a point that BERT does worse than their model in this case and that this is not good for transfer, but still I find this to be artificially constructed.\n \n The variations in the numbers seem small and possibly attributable to other factors. For this reason, I feel the authors should have continued showing results for the other baselines from the first experiment. I would also have loved to see some visualizations for a, r, A and R in the appendix. Some visualization and anecdotal results might have helped me see that the motivation is backed up by the results. I hope the authors have the time to do this and consider the extra experiments."}