{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a scheme for training layered feedforward neural networks with backwards accumulation of gradients. In the proposed scheme, the intermediate activations during a forward pass are constrained, using an L2 norm penalty, to be close to the activations of a model that inverts the operations of each layer and transforms the data in reverese order (from output to input). The second, inverse, network shares all parameters with the feedforward network.\n\nThe paper hypothesizes that such training, which they call racecar training, results in features that are transferable between tasks; i.e. using racecar training to learn the features, then applying regular training on a novel task. To support this hypothesis, the paper provides an empirical analysis of the mutual information between inputs and intermediate features, and between intermediate features and outputs (as proposed in the information bottleneck literature). Under this analysis, the paper shows that using racecar training the intermediate layers contain less information about the outputs than standard feedforward training. The paper states that this makes the features learned by racecar training better for transfer: features that enable the network to achieve high predictive accuracy on a particular tasks but that carry little information about the output distribution, thus less specialized. The paper continues this analysis to the transfer setting, first to the same initial task, and then the a new task. In both cases one of the variants of the proposed method appears to produce higher accuracy than standard pre-training.\n\nI'm inclining to reject this paper given that the results on the main hypothesis (i.e. transferability of features) seem to provide only marginal improvement, and we have no idea about the repeatability of the results ( how many times did the authors run the experiments for figures 3,4,6,7,9? What's the spread of the results? Are these results significant?). The results on style transfer and super resolution are promising, but these are only illustrative examples: these results do not provide insights on how well the method works in general, or when does it fail.\n\n\n\n"}