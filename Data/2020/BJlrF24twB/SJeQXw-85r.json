{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a Pytorch framework for experimenting with first and second order extensions to standard gradient updates via backpropagation.  At the time of writing, the implementation supports feed-forward networks where there is a strict module ordering (by which I mean residual connections are not possible).\n\nThe framework enables researchers to easily experiment with techniques which involve modifying the update according to quantities computed over the batch of gradients (i.e. before they are summed to form the standard SGD update)\u2014these are \u2018first-order\u2019 extensions\u2014and it also makes use of the block-diagonal factorisation of the Hessian outlined in Mizutani & Dreyfus as well as Dangel & Hennig to enable the computation of second order quantities via ~ \u2018hessian prop\u2019.\n\nI think the paper is a strong accept: the framework has some limitations in the current form (mostly in terms of what architectures are supported), however it still provides a very useful and extensible tool for researchers to efficiently experiment with a variety of more complex optimisation architectures.  This is (as the paper states) a large bottleneck for much optimisation research in deep learning.\n\nIn section 2.3 you state that generalised Gauss-Newton (GGN) is guaranteed positive semi-definite.  It would also be nice to add a sentence as to when (even intuitively) the Fisher information coincides with GGN; (in practice, as the GGN uses a (possibly rank-bounded) sample size of \u2018N\u2019, while the Fisher is the expectation under the data generating distribution, one could argue that even when they should be ==, it would only be as N->\\infty).\n\n\n\n"}