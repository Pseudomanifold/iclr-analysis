{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates wider networks using a recent feature visualization technique named activation atlases. By analyzing what the hidden layers of wider networks respond to, the authors showed that wider networks learn more transferable features. However, I tend to reject this paper since it doesn\u2019t show very compelling evidence through experiments.\n\n1. This paper does not present any novel methods, and so the experiments need to be very solid. But all the datasets and architectures used in this paper are quite simple from the view of deep learning. It is not clear whether the conclusions will still be valid for larger datasets or deeper networks. \n\n2. The most important observation of this paper is to find that wider networks can be easily transferred to a new task. But in Section 5.1, all layers except the last classification layer are fixed when fine-tuning the networks for the second task. It is so obvious that wider networks with fewer previous layers can perform better. In Section 5.2, the authors did not show the network details, and also it is not fair to compare the networks with a linear classifier. The authors should include more competitive baselines.\n\n3. I encourage the authors to show the training/validation curves to testify the data efficiency of the wider networks when training or fine-tuning on a new task."}