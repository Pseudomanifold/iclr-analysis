{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "PRIVACY-PRESERVING REPRESENTATION LEARNING BY DISENTANGLEMENT\n\nSummary\nThis paper introduces a method to disentanglement the private and public attribute information in representation learning.\n\nStrength:\n1. The idea of introducing the confusion term to disentanglement private and public information seems novel. \n2. The problem studied in this paper is very important.\n\nComments:\n1. The existing results are not sufficient to validate the effectiveness of the method to prevent privacy leakage. More comparison with other previous methods should be conducted. For example, the previous work [1] has both theoretically and experimentally validated the effectiveness of DP based methods for preventing attribute attack. Recent work[2] also tries to reduce information leakage in representation learning.\n2. Some important related works are missing. The difference between the proposed method and previous works with the same purpose should be made more clearly. See comment1 for some concrete examples of previous works. \n3. The notation of $z$ is confusing. The author denotes $z=(z_{*}, z_{.})$, however, these three vectors' dimensions are the same, which really confuses me.\n4. Some details of $L_{VAE}$ are missing. According to Equation~1., the author uses $z$ to build the reconstruct loss. However, based on previous notations, $z$ comprises of both private and public representations. So, what's the strategy to combine these two representations (concat?)?\n5. Typos. For example, blue-> read in the caption of Figure 1. \n6. The threat model in this paper needs to be made more clearly. The method proposed in this paper can only be used in the context where the private attribute is well-defined. There are many other threat models in secure machine learning research, such as membership attack and adversarial attack, which are not covered by this paper (My suggestion is adding a specific sub-section of threat model and do not use  \"privacy-preserving\" in the original title ). \n[1] Privacy Risk in Machine Learning: Analyzing the Connection to Over\ufb01tting\n[2] Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach"}