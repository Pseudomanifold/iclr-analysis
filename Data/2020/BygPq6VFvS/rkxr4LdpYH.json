{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work aims to incorporate phrase representation into attention mechanism. The proposed method is straightforward (which is good): a convolution window with size n is used to calculate representation for an n-gram, which then replaces the token representation in a standard attention model. The paper implements the multihead version of the proposed phrase-based attention, more specifically, in a transformer model. Experiments with machine translation and language modeling show that it outperforms the token attention counterpart.\n\nMy main concerns are about the experiments: \n\n- For the translation experiments, the transformer numbers are lower than those reported by Vaswani et al. (2017) across the board, for both the \"base\" and the \"big\" settings. I didn't find convincing reason for this. Could the authors comment on this, and also on why they do not directly compare against Vaswani et al. (2017). The same for the language modeling experiments.\n\n- Table 3. To the best of my knowledge, neither Vaswani et al. (2017) or Shaw et al. (2018) report language modeling results. I'm guessing the authors use their implementation and establish their own baselines, which is okay. But please indicate this instead of putting a citation in the table, which can be misleading.\n\n- Minor: the start of Section 3 reads like source code and comment, and is a bit hard for me to follow.\n\nI do not recommend the that the paper is accepted, until the authors address my concerns on the baselines. \n\n\nMissing references:\nPaLM: A Hybrid Parser and Language Model. https://arxiv.org/abs/1909.02134"}