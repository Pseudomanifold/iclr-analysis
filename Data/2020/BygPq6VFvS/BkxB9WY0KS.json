{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes an attention mechanism that directly reflects the phrasal correspondence by employing convolution. The n-gram aware attention is incorporated into Transformer and shows gains in translation and language modeling tasks.\n\nIt is a novel and straightforward way to incorporate phrasal relation in the attention mechanism. The gains reported in this paper are meaningful, thought might not be SOTA. The analysis on attention is also interesting in that the phrasal relation is employed in lower layers, but not in the higher layer.\n\nOther comment:\n\n- It is not clear to me how the n-gram aware phrasal attention is incorporated into multi-headed attention described in section 3.2. Did you completely remove the multi-head attention, but used only n-gram attentions? Or, did you keep multi-head attention and incorporated phrasal attention for each head?\n\n- It is unfortunate that this work does not report empirical results when applied to a big model configuration. Did you encounter OOM when running on a big model?"}