{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a new way of decomposing 3-order tensors which uses interpolation\nbetween the Tucker and CP decompositions, called CPT. The main idea is to present the components of the CP model\nwith an additional low-rank structure.\nThe authors also provide a new optimization algorithm called ADA-imp, for learning this decomposition,\nwhich is a variant of Adagrad adapted to their settings. \nThe paper is overall interesting, clearly written and well-motivated. \nThe mathematical derivations are, as far as I could follow, correct and non-trivial. \u00a0(I did not read all the details in the Appendix). \nThe authors also show favorable experimental results on two knowledge-base datasets, with improved loss vs. #parameter used tradeoff.\nA few unclear issues and suggestions for improvements are below.\n\nThe authors present the problem as completion of a binary 3-order tensor, i.e. predicting for triplets (subject, predicate, ?) if '?' refers to 0 or 1.\nBut they also write 'we formulate this problem as a multi-class classification problem, where the classes are the entities of the knowledge base' \u00a0- so this is not a binary problem? does this mean there is some structure that must be present in the tensor? (e.g. there is exactly one '1' in each column of length N? This should be clarified. \n\nIt would be good to make the description of Algorithms 1 and 2 more precise and detailed. \nFor example, the operation/algorithm AdaGrad(\\eta;w_k; g_k;G_k) is not defined. AdaGrad is described in the Appendix but it is hard to match it to get the precise operation used in Algorithm 1. \nAlgorithm 1 shows one step of PComplEx, and it would be good to add the entire PComplEx algorithm, with input,output&parameters. \n\nThe authors present their method in the context of knowledge base completion, thus for tensors of order 3, but it is not clear if any of the components they proposed indeed specialized for this problem, or is it a contribution to general tensor decomposition. Some remarks regarding the (in?)applicability of the method more generally would be helpful. \n\nFigure 3 describing the experimental results should be explained better. There are few methods shown only in some of the graphs and only for some parameter values - why?\nThe complexity measure 'parameters-per-entity' should be clearly defined (I didn't find it in the text). Similarly, the performance measures 'mean reciprocal rank' and 'hits at 5%' \nshould be defined in terms of the tensor. \nThe authors should also add running times of the different experiments and methods. \n\n\nMinor:\n--------\nIn the main paper, the authors define an (N,L,N) tensor, but in the appendix Section 9.9 they list N and P. Does P refer to L here? \n\nThe authors mention a few times usage of 'deep-learning techniques' - but I believe that in at least some of the contexts, they refer to optimization methods which are typically used in deep learning, and \u00a0are applied here to train other models presented in the text, and not to the usage of actual deep learning architectures - this is confusing and should be clarified. \n\nPage 7, top: what are the matrices M^(1), M^(2), M^(3)? they seem to be different for different decompositions \u00a0\n\n"}