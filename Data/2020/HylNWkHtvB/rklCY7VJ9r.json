{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors present a new adaptive gradient method AvaGrad. The authors claim the proposed method is less sensitive to its hyperparameters, compared to previous algorithms, and this is due to decoupling the learning rate and the damping parameter.\n\nOverall, the paper is well written, and is on an important topic. However, I have a few concerns about the paper, which I will list below.\n\n1. The fact that adaptive gradient methods converge with a fast rate when the sum in the denominator is taken till the t-1th iterate has appeared in previous papers before [1]. The convergence rate analysis for this case is fairly simple, and I am not sure if analyzing RMSProp/Adam in this setting should be considered a significant contributions of the paper.\n\n2. The proposed algorithm AvaGrad is a simple but interesting idea. I have a number questions about the experimental evaluation though, which makes it hard for me to evaluate the significance of the results presented:\n\na) Was momentum used with SGD?\n\nb) How is the optimal hyperparameters (learning rate and damping, i..e, epsilon, parameters) selected?\n\nc) Do any of these conclusions change when trying out a very small or very large batch size?\n\nd) I am not convinced that using the same optimal hyperparams as the WRN-28-4 task on the WRN-28-10 and ResNet50 models is a reasonable experiment. Why is this a good idea? While this does support the claim that adaptive gradient methods are less sensitive to hyperparameter settings, but makes the other claim about AvaGrad generalizing just as well as SGD weaker?\n\ne) One of the key claims that adaptive gradient methods generalize better when using a large damping (epsilon) parameter has appeared in previous papers as well [2, 3].\n\n\nOverall, in my view, this is a borderline paper mostly because I think a number of the results presented have been shown in other recent papers. My score reflects this. However, I think decoupling the learning rate and the damping parameter by normalizing the preconditioner is a simple but interesting idea, and I am willing to increase my score based on the discussion with the authors and other reviewers.\n\n\n[1] X. Li and F. Orabona. On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes. In AISTATS 2019\n[2] M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. in NeurIPS 2018.\n[3] S. De, A. Mukherjee, and E. Ullah. Convergence guarantees for rmsprop and adam in non-convex optimization and an empirical comparison to nesterov acceleration. arXiv:1807.06766, 2018.\n\nA few more minor comments:\n\n1. The authors say that methods like AMSGrad fail to match the convergence rate of SGD. But this statement seems misleading since it is not clear whether the worse rate is due to the analysis (which gives an upper bound) or the algorithm?\n\n2. In the related work section, the authors discuss convergence rates of algorithms with constant and decreasing step sizes together. This can be confusing to the reader, and it is best to explicitly mention the setting under which the different results were derived."}