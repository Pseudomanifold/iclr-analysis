{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a new adaptive method, which is called AvaGrad. The authors first show that Adam may not converge to a stationary point for a stochastic convex optimization in Theorem1, which is closely related to [1]. They then show that by simply making $eta_t$ to be independent of the sample $s_t$, Adam is able to converge just like SGD in Theorem2. Theorem2 follows the standard SGD techniques. Next, they propose AVAGRAD, which is based on the idea of getting rid of the effect of $\\epsilon$. \n\nStrength:\nThe experiment results are impressive. They show that Adam can outperform SGD on vision tasks.\nRecently, people have found out that $\\epsilon$ is a very sensitive hyper-parameter. It is good to see some research directly addresses this problem. \n\nWeakness:\nThe word \"domain\" is confusing. \nIf the Adam-type algorithms are the delayed version in Table 1?\nIt is not compatible with AdamW. \nThe results on the image datasets seem too good to be true.\n\nImplementation Issue:\n***Many implementation details in the below discussion are different from the paper (e.g. hyperparameters and network architecture). So the following experiment results may not be used for assessment of the quality of the proposed method.***\nI tried the proposed Delayed Adam on CIFAR-10 using the codebase in (https://github.com/LiyuanLucasLiu/RAdam/tree/master/cifar_imagenet). The performance seems the same as Adam. Delayed Adam even leads to a *divergence* problem, especially with a large learning rate (0.03). The divergence problem never happens when using Adam and AdamW with the same hyperparameters.\nImplementation details: \n1. Replace the optimizer with the original PyTorch Adam implementation. (https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) \n2. Swap line 96 and 108 as suggested in the paper. \n3. Modified line 89 (bias_correction2=1 - beta2 ** (state['step']-1)) \n4. Do not run line 97-107 when state['step']==1. \n5. Run the following code: python cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer adam  --beta1 0.9 --beta2 0.999  --checkpoint ./logdir --gpu-id 0 --model_name adam_003 --lr 0.03\n\nIf the authors can provide more implementation details, I would promote my rating. \ne.g., \n1. Are you still using bias correction in the proposed method? If so how do you use them?\n2. Do you update the model for the first step?\n\nReference:\n[1] On the convergence of adam and beyond\n"}