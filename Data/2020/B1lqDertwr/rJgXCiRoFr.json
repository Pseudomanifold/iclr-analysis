{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "An interesting paper on the role of regularization in policy optimization\n\n\nIn this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning. The authors provide a detailed investigation of the effect of regulations on the performance and behavior of agents following these methods.\n\nThe authors present that regularization methods mostly help to improve the agents' performance in terms of final scores. Specifically, they show that direct regularizations on model parameters, such as the standard case of L2 or L1 regularization, generally improve the agent performance. They also show that these regularizations, in their study, is more proper than entropy regularization. The authors also show that, in the presence of such regularizations, the learning algorithms become less sensitive to the hyperparameters. \n\nFew comments:\n1) The paper is well written and easy to follow. I appreciate it. I found the writing of the paper has a bit of repetition. The authors might find it slightly more proper to remove some of the repetitions (e.g. section 4.2)\n\n2) While I appreciate the clear writing and reasoning in this paper, I might suggest a slight change in the second paraphrase of the intro. I agree with the authors' reason on the first three lines, but I think it would be useful to also emphasize the role of the questions the researchers investigate to answer. I might also add one the main reason that the researchers in the field of DRL have spent less time on regulation or architecture search was their focus on more high-level algorithm design which is in the more immediate step of relevance and specialty to the field of reinforcement learning. \n\n3) I would suggest rephrasing the last two sentences of the second paragraph in related work: \"Also, these techniques consider ...\". Regularizing the output also regularizes the parameters, I think the authors' point was \"directly regularize\" the parameters. \n\n4) In the \"Entropy Regularization\" part of section 3, I guess the Hs has not been defined. \n\n5) Repeated \"the\" in the last paragraph of section 4.1 (despite it already incorporates the the maximization of)\n\n6) The authors used the term \"not converge\" multiple times. While it is hard from the plots to see whether the series converges or not, I have a strong feeling that by this term the authors mean the algorithm does not converge to a resealable solution rather than being divergent up to a bandwidth. Maybe clarifying would be helpful.\n\n7) In section 5, the authors study the sensitivity to the hyperparameters. In this section, I had a hard time to understand the role of term 3\n\"BN and dropout hurts on-policy algorithms but can bring improvement only for the off-policy SAC algorithm.\" Does it mean that deploying BN, results in a more sensitive algorithm? or it means that the performance degrades (which is a different topic than section 5 is supposed to serve)?\n\n8) In section 7, the authors put out a hypothesis \"\nHowever, there is still generalization between samples: the agents are only trained on the limited\" but the provided empirical study might not fully be considered to be designed to test this hypothesis. In order to test this hypothesis, the author might be interested in training the models with bigger sample sizes, more training iteration, different function classes, and more fitting in order to test this hypothesis.\n\n\n9) Section 7 on \"Why do BN and dropout work only with off-policy algorithms?\" while I agree with the authors on their first reason which is quite commonly known, I might hesitate to make the second statement (2)\n\n\n\nGenerally, I found this paper an interesting paper and appreciate the authors for their careful empirical study. But I found the contribution of this work to be not significant enough. Most of the statements and claims in this paper are well know in the community, especially among deep learning practitioners. While I acknowledge the scientific value of this study, its concreteness, and appreciate the contribution of this paper, due to the low acceptance rate of this conference, I might be reluctant in accepting this paper. \n"}