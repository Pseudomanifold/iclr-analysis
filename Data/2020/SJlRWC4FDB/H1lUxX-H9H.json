{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Thank you to the authors for clarifying that they disclosed the vulnerability to YouTube. It would be good to surface this in the main text rather than a footnote, and to include some details as to what the response from YouTube was.\n\nThe paper demonstrates that adversarial examples are a practical scheme for evading copyright infringement detection tools. The study of this application is well motivated: adversarial examples need not be played in the physical domain (since the audio/video is uploaded directly in a digital format) and copyright detection is a bit more complicated than object classification (because false positives are expensive), small margin between classes. \n\nCould you motivate the choice of the L_inf norm as an appropriate metric for the magnitude of the perturbation added to audio samples? Would the perturbation affect a listener\u2019s ability to follow the audio that the attacker is trying to avoid detection of? In Section 4.5, why is inserting a perturbation that is close to a different audio sample a successful attack? How would the baseline of playing the target audio sample at a very low volume perform in comparison?\n\nWhile the paper considers a system that was designed by the authors, because they demonstrate transferability to commercial products, this is not a limitation of their work and demonstrates a realistic threat model. The description of experimental results is however missing important details required to evaluate the results presented:\n\n* What are the details of the dataset used to evaluate the approach? How many songs were included in the dataset? \n* Did you conduct a study to verify that the perturbation added did not affect legitimate listeners?\n\nThe paper is well written and easy to follow. \n\nA simple nitpick on page 2: do you have any references to point to for the following claim? \u201cMost audio, im- age, and video fingerprinting algorithms either train a neural network to extract fingerprint features, or extract hand-crafted features.\u201d \n\nAnd on page 7, the following sentence might be a typo given that the perturbations introduced in this paper are adversarial: \u201cTherefore, one would expect that low-amplitude non-adversarial noise should not affect this system.\u201d"}