{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a variant of gating functions for recurrent neural networks and feed-forward layers of Transformer and apply it to variety of tasks including toy tasks such as sorting, tree traversal and more realistic tasks such as machine translation. The gating function is applied recursively for N number of steps and depth of recursion is learned softly in data-driven function. Authors show similar or slightly better performance of their approach when applied to LSTM and Transformer compared to vanilla LSTM and Transformer.  \n\nI have several comments regarding this work:\n\n1) I believe there is a typo in equation in section 2 describing parametrization of o^{n}_{t} where h_{t-1} has un-necessary upper-script {n}\n\n2) I would like to see the equations showing differences/similarities between MetaGross gating function and GRU/LSTM in Section 2 to better understand how MetaGross relates to previous work.\n\n3) Calling parallel version of MetaGross that computes gating values given entire sequence *non-autoregressive* is really correct since decoding for machine translation is still done in autoregressive fashion one token at a time. I would suggest the authors to remove word non-autoregressive and just stick with word parallel.\n\n4) The obvious connection with this work and work of Alex Graves on Adaptive Computation for Recurrent Neural Nets and many of its followups including Universal Transformers by Dehghani et al is missing from experimental comparison and is not even mentioned in related section. I believe that not mentioning these papers and not comparing to them empirically  this is a major drawback of this paper. \n\n5) I don't think that Task 3 (Logical Inference) contains a language vocabulary of six words because it is a natural english language unless I am misunderstanding something. \n\n6) What does EM stand for in Table 1. Would be great if you could include description of what EM, P(perplexity), n(depth of recursion) stands for in caption of Table 1\n\nOverall it is a good and well written paper, although I believe that the variants of recursively gated functions have been proposed and applied before (see comment 4). Would be open to discussions and raising scores if authors convince me otherwise. "}