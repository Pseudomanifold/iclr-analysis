{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nEnsembles of probabilistic models (e.g. for classification) provide measures of both data uncertainty (i.e. how uncertain each model is on average) and model uncertainty (i.e. how much the models disagree in their predictions). When naively distilling an ensemble to a single model, the ability to decompose total uncertainty into data uncertainty and model uncertainty is lost. This paper describes a method for ensemble distillation that retains both types of uncertainty in the case of classification. The idea is to train a prior network (i.e. a conditional Dirichlet distribution) to model the distribution of categorical probabilities within the ensemble. Both total uncertainty and data uncertainty can then be computed analytically from the prior network.\n\nPros:\n\nThe paper considers an interesting problem, and makes a clear contribution towards addressing it. The paper motivates the problem well, and explains the contribution and the method's limitations clearly.\n\nThe proposed method is simple, but well motivated, sound, and well explained.\n\nThe paper is very well written and easy to read. I particularly appreciated the toy experiment in section 4 and the visualization in figure 3, which showcase clearly what the method does.\n\nThe experiments are thorough, and the results are discussed fairly and objectively.\n\nCons:\n\nThe scope of the paper and the method is limited to the problem of probabilistic classification. However, one could have a more general ensemble of conditional or unconditional distributions. The method could in principle be applied to this setting, by having a hypernetwork learn the distribution of the distributional parameters of the models in the ensemble (the method presented in the paper is a special case of this, where the hypernetwork is a prior network and the distributional parameters are categorical probabilities). However, it's not clear that the method would scale to models with arbitrary distributional parameters. I would suggest that the paper make it clear from the beginning that the scope is probabilistic classification, and at the end discuss the extent to which this is a limitation, and how the method could potentially be extended to other kinds of ensembles.\n\nThe prior network used is essentially a conditional Dirichlet distribution, which (as the paper clearly acknowledges) may not always be flexible enough. A more flexible prior network could be a mixture of Dirichlet distributions, where the mixing coefficients and the parameters of each mixture component would be functions of the input, similarly to mixture density networks (http://publications.aston.ac.uk/id/eprint/373/) but with Dirichets instead of Gaussians. I believe that equations (9) and (10) would still be tractable in that case, as it's tractable to compute expectations under mixtures if the expectations under mixture components are tractable.\n\nOne limitation of the approach is that the prior network may not give accurate predictions for inputs it hasn't been trained on (as the paper discusses and section 4 clearly demonstrates). It's not clear how this problem can be overcome in general, and further research may be needed in that direction.\n\nSome of the results in Table 4 are puzzling (as the paper also acknowledges). In particular, the EnD model should be able to retain the performance of the ensemble when using total uncertainty but it doesn't. Also, using knowledge uncertainty doesn't always seem to be better than using total uncertainty, which to some extent defeats the purpose of the method (at least in this particular example). It would be good to investigate further these results. In any case, I appreciate that the paper acknowledges these results, but avoids unjustified speculation about what may be causing them.\n\nDecision:\n\nOverall, this is good work and I'm happy to recommend acceptance. There are some limitations to the method, but these can be seen as motivation for future work.\n\nMinor suggestions for improvement:\n\nThis older work is relevant and could be cited (but there is no obligation to do so).\nOn compressing ensembles:\n- Model compression, https://dl.acm.org/citation.cfm?id=1150464\n- Compact approximations to Bayesian predictive distributions, https://dl.acm.org/citation.cfm?id=1102457\n- Distilling model knowledge, https://arxiv.org/abs/1510.02437\nOn information-theoretic measures for decomposing total uncertainty as in eq. (4):\n- Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning, https://arxiv.org/abs/1710.07283\n\n\"[Knowledge uncertainty] arises when the test input comes from a different distribution than the one that generated the training data\"\nThis is only one way knowledge uncertainty can arise, it could also arise when the test input comes from the same distribution that generated training data, but there aren't enough training data available.\n\nSome parentheses are dangling at the bottom of page 2, top of page 3 and middle of page 4.\n\nIn figure 1, it'd be good to make clear in the caption that the figure is illustrating the simplex of categorical probabilities.\n\nThe last paragraph of section 2 uses the term \"posterior\" repeatedly to refer to P(y|x, \\theta) which is confusing. I would call P(y|x, \\theta) the \"model prediction\" or something like that.\n\nEq. (5) should be without the negative sign I think.\n\nIn section 3, I would use a different symbol (e.g. \\phi) to denote the parameters of the prior network, to clearly distinguish them from the parameters of the models in the ensemble.\n\n\"Optimization of the KL-divergence between distributions with limited non-zero common support is particularly difficult\"\nSome more evidence in support of this claim is needed I think; either explain why or provide a reference that does.\n\nIn section 4, the text says that EnD has \"a minor degradation in performance\" but table 1 seems to show otherwise. Also, the results of EnD in table 1 and table 2 are different, which makes me think there may be a typo somewhere.\n\nMaking the references have consistent format and correct capitalization (e.g. DNNs, Bayesian) would make the paper look even more polished.\n"}