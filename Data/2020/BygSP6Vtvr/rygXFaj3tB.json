{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n========= Summary ========= \n \nDistilling an ensemble of deep networks into a single student network is a common approach to reduce the inference-time computational complexity. In those cases, the paper poses the question of whether it is useful for the student to capture the diversity of ensemble members\u2019 predictions on top of the mean distribution (as in the standard distillation). \nThe main motivation is that this captured diversity will enable the student network to decompose the total (predictive) uncertainty into data (aleatoric/irreducible) and knowledge (epistemic/model) uncertainty components. \nIn order to distill the diversity, it proposes to use \u201cprior networks [Malinin&Gales 2018]\u201d to output parameters of a  Dirichlet distribution over the simplex of possible predictive categorical distributions.\nIt uses one toy dataset as well as 3 small image classification datasets to compare the performance of the proposed method (EnD^2) with a) standard ensemble distillation (EnD), b) ensemble of deep networks, as well as c) an individual deep network. The performance is measured in terms of classification accuracy, and the quality of the predictive uncertainty for in-distribution (ID) and out-of-distribution (OOD) samples.\nIt shows that for OOD sample detection, the proposed EnD^2 outperforms the standard EnD and bridges the gap to the full ensemble. The classification accuracy and ID uncertainty quality is similar to the standard EnD.\n \n \n========= Strengths and Weaknesses ========= \n \n+ the paper studies the interesting problem of decoupling data/knowledge uncertainty in the commonly-used framework of knowledge distillation. It proposes a simple solution to the problem. So, it is highly relevant for the field.\n+ Results in Table 3 suggest significant improvements over the standard distillation (EnD). \n+ Figure 3 is pedagogical and intuitive for the data vs knowledge uncertainty decomposition and the effectiveness of auxiliary dataset for this toy problem.\n \n(I) General concerns:\n- The technical novelty is limited to the combination of knowledge distillation and prior networks.\n- Two closely-related works are not cited:\n[Li&Hoiem, \u201cReducing Over-confident Errors outside the Known Distribution\u201d 2019] uses auxiliary dataset for ensemble distillation.\n[Englesson&Azizpour, \u201cEfficient Evaluation-Time Uncertainty Estimation by Improved Distillation\u201d, 2019] addresses the problem of efficient uncertainty estimation using knowledge distillation.\n \n(II) Concerns regarding the experiments\n- As also found curious by the authors, it is concerning that the EnD_Aux results are so low for OOD detection. Both [Li&Hoiem 2019] and [Englesson&Azizpour 2019] suggest that the standard distillation can perform similarly to (or even outperform) the individual model and the ensemble using an auxiliary dataset [Li&Hoiem 2019] or teacher label for augmented samples [Englesson&Azizpour 2019]. It should be mentioned that the OOD dataset setup is different across these works.\n- Why only 100-ensembles are used for the real-world experiments? Does a \u201csuccessful\u201d training of the prior network require a large number of samples? This would be important since prior ensemble works (e.g. [Lakshminarayanan et al. 2017] ) hardly improve beyond 5-15 networks. Training 100 networks is very costly which would be an important limitation of this work in case it\u2019s necessary. An ablation study on the number of networks in the ensemble is required for investigating this trade-off.\n- Hyperparameter optimization: Appendix A provides the final values for the hyperparameters of each method. However, it is not clear how these were optimized? Was grid search used? What was the range for all the hyperparameters? Is there a validation set or cross validation is used and in each case how is the split done? Is it exactly the same hyper-parameter optimization that is done for EnD and EnD^2? \n \n(III) Missing from experiments\n- The main goal/motivation of the paper is to be able to decompose the total uncertainty when distilling an ensemble into a single model. In that respect, richer and more experiments are required to evaluate this ability. Currently, the experiments are focused on showing that EnD^2 outperforms EnD. The toy dataset, qualitatively, evaluates the decomposition quality with interesting results but is not enough to make conclusions for real-world datasets. For instance, as an additional experiment, plots/statistics can be given on data vs knowledge uncertainty for ID vs OOD samples.\n- Prior network [Malinin&Gales 2018] in its standard form is an important single-network baseline that should be included for both ID and OOD experiments. \n- Temperature scaling is mentioned to be a vital part of the model. As such, it requires a thorough ablation study to see the results with or without it as well as when changing the temperature and the annealing factor. It should be studied from two aspects: accuracy and convergence failure (as mentioned by the paper)\n- P.7: \u201cPrediction Rejection Ratio\u201d (PRR) is a measure proposed in this work but it\u2019s only defined in the appendix. That is the only metric that measures the quality of uncertainty for ID samples (Table 3). As such, I believe it\u2019s important to define PRR in the main paper and also further include more standard metrics such as NLL, AUROC, AUPRcurve in table 3 so that some context is given to the newly-proposed PRR.\n- Along the same line, it seems NLL is consistently and \u201cmore significantly\u201d worse for EnD^2 compared to EnD.\n \n(IV) Missing training and implementation details\n- More details should be provided for the training of the student prior network including it\u2019s loss function given a dataset and Dirichlet distribution. This can be obtained from [Malinin&Gales 2018] but it\u2019s important for this work to be self-contained.\n- the loss function in equation 8 has log(p(\\pi|x;\\theta)); is there any numerical issue regarding the Dirichlet distribution and/or the log? If so, how significant and what measures are accordingly taken? Could it be that the temperature scaling is more a way of alleviating these issues as opposed to the shared support of distributions for KL divergence?  \n- the details of the annealing algorithm is entirely missing.\n \n \n========= Final Decision ========= \n \nThe paper addresses a highly relevant problem in a simple (and potentially effective) way. This is great. However, there are several concerns as listed above which altogether makes me lean towards an initial \u201cweak reject\u201d rating. (II) and (III) are more central to this initial rating. I will carefully read the authors rebuttal as well as other reviewers\u2019 comments before I finalize my rating.\n \n \n========= Minor points ========= \n \ngeneral:\n- P.2: \u201c[...] limitation of ensembles is that the computational cost of training and inference [...] One solution is to distill [...]\u201c -> this only remedies the computational complexity of inference and in fact increases the training time.\n- P.3: [Malinin&Gales 2018] should be cited for equation 4 and the discussion around it.\n- P.5: \u201cEnsemble Distillation (EnD) is able to recover the classification performance [...] with only very minor degradation in performance\u201d. Table 1 does not show any degradation for EnD. It shows some degradation for EnD^2 when 100-ensemble is used.\n- P.7: \u201cNote, that on C100 and TIM, EnD2 yields better classification performance than EnD\u201d: almost all of the classification improvements are well within one-std. In the specific case of C100, it\u2019s a stretch to call it a \u201cbetter classification performance\u201d.\n- P.7: how many runs are used to obtain the mean and std reported in table 3 and 4.\n- P.7: is the PRR in table 3 calculated using the total or knowledge uncertainty for ensemble and EnD2?\n\nTypo:\n- P.2: \u201c. Consider an ensemble of models {P[\u2026]\u201d --> a closing parenthesis is missing.\n- P.3: \u201cEach of the models P([...]\u201d --> a closing parenthesis is missing.\n- P.3: \u201c the entropy of the expected and\u201c -> \u201cthe entropy of the expectation\u201d\n- P.4: \u201c=\\hat{p}(x,\\pi)\u201d \u2192 \u201c\\sim\\hat{p}(x,\\pi)\u201d\n- P3-4.: hat and star seem to have been arbitrarily used for input x, parameters theta, pi and p with most of them undefined.\n- P.5: script MI is used for mutual information while in P.4 script I is used.\n- P.6: \u201cmay require the use additional training\u201d -> use of additional\n- P.6: Results of EnD in table 2 does not match table 1.\n \n"}