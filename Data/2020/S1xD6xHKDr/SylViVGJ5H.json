{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary: The paper describes a method for providing explanations for the predictions made by a text classifier. The method is novel in that it aims to explicitly model interactions between words so that e.g. the phrase \"not bad\" appears as an explanation itself, rather than as separate words \"not\" and \"bad\". They produce these explanations in a hierarchical way, by greedily dividing the input phrases into two consecutive subphrases until reaching the word level. The model is compared against and out-performs a variety of baselines on word level-importance scores, using two metrics from prior work (AOPC and LogOdds) which measure how much predictions change when high-importance spans are ablated. The authors also introduce a new metric (\"cohesion loss\") which is meant to evaluate span-level importance. This metric works by measuring how much predictions change when the words from important spans are broken up and scattered throughout the sentence (rather than appearing as one span). On this metric, they outperform one baseline which also produces span-level importance scores. Finally, they run a short human eval on MTurk in which they measure how well humans can anticipate model predictions given the explanations. Here they outperform one baseline (it is unclear why they can't compare against more baselines).\n\nEvaluation: The paper seems to present a nice idea. The qualitative results (example hierarchical explanations) are compelling. I have a few concerns related to overall clarity of the paper itself, which make me hesitant to recommend publishing it as is. My primary concern is that the most important evaluation (evaluation of phrase-level importance scores, Sec. 4.1) relies on a newly-proposed metric and thus we cannot be confident in the soundness of this metric. Similarly, the human eval seems rushed and only compares against one baseline, when in fact this should be one of the primary evaluations as the entire contribution of the paper depends on this method being superior at offering explanations. As a lesser concern, I also felt that the description of the method itself (Sec. 3.2) was quite terse and lacking in motivation/intuition, even though I don't think that the overall procedure is in fact very complex (and thus excessive notation is probably removing versus adding clarity). It was difficult to evaluate the novelty and reasonableness of the chosen scoring functions without consulting several other papers/resources. I think the authors need to rewrite these sections prior to publishing. Additional specific questions below.\n\nSpecific Questions for Authors:\n--> Section 3\n* Is the subscript under max in Eq. 1 correct? feels like it should be y' neq yhat, and y' in Y?\n* In Eq. 1 you say you replace with PADs. There is always some weirdness when getting scores on inputs that don't look like the training inputs--e.g. phrases surrounded by PADs are not the same as sentences. Can you comment on how this might effect the reliability of the results in terms of fidelity to what the model is actually doing?\n* You should give short a definition and explanation of Shapely value, and summarize how it applies to your approach (and the prior related work which also used it). I wasn't familiar with this before reading this paper so had to look it up, and I don't think its something that can be assumed to be well-known by ICLR audiences. It would help a lot if you spelled out what it is, what its original use was, and then highlighted the ways you differ from the out-of-the-box application (i.e. how/why you have to adapt it, and the intuition behind those adaptations).\n* You don't introduce what subscripted z^l notation is, its a bit obtuse\n* I am confused on the motivation behind Eq. 3 and what role it plays in the overall process. I thought the Shapely score (Eq. 2) is supposed to capture the feature interaction, but Eq. 2 depends on Eq. 3, which is also described as being \"feature interaction\". Can you explain more what these different scores are for, why both are needed, and which are novel vs. given by prior work in game theory and/or ML?\n* End of section 3: \"in generally\" -> \"in general\"\n* Note about complexity: You make the assumption that you can only get interactions between adjacent spans. While this is fine, it is not t necessarily ideal, right? E.g. in Fig 4, the ideal explanation i'd want to see is \"not bad at all\" regardless of the intervening noun (a journey), but this isn't accomplished if you can only measure interaction between consecutive spans. Longer phrases with richer parse structures could be harder to explain given this assumption. You should comment on/discuss this assumption and potential limitations associated with it.\n\n--> Section 4\n* Forgive me if this is a naive question, but why can't these AOPC and LogOdds metrics just be used directly for importance score? If we believe they are an accurate measure of what the model is using, why not just use them to generate the scores in the first place? \n* Cohesion Loss: This metric relies on interpreting the predictions the model makes when given malformed input that would be unlike what it sees in training (in this case, scrambled sentences). Again, I have some concern about getting/interpreting model predictions on malformed input, esp. for higher-capacity models like BERT which may be sensitive to the weirdness and behave worse as a result. Are there baselines for this metric? Or any way you could justify that its meaningful and reliable? Its suspect to have the paper's primary evaluation be based on a newly-defined metric, since its hard to know whether or not to trust it.\n* nit: Calling it \"cohesion loss\" implies lower is better, so \"cohesion score\" might be more appropriate\n* Why can't you run the human eval over many more baseline models, e.g. models that offer only word-level explanations (all the ones in Figure 2)? I'd like a more thorough human eval since the only reason we would want explanations is for human consumption, so that is the main point of this entire project.\n* Overall, the human eval section feels comically last-minute. The rate of typos in this section is 10 fold what it was in the rest of the paper. Make sure you give it a proofread. :) I am glad you included it, but feels rushed.\n"}