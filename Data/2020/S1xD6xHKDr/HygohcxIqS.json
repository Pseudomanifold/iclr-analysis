{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nThis paper proposes a method to generate hierarchical explanations for text classification by feature interaction detection. The method is model-agnostic and is demonstrated on a diverse set of text classification models: LSTM, CNN, and BERT. \n\nThe paper starts by proposing a method for computing the importance of a subset of features. This is done by subtracting the first and second top class probabilities at a subset of word inputs, where other words are replaced with <pad>. Next, the shapley interaction formulation from [1,2] is invoked to detect feature interactions. \n\nLastly, a hierarchical interpretation is generated top-down by minimizing interaction between sentence partitions, and repeatedly splitting generated partitions.\n\nInterpretations are evaluated via AOPC/log-odds scores for word importance, cohesion loss for phrase importance, as well as a coherence score in mechanical turk studies.\n\n\nRecommendation:\n\nReject, because 1) the paper does not compute shapley interactions with the proposed formulation of subset feature importance, and 2) the paper does not acknowledge related work on interpreting feature interactions in prediction models beyond SHAP interactions [3,4,5]\n\n\nSupporting arguments:\n\nThe interaction definition shown in Eq. 3 does not match the corresponding equation referenced in Lundberg et al. [1]. Based on [1], the $f$ in Eq. 3 should be the expected value of a model output conditioned on a subset of input features. The $f$ defined in Eq. 1 does not compute this expectation; therefore, the meaning of the Eq. 3 result is unclear - although it is extremely important to this paper. \n\n\nSuggestions:\n\nWithout justifying $f$, this work needs to justify that $\\phi$ is an estimate of interaction importance. One way to do this is by evaluating InterShapley against ground truth interactions. For example, an LSTM (or CNN, BERT) can be trained on synthetic data generated by functions with known interactions (e.g., x1*x2). Then, InterShapley can be validated by identifying these interactions. \n\nFurthermore, it is important to discuss how InterShapley is novel, especially when the idea of model-agnostic hierarchical explanations has been proposed already in [5].\n\n\n[1] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888, 2018.\n[2] Katsushige Fujimoto, Ivan Kojadinovic, and Jean-Luc Marichal. Axiomatic characterizations of probabilistic and cardinal-probabilistic interaction indices. Games and Economic Behavior, 55 (1):72\u201399, 2006.\n[3] Jerome H Friedman, Bogdan E Popescu, et al. Predictive learning via rule ensembles. The Annals of Applied Statistics, 2(3):916\u2013954, 2008.\n[4] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 623\u2013631. ACM, 2013.\n[5] Michael Tsang, Youbang Sun, Dongxu Ren, and Yan Liu. Can i trust you more? model agnostic hierarchical explanations. aXiv preprint arXiv:1812.04801, 2018."}