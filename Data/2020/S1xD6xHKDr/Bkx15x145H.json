{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Overview/Contribution:\n====================\nThe paper proposes an interpretation method that is based on feature interactions and feature importance score as compared to independent feature contributions. They evaluated their methods on three models for text classification task with two datasets. They evaluated the quality of the generated interpretations using metrics as well as human validation.\n\nOverall, I strongly don\u2019t recommend accepting this paper as it is in current form. I back the decision with the following strengths and weaknesses. I suggest to the authors to address the weaknesses pointed out to make the paper more organized, motivating the need for such feature interaction based interpretation in a more clear manner. I would like to note that I have read the paper well and seen the supplemental appendix.\n\nStrength:\n========\n+ Human understandable interpretations are essential to describe decisions made by deep models to end users in sensitive applications such as healthcare and security. Interpretations that consider multiple features at a time have potential for generating more meaningful interpretations.\n\nWeakness:\n===========\n- The flow of the paper is not smooth to follow. Experiments comes very quickly without sufficiently detailing the proposed method. It is seriously flawed.\n- The method is neither described in detail nor motivated well enough. Authors are using the word explanation throughout while they are attempting a very simple feature/input importance score based interpretation. These two are used interchangeably used but they mean completely different things especially when talking about human level/understandable explanations [1]. \n- Nothing was described about the evaluated models. The names LSTM and CNN are very generic. No description what so ever about the datasets before giving the results on Table I.\n- Experiments and results are so intermingled that it is really hard to interpret and unravel the main points.\n- Feature importance score does not necessarily mean interpretations from the importance are good human understandable explanations. The proposed AOPC metric is evaluating feature importance by evaluating the probability of predicted label given the subset of input features, i.e. \u201cCorrelation does not necessarily mean causation\u201d.\n- In general, the paper is not ready to be published at ICLR.\n\n1) Bekele, E., Lawson, W. E., Horne, Z., & Khemlani, S. (2018). Implementing a Robust Explanatory Bias in a Person Re-identification Network. In\u00a0Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops\u00a0(pp. 2165-2172)."}