{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed VHE-GAN for the text-to-image generation task. The proposed method utilizes the off-the-shell modules and feeds the VHE variational posterior into the generator. The experiments are conducted on three datasets. \n\nThe motivation for the paper is not clear. Most of the components used, such as text-encoder, image-encoder, generator-discriminator follow previous works. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, I did not see the clear motivation for this part. \n\nBesides the basic version VHE-StackGAN++, it proposed another version  VHE-raster-scan-GAN. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. \n\nThe experimental results are not solid. The comparison only included old baselines. However, several recent state-of-the-art approaches are missing: a. attnGAN (CVPR18), b. TA-GAN (NIPS18), c. Object-GAN (CVPR19). Without these comparisons, it is difficult to evaluate how the method works. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode).   "}