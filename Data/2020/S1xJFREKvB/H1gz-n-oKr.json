{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper provides a new simple method to incorporate Nesterov momentum into standard SGD for deep learning, with good empirical and theoretical results. Overall I think this paper should be accepted, some minor comments follow.\n\nAt no point does Polyak's heavy ball method get mentioned, even though the variant of Nesterov acceleration you are considering is very similar to it (since the momentum parameter is fixed, which is not the usual form of Nesterov except in the strongly convex case). It would be beneficial to delineate how this is or isn't related to heavy ball.\n\nThe experiments would benefit from a wall-clock time comparison too, rather than just epochs since these new methods would be slower (but presumably not by much).\n\nThe appendix is huge with most of the technical details relegated there which I did not read fully. I think this impacts the readability significantly, though not grounds for rejection. Perhaps it suggests that a conference with a small page limit is not the best venue?\n\nIt seems that SGD still has better convergence early on. The authors suggest their method fixes this (relative to standard nesterov SGD) but it doesn't seem to be quite as good as SGD. Can you explain or discuss why this is still the case?\n\nThe assumptions require some explanation, they are just listed with no context. What are they and why are they useful?\n\nStep size \"should be constrained to O(1/L)\" is misleading, you should say explicitly that step-size <= 1/L (or whatever it is depending on the algorithm).\n\nSome of the writing is a bit strange / sloppy, e.g.:\n\"AM2-SGD is a bit tricky in randomness\"\n\"However, full-batch loss is way too expensive to evaluate.\"\n\nIn Algorithm 1 AM1-SGD:\n\"xk+1 \u2190 xk+1 + \u03b2 \u00b7 (\u02dcx+ \u2212 x\u02dc)\"\ndoesn't parse because x_{k+1} appears twice.\n\nMissing references:\n\nAccelerated proximal algorithms:\n\n*) Beck and Teboulle: A Fast Iterative Shrinkage-Thresholding Algorithm\nfor Linear Inverse Problems\n\n*) Nesterov: Gradient Methods for Minimizing Composite Objective Function,\n\nRestarting (slightly different to your approach but still relevant): \n\n*) O'Donoghue: Adaptive Restart for Accelerated Gradient Schemes "}