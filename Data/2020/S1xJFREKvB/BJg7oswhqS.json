{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\n\nThis paper proposes two variants of Nesterov momentum that maintains a buffer of recent updates. The paper proves optimal convergence in the convex setting and makes nice connections to mirror descent and Katyusha.\n\nI vote to reject the submission, with my main concerns being with the experimental results. I would consider raising my score if my concerns are addressed.\n\nConcerns\n-The learning rate schedule on the CIFAR experiments is unconventional. The original ResNet paper trains for 64k iterations (roughly 160 epochs). It\u2019s standard to train for at least 200 epochs (see schedule from Smith et al.). Do the results hold under the standard schedule with careful tuning for the baselines?  \n-The discussion on \u201cTrain-batch loss vs. Full-batch loss\u201d in Section 2 is unclear. On smaller datasets, it is feasible to perform evaluation at the end of the epoch on the entire batch. Furthermore, reporting test accuracy couples optimization and generalization, and I am not sure what is meant by the statement ``test accuracy is too informative.\u201d\n-Reporting the peak test accuracy is strange. In general, we do not have access to the test set. It\u2019s more natural to report the final test accuracy or have a holdout set to determine an iteration for evaluation.\n-It\u2019s not clear how significant the results on ImageNet and PTB are. Namely, a comparison to AggMo and/or QHM would be good, since the flavor of these algorithms is quite similar. Experiments in the AggMo paper suggest that AggMo performs better on PTB. In the comparison given in Appendix A3, it seems like AggMo performs slightly better throughout training. SGD should also attain better performance with learning rate tuning on ImageNet.\n-I\u2019m not sure how useful \u201cTest Accuracy STD%\u201d is useful as a metric since it is influenced heavily by the learning rate and its schedule. Tail averaging schemes in general seem like they would increase \u201crobustness.\u201d In addition, there seem to be situations where a higher final variance is beneficial (just run the method for longer and you can find a better solution). It would be nice to expand the discussion on the notion of robustness defined in the paper.\n\nMinor Comments\n-The dashed line in figure 1b) is hard to read. I would recommend removing the grid lines and make the colors more differentiable.\n-Algorithm 1: use of both assignment and equality operator? Whereas other boxes use equality\n-Spacing looks a bit off in parts of the paper. 1) after the first sentence in the introduction 2) \u201cgeneric optimization layer (Defazio, 2018) . However\u201d)\n-M-SGD and M-SGD2 can be potentially confusing and are not too informative as acronyms.\n-Remark on Theorem 1b: depicts does not seem like the right word\n\nSmith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489."}