{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors consider composition problems and use the stochastically controlled stochastic gradient method (SCSG) to approximate the gradient G(x) and \\nabla f(x). The authors also provide convergence analysis of the proposed method for strongly convex problems and non-convex problems. Authors then conduct experiments on the\nmean-variance optimization in portfolio management task and the nonlinear embedding problem, results show that the proposed method is faster. \n\nThe following are my concerns:\n1) There are several important related works missing in the paper, e.g., [1][2].  \n2) The convergence results of the proposed method in the paper are not state-of-the-art. For a strongly convex case, the result in the paper is O( n+ k^2 min(n, 1/u^2) log1/e), it is not necessarily better than O(n+k^3 log 1/e) in [1] or O(n+kn^{2/3} log 1/e). For a non-convex case, the result in the paper is O(\\min{1/e^{9/5}, n^{4/5} / e}), it  is not necessarily better than O(n^{2/3}/e) in [1] or [2]. \n3) More compared results should be conducted in the experiments, e.g. [1][2]. \n\n[1]Huo, Zhouyuan, et al. \"Accelerated method for stochastic composition optimization with nonsmooth regularization.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n[2] Zhang, Junyu, and Lin Xiao. \"A Composite Randomized Incremental Gradient Method.\" International Conference on Machine Learning. 2019.\n\n"}