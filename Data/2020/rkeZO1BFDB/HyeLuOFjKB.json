{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper accomplishes several things: they present the first cross-lingual VLN dataset (by collecting new Chinese annotations), they conduct analysis between English and Chinese corpus, they introduce a zero-shot task of cross-lingual vision-language navigation, they propose a visually grounded alignment module, and show the performance of an adversarial domain adaption loss. \n\nAlthough I'm not a domain expert in this area, this seems to be a nice contribution. I like several things about this paper:\n-I think the idea of using images as a pivot for crosslingual grounding is really neat. I'm not a domain expert in computer vision, but this seems to be a really elegant \n-the model performs well, reaching nearly the same level of performance with crosslingual vision-mediated training and training with one language's annotations.\n\nI'm still curious about:\n-is the Chinese attention more accurate because the Chinese utterances were shorter on average?\n-how much do you think the transfer was enabled by the fact that English and Chinese are both analytic languages that are relatively syntactically rigid, particularly in the context of instructions?\n\nThings that need more work:\n-explain this module a bit more:  txt2img module\n\nSmall comments:\n-I assume you used *Mandarin* Chinese; please specify this, as there are many Chinese languages (Mandarin itself has many dialects). You should also state that the instructions are simplified (not traditional)\n-Fig. 2 could have less space between the figures and larger text. The legends in A and B are way too small. It would be prettier too if you remove unnecessary axes and gridlines in C and D. One of the tag labels in D is missing. (I suggest color scheme from \"sns.cubehelix_palette\" that allows for good greyscale and colorblind reading). \n-have you tried a genuine bilingual situation where instructions can be given in either language? That seems to be a natural extension.\n-I'm not sure I understand the relationship between your notation D' and D (is D' the union of all D?)\n-This sentence surprised me: \"nouns and verbs, which often refer to landmarks and actions respectively, are more frequent in Chinese dataset (32.9% and 29.0%) than in English one (24.3% and 13.7%)\": why do you think that is? Do Chinese speakers prefer nouns more? \n-Table 1 should have Stdevs\n-Fix phrasing on: the caption for fig. 6: \"a succeeded instruction\", \"the meta-learner trusts more on the human-annotated Chinese instruction which is of better quality\", \"While the attention on English is more uniform and less accurate than on Chinese.\"\n-typo p 11. \"meta-leaner\"-->\"meta-learner\"\n-please report how much you paid your workers, how many there were, etc. (can be in an appendix)"}