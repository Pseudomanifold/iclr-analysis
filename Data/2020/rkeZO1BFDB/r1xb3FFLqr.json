{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a new dataset for vision-language navigation, which they frame as cross-lingual understanding benchmark. They evaluate some pre-existing methods adapted to the cross-lingual VLN problem as baselines.\n\nI personally do no think that this task will help the community make further progress either in cross-lingual understanding or in multimodal learning compared to already existing benchmarks. This is a very specific task, which merges two important problems of natural language understanding but with little motivation as to why the combination of the two makes this problem even more interesting. The cross-lingual understanding and multimodal problem are problems that are already difficult enough and unsolved, that at this point, I do not see the value of combining both as done in this paper.\n\nThe paper also lacks novelty in the methods proposed, as they apply previous approaches from Ganin et al. in their specific setting, which has been used in other bimodal setting such as unsupervised machine translation. In summary, this paper is well written but does not provide enough technical novelty or an interesting enough problem to solve to be accepted at ICLR."}