{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a classification model focused on interpretability. The model, Explaining model Decision through Unsupervised Concepts Extraction (EDUCE), is applied to a text classification task, while the authors argue in the appendix that this is also applicable to a wider problem, such as image classification. \n\nThe model is composed of three parts: the first part is detecting salient spans of text relevant to the text classification problem, the second part assigning each salient span a concept label, and the third part which does the classification task based on the binary concept feature label. The models\u2019 loss is composed of two parts: (A) minimizing the cross-entropy of text classification loss and (B) minimizing the cross-entropy of concept classification loss. For (A), as the first and second part of the model introduce discrete choices, they use a RL with Monte-Carlo approximation of gradients. \n\nThe system is evaluated under two measure: 1) classification accuracy and 2) concept accuracy. They define the concept accuracy as follows: after training, they train a classifier that takes output (in the form of <salient span, their concept label>) of the model from the test portion of the data. They split this output into train and test, and report the test accuracy. This aims to show how consistent is the labeling of the salient spans for different methods: if the concept label set correctly merged together semantically similar spans, this \u201cconcept accuracy\u201d would be higher. This is a new metric they are proposing. While it is interesting, I would like to see *some* studies on how this correlates with human\u2019s judgements on how interpretable the model is. The paper is introducing a new measure *and* new model, and it\u2019s hard to be persuaded the model is doing well based on this new measure, when there is little ground to know what this measure really measures. \n\nOverall, I\u2019m not impressed with the models\u2019 performances. The aspect rationale annotated beer sentiment dataset, presented by Lei et al (2016), has provided one of few opportunities to evaluate interpretability / rationale model quantitatively. The paper evaluates on this measure, which is included in the appendix, and the results are pretty disappointing compared to the existing models such as Lei et al\u2019s initial baseline or Bastings et al. While the paper argues this method isn\u2019t necessarily designed for this task unlike the other methods, I\u2019m not sure this is necessarily the case. Bastings et al could be applied to other tasks that model is evaluated on, such as DBPedia and AGNews classification. The difference comes on how easy it is to interpret the methods, as these other rationale-based text processing methods would make use of captured words, while EDUCE would make use of detected \u201cconcept\u201d clusters. Currently, the only real baselines are the ablations of its own model. \n\nTable 3 is quite interesting, different \u201cconcepts\u201d capture different aspects fairly well. \n\nNot having a concept loss actually helps the classification accuracy. Would the concepts learned without concept loss qualitatively very different? This goes back to my original point that their new measure of \"concept accuracy\" is vague. \n\nOther comments and Q: \n- Figure (3), the visualization is a bit confusing cause it is unclear whether it is each span is a set of spans or a single span. Also, I would recommend making figures colorblind friendly, if possible. \nQ: what kind of classifier was used for the evaluation metric \u201cconcept accuracy\u201d classifier? I don\u2019t think it\u2019s mentioned. \nQ: why are you sampling a test set for DBPedia experiments? Is it for efficiency reason?\nQ: how sensitive is model\u2019s performance to the hyper parameters, especially the number of concepts?\nQ: the current baseline classifier is a simple BiLSTM one, which definitely perform a lot worse than recent pre-trained LMs such as BERT. Would it be easy to use this method on top of richer representation such as pertained LM outputs?\nQ: how would this connects to saliency map literature in computer vision? I guess these would be mostly \u201ca posteriori\u201d explanations? Discussion would be helpful. \n"}