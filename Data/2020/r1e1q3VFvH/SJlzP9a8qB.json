{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a regularizer that can quantize weights to certain bits. To further reduce the dis-match between float weights and quantized weights, a method that can adaptively change lambda is proposed. \n\nThe proposed method is simple and intuitive (while the method is incremental, I am fine with the novelty). My main concerns are on experiments:\n\nQ1. Why many other methods are not compared in experiments?\n- For example, \"Loss-aware weight quantization of deep networks. ICLR 2017\", \"ProxQuant: Quantized Neural Networks via Proximal Operators. ICLR 2019\"\n\nQ2. Why not perform comparison on more advanced networks?\n- EffcientNet, i.e., \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019\" can achieve much higher accuracy than MobileNet and SuffleNet. \n  EffcientNet Top-1 accuracy 80.3%.\n  Experiments on these networks are more meaningful.\n - Another architecture can be considered is \"Darts: Differentiable architecture search\", with 73% accuracy and is also higher than MobileNet and SuffleNet. "}