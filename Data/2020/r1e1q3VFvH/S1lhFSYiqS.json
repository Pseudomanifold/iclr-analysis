{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: In order to quantize and compress DNNs the paper proposes to use a pipeline consisting of 3 steps. First it uses partial L2 regularization to entirely prune some small weights. Second (most important novel contribution here) it introduces  a mean square error quantization regularization term into the loss being minimized *and* the weight of this regularization term is also learned via gradient descent. Third, it uses lossless compression to further compress the memory footprint of the model. Experimental results when applied on ResNet-18, MobileNet V1 and ShuffleNet models indicate that the proposed method does a decent job of compressing the models without losing too much accuracy despite various levels of pruning/quantization. \n\nEvaluation/suggestions\nUnfortunately the paper does not bother mentioning (in previous work section) or comparing against several recent state of the art results from earlier this year which all seem to do even better. Note that they dont use lossless compression or pruning but those are not the major contributions of this paper and can be easily added to those methods as well. I suggest describing in related works section and a detailed comparison against at least the following (these are the latest , building on a large literature that goes back many years; I picked them since they already achieve better results than those provided here so I feel the authors may benefit from this). \nhttps://arxiv.org/pdf/1902.08153.pdf \nhttps://arxiv.org/pdf/1903.08066.pdf \nhttps://arxiv.org/pdf/1905.11452.pdf \nhttps://arxiv.org/pdf/1905.13082.pdf\n\nNovelty: The novelty of this paper is not very small, but not as much as the authors seem to indicate by ignoring the above recent literature. For example the work on \"differentiable quantization\" above also optimizes quantization as part of the gradient descent. I work still feel that the present paper is novel enough to be interesting but the authors really need to carefully explain their contributions in context. \n"}