{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The authors propose to compress deep neural networks with learnable regularization. Instead of treating the coefficient of the regularization as a fixed hyper-parameter, the proposed method learns the coefficient by adding additional term into loss function. Moreover, the authors apply the proposed method to network quantization and pruning. Experimental results on ImageNet demonstrate the effectiveness of the proposed method. However, the novelty and contribution of this paper are limited. More explanation and experimental results are required. My detailed comments are as follows.\n\nStrengths:\n1.\tThe authors propose a learnable regularization method to learn the coefficient of the regularization. \n\n2.\tThe authors apply the learnable regularization to network quantization, which reduces the mismatch between the full-precision weights and quantized weights.\n\n3.\tThe authors apply the learnable regularization to network pruning, which encourages the weights below the threshold to move towards zero.\n\nQuestions and points needed to be improved:\n1.\tThe novelty and contribution of this paper are limited. The proposed method only introduces an additional term into loss function to control the coefficient of the regularization. The key idea of the proposed method is similar to gradually increasing the coefficient manually during training. However, the authors do not compare the proposed method with such baselines. Moreover, the proposed learnable regularization introduces additional hyperparameter $\\alpha$, which contradicts the motivation of learnable regularization. \n\n2.\tSome method details are missing. For example, how to set the initial values of $\\lambda$ and $\\delta$? This is very important at the beginning of quantization.\n\n3.\tIn Table 1, why the performance of the full-precision ResNet-18 (Top-1 Accuracy: 68.1%) is worse than the one (Top-1 Accuracy: 69.6%) in LQ-Net [1].\n\n4.\t$\\alpha$ in Equation (5) plays an important role in controlling the regularization term. However, the authors do not provide any ablative study on $\\alpha$.\n\n5.\tExperimental results are not sufficient. More recently quantization methods should be considered in the comparison, including LQ-Net [1], Distillation [3], and QIL [4]. \n\n6.\tIn Table 4, the comparison with DoReFa-Net [2] is unreasonable. Compared with the fixed scaling factor in DoReFa-Net, the proposed method learns the scaling factor during training, which can improve the performance of the quantized model. \n\n7.\tQuantization results of SR models on Set-14 are not sufficient. More results on Set5, BSDS100, Urban100, and Manga109 are required.\n\n8.\tFigure 4 should be clear. The legend in the figure is too small to read. \n\n9.\tFigure 7 is confusing. Activations quantization does not change the network size. However, the size of the network with 8-bit activations quantization is larger than the one without activations quantization.\n\nReferences:\n[1]\tZhang, Dongqing, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. \"Lq-nets: Learned quantization for highly accurate and compact deep neural networks.\" In Proceedings of the European Conference on Computer Vision (ECCV), pp. 365-382. 2018.\n[2]\tZhou, Shuchang, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. \"Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.\" arXiv preprint arXiv:1606.06160 (2016).\n[3]\tZhuang, Bohan, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. \"Towards effective low-bitwidth convolutional neural networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7920-7928. 2018.\n[4]\tJung, Sangil, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. \"Learning to quantize deep networks by optimizing quantization intervals with task loss.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4350-4359. 2019.\n"}