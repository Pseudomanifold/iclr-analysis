{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents an approach for quantizing neural networks, in order to make them more efficient. Quantization is difficult because training a quantized net can lead to gradient mismatches that impair accuracy. In this paper, the authors circumvent this difficulty using a three step compression approach that relies on retraining with regularization. First, they prune the synaptic connections of a pre-trained un-compressed network after retraining with an additional L_2 regularizer. Second, they retrain the network with a regularizer that uses the mean-squared error between the weights and their nearest quantized value. Third, they re-train the network with a regularizer that uses the mean squared error between the activations and their nearest quantized value. In some cases they also then use a final compression algorithm to render the quantized network as compressed as possible. They show that the accuracy losses on ImageNet associated with this technique are not too bad, and result in comparable or even better accuracy when compared to the TensorFlow 8-bit quantization, even when using less than 8-bits. \n\nUltimately, this is a good paper. It was easy to read and the ideas were sound. The major thing it is missing, in my opinion, is some analysis, particularly:\n\n1) How bad is the impact on the gradient calculations of this technique compared to other approaches? This is ultimately a key claim in their paper, that they do a better job of following the gradient. But it is not directly demonstrated, only indirectly shown via good accuracy. An explicit comparison in the bias on gradient calculations between their technique and other techniques for quantization would be ideal. One possibility is to do something like show the angle between the weight updates given and the true gradient, for the different approaches. A direct demonstration of this central claim is an important missing factor in this paper.\n\n2) Can any analytical guarantees, or even estimates, be given on the accuracy drop in the event of the gradient being well-followed? It would be nice to know how badly the quantization would affect performance even in the best case scenario, for comparison sake.\n\n\nMinor items:\n\n- Figure 2 doesn\u2019t add much. It could be removed to make space.\n\n- It would be easier to see what is going on in Figure 5d if the y-axis range were reduced and the peak clipped. As it stands, it looks like all the weights are zeroed out.\n\n- Why is Figure 6 presented as a bar graph rather than a table? "}