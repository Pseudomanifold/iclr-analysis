{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Authors proposes an interesting statistical method to detect saliency in images. Authors provides a specific estimator that is fast to compute and characterize its performance w.r.t. parameters.\n\nMy main concern is the experiment section. \"For computational efficiency, we compute saliency maps on a 28 by 28 grid (i.e. \u03b3\u02dc \u2208 R 28\u00d728) although the standard input for VGG-19 is 224 by 224. T\". Shouldn't we do the same thing for all baselines? The seemingly good sailency results might stem from this artifact."}