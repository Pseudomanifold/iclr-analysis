{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Review for \"Posterior Sampling for Multi-Agent Reinforcement Learning\".\n\nThe paper proposes a sample-efficient way to compute a Nash equilibrium of an extensive form game. The algorithm works by maintaining a probability distribution over the chance player / reward pair (i.e. an environment model).\n\nI give a weak recommendation to accept the paper. Although I haven't checked the proofs in detail, the premise seems to be sound - the authors extend model-based exploration results from MDPs to games. The essence of the argument seems to be that the model of the chance player becomes close to d^\\star quickly enough to get a sub-linear bound.\n\nThe main complaints I have about the paper concern clarity.\n\n1. The paper is very densely written. This isn't necessarily bad, but it makes the paper a bit hard to understand. It would benefit the manuscript greatly to provide a figure which shows how the algorithm works for a small toy game. There is space left in the paper, so even a one-page figure would fit in. The figure should show all the major quantities: d, \\sigma, u.\n\n2. The meaning of the quantity \\mathcal{G}_T^i should be more thoroughly described, given it is important in the proof. \n\n3. You define a game with N players, but the algorithm works with 2.\n\n4. Do you really need all the notations in section 2.1? Why not just define the ones used in the algorithm?\n\n5. Can you discuss how large the constants \\xi can become in practice? The definition of \\xi^i seems to be different on page 10 and in Theorem 1 - please disambiguate.\n\nI ask the authors to add a figure and address the issues above. \n\nI am not an expert in this sub-field so I may have missed aspects of the paper.\n\nMinor points:\n- In Figure 1, please say that \"default\" is your algorithm.\n- \"optimal in the face of uncertainty\" => \"optimism in the face of uncertainty\" "}