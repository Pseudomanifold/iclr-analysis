{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information\n================================================================\n\n\nThis paper investigates the use of Thompson sampling in multi-agent reinforcement learning.\nThey present a natural extension of the PSRL algorithm paired with counterfactual regret minimization, rather than expected reward maximization.\nThey provide support for this algorithm's efficacy through a theorem that proves polynomial learning rates, together with empirical evaluation where this approach is competitive with state of the art.\n\n\nThere are several things to like about this paper:\n- This paper is definitely \"groundbreaking\" in that it makes a true extension to the existing literature: PSRL has been relatively well-studied in single-agent RL but never (to my knowledge) in the multi-agent setting.\n- The extensions from single agent to multi-agent are natural, but also non-trivial, and it seems like this is a genuinely novel piece of work that can be interesting to both side (exploration and multi-agent).\n- The general structure of the paper and presentation is good.\n- The support from the theorem is great, and also the empirical evaluation is convincing.\n\nThere are a few places where the paper might be improved:\n- It might be helpful to draw the connection to Thompson sampling more explicitly at the start. PSRL is really an application of Thompson sampling principle, but it is important that it doesn't happen every step but instead on a longer timescale. It might be helpful to cite \"a tutorial on thompson sampling\" Russo et al.\n- Do you think there are promising avenues towards PSRL with generalization (rather than tabular)? It feels like actually this should carry over naturally... so maybe you should mention this?\n- I'm not really an expert on the novelty / impressiveness of this algorithm in the multi-agent setting so cannot fully comment on that.\n\n\nOverall I think this is a really interesting paper that should be of interest to the ICLR community.\nI can't say this with full confidence (especially with respect to the multi-agent side) but I do think it's something that probably would add value to the conference!\n"}