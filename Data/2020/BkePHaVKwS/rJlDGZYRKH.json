{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors propose to learn surrogate loss functions for non-differentiable and non-decomposable loss. An alternative minimization method is used for training the surrogate network and prediction model. Learning surrogate loss functions for different tasks is somewhat novel, although there are some prior works on learning the loss, e.g., [1]. \n\nPros.\n1.\tThe paper is well written and easy to follow.\n2.\tLearning a unified loss for different tasks is interesting. It has the potential to reduce human efforts to design losses.\n\nCons.\n1.\tSome important details of learning the surrogate loss are missing. The function g for extracting the latent error component is not clear. The composition function h is not provided either.\n2.\tThe details of the experiments are not clear. Are the results on the test set? How many independent runs are performed?\n3.\tLearning surrogate loss incurs additional approximation error, time complexity, and model complexity. The benefit of the trade-off is not systematically evaluated.\n\nQuestions\n1.\tThe function g for extracting the latent error component is not clear. Is it required to design for different tasks by an expert specifically? Or, does it have the same differentiable form for different tasks? Please provide the details of the function g for the different losses in the experiments.\n\n2.\tCan the proposed loss work well for multi-class classification tasks? In the experiments, only binary classification is evaluated. Multi-class classification will increase the number of classes, thus increasing the difficulty of approximation. It is better to provide MCR compared with CE on the CIFAR100 dataset for evaluation. Also, please provide the running time of CE and SL-R in the same running environment.\n\n3.\tIs the results in Table 3 on the test set? CE has fewer parameters compared with the proposed loss, why does SL-R have better generalization performance compared with CE? How many independent runs performed in experiments?\n\n4.\t CE does not need training the loss compared with SL-R. Please provide the running time of CE in the same environment for a fair comparison. \n\n5.\tThe time complexity analysis treats extracting function g as a black-box function. However, the complexity of function g depends on the tasks. Please provide a detailed discussion about time complexity for different tasks (e.g., AUC, F1, MCR for multi-class classification, and ranking tasks). \n\n\n[1] Learning Loss Functions for Semi-supervised Learning via Discriminative Adversarial Network, 2017\n\n"}