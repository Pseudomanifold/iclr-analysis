{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\tThis paper takes a different approach for tackling the hierarchical RL problem. Their approach is to decompose the policy into a bunch of primitives. Each primitive acts according to its own interpretation of the state. All the primitives are competing with each other on a given state to take an action. It turns out that these primitive policies can be transferred to other tasks as they represent subtasks of a bigger task. The paper performs extensive experiments to show that this scheme improves over both flat and hierarchical policies in terms of generalization.\n\t\n\tThis paper is well-written. I enjoyed reading it. Almost all my questions and doubts are explained when I read through the paper. The framework about using primitive policy to solve a big task is novel and original. The experiments are nice and convincing.\n\t\n\tMinor comments:\n\t\u2022 I am understanding the methods as a decomposing the policy into components. Different components are combined together using a probability distribution. To balance the competition and overlap of different primitives, different regularization objective are used.  Did you think about other simple methods, e.g., decompose the policy using linear combination work? It may worthwhile to compare your method with this baseline?\n\t\u2022 About the prior p(z), how to choose it in a meaningful way? I do not see why a unit gaussian is a good prior. It seems you want p(z|s) to be as close to a unit Gaussian as possible?\n\t\u2022 Figure 4 does not show a clear cluster structure. Explain more?\n"}