{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The authors conduct an empirical study of the representations that deep neural networks learn as a function of model size, as measured by two different metrics: how quickly performance degrades when random neurons are removed (\"robustness\"), or how similar neurons are (\"redundancy\"). They show that deep neural networks tend to become more robust and more redundant as model size increases, though this is not necessarily true. In general, I believe that this sort of empirical approach is useful, and I appreciate the authors' efforts to study the phenomena of robustness and redundancy empirically.\n\nHowever, in my opinion, I do not believe that this paper is ready for publication in its current state. My central concern is the current imprecision of the paper, which undermines its scientific contribution. Many terms are ill-defined or fuzzy. For example, in section 2, 6 different capacity-constrained features are enumerated but not well-defined, including \"semantically redundant units\" that are \"units which are not linearly correlated to others and which provide a different representation from the previous one, similarly related to the learning task.\" What does it mean to \"provide a different representation from the previous one\" (and what previous one?), or to be \"similarly related to the learning task\"? \n\nCritically, there is a significant amount of speculation throughout the paper that is not clearly delineated from experimental observation.\nThis speculation involves the above ill-defined terms and/or are not backed up. As non-exhaustive examples:\n\n1) In page 3, robustness (as measured by neuron dropouts) is asserted to aid in distinguishing \"redundant functional units\" and \"semantically redundant units\" from other units because of differences in how robustness of those units change with model size; but this is not backed up and as far as I can tell, not experimentally investigated. Similar statements about redundancy and similarity are made in page 3. \n\n2) In page 3, \"L is more robust and equally or less redundant than S\": what is a holistic class representation? Why is that less robust than a \"bag-like representation\"?\n\n3) Section 4 (results) has a lot of speculation as well. For example, \"we take this discrepancy between robustness and redundancy trends as strong evidence that these models... are autoregularizing largely by forming qualitatively different representations... with units that are merely semantically redundant.\" These terms are not clearly defined, and there is scant evidence to back up these conjectures. \n\nAs it stands, it is difficult to accept the paper because of the many speculative claims it contains. The experimental findings are interesting and should be reported; and implications and conjectures should be discussed; but they should be carefully separated. I encourage the authors to critically examine the paper and remove or reword claims that are not directly supported by the results.\n\nOn the experiments: \n\na) For Figure 3, shouldn't the inceptionv3 and inceptionv3-layer graphs be identical when the model size factor is 1x, since the models are identical? What explains the large discrepancies?\n\nb) The number of similar neurons per neuron (e.g., Figs 3c and 4c) should increase with model size just because there are more neurons, right? (Mathematically, if we consider a set of $n$ random vectors in $\\mathbf{R}^m$, then as $n$ increases the number of vectors that correlate with each other should increase.) Is the increase that is experimentally observed any different from this?\n\n==\n\nAnother comment (no need to respond): Experimental procedures should also be well-defined. For example, since neuron ablations are central to the experiments, the procedure should be explained in the main text instead of being buried in a citation."}