{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper present a study on the \"mutual information plane\" during training of neural networks. They concentrate on the effect of a binning procedure, the different actications (relu vs tanh), the effect of noise, early stoping, etc... \n\nIn my opnion, this paper should be rejected because (1) the bining approach make it such that only very small system can be studied, therefore limiting drastically the conclusion that can be done. (2) the paper simply repeat, with more care, some simulation from Tishby and al, and Saxe et al, without much new results.  (3) the paper is ignoring some key results for Mutual Information (see below).\n\nHere are some more precise comments:\n\n* All the  works in the \"bottleneck story\" focused either on small network models with discrete (continuous, eventually binned) activations, or on linear networks. It is hard to see how any of the conclusion can be generalised to anything sensible, and the present paper is working in the same regime. Even if we know what is going on in this simulation, and could understand the effect of binning,  how this can be relevant to deep learning and estimating mutual information in high dimensional problem is unclear. The simulations presented here, in particular, seemed very incremental w.r.t. Saxe and Tishby's\n\n* The work of Gabrie et al seems to be unknown to the authors. In  \"Entropy and mutual information in models of deep neural networks\", in NeurIPS 2018, rigorous results were given for the mutual information and entropy in deep neural networks, as well, as computations of the flow of the Mutual Information. These results showed, for instance, that a wide possibilities of behaviour could be observed, not all of them consistent with the Bottleneck story. Additionally, this paper also showed that compression WAS definitely possible with sigmoid network.  Note that this work computed the MI without any binning thanks to (rigorously proven) replica formula. \n\nWhen computing the effect of binning, etc... it would be natural to compare, again, to some of the rigorous predictions. Since this work was published, I am a little bit skeptical of any claims of computing \"the MI\" right when it is not confronted with the  exact results.\n\n* The most interesting fact seems to be that information plane tracks the clustering of activations. But this is, as the authors pointed themselves, a message sent by the previous paper by Goldfeld et al.\n\n\n\n"}