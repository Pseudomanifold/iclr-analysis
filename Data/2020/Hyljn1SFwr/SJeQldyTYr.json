{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n===========\nThis paper reevaluates the claims made in the debate around Information flow in deep neural networks. Schwartz-Ziv & Tishby (2017) introduced the Information Bound(IB) theory of deep learning with three claims: training of deep learning models occurs in two phases: empirical error minimization (ERM) and representation compression, the generalization performance of deep neural networks can be attributed to the representation compression, and the compression phase occurs due to random diffusion by SGD. However, these claims were refuted by Saxe et al. (2018). Later, Goldfeld et al. (2018) showed using elegant formulation that IB methods were measuring information about clustering of the internal layer representations and not compression. Evaluating the claims made by the papers mentioned above among others, the paper makes the following conclusions: Information plane (IP)is tracking clustering in layers, which in turn is affected by bin size. Mutual Information for discrete values is indeed constant on the two datasets evaluated. However, the information plane does not show constant MI due to loss of precision during the process of discretization into bins. Consequently, smaller bin sizes make the layers drift towards a fixed point in the Information plane, showing many differences between the layers in the IP can be attributed to the bin size selection. The aforementioned effect is influenced by the activation function, with a network showing different patterns based on whether the starting layers had ReLU activation or tanh activation. Different performance by ReLU and tanh is possible due to binning at zero done by ReLU. The authors did not find a universal connection between early stopping and perfect stopping in terms of plots along the IP, but find that at least in the scenarios they tested the behaviour along the IP plane of different layers could be used to prune unnecessary layers and simplify the model. \n\n\nPros:\n(1)\nThis paper addresses a topic of high interest to the DL community which was the subject of much debate. It reaffirms the findings made by Saxe et al. (2018) and Goldfeld et al. (2018) about the IB theory for deep learning, and points to where it could be used. \n(2)\nIt includes comprehensive empirical analysis with many supplementary figures, asking follow up questions arising from the conclusions drawn. For example, the authors claim that the differences between activation functions can be attributed to the binning in ReLU at 0 and Clustering tracked by the Mutual Information can be used for network pruning.\n\nCons: \n(1)\nThe paper is not well written, is full of grammatical errors and hard to follow. Terms like \u201chard IB constraint\u201d are used without definition. Figures are referenced as both fig and Figure and sometimes out of order. The abstract is not summarizing the paper properly and is riddled with errors. The authors need to restructure the text substantially to improve readability.\n(2) \nWhile the reviewers understand the value of independent evaluation of previous claims, especially in the case of a hotly debated topic of great interest, the paper offers limited novelty in findings. Network pruning using IP is the only application offered and it has not been sufficiently explored. For example, it could have been used to investigate redundancy present in well established deep learning models. \n\nMinor comments:\n----------------------------\nAbstract: Information Plain instead of plane\nAbstract: \u201cHowever, this finding is controversial, which has\nbeen supported by e.g. Achille et al. (2017); Achille & Soatto (2017); Noshad et al. (2018) and denied.\u201d This sentence is grammatically incorrect.\n Introduction: \u201c Section 2.3 explains why the IP looks the way it does.\u201d This sentence is vague.\nSection 2.1: should be \u201c784-number of classes\u201d instead of \u201c784-Amount of classes\u201d\n Section 2.1: should be \u201cnumber of bins\u201d instead of \u201camount of bins\u201d.\n Section 2.1: should be \u201cwe chose to use bin size instead ...\u201d instead of \u201cit is chosen to change to use bin size ...\u201d.\nSection 2.2.2: should be \u201cHere we find that TanH-layers make a significant impact on deeper ReLU layers\u201d instead of \u201c Here there is a significant impact of the TanH-layers on the ReLU-layers findable\u201d.\nSection 2.3: repetition of word mutual.\nFigure 4 Caption: Vague statement: \u201cbecause it is from a different notebook that did not gather all data at the same time\u201d.\nSection 2.3: \u201c its filled bins down to two bins - 0 and 1\u201d. Needs a : instead of the -.\nSection 2.3: \u201cmultiple zeros and a one\u201d instead of \u201cmultiple zeros and one 1.\u201d\nSection 2.4.1: Weird phrasing in sentence: \u201cTherefore, there seems to be a not general connection between compression and the first minimum but no connection to the global minimum.\u201d\nSection 2.5: \u201cWe believe\u201d should be replaced by \u201cWe postulate\u201d?\nAppendix D: \u201c The biggest difference is that there is no distinct behaviour for Early Stopping on this dataset.\u201d instead of \u201cThe biggest difference is, that there is no distinct behaviour for Early Stopping on this dataset findable\u201d \n"}