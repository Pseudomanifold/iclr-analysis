{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes an alternative mechanism of training the attention values of a sequence to sequence learning model as applied to tasks like speech synthesis and translation.  During training they compute two forms of attention: (1) the standard soft-attention from a decoder fed with teacher forced output, and (2) the inference-time attention from a decoder fed with predicted outputs.  Their training objective consists of two terms: The first is the token-wise cross entropy loss but by conditioning on the predicted output  but with teacher-forced attention.  The second is a KL distance between the above two types of attention distributions.   Experiments with mechanical  turks indicate that their attention forcing mechanism is strongly preferred over the existing teacher forced output and attention model.  On translation their method provides little or no improvement.\n\nI am inclined towards rejecting the paper because the experiment and related work section still requires a lot of work before 1. The claimed utility of the idea is established, and 2. The novelty over the many existing attention architectures is established.   I elaborate on each of these next.\n\nRelated work: Recently, many papers have directly or indirectly handled the problem of exposure bias that this paper attempts to address.  The paper does not discuss most of these.  Here are some that are missed from the paper:\n\n1.   Sequence level training with recurrent neural networks\nMA Ranzato, S Chopra, M Auli, W Zaremba, 2015.\nThis paper shows that the scheduled sampling method (discussed in the paper) is much worse than a reinforce-based training mechanism of handling exposure bias.  \n\n2. An actor-critic algorithm for sequence prediction\nD Bahdanau, P Brakel, K Xu, A Goyal, R Lowe\n\n3.  Posterior Attention Models for Sequence to Sequence Learning\nS Shankar, S Sarawagi - 2019\n\n4. Latent Alignment and Variational Attention\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander M. Rush 2018\n\n5. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings\nShaohui Kuang, Junhui Li, Ant\u00f3nio Branco, Weihua Luo, Deyi Xiong\n\nExperiments:  Their experiments are rather sketchy and limited.\nThe TTS experiments are only on one dataset.  Their method is compared only with the standard seq2seq learning approach.  Even the scheduled sampling or professor forcing methods are not compared with.  In addition, state of the art TTS methods have gained significantly from hierarchical attention.  As such as far as the TTS task is concerned the significance of the improved quality over a baseline seq2seq method is limited.\n\nFor translation they consider only the English-Vietnamese task whereas there are tens of other translation tasks that are used in recent literature.\n\nOverall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete.\n"}