{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel training scheme for seq2seq models where attention or reference alignment is used in combination with free-running mode for improving training.\n\nThe positives of this paper are that it is well written and very clear. It also is very relevant as seq2seq models can be hard to train and techniques like scheduled sampling and x-forcing algorithms are good heuristics but heuristics none-the-less.\n\nThe downside of this paper is in the experimental results and also complexity. It would\u2019ve been good to see a broader set of experiments to really benchmark attention-forcing from other self-attention models.\n\nAttention forcing also requires a reference or ground-truth alignment, which is often not available. Hence the authors propose to simultaneously train another teacher-forcing model to estimate the reference alignment. However, this would incur twice the computation complexity. \n\nAttention forcing could also be used in conjunction with scheduled sampling. How does that compare with the reported results for attention forcing?"}