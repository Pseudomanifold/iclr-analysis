{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method for fixing exposure bias (ie. training vs generated distribution mismatch) in seq2seq modeling with attention, particularly for the application of speech synthesis where reference alignments are available.\n\n\nRelated Work is missing:\n\n- Another paper that studies fixing exposure bias in seq2seq learning:\n\nWiseman & Rush. Sequence-to-Sequence Learning as Beam-Search Optimization\nhttps://arxiv.org/pdf/1606.02960.pdf\n\n- Other papers that try to enforce attention to attend to specific locations:\n\nBao et al. Deriving Machine Attention from Human Rationales. https://arxiv.org/abs/1808.09367\n\nLiu et al. Neural Machine Translation with Supervised Attention. https://www.aclweb.org/anthology/C16-1291/\n\nYu et al. Supervising Neural Attention Models for Video Captioning by Human Gaze Data. https://arxiv.org/abs/1707.06029\n\nWithout the comparison against other related papers that also aim to supervise attention mechanisms (there are other beyond the ones I cited above)s, it is unclear how much is novel about this paper.\n\n- Furthermore, it is conceptually clear to me that attention-forcing fully matches the training vs generated distributions. The authors should describe in greater detail why this happens this, or whether these distributions are not required to fully match in attention-forcing (and in this case, why this would be desirable).\n\n- The experiments are not very convincing (only 30 human evaluators for Speech synthesis with no other quantitative evaluation, NMT results\u00a0that are not particularly promising).\n\n- Use of non-anonymous github link is questionable for blinded submissions."}