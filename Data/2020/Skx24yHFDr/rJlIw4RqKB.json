{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "First, some minor issues.  I didn't understand equation (3).  It seems to be a variant of equation (4), and seems to be in disagreement with equation (6).  Might be better if the equation was just dropped.  For equation (9), you should have brackets \"()\" around the argument to the exp.\n\nSecond, in terms of comparisons, the paper lacks adequate related work.  Some non-parametric but non-neural\nmodels not implemented in GPUs substantially beat LDA, and will run on all the big data sets you list, though perhaps\nnot quickly!   There has also been a number of neural and hybrid topic models developed.  \nDocNADE and LLA (Zaheer etal), for instance, work very well in PPL.  Then there are many new deep topic models.  Some use the amortised inference that you adopt in section 5.    Some incorporate word embeddings or document metadata to\nfurther improve performance metrics.  Note some of the earlier ICLR/NeurIPS papers with deep models didn't\ndo extensive comparative empirical testing, so may not work well against DocNADE or more recent algorithms.\n\nIn terms of related work, topic models is a bit of a mine-field because there is a huge amount of work in\na huge number of venues, and few authors do a good job of covering related work.  What you have listed are mainly the\nolder works.  Recent work also includes Poisson Matrix Factorisation and its variants, as well as hierarchical\nvariants of LDA, much better than the 2004 paper you mention.\n\nTo do the coherence comparisons, easiest way is to use the Palmetto software.\nYou can also evaluate models by using them as features in a classification task.\n\nIt was interesting that you only did one layer for your networks, i.e.,  equations (4)-(6).  Why was this?\nI would have liked to have seen the impact of more layers. However, your model is remarkably simple \nso if it works well, that is good.\n\nAnyway, the experimental evaluation shows good results on all three datasets for your models, but its hard to be sure\nsince you only have one comparison, an old LDA, and nothing recent.  So promising work, but\nrelated work and experimental work need to be improved.\n\n"}