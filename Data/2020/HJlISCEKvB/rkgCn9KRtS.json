{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to improve GAN by learning a function that splits the latent space in several parts and then feed each part to a different generator, this enables GAN to model distribution with disconnected support. They try the proposed approach on several toy examples where the support of the distribution are disconnected, the data is imbalanced or the support of the distribution intersect,  they in all this cases that the proposed approach improve performance over baseline, in particular the proposed method doesn't produce outliers. They also show how the method is robust to the choice of number of generators used. Finally they show improved performance on more challenging dataset in particular on the CelebA+Photo dataset as measured by FID score.\n\nI'm slightly in favour to accept the paper. I think the idea is well motivated and shows real advantage over other methods on the toy experiments. The major downside of the paper is that the proposed method doesn't seem to improve that much in more realistic setting.\n\nMain Argument:\n+ The idea is well motivated and the paper precisely explain that they try to address the problem of modelling data when the manifold is disconnected and the class are imbalanced. They illustrate how the proposed approach is able to address this problem on some toy example\n\n+ The paper also show how the method is robust to the choice of number of generators\n\n+ Figure 7 and A1 are quite interesting showing how the different generators can learn different part of the data like different class.\n\n- The main counterpoint is that the method doesn't seem to improve the performance that much in more realistic settings. The author point to the fact that this might be due to the fact that the metrics we used are not sensible to outliers. I wish the author had used another metric to show and confirm this hypothesis.\n\n- I found the explanation of the proposed algorithm a bit confusing, it would be nicer if the final loss was clearly defined in the paper and the derivation of the loss explained. In particular in Algorithm 1 I don't understand why $L_{GAN} = D_{real} + (1-D_{fake})$ shouldn't there be some logarithm somewhere ?\n\n- The author propose some measure of standard deviation for the different models but this computed on the three last model checkpoints. It would be much more valuable to compute the standard deviation and the mean with different seeds.\n\nMinor comments:\n- Conflicting notation: In the related work you use $G_i$ to denote a random Gaussian, the same notation is used to denote the generator. \n\n- The part about Machine Teaching and knowledge distillation seem a bit irrelevant to me. I don't really understand what they bring to the paper. Also this would save some space and enable to put the CIFAR10 results in the paper\n\n- In the toy experiments I would be curious to see the results when the different modes have the same probability.\n\n- It would be interesting to have the influence of the number of generators on the FID."}