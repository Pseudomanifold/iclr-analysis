{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "Comments: \n\n-As I understand it this technique is relatively simple.  There are K distinct generators with different parameters.  There is a network NP(z) which maps from an input z to a weighting over the generators.  All of these generators are used during training and the loss on the discriminator is reweighted using NP(z).  \n\n-This doesn't involve any explicit discrete decisions so the entire thing along with NP(z) can be trained end-to-end using the usual backprop.  \n\n-It would be interesting to consider simply doing importance sampling after sampling NP(z) to select the most relevant generators to train on, as NP(z) is presumably much cheaper than G(z).  \n\n-One thing that I'd like to see is a variant where the \"different\" generators share almost all of their parameters, but have different batch-norm \"mean/sigma\" parameters.  This would be the same as conditional batch norm.  I think it would be interesting if this could have some of the strengths of this method while still allowing the generators to share most parameters.  \n\nReview: \n\nThis paper presents a simple yet well motivated new method for helping GANs to model disconnected manifolds.  There are a few more explorations that I'd like to see (discussed in comments), but I still appreciate this paper for directly addressing an important challenge.  The 2D toy experiments do a good job of illustrating why the method helps and also the improvements on the \"disconnected face/landscape\" dataset are quite good.  It is a bit disappointing that it doesn't help on CIFAR10, although I think it's reasonable to leave this for future work.  "}