{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n======== Summary ======== \n \nThe paper proposes a method to decrease the computational complexity of training a student model in a knowledge distillation scenario. \nIt has been shown, in several works, that a student model, trained using knowledge distillation objective, can achieve a better generalization error than the same model trained in isolation and directly on the training data e.g. [Furlanello et al. 2018]. \nHowever, the standard process of knowledge distillation requires two sequential training of the teacher model(s) and then the student model using the teacher\u2019s output(s).\nThe paper adopts a training process similar to SnapShot Distillation (SD) [Yang et al., 2018]. Additionally, this work replaces the *per-sample* teacher labels to a proposed \u201cper-class\u201d label. The per-class label of each class is obtained as the average of the per-sample teacher outputs of that class. \nThe paper tests the idea on several tasks, datasets, as well as architectures and demonstrates improvements on top of training the model with knowledge distillation. \n \n \n======== Strengths and Weaknesses ======== \n \n+ Knowledge distillation is of wide interest.\n+ The experimental effort is laudably comprehensive: For image classification, multiple recent architectures are used on the two datasets of CIFAR100 and ImageNet. For transfer learning, 4 different target datasets are used. The method is also tested on a NLP task.\n \n \nIn page 2, the main contributions of the paper are summarized as 1) being the first method to go beyond singular samples for knowledge distillation 2) developing a computationally-efficient distillation framework that brings significant improvements.\n \n- Regarding the first contribution, the following closely-related paper is not cited. It goes beyond knowledge distillation of isolated exemplars by considering multiple samples and distilling the mutual relations among them to the student network.\n[Park et al. \u201cRelational Knowledge Distillation\u201d, 2019]\nLess similar but still capturing the distribution of teacher outputs are the following works:\n[Xu et al. Training Student Networks for Acceleration with Conditional Adversarial Networks 2018]\n[Wang et al. \u201cAdversarial Learning of Portable Student Networks\u201d, 2018]\n \n- Regarding the second contribution, I believe SnapShot Distillation (SD)  [Yang et al., 2018] already proposes a fast and effective method. \n \n- that being said, the idea of averaging the teacher\u2019s output is different from those works.\n \nPoints regarding experimental setup:\n- Since the performance of different methods are quite close (especially for SD vs DUPS+) , it would have been more conclusive to have multiple runs and report mean and std over those runs for all the methods (at least for CIFAR100 which is smaller and more manageable).\n \n- regarding the class-level teacher labels, both SD and LS are important immediate baselines and as such should be included in all tables. Table 3,4,5 miss SD results.\n \n- Moreover, the following baselines should also be included in the tables for a proper comparison: \n1) standard KD, since the proposed method is supposed to make the training stage of the standard KD faster while maintaining the performance \n2) [Pereyra et al, \u201cRegularizing neural networks by penalizing confident output distributions\u201d 2017] can be seen as a generalization of label smoothing and thereby is a relevant baseline.\n \n- How are the hyperparameters optimized for different algorithms? Was the same budget used for all baselines? How about the search space and intervals? On what metric and data were the hyperparameters optimized?\n \n- \\lambda is mentioned as the hyperparameter for DUPS only while label smoothing can benefit from an optimal weighting as well.\n \nAlso on a more general note:\n- The paper lacks a discussion regarding why removing the sample-level \u201cdark knowledge\u201d and only using the class similarity/confusion (as learned by a teacher) should work better than the standard knowledge distillation.\n \n \n======== Final Decision ======== \n \nKnowledge distillation is a very popular method both in the literature as well as in the real-world applications. Thus, improving its training time is an interesting research direction which can make the paper attractive. The \u201cweak reject\u201d rating is mainly based on the limited novelty due to the large overlap with prior works.\n \n \n======== Minor points ======== \n \nHere, I list some minor points which I hope the authors find useful when updating their manuscript:\n \n- Abstract: the paper\u2019s proposal is stated as \u201cwe present a novel [...] in one generation\u201d. This could benefit from a rephrasing such that an intuition or the high-level idea is better conveyed. \nP.3: section 3.1 has the f function mapping input samples to points in the label space. However, equation 1 assumes f\u2019s output is a probability distribution, to be used in the cross-entropy and knowledge-distillation loss functions. In the end of p.4, f\u2019s output is assumed to be the logits.\nP.4: definition of Y_{y_i} was already given on top of the page but is repeated in the middle.\nP.4: \u201cwe define f...] to be zero\u201d: what does \u201czero\u201d mean here? For instance, in the case of the categorical output distribution, shouldn\u2019t y elements sum to one?\nP.5: while the text suggests that a random subset of samples for each class is used to determine the \u201cteacher\u201d label for that class, the algorithm in page 5 suggests all samples  are used. Which is the case?\nP.9: on which dataset, figure 1 and 2 are produced?\n \nfinally there are some typo including the following:\n\nAbstract: \u201caverage improvement of 1%-2% on various tasks\u201d -> \u201caverage accuracy improvement of 1%-2% on various image classification tasks\u201d\nP.1: \u201cStudent models, with the availability of softened output\u201d -> For classification tasks, student models, with \u2026\nP.1: \u201cIt is ad meaningful objective of\u201d: doesn\u2019t read correctly.\nP.2: \u201cinclude ImageNet\u201d -> including ImageNet\nP.2: \u201cgrammian matrix\u201d -> Gramian matrix\nP.3: \u201cimage classification objects\u201d: rephrase\nP.3: \u201cIn this paper We\u201d -> , we\nP.3: \u201cD \u2208 XxY\u201d -> D \u2282 XxY\nP.4: \u201crandom select\u201d -> randomly\nP.4: \u201cour method do not\u201d -> does\nP.4: \u201cthe all samples\u201d -> all samples\n \n======== Points of improvements ======== \nI think one way to improve the work is to first isolate the novelty of the paper with regards to all the related works and then accordingly divert the discussions in the paper to 1) conceptually motivate those novelties and more importantly 2) have the experiments highlight the empirical improvements brought in by these novelties.\n"}