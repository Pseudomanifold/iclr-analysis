{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to address the extra computational cost of training with knowledge distillation, which is typically done in two stages (first training the teacher model, and then training the student model based on the teacher's predictions). More concretely, this paper builds on the recently proposed Snapshot Distillation (SD; Yang et al., 2018) technique of distilling the knowledge of the same model, albeit obtained from earlier stages of the training process, hence eliminating the need for a separately trained teacher model.\n\nThe key technical contribution is to utilise the predictions of the earlier model (which here functions as the teacher model) from multiple related training instances (i.e. peer samples), rather than only at a single data point. This takes the form of the average probability distribution (i.e. the whole softmax output) assigned by the earlier model to the peer samples that share the same label as the current training instance, and use this as the target teacher probability distribution. Experiments on image classification, out-of-domain image classification, and language modelling demonstrates improvements over the baselines.\n\nOverall, while the proposed approach is an interesting one, I have some serious concerns regarding the presentation and experiments, as listed below. I am therefore recommending a \"Weak Reject\" ahead of the authors' response.\n\nConcerns:\n1. I am not convinced about the strength of the baseline models, especially in the language modelling experiments. The reported perplexities in Table 5 (the best one was 85.7 perplexity) are generally much worse than various published work, such as the AWD-LSTM (Merity et al., 2018) and a well-tuned LSTM (Melis et al., 2018), both of which achieved <60 perplexity on Penn Treebank test set. This makes it unclear whether the improvements come from the DUPS approach, or simply from the use of weaker (e.g. under-regularised) baselines. Rather than just reporting the authors' own implementation, I would suggest adding the DUPS technique on top of the current state of the art models to see whether DUPS offers complementary gains on top of strong baselines.\n\n2. While I understand how DUPS works from the equations, the paper offers very little explanation or intuition about why DUPS works better than snapshot distillation. Why does DUPS result in a better teacher model than using snapshot distillation? Section 3.4 mentions that one of the weaknesses of snapshot distillation is that it relies on a cyclic learning rate, but it is not explained how using DUPS can address this problem. More confusingly, the paper also ends up adding the same cyclic learning rate to the DUPS model (called \"DUPS+\" on Table 1), even though the cyclic learning rate was claimed to be problematic in the first place. In short, it is not very clear: (i) what problem this paper is trying to solve, and (ii) why the proposed DUPS solution is a good idea to solve this problem.\n\n3. Related to point 2 above, the paper is not very self-contained. Some of the ideas are hard to follow for readers who are not very familiar with snapshot distillation (Yang et al., 2018).\n\n4. The analysis presented at the Discussions section (4.4) also fails to adequately explain why DUPS is beneficial compared to the baselines.\n\nMore minor comments:\n1. The use of the term \"logits\" is very confusing here. Typically \"logits\" refers to the unscaled tensor before the softmax activation is applied. It seems that what the paper means here is not really the \"logits\", but rather the output of the softmax function, which forms a valid probability distribution (distillation minimises the cross-entropy between the teacher and the student's probability distributions; both of which are obtained after applying a softmax activation to the logits).\n\n2. In Section 3.1, it is mentioned that \"[The KD loss] is typically computed by cross entropy ... or Kullback Leibler divergence ... between the logits produced by the teacher model and the student model\". This dichotomy is quite misleading, since minimising the cross entropy is equivalent to minimising the KL divergence (up to a constant). \n\n3. In section 4.1 (Tasks and Datasets), the task \"Natural Language Processing\" should be referred to as \"Language Modelling\", as NLP is much broader than just language modelling.\n\n4. This paper can benefit from carefuly copy-editing to improve clarity. Some examples:\n\na. In Section 3.3, \"we random select n peer samples ...\" should be \"we randomly select\".\n\nb. In Section 3.4, \"are a random subset of the all samples ...\" should be \"are a random subset of all the samples\".\n\nc. In Section 4.1, \"... the popular subset ILSVRC2012 which containing ...\" should be \"which contains\".\n\nd. etc.\n\nReferences:\nGabor Melis, Chris Dyer, and Phil Blunsom. On the State of the Art of Evaluation in Neural Language Models. In Proc. of ICLR 2018.\n\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Models. In Proc. of ICLR 2018.\n\nChenglin Yang, Lingxi Xie, Chi Su, and Alan L. Yuille. Snapshot Distillation: Teacher-Student Optimization in One Generation. In Proc. of CVPR 2019.\n\n"}