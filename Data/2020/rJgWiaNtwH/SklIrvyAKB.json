{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Summary:\n\nThe authors propose changing the objective function used to train VAEs (the ELBO) to an alternative objective which utilizes an L1-norm penalty on the aggregated variational posterior means. The authors motivate this modification through the maximum mean discrepancy but I was unable to follow this argument --- adding additional details to the paper for this argument would greatly help. The empirical evaluation shows that the proposed technique is able to improve utility on downstream classification tasks but ultimately has significant issues with the experimental setup.\n\n\nOverall:\n\n1) One general issue I found with this work is the claim that the KL divergence is the overarching cause of posterior collapse. This remains a popular story within the community but existing work has provided evidence that the actual cause is not so simple. For example, [1] showed that with a sufficiently powerful decoder posterior collapse may occur even when training with marginal log-likelihood. For the authors reference, [2] is a concurrent ICLR submission which breaks down various categories and causes of posterior collapse.\n\n2) In several places the authors write that a smaller KL term leads to less informative latent variables. While I understand the intention of the authors I believe this argument is too imprecise. For example, one can encode an arbitrary amount of information into a single real-valued scalar variable (even doing so when that value is << 1). In which case, all dimensions but one can be collapsed completely (significantly reducing the KL) and the final dimension can feed all information into a decoder powerful enough to decode it.\n\nFurthermore, the breakdown of equation (3) is a bit troubling. The same analysis could lead one to assume that L2 regularization forces parameters to be zero leading to problematic predictions. It is important that the balance between the KL term and the conditional log-likelihood is considered together.\n\n3) Page 1, footnote 3 refers to the KL divergence as \"one such term\" for regularizing a VAE. The KL divergence is necessary for the VAE objective and any other regularization term would imply a difference model. In fact, in this work the model is referred to as the $\\mu$-VAE; while relatively minor I would argue that this is inaccurate as we are no longer optimizing a VAE objective or even recovering a tractable lower bound on log marginal likelihood. This leads into my primary concern, which is that it is not clear what the objective function to be optimized represents or indeed what the advantages in doing so are compared to ELBO.\n\nReplacing the L2 norm of the mean with the L1 norm of the aggregated means (note that this is not merely replacing the L2 norm in the per-datapoint KL with the L1 norm) changes the objective significantly but is not discussed in enough detail in the paper. The authors claim that this is emulating MMD but I am not sure exactly what they mean. MMD depends on a feature map lying in a reproducing Hilbert space and measures the distance in expectation between the features of two distributions. What is the feature map being used here? How is this objective related to MMD exactly? Moreover, the remaining terms in the KL divergence are retained in the objective which adds further confusion.\n\nOverall, I felt that the justification for the change to the objective function was lacking and am ultimately unconvinced that this is a sensible alternative to training with ELBO.\n\n4) How exactly is the latent clipping enforced? There are several possible approaches which may impose different training dynamics.\n\n5) I feel that Section 4.1 is a good inclusion as it allows us to better understand the effects of each of the changes to the training regime. Unfortunately, I felt that the conclusions I drew from these experiments mostly suggested that latent clipping is likely to be most responsible for the performance gains observed in the $\\mu-$VAE. I suspect that the reason we don't see the $\\mu-$VAE without latent clipping is that it experiences unstable learning dynamics and fails to converge. Latent clipping is not applied to the standard ELBO objectives in later empirical experiments --- I believe the authors should also compare downstream utility under the ELBO objective with latent clipping to convince the reader that the $\\mu-$VAE objective really does provide benefits.\n\n6) The empirical evaluation is very limited. The authors compare four different VAE-like objectives on MNIST and FashionMNIST. From the appendix, it seems that the optimizers for all models share the same hyperparameters --- in my experience VAEs are susprisingly sensitive to settings of learning rate and batch size (even when using Adam) and so I would expect a grid search over at least the learning rate.\n\nAdditionally, it seems that the reconstruction loss is simply the L2 distance between the decoder outputs and data and does not include rescaling by the observation noise. This is essentially the same as keeping the observation noise fixed to 1 and has serious ramifications on the quality of the learned latent space [3].\n\n7) The experiments focus on downstream utility which I agree is a valuable metric. However, it would also be valuable to compare the ELBO of these trained models. Even if the $\\mu-$VAE is not trained to maximize likelihood of the same probabilistic model, one might hope that if it reduces posterior collapse it could lead to a better model.\n\n\nMinor:\n\n- In the abstract you claim \"this problem is exacerbated when the dataset is small and the latent dimension is high\". Could you please provide a reference for this claim.\n- \"encoder learns to map the data distribution $p_d(x)$ to a simple distribution such as Gaussian\". This is not quite correct, each data point is mapped to a simple distribution but the data distribution is mapped to a more complex distribution (i.e. the aggregated posterior $q(z) = E_x[q(z|x)]$).\n- What is meant by an \"approximately diagonal co-variance\" at the top of page 5.\n\nReferences:\n\n[1] Fixing a Broken ELBO, Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, and Kevin Murphy\n[2] The usual suspects? Reassessing Blame for VAE Posterior Collapse, Anonymous, https://openreview.net/forum?id=r1lIKlSYvH\n[3] Understanding posterior collapse in generative latent variable models, James Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi\n\n\nNote: This review was edited after release to remove a duplicate paragraph. Paragraph indices were updated accordingly.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}