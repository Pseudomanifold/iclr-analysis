{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to modify the ELBO loss for VAE in order to learn better latent representation of the data. There is a large literature on this topic and the related work section should be written with more care and details. Some references are missing:\n[a] Fixing a Broken ELBO by Alemi et al.\n[b] Learning Disentangled Joint Continuous and Discrete Representations by Dupont\nOn a general level, the motivation for this work is not completely clear. As mentioned in the related work section, one could use a more complex prior such as a mixture of Gaussians or an architecture like [b] to do clustering. At least, in the experiment section, results shuold be compared with these natrual approaches. It should be easy as [b] for example provided his code and gives results on the same datasets as those used here.\nAnother remark is that the title is quite misleading. The connection with MMD is very weak in my opinion. Indeed modifing the KL term in the loss by using MMD has already been proposed in Infovae (last reference of this paper) and in InfoVAE a MMD-VAE is also introduced. This connection should be explained and comparison should be given with this work."}