{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to modify the KL term ($KL(q(z|x)||p(z))$) in VAE so that instead of requiring q(z|x) to be close to p(z), we require the aggregated posterior \\int q(z|x)p(x)dx to be close to p(z). It is argued that the original VAE objective leads to posterior collapse because it requires q(z|x) to be close to p(z), leading to similar latent code across different data points. \n\nI find the motivation in this paper rather weak, and the authors not sufficiently familiar with prior work. Therefore I could not recommend acceptance at this time.\n\nMore specifically, \n1. the fact that minimizers of E_p(x)[KL(q(z|x)||p(z))] are posterior collapse solutions does not imply this term is a bad regularizer, since in general, the optimization process will not end up in such parameters becaues of the data fitting term. In fact, [1,2] showed that this regularizer should have a desirable pruning effect, removing unneeded latent dimensions while keeping the useful dimensions intact. See also the information-theoretic interpretations of ELBO in e.g. [3,4]. [2] also gives a more rigorous discussion on the relation between ELBO and posterior collapse.\n2. The authors propose to match the marginal distributions in latent space. This idea is explored in previous work such as Wasserstein and adversarial auto-encoders. None of this type of work is discussed in the text or included as baselines.\n3. The proposed regularizer, as shown in eq (6), do not fully emulate the behavior of MMD (or any other divergence measure between the aggregated posterior and prior) as claimed: e.g. mu^{(i)}_d can be arbitrarily far from origin. Thus justifying (6) as such is not convincing.\n\nRegarding the experiments, there is a notable lack of baselines as mentioned above; also, for sample quality it is desirable if quantative measures (e.g. FID or inception score) are included.\n\nFor future improvements, the proposed objective could be worth exploring if a new justification is found. Also, the empirical evaluation should be more thorough.\n\n# References\n\n[1] Diagnosing and Enhancing VAE Models, ICLR 19.\n[2] Understanding Posterior Collapse in Generative Latent Variable Models, NeurIPS 19.\n[3] An information theoretic analysis of deep latent-variable models. arXiv 1711.00464.\n[4] Elbo surgery: yet another way to carve up the variational evidence lower bound. AABI workshop, NIPS 16."}