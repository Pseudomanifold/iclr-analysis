{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tries to solve the protein-legend binding prediction problem in the computational biology field. It uses the learned embedding for protein and legend, separately, from two published papers. Then those two embeddings were inputted to another deep learning model, performing the final prediction. Tested on one dataset, it shows the proposed method can outperform the other baseline methods.\n\nThe paper should be rejected for the following reasons:\n1. The idea of the paper is not interesting and novel enough. It only used the results from two published papers and then applied another deep learning model on them. The novelty of the paper is limited.\n2. The experiment part is not that comprehensive. It indeed performs enough ablation studies. However, all the legendary methods in the computational biology field are not included in the comparison. \n3. The experiments are only performed on one dataset. Usually, for application papers, the experiments should be performed against at least 2 datasets to avoid bias.\n4. The discussion part is not well-developed. For the paper which focuses on only one task, the more in-depth discussion is expected beyond the simple discussion of the performance. For example, the authors may try to explain the result: why the model used embedding from unsupervised learning is better than the hand-crafted features. Since they shared the same model, the unsupervised embedding should contain more information. Then, what is the additional information? \n\n\nSome further questions and comments:\n1. What's the sequence similarity of the 1226 proteins?\n2. Can the model generalize well to a completely new protein? \n3. What's the detailed performance of the model on proteins belonging to different families? \n4. I guess if the authors check the detailed performance, they will find nonuniform performance across different proteins. I think the authors can also further investigate that.\n5. Can the authors train the embedding model as well as the classification model in an end-to-end fashion? This can be more interesting.\n6. One big problem of the assay data is that it would not be able to provide the structure information of the interaction between the protein and the legend. Usually, it is a very important piece of information for the biology people. The computational methods based on the assay data will also inherit the flaw. "}