{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: The problem addressed is how to train a DNN to learn a hierarchical representation of the input that has heterogeneously annotated labels at various levels of granularity. After training the network\u2019s goal is to emit sequentially finer grained labels with corresponding confidence. The authors use information theory to propose a network typology of information bottlenecks with skip connections to achieve this nested learning problem.  \n\nDecision: weak reject. \n\nReason: The proposal of learning a hierarchical representation is not new. Nor is the architecture. I think the interesting points (that are not really flesh out as much, but appear to be in the auxiliary material - section B.2) is the training regime. I would\u2019ve liked to have seen more of what the role of the training regime is on the outcomes and how the network\u2019s gradient\u2019s behave in different regimes. But in general, the paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging. The paper also defines too many concepts for every proposition it wants to support, making it cognitively costly to follow. \n\nFeedback:\nPleasure reading the paper. Few points of feedback:\n\n- Perhaps show the network gradients to help understand the dynamics of the learning\n- What do you think is the role of the training regime (section B.2) on the outcomes you are observing? Do you think it would be worth observing the effect of changing the learning regime (and the accompanying gradient) on the outcomes?\n- Generalization to any number of nested labels is not demonstrated\n- The empirical demonstration of contribution of skip connections is not too  \n- Corollary to above, what do you think would be the role of attention in the hierarchical representation learning?\n\nQuestions:\n1- Not clear why complementarity is a necessary condition of learning (section 3, before definition 1). Take for example the vehicle, wheels, truck example in figure 1. Learning F2 (wheels) features isn\u2019t conditioned on correctly learning F1 (vehicle). In fact, could it be satisfactory, to first order approximation, to assume that learning finer-grained features first (roundness of wheels) and then combining lower level features (in what you refer to as Markov chain) may result in equal if not better accuracies? Is it possible to test this?\n\n2- Not clear why calibration of outputs (section \u201ccombination of nested outputs\u201d in p 5) is an approximation of P(Y_i=q).\n\n3- Its not clear to me why the proposed rejection calibration method as a way of handling overconfidence is the right approach. Why this solution? Why not use, for example, regularization instead?\n\n4- Table 1 of results. As the distortions increases the relative improvements of the nested learner appears to increase in CIFAR-10 results. Can you demonstrate this is not an artifact of the distortion generation strategy and is indeed a stable observation?\n\n5- why is the marginal accuracy improvements so much larger for going from a budget of 1 to 2 than 2 to 5 in all cost functions? Does this not refute the claim that the nested model \u201cgradually breaks\u201d?"}