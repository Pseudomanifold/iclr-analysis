{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Nested learning for multi-granular tasks\n\n1. Summary\nThe paper considers a framework for classification with a hierarchy of labels [from coarse to fine]. The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The objective function is the standard classification loss. The experiments show that coarse labels help learning and can improve label efficiency, i.e. don\u2019t need all fine labels to get good classification performance.\n\n2. Opinion and rationales\n\nWhilst I think the execution of ideas is good and the motivation is very practical, I\u2019m leaning towards \u201creject\u201d for this paper due to the reasons below. I welcome the authors\u2019 clarification and am willing to reconsider my view.\n\ni. The paper justifies the proposed architecture as successive information compression into label embeddings using some entropy based criteria (as in information bottleneck literature). This ensures the relationship of the entropies between the label layers. However, I\u2019m not sure this justification is necessary (albeit being a valid one) given that the training/later sections do not come back to this justification.\n\nii. The novelty of the proposed architecture and training approach is low. The network is a nested structure of successive classifiers. The proposed calibration using rejection class and temperature scaling is not new.\n\niii. it would be better if the proposed architecture + method are validated on more real-world datasets with a more natural label grouping scheme, and compared to alternative architectures, e.g. multi-task learning with a shared network except the output layer.\n\n3. Minor details\n\nSome citations should be enclosed in brackets"}