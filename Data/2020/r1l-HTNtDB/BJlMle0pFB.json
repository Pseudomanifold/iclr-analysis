{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an off-policy model-based reinforcement learning approach. The proposed paper combines several techniques together. The general framework fits a maximum entropy reinforcement learning, and the policy and q-function updates are formulated in a soft actor-critic manner. To leverage the learned dynamics, the proposed approach adopts value expansion to substitute the learned reward function and forward dynamics function in the Bellman equation for the value function. Also, the imagined roll-out could be used to update the q-function.\n\nI feel the novelty of this approach is a bit limited, considering that the novelty mainly comes from embedding a more powerful maximum entropy RL formulation upon the value expansion [Freinberg et al., 2018] and stochastic value gradient[Heess et al., 2015], rather than from the perspective of model-based modeling. Even though the experiment result is still very promising and it's good to see the method could outperform SLBO in 3 out of 4 cases.\n\n \nSome other comments:\n- The presentation in Sec 3.2 is not clear. Hard to follow without looking into the MVE paper. The definition of \\tau and H should be given at that place. Also, the authors claim Q is updated with real *transition*, but later part states only initial tuple is from real data.   \n- Up to how many roll-out step k could the model reliably work upon should be clearly stated.\n- For Walker2d, more training steps need to be shown.\n- Robustness of the algorithm: are the results in Figure 1 derived from different random seeds? If so, how many of them?\n- The experiment is only conducted in 4 control domains. The authors may consider to include more task domains, e.g., swimmer, and humanoid as well.\n- Are the steps in Figure 1 correspond to only real simulator steps or total steps including imagined ones?\n- The curves in Figure 2 are not shown with an uncertainty plot."}