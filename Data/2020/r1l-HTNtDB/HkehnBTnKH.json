{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new model-based method that combines the maximum-entropy objective and stochastic value gradient methods. Based on this method, the authors present the Soft Stochastic Value Gradient method (S2VG). Experimental results show that S2VG beats SVG, SAC, DDPG, and SLBO on three MuJoCo tasks.\n\nFirst of all, the paper lacks a good motivation. SVG already directly embed the model into policy improvement. Then the novelty of this paper is adding an entropy regularizer to improve the robustness under the model estimation error. However, this point is not clarified in the paper. As shown in  Figure 1(a),(d), the performance of S2VG is not stable. Thus I do not find the value of adding a policy entropy regularizer on the SVG method.\n\nSecondly, the comparison is not fair. S2VG should be compared with STEVE ((Buckman et al., 2018), the state of the art model-based method. That is, the claim that S2VG beats state-of-the-art model-based methods is not appropriate.\n\nFinally, the performance improvement is somewhat weak as S2VG only beats other baselines on three MuJoCo tasks.\n\nQuestion:\n\n(1) Why is the recursive value gradient missing in Eq. (2)? \n(2) It is interesting that S2VG outperforms SVG significantly. Does the improvement come from better exploration?\n(3) In Figure 2(c), the bias of H=2 is larger than that of H=5. is there any explanation?"}