{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an end-to-end approach for clustering. The proposed model is called CNC. It simultaneously learns a data embedding that preserve data affinity using Siamese networks, and clusters data in the embedding space. The model is trained by minimizing a differentiable loss function that is derived from normalized cuts. As such, the embedding phase renders the data point friendly to spectral clustering. \nThe paper follows the general setup of deep clustering: map data to a feature space while maintaining data distributional characteristic, and make data clustering-friendly in the feature space. The authors use Siamese networks for the first part and use a normalized-cut motivated loss for the second part.  The choices are reasonable and the loss is somewhat novel. \nCNC is evaluated on standard datasets, including MNIST, Reuters, CIFAR-10, and CIFAR-100. The results are impressive. However, deep clustering has been around for quite a few years. It might be time to move on to more challenging benchmarks. \n"}