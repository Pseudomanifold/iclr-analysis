{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper suggests a differentiable objective that can be used to train a network to output cluster probabilities for a given datapoint, given a fixed number of clusters and embeddings of the data points to be clustered. In particular, this objective can be seen as a relaxation of the normalized cut objective, where indicator variables in the original formulation are replaced with their expectations under the trained model. The authors experiment with a number of clustering datasets where the number of cluster is known beforehand (and where, for evaluation purposes, the ground truth is known), and find that their method generally improves over the clustering performance of SpectralNet (Shaham et al., 2018) in terms of accuracy and normalized mutual information, and that it finds solutions with lower normalized cut values.\n\nThe method proposed in this paper is very simple and appears to work well, and so this paper represents an important contribution. However, there are some issues with the presentation that I think should be fixed before publication:\n- Equation (3): I'm not sure I understand the sum over z; don't we just want w_{ij} Y_{ik} (1 - Y_{jk})?\n- Equation (6): I don't think the final objective should be presented as an expectation. It is rather the quotient of two expectations. In general, it might be better to just present the objective as a relaxation of the normalized cut objective.\n\nA question regarding the results and parameterization: was using Gumbel-Softmax necessary to get good results? Did ordinary softmax not work?"}