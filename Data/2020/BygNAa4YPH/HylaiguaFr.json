{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThe paper investigates both Out of Distribution (OOS) and Out of Episode (OOE) tasks. The distinction is that OOE samples are from same dataset but come from classes not represented by the support set. The paper shows that existing confidence scores developed in the supervised setting are not suitable when used with popular few-shot classifiers. The paper proposes two new confidence scores, -MinDist and LCBO. \n\nStrengths\n\nThe paper proposes benchmark datasets for out-of-distribution detection of few-shot classification. These datasets are Omniglot, CIFAR100, miniImageNet, and tieredImageNet.\n\nThe paper presents baseline results for two popular few-shot classifiers \u2014 Prototypical Networks, and MAML.\n\nThe paper shows that a simple distance metric-based approach improves the performance on both tasks.\n\nIt also proposes a parametric, class-conditional confidence score that takes a query x and a class c, and yields a score indicating whether x belongs to class c.\n\nWeaknesses\n\nThe paper is very specific to two few shot learning baselines. The question is how representative these two baselines are. Will the distance metric help for other few-shot classifiers?\n\nWhat is the motivation to detect OOS and OOE in the few shot setting given the accuracy is already low?\n\nThe contribution is mainly the metrics. \n\nOverall, the paper does not have enough interesting results for acceptance.\n"}