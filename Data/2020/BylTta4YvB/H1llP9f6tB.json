{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper provides an empirical evaluation of commonly used discriminator training strategies in estimating the Wasserstein distance between distributions. The paper finds that methods motivated from optimal transport theory, e.g. c-transform and (c,\\eps)-transform, perform better in evaluating the Wasserstein distance than methods commonly used in WGAN practice such as weight clipping and gradient penalty. However, when deployed in WGANs as the discriminator training strategy, these methods do not generate images as high-quality as the gradient penalty method. \n\n[Pros]\nThe question considered in this paper, i.e. how well does various discriminator strategies (as proxies of the infeasible all 1-Lipschitz discriminator) perform for *evaluating* the Wasserstein distributions, is important for strengthening our understanding of generative models. The main result that methods that are better at computing W may not be better at generating images is interesting, and agrees with the theoretical insight (e.g. Arora et al. 2017) that *non-parametric* minimization of W may not be a good explanation for the generative power of WGANs.\n\n[Cons]\nI have concerns about the specific setting of \u201cmini-batch distance\u201d considered in this paper, in that whether it is really a sensible task (and can say anything about computing W) given the curse of dimensionality of estimating W from samples. The paper did not really discuss this issue, and from my own thoughts I don\u2019t think the task avoids this issue. \n\nFrom my understanding, the \u201cApproximation\u201d experiment does the following:\n(1) Set (\\mu, \\nu) to be a random split of a dataset and consider them to be the populations. Let\u2019s let f_\\star denote the (ground truth) optimal discriminator between (\\mu, \\nu).\n(2) Train discriminators (f_wc, f_gp, f_c, f_ceps) (using the different algorithms) from training batches from (\\mu, \\nu).\n(3) Evaluate <f, \\mu\u2019_l - \\nu\u2019_l> where (\\mu\u2019_l, \\nu\u2019_l) are fresh test batches from (\\mu, \\nu) and f is one of the above trained discriminators.\nIn comparison, the \u201cground truth\u201d computes W(\\mu\u2019_l, \\nu\u2019_l) = <f_l, \\mu\u2019_l, \\nu\u2019_l> from the POT package (though not necessarily through explicitly computing f_l.)\n\nThe issue with this is that we expect the method to perform well if f ~= f_l, which can be achievable if all the f_l\u2019s are similar (and hopefully they\u2019re all approximately equal to f_\\star.) However I don\u2019t think this is true -- as (\\mu\u2019_l, \\nu\u2019_l) are samples, and because of the curse of dimensionality, we should expect the f_l\u2019s to be quite different from each other. (Otherwise if they\u2019re really just ~= f_\\star, then we can use standard concentration to show the W(\\mu\u2019_l, \\nu\u2019_l) ~= W(\\mu, \\nu), which we know is not true from curse of dim.)\n\nGiven this, I don\u2019t think the task of comparing <f, \\mu\u2019_l, \\nu\u2019_l> with the POT results really says anything about their power in computing W. I would be glad though to hear back from the authors to see if my understanding is accurate, and adjust my evaluation from there.\n"}