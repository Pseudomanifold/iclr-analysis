{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:  This paper proposes defining a regression model of the form $p(y|x) = \\frac{\\exp{f(y,x)}}{Z(x)}$ where $Z(x) = \\int_{y} \\exp{f(y,x)} dy$.  The normalizing constant is analytically intractable, however, and the paper proposes evaluating it via importance sampling: $Z(x) = \\int_{y} \\frac{q(y)}{q(y)} \\exp{f(y,x)} dy = \\mathbb{E}[\\frac{\\exp{f(y,x)}}{q(y)}]$ where $q(y)$ is set during training according to the observed label.  This model is validated on several vision tasks including object detection, age estimation, pose estimation, and visual tracking.  Results show competitive to superior performance against recently-proposed baselines.    \n\nPros:  Defining more expressive predictive models---ones that can handle multiple modes and heteroskedastic response noise---is an important project, and this paper does seem to make progress towards this goal, as seen in Figure 1.  Despite computational downsides, the paper proposes some efficient work-arounds (e.g. importance sampling, introducing the label deep into the architecture) that allow the model / method to be scaled to realistic vision tasks.  The performance in the experiments seems good (but I was not previously familiar with these tasks).\n\nCons:  I have three significant reservations about the work\u2026\n\n(#1) Incomplete validation of the core methodology:  The paper claims to propose a general framework for constructing flexible regression models: \u201c...a novel and general regression method with a clear probabilistic interpretation\u201d (p 2).  However, the paper considers only real-valued regression---when the core method is clearly applicable to other domains---and discusses only one numerical strategy for computing the difficult normalizing constant.  As the model requires a forward propagation for every sample, such a sampling strategy has clear limitations (additional to the usual difficulties with importance sampling).  The authors admit this and bypass the obstacle by introducing the label late into the architecture, meaning the computation specific to the features can be re-used.  Yet, if the paper is truly proposing a \u201cgeneral and flexible framework,\u201d I think ablation studies and runtime experiments showing that the model is applicable to a range of architectures and dimensionalities must be done.  The current draft only validates these choices for specific vision tasks.  \n\n(#2) Relationship to energy-based models: I was surprised that the authors make no mention of energy-based models---this paper\u2019s Equation 1 is essentially Equation 2 from LeCun et al.\u2019s [2006] \u201cA Tutorial on Energy-Based Learning.\u201d  There are a large number of models discussed in this survey, including many for prediction.  Is the proposed model possibly related to one of these?  As the paper has no citations to or discussion of energy-based learning, it is impossible to tell. The authors need to include some discussion of this related work and probably should revise their model name so that it doesn\u2019t seem like a totally new class of model.  \n\n(#3) No discussion or consideration of alternative training strategies: The paper only considers approximate maximum likelihood learning, and the importance sampling approximation has serious limitations due to needing a forward prop for each sample, as discussed above.  However, there is much work on estimating hard-to-normalizing models via other means such as score matching and contrastive divergence.  There is even recent work on scaling these methods up to models for high-dimensional images: see [Du & Mordatch; 2019] and [Song et al.; 2019].  Can these methods be used to by-pass the normalization issue?  If not, there should at least be some discussion of why.\n\n\nReferences\n\nDu, Y., & Mordatch, I. (2019). Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689.\n\nLeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A Tutorial on Energy-Based Learning. Predicting Structured Data.\n\nSong, Y., Garg, S., Shi, J., & Ermon, S. (2019). Sliced Score Matching: A Scalable Approach to Density and Score Estimation. UAI 2019."}