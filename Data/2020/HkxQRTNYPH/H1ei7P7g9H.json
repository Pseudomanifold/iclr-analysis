{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose MGNMT (Mirror Generative NMT) which aims to integrate s2t, t2s, source and target language models in a single framework. They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. They also talk about related work in this space. Finally, they perform experiments on low and high resource tasks. They also investigate certain specific phenomena like effect of non-parallel data, effect of target LM during decoding, and effect of adding one side monolingual data.\n\nPros:\n- Overall, the paper was clearly written and well motivated. The authors clearly lay out their new framework and establish it for the reader.\n- The set of experiments are very detailed and the authors make sure to compare against all semi-supervised works like BT, JBT and Dual learning.\n- The set of analyses at the end was also interesting and tried to dig deeper in certain phenomena.\n- All training details and hyperparameters have been laid it in the paper.\n\nCons:\n- For all the additional complexity, this newly proposed method only slightly outperforms other semi-supervised methods like BT, JBT & Dual learning as seen in Tables 3 and 4.\n- The authors could have been more upfront about training and inference costs of their proposed framework and compared it to the other setups. For example, decoding costs 2.7x more than a vanilla transformer. A comparison of decoding and training costs of all methods would have provided the right balance between complexity and quality. This additional complexity might outweigh the gains obtained in some cases.\n\nRating Justification:\nDespite the con of added complexity, I like the formulation of the new joint framework and I think this will serve as a good starting point for others to push in this direction further. Hence, I want to see this paper accepted.\n\nMinor comments:\nlast para of section 1: first line is too big. Please break into multiple lines.\n\"Exploiting non-parallel data for NMT\" - second para, please cite Dong et. al and Johnson et. al who also share al parameters and vocab in a single model.\nPage 5, section 3.2, second para - line 1 please rephrase."}