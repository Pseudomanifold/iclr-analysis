{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper describes a self-supervised sentence embedding approach that incorporates a different view from plain text where some extent of linguistic knowledge is incorporated through the application of tree LSTM. The training procedure is standard contrastive framework where the model is encouraged to distinguish between context sentence (sentences appearing close to the target sentence) and negative samples. Evaluations are conducted on 1) downstream tasks, but with a simple logistic regression model on top of sentence embeddings; 2) probing tasks that more focus on surface information prediction, syntactic and semantic tasks; and 3) qualitative analysis with nearest 5 sentences.\n\nAlthough the experiments are thorough, I am in favor of rejecting this paper with the following reasons:\n\nFirst, the proposed model is trained with 4.6M sentences among 78M available for 33 hours. It is unclear why authors stop the training at this early stage but the results on all three evaluations seem to be inferior to the state-of-the-art by a big margin. I am happy to raise my score if authors can show the results of a well trained proposed model.\n\nSecond, the paper has some room for improvement in terms of clarity, to name a few:\n1) Authors can strengthen the motivation for multi-views learning in related work; \n2) Formula 1 for softmax is wrong;\n3) Contrastive LSTM and contrastive tree LSTM are not clearly defined in the paper, although the former should refer to quick-thoughts and the latter means the proposed method;\n4) In qualitative analysis, for the last example, there is exactly the same candidate with similarity score 0.012. According to cosine similarity, wouldn\u2019t this be 0 and also show up in the baseline model regardless of the embeddings?"}