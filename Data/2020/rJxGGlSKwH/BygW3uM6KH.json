{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview: This work proposes to learn sentence embeddings using both contrastive learning and multiple \"views\" of sentences.  This work largely builds off of [1], including using the same objective, but uses a multi-view approach to modeling.\n    - They apply the concept of multi-view models, specifically combining tree and linear LSTMs to learning sentence representations.\n    - They prepare a new, large-scale book dataset, which is useful because the previously commonly used book dataset was taken down for legal reason.\n    - They provide a fairly broad set of analyses on their model, both quantitative and qualitative, performance-driven and analysis-driven.\n- Review: The ideas and models presented in this paper are not new, while the supporting experiments are not very well done or convincing. Overall, I recommend rejecting this work.\n- The models are contrastively learned in that they are trained to embed \"similar\" sentences nearby in the embedding space, and \"dissimilar\" sentence far away, where \"similar\" sentences are defined as consecutive sentences. This method of learning textual representations is well-established in the NLP literature, mostly prominently in recent years with word embedding models like Skip-Gram and in sentence embedding models like in [1], [2], [3] (the next sentence prediction task), and several more.\n- In practice, the multiple views of each sentence that this paper considers boils down to encoding the sentence with a bidirectional LSTM and a TreeLSTM and concatenating the representations from each encoder. This idea again has been established in the literature ([4], [5], [6]).\n- The experiments don't seem setup to demonstrate that the multiple views are beneficial over a single view. In Table 1, there are rows for just an LSTM or just a TreeLSTM, but they seem to be trained with labeled data whereas the proposed method is trained self-supervised. A more informative comparison to demonstrate the value of using multiple views would be to train the LSTM and TreeLSTM with the same objective (and ideally model size). Overall, I don't think the claims in the paper are well-supported by the model proposed or the experiments.\n- I have a number of concerns about the experiments.\n    - \"Models are trained on a single epoch on the entire corpus without any train-test split\": so there is no early stopping? Why stop training after one epoch? Was there any indication you were overfitting the data?\n    - \"The training phase was stopped after 33 hours of training\": Why stop there? Computational constraints? Later comments suggest this is quite premature (\"training phase was completed on only 4.6M sentences among the 78M available\").\n    - The results seem to indicate that this method underperforming recent work significantly.\n\nAreas of improvement\n- Some of the language in the introduction and conclusion are a bit of a stretch. Using a linear and tree LSTM (based on dependency parses) doesn't really represent a \"diversity of linguistic structures\". \n- Related work: There's no mention of pretained language models, which could be seen as a form of representation learning for language, and have been hugely impactful in NLP.\n- Method\n    - Missing negative in the log likelihood\n    - Why do you use inner product if other works \"report excellent results\" with other scoring functions?\n    - \"assumes the underlying structure of the sentence to be a sequence, while allowing for long term dependencies\": If anything, the treeLSTM more easily allows for long-term dependencies than the linear LSTM.\n    - \"Negative examples are obtained using the dependency Tree LSTM\": I'm not totally sure how the negatives are obtained here.\n    - \"The target sequence is encoded using the sequential Tree LSTM, while the positive and negative samples are encoded using the ChildSum Tree LSTM\": why are the sentences not all encoded with the same encoder?\n    - It looks really odd that most of Table 1 is empty. Given your model, I imagine it can't have been that difficult to evaluate more baselines (BiLSTM and TreeLSTM) on the rest of the tasks.\n    - It'd be nice if you could clearly indicate in Table 1 which method is yours.\n- Results and Analysis\n    - The standard evaluation setting for sentence embeddings would be GLUE or SuperGLUE.\n    - A glaringly missing baseline is BERT (or any of its relatives), which is also self-supervised.\n    - The results are underwhelming, and as the author admits, somewhat premature as training didn't seem to finish.\n    - 5.2: what are the contrastive LSTM and Tree LSTM? Are those the learned encoders from the \"Contrastive Tree\" in Table 1, or are they trained from scratch?\n    - I don't think the analyses in Sections 5.2 and 5.3 or Figure 2 are particularly useful.\n- There are a noticeable number of typos. For example, in the abstract: \"this linguist[ic] diversity\" and \"better capture semantic[s]\". It'd be worthwhile to look over the paper closely for typos.\n\n\n[1] AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS. Lajanugen Logeswaran and Honglak Lee\n[2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning. Yacine Jernite,\u00a0Samuel R. Bowman,\u00a0David Sontag\n[3] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin,\u00a0Ming-Wei Chang,\u00a0Kenton Lee,\u00a0Kristina Toutanova\n[4] Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang.\n[5] Enhanced LSTM for Natural Language Inference. Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen.\n[6] Improving Sentence Representations with Consensus Maximisation. Shuai Tang, Virginia R. de Sa."}