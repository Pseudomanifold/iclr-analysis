{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper learns a next-step transition model over embeddings produced by a pretrained CNN (specifically VGG16) and then performs greedy best-first search over this transition model to find a sequence of actions that will go from a start state to a goal state. This approach is evaluated on rotating cars in the NORB dataset from one viewpoint to another.\n\nThis paper demonstrates a nice implementation of a simple model-learning plus planning system. However, (1) it does not differ substantially from existing methods already in the literature, (2) the evaluation is insufficient, and (3) the proposed method is unlikely to scale to even moderately challenging domains. Thus I recommend rejection.\n\nFirst, learning and using models within planning systems has a large and rich literature, and links have already been drawn multiple times between human cognition and machine learning (see [1-2] for two recent examples). It is very common to learn action-conditional state-transition models---both from pixels, states, or state embeddings---and use them in a planning procedure [1]. On the model-learning side, then, it does not seem to me that the present paper is really different from what has been done before. Similarly, the planning algorithm is a very classic best-first search algorithm. While today more complex search algorithms are typically used (e.g. MCTS) the idea is roughly the same.\n \nSecond, the evaluation in the paper is minimal and insufficient. The model is only tested on a single example---one of the cars from the NORB dataset. The particular setup with viewpoint matching within NORB is in and of itself also very simple. To be more compelling, the paper would need to demonstrate results on much more challenging environments (at a minimum, rotation of a much wider range of objects; even better, a non-rotation task such as navigating through a maze).\n \nFinally, I do not think as presented the proposed method will scale to even moderately challenging domains. Greedy search is typically not used because it does not scale to cases with many local minima and maxima. Indeed, the results of Figure 3 demonstrate exactly the problem with using a greedy search.\n\nSome additional comments:\n \nIt is unclear how the algorithm terminates---there is no termination condition in Algorithm 1.  Since the model takes as input high-dimensional continuous inputs (images), it is unlikely to reach exactly the same state as the goal state. So, how is completion defined? For example, is there a threshold on the similarity to the goal state, which if exceeded would indicate that the goal state has been reached?\n\nThe first four paragraphs contain no citations yet make many claims about the brain (e.g. \u201cthe cerebellum receives 40 times more information than it outputs\u201d) and about machine learning (\u201cthere has been much recent work on methods that take advantage of compact representations\u201d). These statements need to be supported with citations.\n\nI didn\u2019t really understand the justification for the experiments with the Laplacian Eigenmaps.\n\n[1] Hamrick, J. B. (2019). Analogues of mental simulation and imagination in deep learning. Current Opinion in Behavioral Sciences, 29, 8-16.\n[2] Pezzulo, G., Donnarumma, F., Maisto, D., & Stoianov, I. (2019). Planning at decision time and in the background during spatial navigation. Current Opinion in Behavioral Sciences, 29, 69-76."}