{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #6", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This is the first work that studies probability calibration for knowledge graph embedding models. In the case where ground-truth negatives are available the authors directly use off-the-shelf established calibration techniques (Platt scaling, isotonic regression). When ground-truth negatives are not available they propose to synthetically generate corrupted triples as negatives and use sample weights to guarantee that the frequencies adhere to the base rate.\n\nIn general the paper is well-written and easy to follow. Given that the paper's major contribution is experimental insight, and there are no major technical contributions, I would have liked to see a more in-depth analysis of how some of the key hyper-parameters influence the calibration of a model beyond the type of the loss, and beyond the correlation with embedding quality. Overall, I would be willing to increase the score if the authors perform a more comprehensive experimental analysis.\n\nSuggestions to improve the paper:\n1) I would expect that especially the negatives per positive ratio \\eta, and the dimensionality of the embeddings have a significant impact on model calibration. It would be valuable to experimentally quantify the impact of these key hyper-parameters.\n2) It is currently difficult to judge how well-calibrated are the models from the reliability diagrams/calibration plots since the total counts are not shown (e.g. total number of instances with mean predicted value between 0.4 and 0.5). That is, it could be that deviation from identity is due to small sample effects, i.e. we are estimating the fraction of positives from a handful of instances. Showing the total counts for each bin will help the reader better understand the calibration of the models.\n3) Several questions can be clarified regarding the sample weights:\n3.1) How essential is the proposed weighting scheme? How do the calibration techniques perform when using synthetic negatives with uniform sample weights?\n3.2) How does the proposed weighting scheme relate to the the general problem of calibrating models that have class imbalance?\n4.1) Can we observe significant difference in terms of calibration between translational distance models and semantic matching models, i.e. using distance-based scoring functions vs. using similarity-based scoring functions. If so is there any reason for that? To help answer this question the authors could compare additional models from each group (beyond the three models used in the paper). \n4.2) Are methods that represent entities as random variable to capture uncertainties (e.g. KG2E) better calibrated?\n5) Platt scaling assumes that per-class probabilities are normally distributed, while isotonic regression makes no assumption about the input probabilities. Given that Platt scaling performs worse in the experiments it would be interesting to investigate whether this can be (partly) explained by a deviation from the above assumption.\n6) Results reported in Table 3 are for WN11. It would be valuable to report similar results for the other datasets in the appendix.\n7) it would be beneficial to explore the different procedures proposed in the literature for generating synthetic negatives and their impact on the calibration. \n\nSuggestions to improve the paper that did not impact the score:\n1) On the triple classification task in Table 4, there is a significant gap between the literature results and the reproduced results on FB13 and YAGO39K. Is there an explanation for this? Furthermore, it would be interesting to investigate how much do the per-relation \\tau_i's deviate from 0.5 when they are learned using both non-calibrated and calibrated probabilities.\n2) In Eq. 6 after the second equality shouldn't there be \"N/(w- + N)\" instead of \"N/(w_{-} + PN)\"?  Is the additional P a typo?\n3) It would be nice to make the figures more readable (e.g. when printed in black and white) by using different markers for each line. \n"}