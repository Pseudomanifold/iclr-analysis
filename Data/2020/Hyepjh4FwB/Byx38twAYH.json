{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The aim of this work is to make deep learning classifiers more interpretable by \"projecting\" each input sample into a small collection of prototype examples (with some weighting over those) and then basing the decision on a combination of the latent representations of the chosen prototypes. In this way, the chosen category can be justified as the input being similar to the selected prototypes. Additionally, this approach makes it possible to obtain a confidence score at test time.\n\nThe choice fo the encoders for the prototypes and the examples is asymmetric (the first using keys and the second queries). This is not justified. Is it empirically better than using the same encoding for both before feeding them to the relational attention?\n\nThis work aims to satisfy many desiderata (listed in section 3). The decisions made to accomplish these are reasonable although somewhat arbitrary. In fact, several ways to encode the desiderata in the loss function are listed in Table 1.\n\nQualitatively, in the presented comparison with influence networks and representer point selection, ProtoAttend seems to choose more representative examples.\n\nIt is not easy to find a direct, quantitative way to compare this type of work with the existing literature, but from a qualitative perspective, the set goal (which is an important one) is achieved.\n\n"}