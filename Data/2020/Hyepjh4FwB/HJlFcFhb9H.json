{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a sample-based self-explaining method for image classification. The basic idea is adopt the attention mechanism to learn the relation between the latent representation of the query sample and training samples, and identify the training samples with higher similarity as the prototype. The classification decision is based on the label consistency between the identified prototypes (with the relation score in attention mechanism as the weight of different prototypes in determining the  label agreement)\n\nThe proposed model is intrinsically interpretable since the prototypes with higher weights can play as the decision explanation. And the authors have conducted experiments to show that such self-explaining mechanism based on attention model can achieve comparable classification accuracy with original black-box models.\n\nThe presentation of the paper is clear and easy to follow. But I have several concerns regarding the choice of prototypes and the evaluation of the interpretation:\n\n1) According to Eq. (2), it seems that all training samples are used as the prototypes (but with different weights). Why not just use the top few prototypes? Would this such setting introduce a lot of noise, since many training samples are from different classes? \n\n2) Since one focus of the paper is to provide interoperation of the classification model, some more experiments are needed to evaluate how well the interoperation is. For example, some crowdsourcing experiments to check if the provided prototypes can help human users correctly guess the model prediction.\n\n3) I think the authors should also compare with the black-box model when we use the attention mechanism as a post-hoc interpretation. One straightforward baseline is that use the black-box model for classification, and pick the top \"prototypes\" with the highest similarity in the latent representation. Such comparison can help to validate if incorporating attention mechanism in the model design can provide better quality prototypes."}