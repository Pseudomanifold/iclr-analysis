{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes an attention-based prototype learning algorithm, which introduces an attention operation to assign different weights to the prototypes. Comprehensive experiments demonstrate that the proposed method is efficient and effective in various tasks. \n\nI have the following comments:\n-\tThe authors did a really good job in empirical studies, verifying the superiority of ProtoAttend.\n\n-\tThe novelty of the main idea is limited and may provide a limited contribution to the research community.\n\n-\tThe authors clarified that ProtoAttend is an inherently interpretable algorithm. However, the interpretability is proved by na\u00efve Prototype learning only. A rigorous theoretical proof should be provided to demonstrate its interpretability.\n\n-\tI would be appreciated if the authors provide the pseudo-code to show the training procedure of ProtoAttend.\n\nOverall, I think this work is not ready for publishing unless the theoretical property is well understood.\n"}