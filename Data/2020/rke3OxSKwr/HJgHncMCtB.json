{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work apply the wait-k decoding policy on the 2D CNN-based architecture and transformer.  In the transformer-based model the author proposed to recalculate the decoder hidden states when a new source token arrives. The author also suggested to train with multiple k at the decoder level with shared encoder output. The experiments showed that the transformer model provide the best quality on IWSLT14 En-De, De-En, and WMT15 De-EN.\n\nThe masking and using causal attention for the transformer has been proposed in previous works. The hidden state updates provide some gains for the model but also makes the decoder more expensive. The training with multiple k provides similar gain as training with one k larger than the value used at the inference time. Overall the contributions are limited.\n\nThere is quite some room for this paper to improve its clarify, especially in terms of annotations and explaining the proposed ideas."}