{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Sorry, this is a very quick review.\n\nThe paper is about an improved method of training latency-limited (wait-k) decoders for transformer-based machine translation, in which the right context is limited to various numbers.  So it's a kind of augmentation method that's well matched to the test scenario.  At least, that is my understanding.\n\nI am not really an MT expert so cannot comment with much authority.  On the plus side the paper says it sets a new state of the art for latency-limited decoding for a German-English MT task, and it involves transformers, which are quite hot right now so the  attendees might find it interesting because of that connection.\nOn the minus side, it is all really quite task-specific.\nI am putting weak accept.. regular-strength accept might be my other choice.\nIt's all with low confidence."}