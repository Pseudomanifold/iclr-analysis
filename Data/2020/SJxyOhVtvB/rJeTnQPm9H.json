{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe paper proposed a IEG method to handle the noisy label learning problem. IEG consists of 3 parts: 1)  isolate the noise\nlabels via meta optimization; 2) escalate the supervision mislabeled data via pseudo meta re-labeling; 3) use small trusted data to guide training. IEG gains improvements over SOTA. \n\nStrength:\n1. The motivation of IEG is very clear. \n2. Ablation experiments are useful for understanding. \n\nWeakness:\n1. The organization of Section 2 is somewhat confusing. Readers may need to jump from one section to another section to follow how the method work. IEG should consist of 3 components, while there are only 2 subsections in section 2. Could the authors clarify which part corresponds to which component? \n2. I am wondering where are the tiny trusted data used? I am not following that from ALG 1. Do I miss something?\n3. For experimental comparisons, I think the authors may miss some important baselines. \n    1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019\n    2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018\n    3) Dimensionality-driven learning with noisy labels, ICML2018\n4. For the ablation study, the authors miss some important experiments. 1) testing the influence of the number of trusted data; 2) showing the rate of isolated misclassified data in the learning process. 3) showing the influence of weight w. \n5. Open-set case is important for understanding IEG. I am confusing why IEG still works so well in the open-set case where most labels of data cannot be predicted correctly because of the open-set setting. The weight w before examples very small? or anything else? The author should provide more evidence on open-set case for better understanding. "}