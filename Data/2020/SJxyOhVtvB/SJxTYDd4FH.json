{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a method named IEG (the first letters of isolation, escalation and guidance) for robust neural network training with severe label noises. Specifically, the letters of IEG mean isolation of noisy labels, escalation of useful supervision from mislabeled data, and guidance from small trusted data. Therefore, the method is a combination of 3 components.\n\nIn fact, \"I\" is just the sample selection direction in learning with noisy labels, and \"E\" is just the label correction direction in learning with noisy labels. They are 2 out of 3 famous directions when there are only training data with noisy labels. The direction of \"G\" is also quite famous when there are also a small set of clean validation data. These directions are conceptually orthogonal, and this is known for long time in this area. A combination of them cannot make the novelty above the threshold of a very competitive ML conference like ICLR! Note that this is an academic/scientific paper, not an industrial product, so you don't need to combine all things that might work.\n\nBTW, in \"E\", no useful *supervision* from mislabeled data is used; instead, some useful *information* from mislabeled data is used. The term supervision refers to the y part rather than the x part of the data in ML or at least in weakly-supervised learning."}