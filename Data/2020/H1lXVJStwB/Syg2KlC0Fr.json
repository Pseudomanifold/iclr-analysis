{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a technique for curriculum learning using Dynamic Instance Hardness (DIH). DIH is defined as value for each training instance that characterizes how hard that instance is and is updated throughout the training process. The DIH value is used to select the best set of instances to learn. It is shown that instances with high  (low)  DIH values maintain the high (low) value throughout the training process.\n\nThe main contributions and pros of the paper are\n1. A notion of instance hardness that is persisted and updated throughout the training procedure.\n2. An objective function for characterizing instance hardness as a dynamic subset selection problem. Presenting a greedy algorithm for online maximization of this function.\n3. Experimental results that show how the property of instance hardness is maintained throughout training and showing how DIH-driven curriculum learning techniques that use random sampling outperforms non-curriculum learning techniques.\n\nCons\n1. The writing of this paper is difficult and in many of the core parts of the paper, the important definitions are not clear. E..g a) the role of the function 'f' that is being maximized in section 3.2 is not clear. To elaborate, it is not obvious what this function is or why should one care about it? \n2. The greedy algorithm has a bound in equation 7 that appears to be quite loose as the value of k is as high as the order of the size of the entire training set (in the experiments, 0.2n <= k < n). Am I misinterpreting it?\nFurthermore, the greedy algorithm -- while being focused on the core of the technical contributions -- is not used in experimental comparisons and instead all the results presented use random sampling. At least the result of DIH-greedy should be presented.\n\n3. This is more of a suggestion: one of the claimed advantages of the algorithm (over non curriculum learning and MCL) is that it requires less training examples to train. Given this, the authors should present training time improvements over large datasets."}