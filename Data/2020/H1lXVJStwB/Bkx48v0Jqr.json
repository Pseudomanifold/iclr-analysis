{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes a curriculum learning approach that relies on a new metric, the dynamic instance hardness (DIH). DIH is used to measure the difficulty of each sample while training (in an online fashion), and to decide which samples to train on next. The authors provide extensive experiments on 11 datasets as well as some theoretical motivation for the use of this approach.\n\n---- Overall opinion ----\nOverall I believe this paper is an interesting take on curriculum learning that is able to achieve good results. I believe this approach is a combination of core ideas from multiple sources, such as boosting, self-paced learning, continual learning and other curriculum learning approaches, but overall it seems different enough from each one of them individually. Because of the resemblance with these many different methods, the method itself does not surprise through the novelty of a new idea, but the authors seemed to have found something that was missing from these methods and that leads to very good results. The experimental results look great, but I believe the paper is missing some ablation studies to assess the importance of certain components (see details below). I also had some trouble understanding certain arguments, which I hope the authors can clarify. \n\n---- Major issues ----\n1. I find the arguments section 2.1 quite difficult to follow. In particular, under the assumption stated in the paper that r_t(i) = f(i|S_{1:t\u22121}) =  f(e_i + S_{1:t\u22121}) \u2212 f(S_{1:t\u22121}) , why does it follow that r_t(i) can be used instead of f in the minimization problem (2). \n\n2. Based on the method itself, it seems to me that the parameter k_t could would have a lot of influence on how well the method doing.  The authors mention in the experimental section what values they use, but there is no indication on how one would choose this value. Moreover, it would be good to see an analysis of how sensitive the results are to this choice.\n\n3. In Figure 1, it is not clear whether the figure on the right shows the actual loss, or the smooth loss using Equation (1) with instantaneous instance (A). If it is the former, then if the loss is so smooth, why do we need DIH? If it is the latter, then what does the instantaneous loss look like? This actually raises the question of how important the smoothing component is -- could we achieve the same results with an instantaneous loss (i.e. set gamma to 1 in Eq. 1)?\n\n---- Minor issues ----\n1. How do you choose T0, gamma and gamma_k?\n\n2. In the conclusions, the authors state that \u201c The reason [why  MCL and SPL are less stable] is that, compared to the methods that use DIH, both MCL and SPL deploy instantaneous instance hardness (i.e., current loss) as the score to select sample\u201d. Since there are so many other differences in the way training progresses, I think we don\u2019t have enough evidence to attribute this to merely the \u201cinstantaneousness\u201d of the loss. In fact, it would be interesting to see how SPL does if you use DIH as a metric (just smoothing the loss over time), but their approach of scheduling samples (easy to hard, and not the opposite and in DIHCL).\n\n3. Appendix C shows some interesting results regarding wall time comparison. I was surprised to see that, despite the extra computations, DHCL is comparable to random mini-batches. This makes me wonder what the stop criteria was, because when you stop matters a lot for run time comparisons. It would also be interesting to see a more ample discussion on this in the main text.\n\n4. In Figure 1, the axes are barely readable.\n\n5. The authors oftentimes reverse the use of \\citet and \\citep, for example \u201chas been called the \u201cinstance hardness\u201d Smith et al. (2014) corresponding to\u201d should have a bracket, whereas \u201cOur paper is also related to (Zhang et al., 2017)\u201d should not have brackets.\n\n6. This is not an issue, but I just wanted to say I appreciated Appendix B.\n\n---- Suggestions ----\n1. It would be interesting to make a connection between the DIH and what other papers have discovered about example forgetting (e.g. Toneva et. al, that was mentioned in the paper).\n\n2. Major issues 3 -> a study on the effect of k and how to choose it.\n\n3. While I understand that the models chosen in the experiments are expensive to train, it would be good to report standard deviations in Table 1.\n\n4. Based on Table 1 and Figure 3, there is no concrete winner among the DIHCL methods. It would be good to include some recommendations in your conclusion on which one to choose and when.\n\n---- Questions ----\n1. \u201cOn average, the dynamics on the hard samples is more consistent with the learning rate schedule, which implies that doing well on these samples can only be achieved at a shared sharp local minima.\u201d -> can you please explain why this is so?\n\n2. See Major issues 3.\n\n3. In Table 1, on some datasets, the authors apply lazier-than-lazy-greedy, and on some not.Why, and how does one decide this for a new dataset?\n\n4. How did you choose T0, gamma and gamma_k, as well as the schedules in Appendix C (page 17)? "}