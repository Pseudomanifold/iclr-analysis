{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new method for interpreting CNN predictions. This approach takes a pre-trained classifier and determines regions of high-relevance for the prediction by selecting neurons with large activation values, and following them backwards through the fully-connected and convolutional layers. This is quite similar to the approach taken for the \"CNN-fixations\" method of Mopuri et al. The primary innovation in this paper is to select which K neurons in each layer are \"important\" based not only on their activations, but also based on the activations of neurons in the two neighboring layers, before and after. This entails taking a weighted score following eq (1), where the weights are selected via a bandit-style RL algorithm run on a small validation set.\n\nWhile the few figures presented in the paper look plausible, the paper is unfortunately lacking both in evaluation and in presentation of the method. I raise a few issues here as questions:\n\n* In many parts of the paper, including the abstract and algorithm block, the adjacent layers are described as \"support\", e.g. Z \"are the calculated supports\". What does \"support\" mean in this context? There are two common usages I am aware of in ML \u2014 support of a distribution (i.e. the set of values with positive probability), and support vectors in an SVM. This doesn't seem to be very analogous to either, and \"support\" as used here is not formally defined. This in particular makes the abstract confusing.\n\n* What is the motivation for the particular functional form of equation 1? Why are the current and preceding layers summed, and then multiplied by the succeeding layer? This seems like a rather arbitrary choice of functional form.\n\n* What sorts of values alpha are considered? It seems like a severe weakness that these are selected from a small, discrete set of possible tuples of values. How many are considered? Are these values strictly positive, or may they be negative? How stable are the learned values across different classifiers, layers, and architectures? How different at each layer are the actual values from those of CNN-fixations?\n\n* It is claimed in section 2 that this method requires no hyperparameters, which seems incorrect. At the very least there are parameters in (a) the gaussian blur, (b) the outlier selection / rejection step, (c) the IOU threshold choice for learning layer-wise weights, (d) the candidate set for layerwise weights.\n\n* Figure 5 is a photo of macaroons. Why is it labelled \"bath towel\"? The perturbed label is \"ice cream\". Are these both perturbed labels? Why does the heatmap not match the image on the \"original\" bath towel label, while finding the object on the perturbed label?\n\n* Figure 4 seems very sensitive to the method by which outlier points are discarded (i.e. the bounding box generation scheme of section 4.1.2). The pointing game metric seems rather unreliable \u2014 it doesn't seem to penalize the number of hits, so why not simply throw away all points except for the median? That would probably score 100% unless it completely misses the object!\n\n* It would be great to see some evaluation along the lines of mentioned in \"Sanity Checks for Saliency Maps\", Adebayo et al. 2018. In particular, how well does this work when tested on a network with random weights? How well does this work when tested on data with labels permuted? Tests like these are important to prevent confirmation bias \u2014 the few visual examples provided (figs 3, 5, 6) provide good feedback, but are not a substitute for quantitative testing."}