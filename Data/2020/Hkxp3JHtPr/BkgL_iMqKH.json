{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The papers proposes to use VAE-like approaches for semi-supervised novelty detection. Two methods are describes:\n(1) the MML-VAE fits a standard VAE to the normal samples and add a repulsive term for the outliers -- this term encourages the encoder to map the outliers far from the latent-space prior distribution.\n(2) the DP-VAE fits a VAE with a mixture of Gaussian prior on the latent space -- one mixture component for normal samples, and one mixture component for outlier samples.\n\nThe described methods are simple, natural, and appear to work relatively well -- for this simple reason, I think that the text could be accepted.\n\nThere are several things that are still not entirely clear.\n(1) without the reconstruction term, the methods are small variations of supervised methods. Consequently, I feel that the authors should try to explain much more carefully why the introduction of a reconstruction term (which could be thought as an auxiliary task) helps.\n(2) given point (1), one could think of many auxiliary task (eg. usual colorisation, or rotation prediction, etc..) Would it lead to worse results?\n(3) proportion > 10% of anomaly is relatively standard for supervised-methods + few standard tricks to work very well. Although I understand that only one small subset of anomalies is presented during training, I think that it would still be worth describing in more details the efforts that have been spent to try to make standard supervised methods work. \n"}