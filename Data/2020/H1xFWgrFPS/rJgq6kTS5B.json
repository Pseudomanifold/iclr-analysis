{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents a method for explaining the output of black box classification of images.  The method generates  gradual perturbation of outputs in response to gradually perturbed input queries. The rationale is that, by looking at these, humans can interpret the classification mechanics.\n\nThe presentation is clear. The coverage of prior work is sufficient (although references should point to the published work instead of arxiv entries, when the former is available).\n\nOne question that is not addressed is how efficient is this method, in terms of computational cost. This is a method that increases the amount of input data (through perturbation). What is the minimum amount of input data that needs to be perturbed in this way, before the method can become human interpretable? \n\nAlso, ideally any work on human interpretability of ML should be evaluated on humans. If not, it is an approximation, and it should be presented and reasoned as such (with a discussion of limitations and caveats, for instance)."}