{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Here are the claims I could find in the intro:\n\"Given a query input to a black-box, we aim at explaining the outcome by providing plausible and progressive variations to the query that can result in a change to the output\"\n > This is well supported as the model generates these and it is very reasonable that it can.\n\"the counterfactually generated samples are realistic-looking\"\n> The images seem to support this.\n\"the method can be used to detect bias in training of the predictor\"\n> Section 4.4 makes it really clear that, at least in the described setting, it works.\n\nI think the idea could be presented in a better way. The general concept of exaggerating a feature that represented a class seems novel and exciting. Just based on the novelty of that alone I think this is worth accepting. I would imagine there would be a cleaner way of achieving all this but maybe it is all necessary.\n\nI don't understand Figure 1a. I don't think this helps to illustrate the point. M_z seems to just be a bottleneck but the writing makes it seem like it is more.\n\nSection 4.2 is a bit hard to read. It is not clear for me what is the goal of this section.\n\nSection 4.4 seems very similar to the idea in this work https://arxiv.org/abs/1805.08841 which studied how bias in CycleGANs can be seen when you vary the bias which I think should be cited here.\n\nTypos:\n\"our application of interested\"\n"}