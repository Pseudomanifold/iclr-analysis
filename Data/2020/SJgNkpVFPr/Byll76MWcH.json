{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "I like this paper: it tackles an interesting and relevant problem (imitation learning when demonstrations come from people with different levels of expertise), takes the natural approach of attempting to infer which expert produced which demonstration, and shows results compared against a large number of baselines. However, I have some worries about whether VILD will work in more realistic settings, and so I\u2019m only recommending a weak accept.\n\nThe experiments are done very well -- there are *many* baselines and a reasonable number of environments. However, the experiments are set up to match VILD\u2019s model, and it is not as clear what would happen in a more realistic setting where there will be misspecification. For example, one hyperparameter of VILD is the assumed number of demonstrators, which is set to exactly the right number (10) in the experiments. I suspect that given the way the demonstrations are generated, it would be relatively easy to cluster the demonstrations into the 10 sets, making VILD\u2019s job relatively easy. In contrast, with real data from humans, I expect that such a clustering would be much harder, since demonstrations from a single human often also have diverse quality. It remains to be seen how well VILD would perform in such a situation.\n\nThe authors do consider one type of misspecification: when instead of Gaussian noise, the true actions are generated with TSD noise. This gives me more hope that VILD will work in more realistic settings. While I would particularly appreciate experiments with real human data, in the absence of that I would like to see an experiment with misspecification of the number of demonstrators. For example, perhaps assume 5, 20 or 50 demonstrators, when there are exactly 10 demonstrators, and assume 10 demonstrators when there is actually just 1 demonstrator. Presumably VILD should not perform as well as e.g. GAIL in the latter case.\n\nI was confused reading Sections 1 and 2. Prima facie, the model in Figure 1b is very strange: given that we have to model both p(at | st) and p(ut | at, st, k), it\u2019s not clear why we even have an extra variable -- why couldn\u2019t we just model p(at | st, k) directly? The answer is only made clear later in Section 3: we are specifically assuming that there is Gaussian noise on top of the chosen action. I would make this clearer in Section 2."}