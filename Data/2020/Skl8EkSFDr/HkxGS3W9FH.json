{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes a method to compress GANs. The motivation is that the current compression methods work for other kinds of neural networks (classification, detection), but perform poorly in the GAN scenario. The authors present intuitive reasons for why this is the case. \n\nHowever, the motivation why we would like to compress GANs is unclear to us. The intro mentions: reducing memory requirements and improving their performance. Sure, compressing networks for object detection and classification on mobile devices is really useful. But GANs are mainly used for unsupervised density estimation, why put a GAN generator on a mobile device? But maybe we are missing something here. \n\nTheir \u201cself-supervised\u201d method works by using the pre-trained discriminator network, while compressing only the generator. They show both qualitative and quantitative gains.\n\nThe paper is clear and well-written. It presents a way of pruning GAN generator network and although of limited novelty, it might be an interesting read as it provides extensive and convincing experiments in a clear manner. It does have several parts though which require additional clarification.\n\nThe idea of using the pre-trained discriminator network seems reasonable, but I am missing what the compression method for the generator network actually is (Section 4). From Table 2 I would assume it is pruning, in which case the paper\u2019s contribution is very limited.\n\nThe authors claim that the \u201cself-supervised\u201d method generalizes well to new tasks and models. \"Generalizes\" seems a strong word here, since the procedure compresses only the generator network. A more appropriate way of putting it might be \u2018can be applied to other tasks and models.'\n\nIn Section 4 the authors write: \u201cOur main insight is found,\u201d but then they describe the GAN method. What is the actual insight there?\n\nThe qualitative results in Figure 1 suggest that their \u201cself-supervised\u201d method is better than the other baselines. \n\nScores from Table 2 also support the claims, but the table itself is not referenced anywhere in the text.\n\nThe analysis in Section 6 seems out of context with the rest of the paper. It is not clear how it relates to the \u201cself-supervised\u201d method.\n\nMissing related work: 1st paragraph: compressing or distilling one network into another is much older than 2015, dating back to 1991 - see references in section 2 of the overview http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html \nThe GAN principle itself is also much older (1990) - see references in section 5 of the link above.  \n\nGeneral remarks:\n\nIn the first read of Section 3 it is not clear what [a], [b], [c] are.\n\nIt would be good to first refer to Table 1.\n\nTable 1: why is there a \u201c?\u201d only on the \u201cFixed\u201d column?\n\nIt would be good to have a larger font size in Figure 2, at least the size of the main text font.\n\nIn its current form, the pdf file has 100MBs (8MBs the main paper and the rest is the appendix). One could instead move the images from the appendix to a website and provide a link.\n\nWe might improve our rating provided the comments above were addressed in a satisfactory way in the rebuttal.\n\n"}