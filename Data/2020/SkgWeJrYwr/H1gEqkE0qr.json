{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a wrapper feature selection method AMBER to use a single ranker model along with autoencoders to perform greedy backward elimination of features. Experimental results on various datasets show that their criterion outperforms other baseline methods. Generally, the paper is well written and easy to follow. However, the idea is simple and the originality seems incremental. \n\nFirst, although taking advantage of the power of neural network to help select better features sounds interesting, it is important for the author to discuss the benefit of doing it. As mentioned by the author, neural network essentially has the ability of selecting features. Instead of selecting features with AMBER explicitly, a more straightforward way is using all features as input and solving the downstream task with deep learning model. Feature selection will be automatically conducted during the learning process. It will be better if the author can explain more about the benefit and motivation of introducing feature selection explicitly in this scenario.\n\nSecond, the author proposed to use an autoencoder with one hidden layer consisting of d\u22121 hidden neurons to calculate feature\u2019s redundancy score. Is there any specific reason for including d-1 hidden neurons. It will be better if the author can give some theoretical analysis of it.\n\nThird, the author calculates the redundancy score and relevance score independently and combine them together to obtain the saliency score. However, it seems unreasonable to regard relevance and redundancy as two independent factors and stiffly combine them. For example, a feature can be both relevant and redundant. Should we eliminate it? How the proposed method solve this case? \n\n"}