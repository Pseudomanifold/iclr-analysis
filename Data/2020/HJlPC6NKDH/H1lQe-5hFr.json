{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work presented an improvement of grid search algorithm for certain hyperparameters in deep neural nets training. These hyperparameters, such as learning rate and drop out, have \"temperature\" like meaning to control the noise injected in the training. With this analogy, the author proposed to use the idea of parallel tempering in statistical physics to allow exchange these hyperparameters during the training. Their empirical results showed this improves standard grid search.\n\nThe main contributions are two folds: 1. the connection between some hyperparameters to its physical perspective (temperature) and 2. the empirical validation of the proposed algorithm. The paper is clearly written and easy to follow. I would like to accept this paper if the authors can address following comments.\n\n1. In Line 11 of algorithm 1, what is \\alpha and how to update it?\n2. How do the authors come up with the values for learning rate and drop out in the experiments? How sensitive the results are to the choice of those values?\n3. What is the value of C used in the experiments. Do you have ablation study of this choice? How to pick this value?"}