{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an analogous optimization algorithm that is inspired by MCMC. The algorithm maintains a series of weight replicas and sweeps them according to loss functions, based on rules from statistical physics.\n\nI appreciate the idea of introducing temperature here, and the part that describes the idea is nicely written. I may have concerns, as in training neural network, most optimization algorithms use stochastic gradient which intrinsically contain noise inside, although certain level of gradient noise helps to escape saddle point on the landscape, there is no necessary analysis whether this additional noise would contribute to the optimization positively or negatively, after all the noise level has to be bounded for the optimizer to converge.\n\nThe experiments are a little insufficient, as the authors used very small datasets, and there is no comparison with other carefully tuned optimization algorithms, which, circle back to my previous concern, is that whether such an algorithm could actually outperform a carefully tuned SGD, ADAM."}