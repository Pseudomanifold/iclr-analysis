{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "1. For imbalanced learning problem, Precision cannot play a good role. Therefore, I recommend using the performance metrics F-value and G-mean to provide comprehensive assessments. \n2. In table 2 of the 5%  data imbalance, the proposed method is not as good as the baseline. Could you provide more results with different percentage of data imbalance, such as 2%, 3%, and 4%. \n2. The authors created their own baseline, and compared against it. There is plenty of baseline methods in literature to compare against such as:\n[1] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. SMOTE: synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321\u2013357, 2002. \n[2] Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In 2008 IEEE International Joint Conference on Neural Networks, pp. 1322\u20131328. IEEE, 2008. \n[3] Han H, Wang W Y, Mao B H. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning[C]//International conference on intelligent computing. Springer, Berlin, Heidelberg, 2005: 878-887.\n3. Figure 2 is hard to understand.\n4. What if projecting both original data and synthetic data into 2D space for visualization, as shown in \u201cModel-Based Oversampling for Imbalanced Sequence Classification\u201d. \n5. How robust is the proposed algorithm when facing different levels of noise?\n"}