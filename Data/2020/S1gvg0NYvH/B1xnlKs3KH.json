{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper studies the dynamics of neural network in the Teacher-Student setting, using the approach pionnered in the last few years.\n\nConcerning the presentation and the review of other works, I am a bit surprise that the \"teacher-student\" setting appears out of the blue, without any references to any paper. This is not a new area! Especially when it concerns learning with SGD, these were the subject of intense studies, especially in the 80s, see for instance:\n* Saad & Sola On-line learning in soft committee machines, '95\n* Exact solution for on-line learning in multilayer neural networks '95\nor the book \"On-Line Learning in Neural Networks\", By David Saad, with contribution of Botou, etc...\n* Many of these results were proven rigorously recently in \"Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup\" by Goldt et al,\n\nThere is a difference with the current formulation: in those papers, both the teacher AND student had a finite-size second (or more) layer, while here, one is working in the mean-field regime where the student is infinity wide. This is indeed a different situation, where the system can be much more \"over-parametrized\". But this does not mean that the subject is \"terra incognita\".\n\nThere are three main sections in the paper, discussing the results.\n\n* The first result is a theorem that, if I understand correctly, says that the stationary distribution of gradient descent on the population loss (i.e. the test error) a necessary condition for the stationary distribution is that it has zero generalisation error (Eq. 4). That seems like an incremental-step compared to previous results that write down the PDE (the four mean-field papers from last year) and it looks very similar to results that show gradient descent provably converges to global optima etc. I am not sure I see the importance of the result. Also, it is not clear \"how long\" SGD should run to reach such a point and this regime might be entirely out of interest.\n\n* Sec. 2.2 also discusses the difference in learning to teacher nodes with large or small weights, resp. Again, this is well-known in the asymptotic limit and rather unsurprising. This is also discussed in, e.g.  in Saxe et al \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\" at least in the linear case.\n\n* The extension to ResNets is definitely more interesting. The authors write down mean-field equations for two models, and prove, if I understand correctly, that it is a necessary condition to recover the teacher weights to generalise perfectly, which, as I said above, seems unsurprising.\n\nIn the end, given the paper is not discussing the relevant literature in teacher-student setting, and that I (perphaps wrongly)\n do not find the results surprising enough, I would not support acceptance in ICLR.\n\n"}