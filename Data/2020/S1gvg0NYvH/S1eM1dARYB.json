{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the mean field models of learning neural networks in the teacher-student scenario. The main contributions are summarized in Theorems 2-4, which characterize the stationary distribution of gradient-descent learning \nfor what are commonly called committee machines. Theorem 2 is for a committee of simple perceptrons, whereas Theorem 3 is for what the authors call the ensemble-ResNet, which is in fact a committee of ResNets, and Theorem 4 is for what the authors call the wide-ResNet, which is actually a stacked committees of simple perceptrons.\n\nThese three theorems are straightforward to derive from differentiation of the expected squared loss E[\\rho] with respect to \\rho and equating the result with zero. The argument in Example 2 in Section 2.1 has flaws. The argument on the wide-ResNet is based on the linear approximation, which effectively reduces the stacked committee network to a large committee of simple perceptrons, so that its significance should be quite limited. Because of these reasons, I would not be able to recommend acceptance of this paper.\n\nThe authors do not seem to know the existing literature on analysis of learning committee machines. See e.g. [R1] and numerous subsequent papers that cite it.\n\nIn Example 2, the authors claim that when \\sigma is an odd function \\rho is a stationary distribution if and only if the difference is an even function. This claim is not true in general. Consider the case where \\sigma is a sign function. Then for any function f(\\theta) satisfying \\int_0^\\infty f(\\theta) d\\theta=0 (such functions are straightforward to construct), let g(\\theta)=f(\\theta) if \\theta\\ge0, and g(\\theta)=-f(-\\theta) if \\theta<0. Then \\int \\sigma(x\\cdot\\theta)g(\\theta) d\\theta=0 holds, whereas g is an odd function. This is a counterexample of the authors' claim here. Many more such counterexamples are quite easily constructed on the basis of orthogonal polynomials, demonstrating that the stated stationary condition in Theorem 2 may not be so strong for characterizing the stationary distribution.\n\nPage 2, line 30: Given a function (f -> y)\nPage 2, line 38: I do not understand what the subscript $x$ of the integral sign means.\nPage 2, line 40: The last term should be squared.\nPage 3, line 7: Should \\theta_j be \\theta_j(t)? Should \\rho(\\theta) be \\rho(t,\\theta)?\nPage 3: Both \\rho(t,\\theta) and \\rho(\\theta,t) appear, which are inconsistent.\nPage 3, line 16: under (the the) Wasserstein metric\nPage 3, line 22: \\rho_0(x) should probably read \\rho(0,\\theta).\nPage 3, line 26: \"should follow normal reflection\" I do not understand the reason for it. How \\theta_i(t) should behave when it hits the boundary depends on how the gradient-descent algorithm is defined in such cases.\nPage 6, line 7, page 7, line 18: \"The GD/SGD algorithm\": What is shown here is the GD algorithm and is not the SGD.\nPage 7, line 14: and (vanishes -> to vanish)\n\n[R1] David Saad and Sara A. Solla, \"On-line learning in soft committee machines,\" Physical Review E, volume 52, number 4, pages 4225-4243, October 1995.\n"}