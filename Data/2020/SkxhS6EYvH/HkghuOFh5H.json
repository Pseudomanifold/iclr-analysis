{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The information bottleneck Lagrangian corresponds to the optimization problem max I(Y;T) - \\beta I(X;T) over all T satisfying the Markov chain Y-X-T. Recent literature has shown that different values of \\beta may correspond to the same value of I(X;T) at the optimum, or the same \\beta may have multiple optimum values of I(X;T). This has motivated alternate versions of the Lagrangian where I(X;T) is replaced by a function of I(X;T). Building up on a recent suggestion to replace I(X;T) by I(X;T)^2, this paper looks at h(I(X;T)) for a strictly convex function h. The main result is that each \\beta corresponds to a unique I(X;T) in this setting, thus generalizing the squared Lagrangian formulation.\n\nThe paper is well-motivated and the author's have cited relevant literature. The paper's contributions, although correct, are incremental in my opinion. The main observation of this paper is essentially contained in the squared IB result, and I don't think the jump from squaring to using arbitrary strictly convex functions is sufficiently novel. For this reason, I recommend this paper be rejected. \n\nMinor comments: I have some comments regarding the technical presentation. These have not influenced my decision, but may be useful in future iterations of the paper.\n\n- Throughout the paper, the authors appear to assume random variables with finite, discrete supports. This is evidenced by their use of entropy as opposed to differential entropy throughout the paper. However, when I read the simulations section, the authors appear to use a continuous representation T. Is the theory valid for continuous representations T? \n- Related to above: What is the set \\Delta in equation (1)? If it is as stated \"the set of all random variables satisfying Y--X--T$, then there is no reason to expect a maximizer to exist. Is is more appropriate to use \\sup in (1) since the existence of an optimizing T is not guaranteed.\n- The notation used to depict distributions of random variables is archaic. For example, the authors use p(x) and p(y) for the pdfs of X and Y. This creates ambiguity: What is p(1)?  The authors should use more modern notation with pdfs denoted by subscripts of the r.v., such as p_X(x), p_Y(y), p_{Y|X}(y|x), and so on.\n"}