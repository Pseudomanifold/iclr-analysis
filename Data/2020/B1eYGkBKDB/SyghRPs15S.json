{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a quantization strategy for Transformer models and demonstrate results on a neural machine translation benchmark. Although the results look promising, the experiments are a bit sparse in terms of details. In my opinion, an empirical performance-focused paper like this really needs to provide a lot of details. Specific recommendations:\n\n- Table 1 compares BLEU scores across different papers. But is is not clear exactly whether they are comparable. In particular, what is the model size for the model in each paper? It would be insightful to show more details such as model size, compression rate, inference time, etc. \n\n- Table 2: the compression rate is shown as x4. But it would be good to show the actual model size as well. Some parameters, such as the bias terms, are not quantized, correct? \n\n- If you are also arguing that quantization gives inference time improvements, it would be good to show timing numbers too. I would recommend dedicate significant portion of the paper to this. \n\n- The ablation study is good. For empirical papers like this, these are the kind of analyses that will be particularly exciting to the reader in my opinion. So more analysis on different variants and practical strategies will be helpful. \n\n- I would cut all the pruning results (e.g. Sec 4.4). I don't see how that is related to quantization, which is the main contribution of the paper. The pruning results seem to be tacked on and make the paper a bit incoherent. \n\n- Finally, it is not very clear how does the proposed quantization method different from previous methods. The previous methods should be described with respect to your approach so it is easy to understand what is novel. \n"}