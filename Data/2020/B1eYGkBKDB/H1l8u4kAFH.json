{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an 8-bit quantization method to quantize the Transformer, which is a popular machine translation model. Uniform min-max quantization is used for computational expensive operations during the inference. The authors also propose to bucket the weights before quantization to reduce quantization error. Experiments are performed on WMT translation tasks.\n\nThe paper is overall easy to follow. One of my main concerns is about the novelty since both the min-max quantization and bucketing the weights before quantization are not new. Another concern is that one important goal of quantization is to speed up inference, however, no inference time is reported in the paper. Can the authors provide the inference time results on some typical cpu/gpus?\n\nFrom Table 4, the proposed quantization has much larger BLEU gain than the base transformer after 100k training steps, than that after full training (Table 2). Does this mean the proposed quantization converges slower at the later stage of training? How many training steps are run to reach the result of the proposed method in Table 1? Can the authors compare the training curve of the base transformer and the quantized one?\n\nFrom Table 4, the earlier the quantization starts, the better the final performance. What about the performance of training from scratch? Will this give better performance?\n\n"}