{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n\nThis paper proposes a method for reducing the required memory space by a quantization technique.\nThis paper specifically focuses on reducing it for the Transformer architecture. \nSeveral similar methods that shared the same motivation have already been proposed in the literature.\nMoreover, the main idea of the proposed method is the uniform quantization method, which has described in the previous study.\nTherefore, unfortunately, the technical novelty seems to be very marginal and narrow.\nIt is hard to say that this paper is innovative or has enough contribution and influence on the community.\nPlease clearly elaborate if the authors think that the proposed method has certain technical novelty, and I missed to recognize it.\n\n\n1,\nHowever, I am a bit surprised by the results of the proposed method; it successfully achieved the on-par result (or even improving) to the standard (non-quantized) models on the well-studied WMT14 EN-FR and WMT14 EN-DE translation tasks.\nAccording to Tables 1 and 2, it reduced the required memory space 1/4 from the original while maintaining any performance degradation, or even it slightly improves the performance.\nIt is good to see such a simple approach offers a better result.\nI think that the experimental results may offer a new insight into the network quantization technique. \n\n2,\nThe paper evaluates the proposed method only on WMT14 En-De and En-Fr benchmark datasets. The effectiveness of the proposed method would be more convincing if it was evaluated on different datasets and tasks as well, e.g. wikitext-103 for language modeling.\n\n3,\nNot much insight and discussion are provided on why the proposed method works better than other baselines. For example, the ablation study offers very interesting observations, but the authors only state the fact, and not providing any hypotheses for further analyzing the reason of working well for the proposed method.\n\n4,\nOne additional thing that I concern about this paper is the reproducibility of the experiments.\nI could not find a detailed explanation of how the authors obtained the reported results written in the tables.\nIf the results were obtained from a single run for each method with the single random initialization, then it may be a bit unreliable.\nIt is because we often observe more than 1point BLEU difference in the same model configuration except for a random initialization.\nTo clear such suspicion, the authors should report the average BLEU of the several runs or release their code for reproducing their experiments in future validation.\n\n\n\n"}