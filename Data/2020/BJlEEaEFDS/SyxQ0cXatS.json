{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization. They experimentally show that it is the moving averages of mini-batch means and variances (tracking) used in Normalization that cause the adversarial vulnerability. Based on this observation, they propose a new normalization method not only achieves significantly better results under a variety of attack methods but ensures a comparable test accuracy to that of BatchNorm on unperturbed datasets. The paper is clearly written, easy to read.\n \nStrengths:\n \nExplore the cause of adversarial vulnerability of the BatchNorm and assume that the tracking mechanism used in original BatchNorm leads to the vulnerability from experiment results.\nPropose a new and simple normalization method and perform extensive experiments to validate the efficacy of proposed method.\n \nWeaknesses:\nThough extensive experiments have been done by revealing what leads the vulnerability and the effectiveness of proposed method. The results seem unconvincing with respect to different datasets, since Cifar10 and Cifar100 are inherently connected. Would you mind performing some experiments on ImageNet? Since adversarial training on ImageNet is time-consuming, can you show us the result of Natural Training of different models with different norms on ImageNet and compare their robustness under different attack?\n"}