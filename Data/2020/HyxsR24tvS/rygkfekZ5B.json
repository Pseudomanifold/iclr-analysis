{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tries to stability problem when training GANs. They first show theoretically that if the generator distribution is not exactly equal to the empirical real data distribution, then the optimal discriminator is not a constant $\\frac{1}{2}$. They then argue that when the generator produces samples almost identical to real samples this leads to gradient exploding or vanishing and causes mode collapse. They then show that if we penalize the discriminator to give similar scores for data points and generated points which are close to those data points, this reduces the norm of the gradient of the generator. They argue that since it's hard to find \"close\" points, a good way to achieve the same results is by using a zero-centered gradient penalty instead. Another problem the author mention is that the exploding gradient can lead to assigning more fake points to a single real points. They show that this issue can be mitigated by considering that fake datapoints (which are close to some real datapoints) are real with some probability. In practice they propose to replace some of the real samples in a batch by the generated samples with the lowest discriminator score. They show empirically that this lead to improved performance on a variety of datasets.\n\nI lean slightly towards rejecting this paper. I think the novelty in this paper is quite small, the work should be better contrasted with existing methods to stabilize GANs and the contributions made clearer. I also think proposition 2 and proposition 3 are a bit vague and I'm not convinced they solve the problem of exploding gradients.\n\nMain argument:\n- Proposition 1 is not very novel, it's already known that when the support of the distributions don't match the standard GAN objective is ill defined see [1] and Arjovsky et al (2017).\n- The discussion about why this leads to gradient exploding and vanishing is a bit vague. Several paper discuss it more clearly for example [1], and already proposed a fix. Can the author contrast better their works with [1] and WGAN ?\n- I find proposition 2 and 3 a bit vague, indeed a regularization can reduce the norm of the gradient but if move the samples even closer, wouldn't the norm of the gradient still explode. Thus the regularization is just slowing down the explosion of the gradient.\n- In the experiments how did you choose of $M_0$ and $M_1$, how does the choice of $M_0$ and $M_1$ influence the performance ? It would be nice to a have a comparative study for different values of $M_0$ and $M_1$.\n- It would be nice to run the experiments with different seeds and report the standard deviation.\n\n\nMinor comments:\n- It's not clear which methods GAN-0GP-sample,  GAN-0GP-interpolation and GAN-0GP-sample-with-our-method refers to ?\n- There is some typos in the paper.\n\nReferences:\n[1] Arjovsky et al. \"Towards principled methods for training generative adversarial networks\" arXiv (2017)"}