{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper proposes mixing real samples, which a GAN discriminator is trained on, with some fake samples. The motivation is to reduce the norm of the gradient which the generator receives from the discriminator, especially at fake samples close to real ones. The paper provides an analysis of why reducing such gradient can stabilize training and avoid mode collapse, and shows some empirical results supporting that.\n\nComments:\n - The writing and structure of the paper needs some improvement. \n       *  Some sections are too verbose, and arguments are not easy to follow.\n       *  Notation is not very clear. For example, it is not clear what's the difference between; m, M1 and M0 in Table 1\n       *  The list of contributions constitutes mainly a list of observations/conjectures. The last point is probably the main solid contribution.\n - The proposed method is not particularly interesting, and while most of the paper is dedicated to motivate it, I don't think the arguments are solid enough to convince the reader.\n - While it is great that the paper experiments with multiple datasets, including CIFAR-10, CIFAR-100 and Imagenet, the paper mainly reports a single plot for the proposed approach compared to a single baseline. It is well known that GANs are highly sensitive to choice of hyper-parameters, so the paper should emphasize more that those results are consistent across an accepted range of hyper-parameters and baselines.\n"}