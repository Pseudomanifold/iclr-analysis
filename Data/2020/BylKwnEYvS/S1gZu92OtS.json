{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper derives results for nonnegative-matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star-convex towards randomized planted solutions. The star-convexity property is also shown to hold to some degree on real world datasets. The paper argues that these results explain the good performance that usual gradient descent procedures achieve in practice. The paper also puts forward a conjecture that more parameters make the loss function easier to optimize by making it more likely that star convexity holds, and that a similar conclusion could hold for DNNs.\n\nThe paper is rather well written, although there are many small typos or notation errors (of which I mention a few below). In addition, I have a few issues with the presentation of both the theoretical and experimental results. Although the results relating to star convexity seem compelling and interesting to understand the good practical performance of the usual simple NMF algorithms, I find the conjecture on concentration of measure a bit hand-wavy.\n\nIn particular, I have some questions regarding the main theorem:\n\n\t- \"r grows as $O(n^\\gamma)$ and m as $O(n)$\": Big-O means asymptotically bounded, i.e., that r is roughly smaller than n^\\gamma (which is a tautology for $\\gamma=1$ anyway), and that m is roughly smaller than n. Is that really what you mean? Or do you mean $\\Omega/\\Theta$ instead? Fact 2 uses \"recall our assumption that $r = c n^\\gamma$\" which seems to imply you mean \"r = \\Omega(n^\\gamma)\", which can also be informally stated as \"r grows as n^\\gamma\", without Big-O notation. Please clarify this notation.\n\n\t- Also, re $\\gamma \\geq 0.5$, I do not quite see why such a strong assumption is made. Wouldn't $r = \\Omega(n^{1/3 + \\varepsilon})$ be enough with respect to Fact 2 to get an asymptotically vanishing deviation probability? i.e., wouldn't the final result work with $\\gamma > 1/3`$, which is more general?\n\nGenerally, this begs the question of whether $r = \\Omega(n^0.5)$ is realistic. Can this be put in perspective with respect to other work?\n\n\t- What is Lemma 9 mentioned in the proof of Lemma 1? Is that in the paper? I could not find it. In particular, I am unclear on \"we can do a polynomial number of union bounds\": polynomial with respect to which variable?\n\nI also have the following additional questions on the experimental findings and the proposed conjecture.\n\n- Section 4.2 and Figure 5 uses the 'relative deviation', which is the standard deviation normalized by the mean. I am not sure what conclusion to draw from this, however. In particular:\n  - Why normalize by dividing by the mean?\n  - What is the evolution of the mean itself? If the mean is always greater than a few standard deviations, then the fact that curvature is getting more concentrated may not matter. How about showing instead the fraction of trials where the curvature was non-negative? (i.e., where we had convexity along the line)\n\n- I struggle a bit to make sense of the conjecture. In particular, 'concentration of measure' is related to randomness, but it isn't quite clear which 'measure' we're talking about here. Table 2 is also not very convincing: although the legend indicates 'increased width makes the loss surface increasingly locally convex', it only seems to hold for the fourth row (epoch=300). In addition, these means are reported without standard deviations, making it hard to judge whether to trust the ordering of these few numbers.\n\nTypos/unclear:\n\nintroduction: \"strictly non-negative factors\", what do you mean by strictly?\n\"randomly chosen or the global minimizer\" is a bit unclear - what is the 'or' over?\n\"Convex along straight paths towards the optima x*\": x* -> $x^*$ to be consistent with Definition 1.\n\"similar to Dvoretzky's\": I fail to see the similarity and how this is related to the present paper.\n\"we will assume that there is a planted optimal solution (U*, V*)\": U*, V* should be in bold to be consistent with further notation.\n\"how r and m depends on n\": depends -> depend\n\"as the size of the problem increaseS\"\n\"loss function *of* equation 2\"\nTheorem 1: \"at least \\geq\" is redundant\nTheorem 1: \"but with exponent -c r^{1/3}\" is unclear: which exponent is this? Write the statement in full instead.\n\"it's second derivative\": its?\n\"for unobserved Data\": why is 'data' capitalized?\nD3: \"clearly, equation 9 holds in this case\": not sure what is meant here - equation (10) does not imply (9) or reciprocally. Did you mean that equation 10 holds instead?\nD3: \"3 evenly spaceD \\lambda\"\nD6: Definition 2: \"iff there *exists* positive constants...\"\nD6: \"one can easily verify that independenT Gaussian variables\"\nD6: \"i=j, j=l, and so on\": what is 'and so on'? all indexes are completely arbitrary?\nD6: \"z = P(X_i, X_j, X_k, X_l)\": clarify notation $P$. In the main text, you've used the notation $p$ instead.\nD6: Equation (12) and (13): should the RHS be exp(-X) instead of exp(X)?\nD6: $- \\mu$ notation is a bit confusing: the text doesn't explain what it is, and lack of bold face hints that this is a scalar and not a matrix.\nD6: \"Lemma 9 says that each one can be expressED\"\nE1: End of proof of fact 3: do not include an equal sign if there is no left-hand side.\nE1: \"trace and linearity are linear operators\": linearity is linear?\n"}