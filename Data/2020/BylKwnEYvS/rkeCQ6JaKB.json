{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies loss landscape of Non-negative matrix factorization (NMF) when the matrix is very large. It shows that with high probability, the landscape is quasi-convex under some conditions. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. Implications on deep networks are also discussed. \n\nThe NMF problem is known to be NP-hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U\u2019,V\u2019, the MSE loss of NMF is convex with high probability.  The extra assumption is that the rank of U and V should also be large enough. Section 3 is devoted to prove this. It seems to me there are some typos which are quite serious and make the equation (3) incorrect. However, the main result (Theorem 1) still seems to hold. The equation (3) should replace W_2 with 2 W_2. The reason is in the appendix D.2, the definition of W_2 has missed this constant 2, which is the hat X\u2019\u2019(lambda) at lambda=0. Therefore, all the constants in the equation (4) need to be modified accordingly. In D.2, to derive the equation (9), it seems to me the McLaurin series should give 2 l\u2019\u2019\u2019\u2019(0) l\u2019\u2019(0) >= (l\u2019\u2019\u2019(0))^2, isn\u2019t it? The whole proof is quite long to check. In Fact 1, is the mu the mean of z? In Lemma 1, what is lemma 9? Fact 10 has a constant 2 which seems to be forgotten. Therefore significant modification is needed to correct all the errors. \n\nRegarding experiments, some data-set in Table 1 does not seem to me relevant to the paper (Assumption 1), in particular those with r < 10. Figure 6 shows that the gradient flow is close to a straightly line, suggesting that the gradient descent algorithm follows a convex landscape. The Figure 6(b) seems to me have not converged yet, as at step 10,000, the cosine is not as flat as the others. This means that maybe the gradient flow does not converge to the local minima (U^*,V^*). Further explanation about this is needed in the paper. Regarding optimization efficiency, it is not that convincing since even in the over-parameterized regime: the landscape become more convex, but there can be a lot of local minima which are not as good as the global minima. Therefore from an optimization perspective, finding global minima still remain challenging. I think it would be better to mention this somewhere in the paper. \n\nMinor typo includes: \nequation (5), write ||W_2||_F^2. \nEquation (12), hat 1 should be 1\n"}