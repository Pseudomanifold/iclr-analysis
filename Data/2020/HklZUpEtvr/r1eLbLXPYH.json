{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents an interesting connection between cycleGAN, penalized Least Squares (PLS) and optimal transport (OT). From the PLS with deep regularization formulation, the authors cast a general OT problem with a specific cost combining measurement and reconstruction errors (eq. 9). From this formulation, the problem is expressed as a combination of a cycle consistency loss and an OT loss. The formulation is more generic than the classical cycleGAN formulation. Qualitative experiments are conducted on two different scenarii: accelerated MRI and deconvolution microscopy, for which the proposed method achieves good performances.\n\nIn general I like the paper and the corresponding idea. However, I had a hard time understanding some of the key elements of the proposed method. Notably, in lemma 1, it is hard to understand in which case \n\\mathcal{X} \\times \\mathcal{X} / A \\union B is not an empty set. It is clearly the case whenever G or H are bijections, or whenever an exact reconstruction is achievable, but I guess a more open discussions could be conducted. Also, the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation. As noted in proposition 2, however, the corresponding OT loss can be computed on the entire subset X and Y (since the Kantorovich potentials match those obtained on the restricted set). As a consequence, we can see the overall training procedure when H is fixed (as it is the case in both experiments), as learning for a generator in a WGAN way, that also enforces a consistency constraint (that can be seen as a regularization of the OT problem). While I like this idea, I think there might be some better justification for it (it all starts with Eq. 9 and the choice of this model. Why enforcing this minimal cost equation ?). Note that the model imposes that p=q=1, which is somehow limited. I guess other choices of p and q could be possible, eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials    \n \nI am willing to revise my note positively provided that a sufficient number of convincing explanations are given on my previous remarks.\n\nMinor remark\nEq. 8 should include a weighting factor between the two terms since the dimensions are not the same. \nIn the proof of Lemma 1, since you derive an equality for T, do you have to establish the first inequality ?\nProp. 1 I guess there is an error on the first line (the integral is not over \\mathcal{X} \\times \\mathcal{X} / A \\union B given the previous definition\nMissing related work \nDLOW: Domain Flow for Adaptation and Generalization Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2477-2486\n\n"}