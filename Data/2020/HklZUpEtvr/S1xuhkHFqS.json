{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors present two contributions:\n1)\tThe primary contribution is to show that CycleGAN can be formulated as a probabilistic version of a particular penalized-least squares problem (theory)\n2)\tAs proof of concept, they apply their version of CycleGAN to accelerated MRI and deconvolution microscopy (application)\n\nWhile I find the idea to be potentially interesting, the presentation of the theory is unclear and not well-motivated; after reading, I\u2019m not convinced that the connection to CycleGAN is as significant as the authors claim. The experimental results are preliminary. My decision is to reject. Below are separate critiques on the sections.\n\nSection 2-3: Hope the authors could clarify / strengthen these points in revision:\n-\tSince the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well-motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al. However, after taking a look at those papers, I don\u2019t understand where this regularization term comes from. In these papers, the regularization term (i.e. equation 2 or 3 of Zhang et al) appears independent of y. Since this term is key to the paper, it should be well explained here. E.g. at the end of section 2.2: G_\\theta(y) is a CNN pretrained on what task?\n\n-\tIn the inverse problem, the objective is to estimate x from y. Therefore we care about \\argmin x in Equation (7). In the probabilistic setting presented in Equation (8), analogously the objective is to estimate \\pi^*, which is the solution to the primal problem. The theory shows that the primal formulation in Equation (8) is equivalent to the dual formulation in Equation (16), but does not show how the dual solution yields the primal solution, which is lacking as obtaining the primal solution seems to be the point of solving the PLS problem. (Interestingly, in Section 4, the authors are using the dual solution x = G_\\theta(y) as if it is the mapping given by pi(x|y)\u2026 this needs to be explained.)\n\n-\tThe authors claim that Proposition 1 shows that the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN. But looking closely at Proposition 1 and its proof, it seems that the equivalence holds only for specific weights, not for arbitrary weights. Additionally, the specific weights are unknown (they depend on the solution \\pi^* to the primal problem\u2026). I do not understand the claim that this is a generalization of cycle-consistency loss, nor do I see how the authors implement their version of the cyclic loss as it depends on unknown weights.\n\n-\tThe connection to CycleGAN seems to hold only when p=q=1?\n\n-\tEnd of section 3: The authors conclude \u201cour cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used\u201d. I don\u2019t see the connection between the theory and this claim. Even with CycleGAN, both generators can be arbitrary or fixed if one of them is known. \n\n-\tThe proofs are easy to follow, though perhaps they could be moved to the Appendix in favor of providing more motivation and explanation in the main text. \n\nSection 4:\n-\tThe authors motivate the problem with the PLS setup but then they use the learned regularization term x = G_\\theta(y) as if it is the mapping given by pi(x|y).  I am confused by this.\n-\tPutting aside the connection to the PLS problem, my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss, where one of the generators is known (and hence only one generator/discriminator pair is needed). I might be missing something, but I\u2019m not sure that this approach is different enough from CycleGAN.\n-\tConsidering that the authors have the ground truth, they could provide quantitative evaluation of their method against other methods, rather than showing a few qualitative results where it is working.\n"}