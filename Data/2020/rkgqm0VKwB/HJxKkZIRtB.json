{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE). The model leverage pre-trained BERT language models, making it very fast to train. The methods is evaluated on 5 standard NER+RE datasets with good performances.\n\nPros:\n\n- the paper is well written and very clear\n- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)\n\nCons: \n\n- I think the main source of improvement comes from the BERT representations used as input. As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.\n- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising..."}