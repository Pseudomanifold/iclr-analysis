{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a new joint learning algorithm that works for two tasks, NER and RE. The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence. Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE. The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks. \n\nThe design of the architecture is novel, but it is also not groundbreaking. Each network branch is from known structures, but the combination is not proposed before. \n\nThe submission has evaluated the proposed algorithms on four datasets and improved SOTA performances. The ablation study justifies the design details. \n\nThe writing is generally clear. \n\nNow critics: \n\nAblation study: \n1. As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors. \n\n2. I'd like to see another ablation study of whether RE helps NER. If you remove the RE component, does the NER performance suffer? \n\n\nWriting: \n3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?\n"}