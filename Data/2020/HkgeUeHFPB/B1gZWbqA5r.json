{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to add an extra vector quantized layer (VQ layer) to the existing QA models to help with the explainability. To be specific, for a given layer that needs to analyze, they map each continuous vector in the layer to a quantized vector through a codebook. These quantized vectors are further fed to more layers leading to answer prediction. After training converges, those quantized vectors can be used to probe the model\u2019s behavior. The experiments are conducted with two QA models, namely, FlowQA and SDNet on two different datasets QuAC and CoQA. The results demonstrate: 1) With only a small codebook size, the QA models can still maintain good performance. 2) The explainability can be achieved by calculating the statistics of the quantized vector. One interesting thing they found is that less frequent quantized vectors often denote more important tokens, which are responsible for making the boundary of answer spans.    \n\n\nStrength:\n1. The idea of using quantized vectors to explain the QA models\u2019 behavior is novel. \n2. The experiments are well-designed and clearly written.\n3. The results show the effectiveness of the proposed method.\n\n\nWeakness:  \n1.  This paper tries to unveil the \u2018\u2019reasoning process\u2019\u2019 using the proposed vector quantized layer. However, the \u2018\u2019reasoning process\u2019\u2019 here is not well-defined. The colored quantized vectors seem like another version of attention heat map to me \u2014 it just shows which part of the context is important regarding the given question, though the authors do find some interesting properties of those quantized vectors. Those vectors do not actually explain the exact \u2018\u2019reasoning process\u2019\u2019 of a QA model. \n\n2.  The authors try to establish that the colored quantized vectors are useful to explain the models\u2019 behavior. However, there are no experiments showing how good those explanations are compared to other methods. I would like to see a comparison between the proposed method with other methods, e.g. gradient-based methods, attention-based methods (Sarthak and Wallace 2019), and how those explanations agree with human judgments. \n\n\n\nJain, Sarthak, and Byron C. Wallace. \"Attention is not Explanation.\" Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.\n"}