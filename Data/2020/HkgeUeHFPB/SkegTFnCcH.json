{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper focuses on analyzing and visualizing trained QA neural models. The main idea is to quantize latent representations during training, making the model more interpretable eventually.\n\nThe general idea is understandable, but the details are very unclear and I found it extremely difficult to follow.\nAs far as I understand, one of the layers (in which kind of a network?) is replaced with a quantizing layer. This quantized layer is different than the original layer only in its forward computation; during backprop - the gradients from the next layer are copied backwards. The network is then trained (this is not said explicitly, until this point I thought that the paper discusses analyzing already-trained models) using this new layer, with 3 losses: QA loss, VQ loss and commitment loss. These losses are not discussed nor defined further. Finally, the quantized vectors are clustered using K-means.\nOverall, I think that this paper should be rejected. It is very unclear, the proposed technique is not compared to any existing technique, and the interpretation of the network - which is the main goal of the paper - does not produce any really useful insights.\n\nFor another example of vagueness, the paper refers to a figure by \"Each color represents a codeword\" (Page 2, para2). What is a codeword? What exactly do the authors refer to here? How did they choose the number of colors and how did they color each word?\nAnother example - \"a neural network can use only 20 or so vectors to cover almost all information\" (page 2, para3). Which vectors? The reader doesn't even know which model architecture is discussed at this point.\n\nThe experiments are based on extracting the quantized vectors from the trained model and running K-means with different values of K. What are the metrics used here? Table 1 shows F1 score, but I don't understand how precision/recall are computed? Are these the original metrics used in the benchmarks? what is exactly the experiment here? Table 2 is not referenced nor explained.\n\nI did not understand Section 5.2 - \"Meaning of Nonsense words\" at all. What are \"nonsense words\"? How they are chosen? Please define these more accurately. Since I didn't understand the settings, I did not understand the results in Table 3 (metrics/units are not mentioned there as well). \nI did not understand Section 5.3 as well (\"Position and Portion of Nonsense words\"). If I understand correctly, the main point here is that \"the higher a codeword\u2019s percentage, the less meaning it preserves\", but I don't understand how is this observed from Figure 3, and how does the paper measure \"preserving of meaning\".\n\nRelated work: \nThe paper misses a discussion and comparison with recent network-interpretation approaches, for example [1][2][3] below.\nAdditionally, some of the existing discussion is very lacking. For example - Doshi-Velez (2017) and Montavon (2018) - I have not read these papers, but why \"only those deep learning researchers are able to understand and utilize the results\"? This is a vague remark regarding these papers. Didn't these papers show do how they interpret their results? Did they release their code? Are these papers too complicated mathematically? \n\nMore questions to authors:\n1. What are codewords and codebooks? These are not defined nor explained.\n2. The main idea in the paper is quantizing some vectors in order to interpret them better. But how interpreting discrete vectors is better than interpreting continuous vectors in this case? If the main applied technique is K-means clustering, why can't we just cluster continuous vectors?\n\nMinor:\n* I personally don't like the use of phrases that humanize machines like \"the machine thinks\" and \"Models should know\". Treating neural models with human characteristics is inaccurate and misleading.\n\n[1] \"What Does BERT Look At? An Analysis of BERT\u2019s Attention\", Clark et al, 2019\n[2] \"Are Sixteen Heads Really Better than One?\" Michel et al, 2019\n[3] \"How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations\" Aken et al, 2019"}