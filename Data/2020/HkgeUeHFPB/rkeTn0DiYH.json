{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a way to understand a neural Question Answering model by first quantizing a layer of latent representations into a small number of codebook vectors, and then visualizing the frequency of such vectors being active as each question is sent into the model for an answer.  The vector quantization procedure uses k-means clustering with random initialization.  In experiments with two standard QA models (FlowQA and SDNet) with various quantization parameters, it is found that only 16 (or less) codebook vectors are needed to capture essentially all the information needed for answering (i.e., with negligible degradation in accuracy).  Additional observations are made on the forward-shifting of codeword activation as the question section progresses, and on the masking or activation of different codewords for non-semantic information like POS tags or starting and ending positions.\n\nWhile the work sets out to produce some intuitive understanding of neural QA models,  what gets achieved seems to be more like an illustration of the effects of some potential simplification.   It is hard to argue that seeing different words being activated for the relevant questions is a form of understanding; at least, such understanding is still very preliminary.  For example, it does not explain why the various other layers are needed,  how training may have affected the weights and attention weights, and how the words in the input questions trigger different words for the answer.\n\nGiven the way the context passages and the questions are arranged, the correlation (e.g. positions and question turns) between the questions and the words needed for answering them is rather obvious, and is hardly surprising.   Also, the questions are highly tailored to the passages, and the passages do not contain much of confusing or redundant text to challenge the models' capabilities.   Making too much of a claim that this produces understanding seems to be unjustified.\n\nIt could be that a better way to position this work is to go after the compression effect that the vector quantization process can offer, and to pursue a more systematic algorithm to generate compact and efficient representation of the models instead.\n\nThe paper is targeted to only readers who are familiar with neural QA models as it assumes much knowledge of such from the readers.\n\nQuestions:\n\nP.2, what exactly is the QA model being analyzed in Figure 1?   Is this your own or some public model you downloaded from somewhere?   What do you define as a context?   Such details should be given upfront, even  as a forward reference to the later sections of hte paper.\n\nP.7, colorful segments that may contain answers shift forward as time goes on.  Is this more a property of the way the used text is written, or the way the questions are ordered?  If you shuffle the questions and ask some later ones first,  will you break this trend?"}