{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper argues that an approximate Bayesian treatment of the parameters of neural networks with RELU activation improves predictive uncertainty and reduces the problem of asymptotic over-confidence for out-of distribution data. \n\nI have mixed feelings regarding the theoretical analysis. The paper is well written and the theoretical analysis is done thoroughly. \nHowever, the results are hardly surprising for anyone working with Bayesian neural networks. Furthermore, I am concerned regarding the usefulness of this analysis, since it is pursued for the trivial case of a Laplace approximation applied to the last layer only. This is neither novel nor convincing compared to previous approaches such as Kronecker-factorized Laplace approximations or variational approximations applied to all parameters. \nThe approach can be seen as just a Laplace approximation of Bayesian logistic regression applied to features extracted by a \u201cMAP-pretrained\u201d neural network. \n\nI suppose that the last layer Gaussian approximation can be seen as a sufficient condition for preventing the problem of asymptotic over-confidence in neural networks with RELU activations. If so, it might make sense to emphasize this. However, the significance of this result alone is questionable since one should not expect well calibrated uncertainties from a last-layer approximation.\n\nThe authors assessed correctly that a MAP-centered Gaussian preserves the decision boundary, whereas other methods such as MC Dropout do not. However, this is only the case for the *last layer* approximation. Furthermore, this is not even a generally desirable property (as presented in the paper). The decision boundary from the expected posterior predictive distribution (the quantity of interest) can be very different from the MAP decision boundary for non-linear models such neural networks for which all parameters are treated as random variables. \n\nRegarding the experiments: Given that the theoretical contributions are a little weak, I would have expected at least a stronger experimental evaluation. \n\nMinors:\n- Fig. 1 b) and 1c) are identical. \n- the probit approximation was first proposed by Spiegelhalter and Lauritzen 1990.\n- Laplace approximations are hardly \u201cfull Bayesian\u201d; it is just one type of approximation as so many other variants.\n"}