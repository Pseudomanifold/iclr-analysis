{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary: In this work, the author proposed a quite complicated model with multiple self-attention and inter-attention mechanisms for multiple-round conversational question answering. They evaluated this model on the CoQA dataset and compare it with several deep learning-based baselines.\n2. Overall assessment: This paper studied a very interesting problem and its motivation to use self-attention and inter-attention to thoroughly model and contextual information carried by passage and previous rounds of conversation make sense. However, this paper still has some space to get improved and it's not good enough to be published at ICLR yet.\n3. Comments:\n3.1 Figure 2 does not help much in this paper. It's hard to relate the explanation with components in this figure. I feel it would be helpful to label some symbols at some key positions and decompose this figure into multiple figures, with one for one component of the model. The authors can then include some more details into the figure and make it easier to read.\n3.2 There lacks some explanation of designs in the model. The reasons to stack soe many layers of attention mechanisms are not well explained. This also makes a bit hard to fully understand the model and the motivation behind it.\n3.3 One big concern to me is it seems to me that this model is overly designed and this design does not really capture the most important information for answering questions. Some error analyses and comparisons over variations of models may be able to prove the effectiveness of each component. But these do not appear in the paper.\n3.4 The baseline models are quite old and many recent proposed models are not used for comparison. Meanwhile, the performance presented in this paper is also outdated. It would be more convincing to see the model outperformed most recently proposed models."}