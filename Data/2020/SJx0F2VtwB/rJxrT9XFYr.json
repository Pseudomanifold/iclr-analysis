{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a model architecture for conversational question answering (QA), which incorporates conversation context into the traditional QA model. It leverages word embedding and BERT encodings, and uses multi-level attention (from question to context) and self-attention in order to output the answer.\n\nOverall, I am not convinced that the paper is worth to be published in ICLR. First of all, I do not see a meaningful model innovation in the proposed architecture---lots of components (question-context attention, self-attention and recurrent units) are traditional, commonly-used components in any QA model for many years (since 2016). Second, the proposed model is only evaluated on one dataset, CoQA, where the proposed model gives much worse performance than BERT baseline or other models. In particular, the fact that the proposed model leverages BERT but is worse than simply finetuning BERT (76.6 vs 78.7, single model) means that the proposed model does not contribute to the performance. In addition, the proposed model is significantly worse than SOTA in the leaderboard (90.4 single and 90.7 ensemble using RoBERTa, 86.8 single and 87.8 ensemble with ConvBERT, which I do not know about the architecture but probably also uses BERT, not better pretrained models). These baselines, including just finetuning BERT, are not reported in the paper, although they are accessible in the public leaderboard.\n"}