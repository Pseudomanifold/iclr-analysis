{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overall, I think the authors tackle a meaningful task of translating images when not all modes are available. The introduction, background and related work are adequate. \n\nHowever, I would like to point out that one of the paper's main contributions, namely the content and style separation, was not addressed enough in the body of the paper. The abstract mentions disentanglement of content and style by making an analogy to skeleton and flesh. In the last paragraph of section 2 (right before 2.1), the paper goes on about how content and style can be separated in the current scheme. However, in the evaluation section, there is no evidence that this separation actually happened. There is good amount of result that shows the image translation quality is competitive, in Table 1, 2 and 3, but none of the results suggest that the model actually learned meaningful separation of content and style. For example, I would expect to see how the generated image changes as the style vector is varied, or how the generation quality degrades if the model is trained without the style part. In fact, I almost suspect that the style-content separation is not really happening in the current formulation, because of the following reasons. Hopefully, the authors \n\nFirst, the content bottleneck is very large. In D.2. of Appendix, the size of the content encoder was W/4 x H/4 x 256. The size of the content bottleneck can contain 16 grayscale input images without any compression. Therefore, without additional constraint, the model can store all information in the content code. The only such constraint in the loss formulation seems to be Equation (3) that enforces that a randomly sampled style code is recoverable by E(G(s)), but it seems rather a weak constraint. \n\nSecond, the reconstruction loss may cause the model to be invariant to style. The formulation of the reconstruction loss, specified in Eq (5) is quite confusing, but let me state what I understood. The content code is sampled to be a subset of the content codes produced by E(x1, ..., xn). (It was rather confusing, because it is unclear what p(c) is, and how it relates to xi. Is the content generated from all existing modes of image x? If so, how is the probability distribution p(c) defined? The explanation under Eq (3), \"p(c) is given by c = E(...), and x ~ p(xi)\" needs more clarification.) The style code is randomly sampled from unit normal distribution. Then, no matter which value of style code is sampled, the loss dictates that the generated images should be same as x_i. Wouldn't this formulation make the generator essentially invariant to the style code? I do not see how this can encourage content-style separation. \n\nThird, the content consistency loss (Equation (2)) is somewhat ill-defined, because the loss of Eq (2) can approach arbitrarily close to zero. Since there is no prior distribution assumed on the content code, given E(x) and G(c), E'(x) = 0.1 E(x) and G'(x) = 10 G(c) will show the exactly same behavior. As such, the content and style code can shrink indefinitely without changing the behavior. \n\nTherefore, I think the paper's claimed contribution on content-style disentanglement is a bit overstated with several remaining points of unclarity. \n\nAdditionally, the paper's claim that the proposed method is a generalization of MUNIT and StarGAN because it can perform image translation when variable number of input modes are missing was not extensively verified. All results of the main body of the paper shows the case when exactly one mode of the input images is missing. For missing more than one mode, the only result is on the face dataset in the Appendix. I think this result is rather weak, because it only makes one qualitative result, and the reader would not have a sense for estimating the quality of the translation. In addition, it would be nice if the columns and rows are more clearly labeled. \n\nTo summarize, the paper is tackling a meaningful problem, and I think it could be a solid paper, but it does have major weakness that makes me hesitant to accept the paper. The two main contributions of the method, namely the content and style separation, and generalization of previous image translation method, were not evaluated or justified. "}