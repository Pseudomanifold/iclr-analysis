{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces the Subjective Reinforcement Learning framework to formalize the problem of using extra information to split large, nonstationary environments into separate, simple, stationary MDPs. First, the paper introduces and motivates the problem on a high level: the same state-action pairs may transit to different successive states and rewards because of nonstationary dynamics/reward function, or variance in the environment or tasks. This phenomenon is termed \"data confusion\". The paper then summarizes some related approaches to dealing with this phenomenon. Next, the paper introduces the subjective RL framework in detail in section 3:\n- Extra information (kappa) is needed to resolve the data confusion.\n- The \"subjectivity\" (h) is a function that maps the extra information to a vector of weights over \"subjective\" MDPs.\n- A policy is maintained for each subjective MDP, and the overall policy is the vector product of h and the vector of subjective policies.\nIn section 4, the paper presents 3 theorems arguing that using the subjective RL framework doesn't harm performance. The paper then gives brief guidelines for designing algorithms using the subjective RL framework before concluding.\n\nAt the present time I recommend rejecting the paper. It does not actually present a concrete solution method, instead simply giving brief guidelines for the reader to design algorithms by. The subjective RL framework unifies and subsumes several existing approaches, but I don't feel that in itself is a significant enough contribution to warrant publication. The theorems presented essentially argue that using the subjective RL framework does not harm performance, but there are no mentions of the computational costs involved with maintaining policies for each subjective MDP. In addition, it's not clear where the subjective MDPs come from.\nThe paper also had issues with clarity, including many grammatical errors.\n\nI think this paper tackles an important problem from an interesting point of view, but stops short of giving a concrete algorithm that can be implemented and tested. It seems like a good candidate for a workshop, which could be a good opportunity for discussion and feedback.\n\nGeneral comments:\n- The definition of rewards is confusing; script R was never defined.\n- \"minimize objective 1\" should be actually be \"maximize objective 1\"?\n- Rewards are usually defined over state-action pairs, not just states. Why choose this unconventional formulation for rewards?"}