{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper suggests that one common problem encountered by reinforcement learning algorithms in open environments is \"data confusion\", which essentially means showing the same input data with different --possibly contradictory-- labels/targets.\n\nThe proposed solution to this conceptual problem is to split the original MDP \"M\" up into multiple simpler MDPs \"Mk\", where M does contain possibly contradictory (\"confusing\") data, while each individual Mk does not contain any such problem and, even better, is stationary.\n\nThe \"subjectivity\" function \"h\" then has as role to split any data tuple across Mk, possibly using extra information kappa.\n\nFurthermore, several theorems show that under several conditions, the return of the subjective policy (learned via Mk) is not worse than that of the \"traditional\" policy.\n\n\nI lean towards rejecting this paper. The whole gist of the framework can be crudely summarized as \"if data contradicts, split up into non-contradictory sets using extra info.\" The motivation keeps repeating that no task-specific prior knowledge being necessary, but I believe this hinges on \"h\" being sensible, which might not be feasible without task-specific prior knowledge.\n\nFurthermore, and this is my main concern, there is not a single experiment demonstrating how any of this would behave in practice. It would be good to have one (possibly constructed) experiment showing that data confusion indeed is a problem in practice (intuitively, it is), and then a specific instantiation of the framework that solves this example. Furthermore, I am not convinced that the proposed bounds can easily be concretized for an instantiation of the proposed framework, especially when considering deep networks; again, this concern could be alleviated by an example instantiation. Proposing something that is in principle more general and \"in principle cannot be worse\" but then not demonstrating that it actually is the case is, in my opinion, not enough.\n\n\n\nFinally, and this is not a deciding factor in my rating, the paper has quite some writing problems. On the first page alone, I found a lot of spelling and grammatical mistakes (see list at end) and the notation is sometimes confusing to me. For example, \"R\" is defined as a mapping of S x /R -> [0,1], but what is \"/R\" (curly R)? And then in (1) R is used with a single argument while in (2) not anymore. I can guess what is meant, but it feels inconsistent. In Theorem 1, I believe it should be \"the gap \\delta >= 0\" and not \"the gap g >= 0\", no?\n\nAbstract and 1st paragraph mistakes (unfortunately, no line numbers in this template!): \"researches\" -> \"research\", \"algorithm designing\" -> \"algorithm design\", \"task-specific prior knowledge about tasks.\" -> \"task-specific prior knowledge.\", \"not known in prior\" -> \"not known a priori\", \"Classical RL model environment...\" -> \"Classical RL models environment...\".\nAlso, quite some citations are missing the year, e.g. Schaul et al., Papavassiliou&Russell, ..."}