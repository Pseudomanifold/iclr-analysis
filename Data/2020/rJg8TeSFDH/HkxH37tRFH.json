{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This exciting and insightful paper presents theorems (and illustrating examples and experiments) describing an equivalence of commonly used learning rate schedules and weight decay settings with an exponentially increasing learning rate schedule and no weight decay, for neural networks with scale-invariant weights. Hence, the results apply to a large set of commonly employed settings. The paper contains an interesting example of a neural network for which gradient descent converges if with batch normalization as well as with L2 regularization, but not when both are used. \n\nFrom a theory viewpoint, the paper offers new insights into Batch normalization and other normalization schemes. From a practical viewpoint, the paper suggests a way to speed up hyper-parameter search, effectively allowing to consider learning rate and weight decay as one parameter. \n\nA small gripe: this paper is a bit rough around the edges, and reads a bit like a draft (see comments on details below). \n\n\nDetailed Comments / advice / questions\n==================================\n\n- It often takes a bit of searching to figure out what proof goes with what theorem / fact. I recommend to add to each occurrence of \u201cProof.\u201d (before a proof) with a reference to the theorem or fact that is being proved, e.g. \u201cProof of Theorem 2.6\u201d. \n\n- The authors state that theorem B2 applies to \u201cgeneral deep nets\u201d. In this theorem, the limit R_\\infty could very well be zero (e.g. for networks with weights that are not scale-invariant), in which case the statement contains a division by zero. I wonder if the authors overlooked this or forgot to state an assumption in the theorem. Maybe I am missing something. Since this theorem does not appear to be critical to the main contributions of the paper, it may be easiest to remove the theorem if the division by zero is indeed a problem. \n\n\n- On page 6, if c_w(x) is independent of w, would c(x) be more suitable? \n\n- At the bottom of page 6, the authors state that \u201cAs a result, for any fixed learning rate, ||w_{t+1}|| ^4= \u2026\u201d. It appears that the authors get the expression for  ||w_{t+1}||^4 from the expression for ||w_{t+1}||^2 above (maybe by multiplying by ||w_{t+1}||^2?), but I don\u2019t see how. Could the authors explain this? Maybe authors mixed up w_t and w_{t+1} by accident? \n\n- The authors claim to \u201cconstrict better Exponential learning rate schedules\u201d. Since the authors only perform a limited evaluation of their proposed learning rate schedule on CIFAR10, I suggest qualifying this statement. \n\n- Theorem 1.1 does not introduce what \\tilde{\\gamma} is. It\u2019s somewhat obvious, but I would state it nonetheless. \n\n- The authors state that \u201c\u2026the exponent trumps the effect of initial lr very fast, which serves as another explanation of the standard wisdom that initial lr is unimportant when training with BN\u201d. I don\u2019t think that this constitutes a full explanation without further argument. I am also wondering if \u201canother\u201d is appropriate here: I am not aware of any (other) mathematically precise explanations of why initial learning rates do not matter in this set-up. If the authors are, they should cite it. \n\n- The authors often forgot spaces before \\cite commands. If you are trying to avoid a line break before the \\cite command, you can use a tilde ~ , like this \u201cGroup Normalization~\\cite{somepaper}\u201d. \n\n- Please introduce the abbreviation LR for \u201cLearning Rate\u201d, and always use the all-upper-case version (not \u201clr\u201d). \n\n- Definition 1.2 has a broken \\cite . \n\n- Theorem 2.7 should introduce \\hat{eta}_t, but it doesn\u2019t. \n\n- Appendix A.1 contains a broken sentence (\u201cas a function of\u2026\u201d)\n\n- It\u2019s odd that the proof for theorems B1 and B2 appear before theorems B1 and B2. I would restructure the appendices to improve this. \n\n- Theorem B2 contains a stray \u201cwhen\u201d, and \u201cexists\u201d should be \u201cexist\u201d\n\n- What the authors call \u201cProof of Theorem 2.4\u201d in the appendix is really a proof of the rigorous version of Theorem 2.4 - Theorem 2.7! The proof should in my opinion be labeled \u201cProof of Theorem 2.7\u201d\n\n- The typo \u201ceventsequation\u201d should be replaced with something like \u201cthe events from equations\u201d\n\n- Replace the colloquialism \u201cnets\u201d with \u201cnetworks\u201d. \n\n- Replace \u201cBatchNorm\u201d with \u201cBatch Normalization\u201d\n\n- \u201cCOvariate\u201d has casing issues\n\n- \u201cRiemmanian\u201d should be \u201cRiemannian\u201d\n\n- \u201cBNhas\u201d should be \u201cBN has\u201d\n\n- The paper\u2019s title states that the results are for batch-normalized networks, while the analysis appears to be more generally for networks with scale-invariant weights, which as the authors point out can arise from mechanisms other than batch normalization. Have the authors considered changing the paper\u2019s title to better capture what their work applies to? In terms of discoverability, the authors would do the community a service by titling the paper in such a way that it captures the set-up well. "}