{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper greatly reduces the gap between binarized and real valued imagenet, using a variety of techniques. The most significant contributions of this paper are engineering based, and the careful combination and integration of approaches from previous papers. I believe that this is of significant practical importance to the field. I particularly appreciate the effort put into developing a very strong baseline that combined ideas from many previous papers!\n\nMy biggest concern is that ResNet is itself a very wasteful architecture in terms of compute and parameter count. If the goal is to develop a compute- and memory-efficient architecture, it would be good to also consider real-valued-network baselines that were proposed with computational and/or memory efficiency as a design goal.\n\nAdditionally, the specific choices for the new student-teacher loss, and new scaling network architecture, seem fairly ad-hoc.\n\nDetailed comments:\n\n\"this implies a reduction of 32\u00d7 in memory usage\" , assuming the parameter count is held constant.\n\nFig 1 right: This is motivated in terms of preserving scaling factors that are lost by the binarization, but the functional form for this makes it look a lot like a learned gating operation. If the sigmoid is dropped from the architecture, does performance worsen? It would be nice to see some discussion of the degree to which this is helpful because it reverses information loss due to binarization, vs. introduces a new architectural feature which is itself helpful.\n\nAdd a sentence describing what \"double skip connections\" are. I wasn't familiar with this phrase.\n\neq. 2:\nThis functional form is pretty weird.\nWhy is Q a square norm rather than a norm? Square error on an already-squared property is an unusual choice.\nWhy is the denominator itself a norm? Taking the norm of a square norm is similarly an unusual choice. (eg, why not just take an average or sum over Q) \nSay what Q_s and Q_t are (student and teacher network from context)\n\n\"thus, at test time, these scaling factors will not be fixed but rather inferred from data\" nit: Would not generally call this an inference process. \"Inference\" typically refers to values that are computed indirectly (eg by Bayesian reasoning), while in this case the values are computed directly. Would rather say that scaling factors are a function of data, or are determined by data, or similar.\n\n\"By doing so, more than 1/3 of the remaining gap with the real-valued network is bridged.\" text is shifting back and forth between using % and fractional gap to describe benefits. Would just use one measure consistently.\n\nComputational cost analysis:\nThis is very useful.\nNote though that ResNet is a very wasteful architecture in terms of compute! It would be good to include a comparison to imagenet architectures that have computational and memory efficiency as a design goal. (eg, MobileNet comes to mind)\n\nVery nice on the ablation studies."}