{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to generate sparse multilingual embeddings. The key idea is to build only one set of source basis embeddings, and then represent all multilingual embeddings as a linear combination of these source embeddings. I felt the paper is a nice extension of the Vyas 2016 paper. Compared to existing approaches, their method will be faster to train and will need less data (particularly useful for low resource languages). Since I am less aware of work in this area, I cannot comment on whether the evaluation is complete. Particularly, I wonder if there is a qualitative way to show interpretability of the sparse vectors created by the method. We currently only have QVEC-CCA numbers to judge interpretability.  A few more suggestions:\n\n1. I got confused by the sentence \".. over a reduced number of parameters for each target language as it treats D_s as D_t .\". Things got clear from the equations, but will be good to fix.\n\n2. Figure 1 was very difficult to understand. Ideally your caption should be enough to understand the figure. "}