{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces SRNN to model the hamiltonian of a dynamical system. Authors break down the design choices made in the algorithm and validate each one through experiments. \n\nI like the motivation of the paper of solving general physics problems using neural networks. The paper is well-written and the ideas are communicated properly.\n\nHowever, I have some concerns regarding the experimental results:\n\nIn Figure 1, even though the fifth mass seems to follow the exact pattern of observations for single-step L-L H-NET, the L2 error is increasing after each time-step suggesting that the fifth mass might not be the best one to consider for comparison between different algorithms here. Would be nice if a comparison between all trajectories was presented and discussed (perhaps in the Appendix)\n\nThe idea tested in section 4.2 seems not novel since previous works have already used recurrence to mitigate problems faced using one-step training. (https://arxiv.org/abs/1902.09689) \n\nI like the optimization method proposed in Section 4.3 and the results in Table 1 and 2 seems to justify its effect. Does this approach also handle degenerate cases in which different initial states can lead to the same trajectories after some amount of time? To me, it seems like a boundary on the noise variance should be assumed. Otherwise, no amount of optimization would be actually able to retrieve the correct initial state. Is this true or am I missing something?\n\n\n\n-minor comments:\n\n1. The conclusion section is missing from the paper. It would be nice to recap your findings there.\n2.  In section 2, the \"universality property\" of Niu's recurrent model should be explained or referenced. \n3. In Figure 1, the second \"Left:\" should be changed to \"Right:\".\n4. In Table 2, the Error std. for O-NET (E-L) should be bold not H-NET (L-L) unless it was intended to highlight the values for the model with the lowest mean error. (please clarify this in the paper.)\n5. Section 6: \"We focus on this section\" should be changed to \"We focus in this section\".\n\n \nOverall I think the work is interesting but it lacks some justifications regarding the claims made (as mentioned above), and although it is generalizable to other tasks and systems, it does not have sufficient novelty in its algorithm and approach.\n\nAs of now, I am recommending a rejection, but I am willing to reconsider my score should the authors address the above concerns.\n"}