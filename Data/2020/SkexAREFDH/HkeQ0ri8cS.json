{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "While network pruning & quantization have been studied intensively in the literature, this paper presents a regularizer that can simultaneously achieve the above two goals. The authors further propose an optimization algorithm based on ADMM for training. Experimental results demonstrate great compression ratios compared with various baseline methods.\n\nOverall, I suggest authors have better placement of their novelties in the literature, more justifications on ADMM and stronger baselines in experiments.\n\nQ1. \"automatically according to a target model size without using any hyper-parameters to manually set the compression ratio for each layer\".\n- Personally, I do not feel this claim is proper. Indeed, by setting S_{budget} in (1), one does not need to care about hyper-parameter at each level separately. However, this does not mean all other methods have to set such hyper-parameters in a layer-wise manner. For example, one can add a sparse regularizer and then set the overall sparse penalty, like \"Sparse Convolutional Neural Networks. ICCV 2015\".\n- Besides, I do not think the method works in an automatical way. 1) S_{budget} needs to be set and 2) how rho (for ADMM) is tuned is not clear.\n\nQ2. It is good that authors organize related work in \"pruning\", \"quantization\" and \"AutoML\". It will also be good if authors can organize all tables in this manner as well.  For example, in Table 2, which category does BC-GNJ belongs to? In this way, it will be easier for readers to understand why the proposed method can be better than all compared methods.\n\nQ3. Could the authors offer model size in all tables?\n- Personally, I do not feel comfortable to see such a huge compress ratio in, e.g., Table 4. While 205x can be impressive, the real reduction in model size can be small.\n- Could authors offer STD in tables for small data sets, e.g., Table 4? I am no sure the reduction in memory size is significant, e.g., between \"Ye et al 2018b and Ours\".\n\nQ4. Since the authors have done a search in methods from three directions, i.e., \"pruning\", \"quantization\" and \"AutoML\", could the authors compare with the more recent or stronger state-of-the-art?\n- While 2120x in Table 2 can be impressive, it is not meaningful if the based network is too weak (i.e., LeNet on MNIST). Perhaps, authors can search on networks identified by NAS methods, e.g., \"Efficient Neural Architecture Search via Parameter Sharing\". If authors can achieve the same/or slightly less compressing ratio on these networks, I will definitely give acceptance for this submission.\n- For Table 4, while MobileNet can be a popular choice, MnasNet can be a better network.\n\nQ5. Could the authors offer the learning curves, i.e., epoch v.s training/validation loss?\n- It is better to show how the proposed algorithm converges, and how fast it converges.\n\nQ6. Could the authors show the impact of rho?\n- There is no convergence guarantee of ADMM on such a problem, the choice of rho can be important.\n- How the accuracy/sparsity/learning curves change w.r.t. rho?\n\nQ7. Could authors offer more discussion with Ye et al 2018b?\n- A combination of  \"pruning\" and \"quantization\" using ADMM is already explored in this paper."}