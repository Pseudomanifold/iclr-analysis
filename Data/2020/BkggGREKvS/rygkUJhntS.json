{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes two methods of biasing agents towards learning coordinated behaviours and evaluates both rigorously across two existing and two novel multi-agent domains of suitable complexity. The rigour of the empirical evaluation and its documentation is exemplary. \n\nThe related work section could be improved by including a section on the closely related work in the area of opponent modelling. This literature is touched on shortly afterwards alongside the related proposed method in Section 4.1 but only by a single citation. A more thorough review in Section 3 would improve the positioning of the paper.\n\nFrom figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better than the existing baseline method MADDPG on the CHASE environment. In particular I think it would improve the discussion of the results to elaborate further on why the proposed methods do not improve, but also do not detrimentally affect the learning performance of agents in this environment. \n\nIn Section 6.2 the authors note \"MADDPG + policy mask performs similarly or worse than MADDPG  on all but one environment and never outperforms the full CoachReg approach.\" I found it also interesting to note that in that one environment (SPREAD) adding a policy mask both outperformed MADDPG and was more sample efficient than CoachReg. Again what is it about this specific environment that causes the policy mask ablation to be sufficient, enabling it to match the asymptotic performance and learn quicker than the full CoachReg method proposed?\n\nAll references to papers both published and available on arxiv should cite the published version (e.g. Jacques et al. 2018 was published at ICML). Please revise all references if accepted.\n\nMinor Comments:\n1) Page 2 \"two standards multi-agent tasks\" -> standard\n2) Page 7 \"improve on MADDPG apart on the SPREAD environment\" -> apart from on \n3) Page 9 \"each sampled configuration allows to empirically evaluate\" -> allows us\n"}