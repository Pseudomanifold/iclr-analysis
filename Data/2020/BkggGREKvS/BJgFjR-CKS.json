{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contribution:\n\nThis paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents:\n - TeamReg: the agents are trained with an additional objectives of predicting other agents' actions, and make their own action predictable to them\n - CoachReg: the agents use a form of structured dropout, and at training time a central agent (the \"coach\") predicts the mask dropout mask that should be applied to all of the agents. An additional loss is provided so that the agent learn to mimic the coach's output from their own input so they can still apply the dropout at test time, when the coach is not available anymore.\n\nThese two methods are evaluated in 4 different MARL environments, and compared against their ablations and vanilla MADDPG.\n\n\nReview:\n\nThe paper is well written and easy to follow. It generally motivates well the design choices, both intuitively and experimentally, using numerous ablations. It includes the analysis and explanations of the failure modes, which is valuable in my opinion.\n\nThe two main limitations of the work are the following:\n - Limited scale of the experiments. The majority of the experiments contain only 2 agents, and the last one merely contains 3. It is unclear whether the additional losses proposed in this work would still perform correctly with more agents, since the regularization pressure will increase.\n - No comparison to SOTA MARL methods. The only baseline method presented here is MADDPG, upon which this work is built. Recent work tend to show that it is easily outperformed, hence comparison to stronger baselines (QMIX, COMA, M3DDPG, ...) would be advisable to assess the quality of the policy found.\n\n\nAbout the policy masks:\n- what are the value chosen for K and C?\n- Is there a reason why this particular form of dropout was chosen? Since it occurs after a fully-connected, it should be equivalent and more straight-forward to mask out C contiguous values.\n\n\nFinally, it seems to me that the two methods presented here (TeamReg and CoachReg) are not mutually incompatible. Is there a reason why you didn't to apply both of them simultaneously?\n"}