{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a method for detecting rumours in text. Early on in the paper, the authors claim that this method:\n1) is more accurate in rumour detection than prior work, \n2) can provide explainability, and\n3) does not need labelled data because it is trained on synthetic \"non-rumour looking rumours\".\n\nAll three of these statements are problematic.\nThe experimental evaluation uses a small dataset of rumour classification (about 15000 tweet related to 14 news topics) and an even smaller dataset of gene classification. The rationale is to use the gene classification task as a proxy for rumour detection. This is not valid. The gene classification task does not contribute to the evaluation of the rumour detection method. The rumour classification dataset is relatively small, but even more importantly, the experimental results on that dataset are not thoroughly analysed, for instance through an ablation test. \n\nExplainability is not evaluated experimentally, nor formally proven. \n\nThe claim that the method does not need labelled data because it is trained on synthetic \"non-rumour looking rumours\" is shaky, because 1) one could train the method on labelled data, and 2) it is not clear how \"non-rumour looking rumours\"  are guaranteed in the synthesis phase (how are they defined? how are they evaluated to be \"non-rumour looking rumours\"? etc).\n\nNote that there is no definition of what sort of data representation corresponds to a \"rumour\" in the paper. \n"}