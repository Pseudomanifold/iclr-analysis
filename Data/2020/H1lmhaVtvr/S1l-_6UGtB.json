{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an approach to do reinforcement learning without reward engineering. Instead, steps to reach goals are used to update policy.\n\nHere are some questions:\n(1) The reason to use cumulative distance instead of greedily chooses the shortest distance is to avoid risky state. But it is also mentioned in the paper that the experiments in the paper all have deterministic dynamics (and one of the assumptions in Appendix B). Can the authors elaborate on this? For example, how is the performance of a policy that just directly minimizes the distance function? Instead of using the made-up toy example.\n\n(2) For the locomotion task examples, I am confused as to why choosing the state with the highest immediate reward will be a good preference state. For example, a humanoid that falls forward can have high forward velocity while has no chance for recovery. While the performance seems reasonable, the systems presented are mostly stable systems (with the exception of hopper). It will be nice to show similar performance in more unstable systems, e.g, walker2d, humanoid.\n\n(3) Some assumption in the proof in appendix B is not true. d(g, g) is not 0 for all dynamical systems. For example, a hopper with a state that has none zero forward velocity will take more than 0 steps to return to the same state.\n\n(4) \u201cBecause the trajectories have a finite length, we are effectively ignoring the cases where reaching sj from si would take more than T \u2212 i steps, biasing this estimate toward zero, but since the bias becomes smaller for shorter distances.\u201d I am confused about this sentence, what does it mean? For example, if the dynamical system is the hopper, (s_i, s_{i+1}) will make d(s_i, s_{I+1}) be 1, and for all we know d(s_{i+1}, s_i) can be huge. How does this sentence apply in this situation?\n\n(5) An appendix with hyperparamters will be helpful. For example, how much data are needed for updating the distance function?\n\n(6) \u201cWe also evaluated DDLUS on the InvertedDoublePendulumv2 domain, where the task is to balance a pole on a cart. As can be seen from Figure 6, DDLUS can efficiently solve the task without the true reward, as reaching dynamically far states amounts to avoiding failure as far as possible.\u201d Why reaching dynamically far states is equivalent to avoiding failure? Shouldn\u2019t the policy learn to stay as close to the initial state as possible?\n\nDespite the questions raised above, the techniques are interesting, simple, and effective, as demonstrated on some image-based tasks on real robots. While I understand that this method will be very effective for relatively static tasks like the valve turning, I fail to see how it can apply in unstable locomotion tasks, as mentioned in some of the questions above. \n\nNormally I will give rating 5 to this paper, but the system doesn't give me this option, so I have to lower it to 3 instead. "}