{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper analyses the convergence of TD-learning in a simplified setup (on-policy, infinitesimally small steps leading to an ODE).\n\nSeveral new results are proposed:\n- convergence of TD-learning for a new class of approximators (the h-homogenous functions)\n- convergence of TD-learning for residual-homegenous functions and a bound on the distance form optimum\n- a relaxation of the Markov chain reversibility to a reversibility coefficient and convergence proof that relates the reversibility coefficient to the conditioning number of grad_V grad_V^T.\n\nWhile the theory applies to the ideal case, t provides some practical conclusions:\n- TD learning with k-step returns  converges better because the resulting Markov chain is more reversible\n- convergence can be attained by overparmeterized function approximators, which can still generalize better than tabular value functions.\n\nThe experiments corroborate the link between reversibility factor and TD convergence on an artificial example."}