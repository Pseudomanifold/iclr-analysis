{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "What is the specific question/problem tackled by the paper?\n\nThe paper studies convergence & non-divergence of TD(0) with value function estimates from the class of ReLU deep neural nets (optionally with residual connections). \n\nIs the approach well motivated, including being well-placed in the literature?\n\nYes.\n\n\nDoes the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nThe support is adequate.\n\n\nSummarize what the paper claims to do/contribute. Be positive and generous.\n\nThe paper takes a first step in bridging the gap between existing analyses of TD(0): convergence with linear function approximators, and convergence in reversible MRPs. The first contribution is a non-divergence result for the method with ReLU deep neural networks with and without residual connection. The second result connects a notion of reversibility of an MRP and the condition number of the neural tangent kernel, saying that better conditioning can make up for the lack of reversibility.\n\n\nClearly state your decision (accept or reject) with one or two key reasons for this choice.\n\nI vote for accepting the paper.\n\n\nProvide supporting arguments for the reasons for the decision.\n\nThe paper is well written, with a clear story and accessible explanations. The paper provides novel results and an interesting line of work that allows us to tradeoff good behavior of the function space and of the MRP in order to have TD converge, in the sense that as the MDP becomes less and less reversible we can make up for it by having properly conditioned matrices.\n\nThe paper also makes an adequate choice of function space to restrict the guarantees to.\n\n\nProvide additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\nIt seems that Theorem 3 cannot recover the linear case results for properly conditioned Phi matrices and irreversible MRPs. It would be good if the result could really interpolate between the two previously studied cases (linear irreversible and nonlinear reversible). Alternatively, a comment about this limitation of the result would improve the paper."}