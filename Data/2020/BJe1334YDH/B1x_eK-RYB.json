{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Using a combination of RL-based solution as a warm start, this paper shows that by adding more improvement and perturbations, the quality of the solutions for capacitated VRP can be further improved. The numerical experiments show that with these extensions, the proposed mechanism is able to perform better than the SOTA LKH3 OR-based method in a shorter amount of time. I find the paper novel and interesting, but I have a few suggestions to further improve it:\n\n1) Most of the mentioned operators are also valid for TSP. To provide a baseline for further research, I am expecting to see the performance of the proposed approach in TSP as well. \n2) I hope that you make your code and saved models publicly available for future researchers since it might not be easy to replicate your numbers.\n3) Maybe in future research, the ensemble idea can be tested for making the solution independent of problem size. This is an important possible extension.\n4) That would be great if you could add a section for sensitivity analysis. How the model trained for VRP100 is working for say, VRP200, or VRP50?\n\nMinor: \n1) It worths mentioning how e-greedy that is primarily used in value-based methods is incorporated in policy gradient exploration.\n2) Page 7, it should be \"Figure 3(a), (b), ...\"\n3) Table 3 and Table 5 overlap. Maybe, it is better to merge them together. \n"}