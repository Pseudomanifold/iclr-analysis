{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This work presents a \"factorized multimodal transformer\" (FMT) as well as the concept of \"factorized multimodal self-attention\" (FMS). The basic idea behind these approaches is to structure the self-attention layers in a transformer to explicitly account for the fact that there are different modalities in the input, i.e., attention is computed across modalities as well as within modalities in a structured way---rather than just naively applying a transformer to the concatenated multi-modal input. Numerous implementation adjustments are made in order to ensure the model is not too overparameterized, with the primary strategy being to use 1D CNNs to reduce the dimensionality of the internal representations after applying FMS layers. One multi-modal language benchmarks the proposed approach outperforms a number of strong and recent baselines.\n\nAssessment: Overall, this is a borderline paper. It is a very incremental contribution in terms of methodology, but the empirical results are strong. The authors themselves acknowledge that there proposed approach is very similar to Tsai et al. 2019, which also proposes a multi-modal variant of the transformer model. The key distinctions---as the authors point out---is (a) the use of 1D CNNs to reduce the dimensionality of the intermediate layers of the network and (b) the use of trimodal interactions, and (c) the use of unimodal self-attention in addition to intermodal attention, and (d) the fact that this approach uses the full time-domain as input. These differences seem to lead to better performance. However, they are not major conceptual changes or advancements and essentially amount to minor architectural differences. \n\nAs another minor comment, the introduction frames the contribution as being generally relevant for multi-modal learning. However, the proposed architecture has many design decisions made specifically for the task of multimodal language modeling, and the model also assumes a time-varying input. This should be acknowledged in the paper and the framing should perhaps be adjusted. \n\nFinally, the paper is very lacking in detail (e.g., detailed equations and derivations), and the paper does not appear reproducible based on the paper description alone (i.e., without code). If the code were not linked, this would be a major issue. The paper is only understandable by reference to the original transformer work and---due to the lack technical detail---does not stand alone as a research contribution. Indeed, I believe I understand the workings of the model, but there are certain aspects that would require me to investigate the code to fully understand, which indicates that the paper could substantially benefit from improved technical writing and more detailed descriptions of the methodology. \n\nReasons to accept:\n- Strong empirical results\n- Thorough treatment of baselines and related work\n\nReasons to reject:\n- Incremental contribution (especially compared to Tsai et al., 2019)\n- Writing lacks sufficient technical detail\n"}