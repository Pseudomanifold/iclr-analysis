{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes Factorized Multimodal Transformer (FMT) for multimodal sequence learning. \nThe \u201carchitecture\u201d consists of multiple multimodal transformer layers (MTL), which in turn consist of multiple Factorized Multimodal Self-attentions (FMS). \n\nUnfortunately, I cannot describe their methods and model in any detail. This is because the paper lacks details about the used methods and contributions. There is not a single equation that explains the model or objective function. In fact, the paper does not contain a single equation at all (there is 1 inline equation describing the data). \n\nInstead, the paper reads like a vague procedure, lacking explanations and details. Some examples:\n- \u201cThe input first goes through an embedding layer, followed by multiple Multimodal Transformer Layers (MTL). Each MTL consists of multiple Factorized Multimodal Self-attentions (FMS).\u201d -> However, the paper never explains what FMS or a summarization network is. Please just put some equation or reference there.\n- \u201cS1 and S2 are two summarization networks.\u201d -> What is that? \n- \u201cthe output of each of the attentions goes through a residual addition with its perceived input (input in the attention receptive field), followed by a normalization.\u201d \n- \u201cFor supervision, we feed this input one timestamp at a time as input to a Gated Recurrent Unit (GRU) (Cho et al., 2014). The prediction is conditioned on output at timestamp t = T of the GRU, using an affine map to dy.\u201d\nBesides these procedural explanations, the paper contains figures to visualize the model. I am sure the authors tried their best to make this very intuitive and easily accessible for readers. I certainly value that intention. \nHowever, these visualizations are useful only if presented together with a detailed description (e.g. a formula) of the model, objective function, etc. I can only speculate what is shown in Fig. 2. The caption does not at all explain what is shown. \n\nI also can not tell what the contributions of this paper are either. The authors propose this FMT consisting of MTL consisting of FMS. But what exactly is novel here? What distinguishes it from related work? Is it the factorization of \u2026 something? Where does anything factorize actually? This seems to be an important part of their approach since the term is used in the title of the paper and in most of the components. But again, there is not a single equation that would show the factorization. \n\nIn summary, this paper should be rejected, because it lacks details about used methods, models, objective functions, algorithms, etc. This makes it impossible to judge the quality and validity of the work and further makes it irreproducible. I am sure the authors can greatly improve the quality of this paper by writing it in a more scientific manner and providing all necessary details. "}