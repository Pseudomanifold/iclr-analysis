{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript proposes a novel formulation of the MLP to address predicting a symmetric positive definite (SPD) matrix from an input vector or matrix.  While the field has had methods for years to estimate SPD matrices (such as the covariance matrix estimate in the reparameterization trick), this manuscript proposes a markedly different approach based on a different layer structure and repeated normalization steps based on Mercer Kernels.  Additionally, the loss used can be modified to use more PSD-specific losses, such as the symmetrized von Neumann divergence, rather than the traditional quadratic loss.  This loss appears to give significantly better solutions on synthetic data.\n\nWhile there is some interesting and potentially useful novelty in the approach, I have some concerns about the empirical evidence and modeling to truly determine the mMLP's utility.\n\nFirst, in the synthetic data the mMLP with the l_QRE loss outperforms the l_quad loss with regards to both E_QRE and E_quad.  Why does mMLP/l_QRE outperform on E_quad? As l_quad is actually designed to minimize this error, this is surprising and needs additional explanation.  Additionally, there are two major changes between mMLP and the MLP.  One is the network structure, and the second is the trace being normalized; which change really induces the improvements in the performance? If the l_QRE loss was used in the MLP, would you get similar improvements?\n\nThe network structure as a whole needs greater validation.  A major difference in (3) is that the weight matrices W_l are applied as a both a left and right multiplication.  Given that the \\mathcal{H} operation symmetrizes and normalizes the matrix, a symmetric operation isn't strictly necessary here.  Using both multiplications leads to quadratic properties, which in my experience are less stable in the optimization.  Can the authors validate this structure versus the simpler structure of simply using left multiplications?  Or, in other words, is this weight multiplication structure helpful or do the benefits really come from the \\mathcal{H} operation.\n\nI think that the heteroscedastic regression experiments don't evaluate on one of the key issues, which is uncertainty estimation.  The trace-1 normalization is highly restrictive, so I imagine that that this method is getting the uncertainty incorrect.  Also, heteroscedastic regression has a long history in neural networks, dating back to at least Nix and Wiegand in 1994.  The manuscript needs to be updated to reflect the historical work and current literature on the topic.\n\nPlease check Table 5(a), which states that you are only using a small number of training samples.  Also, given the relatively small sample size of these datasets, please comment on the uncertainty of the results.  How confident are you that the methods actually improve the prediction?  How were the competing models tuned and optimized?"}