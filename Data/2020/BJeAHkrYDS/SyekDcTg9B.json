{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes an algorithm to combine the ideas of unsupervised skill/option discovery (Eysenbach et. al., 2018, Gregor et. al., 2016, referred to as \u201cBMI\u201d in the paper) with successor features \u201cSFs\u201d (Barreto et. al., 2017, 2018). While unsupervised skill/option discovery algorithms employ mutual information maximization of visited states and the latent variables corresponding to options (typically discrete), this paper adds a restriction that this latent variable (now continuous) should be the task vector specified by some learnt successor features.\n\nWith such a restriction, the algorithm can now be used in an unsupervised pre-training stage to learn conditional policies corresponding to several different task vectors and can be used to directly infer (without training or fine-tuning) a good policy for a supervised phase where external reward is present (i.e. via GPI from Barreto et. al., 2018) by simply regressing to the best task vector.\n\nSuch unsupervised pre-training is shown to outperform DIAYN (Eysenbach et. al., 2018) in 3 different Atari suites (including the full 57 game suite) and also ablations to the proposed model where GPI and SFs are excluded individually.\n\nDecision:\nI vote for accept as this paper proposes a novel technique to combine mutual information based intrinsic control objectives with successor features, which allow for combining the benefits of both in a complementary way. An unsupervised phase can now discover good conditional policies with successor features which can be used to infer a good policy to solve an external reward task in a supervised phase, with such a policy capable of attaining human level performance in several Atari games and outperforming several baselines such as DQNs in limited data regimes.\n\nOther comments:\n- The technique for enforcing the restriction in Eq. 10, as well as being able to use it with generalized policy improvement is a good novel contribution in the paper.\n\n- The detailed comparison with baselines on the full Atari suite is sufficient to back the claims in the paper that the strengths of BMI and SFs do complement each other.\n\n- The fact that fast task inference is sufficient to get good performance is impressive i.e. without the need to fine-tune the best inferred policy.\n\n\nMinor typos:\n- In section 5 para 5, \u201cUFVA\u201d -> \u201cUVFA\u201d, \u201cUFSA\u201d -> \u201cUSFA\u201d.\n"}