{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nSummary:\n\nThe paper proposes to expand the VAE architecture with a\nmixture-of-experts latent representation, with a\nmixture-component-specific decoder that can specialize in a specific\ncluster.  Importantly, the method can take advantage of a similarity\nmatrix to help with the clustering.\n\nOverall, I recommend a weak accept.  The method seems reasonable, and\nthe paper is well-written, but the results are only marginally better\nthan other methods, and there are several weaknesses with the proposed\narchitecture and experimental setup.\n\nPositives:\n\n* The idea of a more expressive variational distribution seems good,\n  although it is not novel.\n\n* The ability to have multiple decoder networks seems reasonable.\n\n* The ability to incorporate domain knowledge (in the form of a\n  similarity matrix S) is a plus.\n\n* The experiments are thorough, although the method is generally only\n  slightly better than competing methods.\n\nNegatives:\n\n* It's not clear if the similarity matrix S is already solving the\n  clustering problem - in which case, why do we need the rest of the\n  model?  For example, in your experiments you often used UMAP to\n  cluster data.  How does using UMAP by itself work?  (Along these\n  lines, it was not clear if your GMM experiments clustered data in\n  the original space, or in the UMAP'd space - please clarify this).\n  A good ablation would be to somehow remove the S matrix, to see if\n  the model can accurately cluster samples.\n\n* There is little variance in the generated samples.  \n\n* There is not a one-to-one mapping of clusters to labels, so it is\n  hard to use this method to generate a specific type of data (for\n  example, it is hard to generate a specific digit).  This is a big\n  difference from, say, a conditional sampler as learned by a GAN.\n  This also arises in Fig. 3, where it is clear that latent cluster\n  assignments do not match human-interpretable cluster assignments.  I\n  suppose this is to be expected, but taken with the previous point\n  (little variance in generated samples) I think it seriously weakens\n  the paper's claim that this is an \"accurate an efficient data\n  generation method.\"\n\n* The method does not do well when the number of clusters is large.\n  Regular GMMs seem to outperform it.\n\n* I felt that this paper made excessive use of the appendix.  The\n  paper is not self-contained enough, effectively violating the length\n  restrictions.  Please make an effort to move key results back in to\n  the main body of the paper.\n\n\nExperiments to run:\n\nAn ablation regarding the similarity matrix S.\n\nClarification of whether GMM experiments are run in data-space, or\nUMAP'd space.\n\nMIXAE features prominently in your related works, but is not compared\nto in your experiments.  It sounds like a natural comparison.  Please\nrun this experiment, or explain why it is not a comparable method.\n\n\n"}