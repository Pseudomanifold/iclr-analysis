{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper combines replay and openMax approach to help continual learning.  The results shows robustness on different dataset include image and audio in the continual learning condition, where the new come data has a different distribution but the model still able to maintain reasonable quality for the previously and newly come examples. To my understanding, this approach was not ground-breaking but seems a reasonable combinations.\n\nI'm learning to give weak reject for this paper because of it's poorly written. (1) It's very hard to align the contribution claimed by the paper and previous work in the introduction section. I highly suggest the author re-write this part and has a separate section about related work and explicit describe the difference compare to others. (2) The contribution seems over-claimed, it said it's a unified framework, but I don't understand what it has been unified. (3) In the experimental part, it use audioMNIST. Any reason to use this dataset? There are more well-defined audio task such as TIMIT for phoneme classification or Aurora for digital recognition. They are more easy to understand since they have well established benchmark.\n\nGiven my limited knowledge on this the literature of this topic, I'm happy to change the score if the written being improved and the following question being addressed.\n\n(1)  Introduction. I take most of space to describe previous work, but hard to find out what's difference of this paper. My understanding it combines A and B and apply it to C. But it claim it's a unified framework. \n(2) \"We fully share our model across tasks and automatically expand the linear classifier with additional units when encountering new classes, thus not requiring explicit task labels.\" I cannot link \"automatic\" with the proposed method. Is that doable because of the proposed framework? \n(3) Why use AudioMNIST which is an unusual task for audio?\n(4) For the giant Table 1, I suggest link each acronym with the reference paper. So it can easily get how it associate with different approach. Highlight some numbers can also help the reader understand what's going on in this giant table.\n(5) Can the author give me some insights, what these KL loss demonstrated in the table? I feel since you use the beta-vae version, the kl scale is depend on different approach, not really comparable for these different models."}