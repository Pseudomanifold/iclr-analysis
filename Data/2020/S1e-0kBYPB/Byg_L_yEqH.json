{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview/Contribution:\n====================\nThe authors present a explanation generation framework that help validate post-hoc explanations when the explanations are generated based on feature selection. They claim to demonstrate their method by showing failure modes of exiting explanation generation methods.\n\nOverall, the paper is not ready to be accepted to the conference and I describe my rational with the following strengths and weaknesses.\n\nStrength:\n========\n+ Explanations make models more transparent and easy to understand for end users of the decision made by complex models such as deep neural networks [1]. In that respect, having a verification mechanism for post-hoc explanations is interesting and useful.\n+ The paper is easy to read and follow.\nWeakness:\n===========\n- evaluating explanations generated for an opaque model with another opaque model (RCNN) is cyclical.\n- Just like many literature in this nascent space, interpretation (which is measuring the contribution of features or subsets of features towards predicted output) is confused as explanation. Human level explanations don\u2019t necessarily depend on the direct interaction or contribution of model derived features. Rather they describe \u2018why\u2019 the model come up with the decision produced.\n- Explanation generation is gaining traction in the deep learning community especially for critical applications such as healthcare and security. However, the authors claim that post-hoc explanations currently are only evaluated for only simple non-neural model. That is misleading given the recent attention toward generating explanations for various deep learning models.\n- As a generalized pos-hoc explanation generators verification framework, the experiments are seriously lacking and are not well designed to illicit broad applicability.\n\n1) Bekele, E., Lawson, W. E., Horne, Z., & Khemlani, S. (2018). Implementing a Robust Explanatory Bias in a Person Re-identification Network. In\u00a0Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops\u00a0(pp. 2165-2172)."}