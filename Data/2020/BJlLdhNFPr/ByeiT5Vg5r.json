{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\"an information theoretic principle, information bottleneck principle\" in the abstract is quite redundant with the use of 'principle' twice\n\n'\"great, great\" and \"great, thought provoking\". They have the same level of sparsity.' What kind of sparsity are you referring to with this example? Why can't sparsity reduce semantic redundancy? Please explain further.\n\n\"However, the first explanation has a large MI with the input document where \"great\" occurs a lot.\" What example input document are you referring to?\n\nYou should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. \n\nI find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn't clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented.\n\nCan you explain briefly how your \"hierarchical LSTM\" works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the\n\nIn Appendix B.1 \"output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator\" The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean?\n\nI find the phrasing of \"Negative Sentiment if any negative words\" and the corresponding title for positive in Fig 2 confusing. What do you mean by \"if any\"? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found.\n\nI find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not?\n\nThe same can be said for the MNIST example regarding an attention map.\n\n\"by the human intelligences\" sounds quite robotic\n\nCan you provide some sense of inter annotator agreement for labeling the images and sentences?\n\nIt does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper."}