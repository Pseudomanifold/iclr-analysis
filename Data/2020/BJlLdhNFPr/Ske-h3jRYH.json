{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method.\n\nThe paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper.\n\nMaybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs.\n\nThe proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria.\n\nIt is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k.\n\nTrivial comments:\n* The example in 3.2 \"great, great\" and \"great, thought provoking\" looks still ambiguous to explain what the section want to say.\n* \"x_i \\times z_j\" in p.iv looks ambiguous.\n* z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)}\n* the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed?\n* f(.) in the 4th eqn. of p.v may be undefined in the main text.\n* \\beta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason?"}