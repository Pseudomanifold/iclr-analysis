{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a very interesting idea of loss function optimization. At first sight, loss function is the goal of optimization and can not be optimized directly. However, the true goal of optimization is the final accuracy (for classification). So lots of loss functions can be designed and combined to form a large search space. In this paper, the authors adopt genetic programming to design loss functions hierarchically.  And experiments show that GLO (Genetic Loss-function Optimization) based loss function can achieve better results than cross entropy. \n\nThe paper is well written and easy to understand. I like the idea. Baikal loss is a form searched by GLO. Interestingly and counter intuitively , it is not a monotonically decreasing function. The authors explain it as a regularizer which can prevent the model to be too confident. \n\nExperiments on MNIST and Cifar10 are conducted to show the effectiveness of the proposed method. This part is very weak since MNIST and Cifar10 are very small datasets and the provided results are far from state-of-the-art results. Experiments on larger datasets such as ImageNet and more analysis about the optimization details are suggested to make this work more promising. Since the optimization is rather complex, it's better to show if it is stable enough to generalize to various datasets and models."}