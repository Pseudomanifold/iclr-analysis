{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper targets to understand some key factors that might influence the training speed of neural networks. It defines a concept called \"gradient confusion\" to quantify these factors. Roughly speaking, this term captures the disagreement of the descending directions suggested by different samples. \n\nOn the positive side, to understand the training procedure is important to the deep learning community. The idea on \"gradient confusion\" is quite straightforward and easy to understand. The paper is clearly written. \n\nHowever, there are some essential drawbacks of the paper that make me lean towards rejecting it. \n\nFirst, this concept of \"gradient confusion\" is very similar to the \"gradient diversity\" introduced more than 2 years ago in [1]. The term proposed in [1] measures the \"gradient confusion\" relative to the average of gradient norms, and is an averaged case rather than the worst case version. (This actually brings a second issue of the \"gradient confusion\" which makes the definition less useful.) However the paper has not discussed this existing work and the contribution on top of it. \n\nSecond, the \"gradient confusion\" is not a robust term even to one outlier sample. Since the definition in Eqn. (3) measures the worst case scenario, one could add an outlier sample that easily makes the eta arbitrarily large and the latter bound on the convergence will be meaningless. In comparison, the \"gradient diversity\" in [1] of the averaged value makes more sense to me. \n\nThird, the proposal of a new theory should be in favor of at least some applicable cases. In this paper, the take-home message seems to be when the samples are pushing the gradient towards the same direction, the training becomes faster, which is very legit and people believe this. However, what can we do about this? The authors fail to propose some interesting applications that could make use of the \"gradient confusion\" to help the training. For instance, the work of [1] has made use of the gradient diversity to accelerate distributed learning which could be one application. I encourage the authors to work towards some useful applications concerning this new concept.  \n\nTo sum up, I don't think this paper brings out some real contributions to the community and should not be accepted. \n\n\n[1]  Yin, Dong, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter Bartlett. \"Gradient diversity: a key ingredient for scalable distributed learning.\" arXiv preprint arXiv:1706.05699 (2017)."}