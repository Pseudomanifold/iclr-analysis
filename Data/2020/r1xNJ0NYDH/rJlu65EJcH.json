{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces the concept of \"gradient confusion\" to explain why neural networks train fast with SGD. They also study the effects of width, depth on gradient confusion. \n- The theoretical results assume that the data is sampled from a sphere and do not really give much insight into the effect of width and depth. \n- There are some confounding factors in the experiments and there needs to be a better comparison to some related work. \nDetailed review below:\nSection 1:\n- Please clarify how \"gradient confusion\" relate to the interpolation conditon of Ma et al, 2017 and the strong growth condition of Vaswani et al? \n- If we run SGD with a constant step-size, it will bounce around the optimal point in a ball with radius that depends on the step-size. If I keep decreasing the step-size, this radius shrinks. How does gradient confusion relate to the step-size? Is it upper-bounded by a quantity that depends on the step-size and the batch-size?\nSection 2:\n- Definition 2.1: Why should this condition hold for \"all\" points w? Isn't it necessary only at w^* or in a small neighborhood around it? \n- The gradient confusion parameter \\eta should depend on the batch-size. Please clarify this. \n- Figure 1: Previous work (Ma et al, Vaswani et al, Gunasekhar, 2017) all have shown that fast convergence can be obtained using SGD with a constant step-size with over-parametrized models and explained it using interpolation. What is the additional insight from gradient confusion?\n- \"Suppose that there is a Lipschitz constant for the Hessian\" - This is a strong assumption and a vague argument, that is confusing rather than insightful. Please justify why this is a valid assumption for neural network models.\nSection 3:\n- If E_i || \\nabla f_i(w)  ||^2 = O(\\epsilon) => gradient confusion = O(\\epsilon). Isn't the E_i || \\nabla f_i(w)  ||^2 exactly the strong growth condition in Vaswani, et al. Can the gradient confusion results be directly derived from the results in that paper? Please compare. Also compare and cite \"Stochastic Approximation of Smooth and Strongly Convex Functions:\nBeyond the O(1/T ) Convergence Rate\", COLT 2019. \nSection 4:\n- Please compare against the previous results that assumed the data to be sampled from a sphere. \n- Thm 4.1: The theorem bounds the probability that gradient confusion holds for a given \\eta. But the bounds of section 3 are vacuous even if the theorem holds with probability one, but for a large value of \\eta. There needs to be an upper bound on \\eta. Please clarify this. \n- Please compare against the results of this paper by Arora et al: \"On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization\"\n- For the effect of layer width, the analysis is only for the initializated weights and does not consider the optimization, which is what the paper claimed in the introduction. Am I missing something? Please justify this. The gradient confusion can decrease as the optimization progresses? \nSection 5:\n- \"We reduce the learning rate by a factor of 10\" But all the theory is for a constant step-size. Please explain this discrepancy. \n- in Figure 2, in the second figure, why is there a sharp full in the pairwise cosine similarities. \n- In all these experiments, explain why the batch-size and the step-size is not a confounding factor?"}