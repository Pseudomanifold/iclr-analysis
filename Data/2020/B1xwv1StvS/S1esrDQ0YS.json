{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new neural network model, called as Dissimilarity Network, to improve the few-shot learning accuracy.\nOverall the idea is well motivated that by emphasizing the difference among classes, the model can achieve more accurate predictions for classes where only limited data points are available for training.\nHowever, the paper is not quite well written.\nFirstly, much of the work is built upon previous work including attention mechanisms, episodic training for few-shot learning. Such components are the core of this work because the attention mechanisms implement the class-awareness, and the episodic training facilitates the LSTM structure. Yet these are not well explained and not much context is provided, thus making the paper hard to follow.\nSecondly, some terms are fairly overloaded, or not clearly defined. For example, the \u201cprior\u201d as mentioned in both the abstract and the introduction doesn\u2019t refer to the commonly interpreted term as in the Bayesian settings, but rather as a hand-waiving term to indicate the model design. Also, the terms, \u201cscore\u201d, \u201cmetric\u201d, \u201cdissimilarity\u201d are mentioned in the paper but the paper is not really learning the metric, to my understanding. Thus the details of the paper is quite hard to grasp. \nLastly, the idea of designing the global embedding and the task aware embedding is interesting but shouldn\u2019t really be restricted to few-shot learning. It would be interesting to test the idea on general classification tasks, for example in a simple cross validation settings.\nThus I think the paper would be stronger if the above are addressed and it\u2019s not ready for publishing yet in its current form.\n\nBelow are some more detailed comments:\n1)\tIn the abstract, the \u201cnewly introduced dataset H-CIFAR\u201d is not precise to me; my understanding is that the paper proposes such an experiment design for testing how well a classifier can predict the labels with hierarchy. The current writing refers to that the authors comprises a completely new dataset with new labels.\n2)\tIn the last sentence of the second paragraph in Introduction, the question is asked \u201cwhat prior\u201d should be reasonable. Since the authors didn\u2019t really add any priors in a Bayesian settings but rather designed an architecture, I suggest to reword something like \u201chow to explicitly encode hierarchies into the model structure\u201d.\n3)\tIn Section 2.1, some more description for \u201cepisodic training\u201d would be nice: why should it be used? How is it used and why it makes sense in the few-shot learning context?\n4)\tIn Section 2.2, it would be nice to add the mathematical definition of \u201cprototype\u201d.\n5)\tIn Section 2.2.1, it would be nice to define \u201cH\u201d.\n6)\tIn Section 2.2.2, is M required to be fixed given it\u2019s episodic training? Also it would be nice to add more details about the attention mechanism.\n7)\tIn the result section, it would be nice to discuss when the proposed method is doing better than other methods, for example RelationNet, as well as when it\u2019s worse since different datasets show different results.\n"}