{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The stated contributions of the paper are: (1) a method for performing few-shot learning and (2) an approach for building harder few-shot learning datasets from existing datasets. The authors describe a model for creating a task-aware embedding for different novel sets (for different image classification settings) using a nonlinear self-attention-like mechanism applied to the centroid of the global embeddings for each class. The resulting embeddings are used per class with an additional attention layer applied on the embeddings from the other classes to identify closely-related classes and consider the part of the embedding orthogonal to the attention-weighted-average of these closely-related classes. They compare the accuracy of their model vs others in the 1-shot and 5-shot setting on various datasets, including a derived dataset from CIFAR which they call Hierarchical-CIFAR.\n\nOverall, while we like the concepts/ideas and the problem is definitely important, we were not enthusiastic about the paper. First, we found the write up to be cryptic, involving very long unclear statements. It read as if the authors were writing for themselves and not for ICLR general audience. Beyond the writing style, we found the paper to have:\n\n* Inadequate description of the model, including mathematical inaccuracies.\n* Inadequate description of Hierarchical-CIFAR, motivation, and evaluation.\n\nDescription of model:\n------------------------------\nThe manuscripts describe the presented approach as metric learning but make no use of a distance function in the different spaces they map to. Instead, the manuscript defines an inner product over embeddings to compute similarities.\n\nThe manuscript describe their \u201cself-attention operation\u201d as a dynamic set-to-set operation. While the usual definition of self-attention is permutation invariant, the definition presented here is not and thus cannot be accurately described as a mapping between sets. Specifically, in the usual presentation of self-attention the only sharing of information between different set elements is during the outer product of the key and query vectors. The use of a BLSTM between \u201cneighboring elements\u201d of the set of prototype vectors violates this assumption and induces a lack of permutation invariance. This makes the method sensitive to permutations of classes, which does not make sense for predicting unordered classes.\n\nIn sections 2.2.3 and 2.2.4 the material is presented twice but slightly differently. For example, the definition of $b_k$ inline before equation 3 differs from equation 6 later in the text.\n\nEquation 7 is incorrect and should not exclude the current class from the denominator.\n\nThe description of how to classify new points after equation 6 is poorly explained. The description of what happens when $h_V$ is a \u201cBLTSM\u201d [sic (should be BLSTM)] is noninformative.\n\nThe manuscript describes two dimension sizes $H$ and $M$ but the definition of $attn$ requires that $H = M$.\n\nDescription of new dataset and evaluations:\n------------------------------------------------------------\nOne of the stated contributions of the manuscript is a methodology to build harder few-shot learning datasets. Section 4.2 is the only place in the text that appears to address this point, but is unclear where either new finer-grained or coarser-grained labels are coming from (new manual annotation or otherwise). The manuscript \u201cleave[s] out the detail of its construction for simplicity\u201d, but it is unclear what is being done here in the first place.\n\nThe manuscript does not detail tuning competing methods on the new dataset and so it is unclear whether it is a fair comparison.\n\nThe manuscript presents evaluations without any discussion of differences in performance between datasets or the 1-shot/5-shot settings. For example, their method is significantly better on CUB on 1-shot but not so much on 5-shot, on the other hand it is not significantly better on 1-shot for H-CIFAR and CIFAR-HS but then becomes better than the rest with 5-shot.\n\nAdditional comments/corrections\n---------------------------------------------\nThere were numerous typos and grammatical errors that were present in the manuscript that did not directly impact this evaluation but should be fixed in the future."}