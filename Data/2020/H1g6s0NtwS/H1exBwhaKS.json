{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "SUMMARY OF REVIEW\n\nThe authors have proposed the use of a neural surrogate model in place of the GP posterior mean and a weighted Reptile algorithm to meta-learn the initial weights of the neural surrogate model. This approach appears interesting. However, there seems to be multiple highly restrictive (at times impractical) assumptions in this work that are atypical of the BO setting adopted by other meta BO algorithms and not discussed, as detailed below. Justifications are required.\n\nClarifications are also needed with regards to how they exactly run their algorithm in the experiments and whether the prior/initial information from related problems/meta tasks provided to the tested algorithms is fair.\n\n\n\nDETAILED COMMENTS\n\nThe authors say that \"We still use the variance in Eq. (4) to measure uncertainty, because the estimation uncertainty should be independent in individual problems.\" This does not seem to hold true. If a meta task or train set is indeed correlated (or provides information) to the new problem, the posterior variance/uncertainty at a point depends on the observations in the meta task or train set near to this point (see, for example, \nFeurer et al. (2018)). Can the authors discuss the implications of such an assumption in their work?\n\nQ and Q_i have always been referred to as problems. In Algorithm 3, Q is suddenly referred to as meta train set. On page 4, you have said that x^*_i is the minimizer of i-th problem Q_i(x) in meta train set. Based on these information, I assume that the authors consider x^*_i as the global minimizer and that x^*_i is known in order to compute the rewards. Can the authors discuss why is this a reasonable assumption?\n\nIn their proposed weight Reptile algorithm (Algorithm 3), the authors have also assumed access to the black-box functions of the related problems or meta tasks, which is not typical of other meta BO works that only require the existing observations or datasets of the related problems/meta tasks. As a result, compared with the existing meta BO algorithms, their proposed weighted Reptile is considerably more expensive due to the need to additionally evaluate the black-box functions of the related problems or meta tasks many times during execution. Can the authors discuss the practical implications of such an assumption and how it affects the types of problems/applications that can be considered by this work?\n\nThe authors have not provided any justification for their choice of reward on page 4. If the black-box function is indeed complex and highly varying, the distance between points may not work well at all. Can the authors provide a justification and discuss the practical implications and limitations with such a choice?\n\nIsn't it more natural to consider a single Bayesian neural network instead of using a neural network for the mean and a GP for the variance?\n\nFor the experiments, it would be good to see two other variants of the proposed algorithm to understand the individual contributions of the neural surrogate model and weighted Reptile algorithm: one without neural surrogate model and the other with simply the use of neural surrogate model.\n\nCan the authors explain in greater detail how they run their algorithms (Algorithms 2 and 3) in the experiments? For example, the authors say that \"WRA-N starts with learned initial surrogate model\". I assume that WRA-N refers to Algorithm 3 based on its acronym. Isn't the learned initial surrogate model the output of WRA-N in the first place? Also, the graphs in Fig. 2 seem to show iteration 1 to 13 in NOE (Algorithm 2). However, Algorithm 3 accepts N_T = 13 and executes NOE for N_T = 13 (and not 1, 2, or 3, ...) for each problem in each epoch. How do the authors generate the plot of WRA-N for iterations 1 to 12? \nWhat seems to make more sense to me is that the authors in fact run Algorithm 2 instead of Algorithm 3 for each experiment and they initialize w in Algorithm 2 to the output of Algorithm 3. In any case, a clarification is needed here.\n\nIt is not clear to me whether the initial/prior information from related problems/meta tasks provided to WRA-N, TST-R, AND TSR-M is fair. Can the authors provide a justification?\n\nTo clarify, for each related problem/function, only N_T number of datapoints are used to train a corresponding neural network with 1 hidden layer of 15 hidden units?\n\nThe authors say that \"Since TST-R needs base models for combination, we sample 20 points from uniform distribution in (\u221210, 10) to construct base models.\" Is this sampling procedure the same as that in (Wistuba et al., 2016)?\n\nCan the authors explain the comparable performance of WRA-N and TST-R in Fig. 5? Why are the error bars missing?\n\nHow does the proposed approach compare with that of Feurer et al. (2018)?\n\n\n\nMinor issues\n\nPage 1, 3: adapt well to new tasks.\nPage 2: The author says \"depends on a GP-based surrogate model fitting function values without learnable parameters\". This is not true: The GP hyperparameters need to be learned and they adapt to new problems.\nPage 4: descent order?\nPages 4, 5: Why is there an input x to Q_i?\nAlgorithm 2: t^* should be at the superscript of x.\nEquation 7: What is N?\nPage 5: Does it make a difference in the performance when delta is set to 0?\nPage 5: well define meta-features?"}