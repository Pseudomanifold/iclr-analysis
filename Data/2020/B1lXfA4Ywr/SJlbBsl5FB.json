{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a novel modular neural architecture for algorithm induction. The modules are fixed and a controller policy is learned which outputs a distribution over modules and input/output locations on a memory tape. An oracle (which knows the correct answer) is necessary to decide when to stop computing. The controller is trained by a variant of REINFORCE.\n\nMy main concern with this paper is that there is zero experimental comparison against previous neural program induction approaches. It makes it difficult to evaluate whether the specifics of the proposed architecture actually are advantageous (albeit the authors argue so in table 1, it is not clear that these differences translate in better learning).\n\nI find it rather disappointing that the controller needs to be stopped using an oracle which knows the right answer, making the practical use of such an architecture essentially infeasible in practice. It is also disappointing that a different subset of predefined modules is chosen for each task. I understand it helps to not have unnecessary modules in the module set, but it seems to be another weakness of the approach, and there was no evaluation against using the same full set for all tasks. Also, it is not clear that the chosen set of modules is sufficiently  general and universal, which  would be necessary to scale this approach to learn arbitrary programs.\n\nSince the controller is trained by REINFORCE, one could be concerned that the gradient estimator has high variance (compared to methods based on soft-attention where one can backprop all the way through a sequence of actions). It would thus have been good to verify how sample complexity worsens as the complexity of the task  scales up, but there is no such analysis.\n\nConsidering the above issues, I suggest to reject that submission.\n\nMinor:\n\nPage 4, 2nd par, the text refers to sigma_t \"shown above\", which has not yet been introduced (comes in the middle of page 5).\n\nPage 7, 2nd par, \"linear schedule starting at 1M and ending at 18M\" needs to be clarified. 1M what?\n\nPage 8, \"generalize perfectly\": it does not seem to be the case since none of the experiments have led to 10/10 successes.  Are the \"runs\" mentioned training runs?\n"}