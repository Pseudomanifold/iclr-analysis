{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "SUMMARY:\nThis paper proposes a method to segment time series into discrete intervals in an unsupervised way. The data is modeled using a state space model where each state consists of a discrete and a continuous part. The discrete state denotes the segment the system is currently in and the continuous state which is conditioned on the discrete one denotes an uninterpretable feature vector. The transition distributions are non-linear. The observation at each time step is high-dimensional and produced by an emission distribution whose parameters are given by a neural network which takes in the continuous state. Learning and inference is done by maximizing the evidence lower bound (ELBO). Problems with the discreteness of latent variables is circumvented by marginalizing (collapsing) them out using the forward-backward algorithm. Problems with making discrete states meaningful when there are non-linear transitions/emissions is addressed by annealing. This annealing scheme forces the conditional distributions on the discrete state to have high entropy (be close to a uniform distribution) at the start by adding a term to the ELBO objective and the multiplier of this term is decreased as the training progresses. There are actually two terms to do this since one alone didn't work.\n\nSTRUCTURE:\nThe paper is well-written and easy to understand.\n\nNOVELTY:\nI found the technique of estimating gradients using forward-backward to be interesting and potentially useful in other domains when parts of generative models can be marginalized out using belief propagation.\nWhile the problem of unsupervised time-series segmentation is an important one, I'm not sure the proposed technique addresses it completely.\nThe main thing that seems to be doing the work is not the marginalization using forward-backward, but rather the annealing scheme which itself seems ad-hoc and it is not clear whether this is generalizable to other domains or it just happened to work on the problems in the paper.\nIt is not clear why maximizing the entropy of the variational transition should encourage meaningful clustering.\nWould this work even if the emission distribution is made much more powerful?\n\nEXPERIMENTS:\nThere are experiments on three synthetic datasets. While the proposed method beats the competing methods, it is unclear that collapsing is helpful. Also, no annealing was used in the baseline methods (like increasing K in SVAE or multi-step training).\n\nCONCLUSION:\nWhile the problem this paper is tackling is significant, it isn't clear that the proposed method tackles it. I would consider bumping up my score if \n- this method is demonstrated to work on a real dataset and/or\n- there is a better understanding of the principles behind why this annealing scheme helps.\nAlso, proper tweaking of the competing algorithms (similar to annealing) is needed to compare the proposed method fairly."}