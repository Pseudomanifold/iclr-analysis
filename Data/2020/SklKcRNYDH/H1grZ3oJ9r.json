{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper considers the problem of the need for memory-efficient optimizers given the increase in model complexity. They study memory-efficient adaptively preconditioned gradient methods, and see the trade-offs among expressivity and preconditioner quality. They show results on a large-scale NLP model, and show that the memory overhead can be reduced by 3 orders of magnitude, without sacrificing performance. \n\n+ show tradeoff among training convergence and memory in the optimizer.\n+ introduce extreme tensoring -- a modification that can be applied to *any* 2nd moment-based adaptive optimizer. It uses a compressed preconditioner. \n+ extend diagonal Shampoo to tensor factorization \n+ nicely written Related Work.\n+ applied their idea to widely used optimizers, such as AdaGrad, Adam, etc.\n+ the derived regret bound is only a multiplicative constant from the regret bound of AdaGrad.\n+ interesting experiments showing tradeoff among lack of memory (SGD) and full memory (AdaGrad), applied on a real-world machine learning setting of large-scale natural language modeling with Transformers. They show how their extreme tensoring modification can achieve an intermediate ground among lack of memory and full memory.\n- I am missing a plot similar to Figure 4 but instead of being on the synthetic data, being on the application of NLP.\n\nOverall, although I do not have a lot of experience in this area, it appears to be that the introduced family of algorithms can be promising as a nice interpolation among SGD-type algorithms and AdaGrad ones. I would personally have preferred to see a wider range of experiments, considering other application scenarios besides NLP, just to see the wide success of the proposed approach. Also, I would urge the authors to spend a bit more text on explaining the algorithm, so that it is easy for practitioners to try it out."}