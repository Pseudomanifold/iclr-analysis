{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new memory-efficient pre-conditioning scheme for stochastic optimizers. The basic idea is to store a coarse-grained pre-conditioner, expressed as a rank-one tensor product, instead of the full-dimensional pre-conditioner typically used in algorithms like AdaGrad. I am not very familiar with prior work in this literature, but the proposed approach seems simple and sound. \n\nA regret bound is provided for the proposed method, however the analysis here seems to be a straightforward application of the results and techniques from a few prior works. So, I was a bit surprised to see so much space devoted to the proofs. These can be safely moved to the appendix in my opinion. Moreover, the bound does not seem to be very useful in practice. For example, in simulations in Figure 3, the proposed ET1 performs better than AdaGrad, but the bound is not able to capture this at all. \n\nThe presentation of the experimental results in section 5 can be improved in my opinion. The authors keep referring to \u201cFigure 5\u201d in this section, but I think this is a typo and these should be \u201cFigure 2\u201d instead. In Figure 2, please indicate on the figure itself what dark blue and light blue colors correspond to (smaller and larger models, respectively). \n\nIn Appendix B, wall clock results are presented for different algorithms. These show that the proposed approach is slower than standard algorithms like Adam or AdaGrad. Please explicitly mention this result in the main text (last paragraph of section 5.1) and discuss why this is the case (is this because of the extra reshaping operations required in the updates?)."}