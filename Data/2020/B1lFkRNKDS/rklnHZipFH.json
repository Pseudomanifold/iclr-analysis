{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel and interesting idea of modulating\nconvolutional kernels with global context.  They use depthwise\nfactorized channel-wise interaction to reduce complexity and parameters.\n\nWhile an interesting idea, I found it hard to see how the global\ncontext would usefully modulate the kernels.  While Figure 1\nillustrates the idea, it is not clear how the modulated kernel is\nbetter suited for the task relative to the original kernel.  It would\nbe nice if you could show a simple/toy example where your modulation\nclearly modifies the kernel in a way that is intuitively useful.\nOverall, I believe the motivation for the method could be made clearer\n- Why is this the best way to incorporate global context?\n\nIt is also unclear to me how the proposed network relates to the\nmotivating and cited Neuroscience literature.\n\nIt is nice that the computational complexity is carefully tracked.\nThe CGC network does seem to converge faster but the benefits to\nvalidation accuracy seem modest in Table 1/Fig 3.  It would be\ninteresting to further explore what is improving the convergence speed\n-i.e. to test your supposition that it \"improves the model's\ngeneralization ability and the gating mechanism reduces the norm of\ngradients back-propagated to he convolution kernels ...\".\n\nThe experiments are well described and extensive and some of them do\ngive some more substantial performance improvements, but overall I think the\npaper does not explain the motivation for the algorithm sufficiently for\nICLR publication.\n\n"}