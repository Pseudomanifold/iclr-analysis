{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper builds a connection between information theoretical MPC and entropy-regularized RL and also develops a novel Q learning algorithm (Model Predictive Q-Learning).  Experiments show that the proposed MBQ algorithm outperforms MPPI and soft Q learning in practice. \n\nThe paper is well-written. The derivation is clean and easy to understand. Adding a terminal Q function, which makes horizon infinite, is a very natural idea. Most of the derivation is unrelated to the additional Q^pi^* (Eq 11-17) and Eq (18-20) are simply plugging in the formula of pi^*, which is derived at Eq (9), which is also quite natural if one knows the result for finite horizon horizon MPPI (i.e., without the terminal Q). For experiments, I'd like to see some results on more complex environments (e.g., continuous control tasks in OpenAI Gym) and more comparison with recent model-based RL work. \n\nQuestions:\n\n1. The experiment setup is that a uniform distribution of dynamics parameters (with biased mean and added noise) are used. Why not using a neural network? \n2. Eq (22): The estimation for eta is unbiased, However, 1/eta might be biased, so is the estimator for w(E) also biased? \n\nMinor Comments:\n1. Eq (16): should the first term be negated? \n2. Page 6, last paragraph: performved -> performed.\n3. Page 7, second paragraph: produces -> produce.\n4. Algorithm 1, L6: N % N_update -> i % N_update. \n5. Appendix A.1, second paragraph: geneerating -> generating. \n"}