{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper investigates the use of Lyanpunov theory as an inductive bias for improving the stability / robustness of policies in a model-free actor-critic reinforcement learning setting. Through viewing the Critic as a Lyapunov function, optimizing the policy with a Lyapunov-based constraint is meant to ensure the stability of the policy through a \u2018cost stability\u2019 metric.. Experimental results show that Lyapunov-based Soft-Actor Critic (LAC) is more robust than SAC on some linear and nonlinear environments.\n\nThe reviewer believes that the study of intersections between Control Theory and RL to be immensely valuable and the authors outline a principled formulation. However,  the implementation, experiments and general manuscript suggest that paper requires further work before it is conference-ready. \n\nAs the author understands it, the current state of the literature of Lyapunov methods for Deep Reinforcement Learning can be summarized as:\nRichards et al, 2018, Classify stable region and learn neural Lyapunov function for a safe exploration strategy\nBerkenkamp et al, 2018: Classify the stable region via GP, move there for exploration\nChow et al 2018 Constrained MDPs for discrete gridworld environments\nChow et al 2019 Constrained MDPs for continuous environments through a projection on the policy\nThis work: Actor-Critic constrained policy optimization with a lyapunov-based value function critic\n\nIn the introduction and the related work, too much emphasis is put on explaining stability and discussing methods like Model Predictive Control (MPC) which do not benefit the rest of the paper. Additionally, the three contributions listed do not seem particularly novel given the past literature. \n\nThe premise of the formulation also presents several unquestioned assumptions and design decisions:\nWhy model-free RL, as the authors also state that many samples are required to validate stability?\nHow does the requirement of stability inform the search strategy in this work? Especially as SAC uses a maximum entropy stochastic policy to aid exploration. \nDo you really get \u2018guarantees\u2019 with sample-based methods? I would expect bounds based on the number of samples\nThe cost-based measure of stability seems open to abuse - i.e. for the half-cheetah environment only the centre-of-mass horizontal velocity in covered in the cost function, the stability of the embodiment (joint angles and velocities) are ignored. For the Fetch Reacher, a cost function in cartesian space ignores instabilities from kinematic singularities in joint space. I would image the cost function needs to be a measure on the entire dynamic state. \n\nThe notion of a Value function as a Lyapunov function is very interesting, and since it was the basis of the work, would have benefitted from more discussion, i.e. for which cost/reward function families the equivalence is valid for, and how it compares to other Lyapunov candidate functions.\n\nWith the RL formulation, the requirement of clipping with the lagrangians is suspicious, as it suggests the objective and/or its numerics are not well posed.\n\nWith the choice of experiments, they do not seem to question the central problem outlined by the paper. Rather than show environments SAC returns unstable trajectories during learning, the experiments aim to demonstrate instead a general robustness. The reviewer appreciates that stability is difficult to assess; however, while stability is heavily linked to robustness, a paper title promising stability guarantees should demonstrate some strong empirical evidence stability.\nAdditionally, the choice of environments do not seem to be ideal test beds for stability - i.e, the half-cheetah is stabilized via interactions with the ground. The reviewer would prefer to see simpler nonlinear environments, such as Markov Jump Processes / Switching Linear Dynamics, where SAC clearly demonstrates instability during learning which LAC is sufficiently regularized against. Additionally, while the `repressilator\u2019 is an interesting application to the domain of bioengineering, its addition does not seem to be especially motivated by the central goal of the paper, so just adds to confuse the reader with unnecessary theoretical content.\n\nMoreover, a brief literature review uncovered some relevant earlier work which was not cited: \nConstruction of neural network based Lyapunov functions, Petridis et al, 2006\nGeneration of Lyapunov functions by neural networks, Noroozi et al, 2008\nLyapunov Design for Safe Reinforcement Learning, Perkins et al, 2002\nSome of the references also appear incorrectly formatted or incorrect, i.e. the reference for Spencer et al, 2018 should be the CoRL 2018 version rather than arxiv. \n\nAlso, the general use of grammar in the manuscript would benefit from another draft. In particular, the title could be improved, i.e.\n    Model-free Control of Nonlinear Stochastic Systems with Stability Guarantees\n"}