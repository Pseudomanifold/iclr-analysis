{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe authors of this paper propose a novel approach for training a robust few-shot model. Adversarial querying is a simple method by changing the query set of meta-learning to adversarial examples, however, it gets very promising results to improve the robustness of the resulting model. \n\n\nPaper strength:\n1.\tThey provide a thorough empirical analysis of the robustness of meta-learning. Specifically, they study the robustness of several variances of meta-learner against well-known strong adversarial attacks and they also study different defenses like adversarial training and transfer learning.\n2.\tCompared with adversarial training with heavy computation cost, the proposed approach is simple and efficient to improve the robustness of meta-learning against a strong attack. \n3.\tSufficient experiment results provide a lot of insights and illustrate the superiority of proposed adversarial querying.\n\nPaper weakness:\n\n1.\tThe organization of the paper needs to be adjusted since I am confused about the adversarial querying in table 3 and section 3.2 before I read section 4. Why not put \u20185-shot Mini-ImageNet-AQ\u2019 in table 3?\n2.\tI would like to see more comparison results for black-box attacks [1,2] since it has shown to be as strong as white-box attacks (e.g. PGD).\n3.\tIt would be more interesting if the authors provide more theoretical analysis about the insights of replacing query set to its adversarial counterparts during meta-training. \n\n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization-based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017.\n\n[2] Li, Yandong, et al. \"NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.\" arXiv preprint arXiv:1905.00441 (2019).\n"}