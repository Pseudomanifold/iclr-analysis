{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary: This paper presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm (e.g. MAML). Extensive experimental results are reported to demonstrate the robustness of the proposed approach on benchmarks. \n\nStrong points:\n\n-S1. The problem of robust meta-learning is interesting. The proposed solution is well motivated and extensively evaluated with promise.\n\n-S2. The paper is clearly presented and easy to follow in general.  \n\nWeak points:\n\n-W1. The degree of novelty in methodology is limited. In my opinion, the proposed method is an adaptation of PGD to optimization-based meta-learning. Although the idea is somewhat interesting and it comes up with a thorough numerical study, the value-added beyond a direct combination of traditional adversarial learning techniques and meta-learning seems not sufficient enough to represent a significant contribution to the addressed topic. \n\n-W2. The method is (over) extensively evaluated but short of justification in principle. The core idea of the proposal is introducing adversarial attack in the query data for task-level training. However, it is not clear why introducing adversarial attack on the query data should work better than on the support data or both query and support data? The principle/intuition behind the idea needs to be more clearly explained in theory and/or experiment.  \n\n-W3. Numerical study is misleading in some places. The goal of this paper is to show the effectiveness of introducing the adversarial attack during query steps to enhance the robustness of few-shot learning. However, some reported experimental results are less relevant to this theme. For an instance, Table 6 shows that by replacing the loss, AQ-MAML can achieve better performance. These results seem not very interesting for the core method evaluation; they just demonstrate that changing the loss function can effectively improve the performance. The experiment reported in section 4.1 also gives the reader a similar impression that the results are weakly related to what the paper is really about. To make the contents in Section 4 more concentrated, I suggest moving some of the less relevant experiments presented in Section 4 to Section 3 as the motivation of adversarial meta-learning, or to the appendix. Overall, the main Section 4 should be more focused on the experimental/theoretical comparison against prior approaches, rather than exploring the effects of different model structures or loss functions.\n\nMinor issues: \n\n- M1. The concepts of ``AQ MAML\u2019\u2019 (at the end of Section 3.1) and ``robust meta-learning\u2019\u2019 (above Table 4) are not defined or referenced in the context. \n\n- M2. On the table citation in section 4.2, the Table 8 and Table 9 are wrongly cited as Table 4.2.\n"}