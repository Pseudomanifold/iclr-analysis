{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper conducts a series of empirical evaluations that compares the performance first-order vs second-order decoders (in a graph NN) for the NLP task of dependency parsing, as a function of dependency length, encoder complexity, and data size.\n\nAlthough I greatly appreciate this type of principled empirical evaluation, which I also think is lacking currently in our field (especially in deep learning), for this particular contribution, I have the following concerns for it to be published at ICLR:\n\n1). The main conclusion drawn by the authors in the introductory paragraphs, i.e., that complicated decoders is needed for big data (especially when dependency length increases) is weakly supported by their experiment results; for example, according to the authors explanation, the reason that the gap between LSTM-2+FO and LSTM-2+SO becomes large as dependency length increases can be attributed to the inferiority of the first-order decoder in finding very long-term dependencies (they imply that the FO decoder cannot fully extract information provided by a powerful encoder like LSTM-2). However, the same trend can be said of the non-neural parser, so to me their explanation is insufficient.\n\n2). Most of the differences between the combinations of encoder-decoder is not significant according to their experiments (for Table 2 and 3).\n\n3). The summary of results at the end of the paper do not add much insight to the current literature; for example, the finding that a LSTM encoder is better than non-neural encoder on large data and vice versa on small data is to be expected."}