{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper explores self-supervised learning in the low-data regime, comparing results to self-supervised learning on larger datasets.  BiGAN, RotNet, and DeepCluster serve as the reference self-supervised methods.  It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation.  A performance gap exists for deeper layers, suggesting that larger datasets are required for self-supervised learning of useful filters in deeper network layers.\n\nI believe the primary claim of this paper is neither surprising nor novel.  The long history of successful hand-designed descriptors in computer vision, such as SIFT [Lowe, 1999] and HOG [Dalal and Triggs, 2005], suggest that one can design (with no data at all) features reminiscent of those learned in the first couple layers of a convolutional neural network (local image gradients, followed by characterization of those gradients over larger local windows).\n\nMore importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.  This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.\n\nFor example, see the following paper (over 5600 citations according to Google scholar):\n\n[1] Bruno A. Olshausen and David J. Field.  Emergence of simple-cell receptive field properties by learning a sparse code for natural images.  Nature, 1996.\n\nFigure 4 of [1] shows results for learning 16x16 filters using \"ten 512x512 images of natural scenes\".  Compare to the conv1 filters in Figure 2 of the paper under review.  This 1996 paper clearly established that it is possible to learn such filters from a small number of images.  There is long history of sparse coding and dictionary learning techniques, including multilayer representations, that follows from the early work of [1].  The paper should at minimum engage with this extensive history, and, in light of it, explain whether its claims are actually novel."}