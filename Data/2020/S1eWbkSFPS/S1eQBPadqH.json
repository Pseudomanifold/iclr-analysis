{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a new GNN model to address the common issue \u201coversmoothing\u201d, namely, Graph Entities with Step Mixture via random walk (GESM). Basically, it integrates both mixture of various steps through random walk, and graph attention network, and demonstrates that it can overcome the SOTA on popular benchmarks.\n\nDetailed comments: \n\n* The oversmoothing problem has been mentioned many times in this paper, yet little has been demonstrated through experiments that the new model can solve the oversmoothing issue. It would be great to show the performance improvement while oversmoothing is mitigated.\n\n* The proposed idea is very similar to the following paper: \u201cRevisiting Graph Neural Networks: All We Have is Low-Pass Filters\u201d. Both use low-pass filtering (via transition matrix) to propagate the information on the graph. I suggest a detailed discussion with this work.\n\n* The major concern of this work is the weak novelty. It combines GAT with multiple random walk under the GNN framework. While this is working well on most GNN datasets, it is not very new by itself.\n\n* Some experimental results are comparable to existing methods, as shown in Table 2 and 3. Maybe the time complexity is the major contribution of this paper. A head-to-head running time comparison with SOTA in Table 4 will be helpful.\n\n* Fewer methods are compared in Table 3 than in Table 2. Can authors add more in Table 3 to give a better demonstration?\n"}