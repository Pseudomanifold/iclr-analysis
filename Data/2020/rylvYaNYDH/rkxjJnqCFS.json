{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a new visualization tool in order to understand the behavior of agents trained using deep RL. Specifically, they train a generative model of game states, and then optimize an energy-based distribution over state embeddings according to some target function, and then by sampling from the resulting distribution they create a diverse set of realistic states that score highly according to the target function. They propose a few target cost functions, which allow them to optimize for states in which the agent takes a particular action, states which are high reward (worst Q-value is large), states which are low reward (best Q-value is small), and critical states. They demonstrate results on Atari games as well as a simulated driving environment.\n\nI enjoyed this paper: the proposed method is straightforward, and the experiments are well-done and demonstrate the potential of the method, and for that reason I\u2019m recommending an accept.\n\nIf I had to name a fault with the paper, it is that the interpretation of the results depends quite strongly on human judgment: we are asked to look at particular images and then told how to interpret them. It would be both more compelling and more interesting to use the results to *fix* problems detected in the agents. For example, Section 4.4 suggests that the Seaquest agent has not learned that it should surface when the oxygen is low. Having done this analysis, can we then fix the problem? Perhaps it would work to sample states according to the T+- objective, and then train the agent to take the \u201cup\u201d action in such situations, perhaps interspersed with regular RL / training on the replay buffer to avoid catastrophic forgetting. If the problem could be fixed, that would be strong evidence for the utility of the method.\n\nMinor questions:\n\nCan you explain why T+-(q) = T-(q) - T+(q) incentivizes \u201csituations in which one action is of very high value and another is of very low value\u201d? It is not immediately obvious to me why this should be true, just from the definition.\n\nTypos:\n\nSecond contribution: \u201cinterstingness\u201d --> \u201cinterestingness\u201d\nSection 3.2: \u201cIn the next section, we will show how to overcome these difficulties.\u201d I assume this is referring to what is now Section 2?\nPlease organize the figures better in relation to the text (e.g. Fig. 5 should be after Figs. 6 and 7)."}