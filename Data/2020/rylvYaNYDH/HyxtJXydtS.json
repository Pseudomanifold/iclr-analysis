{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose learning a generative model of states to visualize the behavior of different RL agents. Given states s from the environment, a VAE is trained to reconstruct states s, with an added loss term encouraging the action of the agent to stay the same between the original s and reconstructed s. The L2 reconstruction loss is also weighted by an attentive loss term, based on saliency of the policy pi, to focus reconstruction on critical regions.\n\nOnce the VAE is learned, we learn a 2nd sampling network, which operates in the latent space of the embedding. This energy-based model aims to sample regions of state space that optimize some target function T. For example, the target function may be \"Q-value of moving left\", in which case the model should generate states s where moving left is expected to be high-value. They examine a number of target functions, for maximizing / minimizing the Q-value of different actions, maximizing the spread of Q-values (max_a Q(s,a) - min_a Q(s,a)), and other saliency approaches. Experiments on Atari and a 3D driving simulator demonstrate that visualized images are qualitatively reasonable, and the states generated aren't just doing nearest neighbor over states in the training set.\n\nThe approach seems reasonable and the experiments seem reasonable as well. The saliency-based VAE objective is new to me as well. However, I contest the claim that this is one of the first works visualizing and diagnosing RL agents. The number of papers focusing purely on RL agent visualization is fairly small, but many deep RL papers include visualization as part of their experimental results, and in fact the paper cites some of these directly (like Greydanus et al, 2017). There are also a number of unclear details I'd want clarified.\n\nSpecific comments\n* When describing the VAE training loss in Eqn (1), the last term can just be KL(f(s), N(0, I_n)), writing out the prior is more confusing, especially because the KL is mentioned in the text.\n* Why is the gradient saliency measured in L1 rather than L2 distance? Every distance measured in the paper besides this one uses L2. Also, it is unclear what d is in the denominator - I assume it is a sum over each dimension of the state space but this is never fully described.\n* Does the action consistency loss require differentiability through L_a? The provided example of the argmax action for policy pi seems like a hard loss function to learn, and it's unclear what L2 diff between two different argmax actions means.\n* For the driving simulator, is there a reason the simulator used is an in-house one, rather than an existing driving simulator like CARLA?\n* I buy the results showing the VAE learns to generate novel states. However, are there examples of novel states that drive insights that couldn't be found by doing nearest-neighbor over the training set? My thinking here is that you train E(x) the same way as before, except instead of x representing the noise passed to the VAE, you have x represent a non-parametric distribution that samples states from the replay buffer with some probability. For example, the paper argues they learned their Seaquest agent doesn't model the oxygen meter well, but was learning the generative model necessary to learn this?\n* Section 2.3 (Target Functions) mentions target function S+, S-, S+-, none of which seem to be defined or mentioned in the main text.\n* For prior visualization work, there is some related work from the adversarial RL literature (https://arxiv.org/pdf/1905.10615.pdf for a recent example), since adversarially attacking a policy tends to expose features that policy cares about. For examining failure states in particular, there is also https://arxiv.org/pdf/1812.01647.pdf which tries to identify catastrophic failures that are rare in the dataset.\n* At a style level, I would not describe this paper as specifically a visualizing weaknesses paper - instead it is more like a framework to learn to generate states that satisfy some predicate of the Q-function (as noted by experiments that try to identify critical states, especially positive states, etc.), and I would consider renaming the paper accordingly.\n\nOverall I feel this paper is very borderline but I'll round to weak accept. "}