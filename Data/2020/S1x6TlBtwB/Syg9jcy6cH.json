{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\nIn this works, the authors propose to use a concrete mixture of Gaussians as a variational distribution. The authors show that the deep ensemble method can be viewed as a special case of a mixture of Gaussians with a categorical prior.\nI think the main contribution is to use the concrete distribution as a mixture prior q(z) instead of a categorical prior.\nHowever, there are some concerns. The following points should be addressed to get a higher rating. \n\n(1) Several papers consider the problem of learning a mixture of Gaussians in the VI framework ([1,2,3,4,5]). The deep ensemble method considered in this paper is just one of the existing ensemble approaches. The related work section should be updated to discuss the novelty of this work.\n\n(2) In the deep ensemble method, the categorial prior q(z) is held fixed. However, it is possible to update the categorical prior q(z). \nIn this work, the proposed distribution is q(w) = sum_{c=1}^{K} Concrete(z=c|\\theta_z) Gauss(w|z=c,\\theta_{w_c}).\nBy using the concrete prior, the gradient can be computed by the local reparametrization trick for q(w|z) and the reparametrization trick for q(z). \nHowever, even when q(z) is the categorical prior,  the local reparametrization trick for q(w|z) is still valid if the variational parameters for each mixture component are not tied.  The main difference is the reparametrization trick can not be used for q(z). \nThe authors should show why the proposed variational distribution is better than the mixture of Gaussians with a categorical prior. \n \n\nReferences\n[1] O. Arenz, M. Zhong, and G. Neumann. \"Efficient Gradient-Free Variational Inference using Policy Search.\" ICML. 2018.\n[2] A. C. Miller, N. J. Foti, A. D\u2019Amour, and R. P. Adams. Variational boosting: Iteratively refining posterior approximations. ICML, 2017. \n[3] O. Zobay. Variational Bayesian inference with gaussian-mixture approximations. Electronic Journal of Statistics, 8(1):355\u2013389, 2014.\n[4] Lin, Wu, Mohammad Emtiyaz Khan, and Mark Schmidt. Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations. ICML 2019.\n[5] F. Guo, X. Wang, K. Fan, T. Broderick, and D. B. Dunson. Boosting variational inference. arXiv:1611.05559v2, 2016. \n\n\n\n"}