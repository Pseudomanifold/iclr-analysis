{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper describes a new neural architecture search method based on monte-carlo tree search that dynamically adapts the action space.\nThe methods consists of two stages. In the first stage, the learning stage, the action space is divided into good and bad regions based on a tree structure. In the seconds stage, new data is generated by sampling new architecture with monte-carlo tree search. Those two stages are iterated with new incoming data.\n\nThe paper proposes an interesting approach which achieves competitive results compared to state-of-the-art methods.\nIn general the paper is well written and easy to follow. However,  I haven't fully understood  how the model space is divided at different nodes. What exactly is the splitting criterium? Are the splits axis aligned?\n\nFurther comments:\n\n- Figure 4 a and b seemed to be flipped?\n\n- Figure 5 top row would be easier to parse if the x-axis is on a log scale.\n\n- Could you also include other Bayesian optimization methods, such as SMAC or TPE, which should competitive performance on NASBench101 and do not suffer from a cubic scaling"}