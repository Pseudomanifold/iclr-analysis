{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses the posterior collapse problem associated with the VAE for text modeling with a modified VAE architecture. \n\nThe paper motivates the problem (noted by several other works - e.g. Bowmman et al) by presenting the gradients for reconstruction and regularization terms in the VAE and the deterministic AE, and noting that the gradients in the case of the VAE tend to drop, as is evidenced in the case of the decoding signal with respect to the encoder. This points to the possibility that the decoder might be ignoring signal from the encoder. \n\nAn architecture is then presented to alleviate posterior collapse by coupling the VAE and DAE, with structure of the encoder and decoder in both networks being shared. By doing so, the authors (seem to) argue that signals arising from this structure are boosted and we therefore get better gradients being propagated. The decoder is encouraged to not ignore the encoder's codes with the extra signal.  \n\nResults are given for various datasets showing that the NLL is lower for their coupled approach, as is the perplexity. Various other models are also compared against. In addition, in order to show that the connection between the input x and the latent code z ~ Q(z|x), they calculate the mutual information between these two quantities (appendix E - interesting estimation). The result is that their model achieves higher MI.\n\nMy thoughts: \nThe paper reads well, and the results are convincing. Using the shared architecture + coupling loss - which adds extra signal, pushing the stochastic signal to match the deterministic one, but not the other way round - helps to overcome encoder/decoder incompatibility, presumably allowing the decoder to not bypass the encoder content. However, I don't get a clear sense of how this might happen. The theoretical case is limited,and using a coupled/shared architecture is also standard (see, for e.g. Unsupervised image to image translation - although the context in that work is very different). "}