{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT. Their approach enables visual information to be integrated into large-scale text-only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.\n\nStrengths:\n- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.\n- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.\n\nWeaknesses:\n- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?\n- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.\n- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.\n- Why are there missing BLEU scores and the number of parameters in Table 1?"}