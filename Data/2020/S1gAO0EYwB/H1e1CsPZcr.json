{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an approach for learning disentangled representation in GAN models, while having an imbalanced training data. They build their model on top of the InfoGAN model. They also show that when the InfoGAN model is trained on imbalanced dataset, they produced imbalanced sample outputs.\n\nPros:\n1. The setting is very useful as most of the public datasets are imbalanced.\n2. The background on InfoGAN is clearly explained, makes it easy for any new reader\n3. The paper is clearly written and is very easy to follow\n4. The ablation studies is the key for this paper - it is well performed and neatly explained.\n\nCons:\nThis involves a couple of questions at philosophical level and few at technical level:\n1. I find the comparison between InfoGAN and Elastic InfoGAN biased in terms of imbalanced dataset experiment. While the Elastic InfoGAN gets the Gumbel smoothened coding vectors as input (c), the InfoGAN gets only a hard uniform probability distribution Cat(K = 10, p = 0.1) as input. Of course, this is with the assumption that InfoGAN authors have made with balanced dataset. The easiest extension to imbalanced datasets, is to have a non-uniform probability distribution p, Cat(K = 10, p = []), based on the imbalance in the training data. This would be a more suitable comparison of InfoGAN and Elastic InfoGAN\n\n2. [Philosophical] Even for an imbalanced dataset, I really dont find InfoGAN performing poorly, especially in Figure 1 (center). Most of the digits are still performing good, while there are obvious and interesting to observe overlap between (6,9) , (2,8), (3,7). I would not consider this poor learning from imbalanced dataset. As a matter of fact, a similar number of overlap is found for Elastic InfoGAN as well in Figure 6\n\n3. Also, the center image in Figure has many digits, still as random shapes. I really wonder, if the authors have kept all the hyperparameters constant between center and the right figure. Or maybe, the center figure is the output of an earlier epoch while the right figure is the output of a later epoch! \n\n4. I do not completely understand the reason for adding the L_ent part to the loss function. A part of the entropy loss is already handled in the L_infogan part. The authors of the InfoGAN paper, in the Eqn 4 of their paper has quoted this, \"However, in this paper we opt for simplicity by fixing the latent code distribution and we will treat H(c) as a constant.\" This is the exactly equivalent to L_ent of this paper. And to further state that L_ent can be an overkill in the loss function, we can observe in the results that in Table 1 for YTF, the ENT is performing poorly after adding L_ent loss.\n\n5. Also, this paper fails to mention, discussion, and compare an important references:\na. \"SCGAN: Disentangled Representation Learning by Adding Similarity Constraint on Generative Adversarial Nets\"\nThe similarity measure proposed in SCGAN and the L_dist proposed in Elastic InfoGAN is essentially the inverse same of each other.\n"}