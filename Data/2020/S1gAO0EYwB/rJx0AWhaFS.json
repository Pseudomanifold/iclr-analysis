{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview:\nThis work introduces a GAN-based generative model that allows to learn disentangled representations of images, and in particular automatically extract the object identity (e.g. digit number or human identity).\nThe model is based on an earlier approach, InfoGAN, and adapts it to better model imbalanced data. Namely, authors propose to put a learnable categorical prior (instead of the uniform) that presumably better models the discrete factors of variation in the data. In order to make the process differentiable, authors rely on reparameterization based on gumbel-softmax trick. In addition, authors propose a data-augmentation technique where they apply identity-preserving transformations (rotations / illlumination changes / etc) to the input images while forcing their latent codes to be close to each other. \nNumerical evaluation is conducted on MNIST and Youtube Faces, and indicates that the model performs better than the original in terms of how well it models discrete factors of variation.\n\nDecision:\nGenerally, the proposed solutions make sense: it is meaningful to use a more suitable prior and enforce consistency of latent codes for the same \u201cobject identity\u201d. However, it is worth noting that the problem that authors are trying to tackle is in fact very specific, and the proposed solution is only beneficial in settings where there exists a simple categorical representation of object identities and where it is easy to define identity-preserving transformations. \nNumerical results evaluation seems to indicate that the model indeed performs better than InfoGAN, which is not surprising given that the proposed model is essentially tailored to maximize the metrics used. However, some important baselines might be missing.\nIn general, I am not fully convinced that the method is generic enough and that experimental evaluation is sufficient, thus the current rating \u201cweak reject\u201d. However, I believe that the work actually has some potential and open to reconsider given that the concerns are addressed.\n\nQuestions / concerns:\n* The identity-preserving transformations make sense in some scenarios such as images of digits, but in some other cases might be quite hard to define, or require additional expertise. E.g. if one wants to learn generative models of molecules or high-dimensional physical measurements, using standard data augmentation pipeline is no longer trivial. Arguably, one of the reasons to design new methods for unsupervised disentanglement is to avoid these manual decisions. In this case, however, authors in some sense provide an additional supervision signal through these additional manually defined transformations.\n* Comparisons to state-of-the-art VAE-based disentanglement methods such as FactorVAE are missing. It would be also beneficial to see if the difference in performance would still persist when using a more stable version of InfoGAN (as e.g. InfoWGAN-GP from Kim\u20192018).\n* The application to faces in general makes sense and has potential, but does not look scalable: Figure 5 states that there is an additional latent variable for each identity, which would mean that in practice, when one wants to learn a generative model for face images, we would have to add the number of variables corresponding to the number of identities. What would probably make more sense if one can extract some more meaningful discrete factors of variation such as e.g. gender / hair color / etc. \n* (minor) I find it a bit concerning that the metrics are actually computed on the output of the classifier, which is trained on the real data whereas at evaluation time it is fed with artificially generated images. \n\n"}