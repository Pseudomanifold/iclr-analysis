{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper builds upon recent work on detecting and correcting for label shift.\nThey explore both the BBSE algorithm analyzed in Detecting and Correcting for Label Shift (2018)\nand another approach based on EM where the predictive posteriors and test set label distributions\nare iteratively computed, each an update based on the estimate of the other.\n\nCrucially, while the former method requires only that the confusion matrix be invertible,\nthe  latter method only appears valid under strong assumptions including the calibration fo the classifier.\nThus the authors propose an approach for \u201cbias-corrected calibration\u201d\nand shows that bias-corrected calibration can improve the performance of BBSE and EM.\nThe method is crucial for EM and with it, the results seem to show that EM,\nin the large sample (8000 examples) regime and with good initial classifiers\n (on the relatively easy CIFAR10 task with a strong baseline)\nthat EM outperforms BBSE.\n\nThe paper is easy to follow an the authors should also be credited for releasing \ncode anonymously with which we could reproduce their results. \n\nI have as few specific concerns/questions about the paper that I would like the authors to address: \n\n * They consider JS divergence as a metric for evaluation. But they don\u2019t consider other metrics\n  like the error in weight estimates which is considered in most of the prior work \n * They don\u2019t compare their results with regularizations suggested on top of BBSE, particularly Azizzadenesheli et. al. https://arxiv.org/abs/1903.09734. \n * They compare methods for particularly limited ranges of Dirichlet shift (\\alpha=0.1,1.0). \n  * What happens when the \\alpha increases to have less severe shifts? \nOptimizing ELBO with EM can lead to local convergence to the likelihood function when the likelihood is not unimodal. \n *  Is this likelihood function unimodal?  Does the EM approach converges to MLE under some appropriate initialization and assumptions? \n  \nA small presentation note: many of the papers are reporting the same metric and ought to be grouped as a large table, not as many tables. Also every table should state clearly what it is reporting in the caption, not just referring to earlier tables."}