{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents an approach to avoiding catastrophic forgetting in neural networks trained on classification tasks in an incremental manner. The authors term their approach \u201cEnsembleNet\u201d. The approach is very straightforward: the network architecture utilizes one sub-network for each class that outputs an \u201copinion\u201d score for an input, which indicates whether the input is a member of the class associated with that sub-network. A multi-layer perceptron then takes the opinion scores of each of the sub-networks and classifies the input. Whenever a new class is introduced, a new sub-network is trained up. The authors provide data showing that this approach does better at avoiding catastrophic forgetting than other approaches that have been proposed.\n\nThis paper was reasonably easy to read, and the motivations clear. However, it suffers from some major flaws that make its contributions quite limited, in my opinion:\n\n1) This approach can only ever work on classification tasks where we know the correct label for the data, or tasks where there is an explicit signal of learning from a new distribution occurring. If we do not know that we are now receiving data from a new category/task, then the network has no way to accommodate the new data. In other words, this approach essentially puts all the onus on the programmer/dataset for indicating whether to use new weights or adapt old ones, unlike other approaches that try to do this in an automated manner (e.g. Kirkpatrick et al., 2017). That effectively makes the problem trivial, by assuming that the hard work has already been done. This is essentially just a standard ensemble approach. Indeed, one could take this approach to the extreme and simply train a new neural network every time data from a new class is encountered. As such, I don\u2019t see how this paper makes any truly interesting theoretical or practical contribution to solving the problem of catastrophic forgetting.\n\n2) The claim that there would be time savings is not well supported. First, the time savings only happen if one assumes that the training is done in parallel on different machines. But, if training of each sub-network is incremental, then it can\u2019t be done in parallel! Moreover, the mathematical analysis in the supplementary is ridden with errors. Aside from numerous typos and errors in writing (see for example the equations at the bottom of page 10, or the broken equation reference on page 11), the basic analysis is wrong in several places, from what I can tell. Consider, \u201cLemma\u201d 7.1.3: the authors start by assuming alpha <=1, they then demonstrate that this can work for any value of c or |S|, since -4*c/|S| < 0 for all c and |S| > 0. Fine, but that doesn\u2019t prove that alpha <= 1! All that shows is that if alpha <= 1 it does not lead to a contradiction. Indeed, they can\u2019t assume that alpha <=1, because alpha is how the training time scales with the number of datapoints, and there is zero guarantee that this is sublinear. From what I can tell, the \u201cproof\u201d that T_en<T_ord is, in fact, invalid.\n\nSome slightly more minor points:\n\n- Critical measures like Omega_new, alpha_ideal, etc. should be defined, at least in the supplemental, not left for readers to go and look up in another paper.\n\n- I am not convinced that the benchmarking results from Kemker et al. (2018) provide fair comparisons. How well were these networks optimized in that work? Did EWC really only achieve 0.001 for Omega_new? That strikes me as potentially a hyperparameter tuning problem.\n\n- Figure 4 seems to tell us nothing beyond the easily stated: we can train each sub-network on a different computer (but, see my first major point above).\n\n- Is the final classifier only trained after all the sub-networks have been trained? That\u2019s what Algorithm 1 seems to imply. In which case, it\u2019s not truly incremental training!\n\nAltogether, given these various considerations, I cannot recommend this paper for publication at ICLR."}