{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2529", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an analysis of the BERT language model on financial text. FinBERT is evaluated on two datasets from the financial domain: a sentiment prediction dataset (classification with 3 different classes) and a sentiment score prediction (the score is a float number between -1 and 1). \n\nI find the phrasing \"FinBERT is a language model based on BERT\" misleading; I think FinBERT is BERT trained on financial text. There is no modification that is done to the original BERT model.\n\nThe paper presents several experiments using BERT as the language model and fine-tuning for the financial tasks. FinBERT is compared to a few baselines such as LSTMs with ElMO embeddings and ULMfit. I find interesting that the model performs better on the subset of the dataset for which there is perfect agreement between the annotators.\n\nI also find the results on training on financial data interesting. The results seem to indicate that further training on financial text does not seem to result in additional improvement when compared to original BERT.\n\nWhile I find the analysis and the experiments presented in the paper interesting, the novelty of the paper is rather low. There is no new idea introduced in this paper, it contains a series of experiments with BERT on financial text and tasks.\n\n"}