{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "I reviewed this paper for NeurIPS 2019 and the authors seemed to have submitted a very similar manuscript disregarding the comments of the reviewers after being unanimously rejected by all 3 reviewers at NeurIPS 2019. The following is very similar to my NeurIPS review.\n\nThis paper proposes a new method which uses variational information bottleneck to learn state representations and successor features to learn a policy for visual navigation.\n\nIn terms of originality and novelty, the method does not introduce very novel architectures or algorithms, but rather introduces a new combination of known techniques for the task of visual navigation. Successor features have been used in several prior works as mentioned by the authors. This paper just uses them for learning the policy. Variational Information Bottlenecks (VIB) is a known method for learning representations, in this paper VIB is used for Siamese style networks.\n\nThe lack of significant technical novelty is acceptable if the proposed method is empirically shown to perform very well. However, I have concerns about the empirical evaluation of the proposed method. Firstly, in my opinion, the agent should have a stop action. The success criterion should not be just reaching the goal state but using the stop action at the goal state. This is important for the experimental setup in this paper because I believe the authors use an episode length of 500 for environments with a total number of states less than 700. A random walk agent would have a significant chance of hitting the goal state (on that note, random walk baseline is missing). Without the stop action, it is difficult to understand whether the policy just learns to visit as many different states as possible or really learns to find the goal. This is especially important as the success rate is very low (20% for the proposed model, 15% for the baseline).\n\nSecondly, instead of defining their own training and test sets, authors should use training and test sets used by some prior work. This helps in proper comparison, benchmarking and reproducibility. With a new train and test set, it is unclear whether the environments were selected to highlight the strengths of the proposed method or the results generalize to other datasets as well.\n\nThe setup could also benefit from stronger baselines. I would have liked to see a baseline based on successor features. \n\nThe submission is not polished. The title in the PDF is wrong, and authors have provided a non-anonymized GitHub link.\n\nOther minor comments/questions:\n- Why is the goal state repeated four times in the input?\n- The authors mention that training a convolutional autoencoder is prone to over-fitting. This is not obvious to me, could you provide a reference?\n"}