{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\n\nThis paper proposes an actor-critic version of the Successor Features (SF) framework that relies on the Universal Successor Features Representations developed by [1,2]. The framework is trained end-to-end, with successor features being learned from a shared Siamese Network architecture, which is in turn regularized by a Deep Variational Information Bottleneck [3]. The architecture is tested on the target-driven visual navigation within the AI2THOR simulator, where it outperforms a Siamese Network baseline [4].\n\nThe algorithmic novelty of the contributions in this paper are marginal and while empirical results indicate a more performant system on the AI2THOR benchmark, these results are not explored and analysed in sufficient depth to meet the bar of publication. \n\nMotivation\n\nThe core of this paper is a Deep RL agent that relies on an actor-critic Successor Features framework. Actor-critic SFs have previously been explored by [1], and while there are minor differences with this framework (specifically in the reward regression loss), I do not see this as a novel contribution. In contrast to [1], the authors learn SF representations using a Siamese network architecture, which in turn is largely taken from [4]. The authors add an IB bottleneck to the Siamese network in the form of [3]. While there is novelty in this application, it is unclear to what extent it helps performance.\n\nEmpirically, the author\u2019s proposed architecture, VUSFA, outperforms [4]. The authors perform a nice ablation study where they show that the A3C-SF framework does not necessarily perform [4]: it is necessary to directly condition the policy on the SFs (this issue only arise in the actor-critic setup if SFs only feed into the critic) and adding an IB bottleneck yields further improvements when the training set is very small. \n\nHowever, these results are obtained by training on a mere 20 goals, representing less than 1.2% of all possible goals, and hence generalization performance do not go beyond a 20% success rate in terms of reaching new goals. It is unclear if this is a constraint of the environment or a choice by the authors. Regardless, the low success rate makes it hard to say anything about how these architectures would behave if trained on a richer distribution of goals. In particular, it seems that this data-scarce regime favors the author\u2019s proposed method, since the IB module is a regularization mechanism. A more diverse training set could change the conclusion of the paper, in particular as it appears allowing the agent to fine-tune renders the IB mechanism redundant in terms of generalisation performance.\n\nAdditional comments\n\n- There absolutely must be a related works section in the main manuscript.\n- The Successor Feature Dependant Policy is not the first architecture to directly condition the policy on SFs; most prior works directly derive the policy from the SFs via the Q function. \n- Section 3.1. You should mention that the Siamese Network approach is closely related to prior work. It appears as a novel contribution in the current manuscript.\n\nMinor comments\n\n- Eq. 3; it is worth noting that SFs incorporate dynamics of the environment - under a given policy (distinct from the goal).\n- Citation of A3C should be moved up to the first time you mention it\n- Section 3.2. Needs to be put in relation to [1], which is very closely related.\n- Section 4. \u2018Traditionally, SFs are not directly consulted when determining an action\u2019. This is incorrect - in most prior works, SFs are used to explicitly derive a policy (e.g. [5, 6]).\n- What is the meaning of Eq. 7? If it is denoting a stop-gradient operation, it needs to be reformulated. It currently appears as a conditional independence between the policy and SFs.\n- Notation in the Information Bottleneck part needs to be revised. The distributions q and E are introduced in equation (9) without a definition (I believe q is never defined), and it is unclear to me what J(.)_{min}  means. Is it the same as min J(.)?\n- Eq. 12 is overloading the notation of E as both expectation and encoder, which is rather confusing.\n- How does Eq. 12 relate to entropy regularized actor-critic methods like TRPO and PPO?\n- Section 71. Weather -> whether\n- Title needs to be updated\n- code link should be anonymous\n\nReferences\n\n[1] Ma et. al. Universal Successor Representations for Transfer Reinforcement Learning. ICLR workshop. 2018.\n[2] Ma et. al. Universal Successor Representations for Transfer Reinforcement Learning. arXiv. 2018.\n[3] Alemi et. al. Deep variational information bottleneck. arXiv. 2016.\n[4] Zhu et. al. Target-driven visual navigation in indoor scenes using deep reinforcement learning. ICRA. 2017.\n[5] Dayan. Improving Generalisation for Temporal Difference Learning: The Successor Representation. Neural Computation. 1993.\n[6] Barreto et. al. Successor Features for Transfer in Reinforcement Learning. NeurIPS. 2016.\n"}