{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper studies the relation between trainability and generalization ability in deep neural networks. In the theoretical analysis, the authors used the Neural network Gaussian process (NNGP) kernel and Neural Tangent kernel (NTK). The paper clarified that the spectrum of the NTK and NNGP has an important role in investigating the generalization and trainability, i.e., the condition number of the NTK. Some numerical experiments showed an agreement of theory with the practical behavior of learning algorithms. \n\nIn this paper, some existing theoretical results on deep neural networks were combined to extract new insight. Thought the attempt of this paper is interesting, the readability of the paper is not necessarily high. \n\n- In equation (2), the operator T is defined as the kernel K(x,x'). However, the definition seems different from that in equation (8).  The authors need to make clear the definition of T.\n- What is the \"DC\" mode in the sentence above the equation (15)? \n- Is the derivation of the left part in equation (9) straightforward? How was the second term, chi_1 q^* p^(ell), derived?  I'm not sure how the dot{T} was dealt with. The argument below equation (3) should be used? "}