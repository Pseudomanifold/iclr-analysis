{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "Summary of the paper:\n  \n        This paper describes a method to accelerate DNN by combining inter-operator and intra-operator parallelizing. The proposed method is validated on several hardware and inference setting with improvements ranging between a 1.2 or a 1.4 factor.\n\nDetailed comments:\n\nThe has important grammar errors: e.g. \"Existing deep learning frameworks has focused...\" and \"Deep neural networks (DNNs) have achieves...\" etc. This makes the text almost unreadable.\n\nThe latex template has been modified. For example, there is no space below some subsections like 4.2 and 5.3.\n\nThe experiments do not seem to have error bars. This questions the statistical significance of the results.\n\nThe improvements obtained seem small which questions the practical utility of the method.\n"}