{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents an approach and implementation to schedule operators in deep learning inference to better utilize parallel hardware. The work is a nice piece of engineering work with good results, and I enjoyed reading the paper. However, the scientific novelty in the paper is very low. \n\nThe approach relies on that we have measured the execution times of each kernel / operator / convolutions in advance. Further, the execution time should not vary too much, otherwise the schedule will be non-optimal. This assumption can be true in some contexts, but hard to achieve in other. For example, it will depend on whether the data is in memory, caches, or somewhere else. Depending on the data location, access times (and thus execution times) may vary. However, the empirical data suggets that this is not a big problem.\n\nThe approach to find an optimal schedule is very straight-forward and done in a standard way. Thus, the scientific novelty is low. For example, there is a lot of work since decades about scheduling and load balancing of multiple tasks on parallel hardware. However, none of the work from that domain is mentioned. In general, there seems to be a big gap between the DL/ML/AI communities and the computer system community. I think there are great benefits for both sides to collaborate a bit more.\n\nThe approach is evaluated on three hardware / graphics cards and four DNN models. The results are good, and the approach performs better than, e.g., TensorRT et al. The work can have some practical impact, when/if implemented in existing frameworks.\n"}