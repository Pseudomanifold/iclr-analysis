{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Attention models has been extensively used to achieve state-of-the-art performance in computer vision and natural language processing. This paper identifies a problem that the attention weights in the standard models tend to concentrate on single or very few locations which fails to reflect relations between entities. The main contribution of the paper is to introduce a relation module in the neural network's architecture and a relation loss to enhance learning relations between different entities. Experiments are conducted on several benchmark tasks including object detection, scene categorization and document categorization to show the effectiveness of the method. \n\nOne major concern I have is about the novelty of the method. In particular, the center-mass cross entropy loss, which is one of the central component in the paper, is indeed the same as the focal loss in [1]. The only difference is to apply it on a matrix to measure relations. This makes the contribution limited. Moreover, I also have some concerns about the experiments, which will be detailed in the following. Overall, I find the paper has interesting observations and results, but the paper is not easy to follow and the contribution is limited. I am willing to increase my score if my concerns are clarified. \n\na. About the training of the relation matrix  \nOne of the key component of the algorithm is to train a relation matrix between entities. I have a few concerns about this step. \n\nFirst, are the entities given by a pre-trained model or is it learned during the training? If these entities are learned, then how do we train the relation matrix? For example let's say they are region of interests (ROI), the region proposal will be very poor at the beginning and the correct regions may not be included so how to handle the relation matrix between these poor regions? Moreover, how do we use the ground truth regions since maybe some of them are not correctly identified even in the well-trained model. \n\nSecond, do we need extra annotations to learn the relation matrix? In particular, the information of relations is needed in order to train the relation matrix, this does not seem to be free from since we need to distinguish whether they are the same instances. This would make the comparison to other models unfair since more information are available here. \n\nb. About the comparison to the benchmarks\nI am not an expert of the tasks considered in the paper so I may not notice the sensitivity of the improvement.  \n\nIn the object detection task, I would expect a better state-of-the-art performance since F-RCNN seems to be a weak baseline. The second column of the object detection table is using FAN + L_{det}, but it cites the Relation Network [2], is this a better baseline comparing to the Relation Network? What are the differences?\n\n[1] Focal loss for dense object detection, Lin et al, 2017\n[2] Relation networks for object detection, Hu et al 2018"}