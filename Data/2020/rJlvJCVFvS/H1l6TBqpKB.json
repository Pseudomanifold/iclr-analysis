{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThe following work proposes some effective and simple pseudo-labels for graph attention on both vision and language tasks. The supervision provided by these heuristic-based allows the final model to outperform the unsupervised attention baseline. Results are demonstrated on the relationship proposal task, object detection (attention is applied to context encoding for each object), and scene categorization, and document categorization. The largest gains appear to be on the relationship proposal task, with small improvements over SOTA on all other tasks.\n\n\nStrengths:\n- The paper was fairly easy to follow\n- The proposed automatic attention labeling schemes appear to be simple and effective\n- Results demonstrated improvements on a variety of tasks.\n\n\nWeaknesses and questions:\n-For visual recognition datasets, the attention targets are defined such that we want to maximize the weights between two overlapping objects of differing classes. This seems very dataset/task specific, and clearly would not apply to other datasets such as human-human interaction datasets.\n-I'm not sure that the center-mass CE loss is strictly a cross entropy loss? Maybe it would be if one takes the log of \\tilde{\\mathcal{W}} in (2) .\n-Should center-mass be center-of-mass instead?\n-The proposed center of mass seems to be a straightforward weighted reduced sum. I'm not sure where the \"notion of center-mass\" comes into play, nor how this is a novel contribution\n-The pairing scheme for language tasks as shown in Figure 2 seems fairly arbitrary. Has this been properly ablated? How much does each particular pairing matter? How much better is it than some pairing scheme that includes the negative set of edges (verb-adjectiv, noun-adverb)?\n-Any form of failure mode analysis is missing from this work. Figure 1 shows qualitative examples of when the proposed focused attention network performs qualitatively better than the Relation Network from Hu et al, but it's hard to believe that this is always the case.\n\n\nOverall, I don't see the proposed loss, nor the conclusion that supervised attention (even with heuristic-based labels) can outperform unsupervised attention to be sufficient contributions. I do think that the discovery of a simple and effective pseudo-labeling scheme (creating edges for overlapping objects of differing classes) to yield some insight. That said, this is certainly much task specific, and such a similarly simple attention labeling strategy may not exist for other vision tasks."}