{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a framework for learning with rejection using ideas from adversarial examples. The essential idea is, while predicting on a point x, we can reject classifying the point if it has an adversarial example very close to it. So, the algorithm can be simply summarized as,\n1. Learn a classifier function f\n2. On the test set, predict on a point, only if it doesn't have an adversarial example close by.\n\nI am inclined to reject the paper for the following reasons:\n1. The proposed approach is a variation of a fairly well-known heuristic. Having a close adversarial example is same as saying that the current point is very close to the decision boundary. Being close to the decision boundary is a heuristic that has been applied in multiple scenarios in machine learning.\n2. The proposed approach is not novel. For example, [1] uses adversarial example style detection to augment their training data and improve their end-to-end model. \n3. There have been approaches which attempt to learn rejection function [2], so it would have been good to at least do a comparison of the proposed approach with such methods.\n\n[1] Adversarial Examples For Improving End-to-End Attention-based Small-Footprint Keyword Spotting, ICASSP 2019\n[2] SelectiveNet: A Deep Neural Network with an Integrated Reject Option, ICML 2019"}