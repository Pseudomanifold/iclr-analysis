{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper wants to study the problem of \u201clearning with rejection under adversarial attacks\u201d. It first naively extends the learning with rejection framework for handling adversarial examples. It then considers the classical cost-sensitive learning by transfer the multi-class problem into binary classification problem through one-vs-all and using the technique they proposed to reject predictions on non-important labels, and name such technique as \u201clearning with protection\u201d. Finally, they do some experimental studies. \n\nThe paper does not show any connection between \u201clearning with rejection\u201d and \u201cadversarial learning\u201d. The method it proposes is also a na\u00efve extension of existing methods. Both the problem setting and the technique does not have novelty. The paper fails to realize that the motivated application is actually called \u201ccost-sensitive learning\u201d and has been studied long time before. The paper also has problems in writing. Finally, there is no comparison with any baseline. Only empirical results of the proposed methods are shown. Due to all these reasons, there is still a long way to go before the paper can be published. I will rate it a clear rejection.\n\nMore specially, \n\nThe definition of \u201csuspicious example\u201d in Sec.3.1 has no relationship with adversary examples. Does the paper focus on adversary examples? If the definition has no relationship, it is classical learning with rejection. \nIn the last equation of Page 3, there is no definition of \\tilde L. Actually, according to Figure 1, x\u2019s is more close to the decision boundary, it is an example more hard to classify, which could also be \u201csuspicious\u201d. \nIn the definition of \u201csuspicious example\u201d at the beginning of Sec.3.1, is both x and x\u2019 defined as suspicious examples in this way?\nIn the last equation of page 2, there is a rejection function, so minimizing this loss is a \u201cseparation-based approach\u201d. However, at the end of Sec.2 the paper states they \u201cfollow a confidence-based approach\u201d. Any comment on the inconsistency?\n\nThe motivated problem is not new. It is called cost-sensitive learning in machine learning and can date back to 2001:\nCharles Elkan. The Foundations of Cost-Sensitive Learning. IJCAI 2001: 973-978.\nWhere they study the same problem when misclassifying one class of data may cost a lot than misclassifying another class of data. The current paper has not discussed any related work of cost-sensitive learning although they want to study a problem in its field. \n\nThe paper should be also improved in writing in the following aspects. \nThere is a lot of inaccurate statements in the paper. For example,  \u201cIn Sections 3 and 4, we propose and describe our algorithm\u201d, what is the difference between propose and describe? \u201can estimator \\hat h might return result that differ greatly from h^* in a case with finite samples\u201d. Actually there are rigorous theoretical results describing how the number of finite samples will impact the estimator \\hat h on unseen data. For example, \nPeter L. Bartlett, Shahar Mendelson. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results. JMLR, 2002.\nSo inaccurate/unclear statements that will mislead readers should be avoided. \n\nIn writing, the paper also lacks the necessary references in many places. For example, \u201cLearning with rejection is a classification scenario where the learner is given the option to reject an instance instead of predicting its label.\u201d, \u201c\u2026classifies adversary attacks to two types of attacks, white-box attack and black-box attack.\u201d, \u201cMethods for protecting against these adversarial examples are also being proposed.\u201d. Necessary references are needed for these places.\n\nThe organization is also problematic. For example, in the second half of Sec.2 introducing two kinds of learning with rejection models, it should be included in a \u201crelated work\u201d part. \n"}