{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "There is still no universal method to deal with adversarial examples, and introducing a reject option to flag potential attacks seems a sensitive choice for many applications. The considered problem is well-motivated and introduced, and I\u2019m unaware of prior work studying classification with reject option in the context of adversarial examples. However, I think there are different dimensions along which the paper could be improved:\n\n- My understanding of classification with a reject option is that the rejection cost c(x) is a design choice that can depend on the specific application. While c(x) is introduced as part of the framework it is then derived in a very specific way, removing the design aspect or at least not explaining very clearly how one would design it. Also, Algorithm 1 doesn\u2019t have corresponding parameters.\n\n- The rejection function r(x) relies on z^* which essentially amounts to computing an adversarial perturbation for a given testing point, see (2). The authors state that they use a 30-step of the PGD algorithm to find z^*. The attacks on which the method is tested only uses 10 PGD steps. What if the attacker is stronger in the sense that it runs significantly more PGD iterations than used to compute the rejection function? How sensitive is the rejection function to different initializations?\n\n- Similarly, what happens if the attacker and the rejection function rely on different norms to compute the attack and the rejection score, respectively? I think it is important to investigate this aspect.\n\n- In Tables 1 and 2, I don\u2019t understand why the precision is the same for all rows corresponding to the same attack (strength), while the other metrics vary.\n\nOverall, I think the paper explores an interesting direction, but would greatly benefit from a revision along the lines outlined above.\n\nMinor comments:\n- P3 3.1 2nd paragraph: \u201cLet B^p...\u201d rather than \u201cLet B^\\infty...\u201d\n- P4 bottom: \u201c)\u201d is missing before \u201c..., where\u201d\n- P6 bottom: (FR) should be (TR). The acronyms FA and FR don\u2019t seem to be introduced.\n"}