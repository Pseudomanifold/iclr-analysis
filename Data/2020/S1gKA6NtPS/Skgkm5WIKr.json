{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper does a good job at specifying a solution, but never states the problem.\n\nFor the problem specification, please see the introductory paragraph in https://arxiv.org/pdf/1905.11481.pdf which I quote here to inform other readers: \n\"In 1601, Johannes Kepler got access to the world\u2019s best data tables on planetary orbits, and after 4 years and about 40 failed attempts to fit the Mars data to various ovoid shapes, he launched a scientific revolution by discovering that Mars\u2019 orbit was an ellipse [1]. This was an example of symbolic regression: discovering a symbolic expression that accurately matches a given data set. More specifically, we are given a table of numbers, whose rows are of the form {x1, ..., xn, y} where\ny = f(x1, ..., xn), and our task is to discover the correct symbolic expression for the unknown mystery function f, optionally including the complication of noise.\"\n\nFor people familiar with policy gradients and RNNs, you need only look at the policy RNN in Figure 1. This is a standard approach for sampling a symbolic expression (just as is often done when an RNN composes another net in AutoML). However, note that the authors introduce a bias (input keeps track of parents and siblings) to effectively incorporate hierarchy. Could the authors please expand on their heuristics for automatically choosing the (parent, sibling) input pair? Adding this to Algorithm 1 would help. It would also help with clarity if you could please add the fitting of the parameters of the symbolic expressions using BFGS to the pseudocode.\n\nThe RL approach is standard and the authors have executed it carefully and conducted the necessary ablations. However, the axes in Figure 2 should be improved. \n\nThe experiments indicate that the proposed RL approach works better than genetic programming (GP) for what appears to be a simple benchmark. However, this is hard to judge. To truly understand the experiments, I advise readers to first look at: https://researchrepository.ucd.ie/bitstream/10197/3528/1/uy_gpem.pdf   \n\nI would have loved to see training and test curves, mathematical expressions for the reward so it is unambiguous, ablations of the dataset (eg varying the number of data). I assume a single net with the same parameters applies to all expressions. Is this correct?\n\nWhile applying policy gradients to symbolic regression is a great idea, the write up of this paper needs to improve substantially.  \n\n"}