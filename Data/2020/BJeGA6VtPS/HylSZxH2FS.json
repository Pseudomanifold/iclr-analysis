{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studies an attack scenario, where the adversary trains a classifier in a way so that the learned model performs well on a main task, while after a certain permutation of the parameters specified by the adversary, the permuted model is also able to perform another secret task. They evaluate on several image classification benchmarks, and show that their trained model achieves a comparable performance on both the main task and the secret task to the models trained on a single task.\n\nI think this paper reveals an interesting phenomenon, i.e., the same model architecture trained with different benchmarks may share similar parameters after a proper permutation; but I am not convinced by the threat model studied in this work. For the attack scenario studied in this paper, it should be ideal to enable the model to perform both the main and the secret tasks at the same time. However, the permutation process could be very time-consuming, especially when the number of model parameters goes large. The time overhead of the transition among different tasks would make the model more suspicious to the user. It would be great if the authors can motivate their threat model better.\n\nOn the other hand, considering the purpose of training a single model to perform the prediction tasks on several benchmarks, I would like to see how general their conclusion holds. For example, what happens if the main task is on a benchmark with a large label set? I would like to know if two models trained on different datasets with a large label set could also share the same set of model parameters under a certain permutation; or if the secret task has a much smaller label set than the main task, how well the performance could be?"}