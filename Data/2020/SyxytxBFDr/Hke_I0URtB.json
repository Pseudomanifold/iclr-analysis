{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary of contributions:*\n\nThis paper introduces a simulation framework for robotics learning called Lyceum. They argue it is 10-20x faster than popular existing frameworks.\n\nThey successfully motivate their work with two arguments. First, they claim that sim2real is a successful strategy, but limited by computational resources required to run sufficient numbers of simulation steps. Second, they claim that real-time model predictive control is limited by the failure of existing frameworks to meet latency requirements.\n\nTheir work is well-placed in the context of the robotics learning frameworks such as MuJoCo, OpenAI gym, and dm_control. Lyceum is an alternative to high-level abstractions (such as OpenAI gym and dm_control) that exist on top of physics simulators like MuJoCo. One advantage of the existing high-level abstractions is that there are a number of algorithms already implemented on top of them. Lyceum does already implement several of the most popular algorithms (PPO, natural policy gradient), and supports the ability to implement other algorithms, but not all the common algorithms are implemented on Lyceum yet.\n\nThey give a description of how the Julia programming language is well-suited for this task.\n\nLastly, they show two experiments: Their first set of experiments (Figure 3) aim to show that sampling from Lyceum parallelizes linearly across up to 16 cores, whereas OpenAI Gym and dm_control do not parallelize linearly. Their second set of experiments (Figure 1) aim to show that Lyceum speeds up wall clock time for end-to-end training by 10-20x.\n\n*Decision:*\n\nOverall, this is a well-written paper and the ecosystem itself sounds like an interesting contribution. If this paper were just an announcement of a new simulation framework, I\u2019d be excited to try out the framework. However, to warrant an ICLR paper I think it needs two major changes: (1) more rigorous experiments and (2) for the code to be available during the review process.\n\nI\u2019d encourage the authors to use the review feedback to improve the paper - I\u2019d be happy to give an \u201caccept\u201d score if this feedback is (very) thoroughly addressed.\n\n*Important comments/questions (these do impact the score):*\n1) Regarding experiments in Figure 1 (the end-to-end performance + speed experiments):\n\na) It should be more clear what this figure is saying. First I think you need to establish that your model achieves the same (or comparable) performance in the same number of time steps.\n\nIt would make sense to use a fixed benchmark for when a game is considered \u201csolved\u201d and then report the scalar values for (1) number of simulation steps to reach that threshold and (2) the wall clock time to reach that threshold, for both OpenAI Baselines and your implementation.\n\nI\u2019m eyeballing off your graphs, but it looks like Lyceum might take 10x less wall clock time to run the same number of simulation steps, but it doesn\u2019t look like you\u2019ve achieved a 10x speedup in the time it takes to achieve a certain score. (Swimmer v-2 maybe gets a 10-ish-x speedup to achieve a score of ~75, but Hopper-v2 looks to get a 2-4x speedup of time to achieve a score of ~2000, and Humanoid-v2 doesn\u2019t ever even reach the same performance as OpenAI baseline.)\n\nb) OpenAI Baselines includes two PPO implementations, aptly named PPO1 and PPO2. Specify which implementation you used.\n\nc) Your PPO implementation gets significantly worse performance on Humanoid-v2 than the OpenAI Baselines implementation. There is one sentence on page 9 explaining why this might be the case (\u201creward and observation scaling, value function gradient clipping, orthonormal paramter initialization, and more\u201d), but the rigorous thing to do would be to improve your PPO implementation to match the OpenAI Baselines performance.\n\nIn particular, because this is the most complex of the three environments you experiment on, the low performance significantly weakens your claims. You are using this experiment to make an argument about the speedup of end-to-end training, but then you qualify the worse performance by saying it doesn\u2019t matter because your sample efficiency is faster and that\u2019s what matters. If your argument is based primarily on the sample efficiency, then there is little value added by this experiment because your other experiment is about the sample efficiency.\n\nd) This experiment would be much stronger if you showed performance results on multiple algorithms (not just PPO), and achieve comparable performance to a high-quality baseline (such as OpenAI Baselines), and use more than one complex environment (such as Humanoid).\n\n2) Regarding experiments in Figure 3 (the sampling efficiency + parallelization experiments):\n\na) Your main argument is that Lyceum achieves a linear speedup in sampling when parallelizing across up to 16 cores, and that OpenAI Gym and dm_control do not. I am convinced that Lyceum does achieve this speedup, but I\u2019m not totally convinced that your parallel implementation of OpenAI Gym and dm_control are optimal, which would weaken the comparison.\n\nYou used the multiprocessing library in Python, but then you say your profiling indicates that it\u2019s an IO-bound task (in which case we wouldn\u2019t expect \na linear speedup using multiprocessing). In addition, you say that Github issues suggest using multiprocessing to parallelize OpenAI gym, but the most relevant Github issue I found (https://github.com/openai/retro/issues/42) suggests using a vectorized environment like SubprocVecEnv to create a separate subprocess for each environment.\n\nb) Smaller note: in Figure 3, it\u2019s confusing to say \u201cours\u201d when you\u2019re comparing Lyceum to your multiprocessing implementation of gym. Both are technically \u201cyours\u201d, so distinguish them by saying \u201cLyceum\u201d and \u201cgym + multiprocessing\u201d\n\n3) Exactly which environments are supported (currently) in Lyceum?\n\n4) When will the code, tutorials and demos be available? The link only gives a \u201cComing Soon!\u201d page. If this were a paper making an algorithmic contribution, I\u2019d be less inclined to reject the paper solely because code wasn\u2019t available yet. However, the contribution of this paper IS its codebase, so the code really should be available during the review process.\n\n*Other comments to improve the paper (I think these should be addressed, but didn\u2019t impact the score):*\nFigure 1: the x- and y-axes should have units.\nFigure 2: the caption is confusing. there is a reference to (top) but the figure is one row containing four images. Rewrite this caption so it is clear what each image is.\nPage 9: paramter -> parameter\n\n*Suggestions (it's up to you if you want to address this):*\nPage 5-6: The full AbstractEnv interface is given, but I think there is some other format that would be a better use of space. One option would be to include an appendix with the full interface, written in the style of documentation (instead of using comments above the signature for each function)."}