{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a new software system for robot learning. The emphasis of the paper is on scalability. The authors rightly identify that many of today's advances in robot learning are out of reach of many groups due to the sheer scale of the computing infrastructure needed to generate results. The key novelty of the new approach is in switching to the Julia programming language which offers the easy of a high level programming language with the speed of a native language. The authors offer bindings to influential robotic libraries and demo some existing RL algo's in their library.\n\nI have a few concerns with accepting this paper in a conference:\n- The authors don't go into a lot of depth on why existing robotics results require so much resources. The examples they give (lots of cores, lots of GPU's) suggest that not the \"glue\" code is the main bottleneck but rather the float-compute heavy parts. I think the paper would be a lot stronger if the authors could do a detailed analysis of existing RL experiments and highlight which bits of the computation they are focussed on speeding up.\n- Related, the experiment the authors show don't address the problems which they quote in their motivation. A strong paper would motivate the need for a new ecosystem from a set of problems and then show that their new software solves those problems.\n- In the experiment section, I believe the authors should also think more carefully about comparing two pieces of software which are leading to a different result: figure 1 clearly shows that the results of OpenAI baseline and their approach lead to very different outcomes. In the description the authors hypothesize this is likely due to very different implementation (i.e. OpenAI baseline doing gradient clipping and many other things). If so, I don't trust the experiment performance results as such.\n\nThere is an interesting discussion to be had when a paper introducing a new piece of software warrants publication in the conference. I can think of a few reasons\n- A library has been validated through significant adoption (e.g. Tensorflow/PyTorch).\n- A library introduces a whole new paradigm/algorithm to solve an existing ML problem (e.g. low precision backprop).\n- A piece of software solves a practical problem (academic/industry) which democratizes an area (e.g. Dactyl on a moderately sized cluster).\nIn this particular submission, I don't see a key breakthrough of the sort above. The paper is well written and a good description of the new library. At this point, this paper feels more like a whitepaper about the new library than a scientific paper which claims to solve a problem in a new way.\n\nMinor:\nTypo in first sentence of conclusion 'intoducing'"}