{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The main contribution of this paper is to learn a universal policy that is able to perform near-optimally on test tasks with transition dynamics that were never observed during training. This is achieved by using a \"probe policy\" to generate short trajectories that are then used to learn a latent encoding to categorise the transition dynamics of the current task. The universal policy is then conditions on both the state and this encoding so that the learned policy can perform well on tasks with different dynamics.\n\nOverall I really like this paper. There has been a big push in the RL community to start evaluating algorithms on test tasks that are different to trained tasks and this paper takes a good step in this direction.\n\nMy main concern with this paper, which I would appreciate some feedback from the authors on, is regarding the trajectory length of the probe policy:\n\n1. The method seems to rely heavily on the ability to learn an accurate latent encoding from a short trajectory which will may not be the case in many domains. It is easy to construct some domain where the dynamics behave identically initially but have some major differences deeper into the task at hand. I noted that the authors did mention this in their conclusion as an area for future research but I want to know if they have any high level ideas on how to rectify this because it seems that this will be a major limitation of the proposed algorithm in practice at least for certain domains.\n\n2. In the experiments I can see that in some cases a longer probe performed more poorly than a shorter probe (for example Fig3a for 2D navigation. My intuition tells me that a longer probe should outperform a shorter probe since it contains at least as much information. Can the authors explain why this occurs?"}