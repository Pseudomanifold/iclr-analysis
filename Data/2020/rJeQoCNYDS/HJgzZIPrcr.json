{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a strategy for single-trajectory transfer of a reinforcement learned policy.  They follow a typical approach in the few-shot supervised-learning community, of assuming that the plausible set of solutions may be modelled as a much lower dimensional latent variable, and then try to quickly infer that latent variable at test time.  In this case, the latent variable is \u2018Z\u2019.\nAt test time, they first run an exploitative algorithm (they call this the probe) to rapidly infer the value of Z, and thereby the optimal policy which this indexes, before switching to running the policy alone.\n\nWeak reject.\n\nI think that this paper (if indeed novel) is interesting, and I do agree that few-trajectory transfer in RL is a potentially impactful area in which to be working.  I think that the magnitude of the contribution is medium->low, however (namely that they have an approach which is wall-clock efficient, for single trajectory transfer), however the precise details of this contribution/claim have not been adequately tested:\nWe do not see experiments which highlight the wall-clock or inference cost competitiveness of the approach, and we do not see experiments against other, existing, approaches for latent-variable based RL (mostly these involve model-based RL, however these would probably be easy to derive for many of the test cases).\nWe also don\u2019t see any experiments which evaluate the performance of SEPT as the time spent optimising the probe policy is varied.\nAs an aside, in figure 2(e) it appears that the Oracle policy is significantly outperformed by SEPT, do you have any thoughts as to why this might be?\n\nTo improve the paper, I would propose a slight rewrite to focus only on the core claim (i.e. why this model outperforms anything else for wall clock competitive single episode RL policy transfer).  I\u2019d also be interested to see how this approach scales as the size of the potential policy set increases (e.g. if we had to produce a policy which would generalise for for all ball sports).\nAny theoretical work would also be very welcome\u2014one may be able to draw on existing work in the k-shot community to make a judgement as to what fraction of the trajectory is required before one should have a low entropy estimate of Z, and it would also be nice to know how the approach scales as the complexity of Z ramps up\u2014i.e. as we need to support more tasks."}