{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper treats the CRF-based NER tagger as the amortized variational posterior in a generative model of text given tags. In this manner, a VAE is defined that can be trained in a semi-supervised manner. Partially supervised learning (PSL) is also explored.\n\nThis paper is not well-written for the following reasons.\n\n1. This paper over-claims:  \"to our knowledge, we are the first to utilize VAEs for NER.\" \n\nThere are some works in applying VAEs to sequence labelling, even the harder seq2seq learning. For example,\n\n[a] Language as a latent variable: Discrete generative models for sentence compression, EMNLP 2016.\n[b] StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing, ACL 2018.\n\nThe authors should concentrate on the novel elements of their proposed method. A straightforward of applying VAEs to NER for semi-supervised learning does not warrant an publication at ICLR.\n\n2. Unfortunately, the novel elements are not elaborated. It is difficult for the reader to tell which parts of the proposed method are novel.\nIf only looking at the technical development in the main paper (without the Appendices), the development seems to be weak, not substantial contribution from applying VAEs. The authors should make a better balance between the main content and the Appendices.\n\n3. The experimental results are also not well organized. Comparisons are here and there, without a main thread.\nMF, MT are used without definitions."}