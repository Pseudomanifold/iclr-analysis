{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers the task of Named Entity Recognition and formulates it as a the task of segmenting a sequence of text tokens by a corresponding sequence of tags. This requires to use a special tag symbol which is labelling text parts that are not in any entity span. Consequently this special tag will never occur in partially annotated training examples. Moreover, the authors consider and explore a multitude of generative models whose decoders (conditional probability of the text sequence given the tag sequence) are beyond simple conditional independent models (as in standard HMMs). To learn such models from semi-supervised and partially labelled training data, the authors propose to use the VAE approach, assuming the encoder to be a linear CRF model (i.e. a conditional HMM). Implementing this program requires (i) to formulate the tasks for unlabelled and partially labelled training data, (2) to use an approximate but differentiable sampler for the encoder model and (3) to compute the KL-divergence for Markov chain models. The authors consider known options for each of these problems and then analyse the resulting approach for a multitude of generative models experimentally. \n\nIn my opinion the task formulation, the related models and considered learning tasks are highly interesting and conceptually relevant. Nevertheless, I would not recommend to publish the paper in its present state for the following reasons. The paper is in my view to much application oriented and cluttered with various application/implementation details, which are rather obscuring several interesting and relevant conceptual questions. The same holds in my view for the  unnecessary large number of considered model variants. The paper can be improved by moving the conceptual and technical explanations from appendices to the main body and delegating application details, details of some model variants etc. to the appendices. \n\nFurther questions/comments:\n- Why are you using a fully factorising prior for the tag sequences? Would it be possible to use a Markov chain model here?\n- Why and where from comes the log probability of the encoder (q) in the supervised loss?\n- The score function estimation and other similar approaches indeed suffer from high variance gradients. On the other hand, the proposed relaxed \"perturb and MAP\" approach is clearly an approximation. It is therefore not clear to me why the latter is to be preferred to the former."}