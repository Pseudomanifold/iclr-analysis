{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work applies CRF autoencoder to the task of semi-supervised named entity recognition. The latent variables are the discrete tag sequences; and to facilitate end-to-end training, an amortized variational objective is used. Different generative model (i.e., generating input tokens conditioning on the latent tags)   architectures are explored. Experiments with NER with the Ontonotes 5 dataset show that, with proper architectures and prior distribution, the proposed model outperforms strong baselines. \n\nOverall I find the paper clearly presented and well executed. However there is not much to learn from the technical part: most of the key components have been around for a while, and the paper does not seem to attempt to solve any key challenges.\n\nTo be more constructive, I think the paper can be improved by:\n\n- Evaluating the proposed method with other tagging tasks, and discuss whether the architecture and prior distribution choices for the generative model are task-dependent. \n\n- Discussing how the model (especially the generative part) translate to other structured latent variables, e.g., syntactic trees and semantic graphs.\n\n- More carefully evaluating the partially supervised learning objective, which seems interesting can could potentially benefit future research.\n\nI'm happy to adjust the score if the authors can justify the technical contribution and take the effort to improve the paper."}