{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tries to control the variance of advantage function by utilizing the independence property between current action and future states. The practical approach they are using is to learn a dependency model of reward function as a control variate to lower the variance. Using the control variate technique they derive a (maybe complicated) algorithm to update the policy by PPO. Empirical results seems competitive.\n\nThe major concern is the novelty. It is similar to many of the control variate technique papers (e.g. Liu et al. (2017)), which learn a model to decrease the variance in a certain way. I don't see from the paper for the advantage of applying control variate over advantage function compared to previous methods.\n\nThe minor concern is for the clarity. Section 4 is unsatisfactory. The derivation seems correct to me, however, there are too many parameters introduce to optimize, which I cannot directly get the main page of what the algorithm is doing for the first time read. From the algorithm box, there are totally 5 different parameter: $\\theta, \\phi, w_1, w_2$ and $\\psi$ to update which make the algorithm pretty messy. I believe there are better way to either get a simpler algorithm or demonstrate your algorithm in a better way. Section 5 is even harder to understand. Could you explain why equation (14) is similar to Liu ei al. 2018?\n\nOverall I tends to reject the paper at the moment. I encourage the authors to do more surveys on control variate technique in policy optimization and highlight the novelty of why controlling the variance of the advantage function can help to boost the performance of policy optimization.\n\nReference:\n1. Liu, Hao, et al. \"Action-depedent Control Variates for Policy Optimization via Stein's Identity.\" arXiv preprint arXiv:1710.11198 (2017)."}