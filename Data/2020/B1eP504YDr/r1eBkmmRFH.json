{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new advantage estimator in reinforcement learning based on importance sampling. This form allows for a significantly lower-variance estimator for situations where the current action \"stops mattering\" to the future state. A control variate, as in Grathwohl et al., is used to combine the importance sampling estimator with the \"standard\" estimator in a way that is always unbiased and attempts to minimize the overall variance.\n\nThe overall setting makes sense. I found your example (in the second paragraph of the introduction) initially somewhat misleading, though: in the setting where a game is composed of fully independent rounds, surely these would simply be modeled as completely separate MDPs. Even if not, settings where the rounds are reset after a variable length of time (e.g. the round ends when one player achieves some objective) would *not* fit the exact independence structure you assume at the start of Section 3, if your current action affects when the game will reset. But of course your estimator does not rely on actual *independence* (C = 0); it can take advantage of only \"weak dependence\" (and moreover this dependence need not be pre-specified). You might think about emphasizing this a little more in the introduction to emphasize that the estimator is general, and you're looking for one that can take advantage of these kinds of situations.\n\nIt might be worth noting after (2) that $C^\\pi_k$ is upper-bounded by $1 / P^\\pi(a_t \\mid s_t) - 1$, so that the importance sampler is always well-defined and unbiased when action probabilities are nonzero. This does raise an issue: a policy which *ever* deterministically avoids an action, i.e. $\\pi(a_t \\mid s_t)$ in (13) is 0, will break the method. This is worth explicitly stating somewhere.\n\nSomething worth thinking about a bit: any choice of weights for your control variate provably doesn't affect your estimator in expectation (and you try to decrease its variance), so that bad estimation of e.g. the quantities in (7) won't lead you to being \"incorrect,\" just higher-variance. But a bad choice of parameters in your $C^\\pi$ estimator *would* bias your estimates. This is in some ways the same as the effect of using a value function or $Q$-function approximator, but can we say anything about the ways in which a bad $C$ estimator would likely affect the overall optimization process, perhaps in some very simple case? Would an unbiased $C$ estimator lead to an unbiased advantage estimator? (Not that it's clear how to get an unbiased estimator of the ratio in $C$ anyway.)\n\nSome minor points on notation: Using $r_{t+k}$ for the control variate was initially confusing to me, because elsewhere you've used e.g. $S_t$ for the random variable of a state and $s_t$ as the value of that state -- it made me think that $r_{t+k}$ was somehow supposed to be the value of a reward $R_{t+k}$. Another letter might be better. Similarly, $V_{w_1}(s_t)$ of (7) isn't really a value function; it's the difference between the value function and the sum of discounted control variates. Also, $C_\\phi$ doesn't estimate $C^\\pi$: it estimates $C^\\pi + 1$, so it might make more sense notationally to just subtract one from the definition of $C_\\phi$.\n\nOverall: I think the idea in this paper is sensible, the derivations fairly clear, and it seems to help empirically. It does add a lot of \"moving parts\" to the already-complicated RL setup, though, and I'm not well-versed enough in the RL literature to have much of a sense of how convincing these experiments are; hopefully another reviewer is."}