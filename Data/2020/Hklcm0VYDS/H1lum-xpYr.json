{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proves that under certain conditions, SGD on average decreases the\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I don\u2019t think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\n\nLemma 1\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I don\u2019t see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\n\nPage 2: existence stationary state?\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\n\nEquation 4 and local approximation\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim \u201cEmpirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum\u201c\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\n\nAssumption on timescale separation\nThis section makes numerous claims that are not properly backed up.\n1) \u201cThis assumption is because there is a timescale separation between\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.\u201c\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\n\nTake away\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could \u201cpotentially improve generalization\u201d but this is not justified and no reference is cited.\n\n\u201cThis indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).\u201c\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\n\nExperiments\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.\n"}