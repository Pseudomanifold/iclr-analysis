{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a new view on policy gradient methods from the perspective of ranking. The end goal in policy learning is to achieve the right ranking of actions at a state (in the case when deterministic policies are optimal), and the paper proposes a method of doing this inspired from the work on learning to rank. They further argue that in the case with stochastic optimal policies, REINFORCE with softmax policies is rank wise optimal, which is not surprising, but at the same time interesting as well. The other main part of the paper is casting off-policy RL as supervised learning similar to work on self-imitation learning and reward weighted regression methods. This section is presented differently from the past analyses of self-imitation methods and requires the existence of UNOP, which seems like a strong assumption. They then instantiate the framework with GPI based exploration, and show that it achieves better performance than IQN and Rainbow on a subset of atari games.\n\nI am leaning towards a weak reject for this paper, although I am happy to revise my score based on the rebuttal. While the paper is interesting with regards to the ranking perspective, I am not fully convinced about the novelty of the reduction of off-policy learning to supervised learning. This appears already in past works (which the paper cites in the appendix) and the assumption of the existence of UNOP seems strong. I find using a Q-learning agent for exploration a bit complicated and perhaps unnecessary. Also, the paper currently lacks intuition about the effectiveness of their policy gradient approach on top of the data collected from an DQN-based agent. Since reward shaping is done at the trajectory level, why would we expect the supervised regression step to do better than the best trajectory in the data? Also, can the same Q-learning based method perform better if one controls for the number of gradient updates? How would the other methods such as self-imitation or reward weighted regression instead of their proposed supervised learning approach perform on top of data collected from a policy iteration based exploration policy? But the results seem quite promising. \n\nI would also appreciate some more clarity in terms of writing and presentation.  While this is done in the appendix, I would suggest making references with regards to the supervised learning section (Sec 5) more explicit and refining some of the text in the paper, although this is a minor point."}