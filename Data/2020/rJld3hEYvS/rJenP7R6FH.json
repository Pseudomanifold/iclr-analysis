{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work first establishes the connection of maximizing the lower bound of accumulated reward and supervised learning on near-optimal policies. Then it proposes a general framework for policy learning: during the exploration stage, the agent will collect near-optimal trajectories while in the exploitation stage, the agent will perform supervised learning on the collected data. Under this framework, the author argues that the ranking loss could outperform the state-of-the-art on the Atari benchmark.\n\nThe overall idea is intuitive yet interesting, and the empirical result is quite impressive. \n\nSome questions which I think the paper could discuss more:\n- In the paper, the near-optimal policy is defined with an absolute threshold, which could be task/environment-specific. I am wondering whether the author tried to set the `near-optimal policy` as a relative value (in the current replay buffer). Then the hyper-parameter could be shared.\n- I think some empirical comparisons of the gradient variance (i.e., for Corollary 2) will be more demonstrative, although I could imagine that the near-optimal trajectories will have smaller variance. \n- The choice of C could be tricky in the method as the whole algorithm is highly depending on it. How does the author choose C? If C is tuned for each environment, I am not sure whether it is a fair comparison with C51/Rainbow/IQN. \n- More algorithm training details on the experimental setting (like the hyper-parameters) are needed. \n\n======\nFrom eq (7) -> eq (8): Does the Taylor expansion near $\\lambda_{ji}=0$ make sense?\n"}