{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to reparameterize the policy using a form of ranking, and updating the policy gradient update accordingly. This on it's own isn't particularly useful, however the authors use this insight to attempt to convert the RL problem into a supervised learning problem. I think this is an interesting paper and approach, however some issues remain.\n\nHigh level:\n\nThe reward threshold appears to require a-priori knowledge of the correct scale to set it at, and the maximum possible reward, which is unfortunate. Also I'm not sure it would make sense in a sparse reward environment, or an environment with small reward leading up to a single large reward, since the trajectories that get small reward might be discarded erroneously. Maybe there should be an annealing schedule for this parameter? This deserves some discussion at least.\n\nThe experimental results are very good, however you are using off-policy replay data and therefore have the issue that using the same data many times can (often) improve performance for any off-policy algorithm. This needs to be controlled and investigated very closely, however I don't even see the batch size being used listed anywhere. The authors need to really demonstrate that the benefit is coming from their algorithm and *not* just the re-use of replay data that the baselines don't get to see as much of.\n\nMore minor:\n\nFirstly, there is very bad writing in places, .e.g, abstract: \"The state-of-the-art uses action value function to derive policy\". Things like this appear in many places.\n\nQ-learning, Q-values sometimes Q is upper case sometimes lower case.\n\nRelative action values are never defined, but if they are what I think they are then they are usually referred to as 'advantage values', and are generally given the notation of A(s,a).\n\nDoes the policy in (2) sum to 1?\n\nAssumption 1 is written very confusingly. I would state it as something like:\n\n\"Assumption 1. For a state s and action i the set of events {E^i_j}_{j \\neq i}, where E^i_j corresponds to action ai being ranked higher than action aj, are conditionally independent, given a MDP and a stationary policy.\"\n\nAssumption 1: What is is conditioned on?\n\nThm 1: the statement of the theorem has an equality, but the proof has a clear 'approximate equality', this is very misleading since the statement is not actually true!\n\nI don't follow the proof of corollary 5, especially the sentence explaining (65) -> (66), which doesn't parse. Again the proof is actually approximate but the statement is given using an equality, which is false.\n\nThe appendix is very messy and several parts of it just consist of series of equations with no supporting explanation.\n\nRefs:\nThe original DQN paper (Mnih et al) should be cited after you mention DQN.\n\nI was surprised not to see references to \nACER: https://arxiv.org/abs/1611.01224 and \nPGQ: https://arxiv.org/abs/1611.01626 when discussing the off-policy / RL + SL work.\n\nFurther, in your discussion about entropy regularized RL \"However, the discrepancy between\nthe entropy-regularized objective and original long-term reward exists due to the entropy term.\" - this has been corrected by a recent work: https://arxiv.org/abs/1807.09647. Probably worth mentioning it here too."}