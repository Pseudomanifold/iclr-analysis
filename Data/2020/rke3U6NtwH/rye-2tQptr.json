{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new hierarchical graph representation learning method for graph classification. It builds upon the diffpool method. Specifically, the authors propose the multiplex convolution and the multiplex pooling operation. The multiplex convolution learns multiple graph convolution operations (potentially with different parameters) and merges them using a neural network. Similarly, the multiplex pooling learns multiple assignment matrices utilizing diffpool and merges them using a neural network. The effectiveness of the proposed model is demonstrated with experiments in several standard benchmark datasets for graph classification. \n\nStrengths\n\nThe overall architecture of the proposed method can be regarded as an extension of diffpool method by using multiple convolution and pooling operations. An analysis on how the number of convolution and pooling operations can affect the performance of the model shows the effectiveness of the proposed multiplex convolution/pooling operations. The effectiveness of the proposed model is further verified by the experimental results on 5 standard benchmark datasets.\n\nWeakness \n\nThe novelty of the paper is limited. The major contribution of this paper is to utilize multiple convolution/pooling operations and merge them with neural networks. \n\nIt is not clear how issues mentioned in the motivational examples shown in Figure 1 can be solved by the proposed model. As discussed in Figure 1(a) and Figure 1(b), small graphs may prefer small embedding while large graphs may prefer large embedding. However, it is unclear how the proposed model achieves this goal as the combination of the embeddings from different convolution operations is through a neural network that is shared by all graphs. Similarly, for Figure 1(c) and Figure 1(d), it is not clear how the proposed model can focus more on one type of information than the other. It would be helpful if there are some empirical results to demonstrate this.\n\nRecommendations\n\nIt would be better if the authors can provide some analysis on time complexity of the proposed model. The major concern of the efficiency is the introducing of fully connected graph after each pooling as mentioned in Section 3.3. \n\nMinor comments\n\nIn section 3.3, \u201cis a GCN different\u201d -> \u201cis a GCN variant\u201d?\nIn Section 3.3, should there be a softmax function after Eq.(5)? \nIn section 4.2, \u201cin ter90o8ims\u201d -> \u201cin terms\u201d?\n\n\n"}