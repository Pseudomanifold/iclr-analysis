{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes an interesting new adversarial attack concern: any machine learning that use a linear classifier on an off-the-shelf feature extractor, suffers the risk that the feature extractor has adversarial examples that output arbitrary feature vectors. The paper proposes a concrete attack where a set of samples is crafted to activate each feature vector. \n \n\nPro:\n\nThe setup is interesting and seems novel. To my knowledge, attack on the features used in transfer learning has not been considered in the literature. Several adversarial attack papers consider attacking the network layer by layer, or attack an intermediate layer, but they are different from the proposed setup in this paper. \n\nThe proposed attack is simple but effective (though unsurprisingly). \n\nThe experiments show reasonable performance even beyond the motivating setup, for example, with fine-tuning or with non-linear classifiers on the feature space. \n\nCon:\n\nI think the range of the input feature (to the linear classifier) should be reported. A simple check on the range of the input feature should be a good defense. Therefore, the input feature should not be unreasonably large. \n\nI think the experiments need comparison with the naive baseline by simply trying random faces. It seems that without any attack, it should be possible to try random faces and find one that is incorrectly classified. Assuming the model it well calibrated, among the time it says it is 99% confident, it should still make a mistake with approximately 1% probability. In addition, deep networks are known to be not calibrated and over confident. \n\nI do not think the attack is task-agnostic, because the attacker still needs to query the final linear layer as a black box to find which feature unit to activate. This is a special hybrid setup, where the feature layer is white-box while the final layers are black box. I think task-agnostic is somewhat an oversell. \n\n\nOverall I believe this paper is okay. It explores a novel task with a somewhat interesting approach, with sufficient empirical support. \n\n"}