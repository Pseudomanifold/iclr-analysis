{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors proposed an attack scheme to any model that pretrained from a general model. The merit comes from that the attacker, by taking advantage of the vulnerability of softmax, has no access to examples from the target task to finetune the model.  \n\nPros:\n-\tThe setting where the authors focused on is much more practical, i.e., the attacker is blind to examples in a target task.  \n-\tThe blackdoor for attack in this work, namely the softmax layer, is novel and interesting to me, at least to my knowledge.\n-\tThe paper is well written and easy to follow.\n\nCons:\n-\tIn the experiments, the authors should still compare one or two black-box attacks which also use model outputs only, to demonstrate the effectiveness/efficiency of the proposed attack scheme. \n-\tConsidering the design of \\gamma and \\beta in Eqn. (1), it is expected to investigate the influence of both on the performance of the attack, as well as the vanilla version of Eqn. (1) by only considering MSE.\n-\tCurrently, it seems that the authors did not study the influence of different target datasets to finetune. It is highly expected that the same adversarial input crafted using Alg. 1 could fool all networks finetuned using different datasets. What if a wildly different dataset is used to finetune the pretrained model?"}