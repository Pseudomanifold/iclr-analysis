{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is very well-written and combines the state-of-the-art NLP model and the domain knowledge in retrosynthetic reaction predictions.  The authors propose pre-training models to help improve the model's generation to rare reactions. In addition, a discrete latent variable model is used in the model to encourage the model to produce a diverse set of alternative predictions.  The experiments in the paper also show the effectiveness of the two main contributions.\n\nThe main contribution of this paper is to apply the state-of-the-art Transformer model and other techniques in NLP to address the specific issues in retrosynthesis. Both the pre-training model and the mixture model are combining the specific domain knowledge to improve the generalization and diversity of retrosynthesis. There is not a huge algorithm novelty for the methods proposed in this paper, but they can well address the domain issues and improve the performance.\n\nMy only concern is that the baseline model compared in the paper is Schwaller's work. I am not pretty sure if this baseline achieves state-of-the-art performance. It would be very interesting to see more comparisons with other state-of-the-art \n work.\n\n\n"}