{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzed the principles for a successful transfer in the hard-parameter sharing multitask learning model. They analyzed three key factors of multi-task learning on linear model and relu linear model: model capacity (output dimension after common transformation), task covariance (similarity between tasks) and optimization strategy (influence of re-weighting algorithm), with theoretical guarantees. Finally they evaluated their assumptions on the state-of-the-art multi-task framework (e.g GLUE,CheXNet), showing the benefits of the proposed algorithm.\n\nMain comments:\n\nThis paper is highly interesting and strong. The author systematically analyzed the factors to ensure a good multi-task learning. The discovering is coherent with with previous works, and it also brings new theoretical insights (e.g. sufficient conditions to induce a positive transfer in Theorem 2). The proof is non-trivial and seems technically sound.\n\nMoreover, they validated their theoretical assumptions on the large scale and diverse datasets (e.g NLP tasks, medical tasks) with state-of-the-art baselines, which verified the correctness of the theory and indicated strong practical implications.\n\nMinor comments:\nThe main message of the paper is clear but some parts still confuse me:\n1. I suggest the author to merge the Figure 3 and Data generation (Page 4) part for a better presentation. e,g which \u201cdiff.covariance\u201d is task 3 or 4 ?  And why we use different rotation matrix Q_i ? \n\n2. In algorithm 1 (Page 5) , I suggest the author use a formal equation (like algorithm 2) instead of descriptive words.\n     -- Step 2, I have trouble in understading this step.\n     -- Step 3, how to jointly minimize R_1,\\dots, R_k, A_1, \\dots, A_k ? we use loss (3)  or other losses ?\n     -- I suggest that the author release the code for a better understanding.\n\n3. For theorem 2, can we find some \u201coptimal\u201d c to optimize the right part ? Since 6c + \\frac{1}{1-3c}\\frac{\\epsilon}{\\X_2\\theta_2} might be further optimized \n\n4. In section 3.3. (Figure 6)  of the real neural network, the model capacity is the dimension of Z or simply the dimension before last fc-layer ?\n\n5. Some parts in the appendix can be better illustrated:\n    (a) I am not clear how proposition 4 can derive proposition 1.\n    (b) Page 15, proving fact 8: last line \\frac{1}{k^4}sin(a^{prime},b^{prime}) should be \\frac{1}{k^4}sin^{2}(a^{prime},b^{prime}). \n\n\nOverall I think it is a good work with interesting discoverings for the multi-task learning. I think it will potentially inspire the community to have more thoughts about the transfer learning. \n"}