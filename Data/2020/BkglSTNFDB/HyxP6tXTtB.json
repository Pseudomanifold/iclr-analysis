{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considered a Q-learning algorithm with UCB exploration policy for infinite-horizon MDP, and derived the sample complexity of exploration bound. The bound was shown to improve the existing results and matched the lower bound up to some log factors. The problem considered is interesting and challenging. However, I have a few major concerns.\n\n1. Assumptions: The Condition 2 is on the term $X_t^{(i)}$ which is the i-th largest item of the gap $\\Delta_t$ (the difference between the value function and the Q function fo the optimal policy). How to verify this condition in practice? Do you need this condition for all $t$? Does this condition depend on the choice of length $R$? The conditions are listed without any discussions.  \n\n2. Algorithm: The Algorithm depends on the choice of the parameter $R$. How to choose $R$ in practice?\n\n3. Writing: This paper is not well written. There are many typos and grammar errors. In addition, the key components Sections 3.3 and 3.4 are very hard to follow. For instance, in Section 3.3, the authors first introduced Condition 1 and then Condition 2, and then claimed that Condition 2 implied Condition 1. Similarly, in Section 3.4, Lemma 1 was introduced before Lemma 2. Then the authors claimed that Lemma 2 implies Lemma 1. If would be easier to follow if the authors only introduced the latter result, and then discuss the former result as a remark.  \n\n\n"}