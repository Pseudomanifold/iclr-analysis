{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a new algorithm for spike-sorting. It is implemented by a deep autoencoder with biophysically motivated loss functions. The encoder is the main module which conducts the spike-sorting task while the decoder is only used for training the model in an end-to-end fashion. \n\nThis paper should be rejected due to the following arguments:\t\n- The paper lacks a section on literature survey, to let the reader know how/where the proposed method fills the gap in the current state-of-the-art. They do compare their results with the KiloSort (Pachitariu et al., 2016) algorithms, however, no discussion is provided on how it works and why their method outperforms it. \n- It is unclear why the reconstruction loss is chosen to be an L4 norm as opposed to L2.\n- The authors claim that the parsimony loss as defined in Eq. (7) forces \u201cthe network to be parsimonious with respect to the set of MUAP proposals it uses to explain a given sEMG signal.\u201d My understanding, however, is that the only functionality of the loss defined in Eq. (7) is to enforce temporal smoothness. More elaborate explanation is needed to support the authors claim.\n- I could not understand the functionality of the uniqueness loss. Specifically, why should \u201cthe joint occurrence of the temporal similarity between MU spike trains and the spatial similarity between their MUAP waveforms\u201d be penalized? Isn\u2019t that the case that same stimuli should result in similar response? It is unclear what this has to do with forcing to explain different phenomena.\n\nThings to improve the paper that did not impact the score:\n- The method (and the paper) is named deep spike \u201cdecoder\u201d (DSD) while in fact the \u201cencoder\u201d part of the learned deep autoencoder actually conducts the spike-sorting task. This could be confusing!\n- Page 2, Sec. 3.1, line 2: Should use \\times in inline equations in Latex for the multiplication symbol, not character x. Fix everywhere in the text.\n- Page 6, Par. -2, line -2: The word \u201creplicate\u201d is repeated.\n- Non-legible plots axes.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}