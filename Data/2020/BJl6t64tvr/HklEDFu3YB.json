{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper does an empirical study for the problem of \"whether acceleration harms the empirical learning performance\". Based on the study, the authors propose some empirical suggestions to fix the generalization gap for acceleration methods.\n\nOverall, this paper is a lack of insights and looks more like an experimental report. The significance of the paper is not enough. Please see questions below\n\nQ1. \"We re-examine this belief both theoretically and experimentally\". Where is the theoretical part? \n- I wish the authors can offer some useful Lemmas or Theorems, but Section 5 only offers some discussions. \n\nQ2. Since the paper wants to promote some empirical observations, please add STD to all curves, like,  Wilson et al. (2017). Some curves are too close (e.g., those Figure 6), it is hard to judge their statistic significance.\n\nQ3. \"we synthesize a user\u2019s guide to adaptive optimizers\". \n- After reading the paper several times, I am still kind of confused. What is the \"user\u2019s guide\"? Could the authors make some short summary?\n- Since the paper mainly does empirical study, this question is important.\n\nQ4. Why not SGD in Figure 3?\n\nQ5. \"As demonstrated, learning rate schedule is a highly important hyperparameter and requires tuning for each task\". \n- So, how to tune the learning rate?\n- Is the learning rate more important than the choice of the optimizer? If so, the problem with the acceleration this paper investigated has little meaning.\n\nQ6. Is it better to carry on hyperparameter optimization on factors that can contribute to the final performance of each algorithm? \n- In this way, we can see the extreme performance of each optimizer, the performance comparison condition on best possible hyperparameter can make more sense than what has been done in this paper.\n- It will be good if authors can check this paper \"Neural Optimizer Search with Reinforcement Learning\" and then re-design their methodology."}