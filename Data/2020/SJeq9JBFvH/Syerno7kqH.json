{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces  a novel DPS(Deep Probabilistic Subsampling) framework for the task-adaptive  subsampling case, which attempts to resolve the issue of end-to-end optimization of an optimal subset of signal with jointly learning a sub-Nyquist sampling scheme and a predictive model for downstream tasks. The parameterization is used to simplify the subsampling distribution and ensure an expressive yet tractable distribution.  The new approach contribution is applied to  both reconstruction and classification tasks and demonstrated with a suite of experiments in a toy dataset, MINIST, and COFAR10.\n\n\nOverall, the paper requires significant improvement. \n\n1. The approach is not well justified either by theory or practice. There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); Pl\u00c2\u0161otz & Roth (2018) ).\n\n2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)\n\n   The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution. \n\n3. The paper is not nicely written or rather easy to follow. The model is not well motivated and the optimization algorithm is also not well described.\n\n4. A theoretical analysis of the convergence of the optimization algorithm could be needed.\n\n5. The paper is imprecise and unpolished and the presentation needs improvement.\n\n**There are so many missing details or questions to answer**\n\n1. What is the Gumbel-max trick? \n2. How to tune the parameters discussed in training details in the experiments?\n3. Why to use experience replay for the linear experiments?\n4. Are there evaluations on the utility of proposed compared to existing approaches?\n5. Does the proposed approach work in real-world problems?\n6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm.\n"}