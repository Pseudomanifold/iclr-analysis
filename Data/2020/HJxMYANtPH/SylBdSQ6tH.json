{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper contributes to the understanding of neural networks and provides a new clustering technique:\n  1) The paper introduces the interesting notion of local elasticity which considers the relative variation in output values for two different inputs before and after an SGD-style update;\n  2) The derived similarity metric between input samples (obtained by as SGD unfolds) can be used for clustering and is amenable to a kernelized formulation;\n  3) Empirical measurements on visual classification tasks show that the new similarity metric can offer a better clustering performance than PCA + K-means.\n\nI found the paper very interesting but the writing appeared somewhat unclear at times. I believe that some rewriting is needed for the authors to argue that the newly introduced elasticity metric provides a significantly new understanding of neural networks. In particular, I did not find the argumentation around explaining generalization to be very convincing or clear.\n\nThe kernelized formulation of the elasticity metric seems compelling and I found that turning the insights developed by the theoretical section of the paper into an actionable algorithm for clustering was a nice contribution.\n\nUnfortunately, the empirical results did not really convince me that the resulting clustering algorithm really improves on the SOTA in clustering as only relatively weak baselines were considered.\n\nI believe that considering more solid baselines for the clustering experiments would help."}