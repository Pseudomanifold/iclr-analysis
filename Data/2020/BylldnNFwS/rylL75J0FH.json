{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to characterize the decision boundary of deep neural networks. The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. They present and prove a theorem connecting the decision boundary of deep neural networks to tropical hyperspheres.  Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation.  For the former, they show that using the introduced decision boundary characterization using the tropical geometry, one can dimish the number of parameters in a model with insignificant loss in performance. For the latter, they introduce a new method for adversarial examples generation by altering the decision boundary. \n\nOverall, the technical and theoretical contribution of the paper regarding the relation of decision boundary to the tropical geometry is significant and could be useful for further investigation of the decision boundary of deep neural networks. \nHowever, there are several caveats in this paper that need further clarification.\n\n1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical geometry perspective, how we can obtain the complexity of the decision boundary of a deep neural network?\n\n2)  The second part of Theorem 2 should be explained straightforwardly and clearly as it plays an important role in the subsequent results and applications. \n\n3) In the tropical network pruning section, the authors mention that \"since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers\". However, convolutional layers of investigated architectures (i.e., VGG16 and AlexNet) have a large number of parameters as well. So, wouldn't it be necessary to investigate pruning convolutional layers as well if diminishing the number of parameters is the main purpose? Moreover, the size of the last fully connected layer is simply determined by the preceding convolutional architecture, which in fact extracts the salient features (at least for well-known image datasets), while the last fully connected layer just flattens the extracted features to be fed into the subsequent classifier. Hence, it would be interesting and, in my opinion, necessary to investigate pruning convolutional layers as well. \nFurthermore, the authors mention that a pruned subnetwork has a similar decision boundary to the original network. What does it mean exactly by \"similar\"? What are the measures capturing this similarity, if any? Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. I am skeptical if this the case and it needs to be validated concretely. \n\n4) In adversarial examples generation, typically for a pre-trained deep neural network model one is interested in generating examples that are misclassified by the model while they resemble real instances. In this setting, we keep the model and thus its decision boundary intact. In this paper, nevertheless, aiming at generating adversarial examples, the decision boundary and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the model's decisions for original real samples might change as well. Therefore, it is not clear to the reviewer how the introduced method is comparable to the well-established adversarial example generation setting.  \n\n5) Two previous papers investigated the decision boundary of the deep neural networks in the presence of adversarial examples [3][4]. Please discuss how the introduced method in this paper is placed among these methods. \n\nMinor comments:\nIn the abstract, \"We utilize this geometric characterization to shed light and new perspective on three tasks\" --> unclear, needs to be revised.\n\nProposition 1, \"the zonotope formed be the line segments\" --> \"the zonotope formed by the line segments\"\n\nPage 7. Results. \"For VGG16, we perform similarly on both SVHN and CIFAR10 CIFAR100.\" --> \"For VGG16, we perform similarly on both SVHN and CIFAR10.\"\n\n\n\n\nReferences:\n[1] @article{li2018decision,\n  title={On the decision boundary of deep neural networks},\n  author={Li, Yu and Richtarik, Peter and Ding, Lizhong and Gao, Xin},\n  journal={arXiv preprint arXiv:1808.05385},\n  year={2018}\n}\n[2] @article{beise2018decision,\n  title={On decision regions of narrow deep neural networks},\n  author={Beise, Hans-Peter and Da Cruz, Steve Dias and Schr{\\\"o}der, Udo},\n  journal={arXiv preprint arXiv:1807.01194},\n  year={2018}\n}\n\n[3] @article{khoury2018geometry,\n  title={On the geometry of adversarial examples},\n  author={Khoury, Marc and Hadfield-Menell, Dylan},\n  journal={arXiv preprint arXiv:1811.00525},\n  year={2018}\n}\n\n[4] @inproceedings{\nhe2018decision,\ntitle={Decision Boundary Analysis of Adversarial Examples},\nauthor={Warren He and Bo Li and Dawn Song},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpiPMbA-},\n}\n\n"}