{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors tackle the problem of compressing and accelerating the popular BERT model for representing text, and propose a layerwise feature distillation approach for the task (progressive knowledge transfer), and investigate several transfer objectives (self-attention transfer, std. distillation). In contrast with most other current work, their focus is on producing a task-agnostic model, that is, a compressed pre-trained model trained for finetuning on simple downstream tasks via the self-supervised BERT training objectives (masked language modeling, MLM, and next sentence prediction). Thier model, MobileBERT, is about 4x smaller, faster than the base BERT model, and outperforms it.\n\nStrengths:\n-Clear presentation. Good results. Most elements ablated. Important problem to NLP community.\n\nLimitations:\n-The technique advertised in the title (progressive knowledge transfer) is apparently not ablated! The feature distillation/matching is, and plays an important role, but PKT specifically apparently is not, and it is not at clear that it should be neccessary...\n-DistilBERT was just recently published/released, and is mentioned in the appendix, but is not compared to. It is a simpler model/approach, and produces similar results and speed/compression gains. \n-Quantization and pruning are left to future work.\n-The model is apparently not being released. \n-On the lower side wrt ML novelty, standard techniques.\n\nOverall:\n\nAn okay paper. Comparisons with DistillBERT and model release would make the contribution more significant. Lower novelty, advantage of progressive knowledge transfer not clear. Current evaluation is weak reject.\n"}