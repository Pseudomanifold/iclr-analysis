{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents an interesting knowledge distillation method MobileBERT, which is a specially designed method to compress and accelerate large scale pre-trained language model. \n\nThe main contributions include: the progressive knowledge transfer with considering feature map transfer and attention transfer, and the new architecture design with introducing the Bottleneck and InversedBottleneck-modules into the student and teacher networks respectively. \n\nThis KD method is proposed based the following three constraints, the teacher BERT-large and student MobileBERT should have the same feature map size, the number of layers, and the number of attention heads. So the small student model should have the same number of layers as its teacher model. The reduction of model size mainly comes from the smaller hidden size.\n\nThe authors evaluate their approach on GLUE and SQuAD datasets and compare it to other state-of-the-art models.\n\nThe paper is well-written and organized, I have several concerns:\n\n* Progressive knowledge transfer problem: the progressive knowledge transfer process as shown in the left panel of Fig.2 is one of the main contributions of this work, while I cannot find any experimental analysis on its importance in comparison with jointly training the losses from all the layers.\n\n* Number of layers problem: The compression of MobileBERT mainly comes from the reduction of hidden size and it is still a 24-layer deep model. However, as stated in the blog of describing the DistilBERT, \u201cthe number of layers was the determining factor for the inference time, more than the hidden size\u201d, especially when executing on GPUs, the proposed MobileBERT has the limitations of having the same number of layers as its teacher, is it possible to reduce the number of layers of MobileBERT? \n\n* Baseline: In the Table 3, it is better to add the results from teacher network IB-BERT_large, the student should be directly compared to its teacher, but not the BERT-base. \n\n* In the Fig.1(b), in contrast with the standard transformer layer (as shown in the Fig. 1 (a)), a shortcut connection is introduced from the input embedding layer to the top Add&Norm layer, are there some insights behind this architecture design? Or is this a widely adopted design of bottleneck module?\n\n* In the equation (1), the layer normalization is added to H_{t,l}^{tr} to stabilize the training loss, and in the Fig.1(b)(c) the feature map transfer is illustrated as the outputs from a Add&Norm layer, does the H_{t,1}^{tr} denote the output from the Add&Norm layer? or does this mean we need to do layer normalization again on the outputs from a normalization layer? \n\n* In the section 4.1, to align the feature maps from teacher and student, we may only use a linear transformation to reduce the hidden size of teacher to the student model, without introducing the B or IB module, is this a reasonable method? Do you once try this method?\n\n* In the section 4.3, which operational optimization is more effective, removing layer normalization or using relu activation? And only using the relu activation will also hurt the performances? "}