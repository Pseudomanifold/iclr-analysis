{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper compresses by 4x the # parameters and reduces latency of BERT by 4x using a combination of the following ideas:\n1. training a BERT like teacher model with inverted bottlenecks and a MobileBERT student model with bottlenecks in each layer so that within the MobileBERT model we reduce the number of parameters and computations by reducing width, but at the beginniong and end of each layer the widths and attention is identical. Thus the student can be trained one layer at a time to mimic the teacher model.\n2. Bottom to Top progressive feature map transfer and attention transfer\n3. Further fine tuning of the Mobile BERT on the training corpus\n4. increasing the number of feedforward layers following each multi-headed attention  layer\n5. replacing the layernorm with an element wise normalization, and the gelu with Relu activation function. \n\nThrough the above relative simple steps the authors pre-train a generic model which can then be fine tuned just like the original BERT on any task with a limited amount of training data. \n\nEmpirically their accuracy on GLUE and SQuAD1.1 benchmarks is similar to the BERT_{base} model though they use 4x fewer parameters and have 4x less latency. They are also significantly better than Sun et al (2019) whose approach also compresses BERT. \n\nOverall I think the paper is well motivated and well explained. Its experimental analysis is sufficient. I like the paper quite a bit in terms of its likely impact on practitioners (eg if distributed as part of the transformers package at https://github.com/huggingface/transformers). However I am a bit concerned about the relatively limited novelty. All the key ideas introduced here can be directly traced back, and I am struggling to point to some unique novel contribution here other than combining them together. Further they have not compared against DistilBERT which is also at least as good in terms of results. Finally there is no ablation study for the impact of the inverted bottleneck. For all these reasons the paper seems to not be fully ready for publication yet.   "}