{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis paper proposes MobileBERT, a compressed version of BERT. MobileBERT achieves comparable performance as BERT_BASE while being 4X smaller, which is accomplished through careful engineering design of the network structures and a bottom-up progressive knowledge transfer approach.\n\nI recommend for acceptance, as the proposed MobileBERT successfully reduces the computation cost and model size of BERT, without sacrificing the performance (when compared to BERT_BASE). The paper provides an efficient model architecture for BERT compression, and also an approach to train the student network layer by layer in a bottom-up way. Compared to previous work on model compression of BERT, the proposed MobileBERT is more generic -- once pre-trained, it can be simply fine-tuned on down-stream tasks like the original BERT, without requiring a teacher model. It enables the efficient application of BERT to a wide range of NLP tasks.\n\nHowever, I would also like to note that the main contribution of this paper lies in its engineering efforts -- accomplishing a task-independent compressed version of BERT with good performance. On the other hand, the proposed knowledge transfer (distillation) approach is rather unsurprising."}