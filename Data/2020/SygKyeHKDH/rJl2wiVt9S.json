{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work, R2D3 (Recurrent Replay Distributed DQN from Demonstration), which combines R2D2 [1] with imitation learning (IL), is proposed. Similar to the existing works on \u201creinforcement learning (RL) with demonstration\u201d such as DQfD, DDPGfD, policy optimization with demonstration (POfD) [2], hard exploration conditions (sparse reward, partial observability, high variance in initial states) are assumed, which is difficult to achieve good performance with RL without demonstration in general. Eight tasks in such conditions were devised and used to test the performance of R2D3.\n\nI like the fact that the authors of this work have chosen quite challenging scenarios, but I think the novelty of this submission is a bit weak to be accepted to the conference. I believe \u201cRL with demonstration\u201d becomes meaningful when it beats both RL and IL in some reasonable setting. For example, POfD [2] assumes sparse-reward tasks with *imperfect* demonstrations, which is difficult to achieve good performance by using RL or IL. From such a perspective, I have the following concerns:\n\n- Imitation learning baselines: There has been recent advancement in imitation learning. In the submission, it was mentioned that \u201cGAIL has never been successfully applied to complex partially observable environments that require memory\u201d, but there\u2019s [3] that successfully uses GAIL in such a setting. Also, off-policy imitation learning such as DAC [4] is shown to be highly sample-efficient compared to GAIL in MuJoCo domain. However, the submission only considers behavioral cloning (BC) (which shows poor performance at unseen states due to the covariate shift problem) as a baseline among imitation learning method\n\n- Reinforcement learning baselines: The submission adopted R2D2 as an RL baseline, and it seems to me that the R2D2 agent starts from random initialization. For a fair comparison, however, I believe R2D2 with BC (or Batch RL) initialization should be considered.\n\nIn addition to the above concerns, it seems to me that most of the features in R2D3 simply combines those in either DQfD or R2D2, and I couldn\u2019t find out its own algorithmic novelty except \u201cdemo ratio\u201d parameter. \n\nI\u2019ll increase my score if I made wrong comments or misunderstood the contribution.\n\nReferences\n[1] Kapturowski, Ostrovski, Quan, Munos. and Dabney, \u201cRecurrent experience replay in distributed reinforcement learning,\u201d ICLR 2019.\n[2] Kang, Jie, Feng, \u201cPolicy optimization with demonstrations,\u201d ICML 2018\n[3] Gangwani, Lehman, Liu, Peng, \u201cLearning Belief Representations for Imitation Learning in POMDPs,\u201d UAI 2019\n[4] Kostrikov, Agrawal, Dwibedi, Levine, Jonathan, Tompson, \u201cDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,\u201d ICLR 2019"}