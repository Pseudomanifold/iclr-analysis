{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an automatic tuning scheme for learning rate while training neural networks. Since learning rate is the most sensitive and important hyperparameter during training, an automatic versatile method for choosing learning rate for various workloads would be of great importance. \n\nThe proposed method works for various optimizers (SGD/Momentum/Adam shown in the paper) and shown to perform as good or better than typical baseline learning rate schedule practitioners use to obtain competitive performance. There are a good number of empirical checks on both image and language tasks.\n\nWhile I\u2019m not fully familiar with literature on automatic learning rates, the authors claim that this is the first auto learning rate tuning scheme to achieve SOTA performance. \n\nTwo main components of the scheme are based on observation that initial high learning rate seemingly without making any improvements is essential for obtaining good final performance. Therefore initially there is an `explore\u2019 phase and then in exploit phase quadratic local approximation around high learning rate update is used. Novelty in the exploit phase is expansion respect to perturbation around some potentially large step size instead of assuming step size is small. \n\nOne main concern of the proposed method is the choice of seed learning rate and duration of explore phase is still somewhat arbitrary and requires tuning for specific model/dataset. For example, in CIFAR/ResNet there are experiments showing that duration of `explore\u2019 phase is important and choose 50 epochs. In the case of BERT for SQUAD fine tuning 2500 explore steps(half epoch) are chosen. It seems choice needs to be tuned to get good performance and with this the proposed scheme is semi-auto tuning at best. Similar points for seed learning rate could be made. \n\nIn order for the proposed method to be fully successful, either authors need to show insensitivity to general choices of seed learning rate and explore phase, or an automatic method to choose good values. Without that  I do not see significant improvement beyond tuning learning rate schedule via linear or cosine decay schemes (which is also known to perform comparatively to multiple drop schemes). \n\nIn that spirit I think an interesting baseline to compare is tuning learning rate schedule. \n\nThe proposed explore-exploit method has great potential in terms of generality and strong performance on various tasks. I would happily raise my score if the main concern is addressed, however at this point I slightly lean toward rejection. "}