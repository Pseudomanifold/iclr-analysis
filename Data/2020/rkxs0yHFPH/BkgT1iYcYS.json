{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a first framework of large-scale spiking neural network that exploits the the sparsity of the gradient during backpropagation, to save training energy. \nLater, it provides detailed analysis to show the equivalence of accumulated response and the corresponding integer activation ANN.\n\nThe paper is clearly written. The forward and backward process with the spike activation and error activation function respectively to save energy is clearly demonstrated. The response equivalence of the proposed architecture and integer ANNs provides theoretical gurantee for the good performance in training accuracy.  \n\nMy only concern is the lack of empirical support for the energy saving of the proposal. In order to show the effectiveness of the proposal, the authors should also provide time consumptions of the SNN and normal ANN. A mere comparison on sparsity doesn't really show the advantage of the proposal, since there is some computational overhead. For a system-level improvement, it's not sufficient to show the epoch-operation relation.\nIf the authors could provide wall clock time comparisons, I will consider raising my score. "}