{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "First of all, I should acknowledge that this is a last-minute emergency review. Second, I should also acknowledge that I haven\u2019t actively followed the specific research topic handled in this paper, although I work quite a lot in reinforcement learning and dialogue systems. The last paper I read on the topic was Branavan et al\u2019s \u201cLearning to win by reading manuals in monte-carlo framework\u201d (ACL 2011).\n\nThis paper is about language-conditioned reinforcement learning, where the agent is required to perform machine reading of documents to learn policies to solve a task in new environments. The contribution of this paper is seemingly two fold: first, a new benchmark test suite is proposed (named \u201cread to fight monsters\u201d funny acronym), and second, an improved neural model (named \u201ctxt2pi\u201d) which extends FiLM proposed for related tasks. Various experiments on txt2pi are conducted to show how the model performs against baselines, as well as it behaves under different learning settings.\n\nOverall, this paper makes a good contribution to the research on deep models for language-conditioned reinforcement learning, but I see a number of things I miss in this paper. \n\nThe first one is about the test suite RTFM. The authors claim that it poses a new challenge that the agent has to understand the language specification of the goal, the environment dynamics, and the observations *all together* (compared to prior work that misses one or two), I am not confident that it is not challenging enough. \n\n- The main concern I have is that they are all machine-generated, and soon or later, this kind of test suite could be beaten-to-death. A more ideal test suite would be the texts collected by humans, aiming for more diversity, ambiguity, and sometimes incorrectness. \n- The RTFM would combinatorily generate a large number of environments, but still, they all look pretty simple. It would be interesting to have \u201cpartially specified\u201d documents, such as \u201clightning monsters are known to be weak against grandmaster\u2019s or soldier\u2019s weapons, but it is not specifically known\u201d. It would be interesting to have partially observable environments, where the agent has to do some information gathering in the environment to behave optimally.\n\nThe second one is about txt2pi model proposed in the paper.\n- txt2pi adequately extends FiLM to handle multiple sources of information, but what is the lesson being learned? \nI find the discussions in page 15 very enlightening. Could you move the content to the main text and defer the curriculum learning details to the appendix?\n\nIn summary, this paper proposes an interesting test suite for language-conditioned reinforcement learning, but I wish it is more realistic in the language data and more complex in the RL tasks. \n"}