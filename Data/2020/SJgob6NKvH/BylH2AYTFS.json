{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a new environment, Read to Fight Monsters (RTFM), and correspondingly a new algorithm, txt2\\pi, for solving this problem. The RTFM requires the agent to read a description of the rules (x beats y, etc) and a description of the goal (to eliminate y), and perform the task correctly to win the game. The txt2\\pi algorithm uses the newly proposed FiLM^2 module and consists of integrating the visual input (grid world configuration) and the text input (descriptions) to learn a policy and baseline (actor-critic). \n\nPros\n- The presentation is relatively clear.\n- A new environment that can be impactful for the field.\n\nCons\n- Certain design choices are not discussed properly.\n- The experimental evidence is relatively weak.\n\n(1) For RTFM, a complete list of possible elements would be helpful, at least in the supplementary. For example, the possible monsters/elements/weapons, etc.\n\n(2) Some proposals lack motivation or explanation. \n(2.1) How is Eq.(8) a summary?\n(2.2) Why the attention in (18) should base on the vis-doc embedding instead of the goal embedding, or goal-conditioned doc embedding?\n(2.3) It would be helpful to provide an example of the inventory description.\n(2.4) What is the difference between the Goal and Goal-doc in Fig.3?\n\n(3) Experiment\n(3.1) \"no group\", \"no dyna\" and \"no nl\" seem to be suggesting a simpler environment. How is no natural language templated descriptions (no nl) considered as an easier scenario than with a template? The description would be harder to parse or understand without structure (template).\n(3.2) It is mentioned several times that the models are \"trained on one set of dynamics and evaluated on another set of dynamics\". Specifically, what are the differences?\n(3.3) For the curriculum training (Table 2), adding dyna does not hurt the performance (from 84 to 85). Further explanation would be helpful.\n(3.4) How are the baselines and alternative methods perform in the full environment (+group etc.)?\n(3.5) The texts on Fig.5 are not evenly spaced, which makes them harder to align with the attention (also some letters have dots above them). Does this figure correspond to (15)? In Fig.5b, a0 focuses on \"gleaming\" and a1 focuses on \"lighting\", which are not present in the entities (compared to the caption). Something is wrong or missing here.\n(3.6) Eventually, the txt2\\pi algorithm only achieves modest improvement over random agents (65% versus 50%), which is not very impressive."}