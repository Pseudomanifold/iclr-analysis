{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Review ICLR20, Geometry-Aware Visual Predictive Models of Intuitive Physics\n\nThis review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper.\n\nI apologize in advance for being reviewer 2.\n\n## Overall\n\n\n**Summary**\n\nThe article introduces an algorithm that given multiple cameras spectating a robot, can learn a joint 3D representation of the scene by embedding all individual 2.5D RGB-D images into a consistent 3D feature map over time and then using this to plan how actions would affect these entities.\n\n\n**Overall Opinion**\n\nI'm not entirely convinced about the novelty and practicality of this approach:\n- You need ground truth 3D positions and orientations to train this, as stated in your loss (Eq.1), right? So how is this useful? For any real robot, you hardly have access to that (with the robot itself occluding the views if you're trying to use a 3D tracking system). I could easily be swayed on this by seeing some real-robot experiments, but I understand how difficult and expensive they are to come by.\n- I understand that this is an extension of [Tung et al. (2018)][1] to include a forward model for planning, but I think you're vastly overselling the idea. In your list of contributions, you have 4 bullet points. (1) is basically Tung, so not novel, (2) is novel and at the heart of your paper, (3) is false. You just mention \"2-object scenes\" at some point and that's that. (4) is only overselling slightly - you're not showing ablations (plural), but 1 single ablation of your model which is \"no RGB, just depth\". And save for a single row in Table 3, there is no perceptible difference.\n- The method is underspecified. You included Fig.1, which is very similar to Fig.1 from [Cheng et al., 2018)][2] as well as Fig.3 from [Tung et al., 2018][1]. But you fail to provide enough equations to specify how action-tiling + prediction and unrolling work. Please give us some pseudo-code at least.\n- You mention accumulating error multiple times, but it'd be nice to see the data for that: in tables 1-3, add another 2 rows each for obj pos/orientation after 5 steps or something along those lines. According to yourself, this is where your model should shine.\n- (See section \"Intro\" below): please provide PlaNet (or [SLAC][4] if you prefer) or another contemporary method's baseline results as opposed to your 2D multiview.\n\n[1]: http://openaccess.thecvf.com/content_CVPR_2019/papers/Tung_Learning_Spatial_Common_Sense_With_Geometry-Aware_Recurrent_Networks_CVPR_2019_paper.pdf\n[2]: https://papers.nips.cc/paper/7755-geometry-aware-recurrent-neural-networks-for-active-visual-recognition.pdf\n[4]: https://arxiv.org/pdf/1907.00953.pdf\n\nOther than those general issues, here are some minor...\n\n## Specific comments and questions\n\n### Abstract\n\n- weird phrasing: \"3D scene appearance\"?\n\n### Intro\n\n- citation needed: \"Indeed, we, humans, excel in manipulation despite that less than 1% of us know what inertia is\" (also I _think_ you can remove that second and third comma, otherwise there's a weird pause).\n- The whole section about Newtonian physics can be cut. The idea is clear without that paragraph (from \"Though Newtonian physics also describes ...\" to \"... particles and their displacements\")\n- You've failed to cite the very related work of [Hafner et al.,2018][3] (better known as \"PlaNet\"). And while we're at it - they released their full code, so it'd be nice if you could run it as a baseline on your simulated environment and provide the values in Tables 1-3.\n- The intro needs several edits - there is too much literature that then gets repeated in the related works section and there is a whole paragraph (starting with \"We propose learning models...\") that would be a better fit as overview in the Method section (instead, here a 3 sentence summary would be enough - how is this model extending Tung/Cheng?).\n- The entire paragraph starting with \"ii) Methods that predict the future in a hand-designed 3D space\" is not relevant to your work and can be cut.\n\n[3]: https://arxiv.org/pdf/1811.04551.pdf\n\n### Related Work\n\nall good\n\n### Method\n\n- Figure 1: The whole thing is not entirely clear - crucial details are missing, like how the action tiling and rollouts actually work. And next to \"(B)\" there is this illustration for the 2D to 3D unprojection and after reading Tung and Cheng and this work, I understand the method but I still don't understand what this illustration is supposed to mean.\n- You should mention clearly that you need ground truth positions/orientations of all objects for this to work. That's a huge thing to ask.\n- \"constancy\" -> \"consistency\"?\n- \"map,fed\" -> \"map, fed\"\n- You mention GRNNs being inspired by SLAM - so do they use Levenberg-Marquardt optimization or Gauss-Newton? Or is it just the general concept that you mean of integrating multiple views into one?\n- \"grid location holds an 1-dimensional feature vector F, as we show in Figure 1 right.\" -> where is this shown? Also \"a 1-dimensional\", not \"an ...\".\n- Is \"DRAW\" just a discrete insert operation?\n\n\n### Experiments\n\n- You mention that all your trajectories are 5 steps. Bullet Physics has issues running at lower than 50Hz, so 5 steps = max. 0.1s, right? How much movement can be contained in 0.1s, assuming some realistic velocity constraints on the robot arm?\n- \"With this setting we show...\" -> we _intend_ to show, also comma after \"setting\".\n- 4.1 first paragraph discussion doesn't line up with Table 1 - you describe XYZ performing on par with your method, but it doesn't.\n- Major issue: Why is XYZ not the upper bound? Your model is trying to infer the 3D composition of the scene but applying the same rollout mechanic to the ground truth 3D positions/orientations should result in upper bound performance, not worse performance.\n- Why do you care about the Kuka's different end effectors? Wouldn't it be more interesting to add more objects instead or randomize the shape or color of objects, as you promised in contribution list item 3?\n- \"for different tailored to different object shapes\" -> \"tailored to different object shapes\"\n- In 4.3 you need to specify how you're sampling the start/end positions? Like uniformly in the whole .6m x .6m space, both start and end? And how's the camera randomized? And you're testing all in-distribution, not out-of-distribution?\n\n### Conclusion\n\n- The big bold statement \"state representations that themselves obey physics\" is not properly justified? Why did say that? Not shaking under camera movement is not \"obeying physics\". And how is the camera shaking exactly?\n\n### Appendix\n\nAll good."}