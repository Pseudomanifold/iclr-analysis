{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a model to extract 3D object-centric representations of scenes from multiple RGB(-D) scene observations. These scene representations can be used with an action-conditional forward model to predict the state of the future state scene. The full model is comprised of multiple modules (each individual module is a well-known differentiable module) which can be tuned end-to-end by backpropagating the errors from the forward model. The authors emphasize that this is the first model that computes object centric representations in 3D and reasons about scene dynamics in 3D, providing advantages over previous systems that reasoned in 2D spaces or didn't have an object-centric representation. The authors then test the model to perform single or multi step forward prediction and for model-based control, obtaining an advantage over models that use representations of only object position and rotation or 2D object-centric models.\n\nOverall the paper proposes a highly engineered system that seems i) highly specific to a particular setting/task and ii) very difficult to reproduce.\n\nThe model works by inferring 3D object representations from RGB(D) images through neural networks that predict representations which are then manipulated through geometric operations to rotate/transform the scene. The authors reason that this does not require access to ground-truth (GT) 3D information as other previous models do, and can be trained end-to-end with a forward prediction task. While this is true, it also assumes that we can easily infer 3D object centric representations from scene views and that we can perfectly infer the future scene representation through geometrical transforms such as those performed with a 3D Spatial Transformer. This might be true in synthethic environments with 1 or 2 objects, but it seems quite impossible to infer and manipulate these representations from real world scenes with tens of objects, with extreme variations of relative locations, shapes, sizes, materials across objects. Small errors in inferring the scene representation could lead to large prediction errors, or interactions in complex scenes might not be properly captured by spatial transformers. Therefore, while the prior knowledge added to the system is more general than models that use GT information, it seems that the model could only be trained when it is easy to infer the scene state.\n\nIn fact, the experiments are conducted on a synthetic environment with at most two objects, which perfectly fits the inductive bias of the model. It is not surprising that this model would surpass the baselines if properly optimized. I would like to see experiments on real data or way more complicated synthetic scenes with tens of objects to be convinced that this model could be used in a real world scenario. My guess is that such a complex model with many different modules can only be properly trained end-to-end when each of the individual tasks are relatively simple (inferring the 3D scene state, manipulating the scene).\n\nFurthermore, the model is not described with enough detail to reproduce it. The details of the neural architectures used are not given, how the Mask R-CNN module is repurposed to operate in 3D is not explained and the details of the GRNN used are missing. While most of the individual components are already existing models, knowing the specific details would help understand better the model. \n\nIn summary, the paper proposes a highly complex system to learn 3D object centric scene representations with a forward model that doesn't seem that it would generalize past simple environments and that is not described in sufficient detail to be reproduced. I would change my score if the authors showed that such model can perform predictions in simple yet real world scenarios that go beyond the synthetic environment used. For example, the authors should be able to directly use one of the BAIR Robotic Interaction datasets (https://sites.google.com/berkeley.edu/robotic-interaction-datasets) with their model, which replicates fairly well the synthetic environment used to test the model but has real data, and compare the accuracy of the predictions to a 2D video prediction model such as SVG-LP (Denton and Fergus, ICML 2018) that does not make any assumptions about the 3D world nor has an object-centric representation.\n\nMinor note: I'd recommend not including extraneous or non-supported claims such as \"humans excel in manipulation despite that less than 1% of us know what inertia is\" or \"human brain effortlessly disentangles camera motion from object motion and appearance\""}