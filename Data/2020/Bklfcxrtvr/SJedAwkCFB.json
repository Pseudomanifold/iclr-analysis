{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Paper summary:\nIn this paper, a model is learned that can construct a 3D scene representation based on 2D images from multiple viewpoints. Additionally, a 3D object detector is trained on top of the scene representation using ground-truth bounding box supervision. A dynamics model is then learned to predict future object positions, given the actions of a robotic arm that pushes the objects. The key idea of the paper is that, because it uses a 3D representation, the dynamics model can use the permanence of 3D object shape to avoid accumulating object shape errors during prediction. The paper shows experimentally that the learned 3D scene representation improves dynamics prediction over a models using either a 2D scene representation or ground-truth object centroids as object representation. Further, the dynamics model is used for model-based control and performs better than the baseline model using ground-truth object centroids.\n\nDecision:\nThe paper is borderline. While the application of 3D representations to dynamics modeling is a well-motivated and promising direction, the paper only adds a simple supervised one-step prediction model on top of the previously published 3D representation (Tung et al., 2018). The experiments show promising results, but many of the claims made about the predictive model are not sufficiently supported.\n\nSupporting arguments:\n1. The abstract claims that the paper empirically demonstrates \u201cthat the proposed 3D representations learn object dynamics that generalize across camera viewpoints and can handle object occlusions.\u201d I did not find experiments that test either of these claims. Please add experiments comparing evaluation on seen vs. unseen camera viewpoints. Also add experiments showing robustness to occlusions.\n\n2. The list of contributions in the introduction claims to show \u201cstrong generalization to environments of novel objects, novel number of objects, novel objects spatial arrangements and novel camera viewpoints.\u201d None of these cases are shown in experiments. For each of these points, please perform a comparative experiment, showing that the proposed model performs better than the baselines.\n\n3. For the control application (Table 4), why was only the XYZ baseline used? How about evaluating the other baselines (2D-multiview, ours-depth) on this task? It would also be good to compare to some competing models, not just ablations.\n\n4. In Table 3, how can the XYZ baseline (using ground-truth object positions) be worse than all the other models, which use the learned object detector to obtain object positions? Please explain this.\n\nMinor comments:\n1. Please estimate some sort of uncertainty on all of your experimental values, e.g. confidence intervals over a number of random model initializations.\n\n2. It would be helpful to explain Eq 2-4 individually, with text between each of the equations to make it clear what the purpose of each step is.\n\n3. Eq 2-4 use both small and capital Os and 0s in the notation, this is hard to parse. Consider using a different letter instead of O.\n\n4. Also, different letters for m and M might be helpful, since they describe completely different objects (binary occupancy maps vs feature maps, if I understand correctly).\n\n5. In Eq 2-4, the use of subscripted superscripts seems unnecessary. If \u201co\u201d is an object index, we can simply call a different object \u201cp\u201d, instead of adding another index o_i and o_j.\n\n6. Figure 1 could be clearer, with a clear indication of which part of the figure corresponds to which equation in the paper. Also, the choice of pale blue boxes inside orange boxes is not optimal, things are hard to see.\n"}