{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "How to quickly learn control policies with minimized number of environment interactions have long been an important problem. To tackle this problem, this paper proposed a \"hierarchical Bayesian optimization (HIBO)\" algorithm to optimize the \"feature parameter \\phi\" (which I don't know what that is) and the \"policy parameter \\theta\" hierarchically. Under the formulation of maximizing reward J(\\theta), the algorithm firstly uses EI to select \\phi. Given the selected \\phi, the algorithm selects the policy parameter \\theta. The proposed algorithm is evaluated on a Humanoid Postural Balancing task, and shows achieves high rewards faster than the standard EI acquisition. However, the paper is awfully written such that I cannot understand what the \"feature parameter \\phi\" is. Given my limited understanding, I think the paper should be rejected.\n\nStrengths,\n1, The paper deals with an interesting task: Humanoid Postural Balancing. A Humanoid is expected to learn how to balance as quick as possible to reduce the interactions with the environments, which suits well with Bayesian optimization.\n\nWeakness,\n1, The paper is awfully written. The problem statement subsection is unreadable. I don't see anywhere explaining how the states x_t, commands u_t, \\theta, and feature \\phi are related? What is \\phi ? It is super wired why the feature parameter \\phi is jointly maximized with the policy parameter.  Because I don't understand the formulation, I can hardly understand anything else.\n2, From my very limited understanding on the formulation, the proposed HIBO is trivial.\n3, The experiments are limited. The paper only conducts one experiment on the Humanoid control balancing. And they paper only compares with the EI acquisition, while the state-of-art acquisition MES should be also be compared with.\n4, The proposed mental replay is not well justified, qualitatively or empirically."}