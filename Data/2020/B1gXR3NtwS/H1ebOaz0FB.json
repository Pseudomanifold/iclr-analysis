{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper combines ideas from neural architecture search (NAS) and Bayesian neural networks. Instead of maintaining uncertainty in network weights, the authors propose to retain uncertainty in the network structure. In particular, building on cell-based differentiable NAS, the authors infer a distribution over the gating weights of different cells incident onto a tensor while relying on point estimates for the weights inside each cell. \n\nOverall, I liked the paper and vote for accepting it. The notion of maintaining uncertainty about the network structure is a sensible one, and the paper explores an as yet under-explored area at the intersection of state-of-the-art network architecture search algorithms and Bayesian neural networks. Moreover, this is accompanied by compelling empirics \u2014 results demonstrate gains in both predictive performance and calibration across diverse tasks and careful comparisons to sensible baselines are presented to evaluate various aspects of the proposed approach (Table 1). \n\nDetailed Comments:\n+ One issue the experiments fail to adequately disentangle is the effect of weight uncertainty vs structure uncertainty. Are the observed gains in accuracy and calibration simply a product of better structure learning? In particular, I would love to see a baseline where point estimates of \\alpha are learned but posterior distribution over weights is inferred. I realize NEK-FAC was an attempt at providing such a comparison, but since it uses a different structure, it remains unclear whether it\u2019s poor performance stems from the fundamental difficulty of learning posteriors over high dimensional weights or simply a sub-optimal network structure. \n\n+ In a similar spirit, one can imagine a fully Bayesian DBSN where one infers posterior distributions overbite \\alpha and w. Presumably, this would close the OOD entropy gap between random \\alpha and DBSN. \n\n+ How many Monte Carlo samples were used to evaluate Equation 8. In variational BNNs one often finds that using more MC samples doesn\u2019t necessarily improve predictive accuracy over using the most likely sample (the mean if using a Gaussian variational family). It would be interesting to see predictive performance as a function of the number of MC samples for DBSN. \n\n+ Clarity: While I am mostly upbeat about this paper, the writing could be significantly improved. While the overall ideas come across, there are several instances where the text appears muddled and needs a few more polishing passes. "}