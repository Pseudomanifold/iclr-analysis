{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a procedure for reconstructing the architecture and weights of deep ReLU network, given only the ability to query the network (observe network outputs for a sequence of inputs).  The algorithm takes advantage of the piecewise linearity of ReLU networks and an analysis by [Hanin and Rolnick, 2019b] of the boundaries between linear regions as bent hyperplanes.  The observation that a boundary bends only for other boundaries corresponding to neurons in earlier network layers leads to a recursive layer-by-layer procedure for recovering network parameters.  Experiments show ability to recover both random networks and networks trained for a memorization task.  The method is currently limited to ReLU networks and does not account for any parameter-sharing structure, such as that found in convolutional networks.\n\nThe networks used in experiments appear to be substantially smaller (e.g. input/output dimensions on the order of 10 neurons) than those used in real applications.  Is the proposed approach practical to apply to networks used in actual applications?  How does the number of queries per parameter scale? (page 5 mentions sample complexity for recovering the first layer, but it would be helpful to clarify the situation for subsequent layers).\n\nPage 7 states that the proposed algorithm also holds for ResNets, with slight modifications, but defers details to future work.  If the modifications are indeed slight, it would better to include them here as this is an important special case and would increase the potential impact of the paper.\n\nOverall, while the paper does appear to rely heavily on developments made by [Hanin and Rolnick, 2019b], there is a potentially interesting contribution here.  I would appreciate clarification on concerns over practicality and the extension to ResNets.\n"}