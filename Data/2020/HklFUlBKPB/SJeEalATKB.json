{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2331", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a new method to recover the unknown structure of  the network by utilizing the piecewise linearity of ReLU network.\n- Some theoretical explanation of the method is provided.\n\nNote & Questions\n- As far as the author understands, the algorithm does not suppose a fully black-box condition. By seeing the section 4.1 and 4.2, it seems possible to access neurons in the intermediate layers.\n- Also, the proposed method seems to target only a MLP.\n\n\nStrong-points\n- This field is not that thoroughly investigated, and the author proposes a creative method to infer the hidden statistics of the neuron.\n\nConcerns\n- Most of all, the information the experiments conveys is too small to convince the argument of the author. The reviewer could not find the dataset they train (in the Experiment section), and the graph only shows the case of two-layered networks. Moreover, the reviewer couldn't find the explanation of the graph, including their legend (for example, Memorization).\nThe author suggests that this method can be applied to various networks. Still, the reviewer couldn't find any clue that the method actually worked for various settings: different activations, convolutional networks, and so on. More experimental results supporting the argument of the authors are required.\n- Assuming that the network was trained by MNIST and we infer the weight of the networks by the proposed method. Can the recovered network classify the number as well? Then, how the accuracy change?\nMore quantitative results regarding the asking are required.\n- Experimental results for more-than-two layered networks should be provided.\n- Oh.et.al (https://arxiv.org/abs/1711.01768) proposed a blackbox reverse-engineering method and provided experimental settings as well. The author should clarify the novelty and the strong-points of the works compared to the mentioned work.\n\nConclusion\n- The author proposes a new method to recover the weight and bias of the network.\n- The reviewer could not find much clue supporting the author's argument from the experiment section.\n\ninquiries\n- See the Concerns parts."}