{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes the incorporation of \u201cprior knowledge\u201d which enters in the form of the relations between training instances in neural network training. The proposed method is tested on VQA problem, bringing improvements upon the popular soft regularizer. \nThe authors claim that their method is a general technique but in fact, the constraints are drawn from specific tasks (VQA for example). So, I believe the contribution is rather domain-dependent and not general. Can you explain more how this method can be applied to general problems?\n\nOther than that, I have some concerns:\n1. Although the authors claim that they are the first to bring these annotations to VQA, I see their training procedure is closely related to cycle-consistent learning. Recent work in VQA also applied cycle consistency as an online data-augmentation technique (See Shah et al. 2019).\n\u201cShah, M., Chen, X., Rohrbach, M., & Parikh, D. (2019). Cycle-consistency for robust visual question answering.\u201d\n2. In Section 2, the authors say \u201cconstraints on the parameter space of a model are often non-intuitive\u201d. How are they \"non-intuitive\" and why the proposed method is more intuitive in terms of theory? Please clarify this.\n3. Each question in Hud et al. is associated with a functional program, therefore, questions are compositional. However, arbitrary questions don\u2019t need to strictly follow this constraint. Natural language is not exactly suited to functional programming I think. I have doubts about the claim in Section 4 \u201cOur method can use partial annotations and should more easily extend to other datasets and human-produced annotations\u201d. Also, the definition \u201cA question is defined as a set of operations\u201d does not seem correct. A question can be translated into a program that is composed of a set of operations.\n4. Experimental results are not strong enough for such strong claims I believe. Regarding GQA dataset, the authors should compare the proposed method with more works, for example, Hu et al. 2019 and Hudson et al. 2019 achieve much favorable performance upon MAC.\n\"Hu, R., Rohrbach, A., Darrell, T., & Saenko, K. (2019). Language-Conditioned Graph Networks for Relational Reasoning.\" \n\"Hudson, D. A., & Manning, C. D. (2019). Learning by abstraction: The neural state machine.\"\n\nMinor comments: The paper is not really well written. I even found a wrong reference (Section 3).\n"}