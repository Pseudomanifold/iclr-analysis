{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation. The experiments demonstrate not only a strong results over baseline Adam with warmup learning rate but the robustness of the optimizer. The authors additionally demonstrate the theoretical justification behind their optimizer, however I am not very qualified to make the judgement on the theory. Overall judging from the authors description of approach and experimental results, I recommend acceptance.\n"}