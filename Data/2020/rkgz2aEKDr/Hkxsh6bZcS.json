{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training.\nPros:\n1.\tAuthors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam.\n2.\tThe empirical study supports the claim about the large variance.\n\nCons:\n1.\tTheoretically, authors didn\u2019t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc.\n2.\tThe performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive.\n"}