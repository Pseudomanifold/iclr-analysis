{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Summary\n\nIn this paper, the authors propose an auto-regressive deep generative model for graph-structured data, motivated by the goal of scalability with respect to graph size, graph density and sample size.\n\nIn a nutshell, the approach follows closely the ideas in [1, 2], which model graph generation as an auto-regressive process after fixing or sampling an ordering for the nodes. Unlike [1, 2], however, the proposed method makes use of graph convolutions and a graph attention mechanism, closely related to GAT [3], to parametrize the conditional distributions of node/edges given the previously generated graph elements.\n\nThe performance of the proposed approach is evaluated in comparison to [1, 2] in several synthetic and real-world datasets, using MMD [4] between generated and held-out test graphs as metric. Unlike [2], which applies MMD on three graph statistics (degree, clustering coefficient and average orbit counts), this manuscript proposes to evaluate MMD using a graph kernel as well [5].\n\n# High-level assessment\n\nThe main contribution in this paper is to combine a graph attention mechanism, which can be seen  as a simplification of GAT [3], with deep autoregressive graph models, such as DeepGMG [1] or GraphRNN [2]. In this way, the manuscript has a large conceptual overlap with the method in [6], which can be nevertheless be regarded as concurrent rather than prior work. From a methodological perspective, I believe the contribution is sound and sufficiently novel, although perhaps slightly on the incremental side. \n\nHowever, the current version of the manuscript has shortcomings regarding (i) lack of clarity in the exposition of the method\u2019s relation to prior work, low-level implementation details and experimental setup and (ii) insufficient experimental results to back up some of the authors\u2019 claims.\n\nNonetheless, I believe the proposed approach is promising, and encourage the authors to address or clarify these issues during the author discussion phase.\n\n# Major points / suggestions\n\n1. The manuscript presents the proposed approach in a way that does not clearly differentiate between prior work and original contributions.\n\nIn particular, I believe that the ideas in Section 3.1 and 3.2 are almost identical to those in [1, 2], the graph attention mechanism in Section 3.3 can be seen as a minor modification of GAT [3], and Section 3.4 also has a strong conceptual overlap with [1, 2].\n\nI would encourage the authors to be more clear with respect to what is novel and what is borrowed from prior work. Moreover, when slightly departing from prior work (e.g. the modifications applied to the graph attention mechanism in Section 3), I would also encourage the authors to focus on explaining what specifically has changed and what is the rationale behind those design choices, rather than explaining the entire mechanism \u201cfrom scratch\u201d, leaving up to the reader to figure out what is novel.\n\n2. The paper\u2019s clarity could be improved, with some parts presented in an unnecessarily complicated manner (e.g. the graph attention mechanism) and others without sufficient detail (e.g. the edge estimator module, the zero-ing heuristic for attention or the generation of graphs based on \u201cseed graphs\u201d, which is only mentioned in the appendix).\n\nFor example, regarding the graph attention mechanism, I would recommend: (i) explaining more clearly what the \u201cfeature vector of node $v_{i}$\u201d is exactly in relation to the notation of Section 3.1; (ii) if the query, key and value matrices are identical, as the text seems to imply, I would rewrite the equations directly in terms of $X$ which would simplify the notation significantly; (iii) perhaps most importantly, the bias functions $b^{Q}$, $b^{K}$ and $b^{V}$ should be defined mathematically and discussed in greater detail and (iv) the output FNN should also be described mathematically. Finally, as mentioned above, I would emphasise the differences between the proposed attention mechanism and GAT.\n\nThe edge estimator mechanism is described too imprecisely in Section 3.4.4. While Section A.4 definitely helps, I would recommend defining the entire operation mathematically in Section 3.4.4 as well. Likewise, a precise mathematical definition of GRAM-A in Section 3.5.2 would also be helpful.\n\nFinally, as mentioned in this forum by Prof. Ranu prior to this review\u2019s writing, the graph generation procedure described in Section A.7.2 seems unconventional. I would encourage the authors to both clarify what they mean by \u201cfor the convenience of implementation\u201d and to investigate whether the experimental conclusions are affected by this departure from prior practices.\n\n3. Key details about the experimental setup, such as the hyperparameter selection protocol for the proposed approach and baselines, as well as the resulting architectures, seems to be missing, making it difficult to assess if the experimental setup is \u201cfair\u201d. \n\nIn particular, all methods should be allowed to use a similar number of parameters or, alternatively, have their hyperparameters tuned equally carefully for each dataset separately.\n\n4. Most importantly, I believe the experimental results are insufficient to back up some of the claims made in the introduction. \n\n    4.1. Despite the focus on scalability throughout the motivation, there are no experiments systematically exploring how the runtime at train and test time of the proposed approach and the main baselines scales with respect to sample size, number of nodes per graph and graph density. Moreover, no results are provided for large graphs (e.g. ~5k nodes as in [6]). \n\n    4.2. The graph attention mechanism was claimed to be an original contribution. However, no results are provided to evaluate its advantages with respect to the different GAT variants nor ablation studies to see its usefulness relative to a variant of the proposed approach using only graph convolutions.\n\n    4.3 The idea of using MMD in conjunction with graph kernels as a performance metric is interesting. However, there is no investigation of key aspects such as (i) its relation to other metrics and (ii) the impact that the choice of graph kernel, among the many available, and/or of graph kernel hyperparameters has on the resulting metric (see [7] for a comprehensive review on graph kernels).\n\n   4.4. Finally, the results have been reported without error bars, making it difficult to quantify the statistical significance of the observed performance differences between approaches.\n\n# Minor points / suggestions\n\n1. I strongly believe the authors should adapt the manuscript to mention [6] and related/concurrent work. Ideally, including it as an additional baseline would be even better, but not necessary given the limited rebuttal time. Nevertheless, this point was not taken into consideration when scoring the manuscript, given how recent [6] is.\n\n# References\n\n[1] Li, Yujia, et al. \"Learning deep generative models of graphs.\" *International Conference on Machine Learning.* 2018.\n[2] You, Jiaxuan, et al. \"Graphrnn: Generating realistic graphs with deep auto-regressive models.\" *International Conference on Machine Learning.* 2018.\n[3] Veli\u010dkovi\u0107, Petar, et al. \"Graph attention networks.\" *International Conference on Learning Representations*. 2018.\n[4] Gretton, Arthur, et al. \"A kernel method for the two-sample-problem.\" Advances in Neural Information Processing Systems. 2007.\n[5] Costa, Fabrizio, and Kurt De Grave. \"Fast neighborhood subgraph pairwise distance kernel.\" Proceedings of the 26th International Conference on Machine Learning. Omnipress; Madison, WI, USA, 2010.\n[6] Liao, Renjie, et al. \"Efficient Graph Generation with Graph Recurrent Attention Networks.\" *Advances in Neural Information Processing Systems.* 2019.\n[7] Kriege, Nils M., Fredrik D. Johansson, and Christopher Morris. \"A Survey on Graph Kernels.\" *arXiv preprint arXiv:1903.11835* (2019)."}