{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The work proposed Super-AND, which extends the Anchor Neighborhood Discovery model (Huang et al. 2019) and adopts additional losses including unification entropy and augmentation trying to provide a comprehensive approach to unsupervised representation learning. Experiments on image benchmarks show promising results.\n\nFrom the ablation study shown in Table 4 and the main results in Table 1, it seems the data augmentation part solely contributes almost all the improvement in Super-AND (86.4 for Super-AND w/o Aug v.s. 86.3 for AND). Even the unification entropy loss and the Sobel need the augmentation in order to shine. On the other hand, this can be seen as some form of multi-task learning [1]. Then it is interesting to see how other pretext tasks collaborate with AND to justify the choice of augmentation.\n\nOne crucial aspect of unsupervised embedding learning is how it scales with the data. That is, how the model performs when there is abundant of data. The AND model did not excel in this since it did not produce favorable results in the large-scale ImageNet evaluation comparing to simpler models, despite it demonstrated solid improvements in the small-scale settings. Simple tasks may indeed have advantages in the large-scale setting as shown in [2] that rotation and context models work really well when proper architectures are used. Thus, it would be interesting to see how Super-AND works in the large scale setting. But the authors did not provide any evaluation in large scale data sets.\n\nOther comments:\n- In the AND work, the authors showed that the smallest neighborhood (k=1) is the best choice. Here, the augmentation loss is added to introduce more \"positive\" signals into the model. Does this help to include larger neighborhood to facilitate faster learning?\n- The final classification is performed using weighted k-NN classifier, but the authors did not mention how the \\tau value is determined. Is the same \\tau value used for all pretext tasks? If it is the case, this can not be considered as a fair comparison since the best performance of different pretext tasks may require different \\tau values. Also, how sensitive is the model to \\tau? \n\n[1] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. ICCV 2017. \n[2] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. CVPR 2019."}