{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed a so-called super-AND algorithm to learn useful representations in an unsupervised manner, which hopefully reduces the demand for training deep networks with a large amount of labeled samples. The main idea follows the anchor neighborhood discovery approach proposed by Huang et al., by first finding neighborhood for each sample, performing neighborhood selection (curriculum learning) and finally optimizing a unification entropy loss (and data augmentation loss) that distinguished one neighborhood pairs from another. \n\nThe novelty of the paper looks very limited considering its similarity with Huang et al\u2019s paper. Besides, the writing and organization of the paper have a large space of improvement. In its current version, the discussion of basic concept and ideas look fragmented and incoherent. It is not easy to read through the paper to clearly capture the main theme of the paper. Many notations are hard to follow. For example, in defining the problem, authors mentioned that the bold p_i in equation (1) is defined as the probability of image being in its own class- what is the meaning of this? Then why p_i is a vector and the $j$th entry corresponds to one memory m_j?  Does each memory slot correspond to one class? If so, how do you update the memory? And if not, why the probability vector p has a dimension that is the same as the number of memory slots? This would look very confusing to the readers. Another example is that there are many different versions of the loss functions listed in Section~3,  like unification entropy loss and augmentation loss; are they both optimized and can authors clearly state their global loss function?\n\nThe motivation of the paper is to separate samples from different classes far away from each other and local anchor neighbors should have similar labels; however, from the embedding visualization presented in figure~(3), I feel that it is very similar to (or even a bit worse than) the original AND algorithm in terms of class separability. \nI hope that the authors can spend more efforts clarifying their ideas and make their writing coherent so that readers can have a better experience reading it. Also avoid using vague terms like \u201clearning representations that are visually meaningful\u201d without clearly elaborating on its meaning. \n"}