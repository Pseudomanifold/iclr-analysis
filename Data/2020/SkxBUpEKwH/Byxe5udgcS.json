{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents  a controllable model from a video of a person performing a certain\nactivity. It generates novel image sequences of that person, according\nto user-defined control signals, typically marking the displacement of the moving\nbody. The generated video can have an arbitrary background, and effectively\ncapture both the dynamics and appearance of the person. It has two networks, Pose2Pose, and Pose2Frame. The overall pipeline makes sense; and the paper is well written.\n\nThe main problems come from the experiments, which I would ask for more things. It has two components, i.e., Pose2Pose and Pose2Frame. So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.  How about combining only Pose2Pose/ Pose2Frame  with pix2pixHD? Whether the performance can get improved?\n"}