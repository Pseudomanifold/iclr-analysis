{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors study the problem of adding residual connection to GNN for node classification. The authors first identify the problem referred to as \u201cSuspended Animation\u201d in GNN when the depth increases. Then the authors provide both empirical and theoretical characterization for the behavior. To handle this issue, the authors study several different ways to add residual connections in GNN including the na\u00efve method proposed in Kipf et al. The authors carry out extensive experiments on three datasets on different residual connections for the node classification task.\n\nStrength\n1. The authors identify and study an interesting and important issue in GNN as the \u201cSuspended Animation\u201d issue.\n2. The authors provide both empirical and theoretical analysis for the \u201cSuspended Animation\u201d behavior. Moreover, the authors provide theoretical justification for the added residual connection in GNN as gradient norm bound.\n3. The authors carry out extensive experiments on different variants of residual links. Morover, the authors provide code online for reproducibility.\n\nWeakness:\n1. In the theoretical analysis, the assumption that the FC layer is identical mapping is too simplistic. The analysis differs from the actual model especially when the residual links are considered in equation (8), where we have a sum of FC layer output and residual connection. Actually, the empirical results show that na\u00efve residual links work pretty good on several datasets. It goes against the analysis in Corollary 1.\n2. Though the authors provide bound on the norm of gradient under residual links, it would be better if authors could justify the adding of residual link from the perspective of Theorem 1.\n3. The authors study the depth of GNN up to 7 layers at most. It would be interesting to see how the model performs under really deep networks.\n4. The authors mention several factors to affect GNN in section 4.2. It would be interesting to see how these factors like training data set and feature coding interacts with different ways of adding residual connections.\n5. It is very informative that the authors compare their methods on the widely used three datasets. It would be better if the authors could carry out experiment on larger graphs to verify the empirical observations. For example, do we need to have deeper networks for larger graphs?\n"}