{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThis paper studies the \u201csuspended animation limit\u201d of various GNNs \u2013 an important one for how to train a good Graph network. The authors provide sufficient analysis by simplify GNNs as a series of 1-step Markov chains (which is my concern as stated in the section on main issues), while pointing out the limitations quantitatively as in the Theorem 2. Under the assumption, the authors propose several new forms of ResNets for GCNs, which can successfully overcome the limitation.\n\nOverall, the motivation of this work is clear and meaninfgful. The proposed residual architecture is effective, and the presentation is clear and easy to understand. \n\nHowever, my main concerns are on the initial assumptions for analyzing the suspension of GNNs. See the following comments.\n\nThis paper is generally well written and easy to understand. The organization of each part is well-balanced.\n\nOriginality:\n\nTo the best of my knowledge, numerous methods (i.e., targeting on applications) address this problem by augmenting A [1] or X [2] with similarity of feature representation learned from other sources. However, this paper specifically analyzes the problem in a principle way. I consider this work is generally novel.\n\n[1] X. Wang and A. Gupta. Videos as Space-Time Region Graphs. ECCV 2018.\n[2] N. Wang, Y. Zhang, Z. Li , Y. Fu, W, Liu, Y. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. ECCV 2018.\n\nSignificance:\n\nThe significance lies mostly in motivation and the proposed GResNet.\n\nMotivation: This paper studies the phenomenal that GNNs tends to not respond to the input data when certain depth of a network is reached, which the authors called as suspended animation limit. Studying problem is fundamental and important, and also unique since different with CNNs where data are independent, the data instance within GNNs are highly correlated.\n\nGResNet: Given the differences, and within the Theorem 2 where the residual formulation of CNNs does not apply to GNNs, the paper also proposes several new formulations, i.e., in figure 2, where the suspension is avoidable and the performance under the same experimental settings is obviously boosted. \n\nMain issues: \nMy major concern to this work lies in the assumption used throughout section 3 and 4. At the beginning of Section 3.2, the authors assume that W is identity. Since X is assumed to be column-wise normalized, the nonlinearity is removable. However, this is not true in real cases: W is actually learnable and not bounded. When W is learned to be negative, Relu layer is not removable, and the behavior of the network will be completely different with what the paper depicted. Indeed, GNNs contain stacked linear+nonlinear functions, which cannot be simplified as a linear Markov chain. It is analogy to CNNs, which is not possibly be simplified as a group of average poolings.\n\nMinor issues:\n1. I agree that under the assumption, eq. (11) shows that the differences between the learned representations are not discriminative, however the claim \u201cmajority of the nodes are of very small degrees\u201d is not justified and only apply to the internet topology in Faloutsos et al. (1999).\n\n2. I feel the \u201cRaw Feature Coding\u201d and the \u201cNetwork Degree Distribution\u201d are sort of repetitive, and the eq. (11) is eq. (12) at the stationary point.\n"}