{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The paper studies the problem of filling in the frames of a video between a start and an end frame of the order of one second apart, by proposing a residual 3D CNN based architecture that exploits the natural biases of convolution to enforce temporal coherence in the video. Important features of the proposed approach include the treatment of the video synthesis problem entirely in a latent feature space, and the temporally coarse-to-fine synthesis of intermediate frames. Results on standard datasets for video prediction show improvements against some ablated variants of the proposed approach and promising preliminary comparisons against prior work.  I particularly like the idea of using the natural bias of 3D convolution to do smooth interpolation by synthesizing a full video bookended by the first and last frames. Architectural choices are interesting and well-motivated and combine good ideas from many prior works, including prediction in latent space, and coarse-to-fine synthesis (mainly used before along spatial axes for image generation, but used here temporally for video generation). \n\nThe limited experimental settings are, in my view, the main weakness of this work: \n(i) In its chosen setting of video interpolation over about 16 frames, given that there is limited prior work that directly studies this problem, it falls to this paper to make the case that prior approaches studying interpolation over shorter horizons fail in this setting. However, it fails to do this convincingly: the only comparison to prior work is in the short apparently hastily written paragraph at the very end of the paper (Tab 3). Prior methods are not compared against in an apples-to-apples manner: as best I can tell from the incomplete specifications in this paragraph, the 3 prior methods all generate 7 frames between the start and the goal frame, while the proposed approach generates 14. For these 7 frames, the proposed approach appears to perform worse on BAIR and KTH and better on arguably the hardest dataset, UCF101, compared to prior work. From these experiments, the gain of this new approach over prior work on video interpolation is far from settled.\n\n(ii) The proposed approach is complex, containing many interesting non-obvious choices: coarse to fine synthesis, latent space prediction, the gated accumulation of information in z, the additional stochastic component n^(l) in Eq 4, an adversarial loss rather than L2, and multiple discriminators for the adversarial loss. Of these, the only choices studied in experimental ablations is latent space prediction, and (one aspect of) the gating. This leaves me wondering about the value of all the rest of this complexity.\n\nHere are a few other queries/comments for the authors to consider answering in rebuttal or addressing in revision:\n- It is not clear to me why the proposed method is specific to interpolation/\"inbetweening\": as the authors themselves say, the problem of forward prediction is much better studied and offers many more baselines. This perhaps offers an easier opportunity for comparison against prior work.\n- It is also not clear why prior work on video prediction such as SVG [Denton 2018] could not be extended to work well on this task by additionally conditioning it on the final frame. \n- I could not find any scholarly references to \"inbetweening\" in prior work. Given this, why not use a term from the literature like video interpolation or infilling?\n- Intro claims the proposed method allows deeper nets and more stable training, but these are never evaluated. \n- While the paper claims longer time scales than prior work, the scales are still quite short in absolute terms, since the time scales under consideration are less than a second on 2 of 3 datasets. It would be interesting to see if the approach can handle much longer interpolation, over say, 10-20 seconds, or even more.\n- The related work section is large but unhelpful about offering connections of prior work to this work. For example, Vondrick et al 2016 is described in video generation, but without reference to the usage of 3D CNNs also used in this work. There are also missing references I would have expected: time-agnostic prediction for generating intermediate frames, and structVRNN, deepmind transporter, and contrastive predictive coding for latent space prediction. \n- Typos: \n   - Fig 1 \"max\" block is missing a \"0, \". The [1, 9, 17] is not explained. \n   - SxSxC below Eq 4 should be HxWxC?\n"}