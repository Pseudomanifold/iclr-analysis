{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\n-------------\nThis paper proposed to improve existing meta learning algorithms in the presence of task imbalance, class imbalance, and out-of-distribution tasks. Starting from the model-agnostic meta-learning (MAML) algorithm (Finn et al. 2017), to tackle task imbalance, where the number of training examples of varies across different tasks, a task-dependent learning rate decaying factor was learned to be large for large tasks and small for small tasks. In this way, the small task can benefit more from the meta-knowledge and the large task can benefit more from task-specific training. To tackle class imbalance, a class-specific scaling factor was applied to the class-specific gradient. The scaling factor was large for small class and small for large class so that different classes can be treated equally. To tackle the out-of-distribution tasks, a task-dependent variables was learned to emphasize meta-knowledge for the test task similar to training tasks. Additional model parameters are learned through variational inference. Experimental results on benchmark datasets demonstrate the proposed approach outperformed its competing alternatives. Analysis of each component confirm they work as expected.\n\nComments\n---------------\nThis paper is well motivated and clearly written. The empirical evaluation also support major claims in the paper.\n\nCan the author provide more details on the inference of the model? In the likelihood term in Eq. (7), the task specific parameters \\theta^{\\tau} was parameterized by Eq. (3), which contains K iterative gradient updates. How was the gradient w.r.t. \\theta was computed in this setting?\n\nThe task-specific learning rate decaying factor was constrained to be between 0 and 1 using the function f(). The class-specific scaling factor made use of the SoftPlus() function, for the same purpose of scaling learning rate, why do these two different options of functions were applied?\n\nFor the scaling vector of the initial parameters g(z^{\\tau}), for its zero entries, the initialization of the corresponding entries in task-specific parameter \\theta would be zero. Would it be better to apply a linear interpolation between \\theta and a randomly-initialized vector in Eq (2)?"}