{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper discusses concepts of self-induced distributional shift (SIDS) and the hidden incentives when using meta-learning algorithms. It then prescribes a unit-test to check whether there is hidden incentive for distributional shift (HIDS) in the algorithm and proposes to use context swapping to mitigate such phenomenon.\n\nI am not very familiar with this series of research but I am wondering why the paper focuses on meta-learning and its connection to HIDS. Does it use meta-learning as a tool to identify HIDS? But from the description and experiments, it seems the paper is talking about how meta-learning itself leads to HIDS, for example, by comparing different hyper-parameter setting for meta-learning and PBT, it shows the unit-test is failing. So it seems like meta-learning itself leads to distributional shift?\n\nAlso I cannot fully appreciate the utility of this \"unit-test\". It is well known normally an interactive system that can change its inputs have distributional shift. What other information does this \"unit-test\" inform us? Similarly, how does \"context swapping\" mitigate distributional shift? From the experiments, it dumbs the meta-learning algorithms and make it pass the \"unit-test\", but I am not sure what other practical benefits it can bring to improve real systems.\n\nUsually, a reinforcement learning algorithm can meaningfully mitigates the adverse effects of distributional shift by explicitly modeling this interactive process and evaluating rewards with considerations to distributional shift caused by different policies. It is difficult to see how the concepts discussed in the paper provide meaningful approaches to address the issue.\n\nOverall, the paper touches the important question of distributional shift for machine learning systems but I find the concepts discussed in the paper, such as the focus on meta-learning, the \"unit-test\", and \"context-swapping\", less relevant to how we can really mitigate the issues in real systems or how it can provide additional insights about the problem."}