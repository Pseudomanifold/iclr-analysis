{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nIn this paper, the authors present a methodology for generating intrinsic rewards for reinforcement learning agents targeting hard exploration environments. The intrinsic reward is generated using an episodic memory module and a lifelong novelty module.  A state representation is learnt in such a way that the novelty signals are biased towards what the agent can control. A single neural network learns the q-values of exploratory policies with different degrees of exploration. Several experiments on a simple domain and on Atari domains are conducted to evaluate and compare the performance of the proposed method against the baselines. \n\n\nIn my opinion, this is an interesting idea as it present a novel combination of methods (Random Network Distillation and Episodic Memory) that works and can inspire other researchers in the field.  However, I would like to see clearer explanations in the experimental section before acceptance. For this reason I rate this paper as a weak reject but if clarity is improved I will increase my score.  See below for more general comments and detailed explanation about the experimental section.\n\nGeneral comments:\n\n- The authors state that a novel contribution is to disentangle exploration and exploitation. This is not true: see [1] for a recent paper on the topic. I believe the authors should cite this paper.\n\n- In page 3: \u201cTo determine the bonus, the current observation is compared to the content of the episodic memory. Larger differences produce larger episodic intrinsic rewards\u201d When computing the intrinsic reward for a new state, it must be compared to the episodic memory, which is composed by recent states (and old). Therefore, in this situation the intrinsic reward is always high? Specially because it is compared with the k-nearest neighbours?  Maybe the authors could elaborate on this?\n\n\n\nExperiment Section:\n\nThis section should be tidied up in my view.  In general, I feel that there are too many fine-grained results / experiments. I think some aggregation of results would be good for clarity. Without this aggregation there are statements made from the authors that are difficult to believe, e.g. a general result statement is valid for one or two games but not for the rest (but still the statement is formulated in a general way). This can lead to misunderstandings and overstatements.  This together with lack of some experimental details makes the section very dense and difficult to parse. Below I make my points. They are in order of appearance in the main manuscript. I marked with (*) the ones I consider the most important. \n\nSection 4.1:\n\n- Why the exploration policy in this section is set to have $\\beta = 0.3$ ? Is my understanding correct that any value of $\\beta$ would produce similar results since this is just a scaling of the intrinsic reward (i.e. it does not balance exploration vs exploitation)?\n\n- The parameter $\\beta$ appears in the construction of the reward r = r^e + \\beta r^i , but also the output of Algorithm 1 scales the similarity by $\\beta$ does that mean that the final contribution of $\\beta$ is squared? Is this intended? If so, why?\n\n- At the end of this section: \u201cHowever, staying still is enough: staying still every state will produce \u2026.\u201d Since the agent can only take 4 action {left, right, up, down}, what do the authors mean by \u201cstaying still\u201d, is really the agent doing some sort of cyclic policy e.g. left right left right \u2026 ?\n\nSection 4.2:\n\n- In the appendix A it is stated \u201cuse last 5 frames of the sampled sequences to train the action prediction network\u2026 \u201c Does this refer to frame-stacking? I assume it is not since at the beginning section 4.2 it is stated that there is no frame-stacking. If it is not frame-stacking, the authors could explain in more detail what do they refer to.\n\n- In the paragraph \u201cArchitecture\u201d it is stated that 8 games were selected to choose the hyperparemeters and that the results are in Appendix B. However, appendix B only shows 2 games (Pitfall and Montezuma\u2019s Revenge). Is this a typo?\n\n- (*) In paragraph \u201cNGU Agent\u201d: This is the most dense paragraph and the most difficult to parse. First of all Table 1 shows all the results, but as one can see, the different ablations have very similar performance on most games with only a few exceptions. Note that most of the mean performances with error bars, have actually overlapping error bars for many combinations of games and methods. Therefore, further statements about this table are difficult to believe since they could have been just the result of random seeds. I think it is fine to show these fine-grained results on the appendix, but I would say it would be better to aggregate them in the main paper and show that the statements made by the authors still hold for this aggregation.\n\nSpecific comments about NGU agent paragraph:\n\n- \u201c we observe an improvement from increasing the number of mixtures (with diminishing returns) on hard exploration games.\u201d I would say the authors cannot claim this since the hard exploration games are the ones in table 2, which are different from the ones in table 1 (only Pitfall, MR and Private Eye coincide). Also the statement is only true for 2 the three that are hard exploration games (Pitfall and Private Eye).\n\n- (*)  \u201cfor smaller $\\beta$ we observe better performance on Pong and Beam Rider, but worse performance on all hard exploration games\u201d This is a strange result in my opinion. If I understood correctly, the base agent has $\\beta = 0.3$, which supposedly has been selected to be good on hard-exploration games. However, here only changing $\\beta$ slightly, reduces the performance on hard-exploration games (Pitfall and Private Eye) significantly. Is this due to the parameter being very sensitive? If so, I believe the authors should report how sensitive are the results to this parameter, specially, on the full hard-exploration games.\n\n- \u201csuperhuman performance on 3 games\u201d: which ones?\n\nComments on paragraph on \u201chard exploration games\u201d: \n- \u201cwith NGU(N=1)-RND \u2026.\u201d what do the authors mean by this? This seems to be the best setting for Pitfall but it is actually not using mixture of explorations and (I guess) neither RND?\n- Table 2 first row \u201cbest base\u201d what does this mean?\n\n\nMinor Comments:\n\n-In Equation 3 the squared distance is normalized by a running average. Why?\n\n- Right after Equation 3: \u201c\u2026 episodic reward can be found in Alg. 14\u201d . Probably meant Alg 1.\n\n- Similarly why the errors are normalized by a running average when computing \\alpha_t ?\n\n\n[1] MULEX: Disentangling Exploitation from Exploration in Deep RL ( Lucas Beyer et al )\n\n"}