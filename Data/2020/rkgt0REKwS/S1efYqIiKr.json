{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes a new loss function: curriculum loss, which is a meta-loss function that we can still specify an existing surrogate loss to use this loss function. This meta-loss function guarantees to be tighter than using a traditional pointwise-sum loss function as used in the empirical risk minimization framework. Intuitively, the proposed CL loss embed the sample selection process in the objective function. The authors suggest that it is robust against label corruption because it is tighter and provided promising experimental results.\n\n========================================================\nClarity:\nThe paper is well-written and easy to follow. \n\n========================================================\nSignificance:\nThe proposed paradigm is interesting and I am convinced that it can be useful under label noise. The experiments look promising. Future work about the analysis of NPCL/CL is also interesting to consider (e.g., which surrogate loss to use, rigorous theoretical guarantee, etc.). I think the proposed method is impactful. \n\n========================================================\nComments:\nThe proposed method is interesting and can give a tighter bound for any surrogate loss by using this method (CL). Moreover, the author suggested a simple extension of CL for label corruption (NPCL) and the performance is impressive. I would like to vote accept for this paper but the following point highly concerns me and I am not sure about the correctness (see the concern below). It is about the motivation not the proposed method.\n\nConcerns about motivation:\n\nI disagree with the original motivation of this paper. The authors used the result of Hu et al. 2018 to motivate the use of CL. To my knowledge, the main point raised by Hu et al. is as follows:\n\nIn classification, minimizing the adversarial risk yields the same solution as using the standard empirical risk. This suggests that minimizing the adversarial risk may not enhance the robustness of a classifier. Yet, it may still be useful when we consider regression (other settings but not classification). As a result, in classification, we should try other methods to make a robust classifier. Then, Hu et al. considered to utilize some kind of structural assumption to make a robust classifier. From their title: \"Does Distributionally Robust Supervised Learning Give Robust Classifiers?\", I think they suggested \"No\" as an answer and the discussion about 0-1 loss in the curriculum loss paper will be contradicted to them from the motivation perspective. \n\nFurthermore, regarding the adversarial risk, it is not focusing on the label noise but rather the noise of the feature-label pair, i.e, perturb (x,y) adversarially within an f-divergence ball. However, in my opinion, if we randomly flip the label of the data regardless of x (as the authors and existing work did in experiments when considering label corruption: symmetric, partial, etc.), we cannot be confident to state that the f-divergence between test distribution and corrupted training distribution is small under label noise. \n\nAnother point to motivate the use of 0-1 loss that the author mentioned is when we have outliers (Masnadi-Shirazi & Vasconcelos, 2009). This makes sense and this is a famous argument to discourage the use of too steep loss functions, e.g., exponential loss. I think this motivation is fine but it is not directly related to label corruption because we do not add out-of-distribution data but rather the label noise. Furthermore, the authors did not inject any outliers in the experiments in my understanding. I think this is totally no problem because we are focusing on label noise here, but this makes the motivation about outliers less important when we are talking about label noise.\n\nI think the most important direction both in theory and experiments about the robustness to label noise of the 0-1 loss is that 0-1 loss satisfies a \"symmetric property\", i.e., \\ell(z)+\\ell(-z) = Constant for a margin-based loss function in binary classification. Under symmetric label noise, \"the minimizer of the expected symmetric noise risk (a risk that the label is corrupted by coin flipping noise) is identical to the minimizer of the clean risk (normal risk)\". Although it is not empirically but the expected version, it gives a good insight about the advantage of directly minimizing 0-1 loss under label noise. This is first pointed out by \n\n[1] Manwani et al.: Noise tolerance under risk minimization, IEEE Transactions on Cybernetics 43 (2013) \n[2] Ghosh et al.: Making risk minimization tolerant to label noise Neurocomputing 160 (2015): 93-107. \n\n([1] focused on the 0-1 loss while [2] extended it to symmetric losses.)\n\nThen, it was extended to the multiclass loss by the following paper:\n\n[3] Ghosh et al.: Robust loss functions under label noise for deep neural networks. AAAI2017. \n\nThe advantage of symmetric losses is also discussed in this paper that the authors already cited in the symmetric noise experiment section. \n\n[4] van Rooyen et al.: Learning with symmetric label noise: The importance of being unhinged, NeurIPS2015\n\nThe advantage of the symmetric condition and 0-1 loss is also discussed in a more general noise scenario and more evaluation metrics:\n\n[5] van Rooyen et. al: An average classification algorithm. arXiv:1506.01520, 2015\n[6] Charoenphakdee et al.: On symmetric losses for learning from corrupted labels, ICML2019\n\nAnd the following paper that was also cited in the submitted work and compared:\n\n[7] Zhang and Sabuncu: Generalized cross-entropy loss for training deep neural networks with noisy labels, NeurIPS2018\n\nis also inspired by the robustness of the symmetric losses (including 0-1 loss). They argued that although the symmetric loss (MAE) for multiclass proposed by Ghosh AAAI2017 is robust, it is hard to train for challenging datasets, and they try to relevate this condition while making it easier to train.  This paper outperformed [7] and I think it is clearer and better to build a story along this line.\n\nIn short, here is the key message why I think the current motivation does not feel right. When we have noisy labeled data, instead of motivating the use of 0-1 loss by suggesting that \n\n\"If we have clean labeled data, minimizing the \"adversarial\" ERM risk using \"clean\" labeled data yields the same minimizer as minimizing the \"standard\" ERM risk using \"clean\" labeled data\",\n\nI believe the story to motivate the robustness of 0-1 loss under label noise should be \n\n\"If we have noisy labeled data, minimizing the \"standard\" or \"modified\" risk using \"noisy\" labeled data yields the same minimizer as minimizing the \"standard\" ERM risk using \"clean\" label data\"\n\nThe latter statement corresponds to the literature I suggested. \n\nApart from the motivation raised by the authors, as we can see from this curriculum loss paper, NPCL nicely outperformed generalized cross entropy loss in [7], which is impressive.\n\n========================================================\nDecision.\nI strongly feel that motivating the noise robustness of 0-1 loss by discussing about the adversarial risk (Hu et al.) is misleading. Nevertheless, I feel the proposed method itself makes a lot of sense and I am impressed by the results. If the author can convince me that using the current motivation of the paper is suitable, I am happy to improve the score. Another way is to agree to modify the motivation part. Given the experiments were done, it is not to difficult to change the motivation of the paper. At this point, I have decided to give a weak reject. \n\n========================================================\nQuestions:\n1. Is it straightforward to combine NPCL with Co-teaching/Mentornet/Co-teaching+?\n2. Does the traditional theory about classification-calibration (Zhang, 2004, Bartlett+, 2006) can guarantee the Bayes-optimal solution if we use NPCL?\n\n========================================================\nMinor comments:\n1. Page 9: Both our NPCL and Generalized Cross Entropy(GCE) << space missing between Entropy and (\n"}