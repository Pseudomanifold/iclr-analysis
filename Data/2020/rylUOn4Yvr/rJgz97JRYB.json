{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. The method is strongly related to reweighting training examples, where alpha and beta define the shape of this weighting as a function of the model's prediction (i.e., p_i). Experiments show that the proposed method achieves competitive results on several standard benchmarks for noisy-labelled data.\n\nComments:\n- The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper-parameter search, which is not very practical and can lead to overfitting the test set.\n- The paper in general is easy to follow, but the paper is not very rigorous or clear on some important concepts. For example: \n     * No clear mathematical definition of emphasis focus and spread\n     * The term \"semantically abnormal examples\" should be defined in the main text.\n     * It is not so clear what it means to \"babyset\" emphasis focus and spread.\n     * I don't understand what Eq. 6 is supposed to tell.\n     * What are the \\dots in equation \n- The experiments are very thorough and the results are very good, but I have few clarifying questions:\n     * The procedure for choosing beta/gamma is not clear, and I see that for every experiment those values change.\n     * It would be nice if the CIFAR-10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. For example, GCE results in (Zhang & Sabuncu 2018) are much that the reported ones. While it seems that you're using GoogLeNet V1 architecture similar to Jiang et al. 2018, it's not clear which experimental setting you are comparing against.\n     * Can you be more specific what do you mean by \"with a little effort for optimizing beta and gamma\" in caption of Table 5?\n\nMinor:\n     * Grammer mistake: \"what training examples...focused *on*...\"\n     * Citations should be done with parentheses\n"}