{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nThe paper proposes a new way to evaluate generative models that don't have tractable likelihoods, such as VAEs or GANs. Such generative models are composed of a prior over latent variables and a decoder that maps latent variables to data. The idea is to evaluate a trained model in terms of the best (lossy) compression rate that can be achieved by encoding a datapoint (e.g. an image) into the latent space, as a function of a permitted distortion between the datapoint and its reconstruction after decoding. The paper describes a method that estimates an upper bound on this rate-distortion curve using annealed importance sampling. The method is applied in evaluating and comparing a few VAE, GAN and AAE architectures on images (MNIST and CIFAR-10).\n\nOverall evaluation:\n\nThis is a very good paper, and I'm happy to recommend it for acceptance.\n\nThe problem considered (evaluating generative models with intractable likelihoods) is an interesting and important one. In general, such models are hard to evaluate and compare with each other. The paper proposes a new method for evaluating them, which can also improve our understanding of these models and potentially diagnose them in practice.\n\nThe method is well-motivated and backed by theoretical results. One clever aspect of the method is the way annealed importance sampling is used to approximate the rate-distortion curve: instead of sampling separately the rate for each distortion level with a different AIS run, a single AIS run is used to approximate the whole curve. This is done by taking the various points on the curve to correspond to intermediate distributions in AIS, which is quite clever.\n\nThe paper is well written, precise, and contains sufficient theoretical background to motivate the method.\n\nThe experiments are done carefully, and the results are interesting. I found particularly interesting the fact that VAEs behave differently to GANs (in terms of their rate-distortion tradeoff) when the dimensionality of the latent space is increased.\n\nSome discussion and critical feedback:\n\nI found the paper too long (10 full pages). I appreciate the detail, precision and depth of explanation, but I think it would be good to reduce the amount of text if possible.\n\nI though that the introduction was too specific to VAEs/GANs and to image modelling, which may give the impression that these are the main models/tasks of interest. I understand that these are the models and tasks that the paper is interested in, but I think it would be better if the introduction acknowledged the existence of other types of generative models (e.g. likelihood-based models such as autoregressive models and normalizing flows) and were less specific to image applications.\n\nProposition 3 is true almost by definition, since R_p is defined to be the minimum of all rate-distortion curves. I wonder if something more informative can be shown here. For example, my understanding is that the reason R^{AIS}_p is not optimal is due to the bias of the importance-sampling estimator in eq. (12). Since this bias asymptotically goes to zero, I suspect that R^{AIS}_p may become equal to R_p for M -> infinity, and perhaps the bound improves monotonically as M increases.\n\nIf my understanding is correct, the reason for the inequality in proposition 4 is that log\\hat{Z} is a biased estimate of logZ (due to Jensen's inequality) despite \\hat{Z} being an unbiased estimate of Z. If that's all there is to it, the proof in the appendix, although precise, is a bit of an overkill. It also seems to me that the log\\hat{Z} bias also approaches zero as M -> infinity, so this inequality maybe also becomes an equality asymptotically.\n\nIn future work, it would be interesting to also evaluate flow-based models (such as Glow). Since these models give exact likelihoods, it may be good to observe how the evaluation based on rate-distortion curves compares with a likelihood-based evaluation.\n\nSection 6 attributes the performance drop of VAEs in the low-rate regime to the \"holes problem\". If that's true, then I would expect the situation to be improved with more flexible prior / posterior models. What prior / posterior models were used in the experiments? If only diagonal Gaussians were used, then it would be interesting to see whether more flexible priors / posteriors such as normalizing flows would change the results.\n\nMinor corrections and suggestions for improvement:\n\n\"For continuous inputs, the metric is often dominated by the fine-grained distribution over pixels rather than the high-level structure\"\nThis statement is specific to images, not continuous inputs in general.\n\nOn the quantitative analysis of deep belief networks, https://dl.acm.org/citation.cfm?id=1390266\nis an older example of AIS used to evaluate generative models that could be cited.\n\nI think it would be better to drop z_0 and T_0 in equations (1) and (2), and have z_1 sampled from p_0 directly, to make the equations consistent with the equations that follow afterwards.\n\n\"using a latent variable z with a fixed prior distribution\"\nIn VAEs the prior can also be learned, it doesn't have to be fixed (this may in fact help alleviate the holes problem).\n\nIt could be mentioned that the objective in Eq. (9) has the same form as a generalized VAE objective, such as the one used by beta-VAE, https://openreview.net/forum?id=Sy2fzU9gl\n\nAt the bottom of page 4 and page 5, R_p(D) uses a different font for R than the rest of the paper.\n\nFig. 1 is too small to read on paper, I had to zoom in using the pdf in order to see it properly.\n\nLast paragraph of section 4 specifically mentions images, even though it doesn't need to (datapoints don't have to be images).\n\nThe last two paragraphs of page 7 contain a few grammatical mistakes:\n- fixing the architecture of neural network --> fixing the architecture of the neural network\n- as the result --> as a result\n- there exist a rate distortion code for any rate distortion pairs  -->  there exists a rate distortion code for any rate distortion pair\n- While in our definition of rate distortion -->Whereas, in out definition of rate distortion\n\nTop of page 8, use \\citet instead of \\citep where appropriate.\n\nCapitalize names and acronyms in references, such as ELBO, GAN, MMD, VAE, Bayes, Monte Carlo, etc."}