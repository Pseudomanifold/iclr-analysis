{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This work proposes a self-supervised objective to learn phrase/sentence representation. A text span is first encoded and clustered, and then used to predict neighboring tokens, minimizing mean square error between their representations. Experiments show that the proposed method outperform baselines on unsupervised text similarity tasks. \n\nAlthough the idea is very straightforward, the paper did not clearly present it. The approach appears very similar to many existing works, e.g., the only difference from skip-thought seems to be that here the orders of the surrounding context are ignored. Maybe the authors can justify their technical contribution. As for the experiments, I'm no expert in this specific field, but I would be really surprised if this work is among the most competitive ones, and this paper does not compare to many recent efforts, such as [1], which I found by a quick Google search. Last but not least, the paper does not do enough in explaining why the objective does very well in text similarity tasks, and makes no attempt in evaluating it on other downstream tasks.\n\nI do not recommend that the paper is accepted at its current status.\n\n[1] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. https://arxiv.org/abs/1908.10084"}