{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "A one-line summary of the paper: The paper proposes a sequence-to-embedding based neural approach with non-negative sparse coding loss to learn multiple mode representations of a given phrase or sentence.\n\nOVERALL COMMENT\n------------------------------\nI didn\u2019t think that the paper was very clearly written. While the proposal for obtaining multi-facet embeddings might be important, neither the task nor the experiments were motivated well enough. The use of NNSC seems promising but there are various concerns about the experimental details. I believe the following (detailed comments) should be clarified for a better understanding of the tasks, experiments and the paper in general. I had trouble grasping the major contributions other than the model.\n\nPlease correct me if I am wrong somewhere in my understanding.\n\nOverall, due to a lack of motivation and the below-mentioned concerns, it\u2019s hard for me to envision this paper offering a significant contribution to the community.\n\nDue to the above reasons, I am giving a score of 3. \n\nDETAILED COMMENTS:\n--------------------------------\nConcerns/Typo:\nPage 1: The first paragraph of the introduction mentions examples of completely unsupervised representation learning techniques. BERT uses minor supervision in the form of Next Sentence Prediction.\nPage 1: |E| should be the embedding dimension and not the \u201cnumber of embedding dimension\u201d\nPage 2: Figure 1: Stop words are shown to be important here, although in the paper you mention that stop words have been removed.\nPage 3: Please clarify the procedure for getting \u2018co-occurring words\u2019 from similar sequences. This seems like an important aspect of the paper that hasn\u2019t been focused upon.\nPage 4: Normalization makes the cosine distance between two words become half of their Euclidean distance -> half of square of their Euclidean distance.\nBaselines for sentence and phrase similarity: While token level embeddings obtained using BERT are useful, averaging token embeddings or using [CLS] embeddings without finetuning are generally not strong enough. A stronger baseline would be to use sentence encoders like [2] and/or [3].\nWhile table 1 is important to show the various \u2018facets\u2019 learned by the model, the other experiments are not motivated well enough. Why should obtaining multiple facets or learning representations using multiple facets help in better phrase similarity, sentence similarity and so on?\nWriting: The paper is difficult to read. There are multiple sentences with length  > 50  which are very difficult to comprehend. \n\nQuestions:\nPage 3: Is the only difference between the training signals for phrase and sentence, that of static and dynamic window size respectively? Or is there more to it?\nPage 4: Is the process of optimization alternating between optimization of M^{O_t} and parameters or just a one-time optimization of M^{O_t} followed by multiple backpropagations. If it is the former, it would be good but not necessary to show that the algorithm will tend to converge.\nPage 5:  \u2018Similar to BERT, we treat the embedding <eos> as sentence representation\u2019.\n-> BERT uses [CLS] embedding for that.\nWhat is the need of a transformer-based decoder since the \u201clatent modes\u201d are neither learned auto-regressively nor decoder based self-attention would be useful since the modes might capture distinct topics. If they capture very similar topics, then one concern might be the utility of having different phrase embeddings in the first place. Since it is a non-auto-regressive decoder, something as simple as a Neural network could work ( or as proposed in [1]). Having an LSTM or any auto-regressive decoder is an overkill.\nPage 7: First paragraph about SC. What is the reconstruction error formulation? In its vanilla L_2, reconstruction error should be symmetric function. What is the need of having both the error functions? Also, the ordering of topics (multi-fact phrase embeddings: columns of F_u) should matter here. If both of the phrases represent similar \u201cfacets\u201d but in a different order, then the symmetric distance might become larger than required.\nPage 9: Hypernymy detection: \u2018...often reconstruct the embeddings of its hypernym better than the other way around\u2019 -> Please provide some justification as to why that happens? \n\nI am open to updating my scores if the concerns/questions are addressed.  That being said, it is important to majorly revamp the write-up of the paper.\n\nREFERENCES:\n----------------------\n[1] Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).\n[2] Cer, Daniel, et al. \"Universal sentence encoder.\" arXiv preprint arXiv:1803.11175 (2018).\n[3] Conneau, Alexis, et al. \"Supervised learning of universal sentence representations from natural language inference data.\" arXiv preprint arXiv:1705.02364 (2017)."}