{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposes a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. The idea is simple and reasonable and the results are promising.\n\nI have several questions about the paper:\n\t1. \"This enables a fair comparison, because it ensures that each model sees roughly the same number of training examples.\" This is not a fair comparison. Note that those models are of very different size, and thus they may need different numbers of samples for training. For example, a 1-1 model should need much less data for training than a 6-6 model. If the number of training samples is ok for the 1-1 model, it might be insufficient for the 6-6 model. Therefore, I think development set is necessary for a fair comparison.\n\n\t2. I don't understand Eq. (3). What do x and y_k mean in this equation? Are they corresponding to x^I and y^i_k in Eq (2)? However, y^i_k in Eq. (2) is a translation, i.e., a text sentence, while y_k in Eq. (3) looks like a number in [0,1].\n"}