{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors make good points, starting from the exposure bias and label bias suffered by the mainstream neural auto-regressive models.\nResidual EBMs are defined and trained using NCE. Experiments on two large language modeling datasets show that residual EBMs yield lower perplexity and generation via importance sampling is of higher quality, compared to locally normalized baselines.\n\nIn generally, the paper is well motivated and interesting. But I have some concerns.\n\n1. Missing important relevant references.\n\nEBMs (a.k.a. un-normalized models, random fields) have been successfully developed in language modeling in recent years. A large body of this paper has been studied in [5,6], including the model and the NCE estimation method. The model Eq.(2) is exactly the model in [5], defining the model in the form of exponential tilting of a reference distribution.\nConnecting and comparing to these previous works are needed.\n\n[1] R. Rosenfeld, S. F. Chen, and X. Zhu, \u201cWhole-sentence exponential language models: a vehicle for linguistic-statistical integration,\u201d Computer Speech & Language,  2001.\n[2] B. Wang, Z. Ou, and Z. Tan, \u201cTrans-dimensional random fields for language modeling,\u201d ACL, 2015.\n[3] B. Wang, Z. Ou, and Z. Tan, \u201cLearning transdimensional random fields with applications to language modeling,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.\n[4] B. Wang and Z. Ou, \u201cLanguage modeling with neural trans-dimensional random fields,\u201d IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.\n[5] B. Wang and Z. Ou, \u201cLearning neural trans-dimensional random field language models with noise-contrastive estimation,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n[6] B. Wang and Z. Ou, \u201cImproved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation,\u201d IEEE Spoken Language Technology Workshop (SLT), 2018.\n\n2. I am a little bit concerned that the theoretical contribution seems weak. \nThough Eq. (4) and (5) seem to be novel, I am not sure whether such a contribution is substantial enough to motivate acceptance.\n\nI'm happy to adjust the score if the paper can be better placed in the literature and the authors take efforts to improve the paper."}