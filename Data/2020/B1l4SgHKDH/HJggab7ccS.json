{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work is an interesting extension of Gutmann and Hyvarinen (2010), where the parametric model is the combination of a noise model (language model) and an energy function (residual energy), so the difference of parametric model and the noise model cancels out the noise model. Therefore optimizing (3) under some conditions converges to a set of parameters of the parametric model (P\\theta(x) here) that best describes the data.\n\nOne important assumption of Gutmann and Hyvarinen (2010) is that there exists a set of optimum parameters for the parametric model such that the probability of data and the parametric models match for these optimum parameters. This should be mentioned in Theorem-1.  \n\nDoes Theorem-1 need extra parameters to act as a normalization constant in order for the theorem to hold at the optimum?\nlog P_lm(x) - E(x) + const = log p_data\n\nTo sample from the model, the authors first sample from the language model and re-sample it with respect to the energy values of the residual model.\n\n\nTo compute the perplexity, they have given an upperbound and lowerboud for the partition function based on number samples in Theorem 2, but I haven't checked the correction of the bounds.  They also factorize the joint model in auto-regressive factorization to compute the perplexity by approximate marginalizing. \n\n\nAs mentioned in Section 5, this approach heavily depends on a strong pretrained language model. \n\nHave you considered improving the language model during training?\n\nThe described idea is simple and effective and I really liked it."}