{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends recent work by Khemakhem et al on nonlinear ICA to allow for unknown number of generative factors. This is tackling an important problem in the field of generative modeling, where one would like to extract the generative factors of a dataset that independently control its features (i.e. disentanglement).\n\nThe paper is very clearly written, does a great job at motivating the problem and presenting the recent results from Khemakhem et al, before extending them with some simple theorems and demonstrating their application on toy data + EMNIST.\nI think that this research direction is extremely promising, and obtaining a theoretical understanding of when/why disentangling could work would be particularly valuable to the field, and I would lean towards acceptance so that this work gets more attention.\n\nIt is slightly surprising however that their empirical results seem to indicate that these theoretical conditions do not seem to be necessary, which should be investigated further (and might help illustrate the dichotomy in some claims and results obtained in the disentangling literature recently).\n\n1.\tIt was unclear to me how one should/would decide what to use for $u$ or what to leave out to be factorized by the method. \n\tThis may be out of scope for this current work, but one way to answer that would be to leverage datasets with more fully labelled factorised data (e.g. dSprites [1]) and present how one should leverage these with $u$, in order to identify the original generative factors.\n2.\tRelated, using EMNIST was interesting, but given the lack of \u201caccepted\u201d generative factors to be recovered, it is hard to tell if the 22 variables found are \u201ccorrect\u201d or more similar to using a generative model which would entangled the data more (and hence would falsely introduce extra latent variables).\n3.\tThe toy dataset, with its random RealNVP network to produce the data, was less \u201cmixed\u201d than I expected, looking at Figure 2B. In its projected view, the clusters are still rather easy to identify by eye, which surprised me somehow? Could you comment a bit more on how difficult is the task, or present a VAE baseline that would fail to explain that data?\n4.\tI did not see how the threshold for selecting 22 latent variables on EMNIST was set? Was the 23rd latent variable significantly less informative? The spectrum on Figure 3A was not precise enough to assess this fairly and the single mention of \u201cmeasured by the standard deviations of test data transformed to the latent space\u201d was too vague to reverse-engineer the decision.\n5.\tIt was interesting to read about the observations of when this method should fail. It would be interesting if a dataset with explicit \u201cgaps\u201d would be constructed to analyze this case.\n6.\tFigure 4 might benefit from mentioning \u201cwhat\u201d each variable controls for directly in their individual caption / on top of them, instead of having to read this through in the full figure caption.\n7.\tThe current model in the end is modeling the latent state using a mixture of gaussians (although these now have a theoretical connection to the true generative factors). How much does this differ to existing generative models using VAEs with a mixture of gaussian prior [2, 3]?\n\n\nReferences:\n[1] dSprites dataset: https://github.com/deepmind/dsprites-dataset \n[2] https://arxiv.org/abs/1611.02648\n[3] https://arxiv.org/abs/1902.03717\n"}