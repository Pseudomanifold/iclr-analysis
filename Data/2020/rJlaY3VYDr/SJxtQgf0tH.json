{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:  Inspired by recent methods that aim to make neural networks (NNs) robust by training on both in- and out-of-distribution (OOD) data [Liang et al., 2017; Lee et al., 2018], this paper addresses the question of if the same level of robustness can be obtained without training on an OOD set.  The authors propose introducing an auxiliary variable $d_{\\text{in}}$ to directly model whether a given input originates from the training data or not.  Of course, training a function to predict $d_{\\text{in}}$ directly would require OOD data or at least a simulation of OOD from in-distribution data.  To bypass this obstacle, the authors make an argument that parametrizing the softmax input with two functions---$f_{c}(x) = h_{c}(x)/g(x)$---is reminiscent of computing the conditional probability $p(y|d_{\\text{in}}, x) = p(y, d_{\\text{in}} | x) / p(d_{\\text{in}}| x)$, with $h$ representing the joint density and $g$ the marginal.  The paper also proposes an input perturbation strategy similar to Liang et al.\u2019s [2017], the difference being the perturbation magnitude is set by maximizing $h$ instead of on OOD data.  Experiments are reported showing performance on OOD detection, OOD detection when having access to OOD data during training vs when not, the effect of different choices of $h$, the effect of the proposed data augmentation strategy, the effect of number of samples, classes, and architecture parameters, and the effect of the distributional shifts being semantic vs non-semantic.       \n\nPros:  Making deep learning methods robust to distributional shift is essential for their safe deployment.  As the paper notes, many previously proposed methodologies require access to OOD data.  However, this reliance on OOD data not only adds implementation costs (extra memory and computation) but also makes the choice of OOD set a crucial modeling decision.  I am unaware of any rigorous guidance for selecting a representative OOD set for a given training set.  This paper makes some incremental but important contributions in relieving ODIN of its dependence on OOD data.  These proposals are supported with a fairly thorough experimental investigation.  The paper reports not only comparisons against sensible baselines but also performs ablation studies to isolate the effect of parametrization choices and their data augmentation strategy.    \n\nCons:  My most significant critique of the paper is that it exaggerates its contributions and ignores a large body of previous work.  There are two falsely claimed contributions in particular.  The first is that the paper is the first to consider OOD detection without training with OOD data: \u201cTo our knowledge, this is the first study focused on such a minimal setting, investigating the feasibility of learning out-of-distribution detection without out-of-distribution data\u201d (p 2).  This is patently false.  Classification with a rejection option has been studied for many years, and much of this work does not assume access to an OOD set.  For just one example, Hellman [1970] considers k-nearest neighbor classification with a rejection option and requires no OOD set.  Perhaps the authors meant this as a contribution w.r.t. neural networks, but that is not the case either.  Cordella et al. [1995] studied rejection rules for MLPs, and Geifman & El-Yaniv [2017] studied selective classification for deep networks.  To the best of my knowledge, neither of these works require OOD data for training (although I am not intimately familiar with the methodology). \n\nThe second falsely claimed contribution is the use of an auxiliary variable to represent that the data is in-distribution: \u201cWe specifically add a binary variable, representing whether the data is in-distribution or not\u201d (p 2).  It surprises me that the authors would think that this idea hadn\u2019t been proposed before, given its a rather obvious thing to try.  The idea dates back at least to Cortes et al. [2016]; see Section 2.1 on \u201cGeneral Rejection Models.\u201d  Geifman & El-Yaniv [2017] use the same framework in the deep learning setting, defining a binary \u201cselection function.\u201d  Given these omissions, the paper cannot be accepted in its current form.  It requires substantial revisions: discussion of this previous work and modification of the claimed contributions.  I think a proper characterization of the work would to be to call it an extension of ODIN: using a selection function to parametrize ODIN\u2019s temperature.  \n\nYet this brings another question to the forefront: why is $h(x)$ used to determine rejection when $g(x)$ is supposed to be a proxy for $p(d_{\\text{in}} | x)$ (Equation 4)?  In the public discussion, the authors state that $g(x)$ does indeed work to a degree, but just not as well.  This seems to call into question the foundational motivations of the procedure, and the authors should address this by including experiments comparing $h$ and $g$.  \n\nFinal evaluation:  The paper over-claims its contributions and ignores some very related work (e.g. [Geifman & El-Yaniv, NeurIPS 2017]), thus necessitating substantial revisions to the text.  Moreover, the $g(x)$ selection function has not been demonstrated to behave as the core motivation for the work implies it should---namely, modeling $p(d_{\\text{in}} | x)$.  \n\n\n\nReferences\n\nCordella, Luigi Pietro, et al. \"A method for improving classification reliability of multilayer perceptrons.\" IEEE Transactions on Neural Networks 6.5 (1995): 1140-1147.\n\nCortes, Corinna, Giulia DeSalvo, and Mehryar Mohri. \"Learning with rejection.\" International Conference on Algorithmic Learning Theory. Springer, Cham, 2016.\n\nGeifman, Yonatan, and Ran El-Yaniv. \"Selective classification for deep neural networks.\" Advances in neural information processing systems. 2017.\n\nHellman, Martin E. \"The nearest neighbor classification rule with a reject option.\" IEEE Transactions on Systems Science and Cybernetics 6.3 (1970): 179-185."}