{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper proposes a method for detecting out-of-distribution (OOD) data. Unlike most methods in this direction, this paper considers the scenario where a sample of out-of-distribution is not observed. Thus, the goal is detecting the out-of-distribution data using solely in-of-distribution data. First, the authors proposed to use input preprocessing similarly to ODIN (Liang et al., 2017) but the proposed scheme does not require out-of-distribution data. The way to search a parameter is different from the existing work (Liang et al., 2017). The proposed scheme looks simple and highly effective in the experiments. They also propose to decompose the learned function $f_i(x)$ into two parts for making a good confidence score function. Finally, they illustrate a good performance on OOD detection using their proposed methods (combining the decomposition and preprocessing).\n\n========================================================\nClarity:\nOverall, I found this paper is well-written and easy to follow. Only Section 3.1 highly concerns me on the clarity and correctness of the paper.\n\nThe part that I am most concern about is the decomposed confidence, I am not sure that the argument of the proposed method about confidence decomposition is correct. If the authors can convince me that it is correct, I will consider improving the score.\n========================================================\nComments:\n\nOn the claim of the novelty of the problem setting:\nThis paper claims that this paper is the first study focused on a minimal setting, where I think it means the setting that we do not assume that we have OOD sample when training a model. However, there already are existing works on this issue. And it is more important to discuss them in the Related Methods section (2.1) rather than mentioning the most related works as the work that requires true OOD sample when training. I read the discussion in the comment section on how the authors tried to state the reason why they think it is novel, but I think it is not sufficient. It is the freedom to choose how to solve this problem, e.g., one may try to construct pseudo-OOD data from in-distribution data. But the core setting is the same, that is, OOD detection without OOD data. The authors may instead criticize that the method that generates pseudo-OOD from in-distribution may be unreliable, biased, and that's why other methods perform better. For the discussion about Hyperparameter-free, to me, it is simply a unique feature that Techapanurak & Okatani (2019) wanted to highlight as a contribution. Thus, the author may suggest a way to criticize that paper, e.g., it's better to tune so we can do well on the dataset we are interested in instead of saying that hyperparameter-free makes the setting different. Not only these two works I mentioned, I believe OOD detection itself has a long history and I do not think it is important to state that this paper is the first one to consider such a setting. \n\nOn the confidence decomposition Section (3.1):\n\nOn why the network is overconfident:\nIn 3.1, the authors attempted to illustrate why neural networks may give an overconfident value for OOD data. I strongly agree that it is an important problem to discuss. In the example in 3.1, it is understandable that in Eq. (4), both values of numerator and denominator can be low when using OOD data. However, from my viewpoint, Eq. (4) suggests that in practice, the left-hand side of Eq. (4) can be unstable when x comes from OOD data (since the denominator can be very small, so as the numerator). And we do not have much data in that region, which makes p(y|x) estimation difficult in that region. From this logic, I think this equation does not only suggests overconfident but also it can tend to be underconfident too, especially in the semantic shift scenario. Thus this Eq. (4) does not suggest that the network tends to be overconfident but tend to have an extreme value (please correct me if I misunderstood). Therefore, it would be better to focus on discussing this issue of stability rather than giving a bit ad-hoc example 0.09/0.1 = 0.9 (overconfident). Although underconfidence can be easily handled because those data are easy to detect them as out-of-distribution.\n\nOn the influence of Eq. (4) to inspire $f_i(x) = \\frac{h_i(x)}{g(x)}$:\nI cannot see how $g(x)$ will relate to the denominator of Eq. (4). It is true that $g(x)$ is a function that is not class-specific, similarly to the denominator of Eq. (4). However, the denominator in Eq. (4) is just simply marginalized y out by taking a summation of all class-specific function. Thus, I think the only way to relate Eq. (4) to Eq. (5) is $\\sum_{j=1}^C \\exp f_j(x)$ is the unnormalized version of the denominator of Eq. (4), while $\\frac{h_i(x)}{g(x)}$ is an unnormalized version of the numerator. Thus, I am not convince that Eq. (4) motivates $f_i(x) = \\frac{h_i(x)}{g(x)}$.\n\n\nOn the input preprocessing Section (3.2):\nI found the method for selecting the perturbation magnitude is very interesting and this is the most thing I like about this paper. It made me convinced that input preprocessing is highly useful and impactful for tacking OOD detection without OOD data (while ODIN already suggested this fact in the context where we have OOD sample). It enhances the performance of almost all cases as we can see from Figure 2. I have a positive impression on this part. It would be nice to see the performance of Deconf-C, which is almost identical to Techapanurak & Okatani (2019) in Table 1 to highlight the improvement from this preprocessing for different OOD datasets instead of averaging them all (as Figure 2). I think it is not too difficult to modify this part to highlight a contribution of the preprocessing part (currently only Figure 2 highlights the usefulness of it).\n\nOn experiments: \nIt is also important to see a standard deviation of the results. Especially if we only run only a few times (three times in this context). If the standard deviation is high, it is highly suggested to run more times to get a reliable result. In addition, we may use a better criterion to bold a score (e.g., one-sided t-test). I also would like to know if the existing method has a higher standard deviation due to less stability when g(x) is not 1 or a constant (T) as they were in most existing approaches. Other than this point, I think the authors did a great job to visualize and show the effectiveness of decomposing a confidence score and the advantage of input preprocessing. Experiments on semantic/non-semantic shift is also interesting.\n========================================================\nDecision:\nAlthough this paper proposed an interesting simple yet effective way to preprocess the data combining with decomposing a confidence function into two parts, I feel that the discussion of the relationship with existing methods and the clarification of the novelty have to be improved. Also, I disagree with the explanation of decomposed confidence part, and it simply looks like a scaling factor that depends on x to me.  Finally, reviewers are suggested to put a higher standard for evaluation for a paper more than 9 pages. For these reasons, in the current form, I vote to reject for this paper. If I am convinced that the discussion about decomposed confidence is valid, I will consider improving the score.\n========================================================\nMinor comments:\n1. Does \\log between gradient and S exist in Eq. (7) like the paper by Liang et al., 2017 ?\n2. typo: Section 4.3: We therfore -> We therefore\n========================================================\nOn the similar work by Techapanurak & Okatani (2019):\nI am aware that the two works are concurrent and I still treat this part as the authors' contribution in my evaluation.\n\nIn my view, both works are are highly related.  I am aware that the work by Techapanurak & Okatani (2019) is also very recent (25 May 2019: arXiv) and the authors suggested it to be a concurrent work. It is also nice and I appreciate that the authors cited them. But since the authors discussed a little bit about their work, I think it is fruitful to discuss them in detail and point out the difference clearly, for example in the related work section. \n\nHere are four reasons I think why they are highly related. \n \n1. Same setting: OOD detection without OOD data\n\n2. The authors attempted to clarify the difference and pointed out that the proposed framework is more general than Techapanurak & Okatani (2019). However, in my view, Techapanurak & Okatani also proposed to decompose the function into two parts, where $s$ (or $s(x)$) in Eq. (3) of Techapanurak & Okatani (2019) is strictly the same as $\\frac{1}{g(x)}$ in this submitted work. Even before Techapanurak & Okatani (2019), people have been using this scaling parameter to improve the class posterior estimation (perhaps s = 1/T is the most common, although it does not depend on x). For the numerator $h_i(x)$, Techapanurak & Okatani (2019) proposed to change the standard $h_i^I(x)$ to $h_i^C(x)$. Although they did not write clearly that $h_i^I(x)$ can be something other than $h_i^C(x)$, the fact that they changed it is straightforward to see that we are able to generalize it and try other $h_i^I(x)$. Thus, the novelty of the generalization of Techapanurak & Okatani (2019) that this paper suggests is modest. \n\n3. The choice of $g(x)$ in this paper and $s(x)$ in Techapanurak & Okatani (2019) are almost identical. It is very slightly different in my view (not sure if it changes the performance). In this context, if I understand correctly, Techapanurak & Okatani (2019) used $g(x) = \\frac{1}{exp(BN(linear-transformation))}$ while this paper proposed $\\frac{1}{(1+exp(-BN(linear-transformation))}$.\n\n4. The $h_i(x)$ that works best is the $h_i^C(x)$ proposed by Techapanurak & Okatani (2019). \n\n"}