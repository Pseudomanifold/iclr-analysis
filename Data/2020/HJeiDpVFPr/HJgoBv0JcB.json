{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows how to enforce and learn non-Euclidean\n(semi-)norms with neural networks.\nThis is a promising direction for the community as part\nof a larger direction of understanding how to do better\nmodeling in domains that naturally have non-Euclidean\ngeometries.\nThis paper shows convincing experiments for modeling\ngraph distances and in the multi-goal reinforcement\nlearning setting.\n\nOne clarification I would like is related to some\ntasks inherantly not having a Euclidean embedding.\nAre there works that theoretically/empirically characterize\nhow bad this assumption can be for some problem classes?\nEven though some tasks are impossible to embed exactly\ninto a Euclidean space, are there sometimes properties\nthat, given a high enough latent dimensionality,\nthey can be reasonably approximated?\nAnd in the table of Figure 1, whta dimension n was used\nfor the Euclidean norm?"}