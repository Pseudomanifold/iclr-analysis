{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication.\nThe theoretical part (secion 2) is hard to follow. For example, the authors introduce in 2.1 a mapping M from vertices to labels. Then in Theorem 1, this mapping M indeed maps features of the vertices to labels. But then waht is the meaning of a L2 norms between features? At this point, I had a look in the experiment section to see what features are considered in practice. In section 4.1, it is written for the citation networks: 'each node has a sparse bag-of-words feature vector' or in the coauthor networks: 'Node features represent paper keywords for each author's paper'. How do you relate these claims to your theoretical analysis? Things get even worse in section 2.3, where derivatives with respect to initial feature vector are taken. How do you take a derivative with respect to a bag of words?\nEquation (5) is not clear at all, we need to read the end of this section to understand that A^* is constrained to have the same support as the adjacency graph and computed as a function of the node features. The authors shuold also define clearly y_hat in (5)."}