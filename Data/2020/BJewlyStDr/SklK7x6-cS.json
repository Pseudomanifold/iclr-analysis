{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper evaluates the recently proposed exploration methods that achieve ground-breaking performance in the difficult exploration problem, Montezuma's Revenge.  The authors combine Rainbow with different exploration methods, such as count-based bonus methods, curiosity-driven methods, and noisy networks. Results show that these methods fail to beat epsilon-greedy on other Atari games, even if the parameters of these methods are tuned. \n\nThe paper is very well written, and they claim that evaluating the exploration methods on the Montezuma's Revenge and tuning parameters on this environment are not suitable for the total ALE environments. The claim is very interesting and important for the exploration community. \n\nTo support their claim, the authors firstly compare bonus exploration methods, noisy networks, and epsilon-greedy on hard exploration games. Then results in easy games and other games are presented. The results are very impressive.\n\nQuestion:\n(1) The authors compared these methods based on Rainbow, which employs many techniques, such as the prioritized replay buffer. Can you show the comparison results on Rainbow without the prioritized replay buffer? It will strengthen the understanding of these exploration methods.\n(2) The noisy networks perform well on most games, while bonus methods perform well on hard games. Is there any combination method to achieve better performance?"}