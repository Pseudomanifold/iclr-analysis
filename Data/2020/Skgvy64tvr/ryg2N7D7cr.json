{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper suggests using the activation function k-Winners-Take-All (k-WTA) in deep neural networks to enhance the performance of adversarial defense. Their experiments show that the robustness is improved when they simply change the activation function to k-WTA. They also give reasonable theoretical analysis for their approach. \n\nI find that the idea is simple and elegant. Since they only change the activation function, their approach can be easily applied to almost all network structures and training processes. Their experiments show that the robust accuracies are significantly improved on all evaluated methods when they use the k-WTA activation function. I also appreciate their theoretical analysis. They show that k-WTA makes the network very discontinuous with respect to the input x, and thus the adversary could not get useful gradient information. In contrast, if the network is wide enough, then the discontinuities with respect to the weights w is sparse. This is why the network is still trainable though itself is not continuous.\n\nThe paper is also well-written and easy to follow. I recommend the acceptance of the paper.\n\nOne limitation of this paper is that their approach mainly focuses on defending gradient based attack. But I agree that the gradient based attack is currently almost the best attack method.\n\nA minor question:\n- How do we choose k in general? What is the behaviour for different k?\n"}