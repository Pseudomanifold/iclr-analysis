{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1318", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work utilizes the kNN method on dense vectors to augment the LMs. The method is simple and straightforward, meanwhile, the performance seems great if only in terms of PPL.\nThree of my most concerns:\n1)\tIt seems that this approach heavily relies on the similarity of context distribution between the training and test set. Intuitively, higher performance will be achieved with more similar examples between training and test set. This question should be discussed more in this work. This similarity cannot always satisfied in practice, I thus quite doubt the proposed method can work for general case.\n2)\tThe evaluation is only done for PPL, I notice the LM was trained in a corpus scale as pre-trained BERT, though none of real downstream tasks were evaluated like BERT. Expect to see some MRC or NLI results with the proposed LM.\n3)\tFurthermore, though FAISS is very fast, it is hard to get great results with only a small datastore which makes the retrieving slow. So it seems not suitable for tasks such as generations but maybe open-domain QA can be the scene for this method. It would be great if there are some experiments on such tasks, and also combining with models such as BERT could be much better and convincing.\n\nQuestions\nWhat about other distance functions such as cosine distance? The author only said L2 is better but there is no analysis on it.\n"}