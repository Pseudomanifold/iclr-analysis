{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "The paper proposes a new method for defending against stealing attacks.\n\nPositives:\n1) The paper was very readable and clear.\n2) The proposed method is straightforward and well motivated.\n3) The authors included a good amount of experimental results. \n\n\nConcerns: \n1) You note that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems to decrease as the dataset becomes more difficult (i.e. CIFAR100). I\u2019m concerned that this may indicate that the attackers are generally weak and this threat model may not be very serious. Overall, I\u2019m skeptical of this threat model - the attackers require a very large number of queries, and don\u2019t achieve great results on difficult datasets. Including results on a dataset like ImageNet would be nice. \n2) How long does this optimization procedure take? It seems possibly unreasonable for the victim to implement this defense if it significantly lengthens the time to return outputs of queries. \n3) Although this is a defense paper, it would be nice if the attacks were explained a bit more. Specifically, how are these attacks tested? You use the validation set, but does the attacker have knowledge about the class-label space of the victim? If the attacker trained with some synthetic data/other dataset, do you then freeze the feature extractor and train a linear layer to validate on the victim\u2019s test set? It seems like this is discussed in the context of the victim in the \u201cAttack Models\u201d subsection, but it\u2019s unclear what\u2019s happening with the attacker. \n4) It would be nice to see an angular histogram plot for a model where the perturbed labels were not crafted with knowledge of this model\u2019s parameters - i.e. transfer the proposed defense to a blackbox attacker and produce this same plot. This would motivate the defense more. \n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}