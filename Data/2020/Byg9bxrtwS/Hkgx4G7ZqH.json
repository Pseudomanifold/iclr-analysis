{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the two regimes in the training of overparameterized networks (with small learning rates):\n* kernel regime: the tangent kernel doesn't change much during training. The training behavior is then well approximated by a linear model (Taylor expansion at the initialization). This can happen when the weights are initialized to large values.\n* rich regime: The kernel regime is turned into a rich regime when the assumptions of kernel regimes aren't met.\n\nSpecifically, the paper emphasizes how the scale of initialization controls the transition between the two regimes, which was first pointed out by Chizat & Bach (2018).\n\nMy main concern is that it is unclear what unique contributions are made by the paper, as the theoretical results are not more general than that of Chizat & Bach (2018). The contributions are not clearly stated and I can only see the execution of ideas from Chizat & Bach (2018) and applying them to more concrete examples, which leads to analytical results (for linear networks) in Theorem 1/2. This feels rather incremental. \n\nSome other comments:\n* In experiments it was shown that popular initialization schemes are right on the edge of entering the kernel regime, which is very interesting. How does this change with network widths and different architectures?\n* It's difficult to see what Figure 2b tells because several notations are undefined. What are $e_1$ and $1_d$?\n"}