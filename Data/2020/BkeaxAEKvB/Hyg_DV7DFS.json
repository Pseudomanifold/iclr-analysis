{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThe authors propose a new loss function for solving large-scale inner product search that rely on quantization, based on the intuition that all pairs of (query, database vector) are not equally important for a given query. In particular, the authors weight the reconstruction error so that the pairs with a higher scalar product are more precisely quantized (as they lie among the most plausible candidates).\n\nStrengths of the paper:\n- The paper is well written and easy to follow. In particular, the intuition of the method is well explained in Figure 1 and the setup in Section 3 is well formulated.\n- The proposed method works with a variety of quantization approaches, such as binary, simple PQ or LSQ (even if the authors aren't able to report results for this last method due to technical issues as explained in Appendix 7.7)\n\nWeaknesses of the paper:\n- The related work could be more detailed, see for example: \"Spreading vectors for Similarity Search\", Sablayrolles et al. ; \"Pairwise Quantization\",  Babenko et al ; \"Unsupervised Neural Quantization for Compressed-Domain Similarity Search\", Morozov et al.\n\nJustification of rating:\nThe paper proposes a new loss function that weights the scalar products differently according to their importance than can be applied to a wide range of existing quantization methods. However, the strength of experimental results (in particular the fact that LSQ or other cited references above are missing) remains unclear. "}