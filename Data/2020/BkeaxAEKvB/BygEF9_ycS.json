{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes new loss functions for quantization when the task of interest is maximum inner product search (MIPS).\nThe paper is well written with clear descriptions, fairly comprehensive analysis and empirical exploration, and good results, and in general I agree that learning quantization so as to minimize quantization related errors on task at hand is a good strategy.  \nSpecific comments and suggestions for strengthening the paper are:\na) The proposed loss function in (2) includes a weight function that serves as a proxy for the task objective of giving more emphasis to quantization errors on samples with larger inner product.  Instead, why not use the true task objective which for the MIPS task is stated in the Introduction section?  If this was considered please comment on reasons for not including / discussing this in the paper, otherwise perhaps this\u2019ll be good to discuss.\nb) Did the authors consider using a task dependent training data set which will capture both \u2018q\u2019 and \u2018x\u2019 distributions and potentially lead to even further improved quantization?  This has the disadvantage of making quantization dependent on query distribution, but in cases where such data is available it will be very valuable to know if incorporating data distributions in quantization process helps performance and to what extent.\nc) It will also be valuable to consider the closely related task of cosine distance based retrieval and comment on how that impacts the modifications of loss functions.\nd) The idea of learning quantization under objective of interest using observed data distribution has been studied earlier (e.g. see Marcheret et al., \u201cOptimal quantization and bit allocation for compressing large discriminative feature space transforms,\u201d ASRU 2009), perhaps worth citing as related work.\n"}