{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "[Note: I wrote this review after John Schulman's first comment, before any reply, and before Gehrard Neumann's comment]\n\nThe authors propose an actor-critic algorithm based mostly on regression. Being off-policy, the algorithm can learn from multiple policies. It can also be applied to continuous as well as to discrete actions, and it can be trained in a batch RL setting. They compare it to a set of state-of-the-art algorithms in standard openAI gym continuous action benchmarks and show competitive performance despite a much simpler implementation of the algorithm.\n\nI basically subscribe to John Schulman's comment below, both about empirical results and about citing Self-Imitation Learning, but I have a stronger point against the paper, which is insufficient positionning with respect to the relevant literature.\n\nThe paper does not cite or discuss the one below, though it looks VERY close:\n\n@inproceedings{neumann2009fitted,\n  title={Fitted Q-iteration by advantage weighted regression},\n  author={Neumann, Gerhard and Peters, Jan R},\n  booktitle={Advances in neural information processing systems},\n  pages={1177--1184},\n  year={2009}\n}\n\nThis paper also starts from RWR and performs weighted regression based on the advantage rather than on the return. So to me it is exactly the same idea, and it is mandatory that the authors clearly establish what is the novelty of their work with respect to this previous paper.\n\nLess importantly, the authors may also want to have a look at :\n\n@article{zimmer2019exploiting,\n  title={Exploiting the sign of the advantage function to learn deterministic policies in continuous domains},\n  author={Zimmer, Matthieu and Weng, Paul},\n  journal={arXiv preprint arXiv:1906.04556},\n  year={2019}\n}\n\nwhich also uses ideas along the same line.\n\nTo me, a good way to improve the novelty of this work would be to perform a detailed empirical study of the inner mechanisms of the algorithm on very simple benchmarks where the value function and policy could be visualized. In particular, how stable is the estimation of the value function? This is known to be an issue, as most algorithms avoid approximating it and prefer estimating the Q-function.\n"}