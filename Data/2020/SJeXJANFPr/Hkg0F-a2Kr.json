{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: In this paper, the author analyzed gradient regularization in deep multitask learning. They empirically discovered a sharper concentration (low variance) in angles between the task gradient distributions could potentially improve the performance in multi-task learning. Then they proposed a new gradient regularization to enforce the gradient for each task be orthogonal. Empirical results (on Multi-digits MNIST and NYUv2 data-sets) indicate a marginal improvement, comparing with baselines.\n\nMain Comments:\n\nThe discovering in the paper sounds interesting while the work looks like preliminary and unpolished. Particularly I have the following technical and conceptual concerns:\n1.  In high dimensional geometrical space, ** the random high dimensional vectors will be orthogonal with high probability**. The authors can find this conclusion in many places, for example: \n\nhttps://courses.cs.washington.edu/courses/cse521/16sp/521-lecture-6.pdf\n(Theorem 6.3)\nhttps://www.cs.princeton.edu/courses/archive/fall14/cos521/lecnotes/lec11.pdf\n(Corollary 2)\n\nI noticed the author claimed in the paper \u201cBased on empirical observations presented later on we argue that multi-task networks not only benefit when the cosine is non-negative but more so when task gradients are close to orthogonal.\u201d (Page 3).  However, this empirical claim is not convincing for me.  Since gradient of the over-parameterized neural network is in very high dimension, the task gradients closing to orthogonal may happen because of \n(a) the internal high-dimensional geometry property\n(b) or the phenomene caused by the deep multi-task learning \n\nIt will be much better and more clear if the author can provide more analytical, theoretical (e.g provable bound even on linear model) or empirical (e.g ablation study) support to understand the behaviors of gradient in high dimensional problem. \n\n2. In the proposed approach, I am not clear how the author choose the **task weights** (e.g in Eq.1) \u201cw_{t_i}\u201d ? It seems that the author set them as hyper-parameters. Since understanding the task relations and automatically estimating their relationships is the key factor for avoiding negative transfer in multitask learning (e.g Sener [2018]), I think it is not a proper way to simply adjust them as hyper-parameters.\n\n3. In the experimental part, the proposed approach showed almost no or very marginal improved performance. The author should provide more evidences to show the utility or potentials, in order to convince the community to adopt the proposed approach. \n\nMinor comments:\n1. From equation (4) to (5) is not obvious, it will be better to provide the derivation details.\n2. The analyzed problem sounds more like the \u201cmulti-output\u201d or \u201cmulti-label\u201d problem (Section 3 in the paper). In the multi-task learning generally the inputs for each task X are different.\n\nOverall, I feel it is an interesting and promising direction to consider gradient regularization based approach in multi-task learning. However, the current manuscript is not mature for the acceptance.\n\n\nReference:\nMulti-Task Learning as Multi-Objective Optimization.  Ozan Sener, Vladlen Koltun, NeurIPS, 2018\n\n\n"}