{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a latent variable model to perform imitation learning. The authors propose the model in the control-as-inference framework and introduce two additional latent variables: one that represents a latent state (z) and another that represents a latent action (h). For the generative model, the authors use a sequence latent variable model. For inferring the latent action, the authors use a particle filter. For inferring the states, the authors use an \"Adaptive path-integral autoencoder,\" though it was unclear where the controls \"u\" come from. (I assume u is the same as the actions, at which point inferring the states amounts to rollout the policy in the sequence latent variable model). The authors compare to not having the latent states and/or not having the latent actions, and demonstrate that they get better imitation learning scores.\n\nOverall, I found the paper difficult to follow and some of the reasoning a bit unclear. The experiments seemed limited in scope, given that the authors discuss reinforcement learning in general, but only provide results on the reconstruction error when doing imitation learning. It is also unclear to me whether the gains from the experiments are from their model, or from the fact that their model probably has more parameters since it has more components. It would be good for the authors to compare to existing work that uses sequential latent variables models for deep RL, such as [1,2,3]\n\nMore detailed comments:\n\nIt would be good for the authors to substantiate statements like, \"Since training sas? is just a simple supervised learning problem, it had the lowest reconstruction error but the computed action from such the internal model couldn\u2019t make the humanoid walk.\" with plots.\n\nThe statement, \"zaz' also failed to let the robot walk, because reasoning of the high-dimensional action can\u2019t be accurate enough.\" seems similarly unjustified. If the authors wanted to test this, they could train a zaz' model with some low-dimensional action (e.g. left, straight right) and verify that this works.\n\nWhile the authors state that \"h can be interpreted as high-level commands,\" but if it is inferred at every time step, why is this a \"high-level\" command?\n\nNit-picks:\n- \"The procedure consists of outer internal\" --> \"The procedure consists of *an* outer internal\"\n- \"via via\"\n- \"Such the sophisticated separation was\"\n\n[1] Danijar Hafner et al. Learning Latent Dynamics for Planning from Pixels.\n[2] Maximilian Igl et al. Deep Variational Reinforcement Learning for POMDPs.\n[3] Alex Lee at al. Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model."}