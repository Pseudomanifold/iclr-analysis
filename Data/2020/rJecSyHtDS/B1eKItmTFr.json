{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper creates a new task for zero-shot learning of predicates (specifically in cases where the individual predicate components have never been seen in the training, rather than the more traditional setting where the full s-v-o relationship is unseen but each component is).  They create a new subset of the Visual Genome dataset specifically targeted towards predicting unseen predicates.  They also provide results using a knowledge graph (in this case WordNet), to integrate linguistic and visual features for prediction.  Interestingly, their pipeline introduces a new softmax variant, the unbalanced sampled softmax, which addresses the problem of over-predicting common predicates.\n\nI generally tend towards accepting this paper.  The reasons being that this paper has a few strong contributions: (1) they design a new task set-up with data they selected and cleaned from VG, (2) new modelling pipeline with empirical analysis backing the modeling choices, and (3) new softmax variant.  \n\nA few comments about things that could be strengthened or addressed further: \n- There could be more meaningful comparison to other zero-shot learning algorithms.  Even if they are not fully comparable because they were originally meant for a slightly different zsl set-up, it would be nice to have more baselines from external work.\n- Why was the unbalanced sampled softmax was being used for only predicate prediction and not entity prediction? \n- It wasn\u2019t totally clear to me whether all of the verbs/entities were in WordNet and/or Glove.  If not, can the authors clarify what the overlap was and how this might be affecting performance?\n- As noted by the authors, there are some cases in Figure 3 where humans would consider the answer to be confusing or might actually prefer the machine response.  Can human performance on this task could be measured?  Perhaps humans could possibly evaluate a subset of the machine vs. gold answers?\n\nMinor Edits:\n- Related work: WordNet is misspelled in the last line\n- Figure 3: please consider using colors other than red/green, this is not readable for color-blind readers\n- Section 5.2: \u201cThe case c is confusing that even\u201d \u2192  \u201cThe case c is so confusing that even\u201d\n- Section 5.2: \u201cand output a more appropriate\u201d --> \u201cand outputs a more appropriate\u201d\n- In the references: the Devise paper by [Frome et al 2013] is listed twice\n- In the references: the first author of ConceptNet5 should be Robyn Speer"}