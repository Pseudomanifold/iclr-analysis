{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Title: Good work, requires some edits.\n\n1. Summarize:\n\nThis paper proposes a new problem setting in visual relation detection which is called \u201cPredicate Zero-shot Learning (PZSL)\u201d. They provide a clear motivation and description of this setting. They propose a solution to this problem which leverages linguistic priors and knowledge bases. Furthermore they propose an unbalanced sampled-softmax to tackle the long tail distribution of predicates.\n\n2. Clearly state your decision. One or two key reasons for this choice.\n\nI will go for a weak accept for the paper at this stage. (+) I think the proposed problem setting is well-motivated and useful. Also, (+) the proposed initial solution to this problem is interesting. However, (-) they propose a \u201cfast graph convolution network\u201d which seems to be precisely equivalent to a PinSage.  Also, (-) the paper requires to be polished as it lacks clarity.\n\n3. Main discussion\n\nMy first argument is: I\u2019m not sure why the authors have changed the name of PinSage and just mentioned that \u201ctheir\u201d \u201cFast Graph Convolution Network\u201d is \u201cinspired\u201d from PinSage. To me it looks exactly the same. If there are any differences, it should be stated clearly. In fact, I would not be against using PinSage as a part of their approach. However, trying to rename it without clear reasons is not a good idea.\n\nMy second argument is that the paper lacks clarity in writing (for detailed suggestions please refer to comments and feedbacks). Specially the evaluation section lacks details and clarity: a) In the beginning of this section (page 7), the authors talk about \u201cgeneralized\u201d and \u201ctraditional\u201d settings without properly defining them. b) The descriptions for Table 1 and Table 2 fail to provide enough details to help understand the difference between the results in these two tables (one of them states \u201cAccuracy of unseen predicate recognition\u201d and the other one \u201cAccuracy of recognition of triplets with unseen predicates\u201d). \n\n4. Comments and feedback.\n\nIntroduction: \n\nParagraph one in the: \n1. The relationship recognition methods are mainly supervised \u201cthat\u201d \u2192 \u201cto\u201d.\n2. last line: \u2026. and do not study \u201con generalizing\u201d \u2192 \u201cthe generalization of\u201d.\n\nParagraph two:\n1. no manual annotations or \u201creal samples\u201d \u2192 \u201cimage samples\u201d. (a real sample is ill-defined)\n2. For example, no instance of chew \u2192 For example \u201cgiven\u201d no instance of chew.\n\nParagraph three:\n1. \u2026 is difficult since predicates are often abstract not as specific \u2192   is difficult since predicates are often abstract \u201cand\u201d not as specific.\n2. Furthermore, unlike many object ZSL methods \u2026 \u2192 This line to the end is very complicated and hard to understand.\n\nRelated Works:\n\n1. Visual Relationships: I would cite \u201cGraph R-CNN for scene graph generation\u201d since it is the most relevant work regarding the similarity of pipeline (using GCNs).\n2. External Knowledge bases  (KB): I would cite \u201cImproving Visual Relationship Detection using\nSemantic Modeling of Scene Descriptions\u201d since it is one of the most relevant works using knowledge graph modelings to improve visual relation detection.\n\nProblem Setup:\n\nDo you plan to provide the proposed dataset splits so others can work on this setting? I consider this very important given your paper\u2019s contribution. Maybe it is better if it is also mentioned in the paper.\n\nPipeline:\n\n1. Paragraph 2: \u2026 the output of which is fused with \u2026: Given Figure 2, it does not seem like $V_p$ is being created by fusing $V_s$ and $V_o$. It looks more like it is extracted directly from the image (union of bounding boxes).\n\n2. In Figure 2: In the representation of Pipeline (A), the graph is colored by dark blue for objects and light blue for predicates. The represented graphs show Object to Object and Predicate to Predicate connections which I\u2019m not sure if it is correct. Shouldn\u2019t we always have a light blue between every pair of dark blue connections?\n\nEvaluation:\n\n1. Please consider the mentioned points in the Main Discussions.\n2. In Table 1, I suggest re-naming \u201cembedding\u201d to \u201cinitial embedding\u201d.\n3. In Table 1, Hit@k should be Hits@k.\n4. Please define the metrics clearly (Hits@k).\n\nExtra: I have a question regarding the ablation studies with GloVe, Normal and InferSent initialization. The question is whether this initialization is necessary? It seems like in the setting \u201cW/O KG\u201d, even though the embeddings are initialized with GloVe, there is no gain (all of the Hits@k values are 0.0). So GloVe embedding without KG bring no external semantic knowledge? Then why use them? Regarding that, I can see that a GCN, initialized with normally distributed embeddings (row 14 in Table 1) has given 0.0 accuracies, but I find this very counter intuitive, as graph convolution layers already have trainable weights capable of compensating for the lack of \u2018proper\u2019 initialized embedding and getting 0.0 does not make sense to me.\n\nConclusions and Future Work:\n\n1. two lines before the last: please use \u201c\\citep\u201d.\n\n"}