{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers the task of predicting visual predicates (e.g., eat, bite, take) between pairs of entities. In particular, the paper focuses on the zero-shot setting where the test predicates are unseen during training. The model uses linguistic prior from a knowledge graph (WordNet):\nwith graph embedding (fast GCN), unseen predicates are embedded based on the information propagated from seen predicates. The model is trained so that the visual feature vector and the correct predicate embedding are nearby in the joint embedding space. The method was evaluated on a zero-shot split of the Visual Genome dataset.\n\nOverall, as a task and dataset paper, the paper should have sold the task more by highlighting its special properties. While the task of predicting unseen predicates is interesting, the setting and the technique are similar to previous work on predicting other types of unseen labels (e.g., unseen objects, as referenced in the paper). The new task could still be interesting if it presents different challenges (e.g., maybe predicates are more ambiguous than objects, or visual predicates are harder to embed). But from the description in the paper, most of the challenges seem to also exist in other zero-shot settings (long-tail distribution; large space of labels). The paper could benefit from providing examples or statistics that demonstrate the challenges of the task.\n\nThe proposed method looks correct but is a rather direct application of existing methods. The experiment setup looks OK. Based on the error analysis, the labels look very noisy and subjective, but this seems to be a common problem in the visual predicate prediction task (hence the recall-based evaluation).\n\nAdditional questions:\n\n- The provided examples in the error analysis look pretty tricky; e.g., \"swing\" and \"slug\" are judged as different. How well would a human do on this task?\n\n- How much would Hit@k be if the test label is seen during training (not zero-shot)?\n"}