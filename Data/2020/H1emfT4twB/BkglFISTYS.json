{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the effects of using function of ngram statistics as feature to generate attention score per word. The attention score is then used as weights to aggregate document embedding by doing a weighted average on word embedding. The output is finally fed into a ridge regressor to do the final predictions on target labels. \n\nMain comments:\nThis paper has a clear motivation and decent experimental results (though some concern on baseline models, see below). The introduction of using distributional signature to derive attention scores seems interesting and a novel contribution. However I was not able to fully understand the intuition behind the benefit of doing attention mechanism on top of ngram statistics (see my question below as well). \nAlso the reference/baseline models used in the experiment might not be strong enough. If you could compare your model with some latest algorithms proposed in the few-shot-learning communities, that would be more convincing as well. \nTo list a few:\n* P-MAML: [Zhang et al., 2019]\n* Induction-Network-Routing: [Geng et al., 2019]\n* ROBUSTTC-FSL [Yu et al., 2018]\n\nI am leaning to give a \"weak reject\" based on my current knowledge and understanding of the paper. But I will be willing to revisit the decision after we get feedback from the author(s). \n\nIn particular, I would be glad if the author could clarify the questions below.\n\n* From table 1, it seems Method IDF+RR is a competitive model. IIUC, the statistics of s(.) is highly correlated with IDF which also indicates general word importance in corpus. My questions are that, \n1) regarding ablation test \"OUR w/o biLSTM\", how is $h$ calculated in this case (without biLSTM)?\n2) since each word is represented based on two statistical number (map function by t(.) and s(.)), can you give any intuitive explanation that why getting attention score from that makes sense?\n3) do you have any experiments using the distributional signature as a common feature in standard text classification problems? In other words, is this method only (significantly) beneficial to few-short-learning? If it is also useful in general text classification task, it would be a good \"plus\" here.\n\n* From table 2, can you explain why CNN+RR benefits a lot from the BERT embedding? Actually it gets more percentage of improvement than the model \"OUR\".\n\n* For all the usages of pre-trained embedding (fasttext or BERT), are you further finetuning the embedding parameters during your training? Or you freeze the embedding parameters?\n\n[Zhang et al., 2019] Ningyu Zhang et al., Improving Few-shot Text Classification via Pretrained Language Representations. arXiv preprint arXiv: 1908.08788\n[Geng et al., 2019] Ruiying Geng, Binhua Li, Yongbin Li, Yuxiao Ye, Ping Jian, and Jian Sun. 2019. Few-shot text classification with induction network. arXiv preprint arXiv:1902.10482.\n [Yu et al., 2018] Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text classification with multiple metrics. arXiv preprint arXiv:1805.07513"}