{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper focuses on applying meta learning approaches to text classification.\n\nThe primary contribution is an attention mechanism based on word statistics --- most importantly word frequency. This novel attention mechanism is motivated by the observation that the base units in text (lexemes) are more likely to have task specific interpretations than lower level patterns in vision. And, while a lexeme based attention mechanism trained on one task may not transfer well to other tasks, a mechanism based on coarser word statistics is less likely to focus in on task specific patterns.\n\nA secondary contribution is the use of ridge regression [1] to perform meta-learning for text classification.\n\nThe paper presents experiments on a number of text classification tasks from the NLP literature. Aside from the new attention mechanism and the use of ridge regression, the proposed approach makes use of FastText word embeddings or BERT sentence representations, depending on the task. The paper demonstrates significant improvements over baselines that use other methods of aggregating word representations. All of baselines were implemented for this paper.\n\nThe idea of using coarse statistical signatures to calculate attention is an interesting one. However, I have concerns about both the clarity of this paper and the lack of clear comparison to previous work.\n\n== Clarity ==\n\nMany of the details of the model and learning approach are vaguely discussed, or relegated to Figures 4 & 5. I think the paper would benefit from a more formal definition of the entire learning procedure.\n\n== Comparison to previous work ==\n\nThis paper seems to be following the standard FewRel experimental setup. Also, the RCV1 experiments seem to follow the [2] which was cited by the in the paper under review. However, it is not clear if the setups are the same or if the numbers are comparable.\n\nI am not sure about the existence of comparable results for the other tasks, but for FewRel at least the baselines presented here significantly underperform other papers' reports of equivalent models.\n\n  - The paper from [3] that introduced FewRel reported 69.2 / 84.8 for CNN based prototypical networks --- far above the 49.8 / 65.2 reported here.\n\n  - [4] found that a BERT model with no FewRel specific training at all achieves 72.9% on the 5way/1shot task. Which is above all of the BERT based models reported in Table 2.\n\nI may be missing something, but if these numbers are actually not comparable then this paper should contain an explanation of how the experimental setup differs. And if the setup is actually the same as previous work, I expect to see a comparison of results.\n\n[1] https://openreview.net/pdf?id=HyxnZh0ct7\n[2] https://openreview.net/forum?id=SyxMWh09KX\n[3] https://www.aclweb.org/anthology/D18-1514/\n[4] https://arxiv.org/abs/1906.03158"}