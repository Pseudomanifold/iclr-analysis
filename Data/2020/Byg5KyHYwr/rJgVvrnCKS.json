{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors identify and address the problem of sub-optimal and myopic behaviors of self-imitation learning in environments with sparse rewards. The authors propose DTSIL to learn a trajectory-conditioned policy to imitate diverse trajectories from the agent\u2019s own past experience. Unlike other self-imitation learning methods, the proposed method not only leverages sub-trajectories with high rewards, but lower-reward trajectories to encourage agent exploration diversity. The authors claim the proposed method to be more likely to find a global optimal solution. \n\nOverall, this paper is well-written with comprehensive experimental results. The proposed trajectory-conditioned policy sounds, since rewarded trajectory carries significant information of the goal in the exploration problem. Extensive experimental results demonstrated the effectiveness of the proposed DTSIL. However, I have a few concerns below, that prevent me from giving a direct acceptance. \n\n1. The proposed DTSIL changes the original MDP with sparse reward to an MDP with denser reward, which allows the training process to explore more in the \u201cspace\u201d closer to the collected high reward trajectories. Such \u201cexploration\u201d sounds promising. However, it would be nice to compare it with traditional reinforcement learning (e.g., with \\epsilon-greedy policy for random exploration)?\n\n2. In appendix D, the authors discussed what the parameter \\delta_t controls, however, it is unclear how \\delta_t should be chosen in implementation. The authors did not explain how \\delta_t was selected in their experiments. Choosing the right \\delta_t may be hard, but it would be nice to introduce what \u201cheuristics\u201d the authors used and suggest to readers. \n"}