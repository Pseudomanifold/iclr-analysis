{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThis paper proposes a regularization method that forces two samples within a class to have the same output distribution.  The paper also adds sample-wise consistency by augmenting an input data and forcing the output distributions of  original and augment data to be similar.  The idea is to minimize a KL-divergence between two distributions.  The proposed method applies the cross-entropy loss with these two additional regularization terms.  Experiments show that the proposed regularization method works better for many image datasets compared with AdaCos, Virtual-softmax, Maximum-entropy, mixup, and the simple cross-entropy baseline.  The proposed method has the nice property that the output is confidence calibrated compared to other methods, which is also shown in the experiments.\n\n\nPros:\n\nThe proposed method is extremely simple and easy to understand.  The experimental results demonstrate that the proposed ideas are meaningful.\n\n\nCons:\n\nIt would be better to explain the motivation behind forcing the output distributions to be similar among samples within the same class.  This somehow seems to add a strong assumption on the data structure.  For example, if two classes p(x|y=1) and p(|y=2) have some distribution overlap, samples closer to the decision boundary should have a lower underlying temperature but samples far from decision boundaries should have a higher underlying temperature.  The proposed method forces these two samples to have similar \\hat{p}(y|x) even though the true p(y|x) for these two samples are not similar.\n\nThe confidence calibration property of the proposed method is nice.  It would be interesting to have some discussions on why this happens.\n\nCurrently the paper fixes hyper-parameters related to the proposed method in experiments, e.g.,T=4 and \\lambda_sam = 0 for conventional tasks.  Since this is the first paper to propose class-wise self-knowledge distillations, it would be better to have an ablation study, e.g., different temperatures and different hyper-parameter setups, and when only having one regularization term at a time instead of having both, so that readers can have a better understanding of the behavior of CS-KD.\n\n\nOther very minor comments:\n\nIn 2nd paragraph of Section 4, citation link is missing for Srinivas & Fleuret (2018) and the last citation in this Section is also missing a link.\n\nIn Reference section, Pereyra et al. 2017 is cited as an ICLR paper, but is this a workshop paper?\n\nIn Reference section, citation for the VAT paper combines two authors into one, and the order of authors seems to be different.\n\n"}