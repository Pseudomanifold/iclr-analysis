{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to regularize deep neural networks for image classification using a consistency loss (KL distillation) between images of the same class and between augmentations of the same image. The intuition is simple: the output distribution over classes for images should roughly match when they are (i) in the same class and (ii) two augmented versions of the same image (random rotation / crop / color jittered) etc. The experimental results seem to confirm the effectiveness of the proposed method wrt to recent baselines.\n\nThis is a well written paper but, in its current form, it isn't above the acceptance bar: (i) the idea of using consistency terms as regularizer is not novel and related works on consistency losses have not been adequately cited; (ii) the experiments are in general well executed but are not quite comparable with the state-of-the-art.\n\n1) About consistency as regularizer:\n1.1) Consistency loss as a regularizer dates at least back to https://arxiv.org/pdf/1412.4864.pdf (NeurIPS '14, not cited) and https://arxiv.org/pdf/1703.01780.pdf (NeurIPS '17, not cited). Consistency across corrupted / augmented views has been used in a plethora of domains, not only in vision but also NLP (https://nlp.stanford.edu/pubs/clark2018semi.pdf, not cited).\n1.2) Sampling images from the same class as a form of corruption seems novel to me but I feel like it could be dataset specific ? What are the limits of the approach ? That could give more depth to the paper.\n\n2) About the experiments:\n2.1) For CIFAR-100, mixup reports an error of 21.1%, (Figure 3 in https://arxiv.org/pdf/1710.09412.pdf) while you report 23.28%. Why didn't you use the same architecture ?\n2.2) Again for CIFAR-100, there exists another method manifold-mixup which reports 20.3, (Table 1(b) in http://proceedings.mlr.press/v97/verma19a/verma19a.pdf), while your best score is 21.51. Do your gains transfer to comparable architectures as per the mentioned paper ?\n"}