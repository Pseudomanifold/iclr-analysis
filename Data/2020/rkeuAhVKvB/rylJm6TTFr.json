{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN.\n\nIntroduction outlines clearly what is the goal and why this is in principle an interesting avenue to investigate, however I found it could be more precise. In particular message passing techniques can be implemented very efficiently and are highly scalable. Section on Cognitive intuition of the consciousness prior doesn\u2019t tell much beyond the re-stating of what it is, I am more interested in what it brings to the table and on what is the intuition behind its application to this domain.\n\nOne thing I would argue agains is that the input-dependent local subgraph requires access to the global graph, therefore going back to a conventional GNN. The argument is that the message passing can be constrained to reduce the computational burden. However, this can also be done by means of anchor graphs or other data structures, dynamic pooling and so fort. How do the authors compare to such choices?\n\nOverall I like the \u201cglobal conditioning\u201c by means of sampling to have better local representations at each node, an idea that while it can be cast as consciousness prior it is also related to neural processes and architectures alike.\n\nThe implementation section could be made a bit clearer. Currently, for instance, there are references to prediction tasks while I think it would be nicer to have it fully self contained. Also, the aggregation strategy seems to be that of Kipf et al., is it a constraint of the model or other strategies could be used as well?\n\nThe experiment section is clearly explained and results are interesting, showing the potential of the proposed approach. Also the thorough analysis of the model is very well done.\nI wonder what is the performance when using no sampling at all, shouldn\u2019t this be the reference?\n\nThe convergence analysis states that the model converges very fast, does it also translate to better results in case of small amount of training data?"}