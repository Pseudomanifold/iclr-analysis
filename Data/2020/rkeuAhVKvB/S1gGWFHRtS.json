{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "# Summary\nThis paper proposes a new neural network architecture for sequential reasoning task. The idea is to have two graph neural networks (GNNs), where one performs input-invariant global message passing, while the other performs input-dependent message passing locally. The input-dependent GNN employs a flow-style attention mechanism. The results on several knowledge completion datasets show that the proposed method outperforms the state-of-the-art methods.\n\n# Originality\n- The idea of learning an input-dependent subgraph using GNN seems new. \n- The proposed way to reduce the complexity by restricting the attention horizon sounds interesting and seems necessary for scaling up. \n\n# Quality\n- The overall architecture looks like a fairly complicated combination of neural networks (two GNNs with attentive mechanism). However, it is not entirely clear how much each component contributes to the performance. The experiment only shows the benefit of having IGNN.\n- The effect of the proposed complexity reduction technique is not studied in the experiment. \n- The empirical results are hard to parse, as they contain too much dataset-specific results that are not clearly explain the paper.  \n\n# Clarity\n- The paper is too dense with unnecessary details. For example, the introduction is too long (2.5 pages). The problem formulation contains too much details that deviate from the actual problem formulation. The details of each dataset (Table 1) and experimental setup can be moved to the appendix. \n- Many figures in each experiment contain too small texts with lots of unexplained dataset-specific legends. \n\n# Significance\n- Although this paper proposes an interesting neural architecture for knowledge completion tasks, it is not clear how much each component contributes to the performance. Also, the empirical results could be presented in a better way to deliver clear conclusions. "}