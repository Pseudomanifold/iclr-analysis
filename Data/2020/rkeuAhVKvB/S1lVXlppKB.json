{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose to use sampling methods in order to apply graph neural networks to large scale knowledge graphs for semantic reasoning. To this end induced subgraphs are constructed in a data-dependent way using an attention mechanism. This improves the efficiency and leads to interpretable results. The experiments show some improvements over path- and embedding-based methods.\n\nThe paper is partially difficult to read and not well structured (see minor comments below). Overall, I think that the proposed GNN architecture is an original and interesting approach for this specific application. The experimental evaluation presented in the Section 4 shows clear improvements. This, however, is not true for the results presented in the appendix (Table 4). I am missing a discussion of the limitations of the proposed approach. Moreover, a thorough discussion of the hyper-parameter selection and, if possible, theoretical justification would be highly desirable and could strengthen the paper.\n\n\nMinor comments:\n\n- Section 2 start with the paragraph 'Notation', but does not contain any other paragraph.\n\n- The sampling strategy should not be introduced as part of the section 'problem formulation'.\n\n- using standard terms from graph theory for well-known concepts (such as 'induced subgraph') would improve the readability\n "}