{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a novel, feedforward, end-to-end trainable, deep, neural network for abstract diagrammatic reasoning with significant improvements over the state of the art. The proposed model architecture is reasonable and is designed to exploit the information present at multiple granularities \u2013 at the level of objects in the diagram, their relations across diagrams, and diagram subsets. As a multimodule neural pipeline, it seems a reasonable design. Further, it shows significant performance gains over the state of the art.  \n\nHowever, the writing quality is poor and is the primary reason for my giving it a low score. The paper is difficult to read and it\u2019s hard to figure out the terminology and it\u2019s grounding in the problem; the high-level abstract design and design choices that address the nature of the problem from the low level details, etc. \n\nThe paper uses terminology without explaining the reason for it - for example, why is the approach called \u2018Multiplex Graph Networks\u2019? What information is being multiplexed and how? Graphs are conceptual in the proposed approach \u2013 there doesn\u2019t seem to be any graph algorithms or graph based processing. Once the module is run for search space reduction, the set of edges or relations (node pairs) become well-defined (in adjacent rows, columns) as well diagram subsets (edge pairs). The corresponding modules are just computing vectorial embeddings. Similarly, there is no reasoning that\u2019s taking place. Reasoning requires tokens and grammar over such tokens which is not there in this case.  The proposed model is non-interpretable. \n\nThe technical writing is loose and hand-wavy. The appendix is a lot of grammatical mistakes.\n\nA few clarifications may be helpful:\n\n- \u201cThe reasoning module can also be considered as another graph processing module\u201d?\n\n- \u201c\u2026 we use spatial attention to iteratively attend \u2026\u201d \u2013 there is no iterative attention. It\u2019s all parallel.\n\n- What do the \u2018N\u2019 nodes in each layer correspond to? There are clearly not objects or diagram primitives as they can vary in number in each diagram.\n\n- if interlayer connections are between objects in different layers (diagrams), what is this supposed to capture? Clearly, there may not be any unique correspondence between objects across diagrams.\n\n- What\u2019s a cross-multiplexing gating function? If it\u2019s a known concept, please provide a reference else explain.\n\nFinally, I\u2019m open to revising my score upwards if it turns out that I\u2019m the only one who had difficulty with the writing. The architecture design makes sense for the addressed class of problems (though the proposed network is non-interpretable and doesn\u2019t do any reasoning nor uses graphs or graph based processing in a meaningful way), the results are good and the experimental evaluation sufficient.\n"}