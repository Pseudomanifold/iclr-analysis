{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes semi-structured neural filter composed of structured Gaussian filters and the usual structure-agnostic free-form filters found in neural networks. They are optimized using end-to-end training. Effectively, this lead to increased receptive field size and shape with just few additional parameters. Further, this module is architecture agnostic and can also be integrated with any dynamic inference models. Specifically, when applied on deformable convolutional filters, the deformation at each input can be structured using gaussian filters. Empirical experiments suggest that when integrated with state-of-the-art semantic segmentation architectures, the absolute accuracy on Cityscapes improves by 2%. Large improvement in seen on naive / sub-optimal architectures for segmentation.\n\nGiven that this is first work which demonstrates the efficient composition of classic structured filters with neural layer filters, I believe that research community will benefit to good extent if this paper is accepted.\n\nClarification:\n1. I note that single gaussian is shared across different free-form filters. Is same gaussian also shared across input channels ?\n2. For dynamic inference, what is the sampling resolution used ? How is it related to diagonal elements of covariance ? 2\\sigma ?\n3. In case of blurring and resampling, does the model learn another filter for sampling ? To me, sampling seems similar to dynamic inference operation but with static parameters.\n4. As noted in paper, blurring is fundamental hwen dilating. Does DRN-A and DLA-34 models used for comparison in Table 1 includes blurring prior to dilation ?\n\nAdditional experiment:\n1. Does improved receptive field size and shape also lead to improvement in other downstream tasks such as classification, object detection, depth estimation etc. ?\n2. Table 4 shows that the networks with reduced depth when integrated with composed filters can perform as well as large networks. Does this holds true when extended to above tasks ? \n3. I note that in all the presented results, the composed filters are only included at the last few layers. How the results prunes out when included at the lower as well as at the intermediate layers ? Please include a plot of accuracy vs depth (at which it is included).\n4. I am glad to note that Gaussian deformable models performs as good as free-form deformable models with largely reduced parameters. Can you please add total network parameters comparison in Table 5 ? Further, are these also included only at the top few layers ?\n5. In Table 1, DLA-34 + DoG ?"}