{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper contains two semi-supervised dependency parsing algorithms. The two are end-to-end autoencoding models. For one the latent variables are sequences and for the other the latent variables are dependency trees. Experimental results are given showing that unlabeled data help to slightly improve performance. One main contribution is to improve over the algorithm by Corro and Titov at ICLR last year. For this, the paper contains a tractable algorithm for computing the optimal dependency tree structure.\n\nThe paper is not easy to read. A number of important details are not given, either omitted or to be found in references. My main concern is that the main contribution is left in Appendix and that the Appendix does not help much to understand why the assertion stands. Also, using the Eisner algorithm has the drawback that the GAP method is no more end to end. Therefore I am inclined to reject the paper.\n\nDetailed comments and questions.\n\n* First contribution. The paper said that the two methods provide a trade-off speed vs accuracy. But the paper does not consider this question. It should be nice to compare the complexity, both theoretically and experimentally, of the two methods.\n* Second contribution. As said before here is my main concern. The tractable inference is the topic of Section 6.5. But the few lines do not help and refer to the Appendix. The appendix does not help much. It seems that you use a variant of the Eisner algorithm but what are the differences? Also, it is not clear how is computed the transformed scoring matrix $S'$? And why this transformed matrix allows to compute the optimal dependency tree for unlabeled data? The GAP model is no more end-to-end because of using the (a variant of the) Eisner algorithm.\n* Second contribution. It should be nice (in relation to the first item) to give the complexity of the algorithm. The paper should compare time efficiency for the GAP algorithm and the Corro and Titov's algorithm. \n* Related work. Kim et al 2019b is cited two times. You should discuss this paper and recent new developments in unsupervised parsing.\n* Section 3.1. It is not clear whether you use pre-trained word embeddings. For instance, in the Kiperwasser and Goldberg's paper, the distinction between using internal word embeddings and pre-trained word embeddings is made clear with improved performance for the latter. Also, the use of POS tag embeddings is not done in other systems. Please explain why the method uses such embeddings and discuss whether they improve performance. I do not understand the last paragraph of Section 3.1.\n* Section 6.5. Should be an important part of the paper.\n* Section 7. The paper should made explicit the hyperparameters and their default value. It is not clear whether the development set was used. \n* Section 7.1. I do not understand the sentence \"in GAP using POS to POS decoder only yield the satisfactory performance\".\n* Section 7.2. As said before a comparison between the computation time of GAP and Corro and Titov's algorithm should be given.\n* The paper does not consider recent improvements for word embeddings such as ELMo (Peters et al 2018) and BERT (Devlin et al 2018). I wonder whether using such word embeddings would improve greatly dependency parsing algorithms.\n\nTypos.\n* Section 3, l2, a special root token\n* Section 3, l2, length of $l$\n"}