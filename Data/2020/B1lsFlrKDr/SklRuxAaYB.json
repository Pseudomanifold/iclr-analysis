{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes two semi-supervised (projective dependency) parsing models, which are first-order graph-based (only head-modiffier scores are used). Both models, local and global, are auto-encoders. The latent variable in the local model is to represent words, thus the model generates trees sequentially. The latent variable in the global model is to present a whole tree, thus the model can consider all the dependency scores at once. One of the key contributions is that, thanks to the first order property, the authors can make use of available dynamic programming algorithms to compute expectations tractably. The experiments show that both the models work across several languages. \n\nI would accept the paper because: \n- it's well written, with clear motivations \n- the idea of using dynamic programming to compute the global model tractably is thoughtful and innovative. The math details seem correct to me (though I didn't check them carefully)\n- the experimental results do support that the semi-supervised learning does work, and are consistent across several languages. \n\nI would reject the paper because: \n- technically speaking, tractability is very cool, but in this case the global model has to sacrifice the expressiveness of higher-order parsing. It's very unclear to me if that's worth or not. The experimental results do not say anything. \n- I don't know why the authors split 10% - 90% for labelled and unlabelled sentences. The gain (L+U vs L) seems not impressive to me. It is also unclear to me where the gain is from. I suggest to adjust this ratio (e.g. the portion of labeled sentences if from 1% to 100%)\n\nTypos: \n- Algorithm 1 and the text should be consistent with each other for the use of the symbol of the encoder's parameters. \n\nQuestion: \n- The two models are graph-based, thus they not necessarily projective. I see why the projectivity is needed for the variant of inside-outside algorithm. But I'm wondering if there are any dynamic programming algorithms for graph-based non-projective parsing?  "}