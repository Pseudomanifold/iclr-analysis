{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a novel transformer model called R-Transformer. \nBased on the observation that positional embeddings require a \na lot of design efforts in vanilla Transformer, the authors propose to use local RNNs to\nencode local information in replace of positional embeddings.\n\nThe paper is well presented and the proposed algorithm is explained in detail. \nHowever, it is not clear how the proposed model obtains positional information of input nodes. \nIn vanilla Transformer, positional embeddings contains global positional information. \nIn the R-Transformer, however, the multi-head attention layer will not be able to obtain positional information from local RNN outputs. Will such loss in positional information affect the performance?\n\nThe empirical study shows that R-Transformer can outperform vanilla Transformers and recurrent architectures, which is promising. Still, it would be more convincing if the authors could provide comparisons on NMT tasks or larger language modeling datasets such as WikText. Also, I am interested in the training efficiency of these models. How much overhead does the local RNN introduce?\n\nOverall I think this is an interesting paper but experiments could be improved. "}