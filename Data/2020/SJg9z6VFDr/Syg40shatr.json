{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThe authors discussed that most graph NNs to date considered discrete layers and hence are difficult to model diffusion processes on graphs. This paper proposed the Neural ODE on a graph, termed with a graph ODE, to tackle this problem. The authors gave a sufficient condition under which the adjoint method gets instable and pointed out potential issues of training Neural ODEs using it. To overcome the instability issue, the author proposed to backpropagate errors directly at discretized points. Since the naive implementation of the direct method is memory-consuming, the authors used invertible blocks as building blocks of graph ODEs, which do not store the intermediate activations for backward propagation. Finally, the authors conducted empirical studies to see the effectiveness of the proposed method.\n\n\nDecision\n\nI recommend rejecting the paper weakly because I think the extension of Neural ODEs to graphs is straightforward and that the empirical study is not strong enough to compensate for the weakness of the novelty.\nTheoretical justification of numerical instability of Neural ODEs (Proposition 1) and its empirical verification (Section 5.4) give new insights for understanding Neural ODEs. However, if I understand correctly, the formulation of graph ODEs do not use the internal structures of graph NNs, even the fact that the underlying neural network is a graph NN. Therefore, I think the extension from Neural ODEs to graphs is a bit too straightforward. The authors proposed a method which improves the stability and memory-efficiency of training. We can apply this method to general Neural ODEs, too. In addition, we can attribute the idea of using invertible blocks to existing works (Gomez et al., 2017). Finally, regarding the empirical performance of graph ODEs, the performance improvement from existing GNNs is within the standard deviations. Therefore, I think the empirical result is not sufficiently strong to justify the novelty of applying Neural ODEs to graphs. Taking these things into account, although the authors gave a new result on Neural ODEs, I think the contribution is limited from the viewpoint of the study of graph NNs.\n\n\nSuggestion\n\n- As I wrote in the Decision section, the theoretical results are not restricted to graph ODEs but valid for general neural ODEs. Therefore, I think the authors do not have to restrict the application areas to graph ODEs. The possible direction of the paper is to further analysis of training neural ODEs (e.g., instability). On the other hand, if the authors are interested in the extension of neural ODEs to graphs, I expect a more detailed relationship between the neural ODEs framework and underlying GNNs. For example, I am curious how the topological information of graphs affects the graph ODEs via spectral-type graph convolution operations and what is the relationship to the oversmoothing phenomena (Li et al., 2018).\n- Since Theorem 1 is applicable not only graph ODEs but also Neural ODEs, it implies that ordinal Neural ODEs are also vulnerable to the instability. The experiments in Dupont (2019), which this paper referenced, pointed out the instability of Neural ODEs. I am wondering the proposed method can enhance the training of neural ODEs, too.\n\n[Li et al., 2018] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018."}