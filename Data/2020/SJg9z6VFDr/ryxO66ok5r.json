{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposed a deep model called graph-ODE (GODE), which extends the continuous deep model, neural ODE [1], to graph structured data. Two methods of computing the gradient are proposed, one is the adjoint method proposed in [1] which is memory-efficient but not perfectly accurate, and the other method is to discretize the ODE and compute an accurate gradient, which is also memory-efficient but relies on an invertible architecture.\n\nIn general, this paper contains many interesting thoughts, but to me, it seems these ideas are not new and have been proposed before[1,2,3,4]. I could not find a strong contribution of this paper. \n\n- - - about the ODE stability issue\n\nAmong invertible deep models, the advantage of ODE-based continuous models [1] is: there is *no restriction* on the form of f. The drawback is the computed gradient has an error, depending on discretization step size and stability. This paper pointed out the stability problem (which is also discussed in [2,3]), but do not provide a solution in the domain of continuous deep models. \n\nInstead, the solution they provided is to use a discretized version and compute the gradient accurately. Then it becomes a standard invertible DL model with discrete layers, where the invertible building blocks have a specific *restricted form*. The 'adjoint method with discrete-time' in Eq (10) is the same as the chain rule, which has also be pointed out in [4]. To this point, I think GODE is in the class of discrete invertible DL models trained by gradient descent. I think it less related to continuous models, except the step size can be adaptive in the forward pass.\n\n- - - about the invertible building block\n\nThe proposed invertible building block replaces 'sum' in [5] by a function \\psi. This is not novel enough to serve as a contribution.\n\n- - - comparison to graph neural network\n\nI think it is interesting to apply ODE techniques or other invertible architectures to graph-structured data, for which I didn't see similar works before and could be a contribution of this paper. However, for the experimental results shown in table 3 and 4, the improvement is really small. A stronger result is needed to demonstrate the advantages.\n\n[1] Chen, Tian Qi, et al. \"Neural ordinary differential equations.\" Advances in neural information processing systems. 2018.\n[2] Chang, Bo, et al. \"Reversible architectures for arbitrarily deep residual neural networks.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n[3] Behrmann, Jens, et al. \"Invertible Residual Networks.\" International Conference on Machine Learning. 2019.\n[4] Li, Qianxiao, et al. \"Maximum principle based algorithms for deep learning.\" The Journal of Machine Learning Research 18.1 (2017): 5998-6026.\n[5] Gomez, Aidan N., et al. \"The reversible residual network: Backpropagation without storing activations.\" Advances in neural information processing systems. 2017."}