{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper hypothesizes that even though we are able to achieve very impressive performance on benchmark\u00a0datasets as of now (e.g. image net), it might be due to the fact that benchmarks themselves have biases. They introduce an algorithm that selects more representative data points from the dataset that allow to get a better estimate of the performance\u00a0in the wild. The algorithm ends up selecting more difficult/confusing instances.\n\nThis paper is easy to read and follow (apart from some hickup with a copy of three paragraphs), but in my opinion of limited use/impact.\n\nComments:\n1) There is a repetition of the \" while this expression\u00a0formalizes..\" paragraph and the next paragraph and the paragraph \"As opposed to ..\" is out of place. Please fix\n2) I am not sure \n- What applications the authors suggest. They seem to say that benchmark authors should run their algorithm and make benchmarks harder. To me it seems that benchmarks become harder because you remove most important instances from the training data (so Table 4 is not surprising - you remove the most representative instances so the model can't learn)\n- how practically feasible it is.  Even if in previous point I am wrong, the algo requires retraining the models on subsets (m iterations). How large is this m?\n3) Other potential considerations:\n-  When you change the training size, the model potentially needs to be re-tuned (regularization etc) (although it might be not that severe since the size of the training data is preserved at t)\n- How do u chose the values of hyperparams\u00a0(t, m,k eta), how is performance of your algorithm depends on it\n4) I don't see any good baselines to compare with - what if i just chose instances that get the highest prediction score on a model and remove these. How would that do? For NLP (SNLI) task i think this would be a more reasonable baseline than just randomly dropping the instances,\n5) I wonder if you actually retrain the features after creating filtered dataset, new representation would be able to recover the performance.\u00a0\n\n"}