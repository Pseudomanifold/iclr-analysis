{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: The authors propose the Gram-Gauss-Newton method for training neural networks. Their method draws inspiration from the connection between the neural network optimization and kernel regression of neural tangent kernel. \n\nTheir method is described in Algorithm 1, but to summarize it, they use the Gauss-Newton method to train neural networks, and prove quadratic convergence for the full-batch training. They also have a mini-batch version of GGN, the practical version, and this is proven to have linear convergence.\n\nThe authors also provide experiments that includes the usual loss v. epoch, but also loss v. wallclock time (which is nice when proposing second-order-like methods where extra computations are necessary), and a test error v. epoch (which is again nice for second-order methods as explained below).\n\nStrengths: The paper has nice proofs of the theorems, and they show a method with quadratic convergence (but full-batch training) without having to invert the full Jacobian matrix whose size depends on the number of parameters, but rather inverting the Gram matrix, whose size depends on the number of training data.\n\nDue to the seeming extra computational cost of the method, (the method requires computing the full Jacobian matrix which depends on the number of neural network weights) I am grateful that they provided comparisons with wallclock time to SGD.\n\nAnd there is this notion that second-order methods have been shown to not generalize as well as first-order methods, and thus it was nice to see that they had an experiment where they tested generalization.\n\nThe background information was also nice to read.\n\nWeaknesses: They do not compare it with other methods optimization methods, such as Adam (a first-order method) or natural gradient (a second-order method), and I would have thus liked to have seen comparisons to these.\n\nI would have also liked to see a test loss v. time/epoch for the AFAD-LITE task as well (they only have it for the RSNA Bone Age task), at least provided in the appendix if there was not enough space.\n\nIn the references, there are numerous citations of the arXiv versions of papers, but I suggest the authors replace them with the conference/journal versions if those papers were accepted in conferences/journals (and I spot some that were).\n\nOther comments: (i) In the first sentence of 3.3., I think one should replace \u201cGGN has quadratic convergence rate\u201d with \u201cfull-batch GGN has quadratic convergence rate,\u201d as in the subsequent sections you are discussing mini-batch GGN."}