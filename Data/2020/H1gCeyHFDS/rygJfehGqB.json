{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a second order optimization algorithm, along with convergence proof of the algorithm in both batch and minibatch setting. The effectiveness of the method is demonstrated on two regression tasks. My overall assessment is that the method is still quite limited and the method itself is not novel, but I am willing to change my score to accept if my concerns have been addressed.\n\n(1) The method is not novel. The same algorithm was proposed and applied to the RL setting [1]. \n(2) The method is still quite limited to 1-output function scenario, where the NTK matrix is easy to compute. This limitation though is not mentioned in the paper. I hope the author should have a discussion on this and admit this limitation.\n(3) Also due to (2), the experiments shown in the paper are on toy data and hence lack of strong empirical support.\n(4) The method doesn't scale up to large batch size.\n(5) In the theoretical section, the paper states \n\"However, to our knowledge, no convergence result considering large learning rate (e.g. has the same scale with the\nupdate of GGN) has been proposed.\"\nThis is not true. Here are some papers: [2,3,4]\n(6) Lack of some second order optimization baselines, e.g., KFAC.\n\nMisc:\n(1) For section 3.3, first of all, (B) costs at least half of (A) as it requires a backward pass. \n(2) For section 3.3, the authors write:\n\" What is different is that GGN also, for every input data, keeps track of the output\u2019s derivative for the parameters; while in\nSGD the derivatives for the parameters are averaged over a batch of data.\"\nIs there a simple way of implementing/computing the gradient for *every* input data on GPU? How is that compared to computing the average? I wish to see more evidence of showing they're the same as authors claimed.\n\n[1] Towards Characterizing Divergence in Deep Q-Learning.\n[2] The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning.\n[3] Fast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron).\n[4] Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition"}