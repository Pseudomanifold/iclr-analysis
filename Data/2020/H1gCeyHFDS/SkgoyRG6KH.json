{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Based on recent progress on the connection between neural network training and kernel regression of neural tangent kernel, this paper proposes a Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. For overparameterized shallow networks, the authors proved global convergence of the proposed algorithm in both full-batch and mini-batch setting. To my knowledge, the proof of global convergence in the mini-batch setting is novel and might be of independent interest for other work.\n\nOverall, this paper is well-written and easy to follow. It's interesting to see that the proposed algorithm can achieve quadratic convergence while most previous papers only get linear convergence. \nGiven that, I'd like to give a score of 6 and I'm willing to increase my score if the authors can resolve my concerns below.\n\nConcerns:\n- For the algorithm, if I understand correctly, it's actually same as natural gradient descent with generalized inverse. I think the authors should make the connection clear. I would like to see more discussions with natural gradient descent or Newton methods in the next revision.\n- The authors claim that the proposed GGN algorithm only has minor computational overhead compared to first-order methods. I doubt if it's true in general. In section 3.3, the authors argue that computing individual Jacobian matrices for every example in the minibatch has roughly the same computation as the backpropagation in SGD. As far as I know, it's not true in practice. In addition, the inverse of the Gram matrix can also be expensive when the output dimension (the dimension of y) is large.\n\nMinor Comments:\n- In the paper, the theoretical results are based on the assumption of smooth activation function. I wonder if it is possible to include the case of ReLU activation as it's the most popular activation function in deep learning.\n- I don't have a good understanding about why mini-batch version would converge after reading the paper. To me, second-order methods with mini-batch estimation of the preconditioner would lead to biased gradient estimation. Could you comment on that?"}