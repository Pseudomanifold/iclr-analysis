{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper can be viewed as being related to two bodies of work:\n\n(A) The first is training programmatic policies (e.g., https://arxiv.org/abs/1907.05431).  The most popular idea is to use program synthesis & imitation learning to distill from a programmatic policy from some oracle policy.  \n\n(B) The second is training compact policies using a complex model-based controller (e.g., Guided Policy Search).   The idea is to use a step-wise model-based controller to design a good trajectory that maximizes reward, while not deviating too far from the current policy.  Then the new policy is learned from this trajectory.\n\nThe authors contrast with (A) via \"our student does not learn based on examples provided by the teacher, but is trained to mimic the internal structure of the teacher\". The authors contrast with (B) in part by claiming that \"the teacher must mirror the structure of the student\", which is supposedly harder.\n\nThus, it seems much of the intellectual merit & novelty lies how the proposed method tackles this \"structure\" problem, from both the teacher and the student side.  However, I'm having a hard time appreciating this aspect of the proposed approach.  I'm also confused by how the \"student does not learn based on examples provided by the teacher\" if it's doing imitation learning on trajectory-level demonstrations. Can the authors elaborate on this point further?\n\nThe experiments seem ok.  The idea of training programmatic polices that \"inductively generalize\" has been done before on arguably more difficult tasks (see Table 2 in https://arxiv.org/abs/1907.05431).   To contrast with this prior result, it seems the main point is that prior work relied on domain-specific program synthesizers.  Can the authors elaborate on this point further?\n\nMinor comments:\n\n-- Adaptive teaching is a pretty ambiguous term, and I think misleading within ML community (cf. https://arxiv.org/abs/1802.05190). I recommend a different algorithm name.\n\n-- Deriving the variational objective is a lot of work to reduce it to just trajectory design.  Seems overkill."}