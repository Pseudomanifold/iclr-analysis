{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "# Summary\n\nThis paper proposes a technique for synthesis of state machine policies for a simple continuous agent, with a goal of\nthem being generalizable to out-of-distribution test conditions. The agent is modeled as a state machine with constant\nor proportional actions applied in each node (regime), and switching triggers between the regimes represented as\nlength-2 boolean conditions on the observations. The technique is evaluated on 7 classic control environments, and found\nto outperform pure-RL baselines under \"test\" conditions in most of them.\n\n# Review\n\nI am not an expert in RL-based control, although I'm familiar with the recent literature that applies formal methods to\nthese domains. I find the studied settings valuable albeit fairly limited, but the paper's method undeniably shows\nnoticeable improvement on these settings. Inductive generalization is an important problem, and the authors' approach of\nlimiting the agent structure to a particular class of state-machine policies is a reasonable solution strategy.\n\nThat said, the complexity of synthesizing a state machine policy clearly caused the authors to limit their supported\naction and condition spaces considerably (Figure 6). That, I'm assuming, limits the set of applicable control\nenvironments where optimization is still feasible. The authors don't provide any analysis of complexity or empirical\nruntime of the optimization process. Breaking it down for each benchmark would allow me to appreciate the optimization\nframework in Section 4 much more. As it stands, Section 4 describes a complex optimization process with many moving\nparts, some of which are approximated (q* and p(\u03c4|\u03c0,x\u2080)) or computed via EM iteration until convergence (\u03c0*). It is hard\nto appreciate all this complexity without knowing where the challenges manifest on specific examples.\n\nSection 4.2 needs an example, to link it to the introductory example in Figure 1. The \"loop-free\" policies of the\nteacher are, in programmatic terms, _traces_ of the desired state machine execution (if I understand correctly), but\nthis is not obvious from just the formal definition.\n\nThe EM optimization for the student policy makes significant assumptions on the action/condition grammars. Namely, the\nalgorithm iterates over every possible discrete \"sketch\" of every program, and then numerically optimizes its continuous\nparameters (Appendix A). When the action/condition grammars grow, the number of possible programs there increases\ncombinatorially. Is there a way to adapt the optimization process to handle more complex grammars, possibly with\ndecomposition of the problem following the program structure?\n\nSection 5 needs a bit more details on the Direct-Opt baseline. It's unclear how the whole state machine policy (which\nincludes both discrete and continuous parts) is learned end-to-end via numerical optimization. Granted, the baseline\nperforms terribly, but would be great to understand how it models the learning problem in order to appreciate why it's\nterrible.\n\nWhy were the \"Acrobot\" and \"Mountain car\" benchmarks removed from the main presentation of results?"}