{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a notion of feature robustness which is invariant with respect to rescaling the weight. The authors discuss the relationship of this notion to generalization.\n\nThe definition of feature robustness is interesting and could potentially be useful. However, the paper has the following major issues:\n\n1- Related work: It seems that authors are unaware of the related work in this area. There are many relevant work in this area that connect feature or weight robustness to generalization look at [1,2,3,4] for some examples. I suggest authors to do a comprehensive literature review.\n\n2- Theoretical results: The theoretical results presented in the paper have very limited value. For example, authors fail to really connect their suggested measure to generalization in any meaningful way. Instead they end up decomposing the test error to the sum of their robustness measure and the gap between robustness and test error which is trivial. I suggest authors to look at the literature on PAC-Bayesian and compression-based bounds to connect their suggested measure to generalization.\n\n3- Experiments: The experiments are not really convincing. The empirical results show that the suggested measure can correlate with generalization when training with different batch-sizes. When varying other things, the measure is not really correlated. Therefore, this is not any better than the version suggested by Keskar et. al. Moreover, the experiments are very limited and I suggest authors to look at more controlled setting to verify the relationship of their measure to generalization. Also, when looking at the generalization, it is important to set the stopping criterion based on the cross-entropy instead of number of epochs.\n\n[1] Dziugaite and Roy. \"Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data\". AAAI, 2017.\n[2] Neyshabur et. al. \"Exploring Generalization in Deep Learning\", NeurIPS 2017.\n[3] Arora et. al. \"Stronger generalization bounds for deep nets via a compression approach\". ICML 2018.\n[4] Wei and Ma. \"Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation\", NeurIPS 2019.\n\n\n\n"}