{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a flatness measure that is invariant to layer-wise reparametrizations in ReLU networks. The notion of feature robustness, which is a notion the paper proposes, connects the flatness measure to generalization error.\n\nThis paper should be rejected because it is not well-placed in the literature. Similar notions with the proposed flatness measure have repeatedly appeared in the literature. This paper needs to discuss novel insights.\n\nMajor comments:\n1) Many studies proposed or mentioned the flatness measures listed in Table 1 [1, 2, 3, 4]. One of the most relevant work will be [1]. It appears that the Fisher-Rao norm [1] has several advantages over the proposed measure in the submitted paper.\nA) Fisher-Rao norm is invariant to a broader range of linear transformations.\nB) Fisher-Rao norm does not rely on the Hessian, which is more suitable for non-smooth ReLU networks.\nAdditionally and importantly, the Fisher-Rao norm has a direct connection with the size of input gradients, which has a strong relationship with the feature robustness. It is strongly encouraged to discuss the connections and comparisons with the Fisher-Rao norm.\n2) On the connection to the generalization error, Theorem 10 relies on the strong assumption defined in Definition 9. Given the high ability of deep networks to express many functions, assuming that \\phi(S) is epsilon-representative seems difficult to justify. This paper should discuss why the assumption is reasonable. Otherwise, it is hard to claim that this paper connected the modified flatness measure to generalization error.\n\n[1] Liang et al. \"Fisher-Rao Metric, Geometry, and Complexity of Neural Networks.\" AISTATS 2019\n[2] Achille et al. \"Emergence of Invariance and Disentanglement in Deep Representations.\" JMLR 19 (2018)\n[3] Neyshabur et al. \"Exploring Generalization in Deep Learning.\" NeurIPS 2017\n[4] Tsuzuku et al. \"Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis.\" arXiv:1901.04653"}