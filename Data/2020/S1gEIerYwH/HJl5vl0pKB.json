{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Contribution\nThis paper proposes algorithm for transferring knowledge from easy -to-solved to complex tasks or from already solve to new tasks. It relies on homotopy functions and sequentially solves a sequence of optimization problems where the task distribution is gradually deformed from a source task to the target task. Theoretical guarantees are provided and proven in a strongly convex setting. The main results from the theory show that the distance between the final solution and its optimal are less or equal to  relative to the distance of the initial source solution to its optimum. So a near optimal solution for the source task will lead to near optimal solution for the target task. Regression and Classification experimentations show competitive results compared to  random and warm-start initialization schemes.\n\nClarity\nOverall, the paper is well written, well motivated and well structured. The technical content is also very clear and excellent.\n Minor point: Seems that there is a notation error in proposition G.1 and its proof (i instead of i+1).\n\n\nNovelty\nThe novelty in this work seems to be the application of homotopy methods to the transfer learning settings. The mathematical guarantees are also new and may even offer new ways to interpret fine tuning methods that have been so successful in recent literature. \n\nHowever, given the  non-convexity of DNNs, it seems like the analysis in the non-convex settings and its implications  should be part of the main text.\n\nExperiments:\nOverall, the experiments are very insightful but limited since you only show the training loss and the validation performance is not evaluated at all. Other things that would could be  beneficial in better assessing the quality of your method are: comparison to Curriculum learning methods, more in depth analysis of the impact of k, and gamma in both regression and classification settings, and solving  toy convex optimization problems to bridge the gap between theory and application.\n\n\nPreliminary rating:\n* Accept *"}