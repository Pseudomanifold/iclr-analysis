{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a variation of clustering algorithm for feature extraction, by employing the variational Bayes method, on time series data that\u2019s prone to modulations in frequency, amplitude and phase. This approach includes the proposal of a new architecture, multiple decoder RNN-AE (MDRA), in order to capture the dynamical system characteristics. The proposed contribution has been validated by its performance in a classification task on a synthetically modulated time series data as well as driving behavior data.\n\nMain Argument\nThe paper does not do a good job of demonstrating if the problem being addressed is a significant one. The authors incorrectly attribute RNN-AE to Srivastava et al 2015 to establish their motivation and that RNN-AE is indeed a problem which they are addressing by including multiple decoders. But, the architecture presented in Srivastava et al. 2015 is not an RNN-AE but an LSTM-AE. This leaves us wondering what could be the overall impact of the current proposed architecture to the literature. For example, the authors could have instead tried multiple decoders on the LSTM-AE. Also, the use of EUNN (Jing et al. 2016) to avoid vanishing gradient is rather naive, given that sophisticated training techniques for RNNs are available in the literature. In fact, GORU has been proposed by the same authors (Jing et al 2017) of EUNN (Jing et al. 2016) and has shown to be superior to EUNN. This choice of EUNN over GORU or even the use of RNN over LSTM leaves us with little about the impact of this work. The authors could have indeed focused on comparing their proposed method with the current state-of-the-art architectures which include LSTM-VAE/GMVAE and many others. The experiments, although look fine, lack the robustness/generalizability often desired of such complex architectures by trying it on more complex sequence data. Specifically, a section on experimental settings (initialization scheme, hyperparameter optimization, etc.) haven\u2019t been touched upon.\n\nFurther supporting comments:\n1. The MDRA architecture was proposed based on the premise of capturing the dynamical system structure. The interpretation of RNN as describing the dynamical system structure aka h, although good, is particularly serving no specific purpose in this work as the authors themselves point out the emphasis on the transformation rule, in Section 4 Paragraph 1 Sentence 5. Equating the U\u2019s, V\u2019s and W\u2019s to the dynamical system features is a very shallow reasoning and is too limited in explaining anything about the actual dynamics of the system under study.\n2. In the last sentence of Section 3 the authors indicate that they adopted the method, EUNN, proposed by Jing et al (2017). However,  the paper does not justify the choice. Another method, GORU was proposed by Jing et al (2019) and it seems to perform better than EUNN. Why not this one, for example, or anything else?\n3. In fact, given the incredible success of LSTMs in many application areas and the gamut of innovations that have been proposed for superior training of LSTMs, the paper may need to justify the use of a more simpler and problematic RNN-AE in the face of a more successful LSTM-AE (Srivastava et al. 2015). Maybe, a comparison to a similar but LSTM-based multi-decoder architecture would demonstrate the reasoning.\n4. Do the authors have a reason for not trying RNN-VAE/LSTM-VAE with multiple decoders instead of MDRA? What are the distinct benefits of MDRA, in terms of latent representations, over a VAE/GM-VAE/Normalizing flows?\n5. As the discussion of the work relates to RNN, I believe this work should have a wide range of applications beyond just time series data i.e. it should generalize well to any sequence data (speech recognition, sentence classification), The current experiments although decent could have been extended to include more complex datasets and thereby demonstrate the generalizability of this architecture.\n\n\nOther comments:\n1. Section 2 last paragraph, Pascanu et al is out of place. Arjovsky et al should have been cited in the last sentence of the previous paragraph. \n2. In Section 1 Paragraph 2 Sentence 2, the authors incorrectly attribute RNN-AE to Srivastava et al when in fact they proposed LSTM-AE.\n3. Section 1 Paragraph 2 may benefit from a rewrite as it is really vague in building-up the motivation behind the proposed work.\n4. Some use of terminology is unclear\n   1. Example: In Section 1, Paragraph 1, Sentence 3 I doubt if phase shift, compress/stretch, and length variation are outcomes of time series variation or if they are sources of time series variability. \n   2. Example: Section 3, Paragraph 1, Sentence 2: The architecture of the main unit is called cell. Instead, the primary component of an RNN architecture is called a cell (hidden node in other words).\n5. Typos\n1. Section 4 Heading: DERIVATION not DEVIATION\n2. Section 4.1 Paragraph 2. Variables not valuables\n3. Section 6 Heading: DISCUSSION instead of DISUCUSSION\n4. Appendix C Heading: EXAMPLE not EXAPMPLE\n5. Appendix D: quiet not quite\n6. Figure 11 box 2: output not outout\n7. Figure 11 caption: Fully not Full\n8. Section 5.2 Line 1: drivers\u2019 not driver\u2019s\n9. Appendix A3. In the expectation of log_beta with respect to a Gamma distribution, \u2018x\u2019 should have been \u2018beta\u2019 instead.\n"}