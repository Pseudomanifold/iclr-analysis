{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "SUMMARY: A new similarity function replacing the dot product of key and query in attention modules - instead take a shared-weighted sum\n\nGood paper, sound theory, very clear explanations. Literature review was sufficient to explain the problem and underlying theory.\n\nResults look convincing, although they cannot be verified unless code is shared.\n\nReasonable direction of exploration - there are several possible similarity functions, this paper explores one of them that offers significantly less computational resources, which are essential for on-device applications. Thorough exploration of this idea was done and I am convinced this is a good alternative to regular attention.\n\nAll the rest of the paper was to incorporate this new method in different tasks in different architectures using or not using attention, and seeing the differences in terms of computational resources. Well thought experiments and results. Again, cannot be verified unless code is shared. "}