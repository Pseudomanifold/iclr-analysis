{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a new mechanism to perform the attention operators. The similarity between a key and a query is performed as the dot product between a trainable weight and the addition of the key and query.  The proposed Siamese attention operator is much more efficient than prior attention methods in terms of speed. The evaluation on a few computer vision tasks shows the presented method performs as well as the typical attention methods, but it runs much faster. \n\nFirst of all, I think this attention method should be quite useful for various neural networks. It is faster and performs equally well as other attention operators.  However, I do have some concerns about this method.\n\n- It is not clear to me why this method works. (a+b)^T*w is a strange expression to compute the similarity between a and b. No much explanation or intuition is given in the paper, and I have no clue why this works.\n\n- It seems to me that the proposed method performs slightly worse than the regular attention, according to Figure 6. \n\nMy overall rating is borderline. It would be great if the authors can resolve my concerns.\n\nOther questions:\n- Why SANet (MobileNetv2+SAO) has fewer parameters than MobileNetv2 in Table 3?\n- Is attention an operator that significantly slows the speed of the whole network? If not, the speedup of attention is not that important.\n"}