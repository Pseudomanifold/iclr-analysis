{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new policy gradient method that is based on stochastic mirror descent. It has been shown theoretically and empirically that the method achieves more sample efficiency. \n\nOverall, the proposed idea is interesting. The paper has good contributions in both theoretical and algorithmic aspects to policy optimization family. The empirical results are also very promising. I have only some following concerns.\n\n- It would be really nice if the discussions on connections to existing policy gradient methods on page 4 can be elaborated. This is to make the readers understand better how the reduction from the proposed method to an existing one can be made. It would also be more interesting after such reductions can be made, one can compare the sample efficiency of the proposed method with such state-of-the-art policy gradient methods. That means Table 1 can be extended with rows of such methods. \n\n- The convergence analysis in Theorem 2 and 3 are really strong, but it would be very useful if there are ablations that can reflect the trade-off between the convergence property and hyperparamters used in those theorems.\n\n- The contribution of the Bregman distance D to the update should be worth further discussions. It is an important part of SMD, but for policy optimization one might want to see how it contributes to stabilizing the update OR policy exploration. As seen, with a different choice of D, the method is reduced to policy gradient or entropy-regularized policy optimization. Ablations without D might show some interesting results too.\n\n\n- Some minor comments:\npage 8: \"The result in Figure 1 shows that MPO converges faster significantly and achieves a better performance than both REINFORCE and MPO.\" -> should be \"... both REINFORCE and VPG.\""}