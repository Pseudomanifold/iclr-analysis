{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper proposes MPO, a policy optimization method with convergence guarantees based on stochastic mirror descent that uses the average of previous gradients to update the policy parameters. A lower-variance method, VRMPO, is then proposed that matches the best known convergence rate in the in the literature. Experiments show that (1) MPO converges faster than basic policy optimization methods on a small task, and (2) VRMPO achieves a performance comparable to, and often better than, popular policy optimization methods (TD3, DDPG, PPO, and TRPO) on MuJoCo.\n\n[Decision]\nThe proposed methods are well-grounded. The experiments are, however, limited and miss important baselines discussed in previous sections. The presentation is not clear. Imprecise statements, undefined terms, and grammatical errors make the paper hard to follow. Overall, I am leaning towards rejecting the paper.\n\n[Explanation]\nSection 5 provides a comparison between the convergence rates of VRMPO and previous methods (VPG, REINFORCE, SVRPG, and HAPG). I was expecting to see a similar comparison in the experiments section but SVRPG and HAPG do not appear in the experiments although their convergence rates are comparable to VRMPO. I especially want to know how VRMPO differs from HAPG. Their convergence rates are the same and their updates look similar.\n\nThe description below Fig. 1 says that VRMPO converges faster and achieves a better performance than the baselines. While VRMPO does converge faster, all the three methods seem to converge to the same (optimal) solution.\n\nThe paper needs more polished presentation. Here are some examples for improving the writing:\n- The introduction section: \"our algorithm outperforms state-of-the-art bandit algorithms in....\" The compared methods are RL algorithms.\n- \"converges faster significantly and achieves a better performance than both REINFORCE and MPO\" -> \"... than both REINFORCE and VPG\".\n- \"The task is to estimate the value function of state s_1\" -> This seems to be a control task where the goal is to achieve the highest return.\n- \"Eq.(23) has a closed implementation\" -> What is a closed implementation?\n- Some terms like \"projected gradient\" and \"baseline\" (in the context of variance reduction) are not defined. The compared methods in 6.2 are not described in the paper (except TD3 whose brief description appears in D.5).\n- \"The traditional policy gradient methods such as REINFORCE, VPG, and DPG are all the algorithms update parameters in Euclidean distance\" -> \"... such as REINFORCE, VPG, and DPG update parameters in Euclidean distance\"\n- In Table 1 the performance of VRMPO on Reacher is in bold while it is not the maximum value.\n- \"It is different from (Du et al., 2017)...\" -> \"VRMPO is different from...\"\n- \"due to it doesn't require...\" -> \"because it does not require...\"\n\n[Minor comments]\n- How is the bound in Eq. (18) proved?\n"}