{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed a variant of policy gradient algorithm with mirror descent update, which is a natural generalization of projected policy gradient descent. The authors also proposed a variance reduced policy gradient algorithm following the variance reduction techniques in optimization. The authors further proved the convergence of the proposed algorithms and some experiments are conducted to show the effectiveness of their algorithms. The paper is not written in a very good way due to many typos and misleading notations. Moreover, there seem to be some technical issues in the proofs of both main theorems.\n\nThe notations are inconsistent. In eq (14) the trajectory index is k, while in eq (16) the index changed to n. In eq (28), the trajectory is denoted by $\\tau_j^t$, while in the algorithm, there is no index $t$ in $\\tau_j$.\n\nDefinition of P in eq (36) seems to overlap with that of $\\mathcal{G}$ in eq (10), which means the same quantity was defined twice using two different notations.\n\nSection 3.1 is almost the same as in Ghadimi et al., (2016), both the theorem and remarks. Since this is not a new finding, I suggest the authors to simplify the current statement in this subsection.\n\nIn Theorem 1 & 2, the authors used the criteria $\\mathbb{E}[\\|\\mathcal{G}(\\theta_n)\\|_2^2]$ to establish the convergence result. However, they used $\\mathbb{E}[\\|\\mathcal{G}(\\theta_n)\\|_2]$ to establish the convergence result in Theorem 3. This is kind of confusing because in different criteria, the complexity result will be different. \n\nIt seems that eq (32) is the same as eq (28). Can the authors elaborate the differences discussed in the paragraph after eq (32) in more detail? \n\nIn eq (40), $\\hat g_k$ is defined as the average gradient over all the iterates up to $k$, which means $\\hat g_k$ is a function of $\\theta_1,\\ldots,\\theta_k$. But $g_k$, according to eq (21) in Algorithm 1, is just defined based on the current policy parameter $\\theta_k$. I am skeptical why these two terms will be equal. This seems to be a technical issue of the whole proof of Theorem 2. \n\nThe derivation in (55) seems incorrect. In particular, the authors called Lemma 3, which is the result in Fang et al. (2018). However, the recursive gradient is defined differently in the current paper and in Fang\u2019s paper. In Fang\u2019s paper, at different iterations, the data are sampled from an unknown but fixed data distribution. But in this paper, since the data are sampled from the current policy which varies at each iteration, the data distribution is changing all the time. Therefore, Lemma 3 does not hold in the setting of this paper. \n\nIn all the experiments, it is strange that the proposed VRMPO was not compared with any of the other variance reduced algorithms listed in Table 1. I think it would be more convincing to demonstrate the improvement in Table 1 by comparing the proposed VRMPO with at least one other variance reduced algorithm.\n\nOther comments:\n\n1. On page 15, the first sentence of the proof: \u201ctrjecories\u201d -> \u201ctrajectories\u201d\n2. What is $\\mathcal{J^*}$ in eq (39)? It was not defined in the paper.\n3. In the statement of Lemma 3, the authors said \u201ctelescoping Eq. (3) ...\u201d However, Eq (3) in this paper is the definition of policy gradient. I assume this is a typo? \n4. What is $\\epsilon_1$ in eq (50)? It seems that this term does not appear in Lemma 4.\n"}