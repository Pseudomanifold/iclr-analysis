{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper models the neural architecture search (NAS) as a network pruning problem, and propose a method to sparsify the super-net during the search of architectures.\n\nOverall, the novelty in this paper is not strong and their experimental performance is weak compared with recently published papers. I do not see a need to have such a new algorithm in the NAS literature. Please see the question below:\n\nQ1. \"Bayesnas: a bayesian approach for neural architecture search\". ICML 2019\n- This paper also takes a pruning's perspective for NAS, but it is much more efficient than the proposed one. Would the authors have some discussion and experimental comparison with this paper? Specifically, Bayesnas considers more complex sparse patterns then the submission.\n\nQ2. \"adaptive stochastic natural gradient method for one-shot neural architecture search\". ICML 2019\n- Could the authors have some discussion with this paper? This paper has comparable performance, but it is also much faster.\n\nQ3. What are the benefits of the proposed method? \n- From Tables 2 & 3, the proposed method is not better than STOA on the accuracy or number of parameters. \n- CNAS + autoaugmented can offer better accuracy, but the comparison is not fair as different pre-processing method is used."}