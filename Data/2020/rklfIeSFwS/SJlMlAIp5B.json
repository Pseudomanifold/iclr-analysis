{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper aims to propose a novel framework for neural architecture search. Although there have been many solutions in the literature, the authors try to build a NAS model that is sparse in structure while being similarly effective as conventional dense models.  The method is straightforward - they select a single fixed operation as edges, and channels as vertices, and the problem of NAS can be directly solved by a gradient descent method. The sparsity can also be achieved on the level of channels.\n\nI have three major concerns, including a lack of novelty, unconvincing experiments, and poor presentation of the work. First, the proposed method is quite straightforward and can be viewed as a quick extension of existing structures. Simplifying the selection of operations make it easy for computation, while it also constrains the applications of the proposed framework. Second, the reported results do not seem to be promising since the improvement was marginal. It is also very difficult to tell whether the contribution is brought by the proposed sparse structure or the adoption of autoaugment since the baseline methods are not applied with it. Third, the paper has not been well written and there are grammatical mistakes throughout the manuscript. I attached the original abstract of the paper and my corrected version below.\n\nThere is growing interest in automating designing good neural network architectures. The NAS methods proposed recently have significantly reduced architecture search cost by sharing parameters, but there is still a challenging problem of designing search space. We consider search space is typically defined with its shape and a set of operations and propose a channel-level architecture search (CNAS) method using only a fixed type of operation. The resulting architecture is sparse in terms of channel and has different topology at different cell. The experimental results for CIFAR-10 and ImageNet show that a fine-granular and sparse model searched by CNAS achieves very competitive performance with dense models searched by the existing methods.\n\nThere is a growing interest in automating designing good neural network architectures. The NAS methods proposed recently have significantly reduced costs of architecture search by sharing parameters, but there is still a challenging problem of designing search space. \nConsidering that existing search space is typically defined with its shape and a set of operations, we propose a channel-level architecture search (CNAS) method using only a fixed type of operation. \nThe resultant architecture is sparse in terms of channels and it has different topologies at different cells. \nThe experimental results for CIFAR-10 and ImageNet show that a fine-granular and sparse model searched by CNAS achieves very competitive performance with dense models searched by existing methods.\n"}