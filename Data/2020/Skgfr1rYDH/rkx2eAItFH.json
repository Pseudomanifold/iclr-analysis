{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tends to explain how the tradeoffs between convergence speed and convergence performance are made by different optimization methods. Moreover, the paper modifies Adam-like updating rules and proposes a novel optimization methods, SoftAdam. Finally, the paper performs numerical experiments on traditional image classification tasks as well as language modeling tasks.\n\nPros\nThe paper involves the language modeling tasks in empirical results besides traditional image classification tasks, which helps to explain the convergence performance of optimization methods in a wider range of applications.\n\nCons\n1. The writing of this paper is not well organised. Section 1 lacks detailed description of the main idea and the proposed optimization methods, which actually confuses the reader. Section 2 describes too much details of SGD and Adam, and lacks a clear \"intuition\" which readers exactly expect.\n2. The notation in the paper is little confusing. In the update rule of z_{t+1} in Section 2, what is meaning of z? In Section 3, n_t and n_\\infty are used before a proper defination, and what is relationship between \\alpha and \\alpha_t in the implementation of the SoftAdam?\n3. The motivation of the proposed method is weak. Such a weak motivation is mainly because of the insufficient \"intuition\" in Section 2. The author mentions \"the Hessians in deep learning problems are not diagonal\", but does not provide further explanation on why more importance should be lay on serving both max and min eigenvalues.\n4. There are also several minor problems on the numerical results. Firstly, why the colors of \"softAdam\" and \"sgd\" are switched several times in Figure 1? Secondly, the figural result in Figure 1 and he numerical results on language processing models both lack a display of the confidence range.\n     "}