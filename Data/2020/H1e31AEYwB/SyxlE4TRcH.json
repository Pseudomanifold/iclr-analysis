{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces the concept of stiffness: a measure of the change in the loss of sample A due to a gradient step based on sample B. It analyses the expected dynamic for A, B samples from the same and different classes, as well as, samples from the train and test sets.\n\nTo better understand the dynamics of optimization in neural networks is an open and important problem and the paper is clearly motivated in this regard. The proposed method is straight forward and I am not aware of a similar method. \n\nIn addition to that, the paper also introduces \"dynamical critical length \u03be\" which is the stiffness of A, B samples based on the cosine similarity of the respective inputs (section 2.4). A linear estimator of when this length becomes 0 is also introduced. Confusingly this is also called the \"dynamical critical length \u03be\" in section 4.2. Later on the term \"dynamical scale \u03be\" and \"dynamical critical scale \u03be\" seem to be used interchangeably. Figure 6 mentions the \"critical length \u03c7\" on the y-axis which seems to be a typo as no such measure was introduced.\n\nThe equivalence between eq. 2 and the two parts of eq. 3 is not obvious. We'd appreciate if the authors would provide a proof of such. \n\nOverall, the paper is written in a simple language but paragraphs remain surprisingly hard to understand. An example of such is e.g. section 4.4: What do the authors mean by \"characteristic distance\" between two input points? What is \"the typical scale of spatial variation\" of a function? etc.\n\nThe paper concludes that:\n\n1.) there is a link between generalization and stiffness\n2.) stiffness decreases with the onset of overfitting\n3.) \"general gradient updates with respect to a member of a class help to improve loss on data points in the same class\"\n4.) \"The pattern breaks when the model starts overfitting to the training set, after\nwhich within-class stiffness eventually reaches 0\"\n5.) This is observed for different models on different datasets\n6.) \"we observed that the farther the datapoints and the higher the epoch of training, the less\nstiffness exists between them on average\"\n7.) \"the higher the learning rate, the smaller the \u03be\"\n\nVerdict: Reject\n\nThe conclusions are self-evident. The paper fails to demonstrate the usefulness of stiffness and most results are expected and provide little to no insights into the optimization dynamics of deep neural networks. In fact, the reasoning in this paper is almost tautological (conclusions 1-6).\n\nE.g. if the A, B samples used to compute stiffness are separately drawn from the train and test set then stiffness is a proxy for the difference between the train error and the test error after another gradient step. The authors then compute stiffness at different points of the optimization procedure and conclude that stiffness decreases when the network starts to overfit. Since overfitting is the point in training where train error and test error diverge it is obvious that this can also be observed with regards to \"stiffness\". Hence, the reasoning is circular.\n\nConclusion 7 is slightly different in that it observes that larger learning rates result in smaller \u03be which, given the previous paragraph, we can rewrite into the statement \"larger learning rates generalise better\". This is a well known empirical observation and has been discussed thoroughly (e.g. on connection with flat and sharp minima or learning rate decay schedules). \n\nDisclaimer: This review was done on short notice. "}