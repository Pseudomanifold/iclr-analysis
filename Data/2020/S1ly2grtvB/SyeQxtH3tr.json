{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper describes a method for learning in semi-supervised settings with label noise. This is an interesting topic with a relatively scarce literature. The proposed method works by first postulating a generative model for the labelled/unlabeled/label-corrupted data. The model is then fitted using a standard variational lower bound maximization.\n\nThe approach is elegant, and appears to work empirically well. Unfortunately, I do not seem to really understand the rationale behind the main novelty (even at an heuristic level) of the paper which is contained in Section 3.2.1, namely Equation (10) which describes the intensity of the label corruption. Why is it a sensible idea? There is only 3 lines of comments after equation 10, which seems a bit short since this is the crux of the paper. Also, the notations of equation (10) are very confusing (to me) since the function f(.) was used before to denote another quantity.\nMinor remarks:\n=============\n\n1. page4 was a bit difficult to digest at first reading since the author did not describe in the main text he form (i.e. factorization structure) of the variational distribution q.\n\n2. second line of Equation 14 seems to be wrong, while the 1st and last line seems correct -- the authors may want to double check\n\n3. the term \"labeled loss term\" was not properly defined, which makes the reading a bit difficult even if one eventually gets what the authors mean\n\n4. In equation (6) I do not understand why f(eps) = log[(C-1)(1-eps)/eps]. Why isnt it equal to log[p(hat(y)|y)] ?\n\n5. notations z_a, z_1, etc.. do not seem to be consistent throughout the text\n\n6. it is quite surprising to me that, in the low data-corruption regime, the method appears to be competitive/better with consistency-based method such as the MT approach.\n\n7. naive question: why directly modeling \\eps as a function of z_a or (z_a, y), possibly parametrized by a neural network, a bad idea?\n\nIn conclusion, it feels like the method has a lot of potential (in views of the numerical simulations) -- if the authors could clarify the exposition, this could be a very good contribution to the field. The method appears to be conceptually simple (i.e. postulate a generative model + fit it by maximizing the ELBO + one trick to estimate \\eps), which is a good thing -- what is missing, I think, is a real discussion of why the proposed manner to estimate \\epsilon is sensible.\n\nEdit after reading [1] \n======================\nThe proposed generative model is same -- the authors should make this very clear in the paper. Although is is acknowledged: \"In this work, we implement the idea in (Langevin et al., 2018)\" -- this is only in section \"4.1 IMPLEMENTATION DETAILS\". After reading (1), it is clear that the novelty of the paper us much less than what I had initially thought. The only difference is in Section 3.2.1, and this Section is far from being satisfying.\n\n\n[1] \"A Deep Generative Model for Semi-Supervised Classification with Noisy Labels\", Langevin & al"}