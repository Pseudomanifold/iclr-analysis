{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "###  Summary\n\n- The paper demonstrates that neural networks that appear to have forgotten an old task still contain useful information of that task in their representation layers. \n- The paper proposes to meta-learn an update rule (parameterized by an LSTM) that acts as a gating mechanism (or plasticity) for each learnable parameter at meta-test time. \n- To meta-learn the update rule, the paper proposes minimizing the difference between representations of a teacher and student neural network. The teacher neural network learns from a batch of data sampled IID from the distribution of the complete dataset whereas the student neural network samples a batch only from the current task.\n\n### Decision with reasons \n\nI vote for rejecting the paper.\n\n1- The claim that neural networks forget mostly due to a miscalibration of the output layer is not well supported empirically (The drop in readout accuracy in Figure 1 is still significant). If the claim is only to the extent that the drop in readout accuracy is slower than original accuracy, then it's not interesting or new. (This is what I believed in before reading the paper as well). \n\n2- While the underlying idea in the paper for learning an update rule is promising and sound, the paper is missing baselines that also use the meta-training dataset in some way. Moreover, a meta-learned update rule is only useful if it can discover some general underlying learning principles. In this paper, the meta-train and meta-test settings are too similar to see if that is the case. \n\n\n### Supporting arguments for the reasons for the decision.\n\n1- The paper claims that catastrophic forgetting in a neural network is partly due to miscalibration of the last layer, and the representation layer of the neural network still contain useful information. However, the only supporting evidence for this claim is that readout accuracy does not drop as quickly as the original accuracy (Figure 1). \n\nFirst, the drop in readout accuracy is still significant to term forgetting 'catastrophic.' Secondly, figure 1 only report results after 300 steps. A more interesting question is the difference between the accuracies when the network has been trained on Task B till convergence. Secondly, it is important to report the read-out accuracy for task A on a random Neural Network of the same architecture to see if the Neural network is maintaining information in the representation layer (as the authors claim), or if a linear classifier on a random CNN is just a strong baseline (Shown to be a strong baseline in many recent papers. One example is Anand et.al 2019 [1])\n\n2- The motivation behind meta-learning an update rule is to discover underlying learning principles that generalize to new settings. Metz et. al. 2019, for example, showed that their learned update rule could be applied to networks with different architecture, non-linearities, and datasets (They went as far as showing it worked on different data modalities.)\n\nAll the results in this paper, however, are for a fixed architecture (The authors do look at generalization to unseen classes, but we care about generalization to arbitrary architectures/problems when meta-learning an update rule). The data at meta-train and meta-test time are also very similar (Different parts of the same dataset). The empirical results, consequently, are not very convincing. Moreover, by reading between the lines, it can be inferred that the learned update rule is very finicky. For instance, to generalize just to unseen initializations, the authors had to use 100 different initializations at meta-training time. That does not instill a lot of confidence in me about the stability of the learned update rule. \n\nFinally, the paper proposes the complex student-teacher learning paradigm while skipping a simple baseline: training on the student model by using data from Task B in the support set and using data from A and B in the query set during meta-training. A similar procedure was proposed by Javed and White 2019 [3]. (Note that the current baselines in the paper do not use the meta-training data at all which makes the comparison extremely unfair. Moreover, even a simple baseline such as LwF that does not use meta-training performs almost as well (See Table 1).) \n\n### Additional evidence that can change my evaluation\n \n1- Showing that the meta-learned update rule can be applied to different architectures/non-linearities/datasets (Train on one dataset, test on another). \n\n### Minor comments that did not play a part in my decision, but should be addressed nonetheless. \n\nThe paper should cite the classic paper by Yoshua. et.al (1991) which proposed the idea of meta-learning an update rule [2]. \n\nThe paper, in its current form, needs to be proofread and reorganized. There are many errors in the grammar (For example just in the first paragraph, New borns -> Newborns, a same -> the same (or 'a distribution')). I find passing my writing through the free version of Grammarly very helpful in getting rid of most such errors. \n\nThe organization of the paper is also not very clear. For example, the third paragraph in \"Related Work\" is about the method proposed in the paper whereas the second and fourth are about related work. \n\nThe writing is also occasionally ambigious. For instance: \n\n\"In human language acquisition, it is found that children who lost their first language maintain similar brain activation to bilingual speakers (Pierce et al., 2014).\n\nInspired by this fact, we propose a novel meta-learning algorithm that tries to mimic a multi-task teacher network\u2019s representation, an offline oracle in our sequential learning setup, since multi-task learning has simultaneous access to all tasks whereas our sequential learning algorithm only has access to one task at a time.\"\n\nIt is not clear how the method in the second paragraph is inspired from the statement in the first paragraph. \n\nI did not take writing quality in account when giving my score because openreview allows updating the paper during the review process. I hope that authors would fix these issues during the writing process. \n\nOn an unrelated note, the figures in the paper are well made and clear.  It is possible to understand the proposed methodology just from the figures. \n\n[1] Unsupervised State Representation Learning in Atari https://arxiv.org/abs/1906.08226\n\n[2] Learning a Synaptic Learning Rule https://mila.quebec/wp-content/uploads/2019/08/bengio_1991_ijcnn.pdf\n\n[3] Meta-Learning Representations for Continual Learning https://arxiv.org/abs/1905.12588"}