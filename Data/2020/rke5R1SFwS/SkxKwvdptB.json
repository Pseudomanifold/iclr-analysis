{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper explores learning without forgetting / the online learning setting. They employ a novel meta-learned learning algorithm to this end.\n\nWriting:\nFor the most part the writing was clear and easy to follow. There where a couple typos on the top of page 2 that should be fixed.\n\nMotivation:\nThe motivation for wanting meta-learning as well as various algorithmic choices are clear. \nThe one piece of motivation I did not fully understand is why not forgetting on the feature space is so important. My understanding of the method is that it should be applicable in both settings (with and without relearning the last layer). Infact, I would expect the difference between the meta-learned method and the baselines to only increase in this setting.\n\nI find the distillation based learning to be a clever alternative to the computationally heavy optimizing over past performance.\n\nExperiments:\nThis work provides a nice build up of experiments.\nExperiment 1 demonstrates the principles. In my opinion you should caution the reader given the meta-train, meta-test split. D_{B_1} and D_{B_2} are the same distribution and thus it will be easy for the learned update rule to memorize features. Given your learned update rule \narchitecture I doubt this will be the case though. I believe the authors are aware of this though as this issue is addressed in experiments 2 and 3.\n\nPlease include what the error bars are over in the captions.\n\nExperiments 2 and 3 are interesting and demonstrate the method on a more realistic setting. From the details it seems like this was difficult to get to work -- needing a complex schedule for example. Further elaboration or study of these details (e.g. ablations) would help the field. Also please include what the +- is for the experiments in table 1.\n\nFigure 6 is not referenced in the text. It was also difficult for me to understand though I finally got it.\n\nOverall, I believe the baselines could be made considerably stronger. Meta-learning expends considerable compute to find a good learned update rule. Spending similar amounts of compute tuning the baselines would be appreciated. Second, the meta-learned update rule presented here is essentially a learned optimizer and thus considerably more powerful than SGD. What optimizers did you use for LwF and EWC? Where the hyper parameters tuned here in an attempt to use similar compute? Where there learning rate schedules also tuned? \n\nQuestions / concerns:\n \nCost of running this not discussed. I would expect that both meta-training, and training are considerably more expensive. I am curious in particular \n\nOne motivation for meta-learning update rules in this way is that this cost can be amortized ahead of time and the learned update rule can transfer to new very different tasks. Without transfer like this, however, it's unclear if a method such as this is useful in general. Some discussion to this end I think would be helpful. I am not docking this work for not doing this type of generalization work though as we must start someplace and meta-training on similar data distributions is a logical place to do so.\n\nI am unclear as to your exact meta-training setup from algorithm 1. Does your meta-gradient (DL/dtheta) get computed every inner iteration (iteration of t)? If so how many steps do you back prop through? As of now it looks like your only backpropping a single iteration / application of f. Second, when computing this meta-gradient do you compute the true derivative or a first order approximation common in other work?\n \nOverall:\nI would recommend this paper for acceptance as it presents an interesting approach to solving the catastrophic forgetting issue with a compelling set of diverse experiments.  \n"}