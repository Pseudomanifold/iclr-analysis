{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the mean and variance of the gradient norm at each layer for vanilla feedforward, ResNet and DenseNet, respectively, at the initialization step, which is related with Hanin & Ronick 2018 studying the mean and variance of forward activations. They show that ResNet and DenseNet preserve the variance of the layer gradient norm through depths. In comparison, for the vanilla feedforward network, although the mean of the gradient norm is preserved if  is properly initialized, the variance of the layer gradient norm increases over depths, which may explode or decay the gradient at deeper layers. \n\nThe result presented in the paper is interesting, and the theory and empirical verification have a good match. I  recommend the acceptance after the following points are well addressed.\n\n1. The mean and variance of layer activation norm and gradient norm have been studied in the mean field literatures for example [1] and the paper does not have a good comparison with them.\n2. The experiment of  CONCATENATIVE RELU is not convincing given the small tasks intertwined performance.\n\nMinor presentation flaws:\n1. The sentence in Abstract \"This depth invariant result is surprising in light of the literature results that state that the norm of the layer\u2019s activations grows exponentially with the specific layer\u2019s depth.\" is not clear itself because of no relevant reference.\n2. There are many typos such as \"weather\", \"for of\" in the paper. Please carefully correct them.\n3. The values on the y-axis of Figure 2 and Figure3 are not correctly shown.\n\n[1] Greg Yang and Samuel Schoenholz 2017. Mean Field Residual Networks: On the Edge of Chaos"}