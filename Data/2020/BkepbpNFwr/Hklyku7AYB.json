{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an extensible attention mechanism applied on the previous hidden state of an RNN and resulting in supplementary input for the next RNN step. For each added domain, new pairs of attentions key and values can be added to provide more capacity for the model. This method is applied in the context of incremental domain adaptation for NLP without the possibility of storing of old samples (episodic memory).\n\nPros:\n- Extensive ablation study with the different possible combinations of methods\n- Very interesting comparison between expanding the memory (i.e. attention) and expanding the hidden states. Using the attention results in better results for a same number of added parameters and the activations sizes stay the same even when the attention is extended with new pairs.\n- Paper is well written/motivated\n\nWeaknesses:\n- MultiNLI seem to have too much correlation between tasks. It would have been better to be able to observe catastrophic forgetting for the source domain. In the appendix, the metrics have really strong disagreement so it is tough to judge for these two corpuses. \n- When you give the numbers for multi-task learning, you should use your extended memory method to be fair with MT learning. I would just be interested to see it, just as a proper upper bound.\n\nOtherwise, the paper proposes a novel method which works well in practice so I am leaning towards acceptance."}