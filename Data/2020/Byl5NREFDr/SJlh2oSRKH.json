{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This authors introduce a novel approach to successful modern extraction.  The paper is well written and easy to follow (the two exceptions/oddities are Figure 1 & Table 1, which appear one page before they are refered, which makes them initially hard to understand because they are out of context). The experimental evaluation is both well-thought and convincing.\n\nGiven the \"unreasonable effectiveness\" of the proposed approach, one is left to wonder whether/how it is possible to systematically close the performance gap between the extracted model and the victim one. Would well formed queries help? how about random/smartly chosen training examples from the training/tuning set of the victim model? or anything else?"}