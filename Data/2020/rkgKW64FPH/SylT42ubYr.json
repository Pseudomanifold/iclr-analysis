{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a constant-time approximation for graph convolution operation via theoretical analysis on the number of sampling from each neighbor. The authors prove that both node embedding and gradient can be approximated via constant number of samples among the neighbors. Extensive experiments are carried out to verify the correctness of the proposed bounds.\n\nStrength:\n1. The authors establish rigorous bounds on the number of required neighbors to sample to guarantee good approximation of the graph convolution operations. The bounds are established on both nodes embedding and gradients.\n2. The authors corroborate the theoretical results with extensive experiments. The experiments are carried under the case both when the assumption holds and not.\n\nWeakness:\n1. Though the bounds do not depend on number of nodes, it does have an exponential dependence on the number of layers. For common GCN with L=2, even \\epsilon = 0.1 requires more than 10^4 samples. As a result, the bound is less practical in providing guidance for real-world application. Moreover, the bound does not depend on the embedding dimension at all. Is this to be expected?\n2. The authors only provide constant bound for convolution on individual nodes. For the final metrics like classification accuracy, we need to guarantee good approximation for all node\u2019s convolution. In this case, the bounds will definitely depend on number of nodes at least through the union bounds among all nodes.\n3. The proposed method is exactly the uniform neighbor sampling methods. There is little contribution on the methodology side from the paper. Also, it would be interesting if the authors could provide theoretical analysis for adaptive sampling methods as well.\n4. The authors provide in-approximability results under certain conditionals. However, uniform sampling methods works well empirically under this case as well. It would be better if the authors could explain the gap between theory and empirical performance.\n\nDetailed Comments:\n1. Algorithm 2, please define notation like O(v<-v).\n"}