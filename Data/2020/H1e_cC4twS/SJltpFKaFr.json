{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "[Contribution summary]\nAuthors propose a new model for the DST task that (1) reduces the inference time complexity with an non-autoregressive decoder, and (2) obtains the competitive DST accuracy (49.04% joint accuracy on MultiWoZ 2.1).\n\n[Comments]\n- The proposed model is well motivated and well structured. Empirical results show improvement over other baselines, with the main gain coming from delexicalization, slot gating, fertility output, etc.\n\n- Some of the details are not entirely provided - e.g. please provide the loss hyper-parameter values (e.g. Eq.23) and optimizer parameters for the training.\n\n- Overall presentation, notations, figures, etc. could improve. \n\n- There have been recent work on DST with new SOTA results (e.g. \u201cTowards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset\u201d by Rastogi et al.) -- please consider comparing the approaches.\n"}