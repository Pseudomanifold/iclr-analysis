{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a new one-shot NAS approach. Parameter updating and structure updating are optimized separately. In the process of parameter optimization, different from the previous methods, the network samples the structure according to the uniform probability distribution. This sampling method can avoid the coupling caused by optimizing the structural parameters at the same time. After training the supernet, the network uses the genetic algorithm to search the structure.\n\nPros:\n+ This paper is well-written and easy to follow. \n+ The method proposed in this paper is very efficient in the search stage and saves memory. During the searching stage, the constraints of the model can be restricted by the genetic algorithm. \n+ This method could directly search architectures on the large scale dataset.\n+ The results are promising. Experiments are performed on various structures and including quantization layers. \n\nStill, I have several minor concerns regarding the algorithm and experiments.\n\n1. Why does the author think that the supernet needs to be trained by single path all the time? In the architecture searching space, under the same computational constraints, some models perform well, and some do not. Why do the good models and the worse models use the same probability to sample? Wouldn't it require much more time to optimize the supernet? In the experiment part, the author compares a related work FBNet, which uses the structure parameter \\theta and temperature parameter \\tau to control the sampling. When the temperature \\tau is high, the distribution of the samples degenerates into the single path sampler, and the accuracy is used to optimize the parameter \\theta, which makes \\theta tend to select the well-performed models. With the decrease of the temperature \\tau, the probability of sampling well-performed architectures is higher than the worse-performed models. Therefore, what is the advantage of the single path sampling method over \\tau controlled sampler? It seems that gradually pruning the worse-performed search space would accelerate searching time. More analysis is preferred rather than the superior results from the experiments.\n\n2. Is identity in the search space? For at the last of Page4, the author said 'our choice block does not have an identity branch', and the listed architecture on page 13 has the identity.\n\n3. Is the supernet training for 120 epochs necessary? Is the rank of network structures become steady when training for 120 epochs. Because for the subsequent EA-based network structure search, the loss scale is not important, but the sorting (ranking) is important. (This beyond the scope of this paper, but relates to the NAS area)\n\n4. What is the number of parameters for all the listed models in Tab. 3 & Tab. 4? Listing the parameters makes it easier for the followers.\n\n5. What kind of structure do the mixed-precision networks use? The original resnet module, or the bireal resnet module [r1]?\n\n6. While recalculating the statistics of all the BN,  is backpropagation required or just run the inference without gradient backpropagation.\n\n7. The author uses the approximate complete set of Imagenet to train supernet. If we directly inherit the parameter from the supernet, can we accelerate the training of searched good structure? Will Top-1 acc lower than training from scratch?\n\n[r1] Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm\n"}