{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a distributed recurrent auto-encoder for image compression that uses a ConvLSTM to learn binary codes that are constructed progressively from residuals of previously encoded information. Experiments show that the proposed model achieved better PSNR than other compression auto-encoders and algorithms. The paper claims distributed representation has advantage for image compression.\n\nThe idea in the paper is a good one, except for the part about \"distributed\". 2 main concerns here:\n\n1. The paper uses the word \"distributed\" to mean multi-source or clustered data inputs. This is different from what the deep learning community normally means, and the paper has the risk of misleading readers without clarification or using a different terminology. Why not multi-source / multi-modal?\n\n2. By analyzing figure 4 and figure 5 together, it seems that the PSNR results are similar between multi-source / clustered data and normal auto-encoder. The paper mentioned that the multi-source model may have advantage over information corruption and balance between encoder/decoder model sizes, but no experiments are shown to demonstrate that. This rendered the motivation of the whole \"distributed\" part unclear.\n\nSince the \"distributed\" seems to occupy a significant portion of the paper, I vote for rejection based on the current manuscript."}