{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a method to train image compression models on multiple sources, with a separate encoder on each source, and a shared decoder. The model is based on a recurrent image compression network. The main difference from a standard image compression network is that it has multiple copies of encoders and at training time, it optimizes on all sources jointly. The experiments are conducted on the CIFAR10, MNIST, and Kodak dataset. It shows that the method works slightly worse than jointly training all sources, and clearly outperforms training separate models.\n\nThe paper is clearly written and easy to follow. The proposed method is simple. \n\nMy main concerns are regarding the experiment results: \n1. The experiments are conducted on a not very realistic setting of splitting CIFAR10 and MNIST by class labels. First of all these datasets are small, simple, and low-resolution. Second, by splitting by the labels, the sources are still highly correlated as they are collected from the same distribution. Jointly training multiple larger datasets that show more diverse properties would make the results more convincing. \n2. I don't find the motivations of training multiple encoders clear. Given the fact that jointly training on a larger dataset typically gives a better performance, it's still not clear when we would want to train separate encoders. An extended discussion would help.\n3. I found the results not very surprising. The observations that a jointly trained model sees more examples, so performs \nbetter seems expected  (particularly when the dataset is small). \n4. The technical contributions are light-weight. Using multiple copies of parameters for different data distributions is used e.g., in multi-task learning and domain adaptation literatures. \n\nOverall I think this paper would benefit from improving the experiments and providing better supporting evidence for demonstrating practical value of the system."}