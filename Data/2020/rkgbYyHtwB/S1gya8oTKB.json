{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an imitation learning algorithm that combines behavioral cloning with a regularizer that encourages the agent to visit states similar to the demonstrated states. The key idea is to use ensemble disagreement to approximate uncertainty, and use RL to train the imitation agent to visit states in which an ensemble of cloned imitation policies is least uncertain about which action the expert would take. Experiments on image-based Atari games show that the proposed method significantly outperforms BC and GAIL baselines in three games, and performs comparably or slightly better than the baselines in the remaining three games.\n\nOverall, I enjoyed reading this paper. It proposes a relatively simple imitation method with compelling empirical results.\n\nOne minor comment: on page 15, the sentence \"We initially performed a hyperparameter search on Breakout with 10 demonstrations over the following values: \" ends in a blank space, without actually providing any hyperparameter values. It would be nice if you could actually include those values, or at least how many different values were searched.\n\nThank you for addressing the comments about related work in an earlier thread (https://openreview.net/forum?id=rkgbYyHtwB&noteId=S1lv4r5qvS). Two follow-ups:\n - The chain MDP example clearly illustrates why including the BC cost is important, and how DRIL differs from support estimation methods like RED. Thank you for the clarification.\n - The focus of Sasaki et al. is on reducing the number of environment interactions, but their proposed method also addresses covariate shift: it fits a Q function that classifies whether the demonstration states are reachable from the current state, and thus encourages the agent to return to demonstrated states."}