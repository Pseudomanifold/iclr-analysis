{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work the authors studied the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance. Inspired by the analysis in the distribution-ally robust setting, they derived several sensitivity conditions to study how the radius of the wasserstein ball and the order of the wasserstein distance affect the conservativeness in model uncertainty.  Similar to the robust MDP work, they also show that this robust RL problem has a robust optimal Bellman operator, and the optimal value function can be computed using  robust policy iteration, which can be extended to model-free actor critic algorithm when state/action spaces are large/continuous.\n\nThis work extends the wasserstein uncertainty modeling  to the robust MDP setting. However, I found the contribution rather limited/unclear. First, the proposed robust Bellman optimality result is standard and can be found in many robust MDP work when the uncertainty set is state-action rectangular. The major novel part here is therefore the sensitivity analysis that is specifically tied to wasserstein distance, which has limited novelty. Second, the robust actor critic algorithm is based on a multi-time scale minimax gradient approach, which is also quite standard in this field, and besides asymptotic convergence it is unclear how efficient this algorithm is. Third, the experiment in this paper is very limited (only based on cartpole, and is only compared with the non-robust AC algorithm) and its illustration on the effectiveness of this algorithm is rather limited. More comparisons with other robust MDP methods would be useful to understand how the value of the proposed wasserstein robust RL formulation."}