{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nIn this paper, the authors study the problem of robust MDP (RMDP), where the feasibility set is defined as a Wasserstein ball around the reference transition probability. After formulating the problem, they first prove the contraction of the resulting operator and the existence of a deterministic stationary Markov optimal policy (Sec. 2.3). Then, they provide a sensitivity analysis of the optimal value function w.r.t. the radius and order of the Wasserstein ball (Sec. 2.4). Finally, they propose an actor-critic algorithm to solve the Wasserstein RMDP problem (Sec. 3) and evaluate its performance using simple experiments (Sec. 4). \n\nComments:\n- Section 2.3, which the authors call it Main Result, is not that novel. The Wasserstein RMDP is just a state-action-rectangular RMDP with convex ambiguity set, and Lemma 1 and Theorem 1 are known to be true for such RMDPs. \n- The sensitivity analysis section (Sec. 2.4) is interesting and useful, although it could have been written much better. \n- The algorithm is new, although its section (Sec. 3) is not well-written. There is no analysis for the convergence of the algorithm either. It is a multi-time-scale stochastic approximation algorithm. Similar policy gradient and actor-critic algorithms have been derived in risk-sensitive MDPs for mean-variance, mean-VaR, and mean-CVaR optimization (see the references below), but such algorithms for RMDPs is relatively new. \n- The experimental results are very simple and not very convincing. \n- I would suggest that the authors put their emphasis on the algorithm and try to explain it much better. And support it better with more comprehensive and convincing experimental results. This would definitely improve the quality of the paper.\n\n\nReferences on risk-sensitive MDPs: \n1) A. Tamar, D. Di Castro, and S. Mannor. \u201cPolicy Gradients with Variance Related Risk Criteria\u201d. ICML-2012.\n2) Prashanth L.A. and M. Ghavamzadeh. \u201cActor-Critic Algorithms for Risk-Sensitive MDPs\u201d. NIPS-2013.\n3) Y. Chow and M. Ghavamzadeh. \u201cAlgorithms for CVaR Optimization in MDPs\u201d. NIPS-2014.\n4) A. Tamar, Y. Glassner, and S. Mannor. \u201cOptimizing the CVaR via Sampling\u201d. AAAI-2015.\n5) A. Tamar, Y. Chow, M. Ghavamzadeh, and S. Mannor. \u201cPolicy Gradient for Coherent Risk Measures\u201d. NIPS-2015.\n"}