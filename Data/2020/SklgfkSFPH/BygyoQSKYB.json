{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors  replace the empirical risk term in a PAC-Bayes bound by its second-order Taylor series approximation, obtaining an approximate (?)  PAC-Bayes bound that depends on the Hessian. Note that the bound is likely overoptimistic unless the minimum is quadratic. They purpose to study SGD by centering the posterior at the weights learned by SGD. The posterior variance that minimizes this approximate PAC-Bayes bound can then be found analytically. They also solve for the optimal prior variance (assuming diagonal Gaussian priors/posteriors), producing a hypothetical \"best possible bound\" (at least under the particular choices of priors/posteriors, and under this approximation of the empirical risk term). The authors evaluate their approximate bound and \"best bound possible\" empirically on MNIST and CIFAR. This requires  computing approximations of the Hessian for small fully connected neural networks trained on MNIST and CIFAR10. There are some nice visualizations (indeed, these may be one of the most interesting contributions.)\n\nThe direction taken by the authors is potentially interesting. However, there are a few issues that would have to be addressed carefully for me to recommend acceptance. First, the comparison to (some very) related work is insufficient, and so the actual novelty is misrepresented (see detailed comments below). Further, the paper is full of questionable vague claims and miscites/attributes other work. At the moment, I think the paper is below the acceptance threshold: the authors need to read and understand (!) related work, and expand their theoretical and/or empirical results to produce a contribution of sufficient novelty/impact. \n\nDETAILED FEEDBACK.\n\nI believe the authors missed some related work by Tsuzuki, Sato and Sugiyama (2019), where a PAC-Bayes bound was derived in terms of the Hessian, via a second-order approximation. How are the results presented in this submission relate to Tsuzuki et al approach? \n\nWhen the posterior, Q, is a Gaussian (or any other symmetric distribution), \\eta^T H \\eta is the so-called Skilling-Hutchinson trace estimator. Thus E(\\eta^T H \\eta) is the Trace(H) scaled by the variance of \\eta. The authors seem to have completely missed this connection, which simplifies the final expression considerably.\n\nWhy is the assumption that the higher order terms are negligible reasonable? Citation or experiments required.\n\nRegarding the off-diagonal Hessian approximation: how does the proposed layer-wise approximation relate to k-FAC (Martens and Grosse 2015)?\n\nIB Lagrangian: I am not sure why the authors state the result in Thm 4.2 as a lower bound on the IB Lagrangian. What\u2019s the significance of having a lower bound on IB Lagrangian?\n\nOther comments: \n\nIntroduction: \u201cAt the same time neither the non-convex optimization problem solved in .. nor the compression schemes employed in \u2026 are guaranteed to converge to a global minimum.\u201d. This is true but it is really not clear what the point being made is. Essentially, so what? Note that PAC-Bayes bounds hold for all posteriors, even ones not centered at the global minimum (of any objective). The claims made in the rest of the paragraph are also questionable and their purposes are equally unclear. I would be grateful if the authors could clarify.\n\nFirst sentence of Section 3.1: \u201cAs the analytical solution for the KL term in 1 obviously underestimates the noise robustness of the deep neural network around the minimum...\u201d. I have no idea what is being claimed here. The statement needs to be made much less vague.  Please explain.\n\nSection 4: \u201c..while we will be minimizing an upper bound on our objective we will be referring with a slight abuse of terminology to our results as a lower bound.\u201d. I would appreciate if the authors could clarify what they mean here.\n\nSection 4.1 beginning: \u201cWe make the following model  assumptions...\u201d. Choosing a Gaussian prior and posterior is not an assumption. It's simply a choice. The PAC-Bayes bound is valid for any choices of Gibbs classifiers. On the other hand, it is an assumption that such distributions will yield \"tight\" bounds, related to the work of Alquier et al.\n\nSection 4.1 \u201cIn practice we perform a grid search over the parameters..\u201d. The authors should mention that such a search should be accounted for via a union bound (or otherwise). The \"cost\" of such a union bound should be discussed.\n\nThe empirical risk of Q is computed using 5 MCMC samples. This seems like a very low number, as it would not even give you one decimal point of accuracy with reasonable confidence! The authors should either use more samples, or account for the error in the upper bound using a confidence interval derived from a Chernoff bound.\n\nSection 4.2: \u201cThe concept of a valid prior has been formalized under the differential privacy setting...\u201d. I am not sure what the authors mean by that.\n\nSection 5: \u201cThere is ambiguity about the size of the Hessians that can be computed exactly.\u201d What kind of ambiguity?\n\nSame paragraph in Section 5 discusses why there are few articles on Hessian computation. The authors claim that \u201cthe main problem seems to be that the relevant computations are not well supported...\u201d. This is followed by another comment that is supposed to contrast the previous claim, saying that storing the Hessian is infeasible due to memory requirements. I am not sure how this claim about memory requirements shows a contrast with the claim on computation not being supported.\n\nFirst sentence in Section 5.1: I believe this is only true under some conditions. \n\nSection 5.1: The authors should explain why they add a damping term, alpha, to the Hessian, and how alpha affects the results.\n\n***\nAdditional citation issues:\n\nThe connections between variational inference, PAC-Bayes and IB Lagrangian have been pointed out in previous work (e.g. Germain, Bach, Lacoste, Lacoste-Julien (2016); Achille and Soatto 2017).\n\nIn the introduction, the authors say \u201c...have been motivated simply by empirical correlations with generalization error; an argument which has been criticized \u2026\u201d (followed by a few citations). Note, that this was first criticized in Dziugaite and Roy (2017). \n\n\u201cBoth objectives in \u2026 are however difficult to optimize for anything but small scale experiments.\u201d. It seems peculiar to highlight this, since the approach that the authors are presenting is actually more computationally demanding. \n\nCitations for MNIST and CIFAR10 are missing.\n\n***\nMinor:\nTheorem 3.1 \u201cFor any data distribution over..\u201d, I think it was meant to be \\mathcal{X} \\times  (and not \\in )\nTheorem 4.2: \u201cFor our choice of Gaussian prior and posterior, the following is a lower bound on the IB-Lagrangian under any Gaussian prior covariance\u201d. I assume only the mean of the Gaussian prior is fixed.\n\n\nCitations are misplaced (breaking the sentences, unclear when the paper of the authors are cited).\nThere are many (!) missing commas, which makes some sentences hard to follow.\n\n***\nPositive feedback: I thought the visualizations in Figure 2 and 3 were quite nice.\n"}