{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a method for subselecting training data \nin order to speed up incremental gradient (IG) methods (in terms of computation time). \nThe idea is to train a model on a representative subset of the data such \nthat the computation time is significantly decreased without \nsignificantly degrading performance. Given a dataset D and \nsubset S selected by the proposed method, it is shown that \ntraining on subset S achieves nearly the same loss as training on the full \ndataset D would, while achieving significant computational speedups.\n\nThis paper is a clear accept. The approach is novel and has well-developed \ntheory supporting it. The empirical evaluation of the method shows \nlarge speedups in training time without degradation in performance \nfor reasonably large subsets (e.g. 20% of the data). The paper is \nvery clear, well-written, and was a genuinely fun read.\n\nClarifying questions:\n  - In results reporting speedups, does the reported training time for CRAIG \n  include the preprocessing time? Or only the time spent running IG on the resulting \n  subset?\n  - How many runs are the experiments averaged over? There don't seem to be \n  error bars, which makes it difficult to assess whether the speedups are \n  statistically significant\n  - I imagine that an approach like this would be desirable when working with  very large datasets. Has \n  CRAIG been evaluated in settings with millions of datapoints? Or does it become impractical? I think \n  that the paper stands on its own without such a demonstration, but it would go a long way towards \n  encouraging mainstream adoption of your method.\n  - Figure 3, left: What could be happening at around 40s? It looks like \n  all three of the random permutations have a spike in loss at around the same time, despite being \n  different permutations of the sub-selected data\n  - How were hyperparameters, such as the regularization parameter, step-size etc. chosen? One of the \n  main claims of the paper is that using the subset selected by CRAIG doesn't significantly \n  effect the optimization performance. But if the baselines weren't thoroughly tuned, it could be the case \n  that IG on the CRAIG subset performs similarly to IG on the full training data, but that neither \n  is actually reaching satisfactory performance in a given domain.\n  - Figure 4: isn't 2000s \\approx 30min really slow for MNIST? From what I remember, reasonable test accuracy \n  on MNIST with a feed-forward network with a single layer takes only a few minutes? Though admittedly, I could \n  be misremembering this.\n  \nSomewhat open-ended questions:\n  - To what extent are the results hardware dependent? Do you see similar results on \n  different hardware? I'm wondering how much of the speedup could be attributed to \n  something like better memory locality when using the smaller subset selected using CRAIG.\n  - Section 3.4 mentions that the O(|V||S|) complexity can be reduced \n  to O(|V|) using a stochastic greedy algorithm. Has the performance \n  when training on a subset selected via the stochastic algorithm \n  been compared to the performance when training on a subset selected by the \n  deterministic version? \n\nI have only minor suggestions:\n  - The CRAIG Algorithm \n    - When F is introduced, I had trouble conceptualizing what kind of object it was. I think \n      mentioning what spaces it's mapping between could increase readability.\n    - Mirzasoluiman et al. (2015a) looks like it is supposed to be a parenthetical citation\n  - Figure 4: The caption labels appear to be swapped. In the figure, (a) is MNIST, but in the \n    caption, (b) is MNIST\n  - 5.1: There is a vertical space issue between the introduction of section 5 and section 5.1. \n  I suspect this was necessary to make the max page requirements. If space is an issue, my suggestion would \n  be to move Algorithm 1 to the appendix. It's nice to have a concrete algorithm specification, but I personally \n  did not find that it aided my understanding of your paper."}