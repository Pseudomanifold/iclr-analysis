{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a hierarchical policy structure for use in both single task and multitask reinforcement learning. The authors then assess the usefulness of such a structure in both settings on complex robotic tasks. These tasks include the stacking and reaching of blocks using a robotic hand, as an example. In addition to carrying out these experiments on simulated robots, the authors have also carried out experiments on a real Sawyer robotic arm. \n\nThe particular form of their hierarchical policy for the multitask case is as follows. The policy, which is conditioned on the current state and task index consists of a gaussian mixture, where the individual gaussian densities are conditioned on the state and a context variable. The weights of this mixture are then dependent on a density on this context variable, which is conditioned on the state and task index. The intuition behind this is that the weight portion, which is called the high level component identifies task specific information, while the low level policy learns general, shareable knowledge of the different problems. \n\nThe authors adapt the Multitask Policy Optimisation algorithm for their use by introducing an intermediate non-parametric policy, which is derived by setting KL bounds on the policy w.r.t to a reference policy. Having derived a closed-form solution to this, they go on to learn the parametric policy of interest. \n\nThe authors consider 3 settings of experiments. Firstly, they assess the benefits of the hierarchical structure for single task settings in a simulated environment. For the most part, they find that compared to a flat policy, the hierarchical structure shows benefits only if the initial means of the high-level components are sampled to be different. While the experimental results are shown to support this, further discussion of why this is the case would have been welcome. \n\nThe main benefits of the hierarchical policy are shown in the multitask case, in both simulated and real situations. In fact, the authors have shown that the hierarchical case often shows major benefits in difficult, more complicated tasks (reach vs stacking for example).\n\nI think that the paper was very well written. It is nicely structured, with easy to read language, and without unnecessary jargon or clutter. Where necessary, the relevant extra details were provided in the Appendices.\n\nThe following are some additional notes:\n1) It would have been interesting to see how the hierarchal policy faired in new tasks that were not a part of the original training set, compared to a flat multitask policy.  \n2) Further details about how each task is differentiated from each other in the experiments. That is, what are their different goals, which are reflected by the reward functions. \n\nAs such I recommend this paper to be weak accepted."}