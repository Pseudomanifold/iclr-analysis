{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a novel, information theoretic approach to learning compressed neural networks, whereby model parameters $\\theta$ are expressed in an (approximately discrete) latent space as $\\theta = f_{\\psi}(\\phi)$. During training, \"reparameterizations\" $\\Phi$ are treated as draws from a generative distribution $q(\\phi)$. As per the source coding theorem, penalizing the entropy of $q$ (more accurately, of $\\hat{q}$) directly promotes compressible choices of $\\Phi$ and, therefore, of $\\Theta$. After training, arithmetic coding of $q$ is used to compress $\\Phi$. Empirical results on a suite of common benchmarks demonstrates the proposed algorithm gracefully trades off between model performance and size.\n\n\nFrequency Domain Interpretation:\n  Here, I am operating off of the assumptions that, in Sect 3.2 paragraph 3, $\\phi_{w}$ should be $\\psi_{w}$. If so, can \"Our Method (DFT)\" also be interpreted as frequency domain SQ (of convolutional filters)? When trained on natural images, CNN filters are typically \"smooth\" (see any visualization thereof) and this smoothness translates as a prior of sorts on $\\Phi_{w}$. This insight was previously explored in [1, 2] for purposes of compressing CNNs. Does evidence of this manifest in your experiments? For example, do the empirical distributions of high- and low-frequency components of $\\phi_{w}$ differ?\n\n\nFeedback:\n  The authors propose a general framework where in (seemingly) arbitrary decoders $f$ can be used for purposes of \"reparameterization\". It is therefore somewhat disappointing that, in practice, $f$ is restricted to affine transformations and, in some cases, even fixed. Memory issues notwithstanding, it is unclear how well complex decoders $f$ could be jointly learned with (surrogate) encodings $\\tilde{\\Phi}$ and density models $\\tilde{q}$.\n\n  The lack of comparison between the proposed method's test-time throughput and that of its baselines leaves open questions regarding potential real-world use cases. Much of the time, methods for compressing neural networks divide their attention between both memory footprint and prediction speed. Outright ignoring this aspect of the problem seems odd. As a reviewer, I would much rather see the authors take a proactive approach in addressing this issue.\n\n\nQuestions:\n  - What do results look like if convolutional decoders $f_{\\text{conv}}$ are jointly learned during training?\n  - How sensitive is the algorithm to different groups assignments? Can assignments be reliably made by simply looking at the architecture?\n\n\nMinor Comments:\n  - The general notation of the paper is cumbersome at times, can $\\phi$ to be changed to, e.g., $\\tilde{\\theta}$? Similarly, can notation such as $\\theta_{k, W}$ be simplified as $W^{k}$?\n\n  - Section 2 jumps around a bit. Consider trying to order things \"chronologically\" such that training comes before compression? By the same token, details regarding \"Model Partitioning\" could be move to the end of the section such that the preceding material just refers to $\\theta$ more abstractly?\n\n  - Some additional details regarding training (esp. discretization and model distributions $\\tilde{q}$) would be appreciated.\n\n\n[1] \"Compressing convolutional neural networks in the frequency domain\", Chen et al. 2016\n[2] \"CNNpack: Packing convolutional neural networks in the frequency domain\", Wang et al. 2016"}