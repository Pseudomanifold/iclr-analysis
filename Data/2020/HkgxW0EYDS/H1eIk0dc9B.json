{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper introduces an end-to-end method for the neural network compression, which is based on compressing reparametrized forms of model parameters (weights and biases). Shannon-style entropy coder is applied to the reparametrizations.\n\nReparameterized forms are tensors stored in the compressed format. During inference, they are uncompressed into integer tensors and transformed via parameter decoders into weights/biases of convolutional and dense layers.\n\nDuring training, model parameters are manually partitioned into groups, and within a group they are considered as samples from the same learned distribution. Similarly, parameter sharing is introduced among the corresponding parameter decoders.\nLoss function, which is minimized during training is a sum of rate loss (self-information of all reparametrizations) and cross-entropy classification loss under reparametrization. A trade-off between compressed model size and model accuracy can be explored by varying a constant before the rate loss.\n\nDuring optimization, several tricks are applied. \u201cStraight-through\u201d gradient estimator is used to optimize the loss function over discrete-valued reparametrizations by means of stochastic gradient descent. Relaxation is used to obtain good estimates for both the rate term and its gradient during training.\n\nThe proposed idea is well-founded and intuitive. The proposed method is extensively evaluated on different classification network architectures and datasets. It provides good compression while retaining a significant portion of accuracy.\n\nIn the experiments, It'd be interesting to see a comparison on more efficient networks like MobileNets, ShuffleNets on ImageNet dataset. Also, I wonder whether under the same compression rate the proposed method outperforms DeepCompression (Han et al., 2015) in terms of accuracy? (for example, for LeNets and VGG-16)\n"}