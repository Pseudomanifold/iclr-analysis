{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a compression algorithm for neural networks. It uses a linear projection to map the weights and biases to a latent space where they are quantized using a learnt coding distribution. The method is simple yet it shows strong performance of a variety of compression benchmarks.\n\nOriginality: The core idea is related to (Balle et al. 2018), but the paper puts its own twist to it with the projections and applies it to model compression. It is certainly an interesting direction to explore.\n\nPresentation: The paper is well written. It is easy to read and understand. It properly cites prior works and contains all the technical details. I appreciate that the authors fit the paper into the recommended 8 pages.\n\nImpact: The method shows very strong performance in terms of compression ratio, but its unclear whether the compressed model can be used to speed up inference. Currently the main use case would be saving storage/bandwidth.\n\nQuestions:\n- In section 2.2, the paper talks about the form of the coding distribution. It has d components and l dimensions. How is d determined?\n- Section 2.1, How are the weights grouped for coding? is every filter its own group? If the experiments use different groups for different models, how is it decided which model uses which approach?\n- \\phi was fixed for the CIFAR10/ImageNet experiments. Could you provide further insight into why this choice was made?\n\nAssessment:\nWhile the idea is not groundbreaking, it is very well presented and evaluated and shows strong performance."}