{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work studies how over-parametrization smoothens the loss landscape in neural networks. The work also shows interesting negative results, notably that over-parametrized networks can have bad local minima. It clarified the notion of \"bad local minima\" and distinguishes between weakly global functions and non-weakly-global functions. These negative results are nice and interesting as they go again the widely accepted claim that over-parametrization makes the landscape always \"nicer\".\n\nA major drawback of this work is that it completely ignores the generalization properties of the corresponding neural network. It is not at all surprising that over-parametrization makes optimization simpler. The real point to explain about over-parametrization is why at the same time it does not hurt generalization. The paper admits that it does not consider the generalization properties. But as such it is not relevant to understanding learning properties of deep neural networks.\n\nThe absence of 'bad' local minima seems particularly irrelevant in the view of recent line of work on overparametrized neural networks, shoving that best generalization properties are achieved in the so-called interpolating regime where there exist many global minima of the loss landscape, all with zero training error, but only some of them leading to good generalization. The real question is to explain why the algorithm dynamics find those that generalize well and not the other one. The present work does not shed any light on this question. \n\nSo while I do appreciate some of the negative results I think overall this work does not constitute an important contribution to the current questions and I hence lean towards rejection. \n\n\nMinor comments: \n\nI am not really appreciating the analogy between illnesses of the landscape and the human body. This is maybe useful in a wide audience newspaper article, but I do not see its use here where readers are well aware of the underlying questions and concepts. Talking of \u201cpharmacodynamics\" and related does not really seem of interest. \n\nThe article is full of misprints: last paragraph on page 1: has -> have, inima - minima \n\nI am also noting that the paper has 10 pages and hence according to the paper call higher standards should be applied in the review process. \n"}