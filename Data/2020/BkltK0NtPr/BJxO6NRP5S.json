{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper discusses the effects of over-parameterization in deep neural networks. The main claim is that for any continuous activation function the loss function does not have so-called \u201csetwise strict\u201c local minima, i.e., one can find a path to the global minimum without increasing the energy along the path. The main assumption is that if the number of datapoints is N, the activation function has derivatives up to N^th order and there exists at least one layer in the network with at least N neurons. The paper seems to build upon the theory of weakly global functions which are functions for which all setwise strict local minima are setwise global minima.\n\nThis paper should be rejected for the following reasons:\n\n1. The main claim of the paper is not true. It contradicts previously published results, e.g., https://arxiv.org/abs/1611.01540\n2. The development in this paper is not at all rigorous. I do not see a proof of the claim. The main narrative consists of a host of special cases and elementary examples. The appendix is complicated with more special cases, e.g., Appendix D, proposition 3 is for a 1-dimensional input and one hidden layer. I do not follow the proofs for the claims in Appendix E.\n\nI would encourage the authors to make their main narrative self-contained."}