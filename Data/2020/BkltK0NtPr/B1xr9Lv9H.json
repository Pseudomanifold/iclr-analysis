{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper theoretically studies the benefit of overparameterization for the loss landscape in fully connected deep neural networks. Here the definition of overparameterization is the number of training samples larger than the number of neurons in a hidden layer. This paper shows that if the last hidden layer is overparameterized (with some more mild assumptions) than loss function is weakly global. This implies there are no bad strict local minima. Authors also show by example that overparameterization networks could have bad non-strict local minima could exist.\n\nDue to last minute request, I have not been able to digest the proof so I won't be able to say anything regarding the correctness of the paper. \n\nWhile there are interesting observations one could find on overparamerized network's loss landscape,  with current form the paper's presentation is unclear and I wan not convinced on its relevance to realistic settings.  \n\nPros:\nThe paper is tackling an important question regarding the benefits of overparameterization. As authors note isolating benefits coming from overparameterization is important study.\n\nAssumptions used to prove the theorems are general enough except for the overparameterized assumption.\n\nCons:\nThere are few problems I encountered reviewing.\n\nI found paper unclear to read and understand. Maybe due to last minute submission, the paper does not appear to be polished and needs more work making the presentation clear.  Beyond various typos, broken reference (unnecessary Yu et al. infront of Yu & Chen (1995)), I did not find section 2 from 1-dimensional case especially helpful for understanding impact of the paper. Also since various previous works have similar claims to this paper, it would be clear to distinguish how current results are distinguished in the contribution section. For example as authors say Nguyen et al (2018)/ Venturi et al (2018) also show that non-existence of bad strict local minima. The contribution section may become clear if authors could describe specific contributions beyond what already have been described in literature. Is the relaxation to continuous activation function that is significant for example?\n\nI have concerns about the applicability of the results. For example, in realistic data would A3 or B1 hold? For example with 50k MNIST, one would need 50k hidden layer and the benefit of overparameterization in practice appears for much smaller networks.  Also for Theorem 2 assumption B2 is quite strict which doesn\u2019t include ReLU/TanH/Sigmoid activation functions. \n\nFor applicability, I wonder if there would be a practical guidance based on findings in the paper which would make the results more impactful. \n\nWhile it is important to understand what type of minima overparameterization network brings in, wouldn\u2019t more relevant true phenomenology of deep networks would be captured through statistical arguments in terms of how likely our initialization and optimization algorithm would fall into a certain type of minima. I was not convinced some of the particular examples in low dimensional settings reflects what realistic over/under parameterized neural networks are showing. \n\nQuestions and Comments:\nHow is counter example in section 6.1 an example of overparameterized case? I only see one hidden unit with 1 data point which is not overparameterized by paper\u2019s definition. \n\nPlease remove unnecessary \u201cYu et al.\u201d\n\nTypo p1 last paragraph \u201chas -> have\u201d"}