{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The paper proposes to use a Bayesian nonparametric mixture model for task-free (without explicit task labels) continual learning. The main idea is to use an expansion-based model where the number of mixture components (experts) adapts to the training data/tasks. Specifically, a Dirichlet Process Mixture Model (DPMM) consisting of a set of neural network experts is used. Empirical results demonstrate improved performance on three different datasets over some of the baselines. \n\nI find the methodological contribution in the paper to be somewhat limited since the main idea of the model was initially proposed in the prior work (cited in the main paper): Dahual Lin - \u201cOnline Learning of Nonparametric Mixture Models via SVA\u201d. In fact, the paper claims its contribution is expansion-based task-free continual learning. However, this \u201ctask-free characteristic\u201d is the contribution of SVA based inference. Nevertheless, I do like how the existing SVA based inference has been adapted from an online learning setting to a more general continual learning setting by using various approximations/tricks (like short-term memory with wake-sleep training, point estimates). \n\nPros:\n- The idea of using a nonparametric model for CL is interesting and can lead to follow-up work.\n- Results show that the approach works well.\n- The code has been released.\n\nOverall I am inclining towards voting for acceptance if the authors could address my following questions:\n\n- Could you comment on the creation of test data? It is not clear to me how the model is evaluated if the task-boundaries are not known a priori. Shouldn\u2019t the evaluation be based on tasks? How are you evaluating catastrophic forgetting? I am interested to know what was the test accuracy for the task for which the training data was seen early on during the training.\n\n- It seems to me that the method works on the assumption that the number of data points for each task is at least M (size of STM) and moreover, that these data points appear together sequentially. The method should be sensitive to the size of the STM. How are you choosing M? Would the framework work if data points for each task do not appear together?\n\n- Assuming you have clear task boundaries, how would you adapt this framework? Was the model compared to other methods that assume known task-boundaries like (VCL, EWC, Memory Replay)? \n\nOther comments:\n\n- The method is inspired by a Bayesian framework but calling it Bayesian wouldn\u2019t be fair since only a point estimate is being learned for parameters. This is important to distinguish since there are other methods that are fully Bayesian like Nguyen et. al. \u201cVariational Continual Learning\u201d  (although such methods may have other pros and cons)\n\n- The samples from the base distribution of the posterior (v) are not iid anymore due to lateral connections b/w the representations. Do you think the theoretical result in Appendix B that the number of clusters is upper bounded by O(alpha*logN) is still valid?\n"}