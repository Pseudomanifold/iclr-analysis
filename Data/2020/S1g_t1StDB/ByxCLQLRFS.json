{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the specific question/problem tackled by the paper?\nThis paper tackles the problem of learning language-conditioned policies from reinforcement learning. Unlike most language-conditioned navigation work which relies on human demonstrations (e.g. in the room2room environment), this work only learns from the agent\u2019s experience using a generalization of hindsight experience replay.\n\nMethod overview:\nTHER (textual HER) generalizes HER to cases where goals are not in the same space as states. To deal with this gap, THER learns a mapping from state space to goal space using successful trajectories. This mapping is then used to relabel unsuccessful trajectories with a guess of what goal was reached. This intuitive approach allows the text-conditioned agent to reach  40% at a 2D navigation task when conditioned on text such as \u201cPick the large red circle\u201d. \n\nStrengths:\nThe method is well motivated and would be useful. The ablations of showing how many successful trajectories are needed to learn the mapping (~1000-5000), how many time steps are needed to reach 1000 successes (~400k steps), and how accurate the mapping needs to be for HER to work (~80%) and thorough and easy to understand. This experimental completeness is itself a contribution. \n\nAdditionally, although the authors do not discuss this, this method is actually agnostic to the particular modality (e.g. text) of the goal space and could be used anytime the goal space differs from the state space. \n\nWeaknesses:\nThe primary weakness of the paper is that the testbed environment and the textual goals are very simple. The \u201clanguage\u201d is just a list of up to 4 attributes describing the different objects and the control is simple navigation without any walls of visual variation. Additionally, the method requires accidentally getting successful trajectories early in training in order to train the mapping, and in this environment it is very easy to get successful trajectories. \nI would interested in seeing how this method would work in the room2room environment (or some other more complex task). While it is unlikely to outperform the prior methods that use the human demonstrations, it would be useful to see how close THER can get to that performance and with how many environment steps. The advantage of this environment is that it has real human knowledge, and the textual goals are limited in number, making the experiment much more realistic (as humans are unlikely to sit next to an agent and generate infinitely many diverse textual goals). \n\nA missing ablation in Figure 4 left is THER without waiting 400k steps before relabeling. In realistic scenarios, we would not be able to evaluate the mapper ahead of time to know when to start relabeling. How is performance affected is this knowledge is not available?\n\nOverall, I lean to reject the paper in it\u2019s current form, I believe this paper would be more impactful with experiments involving more language complexity or more policy complexity."}