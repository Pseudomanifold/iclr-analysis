{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the training of overparametrized neural networks by gradient descent. More precisely, the authors consider the neural tangent regime (NTK regime). That is, the weights are chosen sufficiently large and the neural network is sufficiently overparametrized. It has been observed that in this scenario, the neural network behaves approximately like a linear function of its weights.\n\nIn this regime, the authors show that, the directions corresponding to larger eigenvalues of the neural tangent kernel are learned first. As this corresponds to learning lower-degree polynomials first, the authors claim that this explains the \"spectral bias\" observed in previous papers.\n\n-I think that from a mathematical point of view, the main result of this paper is what one would expect intuitively: \nWhen performing gradient descent with quadratic loss where the function to be learnt is linear, it is common knowledge that convergence is faster on directions corresponding to larger singular values. Since in the NTK regime, the neural network can be approximated by a linear function around the initialization one expects the behavior predicted by the main results. From a theoretical perspective, I see the main contribution of the paper as making this statement precise.\n\n-I am skeptical about some of the implications for practitioners, which are given by the authors: \nFor example, on p.5 the authors write \"Therefore, Theorem 3.2 theoretically explains the empirical observations given in Rahaman et al. (2018), and demonstrates that the difficulty of a function to be learned by neural network should be studied in the eigenspace of neural tangent kernel.\" To the best of my knowledge, it is unclear whether practitioners train neural networks in the NTK regime (see, e.g., [1]). Moreover, I am wondering whether some of the assumptions of their theorem are really met in practice. For example, the required sample size for higher order polynomials grows exponentially fast with the order and the required step size goes to zero exponentially fast. Does this really correspond to what is observed in practice? (Or is this a mere artifact of training in the NTK regime?)  Is this what one observes in the experiments by Ramahan?\n\nI think the paper is not yet ready for being published.\n 1. There are many typos. Here is an (very incomplete) list.\n    -p. 2: \"Su and Yang (2019)\" improves the convergence...\"\n    -p. 2: \"This theorem gives finer-grained control on error term's\"\n    -p. 2: \"We present a more general results\"\n    -p. 4: \"The variance follows the principal...\"\n    -p. 4: \"...we will present Mercer decomposition in (the) next section.\"\n2. I think that the presentation can be polished and many statements are somewhat unclear. For example, on p. 7 the authors write \"the convergence rates [...] are exactly predicted by our theory in a qualitative sense.\"\n    The meaning of this sentence is unclear to me. Does that mean in a quantitative sense? To be honest, only considering Fig. 1 I am not able to assess whether the convergence rates of the different components are truly linear. \n\nI decided for my rating of the paper because of the following reasons:\n-I think that for a theory paper the results obtained by the authors are not enough, as they are rather direct consequences of the \"near-linearity\" of the neural network around the initialization.\n-In my view, there is a huge gap between current theoretical results for deep learning and practice. For this reason, it is not problematic for me that it is unclear, what the results in this paper mean for practitioners. (Apart from that, results for the NTK regime are interesting in its own right.) However, in my view, one should explain the limitations of the theory more carefully.\n-The presentation of the paper needs to be improved.\n\nReferences:\n[1] A note on lazy training in supervised differentiable programming. L Chizat, F Bach - arXiv preprint arXiv:1812.07956, 2018 \n"}