{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis paper explores using natural language explanations as auxiliary training data for NLP tasks. It first transforms natural language expressions into a logical form through CCG, and then use a neural module network architecture to label data instances. Experimental analyses are conducted on two tasks -- relation extraction and sentiment analysis, showing that the proposed approach outperforms previous work that incorporates explanations as training data.\n\nOverall, the paper addresses an important issue of how to utilize human explanations as additional supervision source for NLP tasks and shows promising results. Hence, I believe the paper is above the acceptance threshold, and recommend for weak acceptance.\n\nHowever, I would also like to note that the paper has a few limitations. \n\nFirst, the proposed is based on semantic parsing of the natural language explanations into logical forms and is therefore inherently limited by the representation power of symbolic and logical representations. In the two examples shown in Figure 1, the human explanations are very simple and have limited variety, so it is relatively easy to represent them in logical forms. However, on many NLP tasks (such as question answering), the human explanations (in natural language) may often be complicated and difficult to be represented in CCG. Therefore, it is unclear whether the proposed approach can be easily generalized to other tasks. Besides, it is also non-trivial (and sometimes expensive) to collect human explanations."}