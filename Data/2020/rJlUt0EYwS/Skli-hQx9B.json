{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "One recent work that comes to mind from ACL 2019: Leveraging Language Models for Commonsense Reasoning (Rajani et al 2019). In that work, they also have human annotators provide explanations (extending the CommonsenseQA dataset), and they show that by training with these explanations, inference is improved even without them. They also train a language model to generate the explanations, and they show that the language model generated explanations improve performance further at inference time. Seems like a reasonable reference to contrast the more structured approach to using explanations like Srivasta et al (2017), Hancock et al (2018), and this work.\n\nI find the method summary beginning with \"Human explanations are first converted to machine-actionable logical forms by a semantic parser\" until the end of that paragraph to be unnecessary. Actually, as I read it, I find myself asking a lot of questions that get answered below. So I would prefer scrapping that method summary and just getting straight into the Explanation Parsing.\n\n\"indicates the the logical form matches\" redundant 'the'\n\nI can't find a definition for LF(E) anywhere, and yet LF(E) is present in many tables. I see that E is mentioned to be the explanations, but this is only in the caption of Table 2 even though the symbol is first used in the first paragraph of Section 4.1. I'm assuming LF logical forms applied directly to explanations, but this should be stated explicitly. Can you elaborate on why it is so dominant on precision In Table 6 and 7 of the Appendix, but low on recall, rather than just saying this is expected in Section 4.1? \n\n\"For keyword query q, we directly encode it into vector z_q by bi-LSTM and attention.\" Can you elaborate on how z_q is constructed? Is it the final state of a forward LSTM concatenated with the final state of a backward LSTM?\n\nWhy is it so essential that you study the setting in which explanations are low-resource? I'm curious to see what would happen with more explanations.\n\nI am surprised that none of the modules or compared methods include any architecture that use a Transformer or a form of contextualized word vectors (McCann et al 2017, Peters et al 2017, Devlin et al 2018). Is there an explanation for this?\n\nI would prefer to see a larger suite of tested tasks given that each of these datasets is quite small. Would any other tasks from benchmarks like GLUE or SuperGLUE be amenable to your approach? The tasks you've chosen limit the scope of this work and leaves the question of whether it would generally improve across a greater variety of tasks, especially tasks that have seen significant improvement using new methods. Your claim would be much stronger if the explanations were shown to be helpful even to pretrained models like BERT when fine-tuned for a specific task. In particular, it would be interesting to see how the benefits of explanations vary for different kinds of tasks and for different training set sizes. \n\n\"In the real world, a more realistic problem is that, with limited human-power, should we just annotate more labels or spend time explaining existing annotations.\"\" I think that you mean \"should we\" makes it sound like this is a question, but there is no question mark, and it makes more sense as a statement. I propose \"...human-power, there is a question of whether it is more valuable to ...\"\n\n\"Explanations prove to be an very efficient form for data annotation.\" should be \"a very\" not \"an very\".\n\n\" a vital rule\" should be \"a vital role\"\n\nI find the first paragraph of Section 4.3 quite abstruse and bare in its explanation of the method used."}