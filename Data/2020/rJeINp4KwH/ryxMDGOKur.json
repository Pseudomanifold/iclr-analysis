{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work presents a distributed framework for off-policy RL consisting of multiple agents that are trained in parallel while also being regularized to be similar to the current best policy. Maintaining a population of agents can help mitigate issues due to convergences to local optima. The method is evaluated on standard continuous control tasks, and shows some performance improvements over methods that train just a single agent. Ablation experiments are also conducted to evaluate the effects of different design decisions.\n\nThe overall method is sensible and performance looks promising. But the technical innovation is fairly modest. As the authors pointed out, the proximal constraint has been used in previous distributed RL framework, and the main difference in this work is enforcing a trust region penalty against the best policy, as opposed to an average policy. The proof of guaranteed improvement largely follows the proof from other trust region methods, by replacing the old policy with the best policy from the previous iteration. The experiments did compare with a number of previous algorithms, but the comparisons are all to algorithms that train a single agent at a time, and no comparisons are made to other distributed RL algorithms. Including some comparisons to other distributed methods can help strengthen the claims in favour P3S. In particular, how does regularizing using the best policy compare to using an average policy like in Distral [The et al., 2017]? That being said, the performance improvements on most environments appear to be fairly modest, and it is not clear if the improvements are indeed significant. Including additional experiments on more challenging tasks could be helpful here. Overall, the contribution of this work is pretty incremental, and I think it is not quite at the standards for ICLR at this time. But with more thorough evaluation and polishing, I think this work can make for a strong submission.\n\nMore specific notes:\n\nThere are a fair bit of awkward phrasing and grammatical errors in the writing, which hurts the clarity of the exposition.\n\nOne of the advantages of a distributed framework is faster wall-clock time. The current learning curves only compare the sample count. It might also be informative to compare the wall-clock time of the different methods, as well as including comparisons to other distributed frameworks.\n\nSome experiments to show how performance scales with the number of learners can also be helpful. Since one of the possible factors that improve performance for P3S is having multiple agents, providing some insight on how performance varies with different numbers of agents can be valuable in this regard.\n"}