{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The  paper proposes a new approach to multi-actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state-of-the-art mono-actor algorithms and over several other multi-actor RL algorithms.\n\nI'm in favor of accepting the paper despite a few serious weaknesses described below.\n\nA good point of the paper is the related work section which provides a good and concise survey of various multi-actor RL approaches: distributed RL, population-based training and Guided Policy Search (the last part about exploiting best information looks less relevant).\n\nHere a set of random  remarks:\n\nAbout  population-based training, there is quite a lot of repetition between the introduction and the related work  section.\n\nThe P3S approach described in the beginning of Section 3 and the end of Section 3.2 seems to rely on some arbitrary choices and a few hyperparameters whose impact is not much studied.\n\nIn the theoretical study (Section 3.1), a KL divergence is used as a distance between policies, which implies stochastic policies. But P3S is used on top of TD3, where policies are deterministic. This should definitely be discussed.\n\nThe role of \\beta in the theory is not put forward in a way to make the point of Section 3.2 clear. The theory should be clarified in this respect.\n\nThe top of p5 is made of a unique sentence over 7 lines which is completely obfuscating. This part must be rewritten and much clarified.\n\nI would be glad to see the performance on Swimmer, as this benchmark is known to suffer from  a deceiptive gradient.\n\nThe fact that a negative cost of action in Ant-v1 can result in no action in this environment is reminiscent of the same effect shown in the simpler Continuous Mountain Car environment in the Gep-PG paper (Colas, Oudeyer and Sigaud, ICML 2018). Actually, moving to simpler benchmarks would make it possible to provide more detailed empirical studies of the inner mechanisms of the P3S approach.\n\n p6: \"The policies used for evaluation are stochastic for PPO and ACKTR, and deterministic for the others.\" Do you mean you used a deterministic policy for SAC? This would be unusual, as SAC with a deterministic policy is very close to TD3.\n\n p6: \"In Fig.   3,  it is first observed that the performance of TD3 here is similar to that in the originalTD3 paper (Fujimoto et al. (2018)), and the performance of other baseline algorithms is also similar to that in the original papers (Schulman et al. (2017); Haarnoja et al. (2018))\". This sentence can be much compressed: \"In Fig.   3,  it is observed that all other baseline algorithms is also similar to that in the original papers (Fujimoto et al. (2018),Schulman et al. (2017); Haarnoja et al. (2018))\". \n\nWhy did you use the v1 versions of the benchmarks, and not the v2?\n\nI did not check the proofs in appendix.\n\ntypos:\n\np2: hyperparamters\np7: is the way how the best =>  is the way the best...\np7: other all parallel => all other parallel\np22: environmet"}