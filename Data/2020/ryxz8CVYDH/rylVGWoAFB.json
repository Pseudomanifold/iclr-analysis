{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a learning to learn (L2L) framework for zeroth-order (ZO) optimization, and demonstrated its effectiveness on black-box adversarial example generation.  The approach is novel since L2L provides a new perspective to zeroth-order optimization. \n\nHowever, I have some concerns about the current version. \n\n1) The knowledge that one should use to train UpdateRNN and  QueryRNN is not clear. A clear presentation is required \n\n2) Please clarify the optimization variables in (4). In general, the problem is not clearly defined. \n\n3) Eq. 5 is a query-expensive gradient estimate. Will it make training extremely expensive?\n\n4) The computation and query complexity are unclear during training and testing.\n\n5) Pros and cons of L2L? It seems that training a L2L network is not easy. Does its advantage exist only when inference? A better discussion should be made.\n\n"}