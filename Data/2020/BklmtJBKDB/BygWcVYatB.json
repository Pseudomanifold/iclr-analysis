{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper demonstrates how normalising flows can be conditioned. The method is then demonstrated on a set of sequential experiments which show improvements over the considered base lines.\n\nI recommend rejection of the paper, but I can see me changing that assessment if certain improvements are made. The central points are:\n- the paper has errors,\n- the paper does not respect some related work and has been published previously in parts,\n- the paper has a claim that is unsupported in my view,\n- the paper is overcrowded with annoying marketing language; the word \"novel\" appears 16 times according to my pdf viewer.\n\nIn general I like the idea, and the presentation seems solid to a large degree. However, the above points are a show stopper for me personally.\n\nFor one, the statements \n\n- p(y|x) = p(y|x, z) p(z | x) and\n- p(y|x) = p(y|z) p(z|x),\n\nare problematic. I would like the authors to clarify how they arrive at these.\n\nThe paper starts with the claim that \"prior work [...] imposes a uni-modal standard Gaussian prior on the lagent variables\". This is just wrong. The whole literature of stochastic recurrent models does not do this. See  [1, 2] for starting points. Since the authors place their work in the setup of sequential prediction, this is what has to be respected.\n\nFurther, the authors do not seem to be aware of a recently published work [3] that adresses *exactly* this problem. To quote from their abstract: \"To this end, we modify the latent variable model by defining the likelihood as a function of the latent vari- able only and [sic] introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data.\"\n\nI have two more questions with respect to the proposed regularisations.\n\nFirst, I would ask the authors to comment on the relationship of cR and the method proposed in [3]. To me, it appears as if cR is not novel, but has instead been proposed in [3] previously.\n\nSecond, pR fixes the variance of q. The authors claim that the normalising flow of the conditional can undo this fixing by adequately scaling the prior. Hence, so the claim, the expressivity of the model is not reduced.\n\nThis prohibits the posteriors of two distinct data points to share the same mean but not share the same variance. \n\nI request the authors to make a more formal analysis of this, as I do am not convinced how the expressivity of the model is maintained and what influence this has on the ELBO.\n\n\n\nReferences\n[1] Bayer, Justin, and Christian Osendorfer. \"Learning stochastic recurrent networks.\" arXiv preprint arXiv:1411.7610 (2014).\n[2] Chung, Junyoung, et al. \"A recurrent latent variable model for sequential data.\" Advances in neural information processing systems. 2015.\n[3] Klushyn, Alexej, et al. \"Increasing the Generalisaton Capacity of Conditional VAEs.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019."}