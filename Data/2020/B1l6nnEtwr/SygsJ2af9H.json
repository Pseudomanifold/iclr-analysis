{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The work proposes to learn neural networks using homotopy-based continuation method. The method divides the parameter space into two groups (extendable to multiple groups) and introduces a homotopy function which includes the original optimization problem as an extreme case.  By varying the homotopy parameter, one can construct a continuous path from a supposedly easier to solve optimization problem to the problem of interest. The authors prove convergence in the non-convex case, the existence of solution path in the convex case and demonstrate the effectiveness of the proposed method on synthetic and real datasets.\n\nWhile the idea itself is rather intriguing and seems promising, the current presentation and experimentation does not meet the acceptance threshold.  The writing of the draft needs a lot of improvement, in particular the notations the authors used are not consistent throughout the paper, which is very confusing. \n\nThe synthetic example the authors used in section 4.1 are naturally decoupled among the different dimensions of the parameters, which is no surprise the proposed method would achieve 100% convergence as shown in table 1. \n\nIt seems the division of the parameter space would matter. One would imagine there exists certain division leading to much easier to solve subproblems. Do the authors have any insight or experiments comparing different division strategies?\n\nHere's a very closely-related work that should be cited and discussed:\nWang, Xin. \"An efficient training algorithm for multilayer neural networks by homotopy continuation method.\" Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94). Vol. 1. IEEE, 1994.\n\n\nTypos:\n1) Equation following remark in page 2, should H() be replaced by G() or \\nabla H()?\n2) After equation (7), should G():= \\nabla H instead of F?\n"}