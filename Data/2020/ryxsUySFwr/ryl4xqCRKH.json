{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work is tackling the problem of doing out-of-distribution detection in regression. The core idea is to fit a simple generative model to hidden activations and use thresholded likelihood to decide whether an input is in or out of distribution. Using a simple generative model for this is motivated by the intrinsic low-dimensionality of hidden activations.\n\nMy issue with the narrative of this paper is two-fold. \n\nFirst, the intrinsic low-dimensionality of the hidden activation distribution seems entirely problem-dependent. One can train networks with orthonormal layers and therefor flat spectra that work fine and there are also invertible discriminative models that do not discard any information from layer to layer, albeit being very similar to standard ResNets. It might be that this heuristic works well for the problems discussed in the paper, but I don't see any reason for this to hold in general.\n\nSecond, Nalisnick et al. don't show that deep generative models are often overconfident (as stated in the paper here). What they show is that likelihood can be higher for datasets with lower entropy than the one the deep generative model has been trained on. They also show that \"graying\" inputs increase the likelihood and they can even show this analytically for flow models with data-independent determinant of their Jacobian. The GMM approach proposed here is thus a special case of what Nalisnick et al. described and should have the same problem. Reducing the variance of the features should increase the likelihood.\n\nAll in all, the presented method relies too much on heuristics that seem specific to the analyzed architectures and datasets and there is no theoretical reason for this to hold in general as far as I can see.\n\nThe originality of the paper is also somewhat limited, as generative models for OOD detection are well-studied. Thresholded likelihoods are problematic in many settings, so I am not convinced this approach will work well across the board.\n\n----\n\nEric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep generative models know what they don\u2019t know? In ICLR, 2019."}