{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "===Summary===\n\nThe authors propose to perform out of distribution detection for regression models by fitting a generative model in the feature space of the regression model. An input example is deemed to be out-of-distribution if it has low likelihood under this generative model.\n\n===Overall Assessment===\n\nI recommend that the paper is rejected. There are a number of aspects that need to be improved. You should fix these and resubmit to a future conference. \nThe paper focuses on the difference between regression and classification tasks and claims that the paper's method addresses an unmet need for OOD for regression. However, both the proposed method and the analysis justifying it are generic enough to be applied to both regression and classification.\nThe paper handles technical claims far too casually in sec 4 and does not provide sufficient justification that the claims are true.\nThere are natural baselines, such as using a generative model on the raw input space, that are ignored.\n\n===Comments===\n\nRemark 1 feels to me like it was added for the sake of having more math in the paper, not because it is crucial to the paper's argument. \n\nYou remark at various places that existing methods don't naturally generalize from classification to regression. However, you never fully explain why. Also, your proposed method can be applied out-of-the box to classification problems. Your analysis in sec 4 trivially applies to binary classification tasks, and could be naturally extended to multi-class classification where w is not a vector but a num_classes x num_features matrix. \n\nThe parallel should be between classification and heteroskedastic regression, since there you have a distribution per example.\n\nThe logic in \"In-distribution features are intrinsically low dimensional\" is insufficient\n\nThe connection between section 4 and your proposed method is not particularly precise. You also have lots of technical claims in 4 that are unsubstantiated. For example, you write \"this new network will likely have less discarded information than the shallower network\". What does 'likely' mean? In what sense are you making an actual technical statement? Each of the subsections in sec 4 has similar issues.\n\"The CNNs are pre-trained on ImageNet (Denget al., 2009) and the last layer is replaced with a linear layer that produces a single output.\"\nWhy did you do this? Did you fine tune or just retrain the top layer?\n\n\"For these two baselines, the variance of the forward passes is used as a metric for detecting OOD inputs\" \n   Can you explain why these are reasonable baselines for OOD? Why no baseline that fits a generative model in input space?\n\nYou should cite Ren et al. \"Likelihood Ratios for Out-of-Distribution Detection\"\n"}