{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a method for learning smoother convolutional kernels with the goal of improving robustness and human alignment. Specifically, they propose a regularizer penalizing large changes between consecutive pixels of the kernel with the intuition of penalizing the use of high-frequency input components. They evaluate the impact of their method on the adversarial robustness of various models and class visualization methods.\n\nAt a high level, the proposed idea is interesting. Reducing the reliance of classifiers on high-frequency patterns is a plausible way of improving human alignment. However, the experimental evidence presented is either unreliable or not sufficient to demonstrate the merit of the approach.\n\nMy biggest concern is the evaluation of adversarial robustness. The authors perform a number of off-the-shelf attacks using the foolbox library without accounting for fundamental differences between these attacks or basic principles of adversarial evaluation. Specifically, when evaluating the robust accuracy of a model, one needs to specify a concrete threat model (e.g., perturbations of L2-norm at most X), perform various attacks respecting this threat model, and report the number of  examples correctly classified against _all_ attacks (see https://arxiv.org/abs/1902.06705 for additional discussion and justification). Instead, the authors consider attacks with unbounded perturbation (for which robust accuracy does not make sense) and attacks using different perturbation constraints (ADef), while reporting improvements on a per-attack base. Hence, most of the columns of Table 1 are unreliable and cannot be taken into account when evaluating the models' robustness.\n\nEven ignoring these issues, taking into account the only column of Table 1 that is indicative, the PGD attack, we do not see sufficient evidence for the merit of the method. With the exception of a single outlier for MNIST TR (which I cannot interpret), the proposed regularizer only marginally affects the robustness of a classifier. In particular, no models become robust by adding the regularizer and the impact is limited for the case of already robust models.\n\nFurthermore, the evidence in favor of human alignment is relatively weak. While the method does have some effect for the case of simple models (MNIST and Fashion-MNIST), for the case of complex models (for which visualization is actually a challenging problem) the improvement is virtually non-existent. For Figures 5 and 6, the columns with R have essentially the same visual quality as the original columns. The only exception are the L and A columns for CIFAR10 bird and deer for which the original visualization fails. However, I find this confusing. There exist several works by now performing visualization using adversarial models and I have never encountered such a failure before. I wonder if there is some issue with the training of these particular models.\n\nOverall, while the high-level idea of the paper is interesting, the experimental evidence presented is either weak or unreliable. I will thus recommend rejection."}