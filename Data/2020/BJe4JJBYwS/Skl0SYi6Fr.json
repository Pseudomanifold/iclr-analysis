{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new cascaded image-to-image translation method to address the I2I tasks where the domains have exhibit significantly different shapes. The proposed method train cycle GAN on different levels of feature extracted by pre-trained VGG and combine the futures with the AdaIN layer to keep the correct shape from the deep features. \n\nPros:\n1. The proposed method seems to work well on different shape I2I datasets without using semantic masks compared to previous works.\n2. The idea of cascaded translators sounds simple and reasonable which can probably benefit other related tasks. The way of applying AdaIn to combine features of different levels is also a nice trick to keep the correct shape from deep features.\n3. The paper writing is OK, but some explanation and organization should be improved as mention in cons.\n\nCons:\n1. Some figures are hard to understand without looking at the text. For example, in Figure 1, the caption does not explain the figure well. What does each image, the order, and the different sizes mean?  As to Figure 3, the words \u201ctop left image\u201d, \u201cright purple arrows\u201d are a bit confusing.\n2. The \u201cCoarse to fine conditional translation\u201d section describes the conditional translation in the shallow layers. I suggest mentioning it in previous sections for easy understanding.\n3. As to the t-SNE visualization in Figure 9, different methods seem to use different N-D to 2-D mapping functions. This may lead to an unfair comparison.\n\nSuggestions:\n1. The authors use the pre-trained classification network VGG for feature extraction and then train dedicated translators based on these features. I wonder if the authors also tried finetuning VGG on the two domains or training an auto-encoder on the two domains. The domain-specific knowledge may help to improve the results and alleviate the limitations presented in the paper, e.g. background of the object is not preserved, missing small instances or parts of the object due to invertible VGG-19."}