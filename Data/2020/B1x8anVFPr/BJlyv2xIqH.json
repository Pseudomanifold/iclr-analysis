{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Specific problem tackled by the paper:\n\nMoving the LayerNorm layer to be inside the residual connection in a stack of transformers can remove the need for learning rate warm up. This paper provides a theoretical motivation for doing this.\n\n2. Motivation of the paper:\n\nThe authors motivate the problem clearly, performing experiments to demonstrate the problem. Their experiments provide good context for the problem they are solving and acts as a solid reference point for their (and other peoples' future) work. To be more convincing  the authors should have performed multiple runs and shown the standard deviations across runs.\n\n3. Claims of the paper:\n\nThe authors claim that layer norm should be placed inside the residual connection (Pre-LN) rather than outside (Post-LN) it. The authors show, theoretically, that in a Post-LN transformer the magnitude of the gradients in a transformer scale with the number of layers and that the magnitude of the hidden states scale linearly with the layers that that they are output by. While in a Pre-LN transformer the magnitudes of the gradients and states are independent of the number of layers.\n\ni.e. During training a Post-LN transformer is likely to have weaker gradients in the lower (closer to input layers) than at the output layers.\n\nThis theory is backed by an empirical study. It is good that these experiments are repeated ten times however the authors should show the standard deviations too.\n\nThe authors show machine translation results, demonstrating that using Pre-LN rather than Post-LN leads to faster convergence, however the models converge to the same result. However, there is benefit in not having to design a training schedule. Wang et al. have also shown the benefits of Pre-LN rather than Post-LN transformers for machine translation.\n\n4. Decision (accept or reject) with one or two key reasons for this choice and reasons for the decision.\n\nWeak accept.\n\nPros:\n(1) The authors provide theory that supports the use of Pre-LN rather than Post-LN transformers. Using Pre-LN rather than Post-LN transformers may save a lot of time by avoiding hyper-parameter tuning, without loss in performance and this looks very easy to implement.\n(2) The paper is well organised and the problem is very well motivated.\n(3) The experiments are sufficient. They could be improved with repeats and by showing the standard deviations (as mentioned above).\n\nCons:\n(1) This work is incremental. Wang et al. have already shown that Pre-LN transformers are better that Post-LN transformers when the network has many transformer layers, they explain theoretically why this is the case. \n(2) While the organisation of the paper is good, the paper is not well written. The gramma is poor.\n(3) The paper is very long for an incremental improvement.\n\n5. Additional feedback with the aim to improve the paper. \n\n[a] Ideally, it would be good to see repeats for Figure 2 and the standard deviations for Figure 3.\n[b] Without reading the appendix it is not clear where the assumption that W^Q and W^K are zero is used. Making some connection with how this assumption relates to the lemmas would be useful. Additionally, you should explain what this means qualitatively, because this makes the assumption more acceptable. I am assuming that it means that the attention is uniform.\n[c] In Lemma two you are comparing the magnitudes for the input in the Pre-LN and the output in the Post-LN transformer according to how x_{l,i}^post and x_{l, i}^pre are defined in Table 1.\n[d] In Figure 3(b) the gradients are clearly decreasing with the number of layers, are there any comments on this? In the limit this could cause vanishing gradients?\n[e] On the surface Figure 2 and 4 appear to contradict. Is the difference a result of using RAdam? If so, this should be made very clear. If not, why are the results contradictory?\n\nMore minor comments:\n[1] Many gramma errors.\n[2] Figure 1 is referenced before explaining what an FFN is. Figure 1 could also be enhanced by labelling Post-LN as previous work and Pre-LN as current work.\n[3] MultiHeadAtt is not well defined. Multi-Head( ), Attention( ) and Head( ) each take three arguments, while MutliHeadAtt( ) takes two. It would be worth connecting these.\n[4]\"sub-layer --> ...\" here sub-layer is not defined. Should this say self-attention sub-layer?\n[5] Where does equation (4) come from? There should be a citation and/or explanation.\n[6] The BLEU score is not defined (this could be with a footnote).\n[7] Top of page 7: \"As most of the parameters are initialized by Gaussian distributions\" --> Using the word \"most\" is very vague. The authors should be specific about which parameters they are referring to.\n[8] There is a good balance of equations in the main text with most of the proofs in the appendix.\n[9] Inconsistent use of LN and LayerNorm, both are used."}