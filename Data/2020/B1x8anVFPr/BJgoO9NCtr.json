{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the placement of Layer Normalization in the transformer architecture. The authors show that the Pre-LN placement leads to better behaved gradients as the network gets larger. This in turn allows them to remove the warmup stage of the learning rate schedule, leading to a faster and simpler training procedure. They examine a simplification of the attention layer analytically, and show a scaling with the number of layers that occurs with the Pre-LN placement but not the Post-LN placement. Finally, they demonstrate the effectiveness of the transformer changes both for machine translation and in BERT pretraining.\n\nEven though the novelty here is limited, Pre-LN placement has been used in prior work, the potential for accelerating future research is large. In general I think the impact of this kind of research, exploring improvements to commonly used methods that add no additional complexity or even simplify, is underappreciated in the reviewing process. Still, I have some concerns about the relation between the analytic investigation of the gradient norms and the empirical results that are presented, and I am concerned that the analytical results are used to imply something stronger than they actually show.\n\nAt some level, it seems like the theoretical results have come along for the ride but do not clearly demonstrate that there is a problem with Post-LN and that this problem is fixed by switching to Pre-LN, or at least the relationship is not clearly explained. In the empirical study and central to the paper\u2019s narrative is the fact that Post-LN normalization leads to a vanishing gradient problem where gradient in later layers is substantially larger in magnitude than in early layers at initialization, whereas the Pre-LN placement does not suffer from this problem. The point mentioned in the main text (theorem 1) is that the final FFN layer has gradient magnitude that scales at most like 1/sqrt(L) in the Pre-LN case and independent of L in the Post-LN case. This alone does not imply any of the observed empirical behavior because if all of the layers were scaled by this factor 1/sqrt(L) then the same problems would persist for the Pre-LN network. More relevant to the findings is how the scaling of the gradient changes throughout the layers which is examined in appendix section F, where the gradient norm of the Post-LN network can be upper bounded by an expression in which one of the terms scales approximately as (2/3)^(L-l)/2, whereas in the Pre-LN network the scaling is explained to be independent of layer index l. The connection between the expression that scales in the upper bound and the actual gradient norm is tenuous and there multiple places where the argument could break down (upper bound not being tight, scaling of other terms canceling out, validity of the simplifying assumptions being used). It would be useful to verify this sqrt(2/3) scaling on the data from the empirical study that is shown, is the decay shown in figure 3 geometric with this factor. Also the fact that the expectation near the bottom of page 20 is approximately 2/3 needs to be explained as it\u2019s not obvious where that comes from.\n"}