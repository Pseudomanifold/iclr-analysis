{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: The paper investigates the myth about layer normalization and learning rate warmup for the Transformer architecture. It shows, both theoretically and empirically, that putting the layer normalization in the residual blocks rather than between the residual blocks, could make a big difference to the scale of the gradient at the initialization stage.\n\nPros:\n  + A well-written paper with a good organization; notations are clear.\n  + The proof I checked seem correct (but I didn't check all of them).\n  + Good experimental design that compares the pre-LN and post-LN Transformers in different settings/tasks.\n  + Investigating the purpose and the theory behind using LN and learning rate warmup is a very interesting topic to me, as these modules are rarely used when training other kinds of deep nets (e.g., ConvNets, etc.).\n\nI will detail the cons below, along with my other questions/concerns/issues.\n\n----------------------------------------\n\nQuestions/concerns:\n\n1. One of the two major concerns I have is the novelty of this paper in terms of its methodology and empirical value to the community. The Pre-LN setting of Transformers has already been widely used. For instance, Baevski et al. [1] and Child et al. [2] are both well-known works that have applied the pre-LN setting and achieved SOTA results on various very challenging benchmarks. These papers also reported that using layer normalization before self-attention brought \"more effective training\", which is one of the major empirical remarks that this paper made as well.\n\n2. The second major concern I have is the connection the authors established between its theoretical findings and the empirical findings. While the post-LN Transformers may have larger gradient at higher-levels (as in, close-to-the-output levels), actually some (famous) prior works on Transformers have applied gradient clipping to their architecture, such as BERT [3] (https://github.com/google-research/bert/blob/master/optimization.py#L74), sparse transformer [2] and Transformer-XL [4]. But even when the gradient clipping is applied, learning rate warm-up still seems very helpful (and sometimes necessary), as was used in all of these works. Therefore, I think to further verify the theoretical hypotheses of the paper, the authors should at least also study whether (and to what degree) the very simple \"gradient clipping\" (or other gradient normalization techniques) solves the problem (which is a common solution to exploding gradients).\n\n3. While Figure 3 is interesting to see, I don't think it verifies Theorem 1 exactly. What it verifies is the \"extending to other layers/parameters\" paragraph (i.e., the gradient scale decreases with layers). Did you try training post-LN Transformers and pre-LN Transformer with different # of layers from scratch (i.e., different L)? According to Thm. 1, I think we should expect to see a plot where post-LN gradient expectation remains at the same level for all L, and pre-LN gradually decreases with L.\n\n4. The proofs are based on the core assumption that we are \"at initialization\" (e.g., you assumed W_V entries are sampled from N(0, 1/d), that W_Q=W_K=0, and that the input data are normally distributed x ~ N(0, \\sigma^2 I_d)). How will the conclusion/derivation to change when these conditions are no longer met (e.g., after a few steps of warmup)? What do you expect to be the relationship between \"the number of warmup steps\" and solving the \"gradient scale problem\", which you proved on these assumptions?\n\n======================================\n\nSome minor issues that didn't impact the score:\n\n1. You used \\delta = e^{-d \\epsilon^2 / 8} when discussing the tail bound, and later 3e^{-4}, 1e^{-4} for the learning rate. Just for notational consistency, maybe use 10^{-4} for the learning rate instead.\n\n2. Referred to the wrong equation numbers (mentioned in another comment from me). \n\n3. Appendix C: radius d -> radius \\sqrt{d}\n\n4. Appendix C: \"Similarly, we have \\| x_{l,i}^{post, 3} \\|_2^2 = d\" should have an expected value.\n\n======================================\n\nDespite the potential lack of novelty on method, I do think investigating these myths and instabilities of training Transformers is a very interesting direction to pursue. I think this paper can be improved with more experimental settings to verify the claim it proposes (i.e., on the gradient, which brings a lot of different things to analyze/study/fix here). I put the paper slightly above the acceptance borderline.\n\n\n\n[1] https://arxiv.org/abs/1809.10853\n[2] https://arxiv.org/abs/1904.10509\n[3] https://arxiv.org/abs/1810.04805\n[4] https://arxiv.org/abs/1901.02860"}