{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper defines precise semantics\u00a0of disentanglement representations and presents evaluation metrics to evaluate such representations. The authors provides information-theoretic characterization disentangled representations along three dimensions: informativeness, separability, and interpretability; and propose metrics to measure them. The authors argue that:- Informativeness can be defined as the mutual information\u00a0between a particular representation (or a group of representations) w.r.t the data---I(x,z_i)\u00a0- Separability between two representations can be achieved if they do not share any common information about the data---I(x,z_i,z_j) = 0\u00a0- Interpretability with respect to a concept is achieved if a representation contains information about the concept---I(z_i, y_k) = H(z_i) = H(y_k).The authors define a disentangled representation as a representation that is fully separable and fully interpretable and propose a suite of metrics to evaluate disentangled representations.Finally, the authors evaluate several representation learning methods (FactorVAE, betaVAE, AAE) using these metrics on toy and real datasets.\n\nI think this paper addresses an important problem of quantifying and measuring disentangled representations. The proposed metrics are reasonably sound and the authors provide an extensive set of experiments to show how to use them in practice.\u00a0\n\nI have one comment regarding the sheer number of metrics that are presented in the paper and their practical usage. How do the authors see them being used in the future to compare models? Is one of the main arguments of the paper to encourage other people to use all of these metrics? Are all of them needed, given some of them seem to correlate with others? I think it would be helpful to highlight a few metrics or aggregates of them to inform future research in this area.\n\nAlso, for the paper to be more self-contained, I think the authors should include a short discussion about models that they compare in the experiments.\n"}