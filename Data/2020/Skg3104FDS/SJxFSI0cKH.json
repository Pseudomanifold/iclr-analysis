{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an interesting optimization algorithm called first-order preconditioning (FOP). \nThe basic idea of FOP is updating the preconditioned matrix by its gradient, which avoid calculating or approximating the Hessian directly. To make the algorithms more practical, the authors also conduct the low-rank FOP and the momentum-type version. The empirical studies on CIFAR-10 and ImageNet validate the effectives of the proposed algorithms.\n\nMajor comments:\n\n1. Section 2.1 says \u201cwe follow the example of Almeida et al. (1998) and assume that J does not dramatically\u201d. However, the goal of FOP is to encourage J reduce faster. Is there any conflict?\n\n2. In low-rank FOP, the initial preconditioner P contains the term I_m which does not exist in standard FOP (section 2.1). How does this term affect the update procedure? Can you provide some details?\n\n3. Theorem 2 provide a linear convergence of FOP under convex, Lipschitz and PL condition. The proof relaxes the preconditioner P into its minimum and maximum eigenvalues. Since P changes over the course of training, it is difficult to check weather the result of Theorem 2 is stronger than gradient descent method.\n\n4. Why the experimental results not include the other second order optimization algorithms such as K-FAC and KFC?\n\nMinor comment:\n\nThe notations M in (1) (2) and (5) are ambiguous. It is prefer to use another letter to present the preconditioner in (1).\n"}