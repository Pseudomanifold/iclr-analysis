{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a first-order preconditioning (FOP) method to generalize previous work on hypergradient descent to learn a preconditioning matrix that only makes use of first-order information.\n\nPros:\nThis paper extends the idea of hypergradient descent in [Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al., 2017] with a preconditioning method. A low-rank FOP is further proposed to lighten the computation burden for the preconditioning matrix. \n\nCons:\n1-\tThe novelty and contribution is not clear.\n2-\tThe ideas of approximating the preconditioning matrix or factorized approximate inverse have been well studied in the literature, which are not sufficiently cited in the paper, such as Adagrad (Duchi et al. 2011), review in Bottou et al. 2016, etc. \n3-\tDerivation of Eq.(4) seems to be missing. \n4-\tTypo errors such as \u201cis can\u201d in page 5. \n5-\tA mistaken derivation in A.1 Eq.(20). \u201ck\u201d should be \u201ck+1\u201d.\n\nTherefore, I tend to give this paper a Weak Reject score. "}