{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposed an attention-based document classifier based on BERT-LSTM structure. Experiments on patent and ArXiv datasets show the structure is slightly better than three baselines. This paper also analyzed the effection of gradient update settings in different models. \n\nThis proposed model is lack of novelty, it applied the attention sum of LSTM output on top of BERT in the document classification task. Although this model concatenated more vectors (original LSTM output, argmax, attention weight) into the final prediction vector, the reason of integration those vectors is not explained and no experiment prove the effectiveness of this setting. The experiments are only evaluated on two less studied datasets which makes the comparison with other works less persuasive, more results on general document classification datasets are needed.\n\nQuestions: \n1. Why the RNN-BERT and the ATT-BERT only include h2~hm but exclude h1? \n2. As the segment is fixed in each model, why not include all the z1~zm but only include z1?\n3. \u201cRecurrent Neural Networks (RNNs) have been used for short text, e.g. sentiment analysis by Socher et al. (2011) \u201c, the cited paper is a recursive neural network. It is not a recurrent neural network.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}