{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a methods to combine multiple pre-trained language models using an attention mechanism in order to take the whole document into account. The effectiveness of the methods is evaluated on two long document classification tasks (Arxiv publication and patent classification) with state-of the art results.\n\nPros:\n\n- the effectiveness of the methods is experimentally demonstrated using two relevant datasets\n- the inverted wireless experiment clearly shows the interest of the attention combination strategy \n\nCons:\n\n- the methods is very simple (combining multiple segment prediction to perform document classifications) making the contribution of this paper quite weak. In the own terms of the authors, the \"main contribution [...] is to investigate the effectiveness of different combination strategies\". I am not sure that it is sufficient for the ICLR standards.\n- the paper is difficult to read and should be proofread to improve readability\n\nMinor issues:\n\n- Socher et al (2011) uses recursive neural networks (ReNN) and not recurrent neural networks (RNN). RNN are ReNN but restrained to a linear chain structure\n- In Introduction: ... in the  domain extremely complex data that is language ... -> I'm not sure the sentence is correct\n- In Introduction: the last sentence should be shorten and rephrased.\n- many typos "}