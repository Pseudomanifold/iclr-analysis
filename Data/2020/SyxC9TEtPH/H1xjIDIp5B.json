{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Authors provide an extension to the invertible generative models, by fashioning them into conditional invertible generative networks. The networks are conditioned at the input with a feed-forward network initialized with a VGG based classification network. Conditioning is implemented within the coupling layers (Dinh et. al. 2016) of the invertible model by simply concatenating the hidden layer output of the VGG encoder. The model is learned using an MAP objective along with some modifications to the original training procedure (described in sec 3.4). The model is evaluated qualitatively on \"style transfer\" on MNIST digits and image colorization. The technical contribution of this paper is the somewhat straight-forward extension of the cINNs to conditional generative networks. The actual implementation of conditioning seems quite trivial (sec 3.1). Although the results on colorization are claimed to be good, the baselines they compared to are not very recent (e.g. cGANs). Overall, I believe there is very less novelty, technical sophistication and performance improvements in this paper.  "}