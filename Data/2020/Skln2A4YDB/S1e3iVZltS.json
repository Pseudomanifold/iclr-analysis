{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors suggest back-propagating through a learned dynamics model and provide a derivation on monotonic improvement of the proposed objective. They show increased sample efficiency, asymptotic performance matching model-free methods, and ability to scale to long horizons. They provide bounds on the error of the gradient when using the learned model and Q function and the total variation distance between policies trained using true dynamics versus the learned dynamics model. This leads to a theorem showing monotonic improvement of the policy under their algorithm, MAAC.\n\nDecision: Weak Accept. The work contains both theoretical and empirical results on back-propagating through a learned dynamics model compared to other model-based and model-free methods. However, I'm unsure about the novelty of the method itself. How is this different from other planning through back propagation methods? This should be an additional section in related work. As examples, there is Universal Planning Networks [1], Differentiable MPC [2], and Path Integral Networks [3], [4] present a way to differentiate through path integral optimal control. I will increase my score if these concerns are addressed.\n\nNits:\nMissing C in \"Contrary to these methods\" in Related Work section\nConclusion: analized -> analyzed\n\n[1] Universal Planning Networks - Srinivas et al. 2018\n[2] Differentiable MPC for End-to-end Planning and Control - Amos et al. 2018\n[3] Path integral networks: End-to-end differentiable optimal control - Okada et al. 2017\n[4] Mpc-inspired neural network policies for sequential decision making - Pereira et al. 2018"}