{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents MAAC, a model-based reinforcement learning algorithm which makes use of path-wise derivative to optimize the policy. The learned Q function is used to estimate the terminal rewards and reduce instability and is trained by a STEVE-like style and Clipped Double Q. The dynamics model is learned by the same method in PETS. The policy is trained by directly computing the gradient to improve model-boosted Q. The theory shows that if learned model and learned Q function have similar derivative as the real one, the improvement of policy can be lower bounded. Experiments show that MAAC achieves the state-of-the-art and can be further improved by adding MPC. \n\n\nThe paper is well-written in general. The emperical results are very good and are claimed to be the new state-of-the-art. \n\n\nQuestions:\n\n1. Could you please elaborate how J_Q(phi) is calculated? I found one in preliminary but I suppose the one used in Algorithm 1 is different as the paper states that MAAC also uses the model to train Q. \n2. How is H(pi_theta) estimated? What's underlying state distribution of H(pi_theta)? \n3. The objective of policy network is similar to SAC's objective, but the optimization is different. SAC optimizes the objective by minimizing the KL divergence between pi and exp(Q). Do you have any comparison between BPTT and KL divergence minimization? \n4. How do you \"use the cross-entropy method with our stochastic policy as our initial distributions\" in MPC? \n5. Does MAAC work for Humanoid? \n6. How many samples are used to estimate the expectation in J_pi(theta)? For the single sample experiment, what will happen if the batch size is increased? \n7. In Algorithm 1, how does \"Sample trajectories T from \\hat{f}_\\phi\" work? \n8. To my understanding, when H = 0, the policy optimization doesn't use the model. Assuming Q function optimization uses the same H, the only usage of learned dynamics model is that pi and Q are trained in a different state distribution (D vs D_env). The ablation study shows that in this case, MAAC still outperforms SAC. So is it possible that D_model provides data augmentation? "}