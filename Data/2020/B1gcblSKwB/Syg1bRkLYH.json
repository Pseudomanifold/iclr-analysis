{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\n============= Summary =============\n \nThe paper addresses the issue of overfitting the meta-parameters of optimization-based meta learning techniques. It uses an iterative training-pruning-retraining setup to learn the meta-parameters. The pruning step in each iteration is to limit the capacity of the meta-learner and thereby alleviate overfitting. Two pruning methods called DSD and IHT are employed on top of a first-order meta learning technique called Reptile. The combination is tested on 4 few-shot classification scenarios of two datasets; omniglot and miniImageNet. Results suggests improved accuracy on top of the Reptile meta-learning algorithm on miniImageNet.\n \n \n============= Strengths and Weaknesses =============\n\n+ overfitting the meta learner due to the small number of samples (shots) per task and the large number of meta-parameters and/or base-learner parameters is an important problem in meta learning which is the focus of this work.\n+ the results suggest improvements over the Reptile baseline on miniImageNet.\n \n \nGeneral concerns:\n- abstract: \u201cthe existing meta-learning models have been evidenced to overfit on meta-training tasks when using deeper and wider convolutional neural networks.\u201d several methods such as CAVIA and MetaOptNet (among many others) address this issue by limiting the capacity of the learner.\n- what is the formal definition of meta-generalization and meta-overfitting? This definition will be helpful for understanding the paper\u2019s arguments, for instance, why \u201creducing meta-overfitting\u201d and \u201cimproving meta-generalization\u201d are two different matters (as suggested in the last part of the abstract).\n- Figure 1.b: the green bars (proposed method) don\u2019t seem to improve the generalization (testing accuracy). I can only speculate that the figure is to demonstrate that (in the rightmost plot) the difference between train and test accuracy for green bars is less than it is for the red bars. However, one should note that it seems to be due to the training accuracy being lower, which can be achieved, in the extreme case, by a random classifier (zero gap). So, it\u2019s not necessarily useful when testing accuracy is not improved.\n- eq (1) and eq (2): the left-most loss L should be on D^{query}\n- page 4: \u201cIn view of the \u201clottery ticket hypothesis\u201d (Frankle & Carbin, 2018), the model in equation 2 can be interpreted as a first-order meta-learner for estimating a subnetwork\u201c -> eq (2) in the current form still requires a 2nd-order derivative and I cannot see how (Frankle & Carbin, 2018) helps make it first-order optimization.\n- the usefulness of the provided theory is in question since the final method does not enforce the sparsity in practice. That is, the output of the final meta-parameters are *dense*.\n- page 7 mentions \u201ctop k_l entries\u201d: what does \u201ctop\u201d mean here? k_l dimensions of \\theta with the highest absolute value, maybe?\n- why do the two separate steps of only training the pruned subnetwork using Reptile and then retrain the whole network (with the updated subnetwork) again using Reptile? One can instead only train Reptile on the whole \\theta^{(i)}_M (with L0 norm constrained using the M projection) in a single Reptile step. Doing the former should be justified over the latter since its twice as expensive. Also it should be shown empirically (as an ablation study) that the former works better. \n- reference list: arXiv versions of many papers are cited, it\u2019s good to cite the peer-reviewed version of papers when applicable. For instance, IHT pruning method does not seem to be peer-reviewed which is a caveat for the current work.\n- 10 pages seem excessive for the content of the paper. For instance, the experiments section can be shortened extensively and the theories can be put into the appendix.\n \n \nExperimental concerns:\n- Has the standard miniImageNet split been used? This split is especially important to be respected since CAVIA\u2019s accuracy is taken from the original paper. \n- The reported number for CAVIA in table 2 is not the best number the original paper achieve. The best number is 51.82 \u00b1 0.65% for one-shot, and 65.85 \u00b1 0.55% for 5-shot. \n- There are quite a few new hyperparameters (3 iteration numbers for pretraining, pruning/finetuning, and retraining, and then the sparsity level k_l). It\u2019s important to mention how the hyper-parameters are chosen, especially since they are different for different setups. \n- there seems to be no meta validation set for Omniglot.\n- For a fair comparison, the network size of the baseline as well as the learning rate should be searched with the same budget as the hyperparameter search done for the proposed method.  \n- ResNet experiment is especially concerning since 1) there is an even higher-level of hyperparameter tuning for ResNet: first conv layer is not pruned, different pruning rates for different residual blocks are used. 2) the baseline is only tried with one setting for the capacity. It is evident in Table 2 that there is a sweet spot for the capacity of the ConvNet baseline, there is no reason this does not apply to ResNet.\n- Ablation study:  \u201cFor the purpose of fair comparison, only the retraining phase is removed and other settings are the same as proposed in Section 5.1\u201d: this is not fair. I believe a fair comparison would be to repeat the hyperparameter search for the ablation studies with the same budget as the full version of the proposed method.\n- Table 2 only compares with CAVIA while many other meta learning methods could be relevant here such as MetOptNet which also limits the number of base learner\u2019s learnable parameters thereby helping with overfitting. Also, CAVIA has results on ResNet useful for the last experiment.\n \n \n \n============= Final Decision =============\n\nWhile the paper addresses an important problem and reports improvements, there are many concerns with it including the writing, method, and experimental setup.  My \u201cweak reject\u201d rating, however, is mainly based on the experimental concerns especially regarding the hyperparameters and the baselines.\n \n \n============= Minor Points =============\n\n- code is not provided. I think it\u2019s in general very helpful to release the codes for reproducibility and replicability of the experiments, especially in the case of an incremental study.\n- what meta learning task has been used for Figure 1? \n- There are different ways of including batchnorm in a meta learning setup. For instance, Reptile used two different strategies. How is batchnorm implemented for this paper\u2019s experiments? Particularly, indicate if you are using transductive or non-transductive version of batchnorm at test time.\n \nThe text as well as the math notations require polishing on various occasions including the following (non-exhaustive) list:\n- abstract: \u201csparsity constrained\u201d -> sparsity-constrained\n- abstract: \u201ccan not only\u201d -> rephrase so that it\u2019s not read as \u201ccannot only\u201d\n- abstract and intro: \u201cease meta-overfitting\u201d can be interpreted as facilitating overfitting -> maybe better to say \u201calleviate meta-overfitting\u201d\n- intro: \u201cmemorize the experience from previous meta-tasks for future meta-task learning with very few samples\u201d \u2192 memorize the experience from previous tasks for a future task with very few samples\n- intro: \u201cAs can be observed from Figure 1(a) that\u201d -> It can be observed from Figure 1(a) that\n- intro: \u201csparsity benefits considerably to the\u201d \u2192 sparsity benefits considerably the\n- intro: \u201calong with two specifications to two widely used networking pruning methods\u201d: rephrase\n- related works: \u201cmost closely\u201d -> closest\n- page 4: \u201cultra goal -> maybe ultimate goal?\n- page 4: \\theta is not defined\n- page 4: eq (2): i -> m or m -> i\n- page 4: the definition of an unordered set (S) coming as a sample of the product of m \\Tau spaces is not precise. Better to say each T_i \\in \\Tau.\n- page 4: J_l is not defined,\n- sec 3.3: the minimization are done over \\theta, it\u2019s better to put that under \u201cmin\u201d and the conditions should be clearly indicated by \u201cwith or s.t. or similar\u201d\n- page 7: zero-one -> binary\n- page 7: \u201c as the restriction of \\theta^{(t)} over the mask M^{(t)}\u201d -> rephrase\n- page 7: \u201cit deems to reduce\u201d -> rephrase\n- page 8: \u201cthis is consist\u201d -> consistent\n \n- it\u2019s also good to settle on a single term for each concept to make it an easier read, for instance:\n- \u201cmeta-training tasks\u201d and \u201ctraining tasks\u201d have been used interchangeably. I think \u201ctraining tasks\u201d and \u201cmata training dataset\u201d are better choices since a \u201cmeta-training\u201d task can refer to the whole task of meta learning.\n- Meta-overfitting and meta-level overfitting\n- meta-learner, meta-estimator, model\n- meta-level training accuracy, meta-training accuracy\n- meta training, meta learning\n- base learner, learner, model \u2192 I think it\u2019s better to emphasize on learner being \u201cbase learner\u201d, avoid model as it is not clear what it refers to.\n \n \n \n============= Points of improvements =============\nThe paper would significantly improve if 1) the text is polished and the equations revised. 2) hyperparameter optimization is done carefully and thoroughly described for both the proposed method as well as the baselines\n\n\n"}