{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper presents a novel compact parameterization of convolution filters. Normally a filter bank is stored as a 3D array of shape C_out x C_in x H x W = K. This paper proposes to use a flat array instead, and obtain each of the C_out filters of shape C_in x H x W by indexing this array with stride. Thus, adjacent filters will share some of their weights. A fast algorithm is presented for convolving using filters parameterized in this way. An extension is presented where the location of each filter in the flat array is learned. The results show the potential of the method to reduce the number of parameters at a modest drop in accuracy, though it is not clear how the method stacks up against state of the art, or what the improvement in wall-clock runtime is. Overall, I would rate this paper \"weak accept\".\n\nOne minor weakness of the experiments on classification (3.1) is that the training procedure for FSNet and the baselines are different, with FSNet using cyclic learning rates and a larger number of epochs, making the results somewhat difficult to interpret. I suppose that the authors tried using the same training procedure but it did not work for FSNet - is that correct? If not, reporting those numbers would be preferable. If indeed the same training procedure does not work for FSNet that is not a fatal flaw, but we would want to see results for the baseline architectures trained in the same way as FSNet (to make sure the difference is not attributable to the training procedure). Also, we would want to make sure that the baseline results as reported are about as good as they can get for that architecture, e.g. by comparing to results for those architectures published by others, and by copying their training procedure. (I think the baseline results are fine, but it would be good to take away any doubt in the reader's mind).\n\nThe paper has a clear structure and is fairly well written, though it may still be beneficial to go over the text with a native speaker.\n\nOverall, I find that the experiments convincingly show that FSNet provides a way to compactly store filters with modest loss of accuracy. I am not an expert in this field though, and so I don't know how it relates to state of the art. One paper that may be good to compare to is \"Discrimination-aware Channel Pruning for Deep Neural Networks\" by Zhuang et al.\n\nIt would be nice to see wall-clock time / speed improvements reported, instead of only reporting the reduction in parameter count.\n\nTypos & minor issues\nThe paper uses \\cite{} in many places where \\citep{} would be more appropriate.\n\"fist spatial size\"\n\"each filer in a\"\n\"substraction\"\n\"epoches\"\n\"efficiency searching scheme\"\n"}