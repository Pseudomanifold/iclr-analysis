{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer. Moreover, a fast convolution algorithm is designed for the convolution layer with the FS. Some promising results demonstrate the effectiveness in CNN compression and the acceleration on the tasks of image classification, object detection and neural architecture search. I like the idea of weight sharing which seems like a reasonable choice for model compression. I expect it to be a general and standard component for model compression and acceleration. I have the following concerns:\n\n1.FSNet achieves a small model (FSNet-2-WQ in Table 5) with only 0.68M parameters and an mAP of 70.00% on the VOC2007 dataset. Can the authors include the model size of FSNet-2-WQ in MB so that the model size can be directly compared to that of the recently proposed YOLO Nano [1] (with a size of 4MB and an mAP of 69.1% on VOC 2007 dataset)? \n\n2. The backbone of FSNet-2-WQ in Table 5 is SSD300, which is a relatively large model for object detection. Please shed more insights on why weight sharing by Filter Summary can have such a high compression ratio, even with the help of quantization.\n\n3 Please explain why the simple linear quantization is compatible with Filter Summary for image classification and object detection. More concretely, why the simple linear quantization does not hurt the performance of FSNet on these tasks? Can similar results be extended to more models and tasks?\n\n4 Please show the illustration of the learned Filter Summary which can help understand the structure of the filters in this new representation.\n\n5 It could be better if the paper can reveal the correlation between the compression ratio and the accuracy of the compressed models by FSNet.\n\n[1] Wong et al. YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection. arXiv: 1910.01271\n"}