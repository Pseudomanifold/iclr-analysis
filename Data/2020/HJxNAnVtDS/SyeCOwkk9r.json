{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Federated learning is distinguished from the standard distributed learning in the following sense: \n1) training is distributed over a huge number (say N) of devices and communication between the central server and devices are slow.\n2) The central server has no control of individual devices, and there are inactive devices that does not respond to the server; full participation of all devices is unrealistic.\n3) The local data distribution at each device is different from each other; i.e., the data is non-iid.\n\nDue to property 1), communication-efficient algorithms such as Federated Averaging (FedAvg) have been proposed and studied. FedAvg runs SGD in parallel on K (\u2264N) local devices using their local datasets, and updates the global parameter after E local iterations by aggregating the updates from the local devices.\n\nProperties 2) and 3) makes analysis of FedAvg difficult, and previous results have proven convergence of FedAvg assuming that the data is iid and/or all devices are active. In contrast, this paper studies FedAvg on the non-iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T). \n\nOverall, I enjoyed reading this paper and I would like to recommend acceptance. This is the first result showing convergence rate analysis of FedAvg under presence of properties 2) and 3), which is a nontrivial, important, and timely problem. The paper is well-written and reads smoothly, except for some minor typos. The convergence bounds provide insights of practical relevance, e.g., the optimal choice of E, the effect of K in convergence rate, etc. The authors also provide empirical results supporting their theoretical analysis.\n\nSome questions I have in mind:\n- What is \"transformed Scheme II\"? Is it the scaling trick described at the end of Section 3.3? The name appears in the experiment section before being defined.\n- What happens if we choose \\eta_t that is decaying but slower than O(1/t), say O(1/\\sqrt t)? Can convergence be proved? If so, in what rate?\n\nMinor typos:\n- Footnote 3: know -> known\n- Assumptions 1 & 2: f in $f(w)$ is math-bold\n- Choice of sampling schemes: \"If the system can choose to active...\" -> activate\n- mnist balanced and mnist unbalanced: the description after them suggests they should be switched\n- Apdx D.1: widely -> wide, summary -> summarize"}