{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\t\nThis paper aims to compare SGD and SVRG in deep learning, motivated by recent results that SGD performs better than SVRG, despite the latter's theoretical optimality.\nThe idea in the paper is to study this problem through linear regression by establishing \nsome asymptotic bounds for both SGD and SVRG. By looking into the terms of these bounds one can initiate a comparative study. A mixed picture is presented in the experiments which roughly agrees with some of the authors' claims.\n\nThere are, however, several important issues with the paper that require a major revision:\n\n1) The connection between neural networks is never really established. There is also an obscure relationship between 'overparameterized/underparaterized' neural networks and 'without/with label noise'. While this relationship is important to switch our attention to a much simpler problem, the specifics are not explicated.\n\n2) The theoretical content is not novel. All results on second moments (and more) are well known. \nFor example, [4] have both non asymptotic analysis, and a characterization of sampling variance for general SGD --- the assumptions of normal X with diagonal variance are very restricting (and unnecessary).\nAdditionally, the assumption of \\theta_\\star = 0 is not exactly WLOG.\n\n3) The related work is not well cited. Examples: \n\n 3a) \"Instead of using the full gradients, the variants of SGD...\"\n  The citations for SGD here are a bit confusing: Robbins and Monro never talked about SGD; Duchi et al is not about standard SGD, and so on. Better references are [1, 2].\n\n 3b) \"The sampling variance and the slow convergence of SGD have been studied extensively\nin the past (Robbins & Monro, 1951; Polyak & Juditsky, 1992; Bottou, 2010).\"\nNone of this paper studies sampling variance of SGD. RM (1951) only study convergence of stochastic approximation. PJ (1992) is about iterate averaging. Bottou (2010) is also not about sampling variance, and only covers convergence on a high-level. \nLook at [4] for the sampling variance of SGD procedures; also [5, 6].\n\n3c) \"Our main analysis tool is very closely related to recent\nwork studying the dynamics of gradient-based stochastic methods.\"\nMisses important prior work in stochastic approximation dynamics.\nLook at [7].\n\n\n[1] Zhang, \"Solving large scale linear prediction problems using gradient descent algorithms\" (2004)\n[2] Bottou, \"Large-Scale Machine Learning with Stochastic Gradient Descent\" (2010)\n[3] Amari, \"Natural gradient works efficiently in learning\" (1998)\n[4] Toulis and Airoldi, \"Asymptotic and finite-sample properties of estimators\nbased on stochastic gradients\" (2017)\n[5] Li et al, \"Statistical inference using SGD\" (2017)\n[6] Chen et al, \"Statistical Inference for Model Parameters in Stochastic Gradient Descent\" (2016)\n[7] Kushner and Yin, \" Stochastic approximation and recursive algorithms and applications\" (2003)"}