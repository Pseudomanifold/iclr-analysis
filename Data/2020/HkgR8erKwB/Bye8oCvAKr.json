{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper offers PAC-generalization bounds for Bayesian Neural Networks based on a previous result by Alquier et al. (Theorem 1) which connects the generalization gap to the log partition function of the same gap for the prior distribution on the learned parameters (which is identical to the ELBO bound used in Bayesian neural networks for NLL loss). Due to the fact that the optimal bound occurs for the true posterior, the PAC-bayesian bounds offer a novel interpretation as an objective for BNNs.\n\nThe authors note that the log partition function can in general be easily unbounded for loss functions based on NLL (as in the BNN case); their result shows that if the norm of the gradient is bounded, that is enough to bound the overall generalization gap. \n\nWhile this appears to be a technically impressive feat, the the assumptions involved in Theorem 2 seem significant (probably unavoidable for a theoretically tractable statement). Primarily, the conditional of x given y is Gaussian/log-concave (or at least unimodal, more generally ) but the motivation is based on deep neural networks (for why the gradient is bounded).  \n\nThe authors also specialize their bound to the case of logistic regression. Interestingly, the gap in this case has an additive term proportional to the product of the label cardinality and the input dimension (I'm not sure whether how significant this is in terms of tightness).\n\n In experiments, the authors explore and analyze the tightness of the proposed bounds for various hyperparameters like the variance of the weights prior.\n\nThey also perform an exhaustive comparison of the BNN models against non-bayesian alternatives, but it is not clear how the new contributions from the generalization bounds are relevant to the results in, say Section 6.2"}