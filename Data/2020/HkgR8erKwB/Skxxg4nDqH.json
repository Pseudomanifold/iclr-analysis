{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper suggests a PAC-Bayesian bound for negative log-likelihood loss function. Many PAC-Bayesian bounds are provided for bounded loss functions but as authors point out, Alquier et al. (2016) and Germain et al. (2016) extend them to  unbounded loss functions. I have two major concerns regarding this paper:\n\n1- Technical contribution: Since Alquier et al. (2016) has already introduced PAC-Bayesian bounds for the hinge-loss, I think the technical contributions of this paper is not significant enough for the publication. Moreover, the particular format of the bound in Theorem 2 is problematic since the right hand side depends on the data-distribution. When presenting the generalization bound, we really want the right hand side to be independent of the distribution (given the training set) and that is the whole point of calculating the generalization bounds. In particular, I don't see why inequality (1) is any better than inequality (2).\n\n2- Experiments: The main issue with the correlation analysis done in Section 6 is that authors only change depth of the networks and then check the correlation of the generalization bound to the test error. The problem is that in all those networks deeper ones generalize better so it is not clear that the correlation is due to a direct relationship to generalization or a direct relationship to depth. For example, if we take 1/depth as a measure, it would correlate very well with generalization in all these experiments but 1/depth is definitely not the right complexity measure or generalization bound. To improve the evaluation, I suggest varying more hyperparameters to avoid the above issue."}