{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed a pre-training method for strengthening the contextual embeddings alignment. Given parallel sentences from a different language, the authors proposed to enforce corresponding words that have a similar representation by minimizing the squared error loss. The authors also proposed to use the an regulation that prevents the learned embedding from drift too far. The authors evaluated the proposed pre-training on the contextual alignment metric and show the BERT has variable accuracy depends on the language. The proposed method improved significantly on zero-shot XNLI compares to the base model. \n\nThe paper is well written, and the proposed aligned loss makes sense and should augment the multi-lingual pre-training from a high level. The authors did a good job of analyzing the bert for multi-lingual. There some details may help the reader understand the paper better\n\n1: Why use L2 distance as the metric function, what is the performance of using the inner product as a metric function? and what is the difference here? \n\n2: The authors mentioned the word pairs are extracted from the existing method which may be noisy. I wonder is there any ablations study with respect to how the word pairs affect the pretraining? \n\n3: When finetuning on zero-shot transfer, what is the finetune setting? Is there any strategy to avoid the lower layer embedding from drifting away? \n\n4: In table 3, the Fully supervised Base Bert on English is close to the zero-shot setting and the base BERT model is better than Alignment bert, I wonder can the authors explain more on this? \n\n"}