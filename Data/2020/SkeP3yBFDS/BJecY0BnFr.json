{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper attempts to reduce computation in recurrent neural networks. Instead of artificially determining the update pattern for updating the states, the authors propose SA-RNN to predict discrete update patterns automatically through optimization driven entirely by the input data. Experiments on publicly-available datasets show that the proposed method has competitive performance with even fewer updates.\n\nPros:\nOverall, I think the idea of this paper is clear and the whole paper is easy to follow. The experiments clearly show the advantage of the proposed method claimed by the authors.\n\nCons:\n1.\tSome expressions need to be improved. For example, in \u201cThis way, representations can be learned while solving a sequential learning task while minimizing the number of updates, subsequently reducing compute time.\u201d two \u201cwhile\u201ds are not elegant and there should be an \u201cIn\u201d before \u201cthis way\u201d. In \u201cWe augment an RNN with an update coordinator that adaptively controls the coordinate directions in which to update the hidden state on the fly\u201d, the usage of \u201cin which to\u201d is not right. I suggest the authors to thoroughly proofread the whole paper and improve the presentation.\n2.\tSince this paper focuses on the efficiency of RNN, I suggest the authors could provide the time complexity comparisons. Merely the comparisons on skip of neurons cannot show the advantage on the efficiency.\n"}