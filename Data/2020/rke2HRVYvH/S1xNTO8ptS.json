{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors propose Stochastic Prototype Embeddings, as a probabilistic extension of Prototype Networks (snell et al, 2017) for few-shot classification. Authors claim their method leads to better few-shot results in the case of corrupted data, and support their claims with a fair number of results over N-Mnist, with 'Hedge Instance Embeddings.\n\n\nI think overall is a good paper as it contains interesting new ideas and represents thorough work, including their empirical validation. However, I believe it is not strong enough for me to recommend acceptance at this point. I hope the authors will address my concerns\n\n1)The derivation of their probabilistic method is not satisfactorily explained from a statistical perspective. Why do they appeal to a product distribution? the kind of formulae the authors obtain (e.g. equation 4) resembles the usual ones for posterior distributions over gaussians. Instead of talking about a 'product distribution' it would be much better if the authors appealed to statistical principles so that their choices reveal themselves as sensible or natural. Additionally, their \"intersection sampling\" seems as rebranding of usual importance sampling. I hope the authors will comment more on the originality of their approach, and if not original, downplay the contribution of the sampler.\n\n2)Although results are solid, but when going through the results section I got the impressions the authors were not clear about what were their ultimate intentions, what they wanted to prove. Key results are presented in Figure 6 but they led me with the following questions that I hope the authors will be able to better respond. Is the main preoccupation about few(or zero)-shot learning? then, is HIB the proper baseline in figure 6 and 7? My concern comes from the fact that (to my understanding) HIB isn't stated in the context of few shot learning so it seems authors are defeating a straw man.  uthors may compare with a more naive baseline for uncertainty modeling. Alternatively, authors may compare with HIB in non-few shot regimes. In any case, I hope the authors will make this point clear."}