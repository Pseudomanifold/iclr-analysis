{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This study proposed a heuristic approach to decide whether to use incremental training or full training of a pre-trained model with the availability of new data. The idea is that if the new data is consistent with the previous model (therefore, incremental learning achieves a bounded error), one can resort to incremental learning; otherwise, a full re-training is required. To assess this need, the authors proposed to use reservoir sampling to select data for incremental training.\n\nThe idea is intuitive, but it is not supported well by the authors, as no theoretical proof was presented, the results don't show a clear advantage of this method, and it is contradictory with previous results (e.g., that of Golmant et al., 2017). Also, the paper requires more experiments to support the claims in the abstract:\n1- Speed up for different tasks should be shown. Also, an analysis of the worst-case scenario (e.g., where the distribution of the samples changes by the time (non-stationary situation) and full training is required for each scenario) is required.\n2- Catastrophic forgetting was claimed to be avoided in the abstract, while the authors later stated that they try to reduce it using reservoir sampling. Also, it should be measured using metrics introduced in Kemker et al. 2018 to show that whether the idea is really achieving its goal or not.\n3- The threshold for having a bounded error also plays a significant role in the performance of the system. An experiment is required to investigate the effect of this parameter on the speed of the system, and the results should be analyzed and discussed.\nFurthermore, since the results are somehow contradictory to the previous task, more experiments on other tasks (e.g., NER, sentiment analysis) or domains (computer vision, time series such as stock data) should be conducted to draw a meaningful conclusion from the paper.\n\nAs a side note, the authors are encouraged to proofread and rewrite some parts of the paper for the next submission (e.g. \"Approaches to saving on training...\", \"we discuss our approach to informing...\"), better use the \\cite{} and \\citep{} commands of Latex, and present more readable figures (e.g. by including meaningful legends)."}