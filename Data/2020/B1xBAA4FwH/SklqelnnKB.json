{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary\n---\n\n(motivation)\nThere are lots of heat map/saliency/visual explanation approaches that try to deep image classifiers more interpretable.\nIt's hard to tell which ones are good, so we need better ways of evaluating explanations.\nThis paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence.\n\n(approach - correctness)\nAn explanation is correct if it highlights enough of an image for a classifier to tell the correct class with only the highlight parts of the image.\nThe default way to evaluate on only highlighted portions is to set the non-highlighted bckground to black/grey.\nInstead, this method finds images with the same ground truth class which the classifier scored the lowest of all such images, forming a low-confidence baseline.\nIt copies the background from one of these images instead of using a black/grey background\nto try and put the masked image back into the distribution of images from the ground truth class.\nThis style of masking is used to compute correctness.\n\n(approach - consistency)\nAn explanation is consistent if it is invariance w.r.t. a number of mostly semantically invariant transformations.\nThese include small affine transformations, horizontal flips, vertical flips, and adding noise.\n\n(approach - confidence)\nAn explanation is confident if the masked images it produces still have high condidence under the classifier.\nMasked images are produced as for correctness, by copying a distractor from the same class into the background.\n\n(experiments)\nThe experiments compare existing explanations (LIME, Grad-CAM, Integrated Gradients, SmoothGrad) using the proposed metrics.\n1. Correctness: Classifiers have higher accuracy on explanation-masked images than on images they were least confident on (the ones used to fill in the background).\n2. Grad-CAM is most correct, followed by SmoothGrad, Integrated Gradients, and LIME.\n3. Consistency: Grad-CAM explanations are most resilient to the proposed transformations with Integrated Gradients, SmoothGrad, and LIME being successively less invariant.\n4. Confidence: Explanation-masked images have higher scores for their ground truth class than the low-confidence baseline images.\n5. Hyperparameter variations in the correcness/confidence metrics mostly preserve the ranking of methods, though the absolute values of performance do change substantially.\n\n(conclusion)\nThe paper concludes that Grad-CAM is usually the best of the methods tested according to the new metrics and that LIME is the worst.\n\n\nStrengths\n---\n\nI really like the related work section. It could be a valuable resource going forward.\n\nI like the research direction of this paper very much. I think that enumerating a suite of complementary benchmarks is a good way to measure explanation quality because we can only come up with benchmarks that capture a small part of what we want so far.\n\n\nWeaknesses\n---\n\n\nI see some major conceptual flaws with these metrics:\n\n* In section 3.1 it seems like the first reasons that normal masking failed is not solved by the proposed approach. The generated images are still out of distribution because the \"foreground\" and the \"background\" don't match.\n\n* I'm concerned about the low-confidence distractor images used in the background. They are from the same ground truth class as the high confidence images they are pasted into the background of, correct? The correctness metric is supposed to capture whether or not an explanation highlights all the class-relevant content in an image and no more. However, information that the explanation did not highlight (the background) can inform the classifier of the ground truth class because the background came from an image of that class (even if a low confidence one). This is especially true because the relevant objects might be in differrent positions in the two images. Thus it could be that the explanation did not highlight informative content but the classifier still gets the corresponding masked image correct because of the background. How often does this happen?\n\n* Consistency is supposed to measure \"the ability of the explainer to capture the relevant components\" under semantically invariant transformations.\nThe reported metric is mimized when the explanation is the same before and after a variety of transformations.\nIf this were the case then at least one of them must be wrong in the sense that it would not have captured some relevant components\n(unless perhaps it just highlighted everything and was thus useless).\nBecause of the transformation (e.g. 15 degree rotation) the relevant components would have been at a different position, but the best explanation according\nto this metric would have been at the same position. Thus this metric seems to reward explanations for not capturing relevant components.\n\n\nParts I Didn't Understand:\n\n* In section 3.1, I don't understand the second reason that masking failed. In what sense is masking made meaningless? How is that sense different from the out of distribution concern from the first point?\n\n\nMissing Details / Presentation Weaknesses:\n\n* Missing reference to [1] which provides more metrics.\n\n* The meaning of confidence is different than it normally is and this may be confusing.\nNeural networks should be well calibrated, not necessarily confident (in the commonly used sense of [3]).\n\n\nMinor flaws:\n\n* Masking by replacing the background with grey (i.e., the bias of the first conv layer) rather than black is more common (e.g., [2] and Grad-CAM). A grey background negates the bias. It's not clear that the background should cancel the bias, but it would be nice to compare to both grey and black masking in Table 7.\n\n\n[1]: Adebayo, Julius et al. \u201cSanity Checks for Saliency Maps.\u201d NeurIPS (2018).\n[2]: Zeiler, Matthew D. and Rob Fergus. \u201cVisualizing and Understanding Convolutional Networks.\u201d ECCV (2013).\n[3]: Guo, Chuan et al. \u201cOn Calibration of Modern Neural Networks.\u201d ICML (2017).\n\n\nFinal Evaluation\n---\n\nThis paper relies solely on theoretical arguments to show its metrics capture meaningful information. Empirically, it only shows that the proposed metrics can differentiate between some popular explanations. It does not empirically show that the differentiation is meaningful (e.g., by measuring agreement with human judgement). This by itself isn't a problem. However, above I detailed significant flaws in the theoretical justification for the metrics, so I can't recommend these metrics (this paper) on either a theoretical or an empirical basis.\n\nQuality: Per above, I do not think the arguments/evidence in the paper support its conclusions.\nClarity: The paper could be clearer, but can be understood without too much effort.\nOriginality: These metrics are new enough, being novel variations on prior approaches.\nSignificance: If I was convinced the metrics made sense then I would guess this paper would be very impactful. As is, I don't think it will have much impact.\n\nThe quality of the paper is my reason for the low rating. I'm interested to see whether what others think to make sure I've understood the paper correctly and analyzed it accurately. If my understanding is incorrect I could definitely raise my rating.\n"}