{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a novel GAN-based method to tackle PML problem. By using Encoder-Decoder architecture, the authors combine four neural networks including disambiguator, predictor, generator, and discriminator to implement the integrated model and get an overall good performance in various experiment settings and datasets.\n\nPros:\n- It's novel to introduce GAN to solve the PML problem.\n-The model works better than other state-of-art models overall.\n\nCons:\n- The novelty of this paper is limited to some extent. It seems that the model just combines ideas of GAN and PML. \n- I wonder if GAN still works in this architecture in some situations. Under the condition when the dimension of labels is small enough, it will be hard to generate good samples because the information which can be utilized for the generator mightn't be sufficient. However, when the dimension of labels is large, there will be some combinations of labels that aren't in the training set, which may harm the performance of the generator.\n- Because the relations between instances and labels aren't one-to-one in most cases, I wonder whether the five-layer perceptron still works well as a generator if one setting of labels can correspond to various kinds of instances.\n- It will be better to compare the generator part proposed in this paper with a simple interpolation method in the process of extending the dataset."}