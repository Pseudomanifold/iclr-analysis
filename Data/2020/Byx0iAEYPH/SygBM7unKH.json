{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary:  The paper studies the problem of global optimization of high-dimensional sparse estimators regularized by PCP (folded concave penalty). The main result is showing that under certain conditions, with high probability, the desired global solution is an oracle stationary point satisfying the so-called S^3ONC conditions. In light of this result and an existing polynomial-time algorithm for finding an S^3ONC solution, the global solution can be recovered with polynomial computational complexity. Numerical evidence is provided to show the theoretical predictions. \n\nStrong points:\n\n-S1. The global optimization of non-convex (regularized) sparse learning models is a fundamental and challenging problem worth investigating.\n\n-S2. The paper is well organized and clearly presented in general.  \n\nWeak points:\n\n-W1. The loss function in Equation (1) is lacking in explanation. It is claimed that \u201ctraditional statistical learning schemes often resort to\u201d such a formulation. However, it looks like only the logistic loss explicitly admits such a form while some other widely used ones such as squared/hinge/exponential loss do not. So are there any concrete statistical learning models other than logistic loss falling exactly into this framework? It could be beneficial to provide a reference, if any, to this particular problem formulation in machine learning literature. \n\n-W2. The nominal generative model is not clearly defined. Usually, a key component of high-dimensional statistical analysis is to define a nominal statistical model for data generalization. I note the parameter vector of such a model is denoted as $\\beta^{true}$ in this paper. However, the related data generation procedure seems completely missing in the statement of problem setups. Particularly, what\u2019s the definition of the residuals W appeared in the assumption A2? \n\n-W3. The overall novelty of theory is limited. The main result established in Theorem looks closely related to the excess risk bounds established in [Liu & Ye 2019]. Actually, provided that the risk function $L(\\beta)$ has (restricted) strong convexity in $\\beta$, it follows immediately based on the first excess bound in Theorem 1 of [Liu & Ye 2019] that $\\|\\beta^* - \\beta^{true}\\|$ is well bounded from above. Thus if the minimum non-zero absolute value of $\\beta^{true}$ is sufficiently larger than that upper bound, then it is naturally true that $\\beta^*$ shares the same supporting set as $\\beta^{true}$ which in turn implies that $\\beta^*$ is an oracle solution and also is globally optimal under some more stringent conditions. Unless the author(s) can justify the value-added beyond the results in [Liu & Ye 2019] as sufficient, the degree of novelty of the current theory seems fairly low given that prior work.  \n\n-W4. The proof of Theorem 1 has flaws. The proof of main result relies largely on Lemma 5 which basically bounds the cardinality $\\|\\beta^*-\\beta^{true}\\|_0$. However, when invoking Lemma 5, such a $L_0$ bound shifts to an $L_2$-norm one. This misleading point needs to be clarified.\n\n-W5. The numerical study can be improved. The majority of the reported results is about the advantage of FCP over Lasso and ridge regression, which however is less relevant to the global recovery theory developed in the paper. I think the numerical study needs to be re-designed to put more focus on the effects of some key factors, e.g , the sample size n and the signal strength $\\beta_\\min$, on the global optimization performance. \n\n-W6. Concerning the fitness to venue, although somewhat relevant,  I am not sure the main topic of this paper (i.e., statistical analysis of high-dimensional GLM) would gain significant interests in the community of ICLR. Actually in my opinion, the novelty/importance of this work is more suitable to be evaluated in a high-dimensional learning theory intensive journal or conference rather than in a DL/RL conference.  \n\nMinor issues: \n\n- M1. Check the correctness of notation $\\beta$ in Definition 1. \n\n- M2. Equation (3): Why not directly writing out \u201c$0 \\in 1/n\\sum_{i=1}^n\u2026$\u201d?\n\n- M3. Statement of Theorem 1: satisfy -> satisfies. The quantities $t$ and $t\u2019$ appeared in Theorem1 are hard to understand without further explanation. "}