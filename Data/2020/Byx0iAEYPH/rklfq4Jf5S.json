{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=Summary=\nThe authors study the high-dimensional sparse estimation problems, which is one of the fundamental topics in both machine learning and optimization communities. In the literature, the folded concave penalty (FCP) methods have been shown to enjoy the strong oracle property for high-dimensional sparse estimation.  While LASSO solutions can be easily computed, such solutions do not admit unbiasedness and oracle property or require strong conditions. Therefore,  the problems with FCP have received much attentions and been studied in terms of hardness and approximability of the problems recently .\nIn the paper, the authors focus on the minimax concave penalty (MCP) that is a special class of FCP. \nInspire of the problem being NP-hard, they proposed pseudo-polynomial time algorithms and showed the global optimality; using the characterization of a significant subspace second-order necessary condition (S^3ONC), they showed that all local solutions within an efficiently achievable sub-level set are globally optimal. They also investigates the empirical evaluation of proposed methods.\n\n\n\n=Significance=\nIn the technical view,  the part of (a) in Theorem 1 is very similar to Theorem 4 in [Hongcheng Liu, Tao Yao, Runze Li, and Yinyu Ye (Mathematical programming, 2017)]. The main result of this paper may be the part of (b) on Theorem 1. \nHowever, the motivation why the authors wish to compute global optimization is not clear to me.\nIndeed,  for FCP,  [Liu et al, 2017] showed that\n1. Any local solution is a sparse estimator.\n2. Any local solution satisfying a significant subspace second-order necessary condition yields a bounded error in approximating the true parameter with high probability\n3. For MCP (with restricted eigenvalue condition), S^3ONC solution has oracle property.\nTherefore, the global optimality is not necessarily stipulated to ensure the recovery quality.\nI would like to confirm the motivation of this work.\n\n=Missing references=\nAlthough the topic addressed in the paper is related to the complexity of the sparse estimation problems, the following papers are not included in the references.\n\nHuo, X. and Chen, J. Complexity of penalized likelihood estimation. Journal of Statistical Computation and Simulation, 80(7):747\u2013759, 2010.\n\nBian, W. and Chen, X. Optimality conditions and complexity for non-lipschitz constrained optimization problems.\n\nChen, X., Ge, D., Wang, Z., and Ye, Y. Complexity of unconstrained L2 \u2212 Lp minimization. Mathematical Programming, 143(1-2):371\u2013383, 2014.\n\n\n\n=Questions or comments=\n\n1. Time complexity:\nWhy and how the running time of the proposed algorithm can be polynomial time from pseudo-polynomial time?\nWhere is the time complexity of the proposed algorithm explicitly presented in the paper?\nAt least, the definitions of FPTAS and FPRAS and other technical terms of the complexity are missing in the paper. If the author wants to publish the paper in this conference, the definitions of them should be written in the paper, since many of readers in machine learning areas may not know the exact definitions.\n\n2. Can the results presented in the paper extend to the case for SCAD? Since SCAD and MCP are shared with similar properties, I\u2019d like to see more discussion on SCAD case. \n\n3. About the problem setting: \nAll the inputs and parameters are assumed to be rational number?\nAre there any assumption on the loss function?\n\n4. After the definition 1,  \u201csome independence assumptions \u201c or  \u201cthe same setting\u201d are unclear. Please clarify the assumptions or setting to be self contained.\n\n\n\n=Minor comments=\n\nAbout the title: If the paper did not provide any results on SCAD, it is better to use the term \"the minimax concave penalized\" not \"folded concave penalized\"\n\n(Page 3: \u201cThe oracle solution is a hypothetical assumes the prior knowledge on the true support set\u2026\u201d is grammatically wrong?)\n\nRemark 1: remove  one \u201cgradient\"\n\nRemark 4: S^3ONC\n\nReferences: Cun-Hui Zhang, Tong Zhang, et al. \n-> add \"and\", remove \"et al.\"\n\nReferences: Cun-Hui Zhang et al. => remove et al.\n\nPage 18:  delete the memo \u201c move to intro somehwere?\u201d\n\n"}