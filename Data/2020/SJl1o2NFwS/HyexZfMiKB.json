{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper points out a formal analogy between transformers and an ODE modelling multi-particle convection (the feed-forward network) and diffusion (the self-attention head). The paper then adapts the Strang-Marchuk splitting scheme for solving ODEs to construct a slightly different transformer architecture: \u201cFFN of Attention of FFN\u201d, instead of \u201cFFN of Attention\u201d. The new architecture, refered to as a Macaron-Net, yields better performance in a variety of experiments.\n\nPROs\n1. The proposed new architecture is fairly simple.\n2. The experimental results are fairly good.\n\nCONs\n1. Introducing two feedforward layers with *different* parameters W^{down} and W^{up} is a significant deviation from Strang-Marchuk splitting. I expected the two FFNs inside the Macaron to have the same weights. As I understand it, the motivation for the splitting is to improve the numerical performance of the update scheme for the ODE. In contrast, allowing different weights for the FFNs means the \u201cphysical process\u201d is now a lot more \u201cfree\u201d. Is there any \u201cphysical\u201d motivation for the different parameters? (Beyond the fact that it improves performance). How much worse is empirical performance when the parameters are the same?\n\n2. Following on from the above point, the analogy between the multi-particle system and the transformer is quite weak. The equations look similar when you squint the right way. But that\u2019s as far as it goes. Fig 1 is a nice visualization, but it doesn\u2019t provide insight into the dynamics of transformers. What does \u201cParticles move in the space along time (Semantics encoded in stacked neural network layers)\u201d mean? How do the particles connect to the semantics? \n3. The proof of Bobylev & Ohwada\u2019s theorem is included in the paper. Is there any connection between the theorem (or the techniques used in its proof) and transformers? I suspect the answer is no.\n\nSUMMARY\nIn short, the paper (i) proposes two FFN layers instead of one in each block of the transformer and (ii) shows it performs slightly better than before. This is decent, but in my opinion not enough the clear the bar for ICLR.\n\nThe connection to multi-particle ODEs is genuinely interesting. However, it is not sufficiently fleshed out to count as a contribution (yet). It\u2019s possible the authors have discovered something deep. It\u2019s also possible they got lucky with a physically motivated modification of transformers that actually has nothing to do with the dynamics of multi-particle systems. I\u2019m not sure what further experiments would be needed to make the case. But I recommend the authors dig into the equations and the dynamics to see what it really going on under the hood. Just showing improved performance on a few benchmarks is not enough to convince the connection is solid. \n"}