{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Through the lens of Distributional Robust Risk (DRR), this work draws a link between adversarial robustness and Lipschitz constant regularisation. The authors first provide an upper bound of the DRR (with a Wasserstein ball as the ambiguity set) in terms of the true risk and the Lipschitz constant of the loss function under the current model. They show that the standard adversarial risk can be upper bounded by the DRR, emphasizing that the Lipschitz constant regularised loss can be used as a proxy for adversarially robust training.\n\nThe authors then apply this idea to kernel methods and aim to minimise the true risk under a Lipschitz constant constraint. By using a bound between the Lipschitz constant and the largest eigenvalue of the Gram matrix, and by approximating the Gram matrix using Nystrom approximation, they express the Lipschitz constant constraint as a convex constraint. In the case of multiplicative kernels, this approach is shown to be efficient when only using a polynomial number of sample points for the Nystrom approximation.\n\nThis method is tested on 3 standard datasets and is shown to outperform state-of-the-art deep learning based approaches over a wide range of perturbation in both l_2 and l_infinity norm. \n\nI have concerns about the novelty of the two main contributions of this work, which are Theorems 1 and 2:\n* Theorem 1 is a direct implication of Kantorovich duality, well known in optimal transport.\n* Theorem 2 is a rewriting of Proposition 3.1 in [1], which is not cited.\n\nMy second concern is that the value of n (number of sample points used in Nystrom approximation) has not been specified for the presented experiments (unless I missed it). This is an important parameter to specify since the algorithm requires to invert an n by n matrix when applying Nystrom approximation, and scalability is usually the main criticism to kernel methods.\n\nWhen comparing the methods, would it be possible to estimate the Lipschitz constants of the models trained by the various algorithms ? This would enable to observe the true effect of the methods on the Lipschitz constant.\n\nDue to these two concerns above, I tend to reject this submission.\n\nA few comments about clarity:\n\n* It would help to provide the full kernel method algorithm (with more detail than Algorithm 1 in Appendix), e.g., explicit the convex constraint on gamma when training the SVM.\n\n* epsilon notation is used both for the accuracy in the Lipschitz constraint (as in Theorem 3) and the adversarial perturbation (as in equation 3 or Figures 4 and 5). In page 7, paragraph 'Attacks', delta is used once for denoting the perturbation, but epsilon is reused just after. More consistency in the notation would be greatly appreciated.\n\n* It is not clear in the main text that L is a bound on the gradient norms at sample points. It is made clearer in the appendix, so one sentence from the appendix should be added to the main text.\n\n\nTypos:\n* p.6 top: \"and analogously for ...(x_1^a,.)\" instead of '(x_1^b,.)'\n\n[1] Distributionally Robust Deep Learning as a Generalization of Adversarial Training, M. Staib, S. Jegelka, NIPS, 2017"}