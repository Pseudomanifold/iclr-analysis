{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary\n\n This paper looks at the semi-supervised learning problem, where a combination of labeled and unlabeled data is available. It builds off of two major ideas which follow the general trend of perturbing unlabeled data and applying a consistency loss between the original and perturbed outputs: 1) virtual adversarial training (VAT) which picks an adversarial direction for perturbation, and 2) mixup which interpolates between images or features of real data. The method specifically pertubs unlabeled examples by interpolating between itself and the adversarial example. Results are shown on synthetic and standard semi-supervised image datasets. \n\n  Overall, my decision is to reject this paper. The idea is an extremely simple combination of two prior methods, with little justification for why it is the correct thing to do. Indeed the ablation shows that other methods work similarly (ICT+VAT) and newer papers such as MixMatch (which was out in May and accepted to NeurIPS) perform better than this method with normal augmentations and mixup (with multi-view pseudolabeling), showing that the underlying thesis of the work is not necessarily the most important part of the semi-supervised learning problem. \n\nArgument: \n\n  - The set of justifications put forth for the method are not rigorous. While I agree that combining multiple methods such that a larger space of augmentations is used (\"our AdvMixup explores a more comprehensive searching area\"), it is not clear to me why *this* particular combination is more justified than any others one can come up with (mixup between PI perturbation, random augmentations, etc.). In the end, various things are tried and some work better than others. Indeed ICT+VAT already does very well. It's not clear to me that this paper therefore discovers anything novel in terms of generalizable information that can be used for other problems (for example). \n\n  - MixMatch performs better than this method, and has been out since May and accepted at NeurIPS 2019. I would not say it is concurrent given that it has been out for a while now. Further, it shows that the underlying thesis of the method in this paper is not necessarily the crux of the semi-supervised learning problem. \n\n  - VAT is known to be extremely sensitive in terms of its hyperparameters (e.g. epsilon), which has to be tuned per-dataset (you use the value from the paper, which did this tuning to a larger validation set). Does this method offer any decreased sensitivity to that?\n\n  - I would not say VAT has a low computational overhead; you need an additional forward and backward pass per batch, which is not insignificant.\n\nSome specific questions:\n\n  - Why do you use 5k validation examples for cifar-10 and what size labeled data does that correspond to (1k, 2k, etc)? This seems to go against the Oliver et al. recommendations (which you cite).\n"}