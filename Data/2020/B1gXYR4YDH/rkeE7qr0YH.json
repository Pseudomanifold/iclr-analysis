{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is aimed at tackling a general issue in NLP: Hard-negative training data (negative but very similar to positive) can easily confuse standard NLP model. To solve this problem, the authors first applied distant supervision technique to harvest hard-negative training examples and then transform the original task to a multi-task learning problem by splitting the original labels to positive, hard-negative, and easy-negative examples. The authors consider using 3 different objective functions: L1, the original cross entropy loss; L2, capturing the shared features in positive and hard-negative examples as regularizer of L1 by introducing a new label z; L3, a three-class classification objective using softmax.\nThis authors evaluted their approach on two tasks: Text Classification and Sequence Labeling. This implementation showed improvement of performance on both tasks.\n\nStrenghts:\n+ the paper proposes a reasonable way to try to improve accuracy by identifying hard-negative examples\n+ the paper is well written, but it would benefit from another round of proofreading for grammar and clarity\n\nWeaknesses:\n- performance of the proposed method highly depends on labels of hard-negative examples. The paper lacks insight about a principled way to label such examples, the costs associated with such labeling, and impacts of the labeling quality on accuracy. The experiments are not making a convincing case that similar improvements could be obtained on a larger class of problems.\n- The objective function L3 is not well justified.\n- It would be important to see if the proposed method is also beneficial with the state of the art neural networks on the two applications. \n- Table 3 (text classification result) does not list baselines."}