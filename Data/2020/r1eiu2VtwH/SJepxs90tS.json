{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Paper Summary:\n\nThe paper considers training oblivious trees ensemble with gradient descent by introducing a relaxation for feature selection and node thresholding. The relaxation is based on the recently introduced EntMax. The approach is compared with standard gradient boosting tree learning on benchmark datasets.\n\nReview Summary:\n\nThe paper reads well, is technically sound. The approach is novel and relevant to ICLR. Reference to related work are appropriate. Experimental comparison with CatBoost, neural nets could be more rigorous, more ablations could give a complete picture. Overall this is a good paper that gives an extra tool applicable to many practical settings.\n\nDetailed Review:\n\nThe introduction needs to define \"tabular data\". In your case, it seems that you mean mostly numerical heterogeneous features. Could you comment on using categorical features as well? \n\nThe method is clearly explained and references are appropriate, so most of my questions relate to the empirical setup and results.\n\nFirst, it seems to me that the paper would be much stronger if you were to reproduce the results from an established paper. If you take the catboost paper (arXiv:1706.09516v5 [cs.LG] 20 Jan 2019), the error on epsilon dataset is 10.9 which is better than the number your report, similarly click reports 15.6 error rate. To me, the paper would be much better if you simply added an FCNN and a NODE column to Table 2 and 3 of the catboost paper. It does not mean that your approach has to be better in all cases, but it will give a clear picture of when it is useful and it would clear any doubt on the tuning of the catboost baseline.\n\nSecond, the model you propose builds upon the densenet idea while the FCNN you compare with has no densenet connections. It would be fairer to consider neural net with this kind of residual.\n\nThird, I feel you need to report results over CPU as well. Boosted trees primary advantage is their low cost on regular CPU, the entmax formulation requires integrating over more leaves \nthan typical thresholded trees and it would be interesting to compare the effect on CPU. Reporting timing with batch and individual sample evaluation would make sense as well.\n\n As a side note, I would advise to define entmax with its equation. It is too recent to consider it should be known by the reader.\n\nOverall, this is a good paper than reads well. The method is novel, interesting and practical. With the extra experiments, it would make an excellent ICLR paper."}