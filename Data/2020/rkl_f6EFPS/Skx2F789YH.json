{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\nReview: This paper considers the problem of dropping neurons from a neural network.  In the case where this is done randomly, this corresponds to the widely studied dropout algorithm.  If the goal is to become robust to randomly dropped neurons during evaluation, then it seems sufficient to just train with dropout (there is also a gaussian approximation to dropout using the central limit theorem called \"fast dropout\").  \n\nI think there are two directions I find interesting that are touched on by this paper.  One is the idea of dropping neurons as an adversarial attack, which I think has been studied empirically but not theoretically (to my knowledge).  However then it would be important to specify the budget of the attack - how many neurons can they remove and how precisely can they pick which neuron to remove?  Another would be studying the conditions for dropout to be useful as a regularizer (and not underfit), which are again somewhat understood experimentally but could deserve a more theoretical treatment.  \n\nHowever I don't think this paper solved a sufficiently clear problem and the motivation is somewhat confusing to me, especially when it seems like an analysis of dropout, and that isn't even mentioned until the 7th page.  \n\nNotes: \n  -Paper considers loss of function from loss of a few neurons.  \n\n  -Idea is to study more rigorously the fault tolerance of neural networks to losing a subset of the neurons.  \n\n  -In terms of impact of the work, one thing to consider is that even if a normally or arbitrarily trained network doesn't have perfect fault tolerance to dropping neurons, a neural network *trained* with dropping networks could learn hidden states which become more fault tolerant.  \n\n\nMinor Comments: \n  -I'm a bit unhappy with the argument about the brain losing neurons unless it has better referencing from neuroscience.  I imagine it's true in general but I wouldn't be surprised if some neurons were really essential.  For example squid have a few giant neurons that control propulsion.  It's just the first sentence so maybe I'm nitpicking.  \n\n  -  It also seems weird that the opening of the paper doesn't give more attention to dropout, since it's a well known regularizer and seems rather closely related conceptually.  \n\n  - In the intro it says neuromorphic hardware would pass information at the speed of light.  Is this really true?  My understanding is neuromorphic hardware would still generally use electrical or chemical signals but not pass things at the speed of light.  \n\n  - The citation format is not valid for ICLR. \n"}