{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\t\nThis paper addresses the important problem of molecular structures generation, and more generally of efficient point-cloud distributions learning in d-dimensional space.\nThe paper is written clearly, the pseudo-code is presented in a clear way, striking a good balance between explicit-ness and concision.\n\nAfter my first reading I was disappointed in the experiments, because I read the paper quite quickly at first.  After a second more careful read I understood most of it and was much more enthusiastic. Disclaimer: I am inexperienced in this particular field (GANs and GANs for molecular generation) so I lack literature knowledge, and I may be over-evaluating the quality of performances (compared to recent results).\nBut currently I believe that the authors are a bit too humble about their results (a rather uncommon phenomenon). There is a part of the method that I would like to have clarified (about bond/atom types), but other than this clarification, I strongly recommend the paper to be accepted.\n\nThe paper deals with a couple of distinct, related problems.  One is that of sampling valid (Euclidean) distances matrices (EDM). This is done with algorithm 1.\nThe paper uses these EDM to train a generator G directly in EDM-space, against a Critic network C (the architecture of which is taken from existing literature).\nSome of the output configurations have to be discarded due to incorrect bond types assignment (this is the part that is still obscure to me).\nThe remaining outputs are chemically sensible in terms of bond types/valence/local chemistry/etc, but also, and quite impressively, have reasonable ground state energies !  In this sense, the paper produces samples that may deserve to be added into the QM9 data set (After some data augmentation using DFT or other physics-validated methods of course).\n\n\nI have mainly two requests:\n\n1. The bond type or atom assignment is not clearly explained. I ask further detailed questions below, but overall I think you should attempt to make a pedagogical explanation of how the atom types are assigned/learned, and how some of them are later discarded.\n\n'The  generator produces an additional type vector in a multi-task fashion which is checked against a  constant type reference with a cross-entropy loss.'\nCould you explain this better ? At least in the appendix. Otherwise this very dense sentence remains mysterious. Eq.11 is currently the single occurrence of the H, t, and tref terms.\n\nAbout the validity test using Open Babel. Again, what happens with bond types is not very clear. Are they set in stone at generation time, and then most of these affected types are 'wrong' and are discarded by Open Babel ?\nWhy don't you include this validation at training time (if it's very complicated to do, explain why) ?\nIn any case, please clarify this paragraph, as for now it is cryptic.\n\n2. Part of the paper goal is to achieve learning of point clouds distributions and not about chemistry.  However, discovery of new structures (and their conformations) is in itself a big topic.  I think it would be good to mention a couple of follow ups to your work in the conclusion. For instance, it would seem rather natural to me to include some equivalent of OpenBAbel and Energy-estimates within the learning loop, so that the generator directly generates valid (open-babel wise) and reasonable (energy level-wise) structures.\nBesides the conclusion, you should stress out better the significance of your results with a couple of comments here and there. (I have more precise suggestions below).\n\n\nSome other comments to improve the paper (in clarity or other):\n\nIn algorithm 1, you should precise that G() is your NN-based generator network. At this point of the paper G and z and N_z have never been mentioned. This is a problem when reading the paper for the first time.\nLine 5, you could explicitly precise, 'with (5)'.  Also I am puzzled by the use of the softplus here, but later in page 5 you say g() is softplus for the first three eigenvalues, and set to 0 for all others. Why not already anticipate and say this in section 2 ?\nLine 8, I would have written 'eigenvalues .... of D'  (what is the role of Eq. 1 here?)\n\nAt some point (i.e. around end of section 2), a comment would be welcomed, about whether you may sample the space of EDMs uniformly at random using algorithm 1 (I think you do not, and it is ok, but the question naturally arises and it's not addressed).\n\n\u00b4\u00b4which transforms a prior distribution into a target distribution\u00b4\u00b4\nyou could specify that this prior is the Gaussian N(0,1)^Nz in this case, to better connect with algorithm 1.\n\nAbout equation 6: it would be nice to have some intuitive explanation of what the output values of C(x) mean. They are scalars that represent the opinion of C on the molecule x, so they are the probability that the observed molecule is 'a true molecule x' ?  You should recall that for the inexperienced reader.\n\nStill about Eq.6 : could you quickly provide the motivation for demanding C to be L-Lipschit with  $L \\leq 1$ ?\n\nThe drift term Eq.9 seems to be some sort of regularization, maybe it could be mentioned once in the text for completeness?\n\nUsing Mopac Stewart and figure 4: you could insist more on this result. Up to that point I was very dubious about the usefulness of the whole work, because I would have expected many generated samples to be highly unrealistic, i.e. having huge energies, and so being extremely unstable. Instead here you show it is not the case, and even all your energies lie within the range of observed energies !  This is a very strong result, that is not obvious to expect, and is highly valuable (even after keeping only 7.5% of structures, this is still a strong result.)\nI think this test and the corresponding result should be emphasized more.\n\nYou observe new topological types and complain there are not many. I think ~20 new topologies is not a small number (for so few atoms), and you may be rather proud of it.  What is too bad is that you do not have a tool for differentiating between the very similar conformations (that correspond to thermal fluctuations around a given structure) and the conformations that encode a new structure (which does not necessarily means new topology). \nIf you had this tool, you could enrich figure 3 with a curve showing the new structures (not counting conformational variants).\n\nrefs 2017a and 2017b are the same.\n\n"}