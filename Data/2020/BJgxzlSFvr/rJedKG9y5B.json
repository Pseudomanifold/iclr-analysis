{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use attention mechanism for combining different embeddings of the queries and search results. Besides, a decoder mechanism is used to do listwise ranking for the results. The experiments show that the proposed approach outperforms some classic learning-to-rank baselines.\n\nThis paper is below the bar of acceptance for the following reasons:\n\n1.\tLimited technical contribution: some previous papers have explored the idea of learning attention weights for combining different embeddings, and simply applying this idea to learning-to-rank application does not seem to be a big contribution.\n\n2.\tChoice of datasets: the datasets used in this paper are typically used for tesing classification models rather than ranking models. In these datasets, for each query image/doc, there are many images/docs of the same class that could be considered relevant, which makes the ranking task less challenging. Since the paper focuses on learning-to-rank problem, probably the authors should consider include more datasets dedicated to learning-to-rank problems.\n\n3.\tInsufficient baselines: the baseline methods used in the paper are not very recent (e.g., OASIS, RankSVM and LambdaMart have been proposed for more than 10 years). There have been many neural-network based retrieval/ranking methods proposed in the past 5 years. Hence, the experimental results could be more convincing if the paper include more \n\n4.\tLack of justification for the model architecture: some design choices of the model are not well-motivated/justified. For example, how does the decoder mechanism using multiple states in the model (listwise) help improve the ranking results compared to pairwise ranking? Ablation study could help whether such decoder mechanism help show the usefulness of this module.\n\n5.\tParameter sensitivity study: study on how hyper-parameter values affects the model performance could also help.\n"}