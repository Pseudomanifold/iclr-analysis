{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose to use attention to combine multiple input representations for both query and search results in the learning to rank task. When these representations are embeddings from differentiable functions, they can be jointly learned with the neural network which predicts rankings. A limited set of experiments suggest the proposed approach very mildly outperforms benchmark approaches.\n\nMajor comments\n\nTo the best of my knowledge, this is the first paper to apply attention to the learning to rank problem. However, the main methodological innovation seems to be the use of attention to create and train an ensemble of models; this has been previously explored in the literature (e.g., [Kim et al., ECCV 2018]).\n\nThe paper is also missing important context in that it omits developments in using deep learning for the learning to rank problem (e.g., [Pang et al., CIKM 2017; Ai et al., WWW 2018]). The experimental evaluation does not include any other deep methods; thus, it is not clear if the (very minor) improvement in performance are due to the deep models or the proposed attention approach.\n\nThe datasets used in the experiments are not appropriate for evaluating learning to rank algorithms. A variety of learning to rank datasets are available, and these should be used rather than (or in addition to) the toy datasets considered here. Examples: http://arogozhnikov.github.io/2015/06/26/learning-to-rank-software-datasets.html, http://quickrank.isti.cnr.it/istella-dataset/, https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/, \n\nMinor comments\n\nConcerning Section 3.3, in what sense is SGD used to \u201ccalibrate\u201d the model? It seems as though the authors just mean it is used to \u201ctrain\u201d the model. However, is there some other meaning of calibration (e.g., in the sense of a Brier score) here?\n\nIn Table 1, what is the meaning of a dropout p value of 1? In most deep learning frameworks (e.g., Keras and PyTorch), this would mean all nodes are dropped out.\n\nIn what sense are the \u201c5 randomized runs\u201d for the experiments randomized? Are different train, test splits used? or just different random seeds? or something else?\n\nHow is it that the error rates are higher when using superclasses for evaluation?\n\nTypos, etc.\n\nThe paper has several significant problems with the \u201c\\cite\u201ds and \u201c\\ref\u201ds in the paper. First, the \u201c\\cite\u201ds should presumably be \u201c\\citep\u201ds or something since the references are not set off from the rest of the text. Second, the paper includes references to equation numbers which are not present in the paper, such as \u201cequation (12)\u201d. It seems as the equations are in the paper, but are included in some unnumbered environment (\u201c\\begin{align*}\u201d or some such). This makes it very difficult to track down to which equations the authors intend to refer. Third, the reference numbers to figures and tables in the text is wrong. For example, the text refers to \u201cTables 8 and 9\u201d for 20 newsgroups (at the end of Section 4). Clearly, this is supposed to be Tables 6 and 7. It seems like the authors moved the CIFAR-10 discussion to the appendix but did not update the references in the text.\n\nTables 2 and 4 are exactly the same.\n\nFigure 4 is not referenced in the text.\n\nIt would be helpful to put Figure 2 a bit closer to where it is discussed in the text.\n\nThe references are not consistently formatted.\n\n\u201ccomponents of for each\u201d -> \u201ccomponents for each\u201d\n\nPlease define acronyms like MAP at least once.\n"}