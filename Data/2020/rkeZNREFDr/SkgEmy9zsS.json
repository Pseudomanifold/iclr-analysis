{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #6", "review": "Summary: \n\nThis paper proposes a novel architecture that is able to separate different types of features learned at each layer of a neural network through a gating structure -- features that are sufficiently passed through the network are immediately sent to the final output layer. In addition, they provide reasonable definitions of levels of features, in contrast to the standard \"low\" to \"high\" descriptions. Lastly, in order to make the model more interpretable, they utilize an L0 loss on the gates of each layer to prioritize lower level features being used in the final layer.\n\nSignificance:\n\nAlthough gating is not novel, their use to send kth-level features to the final GLM layer is. Other than that, not much is contributed, as their differentiability trick, as mentioned, has already been done. The motivation to separate different types of features is interesting and definitely an issue that should be studied more. \n\nQuality:\n\nThe paper is easy to follow and nicely written, but with a few minor typo issues:\n\n1. Page 1, refers to appendix A.3 but should be for A.4\n2. Page 2, \"section\" is inconsistently capitalized \n3. Page 6, mentions three commonly used datasets but only mentions MNIST and California Housing. \n4. Page 8, mentions Appendix A.11 for CNN but this section is empty.\n\nIn regards to content quality, a few things stand out that could be improved:\n\n1. A major issue is that the interpretability of features with k > 1 are still not explained -- all we know is that they don't need to be sent further through the network. (i.e. solves the separation issue but leaves gaps in interpretability)\n2. Since the gates themselves can be studied, rather than finding gradients, wouldn't a simpler way to explain the network be to look at which features are passed to the GLM layer? This would especially be helpful in the first layer when looking at the original input features. \n3. Currently it is not clear if the architecture learns when features (l_k) are directly \"useful\" for classification, or if they are just not compatible with the features passed on to the next layer (h_k). \n4. In terms of interpretability, only a few other methods are tested, and gradients are the only way they compare. An exploration of other attribution methods could have further supplemented their claims.\n5. Claims are made about how many layers a certain dataset needs for sufficient classification through heuristic experiments; however they are not thorough enough in terms of ablation to fully make this claim. Width of layers are chosen but not analyzed; how is gating affected by the width of the network? For example, in MNIST, would only 3 layers be needed if the width is increased or decreased? This isn't immediately clear.\n6. Extensiveness of experiments -- I do like the toy dataset as an example, but to show effectiveness of this framework, a larger breadth of datasets could have been used. As an example, in the SENN paper, they utilize breast cancer and COMPAS but these were not tested on this architecture.  In addition, the results from convolutional layers would be much more preferred, since the best performing architectures on large vision datasets such as ImageNet primarily use convolutions.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}