{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a feature leveling technique to improve the self-explaining of deep fully connected neural networks. The authors propose to learn a gated function for each feature dimension for whether to directly send the feature to the final linear layer. The gated function is trained with L0 regularization technique proposed in (Louizos et al., 2017) to encourage more low-level features passed to the final layer. Experimental results on MNIST, California Housing, CIFAR10 show that the proposed method can achieve comparable performance with existing algorithms on sparse neural network training.\n\nQuality:\n\nOverall, the paper is well written with some minor formatting errors. The toy example demonstrates the idea of this paper clearly. However, the novelty of this paper, when compared to NIT, is that the L0 regularization is used to pass the feature to last layer. Considering the self-explaining feature, this work can only explain that some of the input features are suited for final layer, while there is no explanation on the other features since they are used to construct higher level features.\n\nClaririty:\n\nSome parts of this paper are not clear:\n1.\tWhy l_k and h_k has to be disjoint? A feature suited for final classification does not suggest that it can\u2019t be used to construct higher-level feature.\n2.\tIn (4), should not B() be an inverse binary activation function: (1-z)?\n3.\tIs g(.) the Bernoulli distribution?\n4.\tIn section 5.2, why compare the gradients of a specific input example while one can directly look at z_k, the gated function?\n5.\tI assume the fully connected layers have bias term. If so, (4) suggests that the gated location will also be added with a learned bias, which is different than what the paper proposes.\n\nNovelty:\n\nThe novelty of this paper lies in the sparse training objective becomes passing as many lower-level features to final layer as possible instead of zeros out the intermediate weights. However, the key technique, L0 regularization, has been proposed and used as stated in the related work. While the authors state the application L0 to a novel context to select features is different from prior work, the novelty is rather incremental.  \n\nSignificance:\n\nThis work demonstrates that the L0 regularization technique for sparse neural network training can also be applied to learn a skip-layer connection. However, from both novelty, performance, and self-explaining perspectives, this work does not introduce much to the field.\n\n\nPros:\n\n1.\tThe paper is well written.\n2.\tThe toy example showcases the issue that this work tries to tackle.\n3.\tThe experimental results show the comparable performance to existing works.\n\nCons:\n1.\tThe novelty is not sufficient considering the prior works on sparse neural network training.\n2.\tThere are some clarification issues as mentioned before.\n3.\tThe performance is only comparable to existing works.\n4.\tThe self-explaining contribution is not clear since only a few input features can be explained if they are passed to the final layers.\n5.\tThere is no experiment on how \\lambda would affect the resulting network architecture.\n\n\nMinor corrections:\n1.\tFirst paragraph on sec. 5: three datasets: two datasets (or mention it\u2019s in appendix).\n2.\t5.2 compare to NIT: the citations are in wrong format. Also the reference for NIT is corrupted.\n"}