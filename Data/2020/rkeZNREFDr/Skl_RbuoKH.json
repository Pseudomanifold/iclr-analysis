{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a neural network architecture to separate low-level features and high-level features. At the k-th hidden layer, 1) the k-th level features, defined as the set of features that requires k-1 hidden layers to extract, are directly passed to the final GLM layers; 2) the remaining features are further processed in the subsequent layers. This separation is achieved by applying the gating mechanism. The model can be interpreted by the weights associated with each of those k-th level features in the final GLM layer. Experimental results on the MNIST classification dataset and the California Housing regression dataset demonstrate the proposed approach can 1) achieve competitive performance (in terms of classification and regression) compared to the FCNN baseline; and 2) has better interpretability performance.\n\nThis paper is clearly written. The description of the model architecture is easy to follow. The introduction of the related works and background material are well organized.\n\nMy major concern with this work is how to interpret those k-th level features. Since the weights of those k-th level features in the final GLM model are used to indicate the importance of those features, interpreting the meaning of those k-th level features seem necessary. In practice, how to interpret the meaning of those higher-level features? For example, for the California Housing dataset, how to interpret the meaning of those features learned at level 2?\n\nThe interpretation of the MNIST classification examples seems difficult to understand. Compared to inspecting the raw pixels, would it be easier to interpret through learning a few prototypes, similar to the approach described in Alvarez-Melis and Jaakkola NIPS 2018?"}