{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to leverage the depth information for relation prediction, arguing that the depth information benefit the prediction of some predicates. To solve the lack of 3D data, an RGB-to-Depth model is trained on external available dataset and then applied to images from visual relation dataset. In the experiments, they investigate different strategies to extract features from depth maps and the explore effectiveness of depth information by comparing the model that only used depth map as input with those which use RGB information. The comparisons with other methods and ablation studies under both Zero-shot setting and normal setting demonstrate the effectiveness of depth information.\n\n+Strength:\n(1) The motivation is reasonable and what the authors make an attempt to explore is very meaningful. Visual relation especially the spatial relation is not likely to be predicted accurately without 3D information. In other words, it seems that visual relation prediction task will be extended to 3D images rather than staying within 2D images. Thus what the authors do is a good exploration for further extensions.\n(2) Comparisons with previous methods and the results show that the depth information is useful to some extent, but not so obvious.\n(3) The writing of this article is good and it\u2019s very easy to understand.\n\n-Weakness:\n(1) The RGB-to-Depth Network is pretrained on other dataset. Is there any gap when it is used for VG or VRD dataset? \n(2) Although the depth map feature extraction seems to work well, it seems to be a little trivial. Why a CNN, e.g. AlexNet, or VGG, can be used to extract depth features? And why the AlexNet trained from scratch performs better than AlexNet pretrained on RGB images for object detection task and VGG net? If the author can give more explanations, this part will be more insightful.\n(3) From the plot which shows the top 10 percent absolute changes in prediction performance per predicate, the advantage of Depth is not obvious compared with RGB. And Depth does not bring the advantage claimed in Abstract. It\u2019s a little hard to understand why depth information can rectify the prediction of (Tower, taller, trees). To sum up, the qualitative results are not so satisfying.\n(4) In Table 1, what really functions seems to be c_so, v_so, and l_so, while the improvement brought by depth is limited."}