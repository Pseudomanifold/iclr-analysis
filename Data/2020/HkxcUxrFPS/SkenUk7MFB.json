{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\n********* Summary *********\n \nThe paper poses the question of whether depth information is informative for visual relationship prediction using still images. It is intuitive that 3D arrangement of objects in an image can be a useful cue for predicting their relationship. As such it is important to see whether and to what extent depth information complements RGB information for visual relation detection. That is the focus of this paper.\nThe paper proposes to use an off-the-shelf monocular depth estimation networks to augment the available RGB information towards better visual relation detection. For that, it proposes a specific network two-stream structure working on RGB image and (predicted) depth image. The proposed model demonstrates improved results upon state of the art for visual relation prediction.\n \n\n********* Strengths and Weaknesses *********\n \n+ A comprehensive set of tests has been conducted. \n+ Zero-shot prediction results are particularly interesting.\n+ The experiment on ranking the predicate classes based on the change in prediction accuracy before and after using depth information (Figure 4) is interesting and intuitive.\n* The final results improve upon the state-of-the-art, especially on the zero-shot learning regime. However, it seems that the improvement is mainly coming from the new architecture as opposed to the inclusion of the depth information. That is, ours_{c,v,l} brings most of the improvement already the last step to ours_{c,v,l,d} is negligible for non-zero-shot case.\n- Along the same line, it\u2019s possible that this small difference between ours_{c,v,l} and ours_{c,v,l,d} for the standard predicate prediction, can be due to a hyper-parameter optimization that is (only or more thoroughly) done for ours_{c,v,l,d}. The hyper-parameter optimization scheme for different baselines is not described. \n- Given the small difference of ablation levels, the comparison will be stronger if done multiple times and reporting mean and standard deviation of the results.\n- For a fair comparison the visual feature vector v_{so} should be tried as the feature of the union bound box of both subject and object same way as it is done for depth feature vector d_{so}. \n- The paper refers to \u201cOurs-d\u2019_{so}\u201d as a baseline that *only* uses depth information with no image/label information. However, it seems that the region proposals for this feature are coming from the image-based network that uses image information. \n \n- Important related but uncited works:\n(1) [\u201cVisual Relationship Prediction via Label Clustering and Incorporation of Depth Information\u201d ECCV workshops 2018] studies the same question as part of their work.\n \n\n********* Final Decision *********\n \nI do not find the paper passing the acceptance bar mainly due to the following reasons together:\n1) The finding is not surprising since most of the visual relations are either explicitly depth-related (e.g., behind) or are semantically constrained by depth (e.g. riding cannot happen at different depths when the image is taken orthogonal to the rider).\n2) an additional depth dataset is used which provides the model with privileged information. Should it have been the case that depth information were inferred without an additional offline dataset, the results would have been more interesting.\n3) the improvements due to the additional depth network are not significant or conclusive.\n4) there is a prior uncited work with the same research question for effectiveness of depth information in visual relation detection which uses a similar approach.\n \n\n********* Minor points *********\n\n- the code is not available. This is especially important since the paper is outperforming prior works which could be a contribution if reproducible.\n- Section 2.2: is l_{so} concatenation of l_s and l_o?\n- Section 2.2: y_{spo} is defined but never used.\n- Equation 2: why do we have both e_p and f in the exponents? Aren\u2019t they the same?\n- Equation 2:  P is never defined.\n- Page 5: \u201ca fully connected hidden layer of 64, 200, 4096 and 20 neurons\u201d: this amounts to 3 hidden layers.\n- Why VGG network for visual feature and AlexNet for depth features?\n- zero-shot learning results on visual genome is missing\n- training procedure is a bit unclear: the text suggest that the fine tuning and/or learning of the three components might happen separately. It is important to clearly state if they are done in an end-to-end fashion and simultaneously or separately; and why.\n- It\u2019s good to name the method in table 2 in the same fashion as table 1. With the current naming (based on architecture) it is a bit confusing to understand the content without additional cross referencing. For instance AlexNet-BN - Raw seems to correspond to Ours_{c,v,l,d}\n- Figure 4: the frequency represented as different shades of red or blue is really hard to notice especially on a printed paper. The red vs blue color coding is not necessary since the bars going up or down indicate the same quality. So, it might be better to use red/blue for frequency instead (e.g. dark red high frequency to dark blue low frequency)\n- Section 3.2: the AlexNet reference seems wrong, it should be \"ImageNet Classification with Deep Convolutional Neural Networks\" NIPS , 2012\n- The structure of section 3.5 is currently flat while the content seems to be nested (two experiments and two sets of corresponding discussions). It will read better if they are organized into subsections.\n \n \n********* Points of extensions (improvement) *********\n\n- I believe *unsupervised* discovery of depth information for visual relation detection can be an interesting direction since it is not limited to the availability of relevant depth dataset. \n- It is not clearly motivated why one should use two separate networks for depth and RGB inputs in light of the additional complexity. For instance, it is good to discuss what is the advantage of the proposed (computationally more expensive) method over the following two simpler baselines:\n- Faster RCNN is used on RGBD input to produce a single feature vector\n- above case with RGB input but have the Faster RCNN predict the depth map as an auxiliary loss.\n\n\n"}