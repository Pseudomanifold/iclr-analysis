{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Sequence-level intrinsic exploration model for partial observable domains\nThis paper tackles the problem of RL in partially observable domains with sparse rewards. To address the sparse rewards issue, it proposes a sequence level intrinsic novelty model to guide policy learning. The sequence model is based on a dual-LSTM architecture. In general, this paper is well-written as easily accessible. Comprehensive experiments are provided to validate the effectiveness of the proposed methods. \nThe main issue with the paper is lacking discussions regarding the effective of the biased incur by the intrinsic reward. Specifically, \n1)\tHow is the scaling factor beta determined? It would be nice if some discussions or experimental comparisons can be provided. \n2)\tThe paper mainly deals with problems with sparse rewards. I wonder how the proposed method perform will in non-sparse rewards cases.  My main concern is that in the non-sparse reward cases, the intrinsic reward will cause bias, which may not guarantee good final performance.  \n\n"}