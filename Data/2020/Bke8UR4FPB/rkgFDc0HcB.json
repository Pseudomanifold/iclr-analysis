{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes locally constant network (LCN), which is implemented via the gradient of piece-wise linear networks such as ReLU networks. The authors built the equivalence between LCN and decision trees, and also demonstrated that LCN with M neurons has the same representation capability as decision trees with 2^M leaf nodes. The experiments conducted in the paper disclose that training LCN outperforms other methods using decision trees. \n\nThe detailed comments are as follows:\n\n1) The idea of LCN is very interesting, and the equivalence to decision trees is also very valuable, as it provides interpretability and shines light on new training algorithms. \n\n2) The derivation of LCN and the equivalence is clear. The analysis based on the shared parameterization in Section 3.5 is helpful to understand why LCN with M neurons could be of equal capability to decision trees with 2^M leaf nodes. \n\n3) One weakness is that the performance of ELCN seems to be very close to RF, as shown in Table 2. \n\nI am not sure whether some similar ideas to LCN have been explored in the literature. But the topic studied in this work is very valuable, which connects deep neural networks and decision trees.\n"}