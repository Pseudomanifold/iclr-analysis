{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a DFO framework to generate black-box adversarial examples. By comparing with Parsimonious and Bandits, the proposed approach achieves lower query complexity and higher attack success rate (ASR).\n\nI have two main concerns about the current version:\n\n1)  Some important baselines might be missing. In addition to (Ilyas et al., 2018b) and (Moon et al., 2019), the methods built on zeroth-order optimization (namely, gradient estimation via function differences) were not compared. Examples include \n[1] There are No Bit Parts for Sign Bits in Black-Box Attacks\n[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks\n[3] SIGNSGD VIA ZEROTH-ORDER ORACLE\n \n2) In addition to attack success rate and query complexity, it might be useful to compare different attacks in terms of $\\ell_p$ distortion, where $p \\neq \\infty$. This could provide a clearer picture on whether or not the query efficiency and the attack performance are at the cost of increasing the $\\ell_1$ and $\\ell_2$ distortion significantly.\n\n\n\n\n"}