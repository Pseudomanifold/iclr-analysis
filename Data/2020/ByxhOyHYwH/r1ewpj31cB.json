{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n# Summary\n\nThis paper deals with few-shot learning from a metric-learning perspective. The authors propose replacing the softmax loss, i.e. softmax + cross-entropy loss, with a so-called \"metric-softmax\" loss which imitates a Guassian kernel RBF over class templates/weights. This loss is used in both stages of training on base and on novel classes and the authors argue that it helps learning more discriminative feature while preserving consistency between train and test time.\nSecondly the authors advance a task-adaptive transformation for stage 2 that maps the features from the previously learned feature extractor to a space which is easier to learn. The contributions are evaluated on the standard mini-ImageNet benchmark and on CUB-200-2011 individually and in domain shift mode.\n\n# Rating\nAlthough some of the results in the paper might look impressive, my rating for this work is reject for the following reasons (which will be detailed below):\n1) the main contribution, metric-softmax loss, is not novel. It has been used and described in multiple works in the past 1-2 years.\n2) a part of the evaluations and comparisons do not follow the usual protocol and are not fair\n3) the second contribution, Fast Task Adaptation (FTA), is not well described and it's unclear in what it actually consists, how does it work and how was it trained exactly.\n\n# Strong points\n- This paper deals with a highly interesting and relevant topic for ICLR.\n\n# Weak points\n\n## Contributions\n- This work ignores a large body of research in few-shot learning and metric learning aiming to improve the efficiency per training sample and feature discrimination. \nThe proposed loss can be traced back to Goldberger et al. [i] in  NCA (Neighborhood Component Analysis). Prototypical Networks are derived from this work and hence similar with the metric-softmax loss. \nQi et al. [ii] point out that when h and W are l2-normalized (using the notations from this submission, eq. 7) maximizing their inner-product or cosine-similarity is equivalent to the minimization of the squared Euclidean distance between them. This leads to the loss from [ii], [iii], also known as Cosine Classifier and which also is accompanied by a scaling factor or temperature as here. \nOther related works on improving softmax and selecting the representative weights for a class include: center loss [iv], ring loss[v], L-GM loss[vi].\nIn this light, the metric-softmax is actually not novel and can be found in several other contributions from last year.\n\n- In my opinion, it is difficult from the paper to understand what is actually the fast adatpation module. The authors describe $g$ as \"simply a zero-offset affine transformation\" $g(h)= M^T h$. In the implementation details we do not find out more about this module and we don't have more insights on what is it doing inside, other than a toy hand-drawn example in Figure 3. I find it difficult to assess.\n\n## Experiments\n- The authors evaluate 3 backbone architectures, Conv-4, ResNet-10 and ResNet-12. For the former they use 84 x 84 images, while for the latter they use 224 x 224 images.  The larger images are not standard in the few-shot ImageNet evaluation protocol. Data augmentation (jittering, flipping, etc.) is used here, while in most works it is not. Chen et al. are the first ones to introduce larger images and data augmentation and acknowledge that the large scores are due to this. \nTesting out new configuration is not a problem as long as the baselines are evaluated in the same conditions. However, in this case they are not and this is not visible in the captions of the tables and descriptions in the paper. Training a network with data augmented images and/or higher resolution images and comparing to baselines without data augmentation and images with 7 times less pixels, for sure does not allow seeing the true impact of the proposed method. I would advise to either evaluate in the usual mini-ImageNet settings, either implement a few representative and easy to train baselines, e.g. ProtoNets, Cosine Classifier[iii] in the same conditions as here and compare against. This should provide a better idea on the effectiveness of the proposed methods. \n\n\n## Other comments\n- the scores for baseline methods are seemingly taken from the paper of Chen et al. who trained them themselves. This should be mentioned in the paper and in the caption\n\n# Suggestions for improving the paper:\n1) Review the experimental section and make sure at least some of the baselines are trained in similar conditions as the proposed method or alternatively evaluate the proposed methods in standard mini-ImageNet settings\n\n2) Provide additional insights, experiments and implementation details for FTA to make it easier to understand, there are some examples in the references below.\n\n\n\n# References \n[i] J. Goldberger et al., Neighbourhood components analysis, NIPS 2005\n[ii] H. Qi et al., Low-Shot Learning with Imprinted Weights, CVPR 2018\n[iii] S. Gidaris and N. Komodakis, Dynamic Few-Shot Visual Learning without Forgetting, CVPR 2018\n[iv] W. Wen et al., A Discriminative Feature Learning Approach\nfor Deep Face Recognition, ECCV 2016\n[v] Y. Zeng et al., Ring loss: Convex Feature Normalization for Face Recognition, CVPR 2018\n[wi] W. Wan et al., Rethinking Feature Distribution for Loss Functions in Image Classification, CVPR 2018"}