{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a novel search algorithm that uses the policy and value predictors to guide search and provides theoretical guarantee on the sample complexity. The aim is to estimate the optimal value of an initial state as well as the one-step optimal action to take.\nThe algorithm uses a priority queue to store all states being visited so far and picks the most optimistic one to expand, according to an upper confidence bound heuristic function. The algorithm assumes access to pre-trained value and policy networks and it uses calls to these networks to prioritize the next state to be explored.\n\nThe authors consider a very restrictive setting: \n- Finite horizon Markov decision tree:  no backtrack \n- No intermediate reward and only reward at the end of the episode.\n- Deterministic transition\n- Importantly, access to value network that gives noisy estimates of the optimal value function \n- The noise model is additive and i.i.d and satisfies a concentration inequality\n\nAll this assumption makes the setting very simple and unrealistic. Moreover, I think we can frame the problem into bandit problem and solve it easily with sample complexity independent of the horizon D.\nIn fact, given an initial state s, we consider the K possible actions  a_1, a_2, \u2026, a_K that lead deterministically to next states (r_1, r_2, \u2026, r_K). As the intermediate reward is zero, the state-action value of (s, a_k) is equal to V_{r_k}. As we have noisy estimates of V_{r_k} and we know precisely the noise model, we can run UCB-like algorithm for multi-armed bandit where each arm corresponds to action a_k and expected reward correspond to V_{r_k}. This determines the optimal action in constant time with respect to the horizon."}