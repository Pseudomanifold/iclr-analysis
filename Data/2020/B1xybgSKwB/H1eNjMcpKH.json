{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work focuses on credit assignment using a self-attention module for transfer RL problems. Specifically, the reward signals are assigned backward to previous states according to the attention weights. This can be helpful especially when the reward signal is sparse. Experiments on the newly proposed Triggers environment and the DMLlab keys & doors environment show that the proposed algorithm, SECRET, can speed up training in the transferred environment.\n\nPros:\n- The writing is mostly great.\n\nCons:\n- Some design choices are not well-motivated or even problematic.\n- Experiments are not sufficient.\n\n(1) On page 3, the definition of Z is problematic. The mask matrix M is applied *before* the softmax transformation, which means future values can have non-zero attention. This is because softmax will never produce zero probability. It would still be problematic even if M is applied after the softmax transformation because in this case, the attention for past elements could become very small and almost surely not sum to 1 (except for the last element). Therefore, regardless of the position of M, the attention will be questionable.\n\n(2) Page 4, the weight w(c) is not defined for the weighted cross-entropy. It is claimed that such weighting is essential, but no evidence is provided to support this.\n\n(3) The proposed potential function is not very well-motivated. It is not clear why it should be defined like this instead of other alternatives. Moreover, for never-visited states, the potential is set to 0, which seems to prevent exploration. This would potentially harm the performance in a new environment especially when the training trajectories are far from optimal.\n\n(4) Sec.2.3 says \"given an underlying environment state and a specific action, their contribution to future rewards is not fundamentally altered.\" Can you elaborate? Also what is \"the rank of the rewards\"?\n\n(5) Experiment:\n(5.1) Why Fig.5 and 6 do not have similar asymptotic returns? Given that they both correspond to 1 trigger and 1 (2) prize(s), the asymptotic return should be close.\n(5.2) As mentioned above, it would be interesting to see whether SECRET will prevent exploration if the behavior agent is (heavily) biased. The random agent in the Triggers environment provides sufficient support for the whole state space, while the PPO agent in the DMLab is well-focused on the \"good\" regions. If a \"bad\" agent (say, exploits some low reward regions) is used, SECRET may slow down instead of speed up training in the transfer environment. This is an important scenario to see whether SECRET will potentially create a negative transfer.\n(5.3) No other method from the literature is used for comparison. Several alternatives are discussed in the Related Work \"credit assignment\" section, but none is compared in the experiment.\n\nMinor comments\n- In other fields than RL -> in fields other than RL.\n- The caption of Fig.3-left: there is no \"key\" in the Triggers environment. It uses switches.\n- WT is not explained in Fig.6. \n- h(s) is defined as the observation given a state s, but it is not used in later discussion."}