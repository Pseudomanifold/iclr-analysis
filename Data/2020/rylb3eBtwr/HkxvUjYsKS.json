{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection. A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers. The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder. The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers. An alternative procedure is applied where the loss terms are applied iteratively during training. Once trained, the reconstruction error is used directly for anomaly detection with a threshold. The AUC is used as a performance measure. The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM). The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption. The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption. An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin.\n\n\nPROS:\n\n* A novel approach to fully unsupervised anomaly detection that beats the state of the art.\n\n* The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient.\n\n* A pseudo-code algorithm is provided in the appendix, which should help reproducibility. \n\n* The paper is well written and the math is clearly laid out.\n\n* The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used.\n\n* The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses.\n\n\n\nCONS:\n\n* There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain.\n\n* The AUC and AP scores need to be defined. \n\n* The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario.\n\n* It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset ?\n\n\nOverall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results.\n"}