{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection. The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D). The projection of the latent space then goes to a decoder that reconstructs the input.\nA transformation matrix A is trained jointly with the autoencoder. Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector.\n \nThe paper claims to generalize the existing RSR framework to the nonlinear case. However, the linear RSR is applied to the latent space of the autoencoder. In addition to that, all the following discussion and proofs are limited to the linear case. \n\nSince the proposed method is using RSB as it\u2019s core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups). However, there is no such comparison.\n\nSince autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer.\n\nThe matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur:\n- The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn\u2019t cause data loss.\n- The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible.\n\nIt is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities.\n \nMy point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. \n\nSome citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN \u2018Robust, Deep and Inductive Anomaly Detection\u2019 ECML 2017;   \u2018Adversarially Learned One-Class Classifier for Novelty Detection\u2019 CVPR 2017; DSVDD \u2018Deep one-class classification.\u2019 ICML, 2018; ODIN  \u2018Enhancing The Reliability  Of Out-of-distribution Image Detection  In Neural Networks\u2019 ICLR 2018; \u2018Generative Probabilistic Novelty Detection with Adversarial Autoencoders\u2019 NeurIPS 2018.\n"}