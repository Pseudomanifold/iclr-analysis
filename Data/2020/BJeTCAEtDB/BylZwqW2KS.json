{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "A lossy transform coding approach was proposed to reduce the memory bandwidth of edge devices deploying CNNs. For this purpose, the proposed method compresses highly correlated feature maps using variable length coding. In the experimental analyses, the proposed method outperforms some of the previous work in terms of the compression ratio and accuracy for training ResNet-34 and MobileNetV2. \n\nThe proposed method and initial results are promising. However, the paper and the work should be improved for a clear acceptance:\n\n- Some parts of the method need to be explained more clearly:\n\n\u2013 In the statement \u201cDue to the choice of 1 \u00d7 1 \u00d7 C blocks, the PCA transform essentially becomes a 1 \u00d7 1 tensor convolution kernel\u201d, what do you mean by \u201cthe PCA transform becomes a convolution kernel.\u201d?\n\n- Could you please further explain how you compute PCA using batch data, how you update online and how you employ that in convolution weights together with BN? Please also explain the following in detail:\n\n(I) \u201cTo avoid this, we calculate the covariance matrix layer by layer, gradually applying the quantization.\u201d What is the quantization method you applied, and how did you apply it gradually?\n\n(II) \u201cThe PCA matrix is calculated after quantization of the weights is performed, and is itself quantized to 8 bits.\u201d How did you quantize the weights, how did you calculate PCA using quantized weights and how did you quantize them to 8 bits?\n\n- Could you please explain the following settings, more precisely: direct quantization of the activations; quantization of PCA coefficients; direct quantization followed by VLC; and full encoder chain comprising PCA, quantization, and VLC? Please note that there are various methods and algorithms which can be used for these quantization steps. Therefore, please explain your proposed or employed quantization methods more clearly and precisely.\n\n\u2013  Please clarify the statement \u201cThis projection helps to concentrate most of the data in part of the channels, and then have the ability to write less data that layers.\u201d.\n\n- Did you apply your methods to larger networks such as larger ResNets, VGG like architectures, Inception etc?\n\n- I also suggest you to perform experiments on different smaller and larger datasets, such as Cifar 10/100, face recognition datasets etc., to examine generalization of the proposed methods at least among different datasets."}