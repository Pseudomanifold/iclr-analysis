{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel quantization method towards the MobileNets architecture, with the consideration of balance between accuracy and computation costs. Specifically, the paper proposes a layer-wise hybrid filter banks which only quantizes a fraction of convolutional filters to ternary values while remaining the rest as full-precision filters. The method is tested empirically on ImageNet dataset.\n\nThis paper is generally well written with good clarity. Several concerns are as follows:\n\n1. This paper is incremental in nature, with a natural generalization of (Tschannen et al.(2018)). But it is still an interesting contribution. For this kind of paper, I would like to see a more complete set of empirical results. However, The experiments only perform comparison on ImageNet dataset. Though this dataset has a reasonable size, however, as in many cases, different datasets can bias the performance of different models. To strengthen the results, could the experiments be conducted on multiple datasets as in (Tschannen et al.(2018))?\n\n2. The proposed method is only designed for MobileNets. Is it possible to apply the hybrid filter banks to other neural network architecture?\n"}