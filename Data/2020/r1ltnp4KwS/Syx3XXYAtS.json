{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to improve the interpretability of a convolutional neural network (CNN). The main idea is to force the CNN filters to be class specific, i.e. to be associated to a specific class. This is accomplished by a gating function that enforces filters to be sparsely activated.  This would make the model more interpretable by allowing to check which filters/classes have been activated. Results are evaluated in terms of performance, sparsity of the filters and localization accuracy on CIFAR10.\n\nI lean to reject this paper because I consider the proposed motivations not clear and partially misleading. In the introduction it seems that the idea of enforcing class-specific filters makes sense in general because it avoids filters ambiguity and it reduces redundancy (not clear how). Instead, in the actual implementation, this idea is applied only at the last layer of a CNN. This makes sense, because in a CNN filters need to be shared among classes. It\u2019s an important form of regularization, especially when the amount of training data is reduced. \nAdditionally, the advantage of enforcing the last layer filters to be class-specific is not clear. To me, instead of evaluating the activation of the classification layer, it is possible to check the activation of the filters of the last convolutional layers. However not much more interpretability is added. Furthermore, the improvement on localization as shown in Fig. 6 is only qualitative as the images could have been cherry-picked and there is no real localization measure.\n\nAdditional comments:\nIn table 1, performance of the proposed training is comparable to a standard training on CIFAR10. However, evaluating the proposed approach on a single dataset and only one network is not enough for a clear evidence. Additionally, the obtained performance is below the standard performance of ResNet on CIFAR10 without a clear reason.\nAnother way to enforce a similar pattern on the gating function would be by maximizing the mutual information between selected filters and classes.  \n"}