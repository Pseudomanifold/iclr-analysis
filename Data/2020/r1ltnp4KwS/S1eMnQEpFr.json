{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed an interesting idea of using Label Sensitive Gate (LSG) structure to enforce models to learn disentangled filters for better interpretability of the DNN model. By periodically training with the sparse LSG structure, the model is forced to extract features from only a few classes. The model is trained efficiently in an alternate fashion (with respect to both the network parameters and the sparse gate matrix.) By disentangling the class-specific filters, the model becomes less redundant and more interpretable.\n\nOverall the paper is well-written and well-organized. I  like the idea of imposing a class-specific gated structure for disentangling the representation. And numerical experiments verify the effectiveness of the proposed method in terms of\n1) Improved performance\n2) Disentangled representation (a small L1 norm on the gate matrix G)\n3) Consistent class activation map for different inputs.\n\nI do have some question though\n1) Table 1 also reports the L1 norm and \\Phi of a STD CNN. What is the gated matrix G here for a STD CNN?\n2) Is it very important to have a constraint $\\|G^k\\|_{\\infty}$ in the model? What if an L-1 norm is used directly?"}