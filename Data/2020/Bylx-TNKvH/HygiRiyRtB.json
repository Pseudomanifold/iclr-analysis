{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows that for ReLU networks satisfying certain conditions, the weights and biases leading to the exact functional form are the ones obtained by neuron permutations and rescalings, and that no other reparametrizations preserving the function exist. The authors provide an explicit algorithm applying these symmetries.\n\nDisclaimer: My knowledge in this particular subfield is limit and I therefore cannot assess the novelty of the approach.\n\nI find the topic very interesting and the authors\u2019 approach reasonable. I appreciate that they clearly qualify the assumptions used. The apparent tensions between the intuition that many different reparametrizations can exist and their result suggesting that in fact only very few weight space points lead to the same function is also discussed in the conclusion, which I really appreciate.\n\nIt would be interesting to study the regime where the functions are not exactly the same. In particular, this is very relevant because we only care about answers on a discrete set of points (train set / test set), and on top of that we only care about the argmax of the logits, rather than the actual detailed answer. I believe such a result would be significantly stronger and more relevant to the practical applications of DNNs, however, I understand that it might be more difficult to obtain.\n\nOverall, I enjoyed this paper and I think it deals with an important problem.\n"}