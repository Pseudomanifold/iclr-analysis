{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper generalizes regret analysis results from convex online learning to\nfunctions that are not Lipschitz but Riemann Lipschitz continuous.\n\nExample:\n$f(x) = -\\log(x) + x$ is convex on the convex domain $X = [0, 2]$ but $f$ is\nnot Lipschitz on $D$ since $f(x) \\to \\infty$ as $x \\to 0$.\n\nA possible Riemannian metric that can be used on the domain of such a function\nis the Poincare metric $g(x) = 1 / x^2$.\nThe norm defined by that Riemannian metric is $\\| z \\|^2_x = z^T g(x) z$.\nThis norm can be used to measure infinitesimal distances in $X$.\nThe Riemannian distance $dist(x, x')$ on $X$ that follows from this is defined\nas integrating over the infinitesimal distances on a curve connecting\n$x$ and $x'$ as measured by the norm defined above.\n\nIntuitively, an infinitesimal distance Lipschitz bound can be seen to be\nconstructed by\n$| f(x + \\delta x) - f(x) | \\approx \\| f'(x) \\| \\| \\delta x \\|$ as $\\delta x \\to 0$\nwhich in the case of our example does not exist since $f'(x) \\to \\infty$.\nBut when using a Riemannian metric based norm\n$\\| f'(x) \\| = \\sqrt{ f'(x)^T g^{-1}(x) f'(x) }$\non the right-hand side we have\n$f'(x) = -\\frac{1}{x} + 1$\n$f'(x)^2 = \\frac{1}{x^2} - \\frac{2}{x} + 1$\n$f'(x)^2 g^{-1}(x) = O(1)$\nwith $g(x) = 1 / x^2$.\n\nIn this way it is possible to bound changes of $f(x)$ relative to changes in\n$x$ for functions that are not Lipschitz continuous.\n\nThe authors show how a suitable Riemannian metric can be transformed into\na regularizer usuable in online optimization.\nThey present various rates that appear to be otherwise known for similar\nregularizers.\nMy knowledge of the online learning literature is very limited so I cannot\nmake a qualified statement about these formal analyses in a reasonable amount\nof time.\n\nThe authors also transfer the results from the convex online setting to the\nconvex stochastic setting and the nonconvex setting.\n\nBased on my limited understanding I would recommed to accept the paper.\nThe analysis to me seems both rigorous and useful in practice\n(at least with regard to the formal definitions of Riemannian metrics and\nRiemannian Lipschitz condition for singular functions).\n\nRemarks / Suggestions:\n- With a similar knowledge of the underlying function it would perhaps be\n\tpossible to perform a nonlinear transformation of the input space that\n\tleads to Lipschitz continuous function.\n\tCan something be said about this?\n\n- Definition 2: Write out l.s.c. (lower semi-continuous?) as it does not seem\n\tto be defined everywhere and not every reader is necessarily familiar\n\tenough with convex analysis\n\n- Page 8: Typo: \"Eucldiean stochastic gradient method\n"}