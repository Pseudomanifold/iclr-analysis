{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for training sparse network without first training a dense network (e.g. the lottery ticket hypothesis or distillation). The method involves a combination of dynamic pruning of weights coupled with a dynamic \"growing\" of new weights given a novel criterion based on the magnitude of the gradient of the loss. As a result, networks can stay sparse throughout training and testing, leading to a large reduction in computational cost.\n\nThe paper's approach of dynamically changing the topology of networks is an interesting and motivated idea that seems to work rather well. I also appreciate the experiments on MobileNet, a setting where one expects investigations into sparse network architectures to have significant application. Relatedly, I appreciate the importance of the fact that the computational cost of training and evaluating the network is proportional to the sparse model size, which is not normally true for masked dense models. Overall, I found the paper to be very clear and of high quality, and thus I find this to be an interest addition to investigations into the lottery ticket hypothesis.\n\nAs the authors state, the novelty of their method is that they use the gradients with the highest magnitudes to grow connections. This is somewhat intuitive given the role gradients play in gradient descent based optimization, but I was wondering if they had any further intuition as to why this is the right criterion?\n\nIn section 4.3, I was a bit confused by Figure 5. My understanding is that many paths between loss landscape minima follow nonlinear paths -- why is it at all significant that there's a linear barrier? Why are only quadric and cubic Bezier curves used, rather than a more general path finding algorithm?\n\nOverall, this is a nice paper that should be accepted to ICLR."}