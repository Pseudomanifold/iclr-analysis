{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Overview:\n\nThe paper is dedicated to developing a more efficient and powerful dense-to-sparse training method. In order to break the limits of the size of the largest trainable sparse model to that of the largest trainable dense model, the author proposes a dynamic method that updates the network topology via parameter magnitudes and infrequent gradient calculation. In the experiments parts, they conduct extensive studies to show the proposed approach can surpass the previous sota with ResNet-50, MobileNet v1 and v2 on the imagenet 2012. What's more, the author also provides some intuitive explanation about why allowing topology change during the optimization is beneficial. \n\nStrength Bullets:\n\n1. Due to the dynamic network topology, the paper's methods exactly achieve the memory and computation efficient. i) Required memory is only proportional to the size of the sparse model. ii) The amount of computation is proportional to the number of nonzero parameters in the model.\n2. The author performs detailed comparison experiments among different sparsity distribution and different pruning methods. And the results overcome the previous state-of-the-art results.\n3. Fig 5 shows some interesting insight. It suggests that static sparse training may be stuck at some local minima which are isolated from improved solutions. However, the dynamic update has a big chance to avoid this problem.\n\nWeakness Bullets:\n\n1. The author claims that the ticket in the paper does not rely on a \"lucky\" initialization. But it doesn't exclude the possibility that starting from the original initial conditions may give a better performance. Even if the connection is dynamic, we can still record the initial point for each weight. It will be better the author can provide related analysis.\n2. In my opinion, in order to prove dynamic pruning is better than static methods, the author needs to provide a comparison with the previous sota method in it's own setting. i.e. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\nRecommendation:\n\nI think this paper is a novel work. Although it has some flaws in the experiment design, the motivation and experiment results are conniving enough. So, this is a weak accept."}