{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors tackled one important research problem in MAML, i.e., the optimization instability, by investigating the two learning rates. Though I appreciate the theoretic contribution this work makes, I am not sure with the practical significance of it.  Below please find my detailed comments. \n\nPros:\n-\tIn this work, the authors focused on an important problem \u2013 training MAML is kind of unstable and tricky, so that developing guidelines that stabilize MAML or its first-order approximations is of significance. \n-\tThis work theoretically discusses the relationship between the inner loop learning rate and the outer one, under a set of assumptions. \n-\tThe paper is well written and easy to follow.\n\nCons:\n-\tSome of the simplifications for proving are empirical, so that the proof itself is not that rigorous. \n  o\tFor example, In Section 3.1.1, the authors ignored Tg based on an observation that Tg is small. However, even in the Appendix, the authors did not explicate the experimental setting where they reach such a conclusion. Will Tg be always small then, in any task and any dataset?\n-\tThe take-away of this work is not clear, in other words, it makes small contribution to the practical training of MAML. \n  o\tPractically, we often train MAML in 5 or 10 steps instead of only one step. Will the conclusion apply to such setups? Or will the model itself diverge during the inner loop as more steps are taken, provided with a larger value of \\alpha?\n  o\tPractically, we merely use vanilla SGD. The figure in the Appendix shows that the main conclusion actually does not apply to Adam, which disempower the practicability of the theoretical guideline. \n  o\tHow can we search the \u201clargest possible\u201d inner loop learning rate? Should we still use grid-search or heuristic search? In that case, what is the implication/shortcut that this proposed guideline brings? We still reach a stable training process by tuning the hyperparameters in a brute-force fashion.  \n-\tThe baselines should be compared, to support the effectiveness of this proposed algorithm. For example, Behl et al. (2019) automatically tuning the learning rates during training definitely needs to be compared, since both aim to stabilize the training of MAML."}