{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors study a method to help tuning the two learning rates used in the MAML training algorithm. First, they derive a necessary condition for the convergence of the gradient descent in the single task setting. The condition relies on the eigenvalues of the Hessian of the task loss. This condition is reminiscent of convergence criteria for gradient descent on quadratic objectives, and the authors make the interesting observation that the criteria for the exterior learning rate beta depends on the interior learning rate alpha. \nHowever, in the setting of interest that is the multitask learning, the trick used to analyse the eigenvalues doesn\u2019t work anymore. To circumvent this issue, the authors provides a sufficient condition for the multitask equivalent of the necessary condition to hold.\n\nThe derivations are correct (few typos, see nitpick below) and the paper is nicely presented. I\u2019m not a MAML expert so I\u2019ll let the other reviewers judge how this paper compares to the current literature. However, I think that the empirical work can be pushed further. The experiments on Omniglot and MiniImagenet are coherent with the theory, but I am not completely sure of their impact. Indeed, the learning rate domain on which MAML converge seems coherent with what the authors are predicting, but it doesn\u2019t allow us to choose the learning rates before training the model. It doesn\u2019t help that the criteria relies on the spectrum of the Hessian evaluated at a critical point, which of course is not known before convergence in non quadratic setting.\nThe paper would be way more convincing if the authors could use their findings to describe some more precise heuristics to tune the learning rates, and conduct a proper comparison of its performance with other methods.\n\nAs a result, I think the paper is exploring an interesting direction, but that the empirical work might be too preliminary for publication.\n\nNitpick:\n\n- Notations in 2.2 are a bit confused.The gradient of L wrt theta is denoted first with nabla, then with partial derivatives. Nabla is then used to denote the Jacobian of theta\u2019 wrt theta. There is a small mistake in the 4th line of 2.2. Chain rule for gradient of L wrt theta\u2019 does not give (d L / d theta) times gradient but instead give gradient times (d L / d theta)^T. The error is fixed when passing from (3) to (4) so it doesn\u2019t impact the paper.\n- Also, a transpose is missing in the taylor expansion on page 3.\n- \"A multilayer perceptron with two hidden units of size 40\" -> layers?\n"}