{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles the problem of how to integrate intrinsic rewards most effectively, in the episodic sparse reward setting. It has two main technical contributions. The first is Scheduled Intrinsic Drive (SID), which trains two separate policies -- one for maximizing the extrinsic (i.e., task) reward and another for maximizing the intrinsic reward -- rather than a single policy that maximizes a weighted combination of both. This uses the same training setup as prior work, Scheduled Auxiliary Control (SAC), except here the extra policy is trained on intrinsic reward rather than an auxiliary task. The second contribution is Successor Feature Control (SFC), a novel approach for computing intrinsic rewards. For a given transition (s, a, s'), the intrinsic reward from SFC is the squared difference in successor features between states s and s'. Since successor features encompass a notion of which kinds of states the agent will encounter in the future after starting from the current state, this type of intrinsic reward is more far-sighted than most state-of-the-art approaches. Empirical analysis shows that SFC leads agents to explore bottleneck states, which is especially helpful for solving navigation tasks.\n\nThis paper is well-motivated and clearly written. The experimental evaluation of this paper is thorough, comparing SID to adding extrinsic and intrinsic reward together, and comparing SFC to two recent approaches for generating intrinsic rewards, ICM and RND. The appendix does a good job of providing implementation details for reproducibility, in particular regarding reward normalization and the variation of prioritized experience replay. I also greatly appreciate that design decisions are justified, for instance that the choice of using a random scheduler was made because it outperformed several versions of a learned scheduler.\n\nMy only concerns with the paper have to do with evaluation. SFC is compared to prior approaches for computing intrinsic rewards that only take into account transition-level information, whereas SFC takes into account trajectory-level information, and naturally performs better. But there are also recent approaches that do take into account trajectory-level information in different ways, e.g. Savinov et al. (2018). SFC should also be compared to approaches in this category.\n\nI would also like to see an analysis of the failure cases that SFC is vulnerable to. Currently the evaluation domains used, with the exception of cartpole, are all tasks involving first-person navigation. So I wonder whether SFC is most effective (compared to existing approaches) on primarily these tasks in this domain, that are partially observable. It would be nice to see a wider variety of evaluation domains, for instance Montezuma's Revenge, which is frequently used to evaluate algorithms for computing intrinsic rewards, as well as other methods for improving exploration of RL agents. It would be neat if agents trained using SFC are better able to navigate through the doors in this game, since that seems to be a clear example of bottlenecks.\n\nMinor questions / comments:\n- In Figure 1b, why are the values on the four bottlenecks not all exactly the same? The maze is symmetric, so I would expect them to be equal.\n- The plots in Figures 3 through 6 should show the standard deviation.\n\nTypos:\n- Page 2, \"inexplicitely\" --> \"implicitly\"\n- Page 4, \"temporarily\" --> \"temporally\"\n- Page 8, \"carpole\" \u2014> \"cartpole\""}