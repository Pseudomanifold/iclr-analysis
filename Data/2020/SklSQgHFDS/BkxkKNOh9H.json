{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: A very nice study on the benefits of successor feature control as an intrinsic drive for hard exploration problems. The work builds nicely on previous work on using SF for exploration and proposes using derived (reachability under $\\psi^{\\pi}$) distances  as intrinsic motivation for an (purely) exploratory policy. This exploratory strategy will be used in conjunction with a policy trained on the extrinsic reward to gather (off-policy) data for both learning processes. The author proposed 'combining' these two policies via a simple scheduler, similar to (Riedmiller 2018).\n\nGood paper/addition to both the intrinsic motivation literature and SFs studies.\n\nPositives:\n1) I like the separation of concerns achieved by training separate policies train for the two reward signals (intrinsic and extrinsic).\n2) SFC as intrinsic reward and the study comparing this with other intrinsic signals (ICM/RND). The more interesting study might in the appendix though (Appendix A). I would suggest moving that into the main paper, as it nice separate the influence of scheduling component and the 'quality' of the proposed intrinsic reward.\n3) Carefully conducted study, with relevant ($\\epsilon$-SOTA) baselines and ablation studies.\n\nPoints of improvement or clarification:\n1) The SFs and the derived reward were done based on random pseudo-rewards $\\phi$ (Pg 5, SF-Nets). It maybe worth exploring learning those to capture more interesting features of the task at hand, especially in situation were there is more signal in the extrinsic reward. Do the authors have a sense of how problematic changing this component throughout training would be? As this acts are a reward signal to the inference the intrinsic reward signal, which then trains the exploratory policy. Thus small changes in one, can have massive implications for the trained Q-net, $Q_{E}$.\n2) It wasn't clear from the exposition which policy is used to train the SFs? The exploratory policy, the uniform random one or the behaviour policy (the combination between the two trained policies given by the Q-nets).\n3) On the SID setup. Did you conduct any studies on M (the number of switches)? For instance, how does this compare with something like episode switching, which has been explored before?\n4) There are a couple of observation/discussion claims that are not really substantiated (for instance, last paragraph in Sec. 3.2). The paper is fine content-wise, without them. I would strongly suggest either removing them, re-phasing them as hypothesis and/or back them by more evidence. \n5) The link to the video (https://gofile.io/?c=HpEwTd.) doesn't work. Please update.\n\n\n"}