{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "* Summary\nThis paper introduces a pseudo-metric based on a notion of Lipschitz continuity for action-value functions between different MDPs.\nThe algorithmic improvement is the use of the pseudo metric with RMax in lifelong RL.\nIn particular, the pseudo-metric is defined as the minimum between two dissimilarity measures, each of which can be solved with dynamic programming.\nThe pseudo metric is then used in conjunction with the optimal state-action value function for a previous MDP to form an upper bound heuristic.\nThis upper bound is used in RMax to improve exploration since it can be tighter than the naive RMax bound.\nEmpirically, the theoretical results are supported by thorough investigation on a variant of the grid-world environment.\n\n\n* Decision\nThe main contribution of this paper is hard to discern, but the ideas presented are interesting.\nDespite the theoretical nature of the paper, I find that the experimentation is overall lacking.\nIn the paper's current state, I would recommend a weak rejection.\nHowever, I am willing to increase this score if the presentation is improved and if the experiments are expanded.\n\n\n* Reasons\nWhile the paper motivates and explains the problem well, the proposed method is much less clear.\nFor example, Section 2 leading up to proposition 3 seems like it should motivate Section 3.\nInstead, I think that Section 2 does not communicate enough how the pseudo-metrics are planned to be used.\nPerhaps proposition 3 can be in the appendix and RMax should be discussed in section 2.\nOtherwise, the pseudo-metrics in Section 2 (and 3) should be motivated some other way.\n\nFor experiments, the paper provides a thorough investigation of one interesting environment.\nI think a similarly thorough investigation of another environment would greatly strengthen the paper.\nMultiple experiments that are less thorough would also benefit the paper, but less so.\nLastly, it would be interesting to compare to other non-PAC-MDP lifelong learning RL algorithms.\n\n\n* Details\n\nMinor concerns:\n\nI have trouble following the statement and proof of your probability bounds (for example, proposition 4).\nIt would be good to make more explicit what is a random variables.\nFor example, you define R_s^a to be the expected reward, but I assume that $(s,a)$ are random variables in the probability bound.\nIn addition, you occasionally refer to the propositions and equations as properties.\n\nPage 2: \"The extension to continuous spaces is straightforward but beyond the scope\"\nThis does not seem relevant since you only look at the tabular problem.\nWhile the definition might hold for continuous state spaces, the extension of LRmax to continuous spaces does not seem straightforward.\n\nMinor typos:\nPage 4: \"this information when the task changes allows to compute the upper bound\"\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}