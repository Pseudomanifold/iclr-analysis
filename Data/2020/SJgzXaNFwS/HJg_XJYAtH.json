{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes the use of hyperdimensional (HD) vectors to represent n-gram statistics. The HD vectors are first generated from the whole corpus. Then, it is aggregated or bundled to a vector for each sample as an input of a classifier training. The evaluation is conducted on four datasets: Chatbot, AskUbuntu, WebApplication and 20 News Group using a bunch of classifier including KNN, Random Forest, MLP etc.\n\nIt is interesting to see how to hash/project the high dimensional n-gram vector into a lower space for efficiency. The approach is useful in online production systems, and it is eco-friendly. However, there are a few concerns detailed as follows:\n\n1. Can it be generalized to contemporary learned embeddings, e.g., word2vec and GloVe? \n\n2. Lack of proper baselines for comparison. Word2vec, GloVe are trained on large corpora once and can be applied directly to other tasks, and they should be served as baselines. Furthermore, the simple bag of word/TF-IDF should be included as baselines as well.\n\n3. Lack of analysis: it is hard to understand what kind of HD vectors are generated. Are these n-grams semantically related projected nearby in the HD space? This helps readers to understand the constructed embeddings.\n\n4. The SentEval benchmark is popular in sentence level representation learning and it is well known. It is better to see some evaluations on it as well. http://www.lrec-conf.org/proceedings/lrec2018/pdf/757.pdf\n\nMinor comments: \n1. The Subword Semantic Hashing is originally from DSSM published in 2013.  (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf) \n2. What is $v_c$ in 4.2?\n"}