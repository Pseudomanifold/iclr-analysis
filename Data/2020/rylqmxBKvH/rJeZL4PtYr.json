{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\nThe paper presents an approach to perform video inpainting from corrupted input only . Their approach proposed a method that uses GANs for denoising images to now handle full sequences of images by building on top work that handles inpainting in single images.\n\nStrengths\n1) The authors present extensive experiments on many datasets. \n2) The presented approach is simple and general enough to be applied to many video problems.\n3) The provided code is well structured and easy to read and atrached webpage showcases their results well.\n4) The paper is well-written.\n\nWeakness\n1) From the code and the description in paper, it seems that a different corruption (https://github.com/anon-ustdi/ustdi/blob/7a81db4972ef9d4eabbd8fe354a8984a7771ae5d/src/datasets/corrupted.py) is applied for each step. Can the authors confirm if that is the case? \n\nIf this is the case, I am afraid the method cannot be called unsupervised as across many steps the model would have seen different corruptions of the same video and across many such corruptions the model can learn what an uncorrupted video looks like. It is okay if that is the case but the claim of unsupervised would not hold. If that is the case, the authors need to make comparison with other supervised inpainting methods as well [1,2]\n\n2) Is there a dataset for which these corruptions exist naturally? May be [3, 4].  It would be nice to have experiments on a dataset where the corruptions are present naturally.\n\n3) While the experiments are incomplete for Table 1,  [4] outperforms the proposed approach for the one dataset the numbers have been reported. Performance comparison with [4] are not fair as [4]'s implementation is in MATLAB.\n\nDecision\nWhile the presented approach is good, further experiments are required to further validate the effectiveness of their approach in an unsupervised setting.  \n\nReferences\n[1] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang. Video inpainting by jointly learning temporal structure and spatial details.\n[2] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Deep video inpainting. \n[3] https://github.com/stayhungry1/Video-Rain-Removal\n[4] https://github.com/nnUyi/DerainZoo\n[5] Alasdair Newson, Andr\u00e9s Almansa, Matthieu Fradet, Yann Gousseau, and Patrick P\u00e9rez. Video inpainting of complex scenes. "}