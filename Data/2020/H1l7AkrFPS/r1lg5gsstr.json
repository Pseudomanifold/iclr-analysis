{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper wants to show that removing the spatial information from the last layers of a deep neural network does not affect the network performance (in some case it obtains even better results). Results seem consistent across different datasets and models.\n\nI lean to reject this paper because the proposed analysis is quite shallow and many considerations about geometrical information are not taken into account. In my opinion, there are few interesting experiments in the paper, but the global plot that spatial information is not useful does not work. In particular 1x1 convolutions do not destroy the spatial information of a network. In the following, I explain it more in detail.\n\nIn this paper is missing a clear definition of spatial information and how it is represented throughout the network's layers. It considers three different ways to \u201cdestroy\u201d spatial information. However, in my opinion only shuffle conv and global average pooling (GAP) really destroy the spatial information in a network:\n- Shuffle Convolution: In this case, after this layer, the spatial information is fully destroyed because the location of the feature maps is shuffled. If enough shuffling iterations are performed during training, the following fully connected layer of the network will learn to discard possible spatial correlations. In Table 2 left is quite interesting to see that a network trained with this shuffle conv layers at the end can almost recover the full performance when shuffling the last 3 layers, while it does not provide any results when applying shuffle net at run time only. However, shuffle net is abandoned for the following experiments presumably because it does not perform well on more difficult conditions.\n- Global Average Pooling: In this case, the spatial information is lost in the sense the all the spatial features are merged in a single 1x1 spatial dimension. It makes sense (in Fig.2) that shuff conv and GAP perform similarly because in both cases the only remaining information is at the feature level. It should be clear that, if the network has enough layers, and therefore enough receptive field, removing the spatial information at the last layers does not affect the performance, because the spatial information learned before it is more than sufficient. This is why many recent network architectures (e.g. ResNet) substitute the final fully connected layer with a simpler GAP layer.\n1x1Conv: When using a 1x1 convolution a layer considers the input features locally, in the sense that it does not look at how those features interact with the neighbors' features (or pixels). However, their spatial information is preserved for the following layers. For instance, if the network ends with a fully connected layer, this layer can collect and exploit the spatial information contained in the different locations of a feature map computed with 1x1 convs. The fact that a 1x1 convolution does not destroys spatial information can be seen also in Fig.3 right. There, even with only 1x1 convolution, the receptive field is still higher than 1 and the classification performance, even if reduced, is still good.\n\nOverall, I think that the paper needs a global rethinking with a better definition and understanding of spatial information and in which sense it is destroyed.\n"}