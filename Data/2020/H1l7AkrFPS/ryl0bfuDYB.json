{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper examines the role of spatial information in image classification. The representation produced by the network is perturbed in a variety of ways including the use of a shuffled convolution where the spatial position of activations is randomly shuffle as information proceeds from early layers to later layers. This is done independently across training and test runs. Experiments reveal that there is a degree of robustness of performance subject to permutations or other operations and that performance is preserved to a surprising degree when fewer layers are perturbed.\nOverall, I find this paper tackles an interesting problem and does so in a direct and natural manner. One question that could receive more attention is the role of spatial pooling in this process. i.e. to what degree is information naturally abstracted spatially by these networks? In the extreme case, one could imagine all values being pooled and a shuffle having zero effect. This is clearly not the reality, but the interaction of shuffling and pooling seems important to the achieved performance.\nAnother consideration that is worth addressing in more detail is why the performance drops so precipitously when all or almost all the layers are perturbed. It seems that having a few stable layers may be enough but this is a necessary condition for performance to be preserved. Any insight as to why this is so necessary would be a nice touch for readers of this paper."}