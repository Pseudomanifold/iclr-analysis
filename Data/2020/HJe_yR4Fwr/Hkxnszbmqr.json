{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper presents a bound on the generalization error of a deep network in terms of margin at each layer of the network.  The starting premise is that extending the existing margin generalization bounds to deep networks worsen exponentially with the depth of the\nnetwork. Recent work which removed that exponential dependency is\nclaimed to require a more involved proof and complicated dependence on\ninput.  The paper provides a new bound that is simpler\nand tighter.\n\nA second contribution is to extend their bounds to robust classifier.\nSince their bounds depend on instance-specific margins, the extension\nto the robust case is straightforward. They just need to relax the\nmargin to the robustness boundary of the input. \n\nFinally, they present a new algorithm motivated by their bounds, that\nmaximized margin on all layers.  They show that the resultant network has much lower error than standard training.\n\nThe paper is well-presented and in spite of being theoretical is very nicely developed so that the main contributions come out clearly to non-specialists too.  \n\nA few minor comments:\nThe inner min in Equation 2.2 seems to be a typo.\n\nIn Theorem 2.1, there is typo around the definition of \\xi.\nBelow thoerem 2.1, the phrase \"depend on the q-th moment\" has 'q' undefined.\n\nTypo \"is has a\" in Theorem 3.1\n"}