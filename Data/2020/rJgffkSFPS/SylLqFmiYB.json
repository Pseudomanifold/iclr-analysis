{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper provide a NAS algorithm using Bayesian Optimization with Graph Convolutional Network predictor. The method apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. The method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off.\n\nThe paper is well-written. The experiments are abundant. However, the paper has following drawbacks that need to be further concerned:\n\n1.\tIn my opinion, the key point of the paper has nothing to do with GCN or multi-objective. The important part is to use BO and EI to sample new architecture. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1.\n2.\tEq.(9) focuses more on models with higher accuracy. However, those models with bad performance will be predicted inaccurately and may have a higher score than good models. For model with ground-truth near 0, arbitrary predicted score results in the same loss. Eq.(9) seems cannot prevent this situation from happening.\n3.\tTable 1 compare different architectures with GCN. However, the LSTM is the worst architecture used among the 3 different architectures in the original paper (Alpha-X), which makes the comparison unfair.\n4.\tTable 2 shows the number of architectures trained. However, the proposed method need to update GCN multiple times during searching, which makes the comparison unfair.\n5.\tTable 2 shows the number of training models before finding the best model. It is meaningless when used in reality, which often contains more than 10^10 different architectures and the best architecture is unknown. In my opinion, the performance of the top1 architecture predicted by the proposed method is much important.\n6.\tAlgorithm 1 uses Pareto front, which does not exist when doing experiments on single-objective search. More details should be clarified."}