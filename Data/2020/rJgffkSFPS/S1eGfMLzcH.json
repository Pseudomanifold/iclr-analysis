{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "--Summary--\nThe authors present an algorithm BOGCN-NAS which combines bayesian optimization and GCNs for searching over NN architectures. The authors emphasize that this method can be used for multi-objective optimization and run experiments over NAS-Bench, LSTM-12K and ResNet models. \n\n--Method--\n\nMethodologically, the contribution is somewhat weak. The main technical contribution is to use a GCN to get a global representation of a graph, which can then be used for downstream regression tasks such as predicting accuracy. It\u2019s not clear how much the GCN generalizes in being able to encode arbitrarily large architectures. The two main examples offered are NAS-Bench and LSTM-12K focus on optimizing cell architectures which contain a handful of nodes e.g. (5 in the case of NAS-101).\n\nGraph embeddings:\nThe authors have not considered other graph embeddings to use in their Bayesian regression setup. E.g. see https://arxiv.org/pdf/1903.11835.pdf for a list.\n\nBayes-opt:\nIn Algo1, step 5. The authors randomly sample a number of architectures in order to calculate EI scores on them. For large discrete combinatorial search spaces, this approach will not scale.\n\nMulti-objective optimization:\nIt\u2019s not clear why GCNs or BO is required for this. Any predictor that generates multiple metrics could substitute in order to create a pareto-curve. Even multi-objective RL based approaches could suffice. Thus multi-objective opt only seems like a minor/tangential contribution.\n\n--Experiments--\nThe main claim of the paper is that this approach works well  for the multi-objective case. However, the results only look at two objectives #params vs accuracy. There\u2019s a pretty strong correlation between the two. It\u2019s unclear how the method generalizes when objectives are not correlated. The authors need to thoroughly demonstrate other objectives/find suitable benchmarks for the same as clearly NAS-101 will not suffice.\n\nOther concerns:\n - Table 1 has correlations using 1000 training architectures. Why 1000? Why not 50 (that\u2019s how sec 4.2 is initialized). Also, the correlation results are less impressive in Figure 9.\n - Table 1 lists the number of params that the predictor uses. Why is this important? How about comparing with a linear regressor?\n - The results in Sec 4.3 are using random as the only baseline. This is a pretty weak baseline.\n - In sec 4.4, the authors pick models M1, M2 and M3 as candidate examples.. How were these chosen ?\n - Sec 4.5 transfer learning results are pretty weak. Transfer across datasets is much more interesting e.g. between ImageNet and Cifar-10.\n\nOverall, this paper has some interesting results, which show that GCNs can be useful models to encode graph structured inputs. However, the methodological and experimental results can definitely be strengthened. The authors may consider the following:\nAddress how GCNs can model and scale to general architecture spaces than a small number of nodes in a cell.\nAddress how to sample better over combinatorial search spaces than random in the inner loop of BO.\nStrengthen MO-opt results. Use better baselines than random and different objectives than accuracy vs #params.\n"}