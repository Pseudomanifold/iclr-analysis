{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to combine energy functions to realize compositionality. This is interesting, and different from previous methods, which use either an explicit vector of factors that is input to a generator function, or object slots that are blended to form an image.\nSpecifically, three operators (logical conjunction, disjunction, and negation) are realized and empirically evaluated through combining energy functions in three different ways.\nExtrapolating concept combinations, continually learning, and concept inference are also evaluated.\n\nThis paper is well motivated, showing compositional generation and inference for images. However, I have some concerns:\n\n1. The experiments on the CelebA dataset are mainly subjective.\n\n2. The equal sign in Eq.(4) should be \\proto.\n\n3. The most serious concern is that although empirical results are promising, I have concern about the correctness that Eq.(6) realizes disjunciton, and Eq.(8) realizes negation.\n\nIt is sensible that Eq.(4) realizes conjuction, according to the idea of Product-of-Expert. Multiplying several energy-based densities reduces to summation of the energies. \n\nFor Eq.(6), the authors ignore the influence of normalizing constants when adding several energy-based densities. The authors seem to assume that the normalizing constants for p(x|c_i) are equal. Justification is needed.\n\nNote that we cannot have :\nlog [0.6* exp(-E(x|c1)) + 0.4* exp(-E(x|c2)) ] will output c1 with probability 0.6 and c2 probability 0.4.\n\nFig. 1 seems to illustrate ideas at first sight, but is not so convinced at second thought.\n\n4. No discussion for the \\alpha in Eq.(8) for concept negation.\n\n5. The description of the baseline joint model in Section 3.4 is missing.\n\n6. For learning EBMs, the following reference is missed, besides (Kim & Bengio, 2016)\nYunfu Song, Zhijian Ou. Learning Neural Random Fields with Inclusive Auxiliary Generators. arxiv 1806.00271, 2018."}