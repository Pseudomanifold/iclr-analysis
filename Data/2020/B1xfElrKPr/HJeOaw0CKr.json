{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors incorporated tensor-product representations within the Transformer.  By creating an attention mechanism called TP-Attention, they explicitly encode the relations between each Transformer cell and the other cells, whose values are retrieved by attention. By introducing tensor products, the proposed algorithm can empirically perform well for noncommutative operations with multiple arguments, such as division. The authors trained models with the proposed algorithm on the Mathematics Dataset and compared the performances with two baselines (simple LSTM and the original Transformer). At last, several model snapshots are provided to help interpret several key elements of the model: the learned roles, the attention maps, the TP-transformer columns and so on. \n\nOverall, the paper is well-written. The experimental results generally support the high-level intuition behind the introduction of tensor-product representation. I would recommend accepting this paper. \n\nSome quick questions:\n\n1. It was claimed in the Conclusion section that the performance of the proposed algorithm beats the previously published state of the art by 8.24%. I guess the number comes from the 2nd and the last row of interpolation accuracy in Table 1. However, these two results are obviously trained for different numbers of iterations: The baseline algorithm was trained for 500k steps, while the proposed algorithm is trained for 1.7M steps. Is it a fair comparison? If the proposed algorithm is also trained for 500k steps, the improvement is around 2.3%. \n\n2. Why is the extrapolation accuracy results for TP-Transformer missing in Table 1? \n\n"}