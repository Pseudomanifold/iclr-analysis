{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a scheme for incorporating compressed (sparsified) gradients with momentum in distributed SGD. The approach differs from others in the literature, comes with theoretical guarantees, and improved performance. The results are correct, and the experiments illustrate that the proposed approach can make a difference (albeit, modest) in the quality of the resulting model.\n\nThe main point I find dissatisfying about the theoretical results of the paper are the use of Assumption 2. The memory vector is a parameter of the algorithm. I realize that one can enforce this with a projection, as argued in the paragraph rationalizing this assumption. However, that specific case isn't analyzed and it isn't clear how incorporating that projection would affect the accuracy, since it would essentially be countering the effect of error feedback.\n\nI also find Assumption 3 to be strange. In the convex setting, one can typically show that this follows from Assumption 1 alone under the additional assumption of a suitably small step size. In the non-convex setting it isn't clear what this means, since w^* is not well defined (if there are multiple global minimizers).\n\nAssumption 1 is also strong. Typically one assumes that the stochastic gradients are unbiased, and either that the expected gradient is Lipschitz continuous (in the smooth case), or the expected gradient is bounded (in the non-smooth case). Assuming that the stochastic gradients are uniformly bounded essentially implies that the noise vanishes when the gradient gets large. \n\nCan you provide examples of functions/problems satisfying these assumptions? Even an example as simple as the case where $F$ is a finite sum of quadratic functions, and one randomly samples one of the terms in the finite sum to compute the gradient (i.e., using SGD to solve a large linear least squares problem) doesn't appear to satisfy Assumption 1.\n\nOverall the results are potentially interesting. I would have given a higher rating if the assumptions didn't appear to be so strong, and if the experimental results demonstrated a more substantial difference with DGC.\n\n\n"}