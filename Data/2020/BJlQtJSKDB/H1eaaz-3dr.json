{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a novel approach to parallelizing Monte Carlo Tree Search \nwhich achieves speedups roughly linear in the number of parallel workers while \navoiding significant loss in performance. The key idea to the\napproach is to keep additional statistics about the number of \non-going simulations from each of the nodes in the tree. The approach is \nevaluated in terms of speed and performance on the Atari benchmark and in a \nuser pass-rate prediction task in a mobile game.\n\nI recommend that this paper be accepted. The approach is well motivated and clearly \nexplained, and is supported by the experimental results. The experiments are reasonably thorough and \ndemonstrate the claims made in the paper. The paper itself is very well-written, and all-around \nfelt very polished. Overall I am enthusiastic about the paper and have only a few concerns, detailed below.\n\n- I suggest doing more runs of the Atari experiment. Three runs of the experiment does not \nseem large enough to make valid claims about statistical significance. This is especially \nconcerning because claims of statistical significance are made via t-testing, which assumes \nthat the data is normally distributed. Three runs is simply too few to be making conclusions \nabout statistical significance using t-testing. I think that this is a fair request to make and \ncould reasonably be done before the camera-ready deadline, if the paper is accepted.\n\n- The experiments in Atari compare against a model-free Reinforcement Learning baseline, PPO. \nWas there a set clock time that all methods had to adhere to? Or alternatively, was it verified that \nPPO and the MCTS methods are afforded approximately equal computation time? If not, it seems \nlike the MCTS methods  could have an unfair advantage against PPO, especially if they are \nallowed to take as long as  necessary to complete their rollouts. This computational bias \ncould potentially be remedied by  allowing PPO to use sufficiently complex function \napproximators, or by setting the number of simulations used by the MCTS methods \nsuch that their computation time is roughly equal to that of PPO.\n\n- I would be careful about stating that PPO is a state-of-the-art baseline. State-of-the-art is a big claim, and I'm not quite sure that it's true for PPO. PPO's performance is typically only compared to other policy-based RL methods; it's hard to say that it's a state-of-the-art method when there's a lack of published work comparing it against the well-known value-based approaches, like Rainbow. I suggest softening the language there unless you're confident that PPO truly is considered a state-of-the-art baseline."}