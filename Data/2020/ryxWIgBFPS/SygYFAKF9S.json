{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a method of discovering causal mechanisms through meta-learning, by assuming that models transfer faster if their causal graph is correct.\n\nFor a possible causal graph, for adaptation to one interventional dataset, the samples are iteratively revealed and the log likelihood of the next sample is measured, after which the parameters are updated using one step of gradient descent on that sample. The sum of these log likelihoods is the \u2018online likelihood\u2019 and a measure of speed of adaptation. The parameters are initialised using maximum likelihood estimation on the train dataset.\nThe meta learning procedure then at each episode samples an interventional distribution and an interventional dataset from that distribution. It then performs a gradient based update of the belief over graphs based on the difference between the online likelihoods of each graph on that dataset.\n\nThe meta-learning approach appears to be a novel contribution. The authors provide a theoretical argument by counting \u2018effective parameters\u2019 to suggest why models using the right causal model obtain a higher online likelihood. Additionally, they prove that the gradient updates to the graph belief are easy to compute and converge. The method is validated with several synthetic experiments which discover the direction of the arrow between two random variables, each either continuous or discrete. Furthermore, they successfully experiment with the combination of learning a representation of a raw data to two random variables with learning the causal direction. The paper is very well written and most claims are carefully proven.\n\nI recommend a weak rejection for this paper, because:\n1) The empirical validation is not strong enough, as no real dataset is used, only toy datasets. The toy experiments themselves could also be more extensive.\n2) I am unconvinced of two of the theoretical claims made: (A) the fact that the expected gradients in Prop 1 are 0, implies that the right causal graph has better online likelihood and (B) that the method is easily extensible to more than two random variables [Appendix E].\n\nSupporting arguments:\n1.1) Fig D.1 suggests that, in the experiments using continuous random variables, the training dataset alone is sufficient to discover the true causal model, under the assumption of independent additive noise, as is done in e.g. [1]. I find it plausible to believe that the training curve on the training dataset alone already makes it possible to disambiguate the causal from anti-causal model. A similar pattern is shown by the authors themselves in Appendix B on discrete variables.\nFor finite training data, the models are distinguishable, while for infinite training data they are not [Fig B.1]. The exact same holds for finite and infinite interventional data [Fig 1].\nAre these experiments then good benchmarks for causal discovery based on intervention when the causal model can already be inferred from the non-interventional training data?\n1.2) The simplicity of the representation learning setup doesn\u2019t convince that the method is applicable to more real-world settings with more complicated encoders. Additionally, some important details on this experiment are missing (see below).\n1.3) All experiments show the effect of intervention on the cause p(A). No experiments are given for intervention on p(B|A). Does the method then still work?\n1.4) No experiments for more than two random variables are performed in this paper.\n2.A) Prop 1 shows that the expected maximum likelihood gradient for one conditional probability distribution is zero if the graph is correct and that CPD is not intervened on. Subsequently a claim is made that this effectively reduces the number of parameters and thus that adaptation is faster. However, the zero-expectation gradient may still be non-zero and even large on the small intervention sample. It is unclear to me why they therefore can be excluded in the number of parameters. Furthermore, whether the online likelihood is large will depend not only on generalisation, but also on the training convergence, since not empirical risk minimization is used, but SGD with a fixed number of steps. Thus, even though the authors prove that the method will converge to the causal graph with lowest online likelihood, it is unclear why this is necessarily the correct causal graph.\n2.B) In appendix E it is mentioned that cycles can occur in causal models. However, it is unclear why factorization (76) is still correct in the cyclic case. Perhaps I am misunderstanding, but it seems to me that p(x) = \\prod_i p(x_i | x_{Pa_i}) only makes sense for a DAG. Hence, how would the online likelihood be computed for cyclic models?\n\nSuggestions for improvement:\n- Could you explain in what realistic settings we would have access to data from a large number of different interventional distributions?\n- Could you show a plot similar to Fig 1, but with online likelihoods? Such a plot may be more indicative of the ideal episode length than Fig 1.\n- Could you provide details on the representation learning experiment? In particular: (1) is \\theta_D different in the train dataset and each interventional dataset? (2) How do the gradients of theta flow through the meta-update steps?\n- A reference to Appendix B appears missing in main text.\n\n[1] Nonlinear causal discovery with additive noise models. Hoyer et al 2009"}