{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a simple rule to ignore the so-called conflicting gradients in addressing multi-task learning problems. The underlying idea is very straight-forward if two gradients are contradictory (the angle between them is > \\pi) then one should not be considered.\n\nI have some questions here. Let's assume the gradients passing to PCGrad are g1, g2, and g3 (in that order).\n\n1- In PCGrad, we start with g1 and will keep g2, or g3 if their direction complies with g1, isn't it better to keep the gradient that minimizes the loss more (or say has a bigger norm) instead? Maybe one should first pick which gradient is more important and then use that to start PCGrad.\n\n2- One may argue that if <g1,g2> <= 0, <g1,g3> <= 0, PCGrad will ignore both g2 and g3, hence, intuitively it should converge slowly and probably not generalizable. Can you comment on why I should not worry about this?\n\n3- Is it possible to generalize PCGrad to work with more than two gradient vectors? One imagines that if it can work with more than 2 gradients, maybe a better and more robust algorithm can be achieved.\n\nBased on the above, I believe that the paper in its current form has not completely studied the problem and hence I am giving the paper a weak reject score at this stage.\n"}