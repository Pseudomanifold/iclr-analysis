{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an adversarial attack for perturbing\nthe actions or observations of an agent acting near-optimally\nin an MDP so that the policy performs poorly.\nI think understanding the sensitivity of a policy to\nslight perturbations in the actions it takes or the\nobservations that it receives is important for having\nrobust learned policies and controllers.\nThis paper presents an empirical step in the direction\nof showing that such attacks are possible, but in the context\nof the other adversarial attacks that are possible, this is\nnot surprising alone and would be much stronger with\nother contributions.\nI think an exciting direction of new work could be to\ncontinue formalizing these vulnerabilities and\nlooking at ways of adding robustness across many\nother domains."}