{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows the efficiency of curriculum learning and using problem specific features for contour detection.\n\nThe authors consider a network trained for class-specific edge detection (e.g. outlining edges of roads in an image). They propose two problem domain tricks to improve the performance:\n- use curriculum learning by training the network to first detect the \"easy\" edges, i.e. edges found also using the Canny edge detector\n- add high frequency wavelet coefficients as additional feature maps to the convnet.\n\nThe two techniques prove important on two tasks:\n- modified MNIST edge detection\n- road boundary detection in aerial imaginery.\n\nMaybe the most important aspect of the paper is that shows that with little data (the real world aerial imaginary dataset had only 11 labeled tiles) manual feature engineering and smart cost function selection are still relevant. \nSince this is a common pattern in many application domains, such as specialized medical image processing where labeled data is scarce, the paper is important. However, it is not clear if the proposed chanes are needed when more labeled data is available and how much do they overfit to the small test set.\n\nThe paper could be strengthened by analysing the impact of the proposed problem-dependent CL and features versus the amount of available training data. Are they still relevant with 100 labeled images?\nThese experiments could be even run on the artificial MNIST set.\n\nMoreover, some analysis of result significance is needed. On the real-world dataset there is only 1 test case!! How much was the network tuned to properly work on it? Maybe the authors can run a  cross-validation to show that the results don't overfit to this one test image?\n\nMinor remarks: You refer to Stephane's Mallat book as Stephane 1999, this is wrong, his first name is Stephane and last is Mallat, please fix the bibliography and use Mallat 1999.\n"}