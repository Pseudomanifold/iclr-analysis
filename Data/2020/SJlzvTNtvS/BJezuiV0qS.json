{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new objective for one-shot neural architecture search (NAS) using gradient-based methods. The authors hypothesize that the finite differences approximation in the second-order approximation in DARTS [1] directs the gradients towards bad local minimas in the architecture space. To this end, the authors reformulate the architecture update rule in DARTS as a sum of the one-shot model training loss and the one-shot model validation loss (scaled by some factor \\lambda). The algorithm is evaluated on the same standard image classification benchmark used in DARTS[1].\n\nI vote for rejecting this submission, mainly for the following reasons: (1) The fixes they propose to previous gradient-based NAS algorithms such as DARTS, SNAS [2], etc., are not clearly motivated and justified, both in theory and practice. (2) Almost no novelty. Except the modified loss, I do not see any difference compared to first-order DARTS. (3) Since the authors used different hyperparameter settings compared to DARTS, it is not clear if the better results in the benchmark results come due to these hyperparameters or their proposed fixes.\n\nMain arguments\n\nWhat this paper is trying to improve, is solely based on claims made throughout the paper, and there are no empirical or theoretical evidences to support these claims. For instance saying that not solving \"exactly\" the inner optimization problem the bi--level settings is always worse than the counterpart, is not true for non-convex inner functions. Performing a few gradient steps to approximate the inner problem solution can act as a regularizer (see [3]). \n\nThe authors also mention quite often the term \"overfitting\". It is not clear in which level the overfitting occurs? Is it in the parameter space or in the architecture space (see [4])? Furthermore, in the NAS context (not only in the single-level optimization scenario), a low value of the validation loss that is minimized during the NAS loop, does not necessarily correlate with the true objective: minimizing the validation/test error of discrete architectures. [4] shows that actually the test accuracy of the final architectures does not correlate with the one-shot validation accuracy, and empirically demonstrate that there is an overfitting on the architectural level.\n\nFinally, I did not find the empirical results and experimental settings to be entirely clear and convincing. There are some crucial issues, such as: different hyperparameter settings compared to baselines in the final training of found architectures. Therefore, it is not clear where the contribution in better resuls comes from; or just one run of their methods in some settings in Table 1 and in Table 2.\n\nOther comments\n\n- In Section 1: SNAS [2] updates both w and \\alpha using all the training data\n-  Throughout the paper: One-shot validation accuracy or derived architecture validation accuracy when retrained from scratch? Which one do you refer with validation accuracy=\n- Section 3.3: How did you tune the learning rate (0.03 vs 0.025 in DARTS)?\n- What are the settings you use for your algorithm in Table 1?\n\nSuggested Improvements\n\n- Empirical results proving the claims made throughout the paper\n- Experiments showing how the performance of the architectures found by bi-level optimization relates with the solution returned by the inner problem, i.e. the number of gradient steps towards the approximated w^*.\n- Evaluation of their methods on other image classification benchmarks and other tasks, such as language modelling.\n\n[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang.  DARTS: Differentiable architecture search.  In ICLR, 2019.\n[2] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR, 2019\n[3] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning.  In ICML, 2018\n[4] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter. Understanding and Robustifying Differentiable Architecture Search. ArXiv, 2019\n"}