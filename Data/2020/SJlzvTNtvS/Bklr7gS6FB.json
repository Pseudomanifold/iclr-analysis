{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose a mixed-level optimization based NAS (MiLeNAS) to alleviate the gradient error caused by second-order gradient approximation in bilevel optimization. In MiLeNAS, the authors minimize both the training loss and validation loss to train the architecture parameters, which is different from existing methods that only minimize the loss of validation data. Equipped with the mixed-level optimization scheme, the proposed MiLeNAS has less training cost and yields better performance than the considered baseline methods. However, there are many questionable arguments. More explanations and experiments are required to demonstrate the effectiveness of the proposed method.\n\nStrengths:\n1.\tThe proposed MiLeNAS aims to alleviate the gradient error caused by second-order gradient approximation in bilevel optimization. By reformulating the NAS problem as a mixed-level optimization, MiLeNAS obtains better results than the baseline bilevel optimization methods.\n\n2.\tThe proposed MiLeNAS is computational efficiency. Compared with existing bilevel optimization methods, MiLeNAS has lower search costs.\n\n3.\tThe proposed MiLeNAS is a generic framework for NAS. Experimental results on two search space settings demonstrate the effectiveness of MiLeNAS.\n\nQuestions and points needed to be improved:\n1.\tWhat is the difference between the proposed method and the first-order variant of DARTS? Why does the proposed MiLeNAS yield better results than the first-order variant of DARTS? Why does MiLeNAS (0.3 GPU day) take less training cost than the first-order variant of DARTS (1.5 GPU day)? More explanations are required.\n\n2.\tThe authors argue that DARTS fails to converge to an optimal solution since the optimal w*(alpha) cannot be obtained via a single step training. However, MiLeNAS also uses the same single step training technique. Thus, this issue also occurs in the proposed MiLeNAS method.\n\n3.\tWhat would happen if the proposed MiLeNAS method uses the second-order approximation?\n\n4.\tThe results in Figure 3 are not convincing. For example, bilevel (1st) method achieves higher accuracy than bilevel (2nd) method, which conflicts with results in DARTS[1]. More discussions on this phenomenon are required.\n\n5.\tSome experiment results are very confusing. In Table 1, the authors provide four results of MiLeNAS but do not mention the differences among them. The similar issue also exists in Table 2.\n\n6.\tIn Section 3.3, the authors mention that the proposed method can further improve XNAS[2]. However, there are no experiments to verify this argument.\n\n7.\tSeveral state-of-the-art NAS methods should be compared in the experiments, such as PROXYLESSNAS[3] and P-DARTS[4].\n\n8.\tThe authors do not release the source code of the proposed method. The page of the released hyperlink is empty.\n\n\n\u2003\nMinor issues:\n1.\tThe authors mention that \u201cevolutionary algorithms and reinforcement learning-based methods require thousands of GPU days to achieve state-of-the-art performance\u201d. However, this argument is wrong. In fact, ENAS is a typical reinforcement learning-based method and only takes 0.5 GPU days to train the model.\n\n2.\tIn the third paragraph of Page 2, \u201cgap between the training loss and the evaluation loss\u201d should be \u201cgap between the training accuracy and the evaluation accuracy\u201d.\n\n3.\tThe structure of this paper can be improved. The main content of Section 2.3 is to describe how to demonstrate the effectiveness of the proposed method, which is more relevant to Section 3. Therefore, it is better to put Section 2.3 in Section 3.\n\n4.\tIn Figure 2, the caption should be put below the graphic.\n\n\nReferences:\n[1]\tLiu, Hanxiao, Karen Simonyan, and Yiming Yang. \"Darts: Differentiable architecture search.\" ICLR, 2019.\n[2]\tNayman, Niv, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi Zelnik-Manor. \"XNAS: Neural Architecture Search with Expert Advice.\" NeurIPS, 2019.\n[3]\tCai, Han, Ligeng Zhu, and Song Han. \"Proxylessnas: Direct neural architecture search on target task and hardware.\" ICLR, 2019.\n[4]\tChen, Xin, Lingxi Xie, Jun Wu, and Qi Tian. \"Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation.\" ICCV, 2019.\n"}