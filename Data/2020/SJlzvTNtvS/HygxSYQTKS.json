{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a new method for  Neural Architecture Search (NAS) by using a mixed-level reformulation. In the bilevel based NAS method, it has to completely train a network to update the network architecture alpha. Based on a reformulation, the proposed method relaxed the hard constraint in the bilevel NAS to a soft constraint.  As a result, it can update the model weights and architecture alternately. The results showed the proposed method achieved comparable results on CIFAR10 with much less training time and better accuracy on ImageNet transfer learning.\n\nThe proposed method seems very reasonable but somehow incremental. Instead of solving the w subproblem exactly, it relaxed the problem and solve the alpha and w subproblems alternately. In this case, a large speed-up is expected. However, reformulating Eq. (1)(2) to Eq. (4) is incremental and completely not new in the optimization field.\n\nSeveral of the statements in the paper is over-claimed. It says that \"The existing second order failed to converge to a optimal solution but the proposed method can be optimized more reliably\". There is no evidence to support this, neither theoretically or empirically. In the results, the proposed approximation obtained comparable results as the second order approximation method DARTS, e.g. in Table 1.\n\nIt also claimed that both single and bilevel optimization are special case of the proposed method. This is not correct. The proposed method is more like approximation solver for the bilevel optimization problem.\n\nOverall, the proposed method does have its value: it brings a first order solver for the NAS problem, and obtained good results with much less training time. However, the benefit is over-claimed."}