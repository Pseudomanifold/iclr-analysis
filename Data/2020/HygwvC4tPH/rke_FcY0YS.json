{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper aims to learn entity representations by aggregating all the contexts that an entity appears in based on English Wikipedia. \n\nThe idea is very simple, basically it represents each entity as a vector, and also represents each context as a vector, and maximizes the cosine similarity between the two vectors using a negative-sampling training objective. The training process is similar as word2vec, and they leveraged all the hyperlinks in Wikipedia, and used BERT to encode the context (instead of learning a context vector for each word). \n\nAs a result, the paper demonstrates that this set of entity embeddings are highly useful, and they were evaluated on 1) entity-level typing 2) entity linking 3) few-shot category reconstruction 4) answering trivia questions (TriviaQA).\n\nI think this is a nice empirical paper and the experiments are thorough. If they are going to release the entity embeddings, that would also benefit the community a lot and also encourage more research in this direction.\n\nI am a bit concerned about the novelty of the approach. It is a bit surprising that nobody has experimented with this before. It seems that Yamada et al took a very similar approach but used simple bag-of-words approaches to encode the context (instead of BERT).  To me, this paper may be better for the NLP community but it should be fine to the ICLR community too.\n\nI am also not completely sure how strong the evaluation results are indeed, esp. related to the comparisons with Yamada et al, 2017, given the approaches are similar. \n\n- It seems to be on par with or slightly worse than Yamada et al 2017 on entity linking.\n\n- For the category completion task, RELIC is doing much better than Yamada et al 2017 on more complex Wikipedia categories but I am not sure if it is a completely fair comparison. It\u2019d be great if the authors can discuss all the key differences between the two approaches, from the model design to all the experimental details, that would help clear out all these confusions. I have read the related work section but can't figure out all the details.\n\n- The entity typing results are very strong. I also like the TriviaQA experiments but the numbers are way behind the standard reading comprehension results. \n\n\nMinor comment:\n- Why sec 5.3 (effect of masking) is listed together with other evaluation tasks? Isn\u2019t better to move it to analyses/ablation studies?\n"}