{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a k-NN method for identifying corrupted labels, and then applies this k-NN in the representation space of a deep neural net rather than the original feature space. Overall the paper is well written and the results look quite convincing\n\nThe theory appears to be important (if somewhat straightforward-looking) contributions of existing k-NN theory to the corrupted labels setting, based on the key quantity the authors defined as S_k, the minimum k-NN spread.\n\nSince the theory highly depends on this quantity, the authors should after Definition 2 justify why they chose to base their results around S_k, and why intuitively it is the right quantity.\n\n- I encourage another experiment where the label corruption is not completely at random, that is it depends on the values of x itself.  \n\n- In particular I encourage a synthetic experiment where the authors look at corruptions with varying S_k (minimum k-NN spread) values, to empirically verify the their theory holds and this really is a meaningful quantity to consider in the label-corruption setting.\n\n- Why don't the authors show the results for vanilla deep kNN trained on the full (noisy + clean) dataset in their experiments. This seems important to ascertain the benefits that might be attributed to simply switching to kNN.  Or is deep kNN generally worse that the original model trained on the full dataset?\n\n- Why didn't the authors show the original model trained on the full dataset?\nIs it because it always does worse than all the baselines considered in the paper?\nI would expect it sometimes does much better than Control (eg. when noise rates are low), and this is the straightforward approach must practitioners would use.\n\n- Why don't the authors present the accuracy of the k-NN method at identifying corrupted datapoints vs the other methods that aim to explicitly identify the corrupted datapoints?\nIn general, it seems the authors did not compare other filtering baselines, which would be more related to their method, for example:\n\nLearning with Confident Examples: Rank Pruning for Robust Classification with Noisy Labels\nNorthcutt et al. (2017). https://arxiv.org/abs/1705.01936\n\n\n- It would be valuable to the scientific community if the authors can comment on: \nRolnick et al. (2018). Deep Learning is Robust to Massive Label Noise. https://arxiv.org/pdf/1705.10694.pdf  \n\n- Some similar looking ideas have been proposed in: \n\nGao et al. (2018). On the Resistance of Nearest Neighbor To Random Noisy Labels\nhttps://pdfs.semanticscholar.org/4227/918020c15b719c415e93eb63d436583f1745.pdf\n\nParvin et al. (2010). A Modification on K-Nearest Neighbor Classifier.\nhttps://globaljournals.org/GJCST_Volume10/7-A-Modification-on-K-Nearest-Neighbor-Classifier.pdf\n\nso the authors should contrast their method/analysis against those papers.\n\n\n- In Thm 1: \"w.r.t. X\" should be \"w.r.t. x\" (lower case)\n"}