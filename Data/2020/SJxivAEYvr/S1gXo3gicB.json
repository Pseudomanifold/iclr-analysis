{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presented reinforcement learning based style transformer for text style transfer on unpaired training corpus. The key idea is to combine RL with decoder-only transformer with a few of other enhancements. The experimental results show the performance gains over existing approaches. Text style transfer is quite crowded research area and there are quite rich previous literatures. Although the experimental results showed some advantages, I found the novelty of this paper is just a combination of several existing works. More importantly, I did not see any reasonably justification why some of choices are made. The whole paper reads like some engineering choices without much rationales. \n\n1) This paper is simply a combination of style transformer [Dai et al., 2019] using decoder-only GPT [Radford et al., 2019], and RL based encoder-decoder [Gong et al., 2019] using RL for unpaired text style transfer. These two formulated the overall framework and a few of specific empirical enhancements (such as some other rewards and warm start using (Sudhukar et al., 2019)) were used to improve the performance. In the perspective of scientific novelty, this work is limited. \n\n2) For most of model choices, there are lack of clear motivations and explanations what and why each of model choices were made. This is very important for developing a scientific model, which we would like to know why not the what. Also, it is also important to discuss the connections and key differences between this work and the existing works. \n\n3) Although authors showed a lot of baselines in Table 1, it is kindly of pointless to me to show these numbers. What the reader really need to see is some really relevant baselines and your approach performed better than them, which verified the proposed model choices.  It is important to take a close look at the numbers and try to provide any insightful take-home messages regarding the scientific merits of the proposed method. "}