{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In the paper, the authors propose a combination reward, which is composed of three parts, i.e., fluency, content, and style, for text style transfer.  Experiment results demonstrate that the proposed objective aligns well with human perception and has improved baselines. \n\nThe main contribution of this paper is the three designed rewards. The other techniques, such as transformer and reinforce, have been already explored in other style transfer papers. However, I am not convinced by the rewards proposed. Why is the style reward better than a style classifier used previous? If a language model (transformer) is incorporated into the model, the fluency score will be significantly boosted, even without the GPT-based reward. Why is BLUE a good reward for content? The paper just proposed these metrics but lack of intuition and explanation. In addition, I am not convinced of the way that weights/mix all rewards into reinforce.\n\nThe experimental results have shown some improvements over baselines. However, the observation is not consistent. For example, the model utilizes BLUE as rewards but fails to beat other baselines on BLUE on Yelp and Captions. From the ablation study, it indicates that the MLE training is very important. The model is also not able to beat MLE for BLUE. Finally, I suggest also use other metrics to measure content preservation (Santos 2018).\n\nSome related work to this paper:\n1. What Makes A Good Story? Designing Composite Rewards for Visual Storytelling\n2. Towards Coherent and Cohesive Long-form Text Generation"}