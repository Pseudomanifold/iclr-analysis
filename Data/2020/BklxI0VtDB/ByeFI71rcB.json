{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary:\nThe paper proposes an intuitive 2-layer hierarchy for robotic object search. The high-level policy does subgoal selection, whereas the low-level layer handles explicit control. Notably, the low-level policy is trained to be aware of both the subgoal and the final goal. The authors conducted a series of ablations, demonstrating the value of training the low-level policy to be final goal-aware, and empirically demonstrated the strength of their method compared to other baselines.\n\nDecision: Weak Reject. The idea is intuitive and seems to be empirically successful (on some metrics). My primary concern is that the work appears incremental when compared to the baselines HRL and HRL with Stop. \n\nI think the acceptability of the paper is contingent on whether the tuning of alpha is considered a sufficiently significant contribution. The authors themselves noted that their method (alpha = 1) is similar to HRL---differing only in the introduction of a termination signal. This in and of itself suggests that the main contribution of the paper boils down to learning a suitable choice of alpha to manage the termination signal. \n\nI would also like to better understand the distinction between the author\u2019s method versus HRL with Stop. Both methods have employ a low-level network capable of pre-emptive stopping. How, then, is the termination signal for HRL with Stop trained?\n\nIf the authors can convincingly demonstrate the novelty of the proposal to learn the terminal signal via extrinsic reward supervision, and if the other reviewers feel similarly convinced, then I would feel more comfortable re-evaluating my concerns about the significance of this work.\n\nI would also, in general, encourage a more thoughtful exposition of the results shown in Table 1. Can the authors posit/explain why, for example, High-Level Only performs so much better on AS than the other models, but so poorly on the other metrics? "}