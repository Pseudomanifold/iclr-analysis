{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a hierarchical approach to perform robotic object search (ROS). \nThe idea is to use a high-level policy which produces subgoals and a low-level policy which produces atomic actions conditioned on both the subgoals and the true goal, and which is trained with a weighted sum of the original extrinsic reward and a reward for reaching the subgoal. Subgoals are consist of different objects in the field of view which the robot can choose to approach. \nThe approach is evaluated on the House3D dataset, where it is shown to perform well. \n\nRecommendation: weak reject. \n\nThis isn't a bad paper, but I'm not sure it will be of broad interest at ICLR.\nIt is very specific to ROS problem and House3D dataset and doesn't seem to propose a general algorithm which can be broadly applicable elsewhere. The mechanism for generating subgoals and training the low-level policy is very task dependent (subgoals are constrained to be objects in the field of view, the intrinsic reward for training the low level policy is dependent on the size of the bounding box of the object defining the subgoal). While this probably accounts for improved performance on the House3D dataset, I think the audience of ICLR would be more interested in a general approach which can be used in many different domains (even at the cost of performing less well on a specific domain than something more tailored). This paper may be a better fit for a robotics conference. \n\nAnother point concerning the experimental evaluation. Sparsity of the rewards is mentioned as a main motivation for the hierarchical approach. However, there are a number of methods which use exploration bonuses to address this issue (pseudocounts, random network distillation, ICM etc. [1, 2, 3]). At least one of these should be included as a baseline.\n\nSpecific comments:\n- using two letters for denote a single variable is confusing, since it seems like a product. I.e. using \"sg\" to denote a subgoal, \"at_t\" to denote area. Please use a single letter and add subscripts if necessary to disambiguate.\n- in the various equations, please use \"\\log\" instead of \"log\" so that it is not italicized.\n- bottom of page 4: \"Q-leaning\" -> \"Q-learning\"\n- page 2: \"way pf\" -> \"way of\"\n- please use more informative names for Settings A/B\n- First paragraph in Section 2: \"hierarchical policy for the robot to perform the object search, motivated by how human beings conduct object search\". Saying the method is similar to how humans behave is a fairly big claim that should be substantiated by appropriate references, or not made at all.  \n\n\nReferences:\n[1] https://arxiv.org/abs/1810.12894\n[2] https://arxiv.org/abs/1703.01310\n[3] https://arxiv.org/abs/1705.05363\n"}