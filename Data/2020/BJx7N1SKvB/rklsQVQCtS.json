{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper investigates the asymptotic spectral density of a random feature model F(Wx + B).  This is an extension of existing result that analyzed a model without the bias term, i.e., F(WX). This extension requires a modification of the proof technique. In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators.\n\nPros:\n- This paper investigates an interesting problem and it successfully extends the existing work. The theoretical curve well matches the simulated curve.\n- The finding that mixture of nonlinearities gives better expected training error is interesting.\n\nCons:\n- The extension to the model with bias seems a bit incremental. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis.\n- The effect of mixture of activation functions is investigated in the \"training error,\" but I don't see much significance on investigating the training error thoroughly. Instead, people are interested in the test error. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. Hence, more expositions about why the training error is investigated should be provided.\n\nMore minor comment:\n- I guess the definition of Etrain  (Eq.(17)) requires an expectation with respect to the training data.\n- Assumptions of the activation function f should be provided; is it just assumed to be differentiable?, ReLU is included?\n- The definition of G(\\gamma) in page 6 had better to be consistent to that in previous pages.\n\n"}