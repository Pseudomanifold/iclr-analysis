{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work advocates for using natural language instead of visual features to represent state in reinforcement learning. Using VizDoom as a testbed, the authors extract semantic states (e.g. how many entities of which type are where)  from the game. These semantic states are then edited using a grammar into language descriptions for the learning agent. The authors compare models that rely on these language representations to those that rely on raw pixels and semantic maps and find that for most cases, the language representations outperform visual representations.\n\nWhile the hypothesis that natural language representations is an effective way to parametrize the state space is interesting, the experiments in this paper do not test this hypothesis. This is due to the following reasons:\n\n1. The authors do not use \"natural\" language, since the language here is generated from a simple template.\n2. Because the language is not natural, they correspond to precise, unambiguous, state representations (e.g. there is no difference from the model's perspective whether the utterance is \"you have high health\" vs. \"feat['high health'] == 1\"), which fundamentally simplifies the learning process --- the vision-based models have to learn how to extract precise state representations, whereas the \"language\" models are given these precise state representations.\n\nThe second point is particular important. For many learning problems (including this VizDoom experiment in the paper), language descriptions do not occur naturally. There are some works that assume that the model can produce language descriptions, which it then uses to facilitate learning (see https://arxiv.org/abs/1806.02724). However, in this case, the language descriptions are being provided by humans. If I understand correctly, the authors are comparing models that learn from raw pixels (e.g. baselines) to models that are fed precise, hand-extracted states (e.g. proposed). The results then are not surprising, nor do they test the hypothesis. The takeaway for me from this paper is that the authors engineered a clever feature extractor for VizDoom, and show that the state composed from the extracted features are more sample efficient to learn from compared to raw pixel values.\n\nI also have some feedback regarding the writing:\n- The introduction is too verbose. The paragraphs are disjoint, with independent overviews on deep learning, semantics, and NLP. Out of the 5 paragraphs, only the last one talks about the content of the paper. The introduction does not go into any detail about what experiments were actually run, and what the results are."}