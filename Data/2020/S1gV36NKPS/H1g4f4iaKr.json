{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis work proposes to study the effectiveness of natural language for representing state in reinforcement learning, rather than using vision-based or featured-based representations. The intuition here is natural: typical means of representing state often include lots of redundant information in order to be a sufficient statistic for next state and reward predictions, whereas language can be far more concise (and often flexible). The primary focus of the study is an empirical evaluation of the learning performance of a benchmark algorithm (DQN) using four different kinds of representations in the VizDoom environment: image-based, feature-based, semantic-segmentation-based, and language-based. The natural language state representation is generated by a semantic parser iterating over patches of the frames of the game and extracting relevant information. Experiments involve training a DQN to play several different levels of Doom given each state representation, with different levels of \"nuisance\" added to determine the effect of confounding information in the state input. Results indicate that in several cases, the language-based representation leads to improved (higher value policy in fewer samples) performance. Additionally, a study is conducted to investigate the impact of the number of patches used in generating the language-state, which reveals that patch-discretization has essentially no impact on learning performance in the level tested (\"super scenario\").\n\nVerdict: The paper brings a fresh perspective to the problem of representation learning in RL---the empirical study is on the thinner side, but I find the initial insights and results to be of sufficient interest to the community to lean toward acceptance.\n\nMore Detail:\n\nI enjoyed reading this paper. The empirical study is interesting, but I suspect the paper would be stronger if at least some of the PPO results (beyond Fig. 7) from the Appendix could make their way into the main paper. Additionally, I see that: 1) the feature-based method was not tested with PPO, and 2) a smaller number of levels are reported for PPO than DQN. Why was this the case? I suspect a plot showing the average performance across all levels (normalized over max value/reward) for both PPO and DQN would be indicative, and would fit given space constraints. I have a few other questions about the experiments, which I leave below under \"Questions\". Evaluation on some environment other than Doom would also strengthen the claims of the work. In particular, I am wondering whether environments where the generated language cannot easily approximate a sufficient statistic of state, and instead induces partial observability due to ambiguity or lack of expressivity. VizDoom is a case where, with the semantic parser used, the language is effectively a full characterization of state, but we can't expect this to always be the case. Perhaps even an environment like breakout would be useful for exploring this phenomena; in breakout, describing the precise configuration of remaining blocks is unnatural. Since ambiguity and implicature are necessarily features of natural language, I am wondering what the impact of these kinds of properties might be on RL in general.\n\n\nQuestions:\n\tQ1: As alluded to above, in an MDP, a state by definition contains all information necessary to predict $R(s,a)$ and $T( \\cdot \\mid s,a),\\ \\forall_a$. Language, on the other hand, is notoriously ambiguous (semantically, phonetically, sometimes syntactically), and often contains more or less information depending on the context, the speaker, or the listener. I am curious: is language appropriate for representing state on its own, rather than informing state estimation? That is, I think of the setup presented as defining a POMDP with language as the observation. VizDoom seems like a case where natural language can very nearly exhaustively describe the state, but other environments might not allow for such a neat characterization of state. I am wondering what the authors think about this point.\n\n\tQ2: In Figure 5, are the shaded regions depicting 95% confidence intervals, one standard deviation, or something else?\n\n\tQ3: The natural language representation seems to struggle in the health gathering case: any idea why?\n\n\tQ4: It is surprising how poorly the feature vector variation performs across the board. Any thoughts as to why?\n\n\tQ5: Figure 7 (and the corresponding experiment) is very illuminating! I did not expect those results, but they were helpful to see. Do we anticipate this trend across all of the levels tested, or is it something unique to the \"super scenario\"?\n\nComments:\n\n\tC1: I would be curious to see how combinations of the features might perform.\n\n\tC2: I would suggest aligning the y-axes in Figures 6 and 8 (making them all have the same scale). It would be easier to compare the three approaches. On that note, it would also be helpful to use subcaptions to denote which is the Language/Semantic/Image based representation, as the text title is quite small. Alternatively making the text larger in the plots would help.\n\nTypos/Writing Suggestions:\n\tAbstract:\n\t- \"we observe, is through\"::\"we observe is through\"\n\n\tSec 2. (Preliminaries)\n\t- \"approach, to a deep learning\"::\"approach to a deep learning\"\n\t- \"a forth method\"::\"a fourth method\"\n\n\tSec. 4 (Semantic...)\n\t- \"environment we\u2019ve\"::\"environment we have\""}