{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an optimization approach in which the optimizer computes the gradient on a given function yet uses another to decide a stopping time. Conceptually those functions are empirical errors on train and validation folds in the most common setting, although the authors seem to use other settings later in the paper to consider decentralized optimization schemes. The authors introduce a bound on the Wasserstein distance between the train and validation distributions in their analysis which plays a crucial role in their results. The authors use these results to motivate variants of existing optimization algorithms. \n\nThe paper is interesting but its message is a bit blurred to me. I had trouble pinpointing one main contribution, since the paper is split as theory (with some results) and a collection of slightly modified SGD type algorithms that are now impacted by this \"gradient somewhere / monitor progress elsewhere\". The theoretical results are worth reading and the idea appealing. \n\nThe paper also requires a *lot* of polishing. It has been sloppily written. For these reasons I am inclined to reject the paper and encourage the authors to improve their draft with a better formulation.\n\n\nMinor comments:\n- I have found the \"main contributions\" paragraph to be poorly phrased. Since the authors only monitor the validation loss and not the training loss, I do not think this falls into the \"standard\" definition of early stopping. \n- please use citet and citep consistently. \n- please add labels to figures and format them properly (e.g. SSGD on p.6) \n- unsure about the format used to display f_T(x_t) in p.6\n- Villani 2008 has over 900 pages. any page in particular?\n- Assumption 2.3 requires significantly more work... All bounds scale as G^2 (e.g. eq.9, 10,11), therefore an idea of what G's impact on the analysis sounds crucial. In Example 2.4 the authors start working out an example, but wouldn't it be more interesting to carry that out completely, e.g. for the KL? What kind of bound would that result in?\n- I find it disturbing that important comments on some of the crucial quantities (such as descent direction Eq.3) are left out of the algorithmic box... This defeats the purpose of having an algorithmic box.\n"}