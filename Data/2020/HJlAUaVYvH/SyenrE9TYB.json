{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "1. Contributions: \nA) Extension of robustness bound based on margin and Lipschitz constant of the network to arbitrary l_p norms. Significance: Low.\n\nB) High probability bound on adversarial robustness based on A) and McDiarmid's inequality. Significance: Low.\n\nC) Lipschitz constant bound for one-versus-all networks (OVAs). Significance: Low.\n\nD) Experimental evaluation. Significance: Low\n\n2.  Detailed comments:\nOriginality: Contribution A) is a simple extension of previously known bounds for the l_2 norm which is rather technical. As such It has questionable novelty and limited potential impact. The same goes for B) which is a simple application of a well-known concentration bound that holds for many statistical estimators based on averaging.\n\nAlso C) is a well-known alternative for multiclass classification (one-vs-all approach) and somehow defeats the purpose of representation reuse from intermediate layers of the network to perform classification.  In OVAs the prediction for each class depends on entirely independent values of hidden layers. As such it is not surprising that. a slightly tighter estimate of robustness can be obtained, as one does not have to worry about interactions between features from each independent sub-network.\n\nQuality: The paper is in general well written, but lacks any depth or clear important contribution. The propositions are rather technical and/or simple extensions. The proposal of OVA networks and the bound on the adversarial risk are two parallel ideas that could be explored and hopefully find a significant contribution.\n\nFor example, what could the authors say about local Lipschitz constant estimates? are they easy/difficult to compute? what new bounds could be derived in term of such constants? For OVA networks can there be some intermediate reuse of hidden layers and still get some improvement in certified robustness? perhaps only the final layers require a split to see the empirical improvement, in this way reducing the burden of training one network from scratch for each different class (ImageNet has 1000 classes so OVAs could be quite expensive to train).\n\nOr maybe for OVAs the complexity of each subnetwork can be substaintially less (e.g., less layers) to obtain the same accuracy/robustness?? It looks from the experiments that this is true but some theoretical insights about why it is the\ncase can yield a strong paper.\n\nClarity: The paper could see some improvements in notation and some erroneous claims:\n1. Authors claim that the robustness can not be assessed when f is non-convex. This is in general, false. There is a constrained maximization procedure. constrained maximization of convex functions is a hard problem.  Not to be\nconfused with certain instances of constrained minimization of convex functions that can be solved efficiently. The same in section 5.1. The true statement is that for SVM the problem becomes optimization of a linear function with some simple constraint.\n\n2. In section 3 it is not clear if the loss l depends on the label y or not. sometimes it appears as an argument, and sometimes it disappears. see for example equation (4).\n\n3. In proposition 2 the assumption is that l is monotonically decreasing and its range is [0, B). Does this mean that B is negative?? this makes the claims in this section seem weird. There is some problem here. Also McDiarmids does not require the Doob Martingale concept, which is not even defined or referenced.\n\nSignificance: I find the significance low as the work is incremental and has technical rather than theoretical statements, and lacks a clear contribution. I vote for rejecting this submission.\n"}