{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThe author show that lipschitz constants of the neural network can be used to bound adversarial robustness. They then propose an adjustment to the network architecture that, according to their assumption, would result in smaller lipschitz constant, which according to the theory would correlate with better robustness.\n\nComments:\n\"A practitioner can bound the worst-case adversarial performance of their model based on its (non-adversarial) validation-set performance and its Lipschitz constant, both of which can be measured efficiently.\"\n-> The Lipschitz constant can be measured efficiently only at the condition that you don't care about getting a tight one.\nThe method of Gouk et al. is a very loose overapproximation of the Lipschitz constant of the neural network (multiplication of lipschitz constant of each layer). \n\n- What happens if we want to obtain results for the 1-norm. Do all the arguments hold up, given that the dual norm is the infinity norm?\n\nNote: I did not assess the correctness of Proposition 2.\n\n\"Unlike the conventional OVA method, where each component classifier is trained in isolation, the networks used in our approach are still trained jointly using a softmax composed with the cross entropy loss function.\"\nHow is each network a one vs all if they are trained jointly? Is there an additional loss term on top of the joint softmax? Is it just that up until that point they are different networks?\n\n\"The core assumption this approach is predicated on is that each of the binary classifiers needs to solve a simpler problem than the original multi-class classification task.\" \n-> There is no validation of this assumption anywhere. This would improve the paper if there was some signs that the justification proposed by the authors have some experimental validation.\n\n\"the Lipschitz constant of the one-versus-all classifier is the `p norm of the vector of Lipschitz constants corresponding to each binary classifier\", end of page 4.\n-> A lipschitz constant, not THE lipschitz constant. This construction gives a valid lipschitz constant but it's entirely possible and very likely that there is a much tighter one.\n\n- Experiments in 5.1 are made on linear SVMs, which means that the lipschitz constant can be computed exactly but this doesn't really reflect how useful the bound will be on network with hidden layers, for which the lipschitz constant will have to be an approximation. This is a valid toy-example / sanity check but not a good validation of the method.\n\n- Experiments in 5.2 and 5.3 seem to indicate that the change in architecture proposed has at least some effect on the robustness to adversarial examples.\n\nNotation / typos:\n- In Proposition 1, on the left side of the inequality, l takes as argument a vector (f(x + eps)) and a coordinate, but on the right side and on the definition of l is a scalar function. Does that mean that the left side is the y-th coordinate? As in, what's in l is f_y?\n- Page 4, \"exmaples\"\n\nSome maybe interesting litterature that the paper might benefit from referencing/discussing:\nhttps://openreview.net/forum?id=HJguLo0cKQ, submitted to last years ICLR observed that ensemble of small models tended to be more robust than large models. I wonder if the reasoning in this paper might explain it at least partially.\n\n\nOpinion:\nAt the moment, the paper is quite confusing and hard to read, and it's not entirely clear what the crux of the paper, the new architecture is doing. It's also not validated where does the effect comes from.  I'm open to updating my opinion of the paper if these comments are addressed."}