{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "1. What is the specific question/problem tackled by the paper?\n\nThis paper proposes to improve the soft attention mechanism of Transformer models by making them more interpretable. The authors motivate this work by citing the work of [1] where it is shown that simply interpreting a higher attention weight to be \"more important\" doesn't hold up for biLSTM models with attention. To address this problem, this paper proposes a discrete/hard attention mechanism by using the Gumbel-Softmax trick to propagate gradients through the hard decisions. This model is called the Single Stream Discrete Transformer model. The authors then propose an extension to this model that combines regular soft attention together with the hard attention model which is dubbed the Two Stream Discrete Transformer model. The authors report results on two tasks 1) machine translation on the WMT and IWSLT datasets 2) language modeling on a synthetic/toy dataset. On the translation datasets results are shown to be slightly worse compared to the soft attention model.\n\n2. Is the approach well motivated, including being well-placed in the literature?\n\nI am not convinced by the motivation of this work. The authors motivate their work by saying that soft attention is not interpretable, however I do not see any empirical evidence or analysis of their proposed hard attention mechanism as being more interpretable. Moreover, the work they cite to argue that soft-attention is not interpretable [1] bases their analysis on a biLSTM model and not a Transformer. Moreover, the work of [1] simply computes adversarial attention weights to fool the model -  such adversarial methods can also be extended to hard/discrete attention weights (see e.g. the following work [2] which showed how to attack discretized models which are not any more robust than continuous models).\nI am also a bit confused about the motivation behind the Two Stream Discrete Transformer - what is the point of the hard attention if you also use regular soft attention along with it?\n\nIn my view the only reason to motivate sparse attention models would be the following: 1) attention over long sequences where full attention does not scale (this is the motivation behind [3] for example) and 2) better generalization than full/soft attention. Clearly the work doesn't tackle 1) and the experimental results do not show better generalization than full/soft attention.  \n  \n3. Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nThe paper does not support the claim that using hard attention is more interpretable, since I don't see any such empirical evidence presented in the paper. Moreover, their hard attention model will also be susceptible to the gradient based attack of [1] using ideas from [2]. Their experimental results are also limited and do not support either the model being more interpretable, or generalizing better or being able to scale better than soft/full attention.\n\n\nTypos & corrections (my comments delineated by *):\n- we can even the guarantee that any hidden state does not depend on input elements not being directly attended to. *extra the*\n- The structure of hard attention ensures that the receptive field is defined by the recursion r(i, l) = r(i, l \u2212 1) \u222a r(z^(l)_i, l \u2212 1) where r(i, 0) = {i} and z^(l)_i is the hard sample taken at layer l for position i. *This should be r(i, l) = r(i, l \u2212 1) \u222a r(z^(l)_i, l)* \n- The first layer representations are set to the corresponding word embedding, g^(0)_i = e_g(x_i) and h^(0)_i = e_h(x_i). *What is e_g() and e_h()? They don't seem to be defined anywhere. Are they learned parameters?*\n \nMissing citations:\nThe works of [3, 4, 5] are relevant and should be cited.\n\nReferences:\n[1] Attention is not Explanation by Jain et al (https://arxiv.org/abs/1902.10186)\n[2] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\n by Athalye et al (https://arxiv.org/abs/1802.00420)\n[3] Generating Long Sequences with Sparse Transformers by Child et al (https://arxiv.org/abs/1904.10509)\n[4] Look Harder: A Neural Machine Translation Model with Hard Attention by Indurthi et al (https://www.aclweb.org/anthology/P19-1290/)\n[5] Sparse and Constrained Attention for Neural Machine Translation by Malaviya et al (https://www.aclweb.org/anthology/P18-2059/)\n"}