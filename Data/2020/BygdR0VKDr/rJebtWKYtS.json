{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a Discrete Transformer. There are three main contributions (1) a discrete and stochastic Gumbel-softmax based attention module (2) a two-stream syntactic and semantic transformer and (3) sparsity regularization.\n\nThis paper makes some valid technical contributions but the key question I find myself asking is \u201cwhy?\u201d Performance results do not outperform regular Transformers and at best perform marginally/comparable. In this case, is there any reason to use the proposed method over the existing methods? \n\nThe motivation for each component proposed is quite unclear and vague. \n\nThe authors enumerate several use-cases (to check for bias?) in the conclusion. Assuming that these use-cases are valid, this is completely under-explored in this paper and should have fallen within the scope of the paper and not as cursory remarks in the concluding statements.\n\nPerhaps the most confusing thing is why is there a need to separate into syntactic and semantic streams? What do these terms mean in this context? I am aware that the authors cite Russin et al. but it is rather unclear how and why they incorporate these intuitions/motivation into their work. I think this work can be more self-contained instead of dropping a reference.\n\nAs of now, I am fairly unconvinced with the premise of this work. There are occasional mentions of interpretability but no resolution or any concrete statements to address if the proposed model is supposed to ameliorate the issue at hand. \n\nToo many things are going on in this paper, for reasons unclear except to maintain performance and to prevent degradation. There seems to be also nothing tangible to be gained.\n\nI would love to hear from the authors and will be glad to increase my score, even significantly, to a positive one if the authors can actually provide a convincing and compelling argument for their model architecture. As of now, the technical exposition is extremely confusing to readers. \n"}