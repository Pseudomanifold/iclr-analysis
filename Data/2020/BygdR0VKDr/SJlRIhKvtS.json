{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nSummary: \nThis paper presents two modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. The first modification is to switch from soft attention to hard attention using a Gumbel-Softmax, which forces the model to attend only to important parts of the input without spreading small amounts of attention throughout. The second modification is to split the model to have two separate streams -- semantic and syntactic -- where the syntactic stream uses a soft attention mask to define the distribution that the hard attention in the semantic stream is sampled from. Hard attention also allows the size of the receptive field to be used as a regularizer, which can further encourage sparsity in the attention map, as shown in the paper. Experiments are performed on a synthetic language dataset and on machine translation. The proposed model achieves similar performance to baselines despite the use of hard attention, and the embeddings extracted from the model perform better than the soft-attention transformer embeddings on a chunking task.\n\nOverall: Reject.\nOverall, I feel that this paper splits the gap between interpretability and a novel contribution and fails to truly satisfy either. If the goal is better interpretability, then there should be a stronger focus on that and proper comparisons with other methods for interpretability. Conversely, if the goal is to provide a new and effective contribution, then more experimental evaluation and more of a contribution is required. In either case, there should be better empirical comparison of other hard-attention mechanisms. I also found the paper somewhat unclear and poorly motivated, particularly the later sections. I still am unclear why the split into the two streams occurred or if it is useful beyond increasing the number of parameters in the model. I do like the use of hard attention and the Gumbel-Softmax for interpretability purposes, however.\n\nClarity: Fairly poor. Notation is lacking at times, and a number of steps and concepts are not well motivated and not well described.\nSignificance: Low. I do not think that the contributions of this paper will have a significant impact.\n\nDetailed questions and comments:\nSection 2.\n- A quick internet search shows a number of other papers that use Gumbel-Softmax-based hard attention, but there is no mention of these papers in the related work (e.g., [1]). Given that the use of the Gumbel-softmax for hard attention is introduced as a novelty here, the comparison to previous work should be more detailed and comprehensive.\n\nSection 4.\n- What are p(y|x) and p(y, z|x) referring to? What are y, x, z here?\n\nSection 5.\n- What is the motivation for splitting into these two streams? \n- Doesn\u2019t this just (roughly) double the number of parameters in the model? \n- Why do different embeddings need to be used for the two streams?\n- Doesn\u2019t the use of soft attention in the syntactic stream negate the interpretability benefits claimed before for hard attention?\n- How does the semantic stream consist of a \u201cfixed\u201d network if the syntactic stream specifies what to attend to? Why would the generated syntactic distribution be fixed?\n- Does the syntactic stream encode syntactic information? What about the architecture implies that it will?\n- Does the semantic stream encode semantic information? What about the architecture implies that it will?\n\nSection 6.\n- As mentioned above, I think this paper would be greatly improved by focusing on either interpretability or novelty. Personally, I would choose the former. What is necessary for both, however, is to compare against other hard attention methods, whereas this paper simply shows ablations of the proposed model and comparisons to a single transformer baseline. How do I know that using Gumbel-Softmax hard attention is more interpretable or more performant than other types of hard attention?\n- Why would the single-stream model perform better on the IWSLT dataset?\n- What do the transformer and single-stream discrete transformer t-SNE embeddings look like?\n- Given the lack of keys and queries in the semantic stream, it\u2019s not particularly surprising that it underperforms when used as an embedding layer; however, it seems like it must learn some amount of information about syntax to be able to get 83% on this task.\n- Please include more examples of words and their nearest neighbors in the appendix.\n\n[1] Fast Decoding in Sequence Models Using Discrete Latent Variables. Kaiser et al. 2018.\n"}