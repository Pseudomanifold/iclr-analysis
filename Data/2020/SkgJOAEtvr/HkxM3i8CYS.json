{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates the question of internal consistency in emergent communication. In other words, the paper aims to answer the question \u2018how is emergent communication improved if we enforce the constraint that an agent must speak in the same way that it listens?\u2019 The paper explores three methods of enforcing internal consistency: self-play, shared embeddings, and symmetric encoding/decoding. They find that, while internal consistency does not help generalization to unseen objects, it does allow agents to generalize over conversational roles (i.e. to perform well as a speaker despite only being trained as a listener).\n\nI have been eagerly anticipating a paper on this topic \u2013 it seems silly that the listening and speaking modules in traditional emergent communication research are completely disjoint. I think coming up with methods/ architectures that combine these two capabilities is an important research direction. I also think paper is very well-written and structured, and it reads very well.\n\nHowever, I have several concerns with the paper. First, half of the results center around the hypothesis \u2018internal consistency helps agents generalize to unseen items\u2019. While ultimately this hypothesis is disproven, it\u2019s unclear as to why this might be expected in the first place. The only justification of this hypothesis I could find in the paper is the sentence \u201cIt is conceivable that <internal consistency> might improve performance even though each <agent> remains in a fixed role.\u201d In my view, the fact that this hypothesis is \u2018conceivable\u2019 is a rather weak argument for it to be such a prominent part of the paper. Thus, I don\u2019t think this half of the results add much to the overall paper. \n\nI also have mixed feelings about the use of \u2018self-play\u2019 to enforce internal consistency, and how it relates to the core result of the paper: \u201cthe proposed constraints enable models to generalize learned language representations across communicative roles, even in the case of where the agent receives no direct training in the target (test) role\u201d. In short, I think the phrase \u201cno direct training\u201d is misleading, as the self-play itself is almost a form of direct training, and thus the result isn\u2019t very surprising.  \n\n More specifically, each agent \u2018Alice and Bob\u2019 are composed of two modules: a speaking module and a listening module. During normal training (say, in the shape environment), Alice speaks and Bob listens (achieving a reward if Bob selects the right shape, which is backpropagated to Alice), and thus the Alice\u2019s speaker module and Bob\u2019s listener module are updated. Alice\u2019s listening capabilities will be equivalent to a random agent, as her listening module is still randomly initialized. Now, during self-play, Alice\u2019s speaker module is trained with her listener module (I believe in the same way as it is trained with Bob\u2019s listener module) to achieve high reward. This listener module is then tested against Bob\u2019s speaker module (which is also trained via self-play). To me, this process isn\u2019t the same as the paper\u2019s narrative of \u2018we only train Alice to be a speaker, and she learns to listen!\u2019 This is especially true since, without parameter tying, the choice of saying that listener module \u2018belongs to Alice\u2019 is arbitrary (since it\u2019s completely separate). An equivalent way of framing this result would be \u201clanguage learning is transitive: if we train agent A to speak to agent B who listens, then train agent C to listen to agent A and agent D to speak to agent B, then agents D can perform well with agent C\u201d. With this framing, the result is much less surprising (in fact, it would be surprising if this weren\u2019t true). \n\n\nFinally, the three methods of enforcing internal consistency are not tested independently --- shared embeddings are only tested on top of self-play, and symmetric encoding/decoding is only tested on top of the other two. While this does make the paper more concise, I suspect another reason for this is that the self-play is the core driver of performance, and without it the other two methods don\u2019t do much. I\u2019d like this to be explained more explicitly in the paper.\n\nOverall, I really like the problem the paper is tackling, however I have some issues with the framing of the paper in relation to the self-play constraint, and subsequently with the importance of the results. Thus, I do not recommend acceptance in the paper\u2019s current form. \n\n\nQuestions:\n-\t\u201cWe set shared embedding agents to always use the self-play objective, because otherwise its equivalent to the baseline agent\u201d -> it\u2019s not clear to me why this is the case. Can this be elaborate on?\n-\tShouldn\u2019t the final row of Table 4 read \u2018Trans, shapes\u2019? \n\nSmall fixes:\n-\tThis assumption is a reasonable -> is reasonable\n-\tSection 5.1.2: \u201cWe observe a no clear trend associated with the shared embedding module (sometimes it helps, sometimes it hurts)\u201d -> I don\u2019t see any results on shared embeddings in Table 2? \n"}