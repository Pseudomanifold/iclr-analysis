{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary \nThe paper proposes several methods for uncertain plan reconstruction, which involve completing a sequence of actions, in which some of the actions are missing, and others are observed with uncertainty and specified with distributions. Several methods are proposed that learn word2vec-style co-occurrence models where the probability of neighboring actions is modeled given observations at a single timestep. Distr2vec represents such observations as a full distribution, RS2vec uses beam search to select the most likely actions from the distribution as observation, and the greedy method selects the most likely action as observation. Alternatively, an LSTM approach is evaluated that takes in the distributional observations (encoded via distr2vec) and predicts the missing actions. Hierarchical softmax is used for prediction.\n\nIt is shown that the proposed approach outperforms the greedy method when the observations are likely to be erroneous, while the greedy method performs better for high-entropy observations as well as observations that are less erroneous. The beam search approach needs a large number of samples to be competitive, which makes it computationally expensive. The LSTM approach is competitive, but takes longer to train than a simple co-occurrence model.\n\nDecision\nThe paper proposes a potentially promising approach, but is poorly presented, evaluates mostly on toy data, shows modest improvements over baselines, and lacks strong baselines. In addition, the paper is poorly motivated and ethical issues are ignored (see below). Due to these significant issues with methodology and evaluation, I recommend reject at this time.\n\nFinally, most of the prior work on plan completion, the problem that the paper addresses, appeared in venues such as AAAI/IJCAI. While I am not closely familiar with this literature (and therefore unable to access the relevance or novelty of this paper), I can only recommend resubmission to one of these venues, where the paper might hopefully get a more fair assessment.\n\nPros\n- The paper proposes to tackle plan completion with probabilistic, instead of deterministic observations, which is a promising direction well-motivated by practical settings.\n\nCons\n- The choices made when designing the method are poorly explained and strong baselines are missing.\n- The experimental evaluation shows that the method outperforms the baselines only in contrived toy settings. Arguably, the high-entropy case, where the action distributions are noisy but often correct, is closer to the practical settings than PER, where the action distributions are corrupted with arbitrary mistakes.  However, the greedy baseline outperforms the proposed method in the high-entropy case. This is confirmed by the experiment on the real-world dataset (with a real-world perception system?) where the greedy baseline performs comparably to the proposed method. \n- The importance of video surveillance, while being the motivation of the paper, is not discussed. On this note, I would like to make clear that as machine learning researchers it is our duty to conduct research responsibly and consider its implications on the real world and society. The paper needs a discussion of why (and whether) video surveillance is a worthy goal and related ethical issues.\n- The paper contains numerous style issues, such as improper handling of parenthetical citations. For instance, in the last paragraph of Sec 2, the author list is duplicated for all 7 cited papers as in \u201cHodges and Pollack designed machine learning-based systems Hodges and Pollack (2007)\u201c In all 7 sentences, the first reference should be replaced with \\citet{<HP07>}, and the last reference should be removed. On page 10, a wrapfigure from the previous page causes incorrect formatting, which could be fixed by moving the figure, or, in the worst case, removing the white space after the figure with \\vspace{-20pt}.\n- The paper is about 9.3 pages and as such should be judged with a higher standard according to the ICLR guidelines. I do not see a justifiable reason for this paper to exceed the 8 page limit. \n\nQuestions on methodology\n- Eq (1) is extremely confusing. Notation Pr(a|b), where Pr is highly suggestive of \u2018probability\u2019, which is very non-standard given that a and b are themselves distributions and not random variables! The text states that the equation expresses \u2018log probability of distributions\u2019. What is a probability of a distribution? What is the graphical model assumed in the paper? It is further stated that Pr(a|b) represents the similarity between a and b. Is the similarity the same as conditional probability? Why is that the case? Finally, it seems that the model Pr is meant to be parametrized and Eq (1) is the objective for further optimization over Pr. This is not explained. \n- Furthermore, it seems that the conditional probability of two distributions is never actually needed in the paper - defining that would involve defining a distribution over distributions, and is unnecessary for the purpose of this paper. Rather, the derivation this paper attempts only needs a conditional probability of a random variable given a distribution (whatever that means) in eqs (1,2). The random variable can have either a zero-entropy distribution (as in prior work) or a full distribution, as proposed in this paper, however, in both cases eq (2) is computed the same way, as the cross-entropy of distributions. To handle the fact that such variable can have a non-degenerate distribution, eqs (1,2) should be re-written in terms of expectations over such distribution.  If derived this way, eqs (3,4), which notice that minimizing cross-entropy is the same as minimizing KL divergence as the entropy of the target distribution is a constant, are unnecessary.\n\nQuestions on comparisons\n- The LSTM baseline seems to be the most natural baseline for this task as it directly learns to output the target values. However, it is never described in detail and not evaluated on the real-world dataset, which makes it hard to evaluate the proposed method. Is the LSTM single-directional? If not, it is missing certain observations. A bi-directional model that makes predictions based on future and past observations would mostly certainly do better than single-directional. It is stated that the LSTM takes 29.5 secs to train. Was truncated back-propagation through time considered for training? Truncated back-propagation allows to cut both training time and memory consumption. Finally, it is stated that the LSTM encoder is pre-trained with distr2vec. Is the distr2vec representation frozen after pre-training? How well does an end-to-end procedure perform?\n- In section 4.3, it is stated that resampling makes RS2vec more computationally expensive. Is the baseline without resampling (just with beam search) more computationally efficient? Was this baseline considered? Additionally, while beam search is computationally expensive as it needs to consider the whole sequence, one can design a simple baseline that is both efficient and powerful by simply taking top S actions from each action individually. This ensures that no actions will be repeated and thus the information is used optimally, thus avoiding both problems of RS2vec. Why is it necessary to consider samples from the trajectories in the RS2vec approach instead of this simple baseline?\n"}