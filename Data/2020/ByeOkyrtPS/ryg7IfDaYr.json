{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper proposes an action affinity method for plan recognition from a sequence of videos. The method attempts to learn embeddings for distribution of actions. The author proposes a loss function by combing: 1) average KL divergence between log likelihood of an action at certain time step and those at future time steps, and 2) hierarchical softmax loss. Experimental results on a real world dataset and a synthetic dataset are conducted to show that the proposed method has a better sample and computation efficiency. \n\nI think the general direction of the work seems interesting. My main concern exists in novelty/significance. The loss of minimizing KL divergence seems to be very intuitive, and the other part of loss using hierarchical softmax loss does not have justification. It simply mentions that using such a loss results in better performance due to previous work. As a result, from a technical perspective, I think the work seems somewhat incremental. "}