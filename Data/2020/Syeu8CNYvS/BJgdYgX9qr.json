{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "To tackle data scarcity issue in biological assay modeling, this paper proposes a method to episodically train Deep Kernel Learning models such that at meta-test time the models requires less examples. \n\nThe proposed Adaptive Deep Kernel Learning extends Deep Kernel Learning (end-to-end train Gaussian Process/Kernel Ridge Regression using Neural Network's final layer embedding) by:\n\n1. using a learnable task-specific kernel generator network \n2. applying episodic few-shot/meta learning to train the system end-to-end\n\nHere are the major issues for rejecting:\n\nModel:\n- Section 3.1 and 3.2 about \"Task Specific Kernel\" are very hard to follow.\n  -- (line 170) equation 12, What is the size of the MLP that is used as the kernel network \n  -- Is there an ablation to show the contribution of using learnable kernel network vs hand-picked kernel for ADKL-GP and ADKL-KRR? \n  -- (line 181) \"where U is a set of unlabeled inputs\" Where does the unlabeled inputs come from?\n  -- Is there an ablation about |U|  <= 50 and its effect?\n\nExperiments and Datasets:\n\n- The ProtoMAML code provided by the authors has only a few lines of code. And in the comment, it says: \"It turns out that ProtoMAML is the same as the light version of BMAML with one particle.\" Such implementation detail should be mentioned in the main paper.\n\n- Section 5 lacks details. \n -- How each meta-train episode is formed? Did they stochasticall sample a subset of molecules for each episode?\n -- What is the total number of tasks in the meta-train and meta-test split respectively? Are there any overlapping between the two splits?\n\n\nPresentation Issues:\n- Section 2 (line 76) \"We extended it (Deep Kernel Learning) to few-shot learning and discuss its advantages over the metric learning framework\" This sounds like the author developed a brand new framework, while the rest of paper is about proposing a specific realization of few-shot learning: Few-shot Deep Kernel Learning, which meta-learn through a differentialble kernel learning process (with the task kernel adaptation network novelty). \n\n- Section 3 (Figure 1) \"The blue and orange colors show the procedure...\" However there are more than two colors (including different shades of blue and orange colors) in the figure, which makes the figure hard to parse .  \n\n- Section 4 (line 201) \"DKL methods lie between neural networks and kernel methods\". To me, at high-level, DKL adds kernel learning as a differentiable layer in the end of a neural network. See how a similar model is categorized in Bertinetto et al. ICLR 2019 paper \"Meta-learning with differentiable closed-form solvers\":  \"The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression,as part of its own internal model, enabling it to quickly adapt to novel data.\" The delta of the paper is adding 1) Gaussian Process and 2) task-specific adaptation. If Bertinetto et al. categorize their method as part of a deep network then I think so should this work.\n\n- Section 4 (line 239) \"Our work goes beyond ... by proposing ADKL: a data-driven manner for computing the correct kernel for a task.\" This is probably a grammar mistake. \"a data driven manner for\" sounds odd. \n\n- Section 5 (line 272) \"Fig 2 highlights three aspects of the collections that make them better benchmarks for evaluating the readiness of FSR methods for real-world applications relative to toy collections.\" What is the other benchmarks the authors are comparing with? Are they generic benchmarks for all types few-shot regression tasks/applications beyond biological assays (e.g. computer vision tasks like: object detection). Maybe defining the proposed benchmark as \"complimentary\" and \"bio-assay application focused\" would be more appropriate. \n\n"}