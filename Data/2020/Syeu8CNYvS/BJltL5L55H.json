{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #6", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The submission is at the intersection of few-shot learning, kernel regression methods, and computational biology.\nThe main contributions are:\n  - A few-shot learning algorithm combining several ideas and features:\n    - Combining metric learning (shared across tasks) and kernel regression within each task\n    - Learning a task representation by maximizing an estimate of the mutual information between the train (support) and valid (query) sets of a task\n    - The addition of learned, synthetic \"pseudo-examples\"\n  - Two datasets for few-shot regression, from real-life biological assays\nThe proposed algorithm outperforms (or is competitive with) mainstream few-shot learning methods on a synthetic 1D regression dataset, as well as the two proposed datasets.\n\nThis paper should be accepted, because it significantly expands the field of few-shot learning by proposing both a novel problem to tackle (few-shot regression from a high-dimensional, noisy input), with public datasets, and novel algorithm to solve it (combining several recent advances in different sub-fields of machine learning).\n\nOverview\nThe overall problem is clearly stated, as well as its main challenges: noisiness of the data, different behaviors of the same input across tasks.\nDespite the complexity of the proposed algorithm, its different pieces and their motivations are clearly motivated, introduced, and tested in ablation studies. I liked the clarity of section 2.\nThe \"related work\" section is clear and presents a good overall picture of the field. Additional papers that may be of interest:\n  - Learned hallucination (Low-shot learning from imaginary data, Wang et al., CVPR 2018) seems to relate to the \"pseudo-representations\"\n  - TADAM (Task dependent adaptive metric for improved few-shot learning, Oreshkin et al., NeurIPS 2018) is another example of task representation used in conjunction with metric learning (in the context of classification, not regression, though)\nThe proposed datasets are an interesting new benchmarking task, that naturally requires learning a task description, I hope it will get traction.\n\nQuestions:\n  - The backpropagation through the kernel regressor (in order to train the parameters of the embedding function and other networks) seems unexpectedly straightforward. Is that only because there is a closed-form solution for the optimal regressor? Were the specific algorithms (KRR, GP) chosen because of that? Or could something like MetaOptNet (Meta-Learning with differentiable convex optimization, Lee et al., CVPR 2019) be used to relax that constraint?\n  - What \"generalization guarantees of kernel-based models\" (l. 122) would be applicable here? Do they hold even when the kernel is applied on top of a learned embedding ($\\phi_\\theta$), or even when it depends on a trained model itself ($C_t$)?\n  - What does \"correlations > 0.8\" mean exactly? (l. 258, 263)\n  - In the \"Binding\" dataset, is it possible that the same protein is used in different bio-assays? In that case, are the different experiments \"merged\" into the same task, or be considered different tasks? Could it be possible that tasks involving the same protein would be in different (meta-)splits, or has that been taken care of during the data collection?\n  - In the meta-test splits, how are examples split between the train/support and valid/query parts of each task?\n  - How were the specific architectures of the different neural networks designed, or selected? Between $\\phi$, v, r, $MLP_\\rho$, the space of hyper-parameters seems huge, and the effects of these choices might be drastic.\n\nAdditional feedback\n  - The caption of Figure 1 could (re-)introduce a definition of the notation. U and C_t for instance have not been introduced yet.\n  - I'm not sure I agree that FSDKL only \"share[s] characteristics with the metric learning framework\", I see it more as being in that framework, but incorporating other elements as well (like other methods do, e.g., TADAM or RelationNet).\n  - The horizontal axis of Figure 4 (a) and (c) are not clear until we see Table 4 of the appendix, and suggest an ordering of the different configurations, rather than 10 different categories. I'm not sure how to improve it though, maybe letters instead of numbers?\n\nOn notation:\n  - On l. 155, is $\\phi$ the same as $\\phi_\\theta$, or a different embedding function for x?\n  - In Eq. 11, should $D^t_{trn}$ be $D^{t_j}_{trn}$ instead in both terms? Similarly, $D^{t_j}_{val}$ in the first term?\n  - In Eq. 12, and l. 173, $\\phi_x'$ suggests it is a different embedding $\\phi'$ of the same $x$, if we want to convey that there are two inputs x and x' instead, $\\phi_{x'}$ may be clearer.\n  - In Fig. 4 (a) and 5, the vertical axis is labeled $\\gamma_{mine}$ instead of (I assume) $\\gamma_{task}$."}