{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a new framework for solving few-shot regression problems. The proposed framework is based on deep kernel learning, which lies in the intersection of neural networks and kernel methods. The authors introduced adaptive deep kernel learning, which learns kernel for multiple task collection and computes the correct kernel for a task in a data-driven manner. The method is evaluated on three tasks collections \u2014 Sinusoids (synthetic dataset), Binding (real dataset of 5717 task where each task represents a binding affinity of small molecules against a given protein) and Antibacterial (real dataset of 3255 tasks where each task represents antimicrobial activity against given bacterium).\n\n\nPros:\n1. The proposed framework introduces a new adaptive method for few-shot drug discovery regression problems. \n2. The paper addressed the question of uncertainty estimation for drug discovery tasks.\n\n\nCons:\n1. The authors only evaluate the performance of the proposed method against other DKL-based methods. They do not consider a comparison with widely used in computational chemistry classical machine learning methods such as random forest, gradient boosting, SVM (which is also a kernel method), etc.\n2. The experimental results in Tables 1-3 show only marginal improvement for real datasets. Also, it\u2019s not clear if the numbers in the Tables 1-3 are on the original scale or on the transformed scale. To estimate how well the models perform it\u2019s useful to transform the targets back to the original scale. \n3. Overall the paper is written in a somehow confusing manner and some details in the description of the experiments important for understanding are omitted.\n\nQuestions:\n1. How well the proposed method performs against classical machine learning methods?\n2. How was MSE in Tables 1-3 calculated? \n3. It would be useful to see the histogram of MSEs instead of just a single number.\n"}