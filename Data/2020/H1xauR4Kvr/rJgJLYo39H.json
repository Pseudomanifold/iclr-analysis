{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this work, the authors develop the discriminative jackknife (DJ), which is a novel way to compute estimates of predictive uncertainty. This is an important open question in machine learning and the authors have made a substantial contribution towards answering the question of \"can you trust a model?\" DJ constructs frequentist confidence intervals via a posthoc procedure. Throughout, the authors provide excellent background and exposition. They develop an exact construction of the DJ confidence intervals in Section 3.1. This is an intuitive approach that the authors explain well. Next, they explain and then develop the concept of higher order influence functions. They do a great job of communicating this concept. Section 3.4 provides the theoretical guarantees for DJ. The related work section is extensive and thorough. The authors have thoughtful experiments that demonstrate positive attributes of DJ. \n\nI suggest that this paper is weak accepted. On synthetic and real data, the DJ empirically works well, based on Figure 4 and Table 2. In addition, the theoretical exposition is very clear and compelling. The intuition provided in Sections 3.1 and 3.2 helps readers really understand what's going on, then 3.3 and 3.4 give theoretical justifications of the utility of DJ. \n\nHowever, I have a few suggestions for improvement that lead to the \"weak\" acceptance. First, I'll cover minor quibbles, then more major points. \n\nIn Figure 1: At first, the blue dots and blue shading were not clear to me. In the legend, maybe explain that the blue shading indicates coverage and the blue dots indicate regions of higher/lower discrimination. \n\nIn Equation 6: Looks like there is a misplaced parentheses. I think the last two terms of the equations should be Q(Vn-)+Q(Vn+) not Q(Vn-+Q(Vn+))\n\nIn Equation 7, and throughout: I think a better notation for the function would be \\mathcal{I}^{(1)}_{\\hat{\\theta}} rather than \\mathcal{I}^{(1)}_{\\theta} since the influence function is a derivative with respect to the optimal parameters. \n\nBelow equation 8: you should probably have an additional k exponent in the numerator of the kth order influence term, i.e. = \\frac{\\del^k \\hat{\\theta}_{i, \\epsilon}}{\\del \\epsilon^k}\n\nIn Theorem 1: could you calculate this without \\grad L(D, \\theta), since L a function of \\ell ? Maybe mention this in the appendix\n\nIt could be nice for an appendix study on how approximating the Hessian impacts performance for cases when we can compute the Hessian exactly. \n\nTable 1 could be expanded to include comparisons on computational bottlenecks and if there's retraining in these other methods. \n\nIn Figure 4, please indicate the order of the IF used in the DJ procedure. DJ(m=?)? \n\nMajor issues: \n\nWhy weren't the other jackknife procedures used as baselines as well? I realize DJ has advantages compared to them, but an apples to apples comparison would be useful. For some researchers, LOO CV might not be prohibitive. This could be a chance to really sell your method: if it does well enough compared to more expensive LOO jackknife procedures, that would be a compelling reason to choose DJ. \n\nCould you please check this reference and let us know if it substantial is different from Influence functions that you develop? \"Higher order influence functions and minimax estimation of nonlinear functionals\" Robins 2008 DOI: 10.1214/193940307000000527 Robins et al. develop a way to compute higher order influence functions, which you do claim you're the first to do \"to the best of your knowledge.\" \n"}