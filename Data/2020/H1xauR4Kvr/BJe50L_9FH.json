{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose using influence functions to efficiently estimate pointwise confidence intervals for regression models. Their central idea is to construct a confidence interval around a point $x$ as a function of how much the model $f$ changes around $x$ when individual training samples are left out. Their technical innovation is to combine a marginal error term that does not depend on $x$ (which ensures coverage) with a local variability error term that does depend on $x$ (to allow for greater variability in areas where the model is more uncertain and there is less data). The authors provide experimental support for the superiority of their method (in terms of coverage and discrimination) as well as theoretical support for consistency.\n\nIn my opinion, the ideas in the paper are exciting; the paper is clear and well-written; and the experimental evaluation is quite comprehensive in terms of baselines. However, I have concerns that prevent me from recommending an accept at this time:\n\n1) Similar recursive formulations for HOIFs have appeared in the literature, so the claims in Section 3.3 should be toned down. See for example lemma 3 in Giordano, Jordan, and Broderick, 2019 (https://arxiv.org/abs/1907.12116); or for an older reference that\u2019s a bit more specialized, see Debruyne, Hubert, and Suykens, 2008 (http://www.jmlr.org/papers/volume9/debruyne08a/debruyne08a.pdf).\n\n2) The paper refers repeatedly to how their proposed method can be applied to deep learning models and, in particular, state-of-the-art deep learning models. In my opinion, the evidence in the paper does not support these claims. The largest experiments are run on neural networks with 100 hidden units, and it is not clear how to scale up their method to state-of-the-art models and large datasets. In particular, the Hessian-vector computations and the need to iterate through the dataset scale poorly with model and dataset size, and efficient approximations for these are non-trivial and an area of active research.\n\n3) Related to the above point, the theorem statements apply to general differentiable loss functions. Are they true in such a general setting? For example, in Section 3.3, the assumption that the inverse Hessian needs to be positive definite is noted but this does not appear in the theorem statements. More importantly, it seems like some notion of strong convexity is required; for example, does the convergence of the von Mises series in Appendix A require that the parameters don\u2019t change too much with $\\epsilon$? This might not be true in a non-convex model. The paper also elides the fact that the global minimizer $\\hat{theta}$ cannot in general be computed in non-convex models like neural networks.\n\nThe paper is otherwise compelling to me, and I believe that the above points can be remedied by being more careful and circumspect with the claims in the paper."}