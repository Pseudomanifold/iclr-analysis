{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on the calibration for the regression problem. First, it investigates the shortcomings of a recently proposed calibration metric [1] for regression, and show theoretically where this metric can be fooled. Later, it introduces new metrics for measuring the calibration in regression problem which are instantiated from similar idea as the ECE calibration metric [2] used for classification problem. The paper defines the uncertainty as the mean square error and like the ECE idea, it divides the samples into different uncertainty bins and for each bin, it calculates RMSE of network output uncertainty. The RMSE versus variance of estimated uncertainty is depicted as the reliability diagram.\nTo summarize the calibration error as the numbers, the paper proposes two other metrics as ENCE and CV that explain the overall calibration error for the network and calibration error for each sample, respectively. It also shows the effectiveness of the metrics, on the synthetic data where the calibration metric proposed in [1] can be fooled VS. more informative metrics like proposed reliability diagram with RMSE and mVAR.  The other experiment is conducted on object detection problem where the uncertainty of bounding box positional outputs is considered as the calibration for the regression problem. \n\n\nPros:\n1- Investigating theoretically the shortcoming of the calibration metric for regression is interesting\n2- Proposing the new metrics specifically  designed for regression in novel, however it needs more accurate investigation. \n\nCons:\nThe paper is not well-written and the experiments and discussions are not support the ideas, more specifically I can mention several concerns:\n\n1- Motivation: The flaw of the previously proposed calibration metric is not explained clearly. The paper only discusses in which scenario, the metric cannot work properly considering. However, the considered assumption (for instance the output of uncertainty estimation from the network has uniform distribution) can never happen in real scenario. Then the importance of redefining of calibration method is not clear.\n\n2-  Accuracy of Proposed Method: The measure proposed for calibration (Eq. 8) should be shown it will converge to the definition of calibration proposed in Eq.5, which is missing in the paper. \n\n3- Lack of Clarity in some Parts: The paper introduces the  new architecture for adding the uncertainty output to the regression network. But this new architecture is not explained clearly. I suggest the authors add more details about this part.\n\n4- Justification: The paper trains the uncertainty part of the network with NLL loss function and later fine-tunes the output with temperature scaling method. But this calibration  method is not related to the proposed metric which is claimed it has better calibration clarity. Then the importance of using the new metric to define the calibration is not clear. \n\n5- Experiments: The experiment setup needs more accuracy. The paper should investigate the shortcomings of the previous metric in the same setting as the new proposed metric. In experiment Sec. 4.1, the settings for obtaining the output of the network uncertainty is different then comparing the results are not fare.  The experiments for real scenario is not wide enough. It just shows that the parameters get calibrated. However we expect to see more results about the CV metric and its importance to define.  \n  \nOverall, I think this paper should be rejected as 1) the novelty of proposed metrics are not enough. 2) the motivation and justification of why the proposed metrics is better than previous metrics is not clear. 3) the experiments are not supporting the idea.\n\nReferences:\n\n[1]  Kuleshov, Volodymyr, Nathan Fenner, and Stefano Ermon. \"Accurate Uncertainties for Deep Learning Using Calibrated Regression.\" International Conference on Machine Learning. 2018.\n\n[2] Naeini, Mahdi Pakdaman, Gregory Cooper, and Milos Hauskrecht. \"Obtaining well calibrated probabilities using bayesian binning.\" Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015.\n"}