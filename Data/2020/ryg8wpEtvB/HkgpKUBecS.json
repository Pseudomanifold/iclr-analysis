{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is concerned with uncertainty calibration diagnostics and re-calibration methods, applied to neural network regression. It is motivated by a flaw in the diagnostic proposed by Kuleshov+ 2018 (abreviated K2018 below), as explained around eq4, and proposes a replacement diagnostic for uncertainty calibration quality (sec 3 before 3.1). It then specialises by considering the class of uncertainty prediction schemes qualified as \"direct\" uncertainty modeling (defined sec1), in which the network predicts the parameters of a parametric distribution over the target output, typically mean and variance of a Gaussian. For this class of schemes, it proposes a recalibration method (sec 3.1), which consists, as shown eq12, of rescaling (with a single parameter $s$) the square root of the variance predicted by the neural network. It then presents experiments (sec4) to demonstrate that the motivating flaw can be evidenced by their diagnostic, and fixed by their recalibration method where it makes sense (ie where predicted uncertainties are not random, i.e. statistically independent of the empirical uncertainty).\n\nThe formal treatment of the issue appears sound, as do the experiments. Nevertheless I believe the paper should be rejected for the following reasons. (1) I fail to see the relevance of the alleged flaw, as explained below. I may have missed something and am looking forward to the author's response. (2) The proposed palliatives are not easy to extend to other forms of uncertainty prediction, even for \"direct\" (parametric) uncertainty modelling schemes, as they are specialised (def of ENCE eq8 and STD coef of variation eq9). (3) The advantage, mentioned just below eq2, of not averaging over data points, breaks down immediately since binning is needed; this is a consequence of outputting probability distributions over continuous variables, in the case of K2018, and of outputting a continuous parameter, $\\sigma^2$, in the present paper.\n\nRegarding (1): I am not surprised that, using K2018's setup, it is possible to recalibrate (i.e. minimise a calibration measure), on a given calibration data set, a model that outputs random uncertainty distributions. The calibration metric alone is not sufficient to measure the goodness of the model: that should take into account prediction sharpness, or test NLL. For a model with random uncertainty output, recalibrated with K2018, I would expect that the miscalibration appears on these other metrics evaluated on the test set.\n\nSuggestions for improvement (did not affect my assessment):\n- the training procedure is mostly common to sec4.1 and sec4.2, and explained relatively clearly: on training set, first train mean using eg L2 loss, then variance output against NLL loss, then recalibration parameter against NLL loss. This procedure could be discussed more explicitly and jointly for both experiments.\n- polishing the text and grammar"}