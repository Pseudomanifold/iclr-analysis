{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors investigated the gradient exploding/vanishing issue in RNNs. \nThey analyze the gradient flow along the temporal dimension as well as the depth dimension. \nA new recurrent architecture called STAR is proposed to resolve the gradient vanishing/exploding issue.\n\nAlthough I think the designing of the new recurrent cell is novel, this paper does not distinguish itself from a  \nlarge number of papers trying to address the RNN gradient vanishing/exploding issue. \n\nOn one hand, the gradient analysis in Section 3.1 is not novel. The two-dimension gradient flow analysis seems to be a trivial extension of \nRNN gradient analysis along the temporal dimension, which appears in many related pieces of literature [1][2][3].\nOn the other hand, both the gradient analysis and the toy example hold under a very special condition ( h=0, W being orthogonal). \nIn practice, this is rarely the case unless spectral constraints are applied to weight matrices [3][4]. \n\nEmpirical results show that STAR can achieve better performance with a deeper structure. \nHowever, it seems for most experiments stacking more layers does not improve the model performance. \nSome time more layers make it worse even for STAR. It would be more convincing if the authors could provide empirical \nresults showing deeper RNNs have better performance than shallower ones.\n\nIn general, I think this is an interesting paper but with limited contribution.  I vote for rejection.\n\n\n[1] Arjovsky, M., Shah, A. and Bengio, Y., Unitary evolution recurrent neural networks. In ICML 2016 (pp. 1120-1128).\n\n[2] Zhang, J., Lin, Y., Song, Z. and Dhillon, I., Learning Long Term Dependencies via Fourier Recurrent Units. \nIn ICML 2018 (pp. 5810-5818).\n\n[3] Mhammedi, Z., Hellicar, A., Rahman, A. and Bailey, J., Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. \nIn ICML 2017 (pp. 2401-2409).\n\n[4] Zhang, J., Lei, Q. and Dhillon, I., Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. \nIn ICML 2018 (pp. 5801-5809)."}