{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates the magnitude of gradients when training deep RNNs, and presents a new type of gated cell named the STAR unit to address the gradients vanishing/exploding problem. The STAR unit is shown to ease the training difficulty and thus enable stacking more RNN layers.\n\n1. The major contribution of this work is to extensively study the gradient vanishing/exploding problem of multi-layer RNNs. However, it is not clear on what dataset the authors did the experiments in Figure 2. Besides, I encourage the authors to validate these results over more datasets for more compelling evidence.\n\n2. As mentioned by the authors, the GFRNN addresses the training difficulty of multi-layer RNNs via vertical skip connections. Please compare the STAR net with this model in experiments.\n\n3. As far as I can tell, the STAR unit is very similar to GRU (both removing the cell state and the output gate). Could the authors shed more light on the originality of the STAR unit over GRU?\n\n4. In Table 1, the authors did not include the performance of some state-of-the-art methods, e.g. tLSTM [He et al. 2017], BN-LSTM [Cooijmans et al. 2016], and sTANH-RNN [Zhang et al. 2016]. All of them yield better accuracy than STAR on the pMNIST dataset. A minor problem is that Table 1 is not discussed in the text.\n[He et al. 2017] Wider and deeper, cheaper and faster: Tensorized LSTMs for sequence learning.\n[Cooijmans et al. 2016] Recurrent batch normalization. \n[Zhang et al. 2016] Architectural complexity measures of recurrent neural networks.\n\n5. On the gesture dataset, the authors might also compare the STAR net with [Zhang et al. 2018]."}