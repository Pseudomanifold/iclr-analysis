{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper aims to tackle few-shot learning by revisiting the simple finetuning baseline, which is supposed to suffer from overfitting when there are only few-shot training samples. They empirically show that in the few-shot learning scenario, finetuning prefers a low learning rate and adaptive gradient optimizers. They also show that finetuning the whole network benefits more in the case of cross-domain few-shot tasks.\n\nThe paper should be rejected because\n1) its writing lacks clarity. The overall structure of the paper requires a significant revision. The authors mix experiments, \nbaselines and their proposed technics in the experiment section, making it difficult for me to find where\u00a0the proposed approach is. I was expecting an approach section which explains a) technics they proposed when fine-tuning the network,\u00a0 b) how the technics solve the issue of overfitting. The presentation of the experimental results is bad too. For example, it is hard to interpret so many numbers in Table 1. What should be compared in this Table? Why many results of different methods are identical in Table 1? Why does fine-tuning works better with VGG16 than with ResNet? In section 3.7, the authors finally start to explain their technics but that's too late. I am confused about which results are achieved with those finetuning technics.\n\n2) little novelty. I can see little novelty in this paper. The normalization technic is from Qi et al. 2018. Using lower learning and Adam optimizer are obviously not new either.\u00a0\n"}