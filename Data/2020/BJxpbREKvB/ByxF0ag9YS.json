{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\n\nThis paper studies fine-tuning methods in few-shot learning. It considers several fine-tuning choices and concludes that fine-tuning is helpful for few-shot and cross-domain few-shot tasks. The results are competitive with those meta-learning based methods.\n\nEven fine-tuning is not a completely novel thing in transfer learning, it is indeed under-explored in few-shot classification. A solid study of baselines approaches in few-shot learning would be very helpful to the whole community.\n\nHowever, I have some serious concerns with the experiments:\n\n1. The experiments lack fair comparisons to achieve a solid conclusion. In Table 4 (the major Table to draw conclusion of this paper), Fine-tune (Ours) uses VGG-16 as backbone, while Baseline/Baseline++/MatchingNet/ProtoNet are using 4-layer ConvNet, which is not a fair comparison. As an empirical study paper, solid ablation studies are needed to get a convincing conclusion that could be helpful to the community. Therefore, it is better to have results for methods with the same backbone, and conduct experiments on more datasets (tieredImageNet, ImageNet).\n\nIn recent years different few-shot methods are proposed with many different backbones, even some of them are not clearly specified, this could be one of the main reasons that baselines are not fairly evaluated. Getting a fair comparison through solid experiments is the value of a empirical study.\n\n2. The novelty of this paper over Chen et al. seems unclear, both in the introduction and the whole paper. Chen et al.  has done very similar experiments as this paper, and the conclusion is also extremely similar. Is this paper improving over Chen et al., by using low learning rate, Adam, and fine-tuning the whole network? I think those is too trivial for an ICLR publication. It's basically network training engineering. The \n\n3. \"Note that we do not compare the performance of our method with the state-of-the-art algorithm in the high-resolution single-domain and cross-domain tasks because the performances for these tasks are not reported in the corresponding papers.\" I think this can be addressed by using their source code or reimplementing them. As an empirical study paper, it is important to include more recent state-of-the-art methods to convince people. Although I guess the conclusion might not change much but it's necessary.\n\n4. Minor issues:\n1) 2.1 Notation should be improved.\n2) The use of word \"retraining\" is confusing, is it equivalent to \"fine-tuning\"?\n3) The captions in tables could be less redundant.\n\n\nOverall, the paper did not compare baselines with common few-shot methods with the same backbone, and on more datasets to make the conclusion solid. Also in my opinion the novelty over Chen et al. is either unclear (or a bit trivial if the network engineering part is the novelty). I vote for rejection in its current form.\n"}