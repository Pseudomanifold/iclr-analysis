{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a few-shot learning method based on fine-tunning, and the experiments are verified on both the standard low-resolution images and high-resolution images. With carefully ablation studies, the authors find some reasonable ways to fine-tune a few-shot learning model.\n\n1. The novelty of the paper:\nIt is a bit hard to evaluate the novelty of this paper. Learning a few-shot model based on a pre-trained model has been investigated in some papers, e.g., [1-4]. By fine-tuning the whole network based on the pre-trained backbone, most methods get improvements. The authors propose to \"revisit\" the fine-tune, it is more welcome to include some discussions that why fine-tune can facilitate the few-shot learning (understand fine-tune from a novel way), and when fine-tune will/will not help.\n\n2. About the method:\nIt is not very clear for the method part. After pre-training the network, it seems the method is fine-tuned with both seen and unseen/novel classifiers. Does it require the meta-train instances? Is it learning in a transductive way? It is better to write the objective function in section 3.4.\n\n3. Experiments:\n3.1 The authors provide comprehensive experiments to investigate the method, e.g., with different network architectures. How will the number of instances in the target few-shot task influence the fine-tune method? For example, when there are more ways and shots in a task.\n3.2 Dataset: It is interesting to test how the number of classes in the meta-train set influences the ability of the fine-tune based model. For example, will the fine-tune methods get better performance given a larger meta-train set (e.g., on tieredimagenet)?\n\n[1] Task dependent adaptive metric for Improved Few-Shot Learning\n[2] Meta-Learning with Latent Embedding Optimization\n[3] Learning Embedding Adaptation for Few-Shot Learning\n[4] Few-Shot Image Recognition by Predicting Parameters from Activations\n[5] On First-Order Meta-Learning Algorithms"}