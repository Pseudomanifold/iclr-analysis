{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I believe this paper should have been desk-rejected, as it is 9p long in the main text (above the suggested limit of 8), plus 4p of bibliography, for a total of 13p, above the hard limit of 10p. It is concerning that all figures are so squeezed as to be unreadable in print version. In the remainder of my review, as well as my rating, I will ignore the length issue, and leave it to the AC to include this fact.\n\nThis paper is concerned with methods to enforce sparsity in neural networks (specially CNN for image classification) at training time. It is motivated by the issue that L1 regularisation violates incoherence and irrepresentability conditions (cf sec1 and sec2 below eq3). The approach taken to encourage structural sparsity is to formulate the training dynamics in terms not only of the neural network weights $W$, but also of their sparse version $V$.\n\nI recommend (other than for the length issue) this paper for publication. My review needs to be relativised as I am not familiar with the literature on inverse scale spaces. Factors for my recommendation are: (1) the paper is clearly written (apart from an important language issue, cf below) and laid out, (2) the problem is well-motivated and contextualised (although I cannot judge novelty), (3) experiments, including analytic and ablation experiments, seem well designed and conducted, (4) experimental evidence shows that the proposed method is efficient, (5) the formal treatment is strong and thorough, with relevant literature cited.\n\nSuggestions for improvement\n- faulty language definitely stands in the way of understanding, in several places. There is about one syntax error in every paragraph of the paper. Sentences are too long as they span several lines, and should be broken up for clarity. In particular, the use of \"that\" and gerunds vs present tense is mostly erroneous. Eg: \"while the other set of parameters learning\", \"structural sparsity set $\\Gamma$ that\", \"comparable test accuracy as\", \"that the whole iterative sequence\", \"which drives $W_t$ closely following\", \"whose global convergence condition to be shown\" etc.\n- to allow non-specialists to read this paper, you should more clearly explain and illustrate the spaces in which different variables are, starting with $V$ and $\\Gamma$.\n- the paper lacks a conclusion which should bring together several strands of argumentation\n- I strongly suggest publishing the source code, not just promising to release them upon request. The methods described here require some skill to be coded, especially I cannot see how to easily re-use existing library components; this makes it hard for the ideas in this paper to be applied and diffused.\n- it is insufficient to state that \"official source code\" was used, as in table 1; this should definitely state the (Github or other) URL where the code is available"}