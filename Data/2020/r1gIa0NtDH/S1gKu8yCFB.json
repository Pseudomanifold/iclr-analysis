{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper the authors present a new generative model for audio in the frequency domain to capture better the global structure of the signal. For this, they use an autoregressive  procedure combined with a multiscale generative model for two-dimensional time-frequency visual representation (STFT spectrogram). The proposed method is tested across a diverse set of audio generation tasks\n\nOverall, The idea of generating audio from 2D spectrogram is original but in my point of view the use of STFT is not appropriate in this context, especially with its lossy criterion. Given the clarifications and the author\u2019s responses, I would be willing to increase the score. \n\n\nDetailed comments:\n\nPro: \n\nMitigate the problem of generating signal using only local dependencies (on a narrow time scale) and this by capturing high level dependency that emerges on larger timescale (several seconds) using spectrogram.\n\n\n\nCons: \n\n(1)The use of  STFT is not justified why not wavelet spectrogram to capture both scale and time?\n(2)It is still confusing how the use of high resolution spectrogram improve the lossy representation?\n(3)If you increase the STFT hope size you come back to the main problem that you are trying to resolve (i.e,  the bias towards capturing local dependencies) \n"}