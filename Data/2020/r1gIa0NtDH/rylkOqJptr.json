{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work treats 2-D spectrogram as image and uses an autoregressive models which factorizes over both time and frequency dimensions. \n\nDetailed comments:\n\n- MelNet is not a \"fully end-to-end generative model of audio\". It generates the spectrogram and relies on other algorithmic component (Griffin-Lim or gradient-based inversion) to generate raw audio.\n\n- MelNet can model the long range structure for unconditional generation of speech, but its audio fidelity is not as good as autoregressive or non-autoregressive models on raw waveforms. The major reason is that MelNet discards the phase information which is useful for high-fidelity speech synthesis. It would be more interesting if MelNet jointly models the magnitude and phase information. \n\n- The mixture density networks are well known. One may omit the details (or put them in Appendix) in Section 3 for space reason. Overall, the paper is clearly written, but it can be shortened in several ways.\n\n- \"making the use of 2D convolution undesirable.\"\nIt's still unclear the conv2d is useful or undesirable for modeling spectrograms in generative tasks. Even in recognition tasks, I have seen different results from different papers for different settings,  e.g., In Deep Speech 2, conv2d is useful to reduce WER in ASR. \n\n- Missing connection in related work: previous conditional generation methods (e.g., Tacotron, Deep Voice 3) are autoregressive over time, but assume conditional independence over frequency bins. \n\n- There is TTS experiments on the demo website, but I didn't find any details. For example, where does the conditional information (**aligned** linguistic feature) come from?\n\nMy major concern is about the usefulness of the model:\n\n1) The unconditional speech generation is an uncommon & less useful task in general. If the task is purposely constructed, the learned representation is more useful than the generation itself (e.g., van den Oord et al. 2017). However, the authors have not demonstrated the usefulness of the learned representation for any downstream task. \n\n2) The MelNet is autoregressive over both time and frequency. Thus, it is as slow as autoregressive waveform models at synthesis with worse audio fidelity, which make it less preferred in potential TTS applications."}