{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "[Summary]\nThis paper proposes two data augmentation methods that combine cutout [1] and sample paring [2] for training CNNs, Copyout and Copyparing. The authors evaluate their methods on the CIFAR-10 dataset.\n\n[Pros]\n- Data augmentation is an important regularization method for training diverse NN models\n\n[Cons]\n- The main issue is novelty. What are the differences of the proposed methods from CutMix [3] and RICAP [4]?\n- Only CIFAR-10 was used for evaluation. The results on ImageNet-1k are essential.\n- In recent papers, data augmentation methods for training CNN backbones should be validated on various architectures and downstream tasks such as object detection and semantic segmentation.\n- The method description is not specific.\n\n[1] Devries and Taylor.  Improved regularization of convolutional neural networks with cutout, ArXiv 2017.\n[2] Inoue, Data augmentation by pairing samples for images classification, ArXiv 2018.\n[3] Yun et al. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ArXiv 2019.\n[4] Takahasi et al. Data Augmentation using Random Image Cropping and Patching for Deep CNNs, ACML 2018.\n\n"}