{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers the problem of model-free reinforcement learning in discrete-time, linear-quadratic Markovian mean-field games with ergodic costs. The authors begin by establishing the existence and uniqueness of the Nash equilibrium in such a setting, and then proposes an mean-field actor-critic algorithm with linear function approximation. The proposed algorithm is shown to converge linearly with high probability under certain standard conditions. \n\nThe paper is novel in the sense that it extends the recent previous works on model-free learning of MFGs to continuous state-action state spaces, while showing linear global convergence under certain conditions. To my knowledge, the previous works either considers the discrete state-action spaces [Guo et al. (2019)] or has only convergence to local Nash equilibrium (NE) [Jayakumar and Aditya (2019)]. However, I have the following concerns and suggestions for this paper.\n\n1. Some claims of contribution is not very accurate. For example, the paper claims that the proposed algorithm does not require a simulator but only observations of trajectories. However, to invoke Algorithm 2 (mixed actor-critic), one has to fix the mean-field state \\mu, which would have required a simulator for running the mixed actor-critic algorithm. Otherwise, the \\mu could change if completely following the trajectory. This is the same setting as in some previous works like [Guo et al. (2019)]. The authors may want to double check if this kind of high level claims are accurate or not.\n\n2. The problem setting is not very well stated in Section 2. \n1) The authors should better call the problem at the beginning of Section 2 a \"linear-quadratic mean-field N_a-player game\", instead of a \"linear-quadratic mean-field game\", to differentiate from Problem 2.1 below. \n2) The dimensions and assumptions of A, B, Q, R are also not mentioned until Section 3.1, which is also slightly breaking the reading flow. \n3) The policies are also not clearly defined -- e.g., are they stationary or non-stationary, random or deterministic? And are we considering symmetric Nash equilibrium only (which should be so according to the later parts), i.e., all policies \\pi^i are the same?\n4) In the definition of Problem 2.1, the Nash policy \\pi^\\star is stated without even defining what the Nash in such a problem is. Similarly, after problem 2.2, Nash equilibrium is mentioned again without defining it. To address the issue, the authors may want to rewrite the cost function in problem 2.1 as J(\\pi,\\pi'), where \\pi and \\pi' are two arbitrary policies, and \\pi' is not necessarily the Nash policy. Then x_t' (instead of x_t^\\star) is the trajectory generated by \\pi'. \n5) The authors should not mention problem 2.2 right after problem 2.1. Instead, the authors should add a problem called LQ-SMFG (linear-quadratic stationary MFG), which is basically problem 2.2 but the goal is to simultaneously find \\mu^\\star and \\pi_{\\pu}^\\star. For such a problem, the objective function can be written as J(\\pi,\\mu), where \\mu basically serves the same role as \\pi' in the suggested modification to problem 2.1 above. The original problem 2.2, which is the subproblem of finding \\pi_{\\mu}^\\star given \\mu according to Section 3, should be put after introducing \\Lambda_1, as it is exactly what \\Lambda_1 is solving. The paper should then completely focus on this LQ-SMFG instead of LQ-MFG, as explained in the next point.\n6) In Definition 2.3, it should refer to LQ-SMFG mentioned above, as \\mu does not even appear in problem 2.1. In addition, the definition of \\Lambda_2 is also not clear. The authors may want to state it more clearly, e.g., using a one-step definition as in [Guo et al. (2019)]. \n\n3. The mixed actor-critic algorithm (with linear approxiamtion) for the subproblem D-LQR for evaluating \\Lambda_1 is not well motivated. \n1) For example, the authors should better highlight the difficulty of having the drift terms. The authors do show through propositions 3.3 and 3.4 how they decompose the objective into a standard LQR problem (J_1) and the problem w.r.t. a drift term (J_2). However, it is not clear why this is a must. In particular, why can't we just simply apply the natural actor-critic algorithm on the joint space of K and b? \n2) Also linear approximation is not mentioned in the main text, which should be discussed given its appearance in the abstract. Otherwise, it seems to be a low-hanging fruit given the previous works like [Yang et al. (2019)]. \n3) Why do the authors use natural actor-critic for finding K, but just classical actor-critic to find \\mu? This should be further explained. And instead of referring to appendix B repeatedly, the authors might want to directly state the assumptions needed for the input parameters on a high level to make the paper more self-contained (e.g., that the subproblem iteration numbers exceed certain threshold).\n\nSome minor suggestions.\n1) The discussion about why the Markov chain of states generated by the Nash policy \\pi^\\star admits a stationary distribution on page 4 is not clear. In general, don't we need additional assumptions like the ergodicity of the induced Markov chain?\n2) The claim that there exists a unique optimal policy \\pi_{\\mu}^\\star of Problem 2.2 on page 5 is not clearly stated with the necessary assumptions. The authors should at least mention that under certain standard conditions, etc.\n3) In (2.1), there should also be \\sigma\\in \\mathbb{R}.\n4) On top of page 6, the authors may want to give an example of the so-called \"mild regularity conditions\" (e.g., positive definite of Q and R, etc.).\n5) At the bottom of page 6, P_K is not defined clearly -- is it the solution to the Riccati equation? And how does it relate to the X in the Riccati equation of assumption 3.1(i)?\n\n"}