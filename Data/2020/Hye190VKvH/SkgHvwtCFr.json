{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript develops a metric-learning with non-Euclidean error terms for robustness and applies in to data reduction to learn diagnostic models of Alzheimer's disease from brain images. The manuscript discusses a metric-learning formulation as a quotient for reconstruction-error terms, how to optimize the quotient based on results from Wang et 2014, an iterated reweighted approach to circumvent the non-smooth part of the l1 loss in zero, and experiments on brain images of Alzeimer's disease.\n\nMy two main issues with the manuscript is that the theoretical part is written very imprecisely and that the experiments are not convincing due to the lack of good baselines and of statistical power.\n\nWith regards to the theoretical contributions, a fraction of the results in the present manuscript are trivial consequences or Wang2014, and yet it comes with errors in the statements. For instance, in equation (9), the present manuscript writes greater or equal, while I believe that it should be strictly greater, as in Wang. Theorem 1 and 4 seem almost the same thing, though with a contradiction between the two. Other statements are inaccurate: the authors claim some results on reaching global optima, while I believe that they can only claim that they reach stationary points. Theorem 2 and 5 seem to be the same thing.\n\nConcerning the iterated reweighted approach, I believe that this is non smooth only for g(x)=0, which is not covered by the theorems of Wang 2014. Is this algorithm needed? Note that Wang apply their algorithm with an l1 norm, ie non-smooth in zero, and do not report problems with out. The manuscript mentions that \"to inverted matrices that divide 0s, which routinely lead to inferior learning performance.\". I am not exactly sure what that means and I would need to understand better the problem. Also, the theoretical contribution that with the added the delta the algortihm converges, seems quite trivial: it seems to me that it is the eta trick.\n\nMinor comments: in algorithm 3, it would be useful to write the full expression of the equations, rather than just reference the numbers. Also, the computational cost of the eigenvectors at each iteration seems quite prohibitive.\n\nHow was the value r=3 selected?\n\nFigure 2 seem to choose that approaches have not converged: they final value is larger than intermediate values?\n\nHow were the p-values between cross-validation assessment of estimators computed? If it was done using standard paired t-test, this is incorrect are the folds are not independent.\n\nWith regards to the experiments, I worry that the model is not compared against simple baselines, such as a PCA."}