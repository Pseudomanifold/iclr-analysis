{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper empirically demonstrates that DNNs can be trained to be identity mappings for small quantities of samples. It also demonstrates that for many parameterizations, these identity DNNs also have a small number of attractors, iterative fixed points, and can also learn short circular sequences of examples.\n\nThe paper is well written and easy to understand, although the presentation could be improved a bit (see comments). Its contents aren't particularly novel in terms of ideas, but they investigate memorization and attractors much further than previous studies. \nIn a way, the memorization results are unsurprising. We know that DNNs can memorize perfectly, including sequences, so it is natural that by increasing capacity, at some point they should be able to memorize entire images (in fact this is what Zhang et al. (2019)'s Figure 1 appears to be showing). \nThe more novel and surprising aspect of this is that DNNs would learn such strong (and so few) attractor basins. The fact that deep autoencoders could have attractors centered on the training points has been postulated before (see [1]), but this work makes a stronger case for it.\n\nA crucial aspect that is missing from this paper in order for me to give in an accept is that there is very little about how this paper positions itself in the current literature. There could be much more discussion about related work, and much more discussion about the impacts of these findings.\n\nI have given this paper a 'weak reject' mark but I think with some work this paper could be of interest to many. To reiterate, I am unable to see anything wrong with this paper, but at the same time I am unable to see how impactful these findings are.\n\n\nDetailed comments:\n- It's interesting that DNNs can implement associative memory, but what is the cost of doing that? Should we be using that in practice? Since there is no sense of how costly the presented experiments are, it is hard to tell.\n- Again, these results are interesting, but after some time pondering about it, I can't really convince myself that knowing the results of this paper will be beneficial to future research. That being said, there are many areas of Machine Learning that I am unfamiliar with. It should be part of the paper to familiarize readers with areas where these results could be impactful.\n- \"the function interpolates the training images\" not sure what this means. Interpolation means making a prediction for a point `u` that is \"between\" two points `x,y` with known values\n- \"black and white\" should be \"grayscale\" if values are in [0,1]\n- Figure 2b is interesting, but I wonder what happens if e.g. a perturbed version of e.g. Example 6 is fed. Presumably since Example 6 is not an attractor (Jacbian with an eigeinvalue > 1), it should converge to another example.\n- Figure 2b's caption numbers, which say you use 1k example, to not correspond to numbers earlier in the text, which say you use 10k examples.\n- \"Since overparameterized autoencoders interpolate the training data\", again this is a fairly important assumption and it needs to be defined very clearly, because it could mean many things.\n- \"it is essential that we interpolate to numerical precision\", I don't think you are using the word \"interpolate\" correctly, do you mean \"inference\"? \"train\"?\n- Adam citation should be \"Adam: A Method for Stochastic Optimization, Diederik P. Kingma, Jimmy Ba\", not Goodfellow et al., RMSprop should also have a citation, Hinton et al. 2012\n- ReLU citation should be \"Rectified linear units improve restricted Boltzmann machines, Nair & Hinton\", Leaky ReLU should be Maas et al 2013, SELU should be Klambauer et al. 2017.\n- The combination of section 3.1 and Figure 3 doesn't make it clear if models trained with Adam and RMSprop have weight decay or not. Can you clarify?\n- \"Note that a minimum width of 100 is needed to allow for interpolation.\" Again I think you mean \"learning\" rather than \"interpolation\".\n- You say that you trained black and white images, but all the images of CIFAR10 in the figures are colored, including the inputs and outputs. Can you clarify why?\n- You might be interested in [2], which is much older work about perceptrons, but still relevant to what is studied here.\n- The linked supplemental material gives a 404 for me. I replicated the MNIST Figure 6 experiment. I was unable to replicate exactly your results but they were similar enough. In particular, the activation function choice seems to be critical.\n\n[1] The Potential Energy of an Autoencoder, Hanna Kamyshanska, Roland Memisevic\n[2] Basins of Attraction in a Perceptron-like Neural Network, Werner Krauth, Marc Mezard, Jean-Pierre Nadal\n\n"}