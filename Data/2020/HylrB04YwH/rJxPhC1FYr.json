{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies a phenomenon of unusual memorisation in deep overparametrized neural networks.\nAuthors observe that, if an auto-encoder overfits to machine precision on a number of images, they can be reliably decoded from random noise and that it is even possible to memorise this way a sequence of images.\nEssentially, images from such a training set become attractors for the mapping defined by the auto-encoder.\nThe impact of network size, nonlinearity and initialization is studied and, quite surprisingly, very unusual trigonometric non-linearities performed the best.\n\nI find the studied phenomenon rather interesting and the analysis well-performed, but I am not sure how practically important is this work.\nFirst, I would argue that to call the overfit auto-encoder a function associative memory, it must be able to retrieve stored images not just from random noise, but from a somehow distorted or partially known version. \nOtherwise we are just left with a ridiculously large network that can only recall a handful of images we could store in the raw format using much less numbers.\nSecond, training until convergence takes prohibitively long time.\n\nI would be also interested to at least an interesting discussion, if not an answer, to the question of why and how exactly trained images become attractors. \n\nIn terms of novelty, it feels like Zhang et al, 2019 already studied a very similar phenomenon and the submitted paper does not add much to understanding of memorisation in neural networks. However, memorization of sequences was indeed a surprise. \n\nOverall, I do not have a strong opinion on rejecting the paper, it just feels like more work in this direction will make the paper significantly better. "}