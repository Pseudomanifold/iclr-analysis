{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the tasks of pruning filters, a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs). This paper gives rise to a fully-automated procedure for identifying and preserving the filters in layers that are essential to the network\u2019s performance.  In general, this paper is very well written and organized.\n\n1, The key concerns come from the Lottery papers [1,2]. One can find sparse structure from an overparameterized model.  The results of pruned network should be improved, rather than getting worse, since some redundant filters/params are removed from original network. In contrast, all the results of this method gets worse results; this is less desirable.\n\n2, the theoretical analysis is very good;  it is worthy publishing themselves. But ever since the \"lottery\" papers, I think it makes sense in locating the sparse and representative pruned structure, which can achieve better performance than full overparameterized model. \nso it\u2019s quite a borderline paper.  \n\n\n[1] THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS. ICLR 2019.\n[2] RETHINKING THE VALUE OF NETWORK PRUNING. ICLR 2019.\n\n"}