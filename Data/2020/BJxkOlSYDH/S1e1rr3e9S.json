{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper attacks the problem of pruning neural networks to obtain sparser models for deployment. In introduces a principled importance sampling approach for which independence of samples allows one to obtain bounds easily. These bounds can be used to control the accuracy of the method. \n\nThe proposal mechanism is very smart. The authors use a measure of the sensitivity of the network outputs to the channels in a particular layer (eqn 1).\n\nThe paper is very well written, but it would help to add a picture where all the symbols in section 2.1 appear. At times it is hard to keep track of the channels and features. It might alternatively be a good idea to specify the equation of a layer (eg what eventually ends up happening at the bottom of page 3) in section 2.1 and then explain the symbols in the equation. This will make life easier for anyone reading the paper for the first time.  \n\nThe experiments are well execute and include reasonable baselines. I would in addition recommend this recent paper:\nhttps://arxiv.org/abs/1902.09574  \n\nIt would also be nice to relate the work to this best paper this year.\nhttps://openreview.net/forum?id=rJl-b3RcF7"}