{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the problem of differentially private data generator. Inspired by the general GAN framework and the PATE mechanism, the authors propose a new differentially private training algorithm for data generator. The problem of training data generator with privacy guarantee considered in this paper is very interesting, and the proposed algorithm looks novel. However, there are lots of unclear statements in the current paper, and I cannot tell whether the proposed algorithm is indeed better than previous methods. Following are my major concerns:\n1.It is unclear what are the loss functions used in equation (1) and (2). Please define k when introducing equation (2).\n2.The training framework introduced in section 3.1 is different from the traditional GAN framework, and thus my concern is that whether this framework will give us good generated samples. Because the performance of GAN has been proved in both theory and practice. The authors should at least empirically show the performance of the proposed framework in the nonprivate setting.\n3.There is no introduction of the $(\\epsilon,\\delta)$-differential privacy before introducing the Definition 1.\n4.There is no definition of Renyi differential privacy, so the statement of Theorem 2 is unclear. In addition, what is data-dependent Renyi differential privacy?\n5.The privacy guarantee of Algorithm 2 is not very clear. Because there are lots of parameters in Algorithm 1 and 2 which may affect the privacy guarantee, and Theorem 3 does not state such requirements. For example, how to choose $\\sigma_1,\\sigma_2$? In Theorem 7, there are some constraints on different parameters, will them be satisfied by your algorithm?\n6.How will the number of teacher models affect the privacy guarantee?\n7.Why you choose random projection matrix with variance $1/k$, and what is the projection dimension for different algorithms?\n8.In Table 1, the results of non private GAN are different from the results of non private GAN reported in PATE-GAN paper. Since the baseline results are much better in the current than the results reported in the PATE-GAN paper, it seems to me that the improvements of the proposed method comes from the stronger baseline.\n\nOther comments:\n1.$\\lambda>1$ in Theorem 3.\n2.Algorithm 2 should be moved to main context.\n3.The last sentence in section 3.2 is not convincing.\n4.Typo \u201cdiffernet\u201d in the caption of Table 1."}