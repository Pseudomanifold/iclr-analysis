{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this submission, the authors propose a method to generate synthetic datasets with privacy guarantee while preserving high data utility. However, the following concerns are important to clarify for the authors:\n\n1) Compared to existing work, the proposed method can preserve high data utility due to the fact that the proposed method doesn't ensure differential privacy for the discriminator. Could the authors provide more details about this key observation and motivation? How about discriminator is not trustable? Can we simply assume that it is safe for discriminator to access sensitive data? How about the discriminator is attacked? I agree for some applications, we can have this assumption about discriminator, but it is very important to better understand the limitation and risk of this assumption. The real-world applications are not simply defined by us.\n\n2) The authors claim that the proposed method is scalable? Could the author confirm this either theoretically or experimentally?\n\n3) The experiments are conducted on a single dataset, simple MNIST dataset. It would be convincing to report experiments on more datasets such as CIFAR-10 and others.\n\n4) For experiment comparison and analysis, the authors adopt quite large epsilon, i.e., \\epsilon = 1 and \\epsilon = 10. Several existing work adopt \\epsilon = 0.2. Can the authors report experiment comparison with such meaningful epsilon?"}