{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper aims to provide an intuitive explanation for why neural networks generalize despite over-parameterization. They argue that neural networks generalize well because flat local minima generalize well since they are insensitive to small perturbations in the data, and they are wide so they tend to attract SGD more than sharp local minima. \n\nOverall, the paper is well-written and contains nice visualizations. However, I don't think the paper provides an explanation to why neural networks generalize for a few reasons:\n\n- First, I don't think the methodology used in the paper to identify bad local minima is a good sampling strategy of local minima in general. It could be the case that adding the second objective in Eq 2 will tend to generate sharp local minima  (since by construction it works well for a subset of the data and works poorly on another subset). In other words, the local minima identified by solving Eq 2 might be sharp by construction. Of course, these sharp minima do not generalize by construction as well. Using this as an explanation is not justifiable since there might be other bad minima that are flat but are never identified by solving Eq 2. \n\n- The explanation provided here does not explain why linear models do not generalize well (the ones that have been tried in Figure 2). Do linear models have sharp minima? It would be good to measure the curvature of the loss for both the optimal solution of the linear model and the local minima of neural networks. \n\n- The explanation that flat minima are wide and, hence, tend to attract SGD is incorrect since we don't know how many of the local minima are wide. What if the majority of local minima are sharp? There is no analysis in the paper of the ratio between the number of flat minima and sharp minima. \n\n- Regarding the discussion about high dimensionality, it is true that flat minima will be several order of magnitudes larger than sharp minima but there is also the generalization penalty of having higher dimensions. Are the generalization bounds that explain the value of flat minima dimension-independent? I don't think they are since the dimension often shows up indirectly in the bounds (such as in the condition number which tends to increase linearly with dimension in random matrices). \n\nIn summary, the paper is nice to read but I don't think it answers the question of why neural networks generalize in practice. \n\n"}