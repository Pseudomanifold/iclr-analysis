{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThe paper studies the generalization ability of neural networks. It is a timely topic with potentially large implications. The paper makes several interesting and intuitive claims about the link between optimization and generalization in deep networks. The central, intuitive, claim is that the optimization trajectory avoids multiple trivial bad minima because they have much lower volume in the overparametrized regime; this is because they correspond to low norm of the Hessian. However, this claim is not validated experimentally well enough. Therefore, my main issue with the paper is that the intuitions that paper proposes (\"Our goal here is to develop an intuitive understanding of neural network\") might be technically incorrect and misleading. On the other hand, many observations (e.g. in Figure 1) are not very novel and statements such as \"Miraculously, SGD always finds its way through a landscape full of bad minima\" are rather misleading. As such I am inclined towards rejecting the paper. \n\n*Detailed comments*\n\n1. The fact that there are many \"bad\" minima that corresponds to a network that output incorrect (random) labels on some held-out data is well known. See [1,2]; in particular, [2] have performed in Section 5 the same experiment (without the visualization). What could have been novel is examining proximity of the \"bad\" minima. However, this is examined only qualiatatively, and as such does not offer additional insights beyond what was explored by papers such as [1,2].\n\n2. There are implicit assumptions that have to be true if the claim is to be correct. However, these assumptions are not experimentally validated. More precisely:\n\n2a) Implicitely, the paper model the optimization process as a random search through all the minimas. Therefore, if there are only a few sharp minima, they will not be selected by this random process. However, why such an approximation of the learning dynamics is justified? For instance, [3,4] show that the optimization trajectory is rather selective for shape of the minima, and therefore is far from such a random sampling process.\n\n2b) It is implicitely assumed in the analysis that wide minima are good for generalization. However, it seems to me that recent evidence points towards the direction that width of the minima is a epiphenomenon, see the experimental data in [5,6,7,8]. Given how central to the paper is that wide minima are \"good\", these references should be discussed in more detail. These papers suggest that the link between curvature and generalization is only a correlation, and therefore the claim that the reason SGD avoids bad minima is because they are few, which in turns is due to their low norm of the Hessian, might not be correct. Additionally, I didn't understand the discussion on Dinh et al. Could you please clarify it?\n\n3. In Figure 5, how are the different minimizers found? Is it by starting from different random initializations? If this is using different random initializations, then this implicitely changes the effective learning rate (for the sake of the argument we can define it as learning_rate/||H(0)||, where H(0) is the Hessian at the initialization). In turn different learning rates tend to select for minima with different shape [3,4]. Hence, this plot could be measuring the effect of the learning rate, and the effect of curvature might be correlational in nature (see also 2b). Do you achieve the same result is the learning rate is normalized? \n\n4. The claim that flat minima correspond to wide margin, while not demonstrated yet convincingly in the literature, has been claimed by other papers. I think [9] (cited in the submission, but cited in another context) is particularly worth discussion in this context, given that it makes almost the same visualization in [9] in Fig.1.\n\n5. The experiments comparing a neural network to a linear model are not very illuminating. It is well known that there is a strong role of the architectural prior, compared to the implicit regularization induced by the choice of the optimizer. I think at least some prior work should be cited here.\n\n*References*\n\n[1] Arpit et al, A Closer Look at Memorization in Deep Networks, https://arxiv.org/abs/1706.05394\n[2] Wu et al, Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes, https://www.padl.ws/papers/Paper%2010.pdf\n[3] Jastrzebski, Kenton et al, Three Factors Influencing Minima in SGD, https://arxiv.org/abs/1711.04623\n[4] Wu et al, How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective, https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective.pdf\n[5] Golatkar et al, Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence, https://arxiv.org/abs/1905.13277\n[6] Yoshida et al, Spectral Norm Regularization for Improving the Generalizability of Deep Learning, https://arxiv.org/pdf/1705.10941.pdf\n[7] Jastrzebski et al, On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length, https://arxiv.org/abs/1807.05031\n[8] Guiroy et al, Towards Understanding Generalization in Gradient-Based Meta-Learning, https://arxiv.org/abs/1907"}