{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an algorithm combining calibrated prediction and generalization to construct confidence sets for deep neural networks with PAC guarantees. \nThe main novelty is that existing approaches do not come with PAC guarantees\nFollowing (Platt et al., 1999) and (Guo et al., 2017), the calibration of the learned model is controlled by its temperature. In particular, the proposed approach exploits another (small) training dataset to learn a temperature that gives the best calibration. \nAn efficient algorithm for constructing confidence sets that are \"small in size\" is also proposed. \nThe framework is introduced for the classification, regression and reinforcement learning tasks.\n\nI vote for acceptance for the following reasons:\n- The paper is well-written, the motivation and comparison to related work is also clear.\n- The paper is very solid theoretically and experimentally. A theoretical analysis is provided, and practical implementations are proposed to deal with scalability issues. VC generalization bounds are studied in detail.\n\nConcerning experiments that are not about Reinforcement learning, the paper proposes different strategies to learn a ResNet architecture for ImageNet. One might argue that the different results might be only valid for the chosen architecture and dataset. \nAlthough the study for that architecture and (large scale) dataset is exhaustive, it might be interesting to study if the reported results are also valid on (smaller?) datasets and other architectures."}