{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper investigates the theoretical support for Generative Adversarial Imitation Learning (GAIL).   Specifically, two main points are shown: (1) For general reward parameterization, the generalization of GAIL can be guaranteed, as long as the class of the reward functions is properly controlled; (2) When the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms. \n\nNumerical experiments are provided on three classic continuous control tasks. RL algorithms are generally questioned in terms of reproducibility. Does the variance of different runs have an impact on the validation of the proposed theory?"}