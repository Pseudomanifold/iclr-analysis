{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a very simple strategy to reduce the variance of a batch of training data. It applies data augmentation operations to expand a training example, so that the mini-batch solely consists of the variants of this example. Since the correlations among examples within the batch is stronger than those in a batch with different examples, the gradient has less variance. While it can reduce the gradient variance during training, which leads to faster convergence, it also has better generalization performance. This is confirmed by empirical studies on difference datasets with a variety of neural networks, such lstm, transformer and convnet.\u00a0\n\nOverall, this seems to be a simple yet effective\u00a0method to improve the convergence and generalization at the same time, which is orthogonal to existing approaches. It might be better to try this method and improve the state-of-the-art on at least one dateset.\n"}