{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes increasing the size of the mini-batches by using existing data augmentation methods performed on the same samples of the mini-batch, called Batch Augmentation (BA). The authors claim that this technique provides better generalization.\n\nThe improvement is justified by reduced variance of the gradient, therefore it might be relevant that the authors compare (computation resources/performances) with Variance Reduced Gradient baselines (see [1]) and analyze it empirically with a better proxy for it (see below).\n\nThe authors use the L2 norm to empirically analyze the variance of gradients. As the above justification is an important claim, in my opinion using a better proxy for the variance could be useful: for example measuring the variance exactly as done in [1] or using the second-moment estimate as in [2].  \n\nAs the idea is fairly simple, and the use case seems somewhat limited for a distributed setting, I could be wrong but, in my opinion, the results are not significantly improving: for example in Fig. 4a the baseline outperforms the validation error of BA in the early iterations and marginally down performs at the later iterations. Also, it is not clear to me if this is due to a poor choice of learning rate for the baseline relative to BA (in this case, could be due to using a larger learning rate then its optimal as the validation error is better in the early iterations).\n\n- For a more fair comparison, each plot could also compare with a baseline where the batch size is M*B using no data augmentation (e.g. in Fig.1,2,4). \n\n- Page 5, 2nd paragraph: the authors mention that increasing M continues the trend of a reduced validation error, whereas from Fig. 3b we see that increasing M starts to saturate the reduction of the validation error.\n\n- In my opinion, more detailed ablation studies could be useful. For example, if Adam is used, it would be interesting to see the average effective learning rates for the parameters throughout iterations. Also, a wall-clock comparison on sequential and on distributed settings would be useful.\n\n---Minor---\n- Page 4, Sec.2.1: what do the authors mean by \u201cBA reduces variance less\u201d \n- Page 5, Fig. 4b: the caption is inconsistent\n\n[1]  On the Ineffectiveness of Variance Reduced Optimization for Deep Learning, A. Defazio and  L. Bottou, 2019. \n[2] Reducing Noise in GAN Training with Variance Reduced Extragradient, T. Chavdarova, G. Gidel, F. Fleuret, S. Lacoste-Julien, 2019.\n"}