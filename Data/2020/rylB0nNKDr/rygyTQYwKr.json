{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduce batch augmentation, replicating instances of samples within the same batch with different data augmentation, to improve both optimization and generalization of standard optimization algorithms. It has been shown in the paper that batch augmentation reduces gradient variance and can serves as a regularizer.\n\nHowever, I\u2019m not fully convinced by the experiments and vote for a rejection for now. My main concern is that the authors didn\u2019t quantitatively measure the convergence improvement over standard small-batch baselines. The goal of large-batch training is to get linear scaling within certain range. I doubt if batch augmentation would retain the convergence speed-up of standard large-batch training.\n\nMain argument:\nThe authors claim that batch augmentation reduces gradient variance, thereby improving convergence. However, I didn\u2019t find any experiments quantitatively measuring the speed-up. Ideally, large-batch training enjoys perfect scaling, i.e., the required training steps get reduced by the increasing factor of batch size. Otherwise, we have to use more computation for training the networks to the same accuracy. If batch augmentation fails to enjoy perfect batch size scaling, then its main contribution would reduce to an effective regularizer and the contribution is minimal in this sense. To this end, I suggest the authors to conduct some experiments as done in Shallue et al, (2018) to check the batch size scaling of batch augmentation.\n\nBeyond that, I hope to see some experiments on understanding the regularization effect of batch augmentation. Currently, the authors just show BA is effective but fail to explain why.\n\nMinor comments:\n- In the main paragraph, the authors misused citet and citep (e.g. the third paragraph of the introduction). \n- The Table. 2 wasn\u2019t even mentioned in the paper.\n"}