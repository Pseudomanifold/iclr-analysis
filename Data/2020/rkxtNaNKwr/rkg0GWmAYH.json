{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to use a two-level optimization process to solve the challenge of optimizing the team reward and the agent's reward simultaneously, which are often not aligned. It applies the evolutionary algorithm to optimize the sparse team reward, while using RL (TD3) to optimize the agent's dense reward. In this way, there is no need to combine these two rewards into a scalar that often requires extensive manual tuning.\n\nI vote for accepting this paper, because it tackles a practical problem in multi-agent learning. The presentation is clear, the algorithm is simple and sensible, the evaluation is thorough, and the results are better than the state-of-the-art.\n\nThe only question that I have is the sharing of replay-buffer for the same agent in different teams (Figure 2). For example, since the second agents in team1 and in team2 might learn to serve different roles in the task and may have completely different policies. I am not sure what is the purpose of this sharing. Considering the two extreme cases of replay buffer sharing, we could share all the data in a single replay buffer, or we could keep a separate replay buffer for each individual agent in each team, the paper chose the compromise between these two extremes. I wonder whether there is any theoretical or practical reasons to make this design choice. Is it important? I hope that the paper could have a deeper discussion if it is an important design decision."}