{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors provide the definition of a (L_0, L_1)-smooth function.\nThey motivate this class of function based on theoretical arguments and\nempirical observations in the context of neural network training loss.\nThe authors go on to show that for (L_0, L_1)-smooth loss constant\nstep-size gradient descent can perform arbitrarily bad with poor initialization.\nSpecifically, the largest constant step size that can be used without divergence\ndepends on the upper bound of the gradient norm.\nIf the function is steep (high gradient norm) at some point during the\noptimization process the steps need to become arbitrarily small\n(especially relevant in nonconvex loss, where gradient norm does not always decrease).\nThe small constant step size slows down convergence everywhere else.\nThe described phenomenon is an argument for step sizes adaptivity in general\nClipping the gradient based on the known (L_0, L_1)-smoothness is a form of\nadaptivity that makes steps small enough to avoid divergence only where the\nfunction is steep and can thus be more efficient.\n\nOverall, the authors give a theoretical justification for the use of clipped\ngradient descent in the context of training neural networks.\nThe analysis of clipped gradient descent is performed rigorously (proofs are\nprovided in the appendix) and extended to the stochastic gradient descent setting.\nThe motivation for using (L_0, L_1)-smoothness and gradient clipping is also\nshown to be relevant in practical experiments with language and image\nclassification models.\n\nI recommend to accept the paper.\n\nThe paper is a clearly stated contribution to optimization theory as motivated\nby machine learning applications.\nThe authors give a clean theoretical analysis of a previously empirically\nobserved phenomenon and an algorithm that is already used in practice.\n\nNotes:\n- Page 1, first paragraph:\nIf f(x) = E_xi [f(x, xi)] then f(x) the expectation is generally not stochastic,\nlike you write before. It is clear what you mean but Maybe use a different symbol.\n\n- Page 2, ...we observe the function smoothness has a strong correlation with gradient norm 2:\nPerhaps write something like \"(see Figure 2)\" instead of just \"2\" here.\n\n- Page 2, after definition of Lipschitz continuous:\nNitpick: Formal definition based on Hessian norm upper bound should also have, for all x \\in \\mathbb R^d\n\n- General: Some equations that are not referenced seem to be numbered, some are not.\n"}