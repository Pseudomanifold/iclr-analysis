{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper applies new assumption on smoothness that assume the norm of Hessian is bounded by a scalar plus norm of gradient. The traditional smoothness is only bounded with a scalar, the proposed assumption is more relaxed because now the norm of Hessian can grow with the norm of gradient. Under this assumption, the authors show clipped gradient converges faster than gradient for general nonconvex problems. The authors provide insights on why the proposed assumption is good for describing neural networks, and empirically verify the assumption with ResNet on CIFAR and LSTM on PTB. \n\nI like the paper in general. It is well written and easy to follow. The contributions are clearly described and the techniques seem to be solid. \n\nI want a little bit more discussion to help me better understand the paper. \n\n1) Intuitively, try best in plain English, why does clipped gradient convergence does not have dependency on L_1 M? Could the authors provide more discussion on the learning rate (hyperparameters) used for clipped gradient and gradient?\n\n2) The convergence rates under the proposed assumption are slower than traditional smoothness assumption. Could you verify the slow convergence rate under proposed assumption aligns better with practical training? Could you provide a toy example, for example x^3, to show the advantage of the proposed assumption and the convergence under the assumption? What is the gap between clipped gradient and gradient, for experiments in figure 4, and possible toy problem? Could the authors elaborate the difference of clipped gradient and gradient with the details of theory, instead of simply claiming clipped gradient is faster?\n\nTypo, page 5, (0, L) -> (L, 0)\n"}