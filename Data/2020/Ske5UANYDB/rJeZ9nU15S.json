{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the interpolated k-nearest neighbors algorithm from a theoretical perspective. Specifically, it studies how the performance of the algorithm is affected by reweighting the k nearest neighbors according to their relative distance. This regime has been considered in prior work, particularly Belkin et al. (2018).\n\nUnder various niceness conditions, the paper proves error bounds for interpolated k-nearest neighbors for both regression (i.e. squared loss) and classification (i.e. 0-1 loss after thresholding).\n\nOverall, I have the impression that this paper contains interesting ideas, but the presentation is very poor. It should be revised and resubmitted before it can be accepted.\n\nIn particular, the paper does not make its contribution clear. The main theorem only appears on page 4 and the reader must consult the appendix to see the definition of all the terms that appear in the theorem. I have no idea how to interpret the (complicated) expression in the theorem. The theorem needs to be explained in intuitive terms. More context needs to be given by comparing the main theorem to prior works (which I am not familiar with). \n\n\n"}