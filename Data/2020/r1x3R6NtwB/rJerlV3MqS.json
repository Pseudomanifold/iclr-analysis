{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a regularization scheme for gradient-based meta-learning. The scheme proposed is DropGrad, where they randomly scale the values of the gradient by a Bernoulli or Gaussian distribution effectively preventing overfitting and enhancing generalization. The authors evaluate the proposed approach extensively in domains ranging from supervised learning to reinforcement learning, and with different variations of their method.\n\nThe overall paper is well written and motivated. The experiment section is extensive and they show in different domains how their method performs better than without regularization. However, I feel that the overall contribution is not enough. First, their approach is not novel enough. Even dropout has not been applied to meta-learning, it has been applied to different  methods and fields successfully. Second, there is no insight why this approach works or should be used. The authors just reference to the success of dropout on other works. However, applying the dropout to a gradient is inherently different than doing it to the activations. For instance, it can result in a direction that is not of descent. Finally, while the experimentation is exhaustive, the results do not show the proposed approach works substantially better than without regularization. In section 4.1 and 4.2, almost all the results are within a standard deviation of each other. Moreover, in section 4.1, you are applying gradient dropout to \\theta and \\theta\u2019, I wonder why that choice since it seems that the proper way of doing it would be to just apply the dropout to \\theta\u2019. In this section, the authors should also compare against other kinds of regularization, for instance gradient penalty, L2, etc\u2026 In section 4.3, the results shown are just one random seed, which given the noisiness of reinforcement experiments renders them not very meaningful. Furthermore, the main motivation of the paper is to avoid overfitting, but here, there are just two tasks. Therefore, regularization should not help unless the improvement is coming from somewhere else. The authors claim that the \u201cimprovement could be attributed by the uncertainty on gradients that provide a better exploration of the policy\u201d, but they do not have evidence for corroborating that claim. The inner update on MAML-RL comes for vanilla policy gradient, which already has high variance. It would be interesting (not only in this experiment but in supervised learning as well) to see the variance of your gradient estimator and w/o the regularization. \n\nAt this stage, this paper does not have enough delta on novelty, insights why the proposed method works (or should be used), nor results."}