{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to apply dropout to the inner-step gradients of MAML as a regularization to prevent robustness against meta-level overfitting. The authors perform experiments on few-shot classification benchmark datasets, reinforcement learning task and online object tracking, on which the proposed regularizer improves the performance of the base gradient-based meta learners. \nThe paper tackles an important and less spotlighted problem of meta-level overfitting. However, it is well known that the meta-level overfitting can be alleviated by applying existing regularizers such as data augmentation, weight decay, label smoothing, and dropout [1], and the proposed DropGrad is simply a generalization of dropout applied to the same problem. \nThus the proposed meta-level regularizer seems very trivial or incremental at best. Without further experimental validations against other types of regularizers applied to the meta-learning framework, the contribution of the proposed work is unclear. Thus I recommend rejecting the paper unless those points are clearly addressed in the rebuttal.\n\nFurther Comments\n- The authors briefly discuss and compare against existing dropout methods. However, there is no information about the range of hyperparameters considered for the baselines, and the experiment considers only 5-shot classification case on the miniImageNet dataset. Could you provide more information and experimental results on this experiment?\n- Dropout could be applied when computing the outer loss as well as the inner loss, and this should be another baseline that DropGrad should be compared against.\n- Why should DropGrad perform better than the existing dropout techniques? What makes the difference between applying dropout technique before / after computing gradients?\n- It would be interesting to see how DropGrad would compare with DropBlock [2] in a meta-learning framework. \n- For MAML, ResNet-18 is one of the lowest-performing backbone networks, according to Chen et al. Could you provide the results on other architectures (ResNet-12) as well?\n \nReferences\n[1] Lee et al., Meta-Learning with Differentiable Convex Optimization, CVPR, 2019\n[2] Ghiasi et al., DropBlock: A regularization method for convolutional networks, NeurIPS, 2018"}