{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes to use a form of dropout (or masking) on the gradients for meta-learning purposes. Authors discuss two forms of masking, Bernoulli and Gaussian masking. \n\n\nThe proposed method is straightforward and reasonable. My only concern here and hence my weak reject score is due to the novelty of the work. Both masking options are well-known and as a matter of fact, been used before. In particular, the MT-Net uses masking on the gradient.\n\nAside from this, can you comment on the sensitivity of the solution wrt to the parameters? For example, in the case of the Bernoulli, can you elaborate on the performances while varying p? (I might have missed this) "}