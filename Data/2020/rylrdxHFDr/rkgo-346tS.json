{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper seeks a solution to the problem of performing imitation learning when the dynamics of the demonstrator are different from the dynamics of the imitator. The authors present a novel approach that combines global alignment by minimizing the Wasserstein distance between state occupancies with local alignment via a state-predictive VAE and inverse dynamics model. The experimental results support the claims that the method works for different dynamics and the proposed approach usually outperforms existing imitation learning methods.\n\nThe problem of dealing with different dynamics between a demonstrator and imitator is an important, but often overlooked problem in imitation learning. The combination of the global and local alignment is novel, nicely motivated, and ablation studies demonstrate that both are needed for good performance. Given the extensive experimental results showing the efficacy of this method I recommend that the paper be accepted. \n\nHowever, I feel that the paper can still be improved. Below are some of my questions and suggestions.\n\nThe success of BC is interesting. Why does it do so well? This seems to violate the motivation of the paper that using (s,a) for imitation learning won't work if the dynamics change. \n\nI thought the experiments for different action dynamics was very nice. The paper mentions that even state-spaces cannot be matched between the point mass and Ant. How do you know what part of the state space to imitate? Do you assume that is prior knowledge? Is this something that could be learned or inferred? How?\n\nHow is the potential updated. Eq(3) doesn't give an update rule. From later discussion it appears you mean take the \\phi that results in the supremum. Is this value learned? How is the optimization performed to solve Eq(3)?\n\nHow would you make the policy prior in Eq(7) work if actions are discrete? What if actions are multidimensional, with different ranges where some actions are not important? What is sigma?\n\nIn the RL community there is interest in transfer learning when the dynamics change. If the reward were observable, would the current approach be potentially useful for boosting the performance of transfer learning in RL? \n\nRelated Work:\n\nThe authors cite AIRL, but could do a better job distinguishing between AIRL and the current work. AIRL also tries to learn a state-based reward that is disentangled from the dynamics. Are there theoretical reasons why this work is better? Why does the proposed method work so much better in practice. On a related note, the results for the disabled ant seem much lower than those presented in the original AIRL paper. Why is this?\n\nThe authors do not mention the work by Brown et al. \"Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations\", ICML 2019. This work also learns only from observations and does not require any pretraining. How is the current work different?\n\nThe authors cite the work by Ayatar on imitation learning from observing YouTube videos. This method is also state-based and uses a similar state occupancy matching reward that would also work with different dynamics. How is the current method different. Would the current method work on domains such as learning to play Atari from raw visual trajectories?\n\nTypos:\n\n\"... able to resume to the demonstration trajectory by itself.\"\n--maybe say \" able to return to the demonstration trajectory by itself.\"\n\n\"... pairs as in an observation-based GAIL (Ho and Ermon 2016). I think this should be Torabi et al. instead.\n\nPage 5 \"state predictive VAE and an inverse *dynamics*\"\n\n\n"}