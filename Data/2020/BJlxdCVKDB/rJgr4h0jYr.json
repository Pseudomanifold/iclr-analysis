{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes an extension to the Viper[1] method for interpreting and verifying deep RL policies by learning a mixture of decision trees to mimic the originally learned policy. The proposed approach can imitate the deep policy better compared with Viper while preserving verifiability. Empirically the proposed method demonstrates improvement in terms of cumulative reward and misprediction rate over Viper in four benchmark tasks.\n\nI tend to reject the paper in its current form because (1) the idea of using mixture of trees to do policy extraction is somewhat incremental; and (2) experiments do not consistently show significant performance gain over the existing approach.\n\n=============================================================================================\n\nNovelty and significance\n\nThe paper addresses an important problem in RL, trying to extract an interpretable and verifiable policy from a deep RL model. The novel aspect of the paper is employing a mixture of expert trees model instead of a single decision tree in Viper. The proposed method is somewhat incremental in the sense that it is equivalent to replacing the first layer of the hard decision tree with a soft decision layer, which enables learning a non-axis-align decision boundaries in the first layer. This is more like a small modification of the original method than a significant contribution.\n\nMoreover, in the experiments, the proposed method does not consistently show significant improvements over Viper. Of course one can expect some improvements over Viper because the proposed method is using more flexible model by employing a soft and non-axis-align decision layer. Besides, for Viper the comparison is not fair because it is comparing with the best performing mixture of expert trees model among many candidate structures. I would like to see how Viper compares with the average performance of different structures. \n\n=============================================================================================\n\nOther comments\n\nThe organization of this paper is good and the paper is easy to read. However the authors can improve the tables and figures by providing more descriptions to be more self-contained. For example, I don't know what the column labels in table 1 (D/R/M/C) are referring to by solely looking at the table. And their actual meanings are really hard to find in the text. And for figure 2, the authors should point out the meaning of colors inside the illustrations.\n\n\n\n\n\n\n[1] Bastani, Osbert, Yewen Pu, and Armando Solar-Lezama. \"Verifiable reinforcement learning via policy extraction.\" Advances in Neural Information Processing Systems. 2018.\n"}