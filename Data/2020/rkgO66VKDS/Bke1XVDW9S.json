{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis paper trains low-precision network with quantized weights and quantized activation. The main idea is to split the scale and quantized values. Both scales and weights are updated with backprop and SGD. The paper presents excellent experimental results on ImageNet. \n\nThe paper is generally well written and easy to follow. However, there does exist quite some grammar errors, especially in abstract, which could be improved. \n\nMoreover, I would like the authors to clarify some technical details. Are the scales s, so called step size in the paper, for every weight, every convolutional kernel, or very layer? How do you deal with BatchNorm?\n\nWhat is the main benefits of the proposed quantization method in general? Is it for fast inference, fast training, or just memory compression? Do the authors see the real benefits in practice besides claiming the accuracy does not drop?\n\nI would suggest the authors discuss and compare with XNOR network in detail. The proposed method looks similar. \n\nI am wondering how the baseline methods are tuned. There are quite a few \u201ctricks\u201d like learning rate scheduler and weight decay, which I do consider them as contributions of the paper. But would baseline methods also benefit from more hyper-parameter tuning?\n\nMinor issue, I donot get the explanation of eq (4), and it looks rather unnecessary. It sounds to me starting from a trained network and then train 90 epochs is a rather long time. Could the authors convince me this is a standard setting by providing some reference?\n"}