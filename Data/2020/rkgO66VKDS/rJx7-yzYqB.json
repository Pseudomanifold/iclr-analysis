{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The motivation of the paper is to be able to train low precision networks to a high-accuracy. Quantization is a useful tool in model compression, and doing it well for very low-precision models (2-3 bit precision specifically), is challenging.\n\nThe main contribution of the paper comes from:\na) Step Size Gradient: They propose a gradient which is sensitive to the distance between the value and the transition point. This is different from other methods which have gradients dependent only on the clip point.\nb) Step Size Gradient Scale: This is an interesting contribution, where they try to match the ratio of average update of the step size \u2018s\u2019 and average magnitude of \u2018s\u2019, with that of the network weights. This leads them to scale the gradient according to the precision and number of parameters. They demonstrate that this scaling actually helps improve the accuracy.\n\nThe results for 8-bit precision are not new. Several results (Quantization and Training of Neural Networks for Efficient\nInteger-Arithmetic-Only Inference, Jacob et al., Quantizing deep convolutional networks for\nefficient inference: A whitepaper, Krishnamoorthi et al.), show 8-bit quantization results where the accuracy matches floating point accuracy, and in some case exceeds it (low precision quantization acting as a regularizer). However, the results for lower precision are impressive.\n\nThere are a few questions:\n1. In sec 2.1, you mention that \u2018each layer of weights and activations has a distinct step size, represented as an fp32 value, initialized to \u2026\u2019. Can you explain the intuition behind the initial value of the step size, and how is it a function of v?\n2. \u2018Model Compression via Distillation and Quantization\u2019 (Polino et al.) shows distillation actually helps significantly improve accuracy. I wonder if the authors have tried different weight combinations for the distillation loss, and using bigger models as teacher models.\n3. I would like to get more details of the inference setup, specifically the size and inference latency improvements over full-precision networks. The practical applicability of low-precision networks, specifically 2-bit and 3-bit networks, equally depends on the inference infrastructure, as it does on the training improvements.\n4. Have you evaluated your method for a non-Vision usecase?\n\nOverall this is a good work, I would tend towards accepting this.\n"}