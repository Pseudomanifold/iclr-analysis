{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose learnable \"kaleidoscope matrices\" (K-matrices) in place of manually engineered structured and sparse matrices. By capturing \"all\" structured matrices in a way that can be learned, and without imposing a specific structure or sparsity pattern, these K-matrices can improve on existing systems by \n* capturing more structure (that was not handled by the existing manually engineered architecture),\n* running faster than dense implementations.\n\nThe claim that \"all\" structured matrices can be represented efficiently is a strong one, and in section 2.3 the authors make it clear what they mean by this. Although the proof is long and beyond the expertise of this reviewer, the basic explanation given in section 2.3 makes their point clear for the non-expert reader.\n\nThe balance of the paper empirically tests the claims of learnable structure and efficiency.\n\nOn the basis that these experiments essentially bear out the claims of the paper, I selected to accept the paper.\n\nWeaknesses:\n\n1. Regarding the ISWLT translation task result:\nWith this dataset, it's a bit of a stretch to say there was \"only a 1 point drop in BLEU score\". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement. There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.\n\n\n\n"}