{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper presents an adaptive method which can be used alongside existing accelerated gradient methods. The paper is difficult to read due to mistakes and poorly defined mathematical notation. I believe that the paper is missing reference to related methods. The theoretical analysis in the paper is difficult to follow and provides little insight into the benefits of the proposed approach.\n\nOverview:\n\nThere are many mistakes throughout the paper which have made it difficult to read.\n\nOverall, I felt that this paper was missing a discussion of the effect of stochasticity on the proposed method. The issue with measuring the variation in the gradient direction is that in regimes where the gradient noise is dominating the signal the gradient direction at each time step is poorly correlated with overall optimization progress --- thus it seems intuitively ineffective to rely on the gradient direction to adjust the algorithm.\n\n1)  At the bottom of page 2, the authors write \"knowing that we do not have any knowledge of what this function looks like\". While minor, I would point out that we are able to compute local statistics of the function and so certainly we have _some_ knowledge.\n\n2) The authors claim that no techniques exist which use the variation of the direction of the gradient. One such example is in [1] which uses (in one case) the variation of the gradient direction to determine and appropriate time to restart the momentum computation. \n\n3) In section 3.2, the Adam moment computation is missing a \"diag\". Assuming that AMRSGrad is AMSGrad (mistyped), then this term is incorrect and matches Adam.\n\n4) There are many mistakes in the Algorithm 1 box.\n\n- The wrong $F$ is used in the input (should be $\\mathcal{F}$).\n- The algorithm takes as input a sequence of functions ($\\phi, \\psi$) which are not used.\n- Within the if statement, $gm_t = g_t + m_t$. I believe this should be an $m_{t-1}$. It is not clear what the vector $gm$ is exactly, and then $\\dot{g}$ is used afterwards which is also not defined.\n- The algorithm checks for $|m_{t-1} - g_t| > S$ while the text uses $|g_{t-1} - g_t| > S$.\n\n5) The first line of section 3.3 is quite worrying: \"We assume that if we are able to prove that modifying one optimizer with the proposed method does not alter its convergence, then the same applies for the other optimizers\". This seems like a dangerous assumption to make and should at the very least be carefully verified empirically. Following this, I am not sure what the authors mean by \"deterministic\" and \"non-deterministic\" methods.\n\n6) I do not understand the claim above Theorem 2 that $\\nabla f(x_{T-1}) = k \\nabla f(x_T)$. Under what conditions does this hold and how is $k$ computed? If I understand correctly, the bound provided in Theorem 2 is worse than that given for gradient descent. Moreover, the bound does not depend on the hyperparameter $S$ introduced in Algorithm 1 and provides limited insights into the method. I could not find a proof of Theorem 2 in the paper or appendix.\n\n7) There are serious flaws with the experimental evaluation in this paper.\n\na) There is no tuning over hyperparameter settings for any of the optimizers.\n\nb) The basic problems are very limited, even for toy problems. The 1D deterministic quadratic tells us very little about the performance of the optimizer. And the 1D cubic problem is particularly confusing. Unless I am mistaken, the gradient will always have the same sign (3x^2) and thus the acceleration condition will never be triggered.\n\nc) I believe that Figure 2 explores stochastic optimization problems which as discussed at top is a crucial evaluation. Unfortunately, due to lack of parameter tuning it is difficult to infer much about the comparison between the methods.\n\nd) Figure 4 compares performance variation over changing the threshold. The y-axis scale across each plot changes making the comparison unnecessarily difficult --- the scale should be the same.\n\nMinor:\n\n- TYPO Line 2, \"minimize ---,\"\n- End of intro, MNIST and CIFAR not cited while IMDB is. Citation uses citet not citep.\n- Bottom of page 2, \"\"\n- Top of section 3.2, \"The pseudo code of our the method\"\n\nReferences:\n\n[1] Adaptive Restart for Accelerated Gradient Schemes, Brendan O'Donoghue and Emmanuel Candes\n\n"}