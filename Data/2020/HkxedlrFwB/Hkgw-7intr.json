{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper describes a technique to speed up optimizers that rely on gradient\ninformation to find the optimum value of a function. The authors describe and\njustify their method and show its promise in an empirical evaluation.\n\nThe proposed method sounds interesting and promising, but the empirical\nevaluation is unclear. In particular, details are missing on the exact\nexperimental setup and some of the presented results are unconvincing. I refer\nto the results of the basic function optimization (Figure 1), which shows that\nseveral of the considered optimizers are unable to even get close to the optimum\nof x^2 after several hundred iterations. It seems that this is extremely easy\nfunction to optimize -- why are the considered optimizers performing so poorly\non it? How were the hyperparameters of the optimizers set? This presumably\naffects the other results presented in the paper as well, and puts the\nimprovement of the proposed method in question.\n"}