{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper study the problem of Mahalanobis Metric Spaces learning, which is formulated as an optimization problem with the objective of minimizing the number of violated similarity/dissimilarity constraints. \n\nI am not an expert in this subarea. From what I have read, the method is based on sound theory and outperforms some classical methods, including ITML and LMNN on several standard data sets. However it is unclear to me what is the state-of-the-art of this field from this paper and its novelty. \n\nSome recent papers might worth discussing and comparing with, e.g.,\nVerma and Branson, Sample Complexity of Learning Mahalanobis Distance Metrics, NIPS2015\nYe et al. Learning Mahalanobis Distance Metric: Considering Instance Disturbance Helps. IJCAI\n\nIn the proof for Lemma2.1., why \u201cadding constraints to a feasible instance can only make it infeasible\u201d? \n\nIn Figure 4, why the running time is not a monotonic curve as the dimension increases?\n\nThe conclusion of the paper is missing. \n"}