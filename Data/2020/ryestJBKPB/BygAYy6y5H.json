{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Contributions:\n1. This paper proposes a semi-supervised graph neural network method for hypergraphs.\n2. A generalization error bound was proposed adapted to the semi-supervised setting with Wasserstein's loss.\n3. Empirical results demonstrate the effectiveness of the proposed method.\n\nThe algorithmic contribution of this paper is clear: it proposes a new network architecture that (1) initialize latent features H^{(0)}_E for hyperedges from a \"hyperedge graph\" and (2) learn latent features for each node and hyperedge using a GCN type network. Since the latent features are formulated as discrete distributions, a Wasserstein distance can be applied for training with the sinkhorn approximation algorithm.\n\nI think the weakness of this paper is two folds, which makes it not ready to publish. First, the paper claims that the performance gain results from the exploitation of directed hyperedges. This is reasonable if I barely see the results for Soft-DHN. However, I find the design of the hyperedge GNN to be fragile regarding the number of layers from the result in Table 4. Normally it is reasonable to use a two-layer GNN, but the result is very bad when doing so (see Soft-DHN 2 layers result in Sec. A.4.1). Also, the result of the proposed model is still good when not applying a hypergraph GNN. So I'm confused about where the performance gain comes from.\n\nAnother weakness is the paper writing. Below are a few comments:\n[Page 3, Sec. 3.1] By saying \"t\\neq\\Phi\", do you mean \"t\\neq\\emptyset\"?\n[Page 3, Sec. 3.2] Please introduce the notation (M, C) right after it first appears in the third line of Sec. 3.2.\n[Page 3, Sec. 3.2] The notation n,m are confusing. Do you mean m=|E|?\n[Page 3, Sec. 3.2] I cannot see how Z=h(\\mathcal{H},X_V,X_E) maps each vertex to a probability distribution. Do you mean each row of Z is a probability distribution for each vertex?\n[Page 4, Sec. 3.2] Please define E_D. Is E_D the same as E_d?\n[Page 4, Sec. 3.3] Be explicit by saying \"approximate the hypergraph by a suitable graph\", what do you mean by \"suitable graph\"? Does it mean use cliques in place of hyperedges?\n[Page 4, Sec. 3.3] t=0,...,\\tau-1\n\nSince the writing significantly affects the paper readability, and the core contribution of the paper seems incremental, I will vote for a reject to this paper."}