{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper aims to predict typical \u201ccommon sense\u201d values of quantities using word embeddings.  It includes the construction of a data set and some experiments with regression models.  The general direction of this work is worthy of study, but the paper needs additional justification for its task, better discussion of recent related work, and more development of its regression models.\n\nThe work starts by describing the construction of interesting crowdsourced data sets that include people\u2019s estimates of typical quantities, what they would consider to be low or high values for a given object in given units e.g. the temperature of a hot spring or the height of a giraffe.  Overall, the data sets are interesting but are not especially large (2300 total [low, high] pairs of 230 different quantities).  Further, the particular task formulation here needs more justification.  I think most of us would agree that common sense is a critical AI challenge, and that the question of whether embeddings reflect typical quantities is important.  But, in a paper where the data set is considered a primary contribution, I would expect more justification for exactly how this task is formulated, and which objects and units were selected.  As one example, why ask about \u201clarge\u201d and \u201csmall\u201d values rather than something with more precise semantics (like the 10th and 90th percentile, for example)?  I also felt that the introduction could be improved to provide more convincing motivation.  E.g., the first paragraph only says that humans apply different adjectives (like \u201chefty\u201d and \u201ccheap\u201d) to different things depending on their numerical attributes (weight, cost), but does not argue why teaching AI systems to use those adjectives is a priority.\n\nRegarding related work, the paper is missing a discussion of several relevant papers that use embeddings to obtain relative comparisons or estimates of commonsense properties of objects, including:\n\nForbes, Maxwell, and Yejin Choi. \"Verb physics: Relative physical knowledge of actions and objects.\" ACL 2017\n\nYang, Yiben, et al. \"Extracting commonsense properties from embeddings with limited human guidance.\" ACL 2018\n\nElazar, Yanai, et al. \"How Large Are Lions? Inducing Distributions over Quantitative Attributes.\" ACL 2019\n\nThe paper then presents the performance of some regression models.  These models are standard existing techniques, and given the relatively low performance I would have liked more development of the models and more analysis of the performance.  For a conference like ICLR I would expect to see a more thorough exploration and analysis of possible models for the task.  Looking at more powerful neural regressors (perhaps using contextual embeddings rather than just fixed word embeddings) might be one option.  Offering an explanation for why ARD seems to work better than the other approaches would be helpful.\n\nMinor: In Table 3, the way that small and large are interleaved makes it hard to compare systems, I think presenting all the small results together, and large results together may help.  In Figure 2, it would be helpful to see the histogram for size-large within the same plots here, so we could see how far apart they are.\n\n\u201cBecause Skip-gram has to handle more words to predict words, we assume Skip-gram will obtain more information about numerical values.\u201d\n-- I didn\u2019t understand what you meant about skip-gram having to \u201chandle more words to predict words.\u201d  Also, I did not understand how this entails that skip-gram would obtain more info about numerical values.\n\n"}