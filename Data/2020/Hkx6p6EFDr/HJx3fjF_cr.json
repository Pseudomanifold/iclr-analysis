{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes Equivariant Entity-Relationship Networks, the class of parameter-sharing neural networks derived from the entity-relationship model.\n\nStrengths of the paper:\n1. The paper is well-written and well-structured.\n2. Representative examples, e.g., the Entity-Relationship diagram in Figure 1, are used to demonstrate the proposed algorithms.\n3. Detailed proofs for some equations are provided for better understanding the proposed equivariant entity-relationship networks.\n\nWeaknesses of the papers:\n1. No effective baselines are used for comparisons in the experiments. Are there state-of-the-art algorithms that have been proposed by other researchers to be used as baselines in the experiments?\n2. No effective real-world datasets are used in the experiments. The authors only take synthesized toy dataset in their experiments. Are there other real-world datasets to be used in the experiments?\n3. In terms of missing record prediction, why do the authors embed, e.g., the COURSE, in this way but not the other ways? What are the motivations of embedding like this? Are there other embedding techniques, e.g., Matrix Factorization and Skip-gram frameworks like that in Word2VEC, can be used for your purposes?\n"}