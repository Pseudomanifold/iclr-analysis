{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper handles the permutation invariance of the entity-relationship data entries by tying the weights of linear parameterization together. Heavy math was used to describe the proposed idea, and experiments were performed on simple problems.\n\nThe main idea of the paper seems to be equation (3) and the associated method of tying weights in the linear layer. This is actually quite simple, and does not require the numerous complicated definitions in section 2 and 3 to reach the same conclusion. Using math is good, but using more than necessary simply slows down the dissemination of ideas.\n\nIn both theory and experiment, the paper does not demonstrate the advantage of the weight tying method compared to an untied baseline. This weakens its necessity and motivation.\n\nThis paper also has very weak relationship with deep neural networks, while the word 'deep learning' was mentioned in both introduction and conclusion.\n\nIn general, it is a weakly motivated and math-excessive paper. Based on these considerations, I recommend rejection."}