{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an algorithm to extend the recently proposed method of \u201ccovering options\u201d from a tabular setting to continuous state spaces (or large discrete state spaces). The proposed algorithm approximately computes the second eigenfunction of the normalized laplacian of the state space, uses it to identify an under-explored region and trains an option to terminate in such a region. Each new learnt option is added to the initial set of primitive actions and a policy over this growing set of actions is learnt separately. An online algorithm is also proposed that does the above option learning process intermittently in addition to training for an external task. The paper shows empirical evidence of better or equal performance to base algorithms which do not discover options, prior work such as DIAYN (Eysenbach et. al, 2019) (that discover options via mutual information maximization between visited states and options), as well as ablations of their proposed method with different number of options.\n\nI vote for weak reject due to (1) the idea of covering options (Jinnai et. al., 2019b) and the approximation for the graph laplacian (Wu et. al., 2019) both have been shown in prior work and the novelty in this paper seems to be limited to putting together these two ideas, and (2) the paper shows quantitative results for options discovered for simple environments whereas only qualitative options are shown for harder exploration tasks, and (3) given that exploration is a key problem being addressed, a comparison to other exploration algorithms which are non-option based methods has not been shown -- which makes for a weak argument for using an option-based method for exploration as opposed to existing methods for exploration.\n\nOther comments:\n- The paper does a good job at explaining covering options, the approximations involved in their algorithm and how options can be learnt with the help of such approximations. It makes sense that an algorithm that takes into account state connectivity will do better than DIAYN (Eysenbach et. al., 2019) which just promotes diversity of visited states across options. \n\n- The paper made some assumptions on the implementation of DIAYN, citing lack of details in Eysenbach et. al (2019). Were these details not available in the code released by DIAYN on their project page? (I\u2019m not sure if I should post the link here, but it is easy to find). I have some concern that a fair comparison may not have been made due to the paper\u2019s implementation of this baseline, but I am also not sure how important of a comparison this is given that their method for discovering options is quite different. I would argue that DIAYN is different enough that the proposed method deserves comparisons with methods for exploration that do not use options.\n\n- In the intro, \u201c...extend a theoretically principled approach for option discovery to the non-linear function approximation case\u201d seems to be highly misleading, since it suggests that the theoretical guarantees are extended, which is not the case in this paper as the approximations have no states guarantees.\n\nThe proposed method seems to have the potential to be a good at exploration but the paper does not show quantitative experiments for hard exploration tasks such as Montezuma\u2019s Revenge, or other Atari ALE environments. I am curious to see how many levels of Montezuma\u2019s revenge can be covered by simply applying deep covering options to it.\n\nReferences:\nAll references are same as the paper.\n"}