{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summarization \n\nThis paper considers the hyperparameters engineering problem in network architecture design. Unlike existing works optimizing these parameters based on human knowledge or grid search, the work proposes to search for them automatically in a gradient-based manner. More specifically, it focuses on three dimensions in network hyperparameter, i.e., network depth, number of channels at each layer and image resolution, and uses natural evolutionary strategy (NES) to jointly search for optimal values in all three dimensions. This work is among the first to introduce NES into network hyperparameter optimization, and demonstrates its effectiveness by applying it to multiple backbone networks. However, the experimental results are limited. \n\n2. Strength\n\nThis paper is well-motivated, as most of the works consider only one aspect of the network hyperparameters at one single time. Jointly exploring hyperparameters for network architecture design is a promising direction to explore. The proposed gradient-based search algorithm (based on NES) is neat and easy to follow. The experiments show that the method is effective at saving computation cost and boosting performance, though it\u2019s limited. \n\n3. Weakness\n\n1) Lack of details in methodology. For example, how to jointly deal with depth and number of channels per layer. As depth varies, the number of hyperparameters used to represent channels changes. So how you define the variables to jointly parameterize both depth and number of channels per layer.  \n\n2) There are some existing works discuss the relation between depth, channels and image resolution, for example EfficientNet [r1]. How this work is related and superior to such works is not presented. \n\n3) Lack of baselines. Multiple works explore towards the same direction, such as FbNet[r2], Mnasnet[r3], and [r4], there is no direct comparison to these methods and discussions. \n\n4) In experiments, most of the cases, the proposed method is only marginally better than baselines. How the searched hyperparameters is different from or similar to baselines are not present and analyzed. \n\n5) On MobileNet V1 and under GPU latency constraints, is the case the proposed methods substantially surpass the baseline. However, it is not well-explained, which makes the results suspicious. \n\n[r1] Tan, Mingxing, and Quoc V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" arXiv preprint arXiv:1905.11946 (2019).\n[r2]  Wu, Bichen, et al. \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[r3] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[r4] https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\n"}