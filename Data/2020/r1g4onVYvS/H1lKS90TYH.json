{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a gradient-based method to optimize architecture hyperparameters of convolutional neural networks such as the number of channels and depth dimensions. The proposed method utilizes the natural evolutionary strategy to make the optimization of hyperparameters differentiable.\n\n- It would be better to compare the proposed method with state-of-the-art methods such as AMC and MetaPruning in terms of the optimization cost.\n\n- To make the contribution of this paper clearer, it would be better to compare the proposed method with a random search.\n\n- It is unclear what uniform rescale baseline is in Table 1 and 2. Please elaborate on it.\n\n- About the baseline results under GPU latency constraints in Table 1, how do you get these results?"}