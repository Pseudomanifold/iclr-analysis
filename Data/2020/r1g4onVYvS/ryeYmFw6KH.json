{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is motivated to optimize the neural network architecture encoded by categorical variables and / or integer variables. The technique used in this paper is based on the so-called natural evolution strategy. The goal is well stated, and the research topic is important. However, there is an non-negligible overlap to an existing work (Akimoto et al, ICML 2019) that has the same motivation and uses a quite similar approaches.\n\nDetailed comments below:\n\n\"However, optimizing these hyperparameters jointly is challenging for existing approaches. There is not yet an effective yet efficient solution.\"\n\nThere are existing works addressing the neural architecture search encoded by categorical and ordinal (integer) variables. For example, Akimoto et al (ICML 2019) proposes a one-shot neural architecture search framework based on the natural gradient. The architecture parameters are encoded by the mixture of categorical variables (representing types of operations, etc) and integer variables (representing the number of filters and their sizes, etc). \n\n\"As the loss function is non- differentiable with respect to the architecture hyperparameters, we use natural evolutionary strategy (NES) to approximate the gradient of the architecture hyperparameters. \"\n\nActually, the above references follows the same idea as the natural evolution strategy. More precisely, the algorithm is derived from the information geometric optimization (IGO) framework (Ollivier, JMLR 2017) that generalizes NES. \n\n\"Our contributions are three-folds: 1) We propose a novel gradient based approach to jointly opti- mize the architecture hyperparameters in a unified manner. 2) We adopt natural evolution strategy to approximate the gradient of the non-differentiable architecture hyperparameters. 3) We verify the effectiveness and efficiency of proposed method with state-of-the-art results in various network structures.\"\n\nFrom above perspective, the first two contributions are not novel in this paper. There are differences in details from the above reference, but there are huge overlap. \n"}