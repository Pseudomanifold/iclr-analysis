{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper introduces an gradient estimator for loss functions that are expectations over discrete random variables. The basic idea is that an estimator over a discrete distribution can be Rao-Blackwellized by conditioning on the event that the discrete realization was produced by being the first sample drawn from an unordered set of samples drawn with replacement. Much of the paper is spent showing how this Rao-Blackwellized estimator can be computed in practice and how it compares to other known estimators.\n\nOriginality: This idea is original and quite nice.\n\nClarity: The paper is very easy to understand and well-written.\n\nQuality:\n- The derivations are all correct from what I can tell, and easy to read.\n- The experiments are all reasonably well done, but I could not find where the authors report which optimizer was used and how the hyperparameters were set. If I missed it, could the authors point that out in the rebuttal? If it is missing, then I strongly suggest that the authors include that in the paper. Even if the training protocol is taken from another code base, I think this paper should be reasonably self-contained. How the hyperparameters are tuned and which optimizer was used could affect the interpretation of the results. Additionally, if overfitting is an issue, I recommend the authors consider the use of regularizers, like weight decay. I see no reason that this would violate the spirit of the paper, and might make their results more compelling.\n\nSignificance: \n-This paper has some interesting ideas, and it is written well. So, it may inspire future work. Yet, from the experiments it is not obvious that this estimator adds much to the existing literature. In all experiments at least one known estimator matches the performance of the unordered set estimator. \n- All of these estimators, which require multiple evaluations of the gradient, are generally less common in practice. "}