{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: \nEven though on-policy methods are easy to use in comparison with off-policy methods (i.e. more stable, need less hyperparameter tunning ), they are sample inefficient. To address this problem, this paper proposes to collect samples using a behavior policy and keep them in the replay buffer.  Then using the off-policy actor-critic algorithm that uses policy forcing through Gumbel noise and clipped importance sampling to optimize the current policy.  Results on 8 atari games show that GumbelClip performs better than A2C but shows mixed performance vs. ACER.\n\nComments and major concerns:\n- It is not clear to me what is intuition behind forcing policy and why it should work. There are some ablation studies but the paper should be very clear and provide some justification for why one should use GumbelClip.\n\n- Even though GumbelClip shows better performance than A2C, it doesn't outperform ACER which should be considered as a baseline and it is more appropriate for comparison. \n\n- This paper only uses 8 atari games for the experiments. Unfortunately, 8 out of 49 atari games are not near enough to say anything about the significance of this method.\n\n- On page 5, the last paragraph says that they used the best 3 of 4 seeds to report results which indicates that they treated seed as hyperparameters. Seeds should NOT be treated as a hyper-parameters and should not report only on the \"best\" seeds. Please refer to the following papers about how to report results in RL:\n    -- Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents ( https://arxiv.org/abs/1709.06009)\n   -- Deep Reinforcement Learning that Matters (https://arxiv.org/abs/1709.06560)\n\n- Another major problem is that this paper didn't compare with recent methods that use off-policy data. For example, the following method should have been used in the experiment sections and should have been compared to GumbelClip:\n-- P3O: Policy-on Policy-off Policy Optimization (https://arxiv.org/abs/1905.01756)\n\n- Besides, this paper should have included a comparison with PPO (Proximal Policy Optimization Algorithms https://arxiv.org/abs/1707.06347 ) to see how it performs when it compares with a better method on-policy method than [vanilla] policy gradient.\n\nSummary:\nGiven a lack of comprehensive results, an incomplete study of previous related works, not enough conclusive results, and failure to compare with recent methods, in my view, this paper is not ready for ICLR and needs major revision. \n\n"}