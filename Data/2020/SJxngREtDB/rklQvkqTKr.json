{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a new off-policy actor-critic algorithm. It uses a noisy version of the current policy, instead of the true policy probability, in the importance sampling part and the policy gradient form. The noisy version is achieved by Gumble-softmax. Intuitively, the noisy policy probability will make the importance sampling ratio less variance, thus improve the off-policy actor-critic algorithm.\n\nI think the contribution of this paper is not enough to meet the standard of ICLR in general. The main reason is that this work seems to be a quiet incremental work on top of off-policy actor-critic [Degris et al 2012] and ACER [Wang et al 2016], at the same time, lack of proper empirical and theoretical justification of the algorithm. \n\n1) The proposed algorithm simply replace \\pi(a|s) with F(s, a) in the off-policy actor-critic algorithm where F is a Gumble-softmax over \\pi(a|s). All other parts (replay buffer, clipped IS) are from ACER [Wang et al 2016]. So the main contribution is actually using Gumble-softmax instead of using experience replay.\n\n2) It needs to be explained more why adding noise to the policy scores can make the off-policy learning better. It's probably (?) understandable that adding noise could make importance sampling stable, but for the gradient of log pi part it's less clear. The paper need to provide more insight on why we want to do this.\n\n3) If we want to add noise to the probability score \\pi(a|s), a simple baseline is directly adding a uniform noise over the score. This paper compares several ways of adding noise to the logit of policy, but directly adding different noise on the probability are also natural baselines. An even more baseline could be GumbelClip without the noise at all, which can end up with a simpler version of ACER. It's useful to see these baselines be compared in all domains instead of just 1.\n\n4) The significance of the experimental results is also unclear. In 3/8 domains ACER eventually beat GumbelClip with a pretty large margin, and only in 2/8 GumbelClip ended up with a slightly higher performance than ACER. I acknowledge that ACER ensembles many other techniques to off-policy actor-critic, but I did not see why GumbelClip could not work with these techniques."}