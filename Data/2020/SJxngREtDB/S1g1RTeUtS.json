{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overall I don't really understand *why* the Gumbel-softmax 'forced' policy is used over the standard policy. There is no discussion on the motivation at all. Using the 'wrong' policy in importance sampling will increase the bias of the updates, and possibly the variance. Furthermore, the point of using the Gumbel-softmax in other situations is to avoid the use of reinforce, however in this paper policy gradient (basically reinforce) is used anyway. On top of this there are no theoretical results showing that this approach is principled in any way. The experimental results are not enough justification.\n\nFig. 1 is very confusing, it took me a long time to understand what the 'sample' row was trying to show me. This is made extra confusing by eq 2 which is introduced using 'the Gumbel-Softmax distribution is defined as', which, despite the claim, is not the definition of a distribution but actually the definition of a *sample* from the distribution.\n\nSeveral of the claims in the introduction are plain wrong, e.g., \"Unfortunately, all action-value methods, including Q-Learning, have two significant disadvantages. First, they learn deterministic policies, which cannot handle problems that require stochastic policies. Second, finding the greedy action with respect to the Q function is costly for large action spaces.\"\nThe claim that it holds for *all* action-value methods is wrong. Moreover the claim that finding the greedy action is costly is just bizarre, since that amounts to simply taking a max over a set of numbers, whereas any policy method requires sampling from a set of probabilities of the same size - much harder to do!\n\nPaper is over the 8 page limit, but without really needing to be. I think some content could be removed.\n\nThree very relevant papers that I am surprised are missing are the following:\n\nPGQ: https://arxiv.org/abs/1611.01626\n\nThis uses the duality between value methods and policy methods to derive an off-policy algorithm. I really think a comparison to this approach is needed, especially because it is also much simpler than ACER to implement, and it doesn't require access to the old policy.\n\nRETRACE: https://arxiv.org/abs/1606.02647\n\nAnother importance weighted approach, must be mentioned (ACER is based on this).\n\nV-trace (in IMPALA): https://arxiv.org/abs/1802.01561\n\nFollow-up work to RETRACE, also very relevant.\n"}