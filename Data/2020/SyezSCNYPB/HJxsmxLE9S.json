{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Disentangled GANs for controllable generation of High-Resolution Images introduces two new high resolution synthetic scene datasets for studying disentanglement in generative models and benchmarks two Style-GAN based architectures for controllable generation on these datasets. The datasets, though still synthetic, provide a significant quality boost over some of the simpler toy datasets previously studied in the disentanglement literature. A variety of experiments are conducted looking at 3 metrics (FID, MIG, and latent reconstruction) as well as qualitative analysis of samples. The paper studies how the performance of these architectures varies with hyperparameter and design decisions. The authors demonstrate that AC-StyleGAN performs well in fully supervised settings achieving the desired conditional generation, but, when controlling only a subset of factors, does not correctly disentangle. To address this, the author\u2019s other architecture modification, FC-StyleGAN an image to image model is demonstrated to improve performance in this setting. The paper provides the reader with a useful overview of the behavior of the two models on these two datasets.\n\nMy rating is weak reject. While the paper has a variety of contributions (most notably introducing two new datasets and modifications of StyleGAN) and interesting results (such as relatively high performance with only 5% labeled data and the disentanglement issues when controlling a subset of factors), the core contributions of new datasets and architectures are not validated or analyzed rigorously enough.\n\n1) No prior work is used as baselines in order to compare / validate the newly introduced architectures. The authors dismiss prior work in the introduction but do not provide any direct evidence that prior work is unable to handle the datasets introduced in the paper. In order to consider acceptance based on the value of the architectures proposed in the paper, there should be some direct evidence that the proposed Style-GAN modifications AC/FC, outperform prior architectures. One particular choice which is unclear to the reviewer is why AC GAN was chosen over the Projection Discriminator of Miyato and Koyama 2018 which demonstrated significantly better results than AC GAN and was adopted by major followup work such as BigGAN.\n\n2) Given the core contributions of new architectures for disentanglement, the lack of any results on real, non synthetic, datasets in order to validate these (for instance, CelebA) which significant prior work on disentanglement has been evaluated on is an unfortunate omission. It is unclear how the introduced architectures handle complex natural image distributions with the much more diverse sources of natural variation they contain.\n\n3) The authors claim their method is effective in the semi-supervised setting and demonstrate this by showing relatively strong performance in a low data regime. This demonstrates the importance of the third term in the loss function, but as other work for semi-supervised learning has shown (Oliver et al 2018) care must be take to properly attribute the performance of a semi-supervised algorithm to the actual semi-supervised components and purely supervised baselines are often quite competitive. The addition of an ablation / analysis demonstrating the contribution of the second term in settings where labels are also available would strengthen the author's claims and fully demonstrate the model is an effective semi-supervised learner.\n\nAdditional comments:\n\nThe datasets introduced in the paper do seem like potentially valuable contributions to the community - more discussion on the motivations behind their creation, the differences with prior work, the open difficulties / challenges, as well as the recommended evaluation protocols could add to their value.\n\nThe authors could more clearly motivate the work / applications the paper is interested in and how each contribution / experiment fits in with this. Without a clear sense of the authors mission / goals with the work, it feels a bit difficult to interpret the results. \n\n"}