{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThe paper is reasonably clear (though see some of my detailed comments on writing below).\nThe idea seems well-motivated and somewhat new though not revolutionary,\nand the new data sets are nice (though see comments below about how I'm not qualified to evaluate them),\nbut I don't understand why the proposed algorithm wasn't evaluated on existing data sets as well,\nand I don't understand why it wasn't compared against other algorithms that purport to do the same thing.\nBarring certain exceptions (again see detailed comments), it seems like those two things are things\nwe ask of essentially all machine learning paper submissions, and I don't see why this paper is different?\nI am open to being persuaded, but as of now I can't recommend acceptance.\n\nDetailed Comments:\n\n> More importantly, only using 5% of the labelled data significantly improves the disentanglement quality.\nHard to parse.\nDoes this mean using only 5% of data is better for disentangling than using 100%?\n\n\n>  Generative adversarial networks (GANs) (Goodfellow et al., 2014) have achieved great success at generating realistic images, such as StyleGAN\nStrange sentence - styleGAN is not itself a realistic image.\n\n\n>  the controllable generation of high-resolution images is possible\nNit: i would cut the 'the'\n\n\n> which characterizes how significant that the model can change a factor.\nWhat does this mean?\n\n\n> present a state-ofthe-art challenge\nWhat does it really mean for a challenge to be state of the art?\n\n> that enables conditional generation of high-fidelity images\nPretty minor nit, but I don't think it makes sense to refer to samples themselves as high fidelity.\nHigh fidelity to what?\nIt makes a little more sense to refer to a trained generator as having high fidelity to the training data set,\nbut TBH I still don't even like the phrase in that context. \n\nFig 1 is good. \nI felt like I could understand what's going on mostly from looking at the figure, which is nice.\n\nThe section surrounding eqs 1 and 2 is a little hard to read for me. \nLots of single letter variable names and it's hard to keep them all in my head when I read the equation.\nI'd replace e.g. c_r with \\text{code}_r and so forth.\n\n> Note that AC-StyleGAN reduces to an InfoGAN variant in the special case\nThis is helpful.\n\n> AC-StyelGAN\n\n>  FCStyleAGN\n\n> by symmetry\nI don't follow this part.\n\n\n> while its high-resolution blocks accounts for fine styles\nI feel like it's worth making a distinction between high frequency bits of an image and non-essential (or nuisance variables or whatever) bits.\nThey often are the same, but not always (it really just depends how close up your photo is, right?) but this technique is really separating high-freq from low-freq, IIUC.\n\n>  handling complex high-fidelity images\nAgain, I just don't feel like this phrase makes any sense.\n\nRe eq 3:\nI guess you can call this the 'interpolation variance'\nif you want, but it's not really a new thing.\nYou're just getting discretized measurements of the Jacobian of the mapping from code to predicted code, and I don't really think in a way that fits with the\nintuition you describe.\nThere are a few things I can think of that are weird about this measurement, but here's just one:\nI might have a dimension of the code that causes one really peaky change at one point, and then the change quickly reverts.\nIf i measure the variance of the output, I won't see much (in fact, depending on how you discretize, maybe I'll miss it altogether),\neven though the fact that the change is peaky doesn't say anything about the 'importance' of the change.\nI think you want something more like a line integral.\nIt's possible I'm misunderstanding something about the description of this, however.\n\nRe: the experiments:\nI like the new data sets, although I'm not familiar enough w/ the robotics literature to know whether they add something really new.\nWhat I don't understand is why there's not really any attempt to do either of:\na) compare the models from the paper to existing models \nb) evaluate the models from the paper on existing data-sets?\n\nI understand that sometimes one comes up with a thing that does something totally new,\nand then reviewers will (maybe unjustifiably) still insist that the some kind of comparison be made even though it doesn't make sense,\nbut that doesn't seem to be what has happened here?\nThis paper proposes a new algorithm for synthesizing images and controlling various attributes of the images, but this has certainly been done in the past,\nso why can't your technique be compared to those prior techniques?\n\nSimilarly, there are lots of data sets that people are already familiar with that you could have evaluated these models on, right?\n"}