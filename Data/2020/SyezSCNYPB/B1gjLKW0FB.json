{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== A. Summary ===\n\nThis paper proposes to train a new conditional GAN model that allows for controllable image generation by changing the input factors of variations (e.g. object color).\nThe supervised labels for the controllable attributes are obtained from a 3D renderer.\nThat is, the work combines the recent StyleGAN (that learns to generate images with disentangle latent vectors in an unsupervised manner) with AC-GAN (a clas-conditional GAN but here class information is replaced by the attribute information that we want to control).\nThe resultant AC-StyleGAN has essentially two latent vectors, one trained unsupervised and one trained with supervised labels.\n\nThe proposed GANs were thoroughly tested with different factors of variations (lighting, camera, objects) and on two different datasets self-contructed via 3D renderer.\nThe work is a solid demonstration that GANs can be used to synthesize images with fine-grained and coarse controllability if we have supervision signals!\nThe authors also released the anonymous code (which is a plus!).\n\n\n=== B. Decision ===\n\nWeak Reject.\n\nI voted for Weak Reject because this paper presents a fairly incremental advance over what has been done (e.g. HoloGAN, StyleGAN, AC-GAN).\nThe impact of the unsupervised work (e.g. HoloGAN or StyleGAN) is much higher since they are generally applicable to real data without labels.\nHere, although reasonable, the demonstration was done on a relatively small-scale 3D synthetic data (where there is only one scene and one object being manipulated).\nTherefore, the claim that only 5% of supervised labels is required may not carry over to larger-scaled datasets with larger scene variability.\nPlus, I don't see any baseline whatsoever being compared with the proposed methods here. \n\n\n=== C. Suggestions for Improvement ===\n\nI have no problems with the novelty or idea of the work.\nFor me, the key problem with this work is the low impact or significance.\n\nSome suggestions for showing the impact of this work:\n- Show how your pre-trained GANs can be fine-tuned or transferred to the real data where we don't have labels.\n- You could also plug in your pre-trained GANs to a separate synthetic-to-real image translation model to show that we could indeed learn to control these factors of variations of the real images. Hopefully, would the above setups yield better results than HoloGAN or StyleGAN?\n- Clarify the main focus points of this paper and try to substantiate the result. The author wrote \"Our work extends the above works by scaling up the disentanglement learning to high- resolution images, and emphasizing the importance of supervision in controllable generation.\" <---- but (1) high-res images here are synthetic and limited in scene variability; (2) the second part is expected given previous work."}