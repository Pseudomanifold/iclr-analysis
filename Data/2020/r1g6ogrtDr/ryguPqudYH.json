{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "*Paper summary*\n\nThe paper combines attention with group equivariance, specifically looking at the p4m group of rotations, translations, and flips. The basic premise is to use a group equivariant CNN of, say, Cohen and Welling (2016), and use self-attention on top. The authors derive a form of self-attention that does not destroy the equivariance property.\n\n\n*Paper decision*\n\nI have decided that the paper be given a weak reject. The method seems sound and I think this in itself is a great achievement., But the experiments lack focus. Just showing that you get better accuracy results does not actually test why attention helps in an equivariant setting. That said, I feel the lack of clarity in the writing is actually the main drawback. The maths is poorly explained and the technical jargon is quite confusing. I think this can be improved in a camera-ready version or in submission to a later conference, should overall acceptance not be met.\n\n\n*Supporting arguments*\n\nI enjoyed the motivation and discussion on equivariance from a neuroscientific perspective. This is something I have not seen much of in the recent literature (which is more mathematical in nature) and serves as a refreshing take on the matter. There was a good review of the neuroscientific literature and I felt that the conclusions, which were draw (of approximate equivariance, and learned canonical transformations) were well motivated by these paper.\n\nThe paper is well structured. That said, I found the clarity of the technical language at times quite difficult to follow because terms were not defined. By way of example, I still have trouble understanding terms like \u201cco-occurence\u201d or \u201cdynamically learn\u201d. In the co-occurence envelope hypothesis, for instance, what does it mean for a learned feature representation to be \u201coptimal in the set of transformations that co-occur\u201d. Against what metric exactly would a representation be optimal? This is not defined.\n\nThat said, I feel that the content and conclusions of the paper are technically sound, having followed the maths, because the text was too confusing.\n\n\n*Questions/notes for the authors*\n\n- I would like to know whether the co-occurence envelope hypothesis is the authors\u2019 own contribution. This was not apparent to me from the text.\n- I\u2019m not sure what exactly the co-occurence envelope is. It does not seem to be defined very precisely. What is it in layman\u2019s terms?\n- I found the section \u201cIdentifying the co-occurence envelope\u201d very confusing. I\u2019m not sure what the authors are trying to explain here. Is it that a good feature representation of a face would use the *relevant* offsets/rotations/etc. of visual features from different parts of the face, independent of global rotation?\n- Is Figure 1 supposed to be blurry?\n- At the end of paragraph 1 you have written: sdfgsdfg asdfasdf. Please delete this.\n- I believe equation 4 is a roto-translational convolution since it is equivariant to rotation *and translation*. Furthermore, it is not exactly equivariant due to the fact that you are defining input on a 2D square grid, but that is a minor detail in the context of this work.\n- Now that we have automatic differentiation, is the section on how to work out the gradients in Equations 5-7 really necessary?\n- In equation 8 (second equality), you have said f_R^l(F^l) = A(f_R^l(F^l)). How can this be true if A is not the identity? Giving the benefit of the doubt, this could just be a typo.\n- Please define \\odot (I think it\u2019s element-wise multiplication).\n- Are you using row-vector convention? That would resolve some of my confusion with the maths.\n- You define the matrix A as in the space [0,1]^{n x m}. While sort of true, it is more precise to note that each column is actually restricted to a simplex, so A lives in a subspace of [0,1]^{n x m}.\n- I think it would have been easier just say that you are using a roto-translation or p4m equivariant CNN with attention after each convolution. Then you could derive the constraint on the attention matrices to maintain equivariance. It would be easier to follow and make easy connections with existing literature on the topic."}