{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Motivated by group action theory, this paper proposes a method to obtain \u2018equivariant\u2019 neural nets given trained one, where \u2018equivariant\u2019 refers to a network that gives identical output if certain symmetry of the dataset is performed on the input (for example, if we rotate a sample the predicted class should not change). \n\nAfter reading the paper, I don\u2019t understand the experiments section. In particular, it is not clear to me how the proposed method differs from regular data augmentation, as to my understanding, the input to conv1 is copied 4 times and performed rotation for 0, 90, 180 and 270 degrees and the 4 times increased number of parameters (in-depth) of conv1 are shared. Furthermore, the same rotations are performed to the input to the second layer-conv 2: as the augmentation is cyclic I don\u2019t understand why the authors perform this operation second time. Could the authors elaborate on this? After reading this section, I don\u2019t understand the proposed fine-tuning procedure (pre-train, finetune and test): (1) what is the accuracy of the pre-trained network that was started from? (2) how is the initial network fine-tuned and modified? (as the authors mention that during training the samples are not rotated). Also, I am confused with the first sentence on page 8: \u2018the complexity of the constructed network does not grow in terms of the number of parameters\u2019. It would be useful if the results in Fig. 4 are more clearly illustrated.\n\nIs the order or increased computation 4x4x4? It would be useful to compare the method (computation & performance) with a baseline where the dataset is enlarged with data augmentation. The authors mention in the introduction that this increases training overhead, whereas the proposed practical method increases the computation at inference as well as the memory footprint of the model and the forward pass. It would be useful if the authors compared empirically with baselines with (1) data augmentation (2) network with an increased number of parameters (same as the proposed one).\n\nIn summary, the idea of using group action theory seems interesting. However after reading the paper, it is not clear to me how the idea is carried out, and although the authors provide theoretical justification, it is not clear how this connects with the practical proposed method and whether it outperforms standard data augmentation (see above). Moreover, I find the writing of the paper quite unclear (see discussion above and examples below).\n\n- If digits 6 or 9 are rotated the label changes, how does the proposed method handle this?\n- page 8, conclusion: The authors claim that the proposed approach yields a \u2018significant reduction in the design and training complexity\u2019. I don\u2019t understand relative to what this comparison refers to, as the regular data augmentation approach is more straightforward in my opinion. Also, given that this is pointed as an important contribution, in my opinion, an empirical comparison must be done with such baselines (see above).\n- Page 1: it is mentioned that \u2018the number of parameters can be the same as the original network\u2019 but the experiments do not include such architecture. After reading the paper I don\u2019t understand how such a network can be implemented and whether it works.\n\n\u2014 Minor \u2014\n- Page 1 & 1par-Pg2: I don\u2019t understand what the authors mean by \u2018uniformly *across layers* of NN? \n- Page 2: In these existing works, \u2026 I don\u2019t understand this sentence\n- Page 2: our .. method use -> uses\n- Page 2: map over the orbits. I don\u2019t understand this\n- Page 2: the first truly equivariant NN. After reading Sec 5 I don\u2019t understand this point.\n- Sec. 4: how to equivarifying -> equivarify\n- Page 4: \u2018pick a generator\u2019, would recommend elaborating this term or only mentioning g as an element of G for clarity for readers unfamiliar with group theory\n- What is the testing accuracy if rotated for different angles than trained (e.g. 45 degrees)?"}