{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "In this paper, the authors propose a method for making a neural network equivariant. Their method also can be applied to make each layer equivariant too. \n\nStrengths:\n-- The paper is very well written and easy to follow with clear notation. \n\n-- The derivations seem to be correct.\n\n\nWeaknesses:\n-- The experiment is nice but very limited and does not demonstrate the benefits of having an equivariant network. For example, the authors do not report the accuracy of recovering the original (0) rotation.\n\n-- The novelty of the work is questionable. While the development is different, the final example for equivarification of a neural network is very similar to the existing works by Cohen and Welling.\n\n-- There are other works on equivarification that are missed by this paper. For example, consider the following paper:\nLenssen, J. E., Fey, M., & Libuschewski, P. (2018). Group equivariant capsule networks. In NeurIPS.\n\n-- The layer-wise equivariant method does have extra computational overheads.\n\n-- The fact that we have to specify the groups that we want to make the network equivariant with respect to is a limitation. The promise of capsule networks, in contrast, is to \"ideally\" learn the pose (variation) vectors in a data-driven way.\nSabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic routing between capsules. In NeurIPS.\n\n-- The following statements need more explanation:\n  * \"However, these may require extra training overhead as the augmented data increase.\""}