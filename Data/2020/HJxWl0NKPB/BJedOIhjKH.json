{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper takes a look at using active learning techniques instead of random sampling for the \n\"state-of-the-art\" semi-supervised learning (SSL) method MixMatch. At least for this one SSL algorithm, the authors give a strong argument that active learning helps MixMatch (4x label efficiency in some cases) and highlight the active learning algorithms that work best. An additionally interesting point is the value of labeled vs unlabeled data in this setting. For these reasons, I argue for acceptance of this paper. However, I have some reservations which are given below, that perhaps the authors can clarify.  \n\n\nThings that would have improved my score:\n\n - This paper relies on MixMatch very heavily as the sole semi-supervised technique. It would be nice to see more of an argument for this choice of a relatively recent paper that hasn't stood the test of time.\n\n - I am confused why the authors \"report the median of the last 20 checkpoints' accuracy where a checkpoint is computed every 65,536 training iterations\". As we see later, the authors train after each batch of selected examples for 32,768 iterations which is half of the time between checkpoints. Can the authors comment on this choice?\n\n\nMinor comments:\n\n - The end of the abstract makes it sound like like the conclusions are universal (\"quickly diminishes to less than 3x once more than 2000 labeled example are observed\"). I would be surprised if the authors meant this as a universal statement since no theory is provided and the experiments are on similar datasets.\n\n - I don't understand why section 3 is not simply titled \"MixMatch\" since the paper doesn't really touch any other \"modern SSL\" methods.\n\n - In the experiments section, 262144 and 32768 iterations seem to come from nowhere. Only later did I realize that these were powers of 2. Can this be clarified?\n\n - What's the difference between MixMatch and MMA with random selection? Shouldn't these perform the same (which they seem to anyways)?\n\n - I really like that this paper assesses the label efficiency of their algorithm, rather than merely reporting raw accuracy numbers which aren't as meaningful.\n\n - I wonder if MMA seems to not give as big gains on CIFAR-100 because the batch size is 10x larger. Generally, I've found that active learning (especially uncertainty sampling methods) work best with smaller batch sizes and I'm not sure I agree with the reasoning that more classes mean one should select larger batch sizes.\n\n\n\n\n\n\n"}