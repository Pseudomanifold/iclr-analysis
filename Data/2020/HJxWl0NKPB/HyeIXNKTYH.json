{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to combine active learning techniques with MixMatch for semi-supervised learning. First, they review active learning and semi-supervised learning, especially MixMatch. Instead of traditional semi-supervised learning with a fixed set of labeled examples, they incrementally grow the labeled set as the training process goes on. They consider several different choices in active learning strategies: uncertainty measure and diversification. Diversification methods are used to balance the samples in different classes and ensure diversity. The cost analysis of adding labeled vs unlabeled data looks interesting. They perform an empirical evaluation on image benchmarks and improve over MixMatch.\n\nOverall, the paper is clearly written and easy to follow. However,  I cannot recommend acceptance because\n\n1. Novelty concern. The combination of two existing techniques seems not novel enough.\n\n2. Missing important baselines in related work and experiments. In the related work on semi-supervised learning (Section 3), the authors only review MixMatch but neglect other literature, e.g.[1,2,3,4]. And semi-supervised learning has a long history and it is not restricted to recent deep learning-based approaches. A thorough review can make the approach well-placed in the literature. In experiments, the authors only compare with MixMatch. I suggest that the authors include the missing literature in the next version.\n\n3. The cost analysis is the most interesting to me. However, Figure 2(b) in Section 5.3 is weird. How can the ratio less than 0? According to the definition in Section 4.3, $L_i \\subset L_{i+1}$ and $|L_{i+1}| > |L_i|$ and similar case for $U_{i,j}$, the cost ratio should not be less than 0. I'm also confused by the explanation in Section 5.3.\n\n4. From Figure 1(b) and Table 2, we can see that on CIFAR-100, the improvement of the proposed MMA is not statistically significant, especially when the label budget is low. But on a simpler dataset CIFAR-10, MMA performs better. How does MMA perform on a more challenging task with more classes, e.g. ImageNet?\n\n5. Training time comparison. At the expense of spending more time on selecting uncertain examples and techniques like k-means clustering, MMA is slightly better than MixMatch. A comparison of training time and complexity would be better to convince me.\n\n\n\n***\nMinor:\npage 4 \u201cStarting with from a fixed pool of n unlabeled sample\u201d\npage 4 \u201cA corollary question is how do various accuracy targets relate to each other?\u201d \npage 5 \u201cWhile there are there additional active learning\u201d\npage 5 \u201c Let\u2019s define cl and cu as the cost of respectively obtaining a new labeled sample and a new unlabeled sample.\u201d --> costs\n\n\n\n***\nReferences\n[1] Temporal Ensembling for Semi-Supervised Learning, ICLR 2017.\n[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018.\n[3] Realistic Evaluation of Semi-supervised Learning Algorithms, NeurIPS 2018.\n[4] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average, ICLR 2019.\n\n"}