{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose to improve abstractive summarization models by using pretrained embeddings, theme modeling and denoising.\n\nThey propose a very interesting idea: to leverage the lead bias in news article to build supervized summarization task from 21.4 M of articles. Details are given how to produce this supervized data using simple heuristics.\n\nThe  model is  train with a denoising loss, by introducing 2 types of noise (tokens from other article and sequence shuffle). Theme modeling is also introduced as a classification problem  (same as BERT) :  the system must learn to classify pairs of sentences from the same article and pairs from different articles. \n\nExperiments are conducted on 3 datasets. The proposed model outperforms the other unsupervized abstractive models and provides results closed to unsupervized extractive models, with a metrics which favors extractive models. Ablation study shows that pretraining yields most of the impact, whereas improvements due to theme modeling and denoising loss are marginal. \n\nIn the Article example : \n\"in the wold\"  ?\n\nConclusion : \n- dataset-agnostic : I don't see why since the approach take advantage of the lead bias.\n- \"outperforms previous systems by significant margins\" : excessive. \n"}