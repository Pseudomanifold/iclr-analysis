{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work describes a method for finding mixed-strategy Nash equilibria in (normal form) games with continuous action spaces. Their work builds off (Raghunathan '19), which is a gradient-based method for finding pure-strategy Nash, based on the NI function. (It comes with all the standard caveats of gradient-based methods for finding Nash). The central contribution of this work is to parameterize a mixed strategy via a learned NN mapping from a simple distribution U[0,1]^d to the mixed strategy of interest.\n\nMy first impression on reading this work is that this learned parameterization of the mixed strategy is exactly how the GAN generator produces a distribution of samples (images). The generator network takes a noise vector sampled from U[0,1]^d or some other distribution, and samples an \"action\" (i.e. an image). Indeed, the original GAN paper (Goodfellow, 14) talks about this: \"Generative adversarial nets are trained by simultaneously updating the discriminative distribution so that it discriminates between samples from the *data generating distribution*\" (Fig. 1). It is not the case, as stated in Section 2, that GANs only produce pure strategies. \n\nThe optimization rule for GNI is different than standard update rules: specifically, there are two step sizes \\lambda and \\rho, if \\lambda -> 0 I think it becomes steepest descent but for \\lambda > 0 it is kind of like an optimistic descent method (because each agent takes a gradient step assuming the other agents have already done SD by \\lambda). This update rule has some desirable theoretical properties, which I believe are mostly proven by Raghunathan '19. It would be very useful to state more explicitly which of these properties are not known to be true for e.g. independent steepest descent for each agent.\n\nThe experimental section is nice in that it shows some toy games where mixed strategy continuous equilibria do better than pure-strategy ones, and shows that MC-GNI can find something better than a pure-strategy equilibrium. But if the claim is that GNI has these good equilibrium-finding properties, and you're already using the GAN decoder strategy, why are there no experiments on GANs? Even Raghunathan has experiments on GANs (albeit ones with pure-strategy (delta-function) equilibria).\n\nNits:\n- MC-GNI is a confusing name because MC usually refers to \"Monte Carlo\" in this context\n- I'm confused at the end of section 3 how \"implying (sic) gradient descent on these function parameters\" is performed. Are points sampled from U[0,1]^d and used as an estimator for F_i, as in a GAN? I don't see how the non-MC-sampled F_i can be computed. It would be nice to clarify one way or another.\n- I don't see \\lambda mentioned on the RHS of the last equation in sec. 3.\n"}