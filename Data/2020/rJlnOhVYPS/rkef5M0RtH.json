{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an unsupervised domain adaptation method for person re-identification. The proposed method handles noises on pseudo labels created by unsupervised clustering. Two networks are used for training, and the estimated confidences of other models are used for the next training iterations. The temporally average model is used for each network to avoid error amplification. Also, soft softmax-triplet loss is proposed to handle soft labels for triplet loss. \n\nThe handling label noises in unsupervised domain adaptation on person re-identification are new. The proposed model produces very high performance and the contribution for person re-identification community is good. \n\nHowever, I would like to see more insights into the proposed model for the contribution of the general deep learning conference. \n\nFirst, this paper lacks a survey of works on handling label noises. For example, \nB.Han, Q.Yao, X.Yu, G.Niu, M.Xu, W.Hu, I.Tsang, M.Sugiyama, Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels, NeurIPS2018. \n\nI could not fully understand why the temporal averaging of model parameters prevents the two models from being the same. I would like to see a theoretical explanation or experimental evidence for this claim. \n\nThe proposed method also uses noisy hard pseudo labels for training, as shown in Eq.(9). \nWhy are the noisy hard labels used? What is the performance when only soft labels are used for model updates?\n\nIn the experiment, \\lambda^t_{id} = 0.5, \\lambda^t_{tri} = 0.8 are used. Why these parameters are different between softmax and triplet losses?\n\np.1 (Zhang et al., 2018b) and p.5 (Zhang et al., 2019a) are missing in references. \n\n\n"}