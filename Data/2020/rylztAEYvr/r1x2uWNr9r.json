{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a data augmentation strategy for a class of problems that the amount of labelled data is limited while the evaluation procedure is easier. Specifically, they are able to incorporate some of the model\u2019s output into training data to guide the training procedure. The idea is quite simple and effective according to empirical results.\n\nPros:\n    The idea is quite simple and easy to implement. It can be applied into a broad problem. Also, it\u2019s not tied to specific neural architecture\nSection 4 provide an interpretation from view of EM algorithm, which is quite interesting.\n    The improvement of empirical studies are desirable, validating the effectiveness of the proposed method on two complex tasks, including molecule optimization and program synthetic.\nCons:\n    K in Algorithm 1 plays an important role. I expect more discussion of how to select K. Is K fixed during training procedure in different epochs? Is K the same magnitude as size of original training dataset.\nIn molecular optimization task, i suggest authors to add more details on setup. For example, on DRD2 and QED, what\u2019s QED(X) and DRD2(X)? In success metric, what is your required constraint on both similarity and property score?\nExtra:\n    I don\u2019t understand why the earlier collected data pairs are thrown away. Have you tried the strategy that incorporate all the augmented data?"}