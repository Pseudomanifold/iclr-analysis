{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new option discovery method for multi-task RL to reuse the option learned in previous tasks for better generalization. The authors utilize demonstrations collected beforehand and train an option learning framework offline by minimizing the expected number of terminations while encouraging diverse options by adding a regularization term. During the offline training, they add one option at a time and move onto the next option when the current loss fails to improve over the previous loss, which enables automatically learning the number of options without manually specifying it. Experiments are conducted on the four rooms environment and Atari 2600 games and demonstrate that the proposed method leads to faster learning on new tasks.\n\nOverall, this paper gives a novel option learning framework that results in some improvement in multi-task learning. While the paper is technically sound and somewhat supported by experimental evidence, the experiments are limited to low-dimensional state space and discrete action space. I do wonder if the method can scale to high-dimensional space with continuous control.\n\nMoreover, the framework requires optimal policies to generate trajectories for offline option learning, which seems to add more supervision signals than prior work such as option-critic. I wonder how the method would perform under sub-optimal demonstrations or even random trajectories generated by some RL policy.\n\nFinally, I wonder how this method can be compared to skill embedding learning methods such as [1], which have been shown to be able to compactly represent skills in a latent space and reuse those skills in high-dimensional robotic manipulation tasks.\n\n[1] Hausman, Karol, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. \"Learning an embedding space for transferable robot skills.\" (2018).\n"}