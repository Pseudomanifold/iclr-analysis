{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes the \"confidence-calibrated adversarial training (CCAT)\" to train robust DNNs against adversarial examples. The idea is to perform adversarial training with gradually smoothed network predictions (in an exponential or power rate to perturbation size). This new approach seems can help detection and defense at the same time to some extent. While the idea is quite interesting, the paper is poorly written and many times hard to read. \n\nHere are some detailed comments:\n1. Introduction. It is hard to get how the \"robustness-accuracy trade-off\" and the \"poor generalization to other or stronger attacks\" are addressed by CCAT. I can hardly agree that \"standard adversarial training strongly depends on the training attack\". PGD-AT generalizes pretty well to other types of attacks such as CW. It needs more concrete evidence to make such a claim.\n2. Related work can be improved. There are a few places inaccurate descriptions, like \"White-box attacks utilizing projected gradient ascent to ...\". And since the focus of this paper is adversarial training, related works deserves more detailed discussions.\n3. Section 3, the 100%/50% adversarial training is not properly explained. For example,  in (3), I cannot tell how the 50% comes from, as input x in the two components are the same. \n4. The confidence calibration seems related to the reverse cross entropy (RCE) proposed in [*] for detection?\n5. Figure 1, missing legends. I cannot find where the PGD-Conf is defined when came across Figures 1/2.\n6. Experiments. The settings are not standard, which makes results hard to interpret. The Err, RErr metrics are confusing. Not sure what 99% TPR is for, detection?\n7. Hard to tell the real improvement. For example , in Table 2, CIFAR-10, L_{\\infty}=0.0.3, CCAT has even lower RErr (\\tau =0) and ROC (\\tau@99%TPR) than standard AT.\n\n[*] Pang, Tianyu, et al. \"Towards robust detection of adversarial examples.\"\n\nAdvances in Neural Information Processing Systems. 2018.\nMissing citations  of a few recent papers in adversarial training:\n[1] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML, 2019.\n[2] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" ICML, 2019.\n[3] Carmon, Yair, et al. \"Unlabeled data improves adversarial robustness.\" NeurIPS, 2019.\n[4] Uesato, et al. \"Are Labels Required for Improving Adversarial Robustness?\" NeurIPS, 2019.\n\nA few additional high-level suggestions: \n1) Show some concrete examples where standard adversarial training fails while CCAT is not. \n2) Show the formulation of CCAT a little earlier in Section 3. Then link back to standard adversarial training or other defense methods. Not the other way around like it did now (taking too long to get to the idea of CCAT). \n3. Use a standard experiment setting, and compare 1-2 more existing methods fairly. \n4. Show understanding and analysis on CIFAR-10 dataset instead of SVHN.\n"}