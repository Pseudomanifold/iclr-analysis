{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a new adversarial training scheme to improve generalization of robustness over unseen threat models, e.g. larger perturbations or different noise distributions. The key idea is to impose uniform confidence on the adversarial examples depending on the distance from the original example, based on a pre-determined distance metric, e.g. L-infinity distance. Experimental results shows its effectiveness on detecting adversarial examples and unseen robustness for MNIST, SVHN, and CIFAR-10 datasets, under L-infinity and L-2 adversaries.\n\nIn overall, I agree that improving generalization over unforeseen adversaries is a very important problem in adversarial training, and the paper addresses this problem with a novel approach. The manuscript is generally well-presented with clear motivation. In particular, I appreciated the simplicity of the proposed idea, and the thoroughness of experiments as a defense paper, e.g. presenting per-example worst-case results across diverse attacks. However, I am currently on a slightly negative side, due to some unclear points in the experimental results. I would like to increase the score if the issue could be addressed, regarding the importance of the problem and their approach, which seems valuable to be shared in the community.\n\nMainly, it is still hard for me to interpret the presented experimental results at the positive side, unless the authors could further address on the important points of the results. In general, the results are not that clear as claimed, especially when tau=0, to show that CCAT improves robustness: At the original threat model (L-inf with the smallest epsilon), CCAT shows much inferior results across all the datasets. It does improves the L-2 results, except for CIFAR-10 which should be a bare baseline to show the scalability of the method. Although the paper also point out that CCAT sometimes achieves much lower clean test error, but sometimes this also signals the less robustness. I hope the paper could justify such points in the Table 2 for better presentation.\n\n- Perhaps ResNet-20 is too small for CIFAR-10 tasks, as the training loss would hardly minimized into 0? I think WRN-10 is a more fair standard for CIFAR-10 in AT, and wonder if the result could show more effectiveness (or get more aligned tendency across datasets) of the proposed method if the capacity of network is increased.\n- Apart from the thoroughness of the attack methods assumed, considering only L-inf and L-2 adversaries may be not enough to claim the \"generalization ability\" of a defense. Could the method also improves robustness against other attacks, e.g. general corruption (such as MNIST-C, CIFAR-10-C), or unrestricted adversarial examples?\n- Eq 4: Does it mean that the logits other than the 2nd predictions are not considered when generating adversarial examples? Personally, I don't much get the motivation of using this objective, even while it is described in below Eq. 4."}