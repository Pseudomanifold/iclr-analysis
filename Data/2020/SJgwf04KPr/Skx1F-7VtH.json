{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary\n========\nThis paper proposes a more granular adversarial training scheme, where the model is trained to have decaying confidence as the size of the adversarial perturbations increase. \nThe idea is quite natural and well motivated and presented. The paper suffers in its theoretical and evaluation sections though, which were both fairly confusing to me. I lean towards rejection for this paper but would consider changing my score if the presentation was improved.\n\n\nComments\n=========\nThe observation that adversarial training enforces high confidence in the entirety of the epsilon ball is interesting, and the motivation for the proposed scheme is sound.\n\nYet, while the intuition of CCAT for addressing the accuracy-robustness trade-off makes sense to me, the purported beneficial effect for perturbations outside the given epsilon ball are not as clear.\nThe authors note in Section 3.1 that adversarial training gives no indications on how the model should behave outside the given epsilon ball, or on other threat models. While this is true, it is not clear how CCAT supposedly remedies this. By making the confidence decay within the epsilon ball, the training also doesn't explicitly enforce any behavior outside this ball or on other threat models. So if CCAT indeed provides benefits in those settings, it isn't clear why those gains should be expected.\n\nThe exposition of the technique in Section 3.2. is very clear, and Figure 1 nicely illustrates the effect of CCAT on the class confidences during an attack.\n\nThe theoretical toy example in Section 3.3. is a bit confusing, and I'm not convinced that the proof of Proposition 1 is correct. First, a classification problem parametrized by the class imbalance is a little weird in this setting. I have a few questions about the proof:\n- You seem to be assuming that the model's confidence is calculated using a softmax. This assumption should be stated.\n- The expressions for \\hat{p}(y=2|x=x) and \\hat{p}(y=1|x=x) are confusing to me. Why are you taking a log over the softmax? These probabilities are now negative... Also, in the second step, an equality of the form e^x / (e^y + e^x) = 1+e^(y-x) is used. This is incorrect, it should be 1/(1+e^(y-x)). In the end, the values log(1+e^a) and log(1+e^(-b)) are not in [0, 1] so they cannot be probabilities.\n- From there on, it isn't clear what the rest of the proof is really saying\n\nThe experimental section is quite thorough and the results seem convincing. The considered adaptive attack, that optimizes (4) seems natural given that the defense thresholds on the model's confidence. I wonder how this is optimized though: Do you try a targeted attack for every possible target other than the true class?\n\nI found the conflation of evaluation metrics quite confusing. It seems the authors know this and have done an effort to explain their choices, but I feel like this could still be improved further. For example, some of the metrics (e.g., AUC) should be maximized, while others (Err or RErr) should be minimized. This makes the reading of the result tables quite difficult.I would also suggest dropping one digit of precision in the Tables for better readability.\n\nOverall, I have some mixed feelings about this paper. The idea is simple and well explained. The motivation, theoretical analysis, and reporting of results are somewhat confusing though. Some extra work on the presentation could definitely help this paper.\n\nSome additional references\n========================\n- In the introduction, when mentioning the trade-off between robustness and accuracy, you should cite \"Robustness May Be at Odds with Accuracy\" by Tsipras et al. rather than Schmidt et al. (that paper shows that robustness may require more training data).\n- Also in the introduction, you could consider referencing the following papers on robustness to different threat models:\n    * Engstrom et al., \"Exploring the Landscape of Spatial Robustness\"\n    * Tramer and Boneh, \"Adversarial Training and Robustness for Multiple Perturbations\"\n- In the related work, you reference a number of works for adversarial training but not the original idea in the paper by Szegedy et al.\n- In Section 3.1., the observation that the epsilon-ball around training examples can cross class boundaries is further analyzed in \"Excessive Invariance Causes Adversarial Vulnerability\" and \"Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness\" by Jacobsen et al.\n\nTypos\n=====\np.6 enumerator -> numerator"}