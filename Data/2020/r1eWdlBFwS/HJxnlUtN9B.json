{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to model multiple datasets from differing distributions with shared latent structure and private latent factors. The main techniques include architecture design which encourages the isolation of shared and private latent factors and a mutual information-based regularizer. The paper is clearly written and easy to follow. I enjoyed reading it. The experiments support the claim of learned population-specific representations.\n\nHowever, I found that the paper has some weaknesses:\n1. The novelty is not enough. All the techniques involved in the paper are not new but from existing literature. The idea is not new. The authors also mentioned several previous works in Section 3, e.g. Multi-level  VAEs, oi-VAEs.\n\n2. More importantly, I did not see any baselines in the experiments except vanilla VAE. As far as I understand, previous methods can be easily adapted to these tasks. For example, [1] tried continual generative modeling for a sequence of distinct distributions. Many important baselines are missing in the experiments, which makes it hard for me to evaluate how significant the work is.\n\n3. What if the populations are not exclusive? The regularizer enforces them to be isolated but they are not in fact.\n\n4. How did you choose the annealing schedule of $\\alpha$ in Section B.1?\n\nMinor:\n\npage 2  eq (1) z_i -> z_{ki}\n\npage 3 last paragraph \u201cit may desirable\u201d\n\nReferences:\n\n[1] https://arxiv.org/pdf/1705.08395.pdf\n\n"}