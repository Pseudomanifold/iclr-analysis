{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The authors introduce a mechanism for exiting the inference process early, without doing any more processing, if the network determines that there should be a classification early. The proposed mechanism is: learn separate linear classifiers on the outputs of layers in the convnet and use a decision rule based on agreement of these classifiers (and their confidences) in sequence, sometimes combined with the final output (described in Algorithm 3). \n\nThe paper is poorly written (there are many simple grammar mistakes, the methods are not well motivated, and the methods are described in a confusing way), and more seriously the evaluation of the method is not complete. \n\nDetails:\n\nThe evaluation is not complete. The threat model that the defense is evaluated under is not realistic: it assumes that the attacker does not have knowledge of the model: \"we assume that the attacker has no knowledge about the detection mechanism but has complete knowledge about the baseline DNN (white-box attack with respect to baseline DNN)\" (page 9). There are a number of mechanisms that are known that work well in this assumed threat model already, and other works have indicated why this is not a realistic threat model, see: [0]. As such the evaluation of adversarial robustness does not stand since there are no best effort, dynamic attacks on the defense presented. \n\n[0] https://arxiv.org/abs/1902.06705", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}