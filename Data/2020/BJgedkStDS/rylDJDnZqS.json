{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The aim of the paper is to reduce the misclassifications of deep neural networks in a way that is energy efficient. Misclassifications in this paper are defined as adversarial and out-of-distribution samples, and natural errors (samples in training and test sets that are misclassified). A main target of the paper is to achieve that while saving energy (number of operations per sample). The proposed approach involves adding Relevant feature based Auxiliary Cells (RACs) after one or more hidden layer to decide, based on the detected features (class-specific), whether or not to end the classification early and declare the sample abnormal or output No Decision. By detecting the abnormal samples early on, the method can save unnecessary subsequent computation operations, and thus saving energy. The added RACs blocks use binary linear classificatiers trained on the relevant features for each class, and at test time they evaluate the activated features to dicide the corresponding class with a confidence score. If the RAC units disagree on their classification, the input sample is deemed abnormal and the classification is ended early. The main trick is where to place the RAC units, as if they are placed after the first layers, the features learned so far are not enough to distinguish the classes, and if they are placed immediately before the last layer, the high-level detected features will be useful but then there would be no chance to save energy.\n\nFirst of all, the paper, especially the introduction, is not well written. The paper requires an unnecessarily high amount of time to grasp how the method works. The method is actually pretty simple. The problem under study is of relevance, as recent workshops on green ML at NIPS and ICLR have indicated. I find the proposed solution, however, quite heuristic. Why does it work? The paper does not shed light on this. \n\nThere is a risk that the use of binary linear classifiers in RACs might end the classification immaturely, as the feature space might not (yet) be linearly separable and might still need more nonlinearities to classify it correctly. This is shown in the experimental results where the deeper nets with large number of classes have larger test error when RACs are used while the save in energy is not worth it (Figure 4). For a method that is proposed to reduce the natural errors, it's not clear how this serves the main target.\n\nThe hyperparameter tuning in the proposed method was dataset-specific, which affects the generality of the method aplicability.\n\nQ.: For which other settings can RACs be put to good use?\n\nMinor:  Note that the description of Figure 2 differs from the legends on the plots.\n"}