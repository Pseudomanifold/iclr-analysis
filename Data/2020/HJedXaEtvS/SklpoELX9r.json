{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper makes a convincing case for improving the usefulness of production level, large scale models by making them quickly editable without extensive retraining. A standard meta-learning method called MAML is used to augment the initial training phase of such models, or to \"patch\" them later on. The paper demonstrates effectiveness for both image classification and machine translation tasks, covering a wide range of relevant scenarios.\n\nI recommend acceptance because:\n- The paper considers a real issue for production models which are becoming widespread, and retraining for targeted modifications is impractical.\n- Experiments are consistently well designed and executed. The proposed benchmarks are relevant for future work as well.\n- The paper is clear and easy to read.\n\nThe method proposed in [1] is very similar, even if it is used in the continual learning settings. I would have liked to see some continual learning solutions used as baselines and/or combined with the proposed method, as they do manage to improve performance in [1]. Some candidates would be L2-regularization and EWC [2].\n\nReferences:\n[1] Khurram Javed, Martha White. Meta-Learning Representations for Continual Learning. CORR 2019. https://arxiv.org/pdf/1905.12588.pdf\n[2] James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks. PNAS 2017. https://arxiv.org/pdf/1612.00796.pdf"}