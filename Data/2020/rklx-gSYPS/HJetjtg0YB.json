{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Firstly, the motivation of the proposed method is not convincing for me. The authors want to propose a general methodology for learning precondition by supervised learning setting. However the method in practice, the x is a complex distribution, it is difficult to handle the map between the gradient and the x. This method proposes log-scaling, but it needs to be stored with a precision of approximately 15 decimal places and the regressed model will be a piecewise constant function, which is very computationally time-consuming.\n\nSecondly, the experimental results are not sufficient for evaluation. This paper shows two\nThe experimental result which includes the result of power function and the logistic function. But\nIt is not clear that the whole process of dual space preconditioned method with the model of computation of precondition given. And without quantitative results given, it is not convincing the \u201cdramatic\u201d speedups\u201d of these methods, because the surprising training process is off-line and time-consuming. On the other hand, because of the different forms of the convex objective function, the network will train for the specific convex objective functions. In my opinion, it is not a general method to lead to dramatically speed up.\n\nFinally\uff0cthe function of x and the gradient is complex, it is difficult to predict the relationship by using a simple network,\n"}