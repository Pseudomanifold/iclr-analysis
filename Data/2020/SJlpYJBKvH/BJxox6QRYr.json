{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a unified way to provide robust statistics in evaluating RL algorithms in experimental research. Though I don't believe the metrics are particularly novel, I believe this work would be useful to the broader community and was evaluated on a number of environments. I do have a few concerns, however, about experimental performance per environment being omitted from both the main paper and the appendix.\n\nComments: \n\n+ I think this is a valuable work and the ideas/metrics are useful, though I'm not sure I would call them novel (CVar and the like have been seen before).  I think the value comes in the unification of the metrics to give more robust pictures of algorithmic performance. \n+ The details of all of these evaluations and individual performance should be provided in the appendix, however, it seems only MuJoco curves were included. Moreover, it says that a blackbox optimizer was used to find hyperparameters, but these hyperparameters were not provided in the appendix or anywhere else as far as I can tell. I think it's important for a paper which recommends evaluation methodology in particular to be more explicit regarding all details within the appendix. I hope to see additional details in future revisions -- including per-environment performance.\n+ I believe clustering results across environments can be potentially misleading. Say that we have an environment where the algorithm always fails but is very consistent and an environment where it excels. These are blended together in the current Figures. While it requires more space, I believe it is important to separate these two. I am concerned that a recommendation paper like this one will set a precedent for only including the combined metrics of algorithmic performance across environments, masking effects.  I would suggest splitting out results per environment as well and pointing out particular cross-environment phenomena. \n\nThere is a missing discussion of prior work on statistical testing of RL evaluation:\n+ Colas, C\u00e9dric, Olivier Sigaud, and Pierre-Yves Oudeyer. \"A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms.\" arXiv preprint arXiv:1904.06979 (2019).\n+ Colas, C\u00e9dric, Olivier Sigaud, and Pierre-Yves Oudeyer. \"How many random seeds? statistical power analysis in deep reinforcement learning experiments.\" arXiv preprint arXiv:1806.08295 (2018).\n"}