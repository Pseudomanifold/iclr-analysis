{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper develops a new method for adapting models trained on labeled data from some source domain to unlabeled data in a target domain. The authors accomplish this by adapting a technique from [1] and [2] enforcing that the deep features learned during training approximately follow a Gaussian mixture distribution. With the learned features in this form, the authors ensure domain adaptation by minimizing the discrepancy between the distributions arising from the source and target datasets.\n\nStrengths:\n + The paper's experiments show an improvement in the model's performance relative to past work, utilizing a large number of comparison models.\n + The use of explicit distributional information within the learned representations seems like a good fit for the task at hand, and the authors' experiments back this up.\n\nWeaknesses:\n - The proposed method for unsupervised domain adaptation is very similar to the prototypical networks approach in [3], with the primary difference being a loss term incentivizing a Gaussian mixture distribution over features.\n - While the authors achieve improved performance over [3], the gains in classification accuracy on the target dataset aren't especially huge (~1-3%).\n - The paper is a bit hard to follow, and would be improved by giving a more explicit comparison of the methods used here to past work, especially [1] and [3].\n\n\n[1] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for loss functions in image classification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9117\u20139126, 2018.\n\n[2] Hong-Ming Yang, Xu-Yao Zhang, Fangying Yin, and Chenglin Liu. Robust classification with convolutional prototype learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3474\u20133482, 2018.\n\n[3] Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei. Transferrable prototypical networks for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2239\u20132247, 2019."}