{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "<Paper summary>\nThe authors proposed Distribution Matching Prototypical Network (DMPN) for unsupervised domain adaptation. DMPN extracts features from the input data and models them as Gaussian mixture distributions. By explicitly modeling the distributions that the features follow, the discrepancy between the distribution of source data and that of target data can be easily evaluated. DMPN is trained by jointly minimizing two kinds of loss, which are classification loss on the source data and domain discrepancy loss that is calculated via the explicit models. Experimental results on two popular benchmark datasets validate the advantage of DMPN over other state-of-the-art methods. \n\n<Review summary>\nThe proposed method seems simple but empirically performs well. The paper is well written and easy to follow, so we can maybe easily implement it. However, I have several concerns mainly about the details and theories of the proposed method, which makes my score a bit lower than the border line. Given clarifications in an author response, I would be willing to increase the score.\n\n<Details>\n* Strength\n + The motivation of using ProtoNet for domain adaptation seems reasonable.\n + The proposed method performs well in the experiments.\n + The paper, especially the experiment section, is well written and easy to follow.\n\n\n* Weakness and concerns\n - Several points on the proposed loss (GCMM and PDM) are not sufficiently discussed.\n  -- Why do we need two kinds of loss? These losses seem to play almost same role. Since PDM loss corresponds to target-side log likelihood regularization term (Eq. (3)), I wonder if we really need GCMM loss. \n  -- Since the authors explicitly model the feature distributions by Gaussian mixtures (GMs), it might be possible to calculate a standard divergence between source and target data distributions by using the parameters of GMs. Compared with such a straightforward approach, the proposed method seems to be ad-hoc and is not theoretically validated. What term of divergence (or distance) does it minimize?\n  -- When a certain class does not appear in pseudo-labeled target data, how can we calculate GCMM loss? (specifically, \\mu^{et}_c)\n  -- Are Eq. (3) and Eq. (6) correct? These are defined as total loss, not average, over each domain. It means that the scale of the coefficients for these terms changes according to the number of training data, but the sensitivity analysis in Fig. 2 does not show such effect.\n  -- Since the proposed losses heavily depend on the pseudo labels on the target data, it should be important to carefully set a proper threshold for the confidence. Is the proposed method sensitive against the change of this threshold? If so, how can we tune it?\n  -- How can we know p(c) in advance?\n\n - The theory shown in 3.5 is not sufficiently validated. \n  -- The authors state ````we minimize the first term through minimizing the domain discrepancy losses,\" but it is not sufficiently supported, because the relationship between the proposed losses and H-delta-H divergence is not clear. \n\n - I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method. \n\n\n* Minor concerns that do not have an impact on the score\n - Using both f^s_i and F(x^s_i; \\theta) is confusing.\n - Typo in Eq. (7): PMD -> PDM\n"}