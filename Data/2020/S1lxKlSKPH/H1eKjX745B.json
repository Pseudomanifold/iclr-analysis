{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The topic of this paper is out of the reviewer's domain (Bayesian optimization, RL, and neuroscience). The reviewer has been reviewing ICLR for several years. Such mismatches had not happened in the past.\n\nThe reviewer doesn't think this paper reached the bar of a good ICLR paper but hesitates to reject.\n\n\nThis work proposed a training stabilizer for GANs based on the notion of Consistency Regularization. Experimentally, the authors had augmented data passed into the GAN discriminator and penalize the sensitivity of the ultimate layer of the discriminator to these augmentations.\n\nThe authors claimed \"We conduct a series of ablation studies to demonstrate that the\nconsistency regularization is compatible with various GAN architectures and loss\nfunctions. Moreover, the proposed simple regularization can consistently improve\nthese different GANs variants significantly. \"\n\n\n\n\n\n\n\n\n\n\n"}