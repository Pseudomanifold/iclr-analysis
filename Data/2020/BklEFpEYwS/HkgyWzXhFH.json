{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary:\n\nIn this paper, the authors propose a new method to alleviate the effect of meta over-fitting. The designed method is based on the information-theoretic meta-regularization objective. Experiments demonstrate the effectiveness of the proposed model.\n\nStrong Points:\n\n+ The authors aim to alleviate the effect of meta over-fitting. In this paper, they mainly focus on alleviating the effect of brute-force memorization in the meta-training process. The problem is important in the meta-learning field.\n\n+ The motivation for this paper is clear. The authors try to maximize the mutual information between x*, \\theta and \\bar{y}^*, D. \n\n+ Experiments on both sinusoid regression, pose regression and image classification show that MR-MAML outperforms MAML and MR-CNP outperforms CNP. \n\nWeak Points:\n\n- My first concern is about the novelty of the proposed model. The framework and the derivations are straightforward. I think the problem is very important, however, the technical contribution may not enough to be accepted. It is better for the authors to clarify their contributions.\n\n- It will be more helpful if the authors can describe the algorithm of the meta-testing process in Appendix A.1. In the meta-testing process, do we need to sample \\theta from q(\\theta|\\tau)? If so, is the accuracy calculated by the averaged value of tasks with sampled weight?\n\n- I am a little curious about the results in Table 5. The results of MAML and TAML is quite high. It would be better if the authors explain more.\n\n"}