{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "################################################################################\nSummary:\n\nThis paper illustrates, identifies, and formally defines a memorization problem in meta-learning -- the model can simply memorize meta-training tasks and ignore meta-training train sets. The paper proposes to optimize the mutual information between testing predictions and the training data (given input and meta model), and upper bound it by imposing a information bottleneck between output and input+model. Unlike related work, this paper specifically is able to generalize to meta-test even when the meta-train dataset is not made confusing enough (i.e. even when model can learn well from test data in meta-train alone), making it applicable to use cases where it is hard to make the dataset confusing.\n\n################################################################################\nDecision\n\nI vote for accepting this paper, since as far as I know this paper gives a novel insight to the overfitting problem in meta learning, and has formulated the problem formally with theoretical insight, and given a working solution with strong experiment results and clean comparative studies. Despite somewhat narrow experiments and sometimes confusing writing, the paper should provide new insight to meta-learning.\n\n################################################################################\nPros:\n! DISCLAIMER: I am not an expert in this field, so take my novelty judgements with a grain of salt.\n+ Novel view into meta-learning's overfitting problem\n(1) Large models can simply memorize which input data corresponds to which task, and memorize the meta-training tasks, without being able to generalize into meta-test tasks.\n(2) Formulates this into a low mutual information between meta-train training data and predictions.\n+ Easy to implement and quite widely applicable as a regularization loss addon to multiple existing meta-learning methods\n+ Impact-wise, the paper takes meta-learning further from memorization, making methods more capable of operating on less-carefully designed, more natural datasets (rather than permutation of datasets)\n+ Experiments are clean with ablation studies and hyperparameter sensitivity tests, and method performs well across real and toy datasets\n+ Motivation part is easy to read\n\n################################################################################\nCons:\n- I'm still skeptical of the novelty of the paper, since the conclusions are, in hindsight, very straightforward.\n- Sometimes sentences are very confusing to readers.\n(1) The term \"mutually-exclusive\" is confusing because the view-point example the paper gives seems to be mutually exclusive (each task has its own kind of data, hence \"exclusive\"). It is unclear whether the task data is exclusive, or the task function is exclusive, and not straightforward to see its relationship with memorization. Consider renaming it to e.g. \"mutually-confusing\" or \"mutually-contradictory mapping\".\n(2) Can you please rename \"information-theoretic meta-regularizer\" to \"meta-regularizer using information theory\"? It is hard to read for non-native speakers.\n(3) Paragraph under definition 1 is confusing and has redundancies.\n(4) Section 4.1, not very clear how the logic goes from the decomposition to adding the upper bound to the loss, and how the other term comes in.\n- For the motivation, it is better to give examples of use cases when it is impossible to make meta-train \"mutually-exclusive\". I'm sure even in the patient example you can shuffle classes or input dimensions.\n- An experiment comparing to others in mutually-exclusive datasets would be nice to have, in order to judge how much this compensates a badly-designed meta-learning dataset.\n\n################################################################################\nImprovements:\n- Clarify each point in the \"Cons\" section.\n- Please also clarify if all methods in all experiments are hyperparameter-tuned separately, i.e. that the experiments are not favoring the MR-* model in any way. (the paper only clarifies it in one of the experiments)\n- For future work, does recent developments in mutual information modeling (e.g. MINE https://arxiv.org/abs/1801.04062) help this method in any way? e.g. try increasing mutual information between some representation of the meta-train training data and some feature vectors before the prediction?\n- First parenthesis in Section 4.1 has a misplaced space"}