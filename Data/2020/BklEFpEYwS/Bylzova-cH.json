{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyses a pitfall of current meta-learning algorithms, where the task can be inferred from the meta-training data alone, leaving the task-training data unused. Such a meta-learner would generalise well on the meta-training tasks, but will fail to generalise on new tasks at test time. This kind of overfitting is formalised as the memorization problem. This problem is implicitly resolved in current meta-learning algorithms by constructing mutually-exclusive meta-training tasks, which is not easy to construct in all scenarios. The paper introduces an information-theoretic meta-regularizer which forces information extraction from the task data (D) by restricting information flow from meta-parameters (\\theta) and input (x^*). Experimental evaluation with one gradient based and one contextual meta-learning method, on non-mutually-exclusive tasks bring out the mettle of the proposed regulariser. \n\n+ves:\n+ The characterization of the memorization problem and the proposed regularizer are novel contributions. \n+ The paper motivates the problem well, before proposing the methodology.\n+ The paper is well-organised and the experimental setting is designed in a thoughtful manner.\n+ The results are promising.\n\nConcerns:\n- The key hypothesis that the proposed meta-regularization method is based on - is that a model with memorization tends to be more complex. What is the basis for such an assumption? This is an important one for the work at the outset.\n\n- Would the proposed regularizer help if mutually-exclusive meta-training tasks are available, as it forces the model to extract maximum information from task training data (D)? The paper does not comment on this, and this would have been useful to know.\n\n- How much is the training overhead (in terms of time) incurred while adding the regularizer to the baseline methods (MAML and CNP)? The paper does not talk about this additional complexity.\n\n- Evidently, the most important results in the Experiments section are the ones in Sec 6.3. However, the results do not clearly distinguish whether the meta-regularization was performed on the activations or weights here (earlier subsections do talk about this). This makes it difficult to make a conclusive inference on what aspect of the methodology actually helped here.\n\n- There have been recent efforts that have attempted addressing overfitting in meta-learning. The paper mentions these efforts in Sec 5, and states that these have been used for existing settings where tasks are mutually exclusive. It would have been useful to include at least one of these methods in the experiments to see how the proposed regularization differs from them in practice.\n\nMinor issues:\n- The abstract says: \u201cThis causes the meta-learner to decide what should be learned from data and what must be inferred from the input.\u201d - what is the difference between data and input?\n- There are some minor typos in the work, which would benefit from a proofread. E.g: Sec 6.3 \u201cneigbhor\u201d -> \u201cneighbor\u201d\n"}