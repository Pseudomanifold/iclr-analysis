{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an unsupervised approach to learn a representation of graphs. The idea comes from an encoder-decoder architecture, which is common in related literature. The paper uses a variational Bayes approach in the learning process. Thorough experiments are provided to justify the feasibility of the method. \n\nThis paper provides an unsupervised style of learning graph representations, which may not be coupled with a specific downstream task so that it may be more useful in general; also, the experiments themselves seem to be at least comparable to the recent methods.  \n\nHowever, I vote for rejecting this submission for the following concerns. \n\n(1) I did not find too many significant differences between this paper and [Kingma & Welling, 2014] in the design of encoder-decoder architecture as well as the learning procedure (I am not an expert in this area so please correct me if I am wrong).\n\n(2) The intuition of learning the representation in an unsupervised manner is interesting and important to me, though the experiments are mostly on the classification tasks. I think it would be helpful to demonstrate the representation power of the learned representation of the graph in tackling other tasks.\n"}