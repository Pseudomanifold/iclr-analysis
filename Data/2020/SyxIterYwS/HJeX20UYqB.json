{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes an approach based on empowerment for reinforcement learning applicable to the cases that the dynamical system is unknown. The model is estimated by a water filling algorithm and is evaluated on two RL tasks.\u00a0\n\nTraining of RL agents via on empowerment and intrinsic\u00a0rewards is an important alternative to conventional\u00a0training algorithms. The paper is tacking an important paper.\u00a0\nThe paper is weak in terms of writing and motivation. Empowerment on section 3.2 could have been explained more intuitive and more thoroughly the make the paper self-contained.\nMoreover, the paper lacks motivation of the design\u00a0choices. It seems to be a combination of a few recent techniques in machine learning or statistics\u00a0that are mechanically attached to each other without sufficient justification or intuition.\u00a0\nThe paper keeps claiming to solve AI however what it actually experimented on RL safety and/or one synthetic environment. They are not really \"AI benchmark problems\". I'd rather the paper focuses on its contribution: direct and concise.\u00a0\nMoreover, the experiments are not sufficient to support the paper or investigate how much each part is contributing\u00a0to the success.\u00a0"}