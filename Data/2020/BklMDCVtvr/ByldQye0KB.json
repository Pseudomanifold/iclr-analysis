{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis work focuses on the understanding of Deep NNs and of their generalization ability for compositional tasks and especially for partially composition tasks such as one encounters in natural language processing.\n\nThe core idea is to design and to learn a neural model (the ROLE network) able to analyse a target neural network (e.g. an encoder in an seq2seq architecture) by identifying the symbolic structures the target network manipulates (the symbols and their roles) in its representations and the compositional rules it has learned on these. The main motivations are the understanding of what is learnt by a such neural network and getting ideas on which architecture to choose for improving generalization on compositional tasks.\n \nThe experimental study shows that ROLE \u2019s results are relevant when dealing with a fully compositional model by design. This is a kind of sanity check, that ROLE may uncover the ground truth from the data, while no prior information is provided on the nature of the roles. \nThe experiments on the SCAN dataset concern a standard RNN model learned from data. I am not sure of the nature of RNN used here. How many layers, which activation function ? The experimental section also provides insights on what is learned by the ROLE model and on how its (compositional) representations match the ones learned by the RNN.     \n\nAs far as in understand, beyond the understanding of the learned representations in RNNs, the paper motivates the work with the expectation that the gained knowledge might be useful for designing better neural nets, with improved generalization ability. Yet i don\u2019t  see clearly what could be done on this line.  "}