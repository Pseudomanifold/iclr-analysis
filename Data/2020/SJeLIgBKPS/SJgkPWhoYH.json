{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The goal of the paper is to formally prove that gradient flow / gradient descent performed on homogeneous neural network models maximizes the margin of the learnt function; assuming gradient flow/descent manages to separate the data. This is proved in two steps:\n  1. Assuming that gradient descent manages to find a set of network parameters that separate the data, thereafter gradient flow/descent monotonically increases the normalized margin (rather an approximation of it).\n  2. The limit points of optimization are KKT points of the margin maximization optimization problem.\nWhile the main body of the paper presents a restricted set of results, the appendix generalizes this much further applying it to various kinds of loss functions (logistic/cross-entropy, exponential), to multi-class classification and to multi-homogeneous models. There seem to be many subtleties in the proofs and the paper seems to be quite thorough. (I must say that I'm not expert enough to assess the technical novelty of this paper over prior works.)\n\nRecommendation:\nI recommend \"acceptance\". The paper takes a significant step by unifying existing results on margin maximization and going beyond them. \n\nTechnical comments:\n- It is clear that in order to define margin meaningfully, some form of normalization is necessary. But a priori, $\\|\\theta\\|_2^L$ is not the *only* choice; $\\|\\theta\\|^L$ could also work for any norm $\\|\\cdot\\|$. But perhaps the choice of $\\|\\cdot\\|_2$ is special (as Thm 4.4 suggests). It will be nice to have some insights/comments on why this choice of $\\|\\cdot\\|_2$ based normalization is the right one.\n- The paper argues that having a larger margin helps in obtaining better robustness to adversarial perturbations (within $\\|\\cdot\\|$ balls for some choice of $\\|\\cdot\\|$). However note that the notion of \"margin\" is not just a function of the decision boundary, but instead depends on the specific function computed by the neural network --- this is unlike margin maximization in linear models, where \"margin\" in determined entirely by the decision boundary. As the paper argues, if we have an upper bound on the Lipschitz constant w.r.t. $\\|\\cdot\\|$ norm, then we get a lower bound on required adversarial perturbations for any training point. However, this does not mean that training longer is necessarily better because by doing so, we might end up with a larger Lipschitz constant (even after normalizing). So even if the \"margin\" is larger, the actual adversarial perturbations (in $\\|\\cdot\\|$ norm) allowed might get smaller. So I'm not sure how relevant this result is for adversarial robustness.\n"}