{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary\n\nThe paper proposes two graph smoothness metrics for measuring the usefulness of graph information. The feature smoothness indicates how much information can be gained by aggregating neighboring nodes while the label smoothness assesses the quality of this information. The authors show that Graph Neural Networks (GNNs) work best for tasks with high features smoothness and low label smoothness by utilizing information from surrounding nodes which also tends to have the same label. Based on these two metrics, the authors introduce a framework, called Context-Surrounding Graph Neural Network (CS-GNN), that utilizes important information from neighboring nodes of the same label while reduce the disturbance from neighboring nodes from different classes. The results demonstrate considerable improvement across 5 different tasks. \n\nStrength\n\nThe authors advocate for better understanding of the use of graph information in learning, which is both an important and interesting problem. Two graph smoothness metrics appears to be intuitive and reflect common situations in graph-based data (1) features from neighboring nodes contribute differently to target node representation (2) neighboring nodes information sometimes causing disturbance if node with different labels tend to be connected. The paper provides some theoretical analysis that supports this claim and thorough experiments that show the correlation between the two proposed metrics with the performance of GNNs. \n\nWeakness\n\nWhile the paper is reasonably readable, there is certainly room for improvements in the clarity of the paper. First, I would suggest the authors to avoid too much word repetition as well as long, obscure sentences. For example, the first sentence of section 2.2 can  be rewritten as \u201cGNNs usually contains an aggregation step to collect neighboring information and a combination step that merges this information with node features.\u201d The flow of the paper is also hard to follow and need some rearrangement. For instance, paragraph 3 of section 2.1 can be pushed until section 3.3. Another suggestion about the flow is to separate section 2.2 into two subsections for features smoothness and label smoothness (also the title of this section need to be refined). Finally, the results would be more clear if separated into different tables or subsections/paragraphs.\n\nQuestions\n\n* Is there a particular reason for using the KLD instead of mutual information?\n* In 2nd sentence of section 2.2, what does \u201cnode\u2019s own information\u201d mean? If it is the individual node\u2019s features then why it naturally the representation vector h_v (which is aggregated with neighboring nodes?)"}