{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to use additional structures within and between sentences for pre-training BERT. The basic idea is to shuffle either some n-grams within sentences or the sentences in texts, then train the model to predict the correct orders. Experiments in this work show that, with this additional training objective, the proposed pre-trained model, StructBERT, obtains good performance on the tasks including natural language understanding and question answering.\n\nOverall, I think the experiments and results in this work are not sufficient enough to support the claim:\n\n- It is necessary to show the performance of BERT only trained with the proposed word and sentence objectives. Otherwise, it is not clear how much benefit the model can get from them and the work is basically incremental. \n- Some justification is needed about why choosing trigrams and why 5% is a good number of sampling trigrams from texts\n\nBesides, there are some recent work on analyzing why BERT encodes any linguistic properties of texts, for example \n\n- Goldberg. Assessing BERT's syntactic abilities. 2019\n- Tenny et al. BERT Rediscovers the Classical NLP Pipeline. ACL 2019\n- Tenny et al. What do you learn from context? ICLR 2019\n\nAll of them show positive results on BERT can capture some syntactic information from text automatically. Which makes me wonder why the simple additional training objective proposed in this work can still lead to performance improvement. Is there an explanation? \n"}