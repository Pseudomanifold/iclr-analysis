{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces two new tasks for large scale language model pretraining: trigram word unscrambling and contextual sentence ordering. Using these tasks to pretrain on top of masked language modelling shows improvements when the resulting model is finetuned on downstream tasks. The proposed tasks are simple to implement, and particularly the sentence ordering task is an improvement over the original BERT next sentence task, which is widely regarded as too simple to drive learning good representations. For this reason, I recommend acceptance of this paper.\n\nSome minor quibbles:\n1) Structure in language usually means syntactic structure. How does unscrambling word trigrams help uncover syntactic structure? The references to Elman 1990 also don't serve to clarify anything, I suggest that they are removed.\n2) Some prior work on word ordering (e.g. [1] and older papers cited therein) is missing.\n3) The permutation objective seems very similar to the XLNet objective. Could the authors elaborate more on this in the paper?\n4) Did the authors try with other n-gram shuffling orders?\n5) The sentence ordering task has been used previously (e.g. [2]).\n6) Table 1 overhangs the right margin.\n\nReferences:\n[1] Discriminative Syntax-Based Word Ordering for Text Generation, Zhang and Clark 2015\n[2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning, Jernite et al. 2017"}