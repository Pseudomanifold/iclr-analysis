{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed to address the few-shot learning problem, especially for the cross-domain setting where a newly coming task originates from a different distribution (or in this work implemented by sampling from an unseen dataset). Basically, the authors constructed a model zoo based on source datasets at hand, and learned an \u201cargmax\u201d meta-selector which takes embedding of a task as input and outputs the model selection index. The idea is very intuitive, and the implementation is kind of an incremental combination of (Rebuffi et al., 2018) building models for multi-tasks and (Oreshkin et al., 2018) tailoring based on task embeddings.\n\nPros:\n-\tThe problem that this work aims to address, i.e., domain heterogeneity, is significant.\n-\tThe author investigated several variants of the proposed method, varying the architecture of the modulator and the inference scheme.\n-\tThe paper makes clear points and is quite easy to follow.\n\nCons:\n-\tThe concern at first priority is the practicability of the proposed method. The authors likely misunderstand why almost all existing SOTA algorithms modulate parameters/activations themselves \u2013 it is because of their advantages of easily being efficiently updated across tasks. However, the proposed method has to store an increasing number of models as a task comes, which takes huge storage cost. Also, when a new task arrives, will all models be trained from scratch to enforce the feature extractor to also be shared by this new task? It is an economically infeasible solution for meta-learning/few-shot learning, to my best knowledge.\n-\tActually while I was reading the paper from the beginning, I was expecting a meta-scheme that learns the weights of the models in the pool. In that case, the inference scheme of averaging in Eqn. (2) could be more intuitively correct, by paying more attention to those models which are similar. \n-\tThe third concern comes from the empirical results. \n    o\tBaselines: the two baselines compared, FEAT and ProtoMAML, are actually un-published. The results of a handful of SOTA algorithms recently published, including (Oreshkin et al., 2018) and (Yao et al., 2019), should definitely be incorporated. I do not see any specific part in your setup, and I cannot understand why (Oreshkin et al., 2018) does not converge. According to my experiences, it is quite easy to converge. Besides, (Yao et al., 2019) is specifically designed to tackle the cross-domain (for an unseen dataset) setting.\n    o\tHow can you validate the effectiveness of the task embedding? It is better to conduct ablation studies to consider those like autoencoder embeddings. \n    o\tCould you introduce the setting of fine-tuning in more details? I am confused why fine-tuning seems always superior than ProtoNet?\n    o\tCould you give more discussion on the results in Table 1? As the number of source datasets increases, an effective meta-model of course should contribute more to the target dataset, while it is not in this work. Does this signify that the proposed method is not that effective?\n"}