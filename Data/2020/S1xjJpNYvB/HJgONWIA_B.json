{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "###Summary###\nThis paper aims to tackle few-shot classification with many different domains. The idea is to build a pool of embedding models, which are based on the same base network. The models are diversed by their own modulators. The high-level intuition is to let the model pool capture good domain-invariant features by the shared parameters and domain-specific features by the selection network, which is desirable to represent the complex cross-domain task distribution, without a significant increase in the number of parameters. \n\nThe model includes the embedding networks f_E and a selection network f_S. The overall training process of the proposed method is:\n(1) Train the base embedding network f_E with the aggregated dataset from multiple domains.\n(2)Sample one domain to optimize the corresponding embedding model. \n(3) Build a selection network through cross-domain episodic training. The task representation is calculated by average the embedding vectors from the base network, which resembles the prototype. Then the model with the highest accuracy on the query set is selected to compute the labels for the target domain.\n\nFor the embedding model f_E, the paper provides two architectures, one with convolution 1x1 and the other with Channel-wise transform, denoted as DoS and DoS-CH, respectively. Instead of selecting the model which has the highest accuracy, the averaging model generates an output by averaging class prediction probabilities of all constituent models, denoted by DoA and DoA-CH, respectively.\n\n The baseline used in this paper includes Fine-tune, Simple-Avg, ProtoNet, FEAT, ProtoMAML. \n\nThe paper performs extensive experiments on a batch of datasets, including Aircraft, CIFAR100, DTD, GTSRB, ImageNet12, Omniglot, SVHN, UCF101, and Flowers. The 5-way 1 shot and 5 way 5 shot experimental results demonstrate that the proposed DoS and DoS-CH can outperform other baselines in the \"see domains\" setting. However, the results on \"unseen domains\" experiments are worse than the averaging baselines (DoA, DoA-Ch). \n\nThe paper also surveys the few-shot classification results on a varying number of source domains to show that DoA and DoA-ch are robust to deal with different settings.\n\n### Novelty ###\n\nThe paper composes of two sub-network, with one baseline network to extract the commonsense knowledge for different datasets and another selection network to select the best model from the model pools for each specific domain. \n\nThe idea of leveraging multiple modulators for domain-agnostic image recognition is interesting and heuristic, thus the proposed framework shows some novelty.\n\n###Clarity###\n\nOverall, the paper is readable and logically clear. The images are well-presented and well-explained by the captions and the text. \n\nWhile this paper misses many prior works both in the track of domain-agnostic learning and few-shot learning. I would recommend the authors to the following materials:\na). Domain Agnostic Learning with Disentangled Representations, ICML 2019. https://arxiv.org/pdf/1904.12347.pdf\nb). Generalizing from a Few Examples: A Survey on Few-Shot Learning, \nhttps://arxiv.org/pdf/1904.05046.pdf\n\nThere also exist some grammar mistakes and typoes in the paper. It will be better to revise and polish the paper. \n\n###Pros###\n\n1) The paper proposes a framework that includes two parts, i.e. the base network and the selection network. The idea is to make deep models more robust to different image domains, which is interesting and heuristic.\n2) The paper provides extensive experiments on a wide range of datasets. The experiments demonstrate the effectiveness of the proposed method.\n3) The paper is applicable to many practical scenarios since the data from the real-world application is complicated. \n\n###Cons###\n\n1) The critical component of the proposed method is the selection network. However, the experiments on the \"novel domains\" show that the proposed DoS and DoS-Ch works worse than just averaging the outputs of the models in the model pool. \n2) The paper should explain the details about the baseline experiments. Since the proposed models have much more parameters than the baselines, what's the effect of the auxiliary parameters? Is the comparison between the baselines and the proposed method fair?\n3) The presentation of the proposed paper should be polished. Many critical techniques used in this paper are not well-explained, such as the ProtoNet. There exist some grammar mistakes and typos that need a revision. \n\n\nBased on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.\n\n"}