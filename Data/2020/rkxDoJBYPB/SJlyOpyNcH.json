{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThis paper proposes an ML-based method to optimize TensorFlow Graph execution. Specifically, it combines graph neural networks (GNNs) and BRKGA (a genetic algorithm) to search over the joint space of TF node-device placement and scheduling. The core claims on the advantages of this method are that (1) it co-searches placement and scheduling space, (2) the trained model can generalize to different graphs and inference cost is very  small. The experimental results show that REGEL can outperform a few baseline methods on this problem.\n\nWriting\n- The paper is well-written and I enjoyed reading the paper.\n- Some more descriptions about the BRKGA algorithm could be added in.\n\n\nMethod and Results\n\nSome confusion if the authors could answer:\n- I am very confused by one of the claims that \u201c the first work on learning a policy for jointly optimizing placement and scheduling\u201d. I don\u2019t see much evidence in the result section about showing the co-searching the joint space yield advantages? I am fairly familiar with the line of work on only optimizing device placement, but it would be good to see some ablation studies showing search over the joint space is advantageous. \n\n- The model is trained with standard REINFORCE -- how many training time and resources are needed to train a REGEL model for a task? How\u2019re the training dynamics looking like (variance, convergence, etc?)? \n\n- In terms of the generalization ability of REGEL, the paper has clearly shown that REGEL is able to generalize to differently shaped graphs, with acceptable cost, but I am wondering for the same dataflow graph, how REGEL generalizes to different input data configurations (size, modality, etc.)? E.g. if the batch size of the input data is changed, the execution time of each kernel and their memory usage (in general, the system treatment) would change; Can a trained REGEL model on a data config A generalize to B? How would this affect the performance of REGEL?\n\n- It seems the method and assumptions about graphs or training data are pretty coupled with TensorFlow and graph-mode execution, how could the method be generalized to other ML frameworks (e.g. frameworks with eager execution)\n\n- Could the authors clarify why the two methods mentioned in \u201cLearning to directly predict a solution\u201d has quadratic complexity w.r.t. # of nodes and whereas REGEL is linear? \n\n- Confusion on Figure 4(b): Could some more critical statistics about the graphs in the training/test dataset be reported? e.g. what\u2019s the average depth of the training graphs? When there are 32 MP layers a node\u2019s feature will be passed across its 32-hop neighborhood, which seems surprising as it is common to observe GNN starts degenerating with increased depth (because all node features become similar during message passing)\n"}