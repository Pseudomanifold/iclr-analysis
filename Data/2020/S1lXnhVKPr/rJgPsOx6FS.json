{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors propose a variance reduced local SGD and prove its convergence rate. In the experiments, they show that the proposed method converges faster than local SGD.  \n\nThe following are my concerns:\n1) I am concerned about the convergence result, it shows that the convergence of the proposed method has nothing to do with the extent of non-iid. However, it is not correct intuitively. It is easy to imagine that non-iid data will converge slower than iid data. \n\n2) In Corollary 5.2, the convergence result is not related to k. It is false to me.\n\n3) It is not clear in algorithm 1 how the \\delta^{t''} is updated.\n\n4) The assumption in equation (11)  \"When all local model x^t, x\\tau and the average model \\hat x converge to the local minimum x\u2217\" is not correct when data is non-iid distributed. Suppose x^t and \\hat x is x^*,  and \\Delta^{t''} = 0.  Because data is non-iid, the solution of the local problem is not equal to the global problem, therefore, x^t will go away from x^*. \n\n5) In the experiment, the setting of k should affect the experiment. However, authors don't analyze this parameter.\n\n\n\n"}