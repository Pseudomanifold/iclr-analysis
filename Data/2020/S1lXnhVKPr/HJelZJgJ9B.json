{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the problem of data-parallel (synchronous) distributed SGD, to optimize the (finite) sum of N non-convex, possibly different (in the so-called non-identical case), loss functions. This paper focuses on improving the communication efficiency compared to several existing methods tackling this problem.\n\nTo that end, the authors contribute:\n\u00b7 A novel algorithm and its asymptotic communication complexity.\n\u00b7 The proof that the common metric of the sum over the training steps of the expected squared norm of the gradient at the average of the N parameters is bounded above.\n\u00b7 Experimental results (training loss function of epoch number) comparing this algorithm with 2 existing ones, solving 3 problems under reasonable settings.\n\n  The training time per epoch of VRL-SGD is claimed to be identical to the one of Local SGD, as the algorithm only have minor differences.\n\n- strengths of the paper: \n\n\u00b7 The main paper is very easy to follow.\n\u00b7 Good effort to give intuitions on why VLR-SGD can improve the convergence rate of existing algorithms.\n  Such an effort is to be highlighted.\n\u00b7 No obvious mistake in the main.   I have not thoroughly checked the full proof though.\n\n-  weaknesses of the paper: \n\n\u00b7 The algorithm, while having differences, is quite reminiscent of Elastic Averaging SGD (EASGD) [1].\n  Indeed in both algorithms the model update at the workers consists in both descending the local gradient plus descending toward some \"moving-average\"obtained through averaging all the local models.\n  In EASGD, this \"moving-average\" is common to every worker and the master, which updates it every k steps.\n  In this paper, each worker has its own \"moving-average\", which update computations are different than in EASGD as the use the instant average of the workers' models instead of the previous \"moving-average\".\n\n[1]Sixin Zhang, Anna Choromanska, Yann LeCun, Deep learning with Elastic Averaging SGD,  NeurIPS, 2015\n\n- Questions I would like the authors to respond to during the rebuttal:\n\n\u00b7 Could Elastic Averaging SGD (in particular their fastest variant EAMSGD) be applied as-is to solve the non-identical, non-convex optimization problem at hand?\n  Despite the authors of EASGD not studying their algorithm in the non-identical case, following what is done in the intuition part of VRL-SGD (in particular Equation (8)), it seems that the update rule of the \"moving-average\" in EASGD is then equivalent to having a momentumSGD with dampening (instead of the \"generalized SGD form\" obtained with the approach of VRL-SGD).  Hence my question.\n\nI suggest acceptance. However I'm willing to change my opinion after reading other more qualified reviewers in the sub-area of variance-reduction techniques.\n\nnote: If EASGD was to be sound in the non-identical case as well, my decision would not change much."}