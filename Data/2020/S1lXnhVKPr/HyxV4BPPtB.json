{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes the variance reduction to the local SGD algorithm and shows the proposed VRL-SGD can achieve better convergence than local SGD when the data are not identical over workers. The idea is interesting and the paper is easy to follow. However, the study does not go into depth. I do not support the acceptance for current situation. \n\n1. The paper does not show the convergence rate when the function is convex/strongly convex, which make it hard to compare with previous works e.g. EXTRA [Shi et al. 2015]. \n2. There are many alternatives to achieve communication efficiency. The paper does not argue why to choose variance reduced Local SGD. A natural idea to reduce the variance is to distribute the inner loop of SVRG to different workers as proposed in [Konecny et al. 2016]. Moreover, the analysis is given in [Lee et al. 2015] and [Cen et al. 2019]. Especially, [Cen et al. 2019] suggests using a regularization term to handle the data load is not balanced over the workers. \n3. The paper states that S-SGD cannot achieve linear iteration speedup due to communication bottleneck. Can we avoid the communication bottleneck by increasing the batch size? There are vast literatures on distributed training of deep neural network by using large batch size.  \n4. The experiment comparison is not complete given there are many related work in this area.\n\nQuestion: How to determine the number of local SGD steps for each communication round? In SVRG, the number of iterations in the inner loop is related to the condition number (strongly convex case). Does the number of local SGD steps have a similar correspondence\n\n[Jason Lee, et al.]  2015. Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity.\n[Shicong Cen, et al.] 2019 Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data\n"}