{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nGeneral Comments\nThe authors proposed a new type of gated recurrent unit for RNNs, and show comparable performance on a number of RNN applications, some achieved with less number of parameters.  \n\nDetailed Comments\n\nSection 3 (Interpretation)\nThis section basically draws similarity between the RNN update equations with discrete temporal difference equations.  It\u2019s unclear why showing that the equations look similar is motivating.  It would be better if the authors can borrow analyses done on differential equations and apply them here.\n\nSection 5 (Experiments) \nTo me, the main contribution of this work lies (potentially) in the newly proposed EINS unit.  \nTo demonstrate that it\u2019s practically appealing, below are my suggestions.  \n# of parameters is only practically meaningful for saving storage. More interesting analyses should include memory cost during training/test (i.e., the total number of activations, and the required gradients for learning), and computational time.   \nEase of tuning (i.e., is this method easier to tune than LSTM/GRU).  One benefit of a new method can be that it\u2019s more robust to tuning-/hyper-parameters.  If the authors can show that EINS is easier to tune, this can be a good result.\nExperiments are diverse, not I rather see a large scale experiment because the strength of the proposed method was motivated as a \u201csimpler\u201d method.  The provided experiments do not strengthen the claim that EINS is \u201csimpler\u201d for the practitioners. Understandably, this will require a lot of effort, but many of the interesting applications of RNNs as of today are large-scale speech recognition, language modelling, and video recognition.  It\u2019s unclear whether EINS has any benefits in that regime.\n\nOverall, the authors demonstrated a variant of RNN updates, but neither the theoretical nor  empirical evidence was strong enough to 1. Convince practitioners to switch from LSTM/GRU to EINS, 2. Provide insight/understanding to the workings of RNNs.  \nIt\u2019s interesting to look for new building blocks for neural nets.  However, perhaps more interesting than manually designing it, maybe you can use the reasonings in this paper to design constraints for neural architecture search model.  \n\nBest,\n"}