{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents an extension to Neural ODEs by introducing stochastic regularization techniques that inject noise to the Neural ODE dynamics. The method consists of adding a Gaussian diffusion term to the ODE dynamics to obtain a stochastic differential equation (SDE). Using a black box SDE solver, the paper proposes to train the parameters of the SDE dynamics (drift and diffusion) via back-propagation using autograd, with an alternative method described in the appendix.\n\nThe ideas in this paper are definitely interesting, but I'm more inclined to reject this paper for the following reasons: the theoretical contributions are hard to follow and, while the empirical results are encouraging, the technical contribution is small when compared to the existing literature (Tzenand Raginsky, 2019), (Jia and Benson, 2019), (Rackauckas et. al., 2019). The paper does mention the differences with the other works, but the title and the text make it seem as if the main contribution was proposing the idea of Neural SDEs. The paper would read better if it was clear from the title and introduction what the main contribution is, which appears to be a  study about the robustness of Neural SDEs.\n\nThe paper requires more work to address some issues, including:\n\n* The paper mentions that injecting noise to a Neural ODE is non-trivial, but that depends on what kinds of noise, and how you interpret it. In this paper, the noise is process noise that affects the intermediate activations (but not the parameters of the dynamics). You could also interpret the noise as providing different samples of the unknown dynamics f, which is related to Bayesian Neural Networks. In fact, this gives another baseline: Training a Neural ODE where the parameters of the drift term are perturbed by noise. Similar how dropout is applied on recurrent neural nets (Gal and Ghahramani, 2016), the noise perturbation should be sampled at t=0, and kept fixed until t=H. This should be a trivial change to the Neural ODE code that provides a point of comparison\n\n* The way Bernoulli dropout is modeled in the proposed Neural SDE framework is rather crude. I Can see how the first two moments of the approximation in equation (7) match the first two moments of the Bernoulli distribution, but the distributions have supports that are way too different to claim that eq(7) is a good approximation: The mass of the Gaussian is concentrated around 0, while the mass of the scaled and centered Bernoulli is away from 0. A more appropriate way to model dropout would be using jump diffusion.\n\n* The proof of robustness can be written more clearly: what is it that you set out to prove? Are the assumptions just saying that f + G is bounded and Lipschitz continuous? The way they are written is a bit confusing. How is the main theoretical result, Corollary 3.0.1, connected to the subsequent experiments? What insights do you get from that result?\n\n* The paper claims that stochastic regularization techniques like Dropout do not apply noise at testing time, but this statement ignores the literature on Bayesian Neural Networks (Neal, 1995) (Blundell et al, 2015) (Gal and Ghahramani, 2016), which use multiple parameter samples to produce an ensemble of predictions .\n\n* The paper would be a lot more interesting if there was a comparison (computational cost and performance) between back-propagating through the SDE solver, and using the method proposed in the appendix.\n\n* The writing could benefit from proof-reading by a proficient English speaker."}