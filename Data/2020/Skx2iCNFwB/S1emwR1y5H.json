{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces Neural Stochastic Differential Equations models which is a follow-up of last year paper on Neural Ordinary Differential Equations.  The stochasticity is obtained by adding an injection noise term. \n\nPros: \nAs there is still no published paper that introduces Neural SDE, this paper would be well placed in the literature. Apart from introducing the Neural SDE architecture, the authors also study both empirically and analytically the robustness of suggested models in comparison with Neural ODE models. Overall the paper is clear and well written. \n\nCons: \nThe transfer from 'Ordinary' to 'Stochastic' is achieved by only adding a Dropout (injection noise) term. So the idea seems to be not 'that' big. \nThere is another papers, not cited by the authors, that suggests stochastic versions of Neural ODEs\u00a0based on ResNets with a specific initialization distribution on the NN parameters (Peluchetti and Favaro, 2019). Their model can be considered as a continuous version of ResNets with applied stochastic gradient descent. The current interpretation of Neural SDE also can be considered, just I find the suggestion of Peluchetti and Favaro to be more natural. \n\nStefano Peluchetti and Stefano Favaro. Neural Stochastic Differential Equations (2019) \n\nMajor suggestions:\nI don't have major suggestions. \n\n\nMinor suggestions: \n\n* In the first and last sentences of subsection \u201cGaussian noise injection\u201d, Section 3.1, there must be references to equation (3) instead of (25), which is in the appendix. And also further in the main text of the paper there are references to (25). \n\n* In equation (7) the identity matrix must be written in bold.  \n\n* Section 3.2. we can also calculates -> calculate\n\n* There are also some missed articles throughout the paper. \n\n* Appendix: \n- I think it would look better to put Equation (12) in Lemma A.3. \n- After Equation (16), there is another letter epsilon.\n- Appendix D contains only Figure 7 which is the same as Figure 3 (left).\n\n* References: \n- Weinen E. -> full name \n- Some papers have already been published, e.g. Xavier Gastaldi\u2019s paper was at ICLR 2017.\n- Conference names are not homogeneous, such as \u2018Advances in Neural \u2026\u2019 and only \u2018Neural \u2026\u2019, written with capital and not capital letters. \n"}