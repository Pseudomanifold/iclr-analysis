{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper attempts to learn discrete speech units in a hierarchical (phone and word) fashion by incorporating multiple vector quantization layers into the audio encoder branch of a model that visually grounds speech segments with accompanying images.\n\nThe model has been tested and compared against two algorithms and implementations that set the SOTA on the Zero Speech 2019 challenge (further improving one of them in the process, it seems), and outperforms these significantly using the ABX metric, so the proposed method seems to perform well (the model is using additional supervision, though). In addition, this is an interesting and timely research problem with implications far beyond the core machine learning setup. The hierarchical setup, and the finding that successful learning here depends on the curriculum, is intriguing indeed. The paper is a pleasure to read and provides a rich set of results and analyses. \n\nA few remarks:\n- It is probably worth explaining how the ABX test is performed, i.e. that features are extracted from some layer of a model, and then a time alignment is performed to get the score - this is in the text somehow, but i had to read it multiple times.\n- Did you try other architectures like 5 layers (rather than 4) in Figure 2\n- Figure 2 is a bit hard to interpret. Maybe plot log of ABX error rate or something, to pull apart the different layers?\n- Could you explain the difference between cold-start and warm-start? One is adding the discretization to a pre-trained model, the other is training from the start?\n- When you measure ABX at layer 2 and 3, in a model trained with quantization, do you measure ABX on the features before or after quantization? does it make a difference?\n- Table 7: some of the top word hypothesis pairs make sense acoustically (building-buildings, red-bed, ...), some could be neighboring words (large-car, ...), but some are just weird (people-computer) - any intuition as to what is going on?"}