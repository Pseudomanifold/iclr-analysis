{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. In the previous works, there was shown that the two local minima of a neural network can be connected by a curve with the low validation/train loss along the curve. Despite the loss on the curve being close to the loss at the end points, there are segments of the curve on which the loss in higher than loss at the local minima. To overcome this problem, the authors proposed two-step procedure: 1. Align weights between two neural networks 2. Launch the path finding algorithm between aligned weights. In other words, the authors proposed to connect not original local minima but local minima that parametrize the same functions as the original ones, but have a different order of neurons. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm.\n\nNovelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. This is a new and significant result, since before the ensembling of points along the curve had the same quality as the ensemble of three independent networks or marginally better.  \nThe weak side of the paper is the PAM algorithm that occupies a significant part of the paper and does not deliver a significantly better result than the simple application of the neural alignment procedure before launching the path finding algorithm.\n\nClarity. Overall, the related work section contains all relevant references to the previous works to the best of my knowledge. The paper is well written, excluding the section Neuron Alignment that lacks notation description.  \n\nThe paper contains several typos and inaccuracies:\n1. \u201cHowever, We find its empirical performance is similar to standard curve finding, and we advocate the latter for practical use due to computational efficiency.\u201d The word \u201cWe\u201d should start with the lowercase letter.\n2. In the sentence \u201cThe first question to address is how to effectively deal with the constraints in equation 6\u201d the index i should be replaced with the index l.\n3. Notation \u03a0|Kl| introduced after equation 5.\n4. In the section describing neuron alignment  algorithm Lie et al. [1] used a different notation. So I would recommend to further extend this section and add all necessary notations. Also, I would recommend to add a direct link to the paper where the problem is described in matrix form.\n5. In the Algorithm 1 \u201cInitialize P \u03b82 := [W\u02c6 2 1 ,W\u02c6 2 2 , ...,W\u02c6 2 L ] as [W2 1 ,W2 2 , ...,W2 L ] for k \u2208 {1, 2, ..., L \u2212 1};\u201d  k is not used anywhere in notation.\n6. In Figure 3, \u201cmodel 2 aligned \u201d sing is out of the plot box for ResNet-32 and VGG-16 architectures.\n7. The appendix contains the sketch of the proof that is quite difficult to follow. I would recommend giving all necessary definitions as it is done in the [2] and extend the proof. \n\nOverall, it is quite an interesting paper but it contains some drawbacks.\n[1] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do different neural networks learn the same representations?  In ICLR, 2016\n\n[2]  H\u0301edy Attouch, J\u0301er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran.  Proximal alternating minimization and projection methods for nonconvex problems:  An approach based on the kurdyka-\u0142ojasiewicz inequality.Mathematics of Operations Research, 35(2):438\u2013457, 2010\n"}