{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new algorithm for solving Noisy Robust MDPs (NR-MDPs) based on computing an approximate gradient using  stochastic gradient langevin dynamics (SGLD). THE NR-MDPs can be thought of as an agent learning in an adversarial environment.\nThe paper is well written and the references pointed to give sufficient background to understand the problem undertaken.\nMost of the theory comes from the \"Finding mixed nash equilibria of generative adversarial networks.\" (ICML, 2019), and the papers main contribution lies in applying the theory to reinforcement learning.\nIt is not altogether unexpected though that Langevin Dynamics give better results than the more standard gradient-based approach considered for saddle-point problems, that's what they are supposed to do. But, this seems to be the first time SGLD has been applied to such a problem.\nThe authors compare on the MuJoCo benchmark with common but not identical instances to \"Action robust reinforcement learning and applications in continuous control\", which also provides the baseline algorithm used for comparison. Mentioned in this paper is the difficulty of solving the \"inverted pendulum\" instance by the algorithm proposed therein. It is infact mentioned as a failure case. Maybe, the authors can show results on the same. \nAdditionally, maybe it would make sense to have an ablation study of the hyper-parameters from Table 1. \n"}