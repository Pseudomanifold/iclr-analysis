{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors explore how stationarity tests can be leveraged to automatically tune the learning rate during training. Their algorithm also add a robust line search algorithm, to reduce the need to tune the initial learning rate. The paper is clear and the literature review is honest and thorough. However, it is unclear to me if the contribution of the authors is enough, as the method used and its presentation are very close to Lang and al. In particular:\n\n- First bullet point on page 2: Because of its conceptual and analytical simplicity, it greatly simplifies implementation and deployment in software packages. It is unclear why the approach proposed by Lang is more complicated to use\n- It is a recurrent theme in the paper that the proposed method is a simple interval test compared to a more complicated equivalence test in Lang et al. It is unclear to me what the authors mean by that, as pages 5 and 6 of Lang clearly details a confidence interval test too.\n- If SASA+ is indeed just a new presentation of the algorithm detailed by Lang, the line search contribution does not justify a paper in my opinion\n- In page 5, \"Another major difference is that they set non-stationarity as the null hypothesis and stationarity as the alternative hypothesis (opposite to ours).\" I am not sure how the authors arrived to the conclusion that non-stationarity was the null hypothesis in Lang and would appreciate some clarifications on this point. The test used in SASA is a simple t test with a variance corrected to account for the auto-correlation of the gradients.\n\nAbout the empirical work:\n\n- The experiments seems plausible. Hyper parameters were search for fairly for the competing methods. Adam could have benefited from a finer learning rate schedule, as it is only decreased once compared to several time for the SGD. I would indeed expect a performance gap between Adam and SGD, but I think most of it in this case comes from the one step schedule.\n- Line search is performed but no metrics were shown to discuss the computational overhead of evaluating the model and its gradients for different parameters during the search.\n\nSALSA appears to be an already existing algorithm on which a line search was plugged in. The line search part, which appears to be the only contribution, is not discussed enough in my opinion (in terms of computation cost for instance)\n\nTo conclude, I think the presented work is too close to the existing literature and that the progress made is very incremental."}