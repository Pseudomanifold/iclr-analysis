{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method of detecting label noise through calculating the Area Under the Loss statistic, which is based on the learning trajectory.  \n\nI believe this paper is NOT the first one to point out the different training behavior of clean and noisy samples in the label noise problem. While the author mentioned that Shen & Sanghavi used the training losses for selecting data, Shen & Sanghavi also observed the training loss for good and bad samples are different across time. Therefore, it would be good to give correct credit to previous work, and not over-emphasizing the contribution. \n\nPlease proofread the paper, and correct the typos. For example, on page 3, there are 2 typos in the last three lines in the paragraph \u201crate of loss reduction indicates memorization need\u201d. \n\nCan you explain why you normalize the loss of each batch by c ? If z_i s are all small within a batch, this loss function does not reweight the bad samples in the correct way.\n\nCan you explain why you use Pearson\u2019s correlation instead of Spearman\u2019s correlation to evaluate the consistency of the ranking? This sounds surprising to me. \n\nAlso, the cited paper \u201cUnderstanding generalization of deep neural networks trained with noisy labels. \u201c by Hu et al. is not in NeurIPS 2019. Please correct this citation error if possible. "}