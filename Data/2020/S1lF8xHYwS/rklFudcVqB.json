{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper describes an approach to domain adaptation that uses\nself-supervised losses to encourage source/target domain alignment for\nunsupervised domain adaptation. The authors propose to use four\nself-supervised tasks (variants of tasks used in the self-supervised\nrepresentation learning for object recognition literature) that are\nused with a combined loss including unlabeled source and target\ntraining samples. The authors also propose an alignment heuristic for\nguiding early stopping. Experimental results on a standard battery of\ndomain adaptation problems are given, plus some intriguing baseline\nresults for semantic segmentation.\n\nThe paper is written very well and the technical development and\nmotivations for each decision are well discussed and argued.\n\n1. The experimental evaluation is a bit limited as the object\n   recognition datasets are a bit limited. Results on Office or\n   Office-Home would be nice.\n\n2. Using location classification for semantic segmentation seems\n   intuitively to be encouraging the network to learn coarse spatial\n   priors (which should be invariant across the two domains). Have you\n   looked at how alignment is actually happening? More qualitative\n   analysis in this direction would be useful to appreciate the\n   proposed approach.\n\n3. Related to the previous point, it would be interesting to see how\n   semgmentations in the unsupervised domain gradually change and\n   improve with increasing alignment.\n\nIn summary: the ideas are simple, intuitive, and well-explained -- I\nthink the results reported would be easy to reproduce with minimal\nhead scratching. The experiments are interesting and not overstated.\n"}