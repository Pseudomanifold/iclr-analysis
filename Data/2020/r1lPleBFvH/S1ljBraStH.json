{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper investigates some limitations of the conditional generative models (or generative classifiers). First, the authors present a counter-example that a good generative classifier fails to detect adversarial attacks. Second, the authors claim that the marginal and conditional terms of the likelihood objective are the source of the problem. Finally, the authors demonstrate some experiments on adversarial attacks, out-of-distribution (OOD) samples, and noisy labels.\n\nPros:\n- While generative classifiers are believed to be more robust than the discriminative counterparts [1], the authors present a counter-example that it may not be true.\n- The authors investigate the marginal and conditional terms of the likelihood objective and demonstrate empirical results that the model fails to capture the outliers.\n\nCons:\n\n1. The imbalance issue of the likelihood objective is not surprising.\n\nAs the data x is far complex than the class y, it is expectable that the penalty from modeling p(x) is larger than the penalty from classifying p(y|x). As mentioned in Table 1 and Appendix A.2, balancing two terms indeed improves the classification performance. However, to meet the high standard of ICLR, the authors should propose an alternative or modification of the likelihood which resolves the existing limitations. For example, [2] decomposes the semantic and background parts to improve the OOD detection using likelihood models.\n\n2. The experiments are not extensively studied.\n\nThe authors conduct experiments on two datasets: MNIST and CIFAR-10. The authors may present more results on other datasets (e.g., SVHN or CIFAR-100) and convince if their findings are consistent. Also, some observations seem to be an inheritance of the datasets, e.g., Figure 4 is natural since MNIST has disjoint support and CIFAR-10 has a continuous one.\n\nMinor comments:\n- On page 8, ',' should be moved after (Azulay & Weiss, 2018).\n- On page 8, (Schott et al.) should be changed to '\\citet' format.\n- PixelCNN++ is doubly cited.\n\n\n[1] Li et al. Are Generative Classifiers More Robust to Adversarial Attacks? ICML 2019.\n[2] Ren et al. Likelihood Ratios for Out-of-Distribution Detection. NeurIPS 2019.\n"}