{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new intrinsic reward method for model-free reinforcement learning agents in environments with sparse reward. The method, Impact-Driven Exploration, learns a state representation of the environment separate from the agent to be trained, based on a combied forward and inverse dynamics loss. The agent is then separately trained with a reward encouraging sequences of actions that maximally change the learned state.\n\nLike other latent state transition models (Pathak et al. 2017), RIDE learns a state representation based on a combined forward and inverse dynamics loss. However, Pathak et al. rewards the agent for taking actions that lead to large difference between the actual next state and the predicted next state. RIDE instead rewards the agent for taking actions that lead to a large difference between the actual next state and the current state. However, because rewarding one-step state differences may cause an agent to loop between two maximally-different states, the RIDE loss term is augmented with a state visitation count term, which decreases intrinsic reward for a state based on the number of times that state has been visited in the current episode.\n\nThe experiments compare RIDE to a selection of other intrinsic reward methods in the MiniGrid, Mario, and VizDoom environments. RIDE provides improved performance on a number of tasks, and solves challenging versions of the MiniGrid tasks that are not solved by other algorithms.\n\nDecision: Weak Accept.\n\nThe main weakness of the paper seems to be a limitation in novelty.\nPrevious papers such as (Pathak et al. 2017) have trained RL policies using an implicit reward based on learned latent states. Previous papers such as (Marino et al. 2019) have used difference between subsequent states as an implicit reward for training an RL policy. It is not a large leap to combine these two ideas by training with difference between subsequent learned states. However, this paper seems to be the first to do so.\n\nStrengths:\nThe experiments section is very thorough, and the visualizations of state counts and intrinsic reward returns are insightful.\nThe results appear to be state of the art for RL agents on the larger MiniGridWorld tasks.\nThe paper is clearly-written and easy to follow.\nThe Mario environment result discussed in section 6.2 is interesting in its own right, and provides some insight into previous work.\n\nDespite the limited novelty of the IDE reward term, the experiments and analysis provide insight into the behavior of trained agents and the results seem to improve on existing methods.\nOverall, the paper seems like a worthwhile contribution.\n\nNotes:\nIn section 2 paragraph 4, \"sintrinsic\" should be \"intrinsic\".\nIn section 3, at \"minimizes its discounted expected return,\" seems like it should be \"maximizes\".\nThe explanation of IMPALA (Espeholt et al., 2018) should occur before the references to IMPALA on page 5.\nLabels for the axes in figures 4 and 6 would be helpful for readability.\n\nThe motivation for augmenting the RIDE reward with an episodic count term is that the IDE loss alone would cause an agent to loop between two maximally different states.\nIt would be interesting to know whether this suspected behavior actually occurs in practice, and how much the episodic count term changes this behavior.\nIt is surprising that in the ablation in section A.5, removing the state count term does not lead to the expected behavior of looping between two states, but instead the agent converges to the same behavior as without the state count term.\n\nAlso, in Figure 9, was the OnlyEpisodicCounts ablation model subjected to the same grid search described in A.2, or was it trained with the same intrinsic reward coefficient as the other models?\nBased on the values in Table 4, it seems like replacing the L2 term with 1 without changing the reward coefficient would multiply the intrinsic reward by a large value.\n"}