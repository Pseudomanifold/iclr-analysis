{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n## Paper Summary\n\nThis paper combines a mutual information maximisation objective a la CPC with objectives for dynamics and reward prediction to learn a representation for downstream planning / skill learning that is more robust to visual distractors than comparable representations learned with reconstruction-based objectives a la VAE. In particular, the MI objective maximises the mutual information between the representation of the current state and future observations. The authors show improved robustness of their representation to visual distractors over a baseline with pixel-reconstruction-based representation learning (PlaNet). As a result they are able to achieve better performance when model-predictive control is used on top fo the learned representation to perform control on simulated DM Control Suite tasks with added simple visual distractors.\n\n\n## Strengths\n\n- the paper addresses a relevant problem with an intuitive approach\n\n- the paper is well written and easy to read\n\n- the analytic toy experiments at the end of Sec 4.2 and in 6.2 help understand the properties of the learned representation\n\n- the proposed method, when applied to the model-based algorithm PlaNet, shows improved robustness in settings with artificially introduced visual distractors in simulated DM Control Suite tasks\n\n\n## Weaknesses\n\n(1) unclear whether artificial distractors are indicative of behavior with real distractors: the fact that distractors do not follow coherent motions but instead randomly change position between two consecutive frames makes it hard to estimate how this would perform on more natural distractors. As there is no MI between the distractor's position in one image and any other image, a CPC-style representation learning objective will naturally be encouraged to ignore them. However, if they move with more natural, coherent motion this might not be the case. I would suggest to add an experiment with more natural distractor motion (see below).\n\n(2) it is unclear how much of the invariance to distractors is coming from the MI objective and how much simply from the fact that the representation is learned with jointly predicting the reward. In order to justify that the MI objective is helpful for learning the representation I would suggest to run an ablation experiment that trains using *only* the reward prediction objective and compare performance. (see also suggested experiment below)\n\n(3) parts of the model formulation require clarification: when reading Sec 4.2 that describes the model some parts were unclear to me (see concrete questions below). \n\n(4) the amount of detail provided in the paper is insufficient for reproducing the results: the paper lacks detailed information about the used architectures, hyperparameters and versions of the baselines employed (e.g. PlaNet with stochastic/deterministic prediction?), code is not provided.\n\n(5) lacks reference to recent work on CPC-style objectives for RL (see suggested references below)\n\n\n## Questions\n\n(A) why is the latent state variable s_t observed in the model depicted in Fig 3 (i.e. part of the input data)? Shouldn't this variable be latent? How can the KL constraint on it be computed if it is not observed in the input data?\n(B) should the first term on the right hand side of equation (2) have \\hat{s}_t instead of \\hat{s}_{t+h}? Otherwise, how is the I_{NCE} computation conditioned on the current state s_t?\n(C) what is the scale of the x-axis in Fig 5, i.e. does it show the number of environment rollouts / steps or the number of model re-training iterations? if the latter is the case, how does that translate to the number of environment interactions?\n\n\n## Suggestions to improve the paper\n\n(for 1) add an experiment where the distractor has more natural dynamics so that there is MI between the distractor positions in consecutive frames (e.g. ball bouncing in the image frame in the background instead of randomly jumping to new positions)\n(for 2) add an experiment with a reward-prediction only baseline, i.e. only action-conditioned reward prediction so that task-irrelevant parts are ignored by default (i.e. also no reconstruction objective, but also no MI objective) -> show how PlaNet performance compares to the so far reported numbers when using this representation for planning\n(for 4) add details about the architecture, hyperparameters and training schedule, for both the method and all comparisons to the appendix\n(for 5) add references to related works that use CPC-style MI objectives for representation learning in the context of RL/skill learning: \n\t- [1] Nachum et al., ICLR 2019 -- applies CPC-style objective to hierarchical RL setting\n\t- [2] Anand et al., NeurIPS 2019 -- investigates MI objectives for representation learning on a wide range of Atari games (don't apply to RL)\n\t- [3] Gregor et al., NeurIPS 2019 -- while the main proposed model is generative they compare to a contrastive version that uses CPC to learn predictive representations (don't use it for RL)\n\t- [4] Guo et al., Arxiv 2018 -- similar investigation to [3] of CPC-style objective for representation learning in RL environments (don't use it for RL)\n\t- it should also be mentioned that the original CPC paper already showed that adding CPC-style auxiliary loss to RL improves performance (even though they did not compare to other model-based methods)\n- add qualitative rollouts for predictions from the PlaNet predictive network both with and without distractor to the appendix\n\n\n## Minor Edit Suggestions\n- \"Learning latent dynamics from pixels\", Hafner et al. is cited twice in the reference section\n- it might help to add the reward prediction module to Fig 3 or mention in the caption that it is omitted, it is only described later in the text and was confusing for me on first sight\n\n\n[Novelty]: okay\n[technical novelty]: minor\n[Experimental Design]: okay\n[potential impact]: high\n\n\n#######################\n[overall recommendation]: weakReject - I am inclined to accept this paper but am not fully convinced that the random distractors provide a good intuition about how the proposed method would behave with more natural distractors. If the authors are able to report positive results on the two requested experiments I am willing to raise my score.\n[Confidence]: High\n\n\n[1] Near-Optimal Representation Learning for Hierarchical Reinforcement Learning, Nachum et al., ICLR 2019\n[2] Unsupervised State Representation Learning in Atari, Anand et al., NeurIPS 2019\n[3] Shaping Belief States with Generative Environment Models for RL, Gregor et al., NeurIPS 2019\n[4] Neural Predictive Belief Representations, Guo et al., Arxiv 2018\n"}