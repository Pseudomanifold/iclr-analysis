{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes using Consistency Regularization and a new bag generation technique to better learn classification decision boundaries in a Label Proportion setting.  The consistency regularization works to make sure that examples in the local neighbourhood have similar outputs. The authors further use K-means clustering to create a new bagging scenario they use to mimic real-world LLP settings. \n\nOverall, this paper sheds light on how we can use the underlying structure to better be able to make predictions on labels even when we don't have original labels but proportional bags. This is a very interesting problem area and will be interesting to the field.\n\n# For the experiments, a few notes and comments?\n1. It's a bit harder to appreciate some of the performance gains here without understanding the error rates around the accuracy. This would be especially good to know for the larger bag sizes where one expects more uncertainty.\n2. For each of the datasets, it would have been also enlightening to provide what the normal labelled performance would be as it would give an indication of the limit when the bag sizes get smaller.\n3. I am a bit confused by the k-means scenario. With bag sizes similar to the uniform bagging scenario, would the performance not be more as we are already capturing latent structure with the k-means algorithm? \n4. Following up on 3, Does this not have implications for actually preserving some privacy?\n\n\n# Other Comments\n5: It would be interesting to see an approach like mixup can be combined with the consistency concept in this case. So what do you expect when you now combine examples and train the underlying mixed bag objective."}