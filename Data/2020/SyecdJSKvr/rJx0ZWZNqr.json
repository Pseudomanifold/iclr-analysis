{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "[Summary]\nThe paper presents a solution to the problem of learning from label proportion (LLP) by incorporating regularization by data generation motivated by semi-supervised learning.   \nIt is argued that, by consistency assumption, the classification function defined on the data manifold should be a locally consistent mapping (in the sense of bag label proportion) such that the discrepancy between two bags of data points within a small neighborhood should be constraint. This is further combined with the conventional LLP loss (cross entropy between bag label proportion and classification results) to produce the final loss function for learning. For bag data generation, image attributes are leveraged to group similar data points in the feature space. Evaluation is performed on three benchmarks against vanilla solution and ROT loss. \n\n[Comments]\nI\u2019m not sure if I fully follow the contribution and several technical details.\n\n It looks to me that the major contribution claimed is the novel loss function that combines the proportion loss and the consistency loss, but both seem to be from off-the-shelf solutions from literature with slight variation. E.g., J_prop is the standard cross entropy loss (Ardehaly & Culotta (2017) and Dulac-Arnold et al. (2019)), and J_cons is from the vanilla consistency definition. I had a hard time getting the novelty here. \n\nThe notion and definition are somehow confusing to me too. What is K in the second line of page 3? Shouldn\u2019t J_prop in page 4 defined as sum of all per-bag losses? In J_cons of pge 4, shouldn\u2019t x under the summation be x_\\hat as x_\\hat is sampled? The equations should be properly numbered for easy reference.\n\nThe use of image attributes in 4.4 for K-means bag generation seems a strong requirement? What kinds of image attributes are used and how are they generated? Looks like this strategy only applies to image classification?\n\nThe results reported could be more clear. The gap between the proposed method and vanilla or ROT does not seem quite big in many cases (less than 1% in the best cases in table 1). I\u2019m not sure if these results are convincing or not as statistical significance is unclear.   \n\nWith all of the above uncertainty, I do not have confidence to have the paper accepted in the current format based on my preliminary assessment.\n"}