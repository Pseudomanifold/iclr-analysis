{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "- This works presents yet another incremental modification of well-known architectures (i.e., inception network). Particularly, authors propose to add an attention mechanism in the inception module to pay more attention to a given set of features. To evaluate the proposed network, authors resort to the task of underwater image classification. \n- The technical contribution of this work is rather limited, being the main contribution the application of deep classification models to underwater image classification. \n- Furthermore, authors mention that this work is a first attempt to simulate the visual correlation between understanding images and background areas through I-A modules. Nevertheless, this was never shown, beyond some classification activation maps in Figure 6. If I understood correctly, the classification task basically reduces to predict whether an image is taken underwater or not. Focusing on the background, however, may introduce errors, since I believe that pictures containing mainly sky may trigger the same activations. It would be more interesting to see also the classification activation maps for the negative class (non-underwater images).\n- Additionally, looking at Fig 6, GoogleNet and ResNet seem to provide more meaningful regions than the proposed network in some cases. Again, showing results on non-underwater images would help to better understand how the proposed method works.\n- Authors mislead some messages: \u2018classification algorithms designed for natural images cannot be directly applied to the underwater images due to the complex distortions existed in underwater images\u2019. Later in the manuscript, they show that those standard classification algorithms achieve almost the same performance as the propose method (1-2% of difference). Does it mean that this 1-2% of improvement makes these algorithms applicable on this task?? In another example: \u2018The classification of images taken in special imaging environments except air is the first challenge in extending the applications of deep learning.\u2019 \n- Some results on Table 3 are useless (e.g., those 5-top error equal to 0%). \n- Authors split the dataset into training and evaluation. Nevertheless, they should also use a validation set to stop the training and pick the best model (based on the validation images) to generate the predictions on the testing set. Otherwise, they may be overfitting the model on the training set. \n- Overall, this paper presents an incremental contribution with respect to existing networks, just to improve 1% the classification performance on an easy task (baseline performance around 98%). Thus, I do not feel that this work may attract the interest of the ICLR attendees.  "}