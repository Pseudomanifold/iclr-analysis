{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper shows a method to classify between underwater and non-underwater images (binary). For this, they proposed a new module in a CNN network, called 'Inception-Attention' module. It combines the inception module (multiple sizes of kernels) with an attention module (learn jointly some masks in order to focus the network to part(s) on the image). Also, their network proposes 2 classifications, by separating the network in 2 branches at the end - with the goal of less overfitting. Their method was compared against other state-of-the-art networks, usually designed for more complex tasks (multiclass classification). Lastly, they showed how their new 'IA module' could be useful in other tasks/networks, by replacing the standard inception network with theirs.\n\nOverall, I found the paper confusing: while some technical aspects are interesting, I still don't get the motivation of this new network for this task. Moreover, I did not understand the goal of the paper until the middle of the paper: I thought it was to do a multi-class classification from underwater images, while it is just a binary classification of underwater/ non-underwater images. I still think there are some valuable arguments, and the last part, 4.4, showing the usefulness of the method for more complex classifications, is to me the most interesting part. I could change my grade if a better motivation or a reorienting of the paper was made.\n\nRemarks/ questions:\n- The writing should be improved as it is even hard to understand some sentences. Some particular help will be given below.\n- My main concern is: have you tested easier classifications methods? It seems that classifying underwater vs. not underwater images would be easy. In fact, even the mean color should be identifiable... do you have a simple baseline to compare to? You are saying that one of the main problems is that salient objects are less visible underwater, there is blurring, ect. : these are for me all arguments why it would be easy to detect underwater images, because standard convolutions will behave differently.\n- Have you tested a simpler CNN? You are right in saying that the state-of-the art methods for multi-class classification are too large for this task. So why to you want to complexify it, and why not use a simpler network?\n- There is no related work on background classification (or 'context' classification); but I am sure that there might be works on this. It would be more interesting than the general image classification models.\n- in 3.1 and 3.2, you are saying 2 opposite things. a) 'the features extracted by a conv. kernel with large size tend to describe the global information [..] and will result in a certain degree of waste of computational resource' and b) 'we adopt convolution kernels and the average pooling to reduce the impact of local features of the image'. Do you want or not want large kernels?\n- 'Moreover, human recognition of underwater images is often based on the background..'. Why? Are you sure? I think it is more based on the color of the image, the texture, ect.\n- Auxiliary classification branch: If you have overfitting problems; why don't you use standard methods for treating overfitting (the first one is to have a smaller and simpler network..)?\n-I don't see a real improvement between the auxiliary classification branch and no auxiliary classifier in the Figure 4. Such a small difference, compared to the large oscillations, is not enough. Why don't you use different runs of your model with different initializations, in order to take the mean?\n- 3.3: not clear: 'the channel of the input image is increased to 32'. I think you want to talk about the number of channels, and not 'the channel'. Same error few lines below.\n- How did you select the 5000 non-underwater images? Randomly? Or just outside views?\n- 4.2: I don't see a validation set. It is important to have 3 sets, one training, one validation, one test. All the tuning of parameters/architectures must be done on the validation, with the test kept hidden until the publication.\n- 4.3: results on the training set should not even be shown...if you want, you can show the validation error, but not the training. An accuracy of 99.3% means that it is an easy task. Yes, it's true that your method works, but i) since you are not using a validation set, you could have just tuned your model until you have a good accuracy on the test set; ii) it might be better to prove the usefulness of your method on a more complex task.\n- 4.4 : this is the important part of the paper I think, you should develop it :)!"}