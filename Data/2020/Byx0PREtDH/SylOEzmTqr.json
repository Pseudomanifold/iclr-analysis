{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tries to handle the unseen attribute-object pairs recognition, which asks a model to simultaneously recognize the attribute type and the object type of a given image while this attribute-object pair is not included in the training set. The claimed contribution includes: (1) they for the first time introduce the attractor networks to recognize unseen attribute-object pairs; (2) their method exhibits much better performance than conventional methods and state-of-the-art methods on two challenging public datasets.  \n\nI found this paper poorly written. First, the introduction of attractor networks lacks intuition, explanation, and experiment support. The only support of attractor network is Table2, which show marginal improvement on MIT-States and significant improvement on UT-Zappos. Even for the two picked dataset, the improvement is not consistent, this is not enough to show the effectiveness of attractor network. The authors could design some visualization/metrics to show what the attractor networks have learned, i.e., the visualization of the nodes. The authors should also provide more ablation studies on the attractor network, i.e., the number of the nodes, the time-tick Tv, etc. After all, the experiment results in this paper shows little evidence of how the attractor network works, and the insight of how it works if so.\n\nSecond, the ablation study and conclusion are confusing. Combining Table 1,2&3, we can see that the decoding loss is the magic. Therefore, I should say that the decoding loss is the main reason for the ''much better performance'' in this paper, instead of the attractor network. However, the baseline method GENERATE already has the reconstruction/decoder loss. Why is its number low? \n\nI did not get the meaning of this sentence in Section 3.4. \"For the images belonging to the same pair, we\nuse the recognitions for all images to vote a pair label to be the final attribute-object pair recognition.\""}