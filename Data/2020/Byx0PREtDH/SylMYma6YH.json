{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper attempts to tackle unseen object-attribute recognition in still images. This follows a line of works that addresses such a problem using embedding (with operators), classification and generative approaches. The paper makes use of the visual attractor network, extracting different visual representations for objects and attributes. A collection of losses (some regularisation and some distance in embedding space) are proposed. I believe the paper (except A below) is technically correct.\n\nThis paper was particularly tricky to read. While convincing at parts, there are two worrying statements. One I currently believe (pending the authors' response) is a scientific flaw (A), the other causes the results to be potentially incomparable to published works (B). I detail these next.\n\n(A) Scientific Flaw: Quoting from the paper, at inference (i.e. for a test image), \"we propose a voting inference method. For an input image, its visual attractor feature AV is computed, ... By computing the L2 distance ... the pair that corresponds to the minimum distance is taken as the initial attribute-object pair recognition for the input image. For the images belonging to the same pair, we use the recognitions for all images to vote a pair label to be the final attribute-object pair recognition.\"\nIn my interpretation, the authors are taking all test images that are labeled using the same pair (e.g. 'sliced-apple' - assuming this is an unseen pair). They then use all these to make a final decision for all these images. \nI believe my interpretation is correct because their statement \"images belonging to the same pair\" can only exist in testing for their target scenario (unseen object-attribute pair).\nIf my interpretation is correct, this is A MAJOR FLAW in the approach. How do the authors know that a collection of test images \"belong to the same pair\"?? This knowledge can only be acquired from labels of the test set and cannot be used as part of any algorithm.\nI would very much like for my interpretation to be naive, but I could not find a different explanation for this statement in page 6. The authors note in the conclusion that this voting approach is part of their contribution.\n\n(B) In reporting the results, the authors state that: \"Using the grid-search method, we set parameters \\alpha\u000b; ... [5 parameters] \f in Eq.17 to be 1:0; 2:0; 6:0; 3:0; 2:0 for the MIT-States dataset and 1:0; 3:0; 6:0; 1:0; 2:0 for the UTZappos50K\ndataset. Attractor parameters cv; cl; Tv; Tl are set as 0:1; 0:2; 20; 10 for the MIT-States dataset and 0:08; 0:16; 25; 15 for the UT-Zappos50K dataset.\"\nThis assumes that the authors are grid-searching all parameters, and evaluating the test set performance for each case, then reporting the maximum possible test performance per dataset. The authors do not reference a validation set they use to set these parameters instead. Importantly, the values are SIGNIFICANTLY different for each test set and there is no explanation of why the parameters vary largely. It is not clear how the performance on an new dataset would be, \nUp to my knowledge, the other methods that they compare to in Table 1, have not reported a grid-search over the parameter space for maximum performance of the test set. This IMO makes the comparative evaluation, which the authors deem to be 'impressive' quite unfair. It is not possible to assess the method's performance using these results."}