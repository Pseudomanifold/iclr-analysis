{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary \nThis papers presents a pixel-autoregressive model for video generation, in the spirit of VPN (Kalchbrenner\u201916). The proposed method uses video transformers and is made computationally efficient by extending block-local attention (Parmar\u201918, Chen\u201918) and sub-scaling (Menick\u201919) to 3D volumes. The block-local attention is separable, meaning that in theory it is possible to connect every two pixels through a sequence of block-local layers. However, for efficient parallelization implement via masking mechanism it is necessary to ignore certain connections, introducing independence assumptions.  The model is shown to substantially exceed state-of-the-art in terms of likelihood as well as quantitative and qualitative visual quality on several datasets, including the very challenging Kinetics-600. Interestingly, it is shown that the model with spatiotemporal subscaling is more robust to higher generation temperatures, which could imply robustness to accumulating errors. \n\nDecision\nThe paper proposes a well-motivated method backed by solid state-of-the-art results. I recommend accept.\n\nPros\n- The proposed method is relevant and well-motivated.\n- The experimental results are strong.\n\nCons\n- The paper novelty is somewhat limited as it is mostly a combination of previously existing techniques.\n- The paper does not provide code which makes the results not easily reproducible. I think a minimal example of the code should be provided that is trainable at least on a simple dataset.\n\nQuestions\n- No videos are provided. Please provide an (anonymous immutable) link to video results.\n- Strong aliasing artifacts can be seen in the supplement on the Kinetics data, such as vegetables becoming increasingly \u201cblocky\u201d as well as general cube-like aliasing artifacts in Fig. 9. This indicates that the introduced independence assumptions are likely hurting the video quality. The paper discusses this in the appendix C, stating that there seems to be no remedy for the independence assumptions that does not increase the computational cost. However, this is exactly the problem that latent variable models such as variational inference or normalizing flows are designed to address. Would a certain combination of latent variable models with the proposed autoregressive approach alleviate these issues?\n\nMinor comments\n- Contrary to the summary in the related work section, Kumar\u201919 does not use variational inference and operates purely on the normalizing flows technique. Similarly, Mathieu\u201916 and Vondrick\u201916 do not use variational inference either instead relying on adversarial techniques. The paper correctly states that Lee\u201918, Castrejon\u201919 use variational inference.\n- Figure 2 is never referred to in the text. \n"}