{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper focus on how activation functions\u2019 nonlinearities shape the loss surface of neural networks. The authors first show why the loss surface of every neural network has infinite spurious local minima. Secondly, the authors prove one theorem to show four properties of the loss surfaces of nonlinear neural networks.\nAlthough this paper is generally easy to follow, and the motivation about nonlinearities and the loss surface is clear, the insight of this paper is somehow shortcoming. Though this work can prove such properties within different preconditions, whether other works\u2019 conditions are inconvenient or not may remain further discussions. This work is established based on several preconditions, while it is hard to assert that most kinds of neural networks can satisfy them perfectly. For instance, this work mentions \u201cDeep learning without poor local minima (NeurIPS2016)\u201d, which requires full-rank and conditional independence of each node. It could be feasible when training a stacked network with particular limitations. This work requires all hidden layers are wider than the output layer, which may not be suitable for image segmentation, generative tasks or super-resolution, etc. Besides, it is laudable to prove fundamental rules in neural networks, while showing or inspiring researchers about how to implement or approximate such results to improve neural networks might be more helpful.\nSome questions:\n\n1. The authors assert that \u201cthe loss surface of *every* neural network has infinite spurious local minima\u201d in the abstract, while in chapter 3 line 2, authors mention, \u201cWe find that *almost all* practical neural networks have infinitely many spurious local minima.\u201d Which one is correct? Based on the following description in this paper, I guess this result is conditionally tenable.\n\n2. In lemma 3, authors construct the local minima by adding very negative biases and show they are spurious. However, it is less likely to learn such negative biases in the real case. Besides, some networks require biases equal to zero to achieve some specific target. My question is: if biases are conditioned on real-world data distribution, will lemma 3 and 4 still work in this case?\n\n3. This paper mentions \u201cinfinite\u201d many times. Based on the reference, I believe that the \u201cneural network\u201d in this work refers to the \u201cartificial neural network,\u201d which is majorly stored within float tensors. So the number of combinations of parameters is finite. So why use \u201cinfinite\u201d instead of \u201cmany\u201d? Finite means I can train a small scale of networks with fewer precisions and check the global minima with a fixed dataset.\n\nAll in all, I believe this paper can be significantly improved if more details and experiments are provided."}