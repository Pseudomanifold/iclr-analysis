{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper studies the landscape of deep neural networks with piecewise-linear activation functions. The paper showed that under very mild assumptions, the loss surface admits infinite spurious local minima. Further, it is shown that the loss surface is partitioned into many multilinear cells. If the network is two-layer with two-piece linear activations, it is proved that within each cell every local minimum is global. \n\nPros:\n  --Constructed spurious local minima for piecewise linear activations, for a broader setting than previous papers. \n  --The paper is well written, with detailed explanation of proof skeleton. \n\nCons: \nThe significance of the results are not clear. Details are given below. \n\n1.\tThis paper only considers piecewise linear activation, which is a special type of non-linear activation. The major examples are ReLU and leaky ReLU. However, related results for ReLU have been studied for a few previous works mentioned in the last two paragraphs of Sec. 3.2. In particular, Yun et al. 2019b already proves a similar result for 1-hidden-layer neural-net with ReLU activation. I think extending the construction to broader settings (any depth, any piecewise linear and more losses) is mathematically nice, but the motivation of this extension is somewhat unclear to me. One motivation is that this is helpful for the purpose of understanding a \u201cbig picture\u201d of the landscape, which I will discuss next.\n\n2.\tThe second major result is Theorem 2, on the \u201cbig picture\u201d with ReLU-like activations. However, Theorem 2 is somewhat trivial to prove, and the link to Theorem 1 is rather weak.\n   (a) The main message of Thm 2 is the partition of the surface into multiple pieces, and each piece has good property. This partition is somewhat straightforward, and has been studied before, in, e.g., [R1].\n   For a global \u201cbig picture\u201d, partitioning itself is not very interesting. Theorem 2 mainly describes the property of each region separately for 2-layer network, which is weaker than [R1]. \n   (b) Theorem 2 seems easy to prove. The 1st, 3rd and 4th property are all straightforward. The 2nd property \u201clocal analogous convexity\u201d was given a 2-page proof in the paper. However, I don\u2019t understand why not use the following simple argument: for each region, the network behaves like a deep linear network, thus directly applying existing result shall imply \u201cevery local minimum in the region is the global minimum of the region\u201d, right? If not, what is the difficulty?\n   (c) The 3rd property says \u201csome local minima are concentrated as a valley in some cell\u201d. What are the formal definitions of \u201cconcentrated\u201d and \u201cvalley\u201d in this sentence?\n   (d) The link to Theorem 1 is weak: the link is the 3rd property of Theorem 2 that \u201csome local minima are in a valley\u201d. It is just about some special local minima and weakly related to the other properties on the \u201cglobal view\u201d. In addition, the fact that \u201csome of them are in a valley\u201d may be due to the very special construction, thus it is not surprising and does not reveal anything interesting about the \u201cbig picture\u201d. \n\n\n3.\tOther issues:\na) While ReLU-type activations are popular, there are still commonly-used activation functions are not piece-wise linear, e.g., tanh, swish. It is not proper to claim that \"this paper presents how nonlinearities in activations substantially shape the loss surface\" and \"almost every practical neural network ....\". I suggest replacing \"nonlinearity\" with \"piecewise linearity\" in both the title and the abstract, and modifying the over-statements. \n   b) In Property 1 of Theorem 2, \u201csmooth and multilinear partition\u201d might be a bit misleading. The loss surface should be fractional in general, where multilinear cells are separated by non-smooth boundaries. \u201cSmooth partition\u201d seems to imply that the boundaries are smooth or the partition method is smooth in some sense.\n   c) The name \u201canalogous convexity\u201d is not appropriate. Analogous convexity is not formally defined in the paper. According to Sec. 4.3 third paragraph, \u201cthe property of analogous convexity that the local minima wherein are equally good\u201d. It seems that \u201canalogous convexity\u201d is just \u201call local minima are good\u201d, which is very different from convexity. It is a weaker property than quasi-convexity, star-convexity, etc, and thus it is better not to call it \u201canalogous convexity\u201d.\n    d) Property 3 of Theorem 2 is very far from \u201cmode connectivity\u201d. The proof of Property 3 relies on a special construction of Theorem 1, and the latter is for two arbitrary global minima. \n\n\n[R1] Soudry and Hoffer. \"Exponentially vanishing sub-optimal local minima in multilayer neural networks.\"\u00a0arXiv preprint arXiv:1702.05777\u00a0(2017).\n\n\nConclusion:  I think this paper is studying an important and interesting question, and the efforts of constructing local minima and understanding big picture are both interesting to me. However, I\u2019m afraid the current form of the paper does not meet the standard of the conference. That being said, it would be a nice paper if the big picture can be explored deeper, and the link to the spurious local minima can be built stronger. \n"}