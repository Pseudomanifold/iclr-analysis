{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the theoretical property of neural network's loss surface. The main contribution is to prove that the loss surface of every neural network (with arbitrary depth) with piecewise linear activations has infinite spurious local minima. Moreover, the paper further characterizes the partition of the local minima. More precisely, the loss surface is partitioned into multiple smooth and multilinear open cells and within each cell, the local minima are equally good. This result can also explain the linear neural network case where there is only one cell, implying that all local minima are global. \n\nOn one hand, I find the paper very clear and the result very clean, which unites a lot of existing results. On the other hand, with a reasonable initialization in practice, we will not attain the local minima constructed in the paper since it requires all the activations to be positive. This limits the plausible implication from this theoretical study. Overall, I am very positive of the paper, the following are some detailed comments. \n\na. Please be more precise in the abstract that the activation function need to be piecewise linear. \nThe current sentence \"the loss surface of every neural network has infinite spurious local minima\" does not include this specification. Moreover, if the activation is differentiable, is the claim still hold? It seems to me from the middle of page 3 that Li et al 2018 shows a non-local minima result in this case.\n\nb. How different is the analysis comparing to existing result?\nI have only go through the skeleton of the proof and have not read into the details. It seems to me the construction of the local minima is very similar to [1], since the main idea is to consider the linear region by activating all the neurons. Could you summarize the main difficulty to extend their results to multi-layer cases? (Maybe it would be good to illustrate with a simple case like 3 layers few neurons per layer)\nMoreover, when considering the local convexity, is it sufficient to say that locally in each cell it is a linear network and then the results on linear network transfers to it locally?\n\n[1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019"}