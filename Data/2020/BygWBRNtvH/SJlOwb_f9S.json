{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper conducts a large experimental study of various methods for adversarial attacks and defenses. The authors evaluate a wide range of proposed methods in four different attack scenarios (white-box, transfer, score-based black box, decision-based black box). The authors pay particular attention to the trade-off between attack budget (perturbation size) and the accuracy that a method achieves for a given attack budget (as opposed to only comparing at a fixed perturbation size). Moreover, the authors also measure the trade-off between the number of iterations of an attack and the resulting adversarial accuracy.\n\nOverall I find comprehensive benchmarks a valuable contribution to the literature. However, for such benchmarks to be insightful, they need to explore all relevant parameters of the experiment. Partially due to the broad scope of the benchmark (e.g., covering four different attack scenarios), I unfortunately find the current paper lacking in some aspects. Hence I recommend to reject the paper at this point. I encourage the authors to extend their benchmark (and possibly reduce its scope if necessary) and submit the updated results to another conference.\n\nIn particular, the following points would improve the benchmark:\n\n- In the white-box experiments, it is unclear how the hyperparameters of the various attacks (step size, etc.) were chosen. While the values are specified in the appendix, it is known that hyperparameters can sometimes have large effect on the relative performance of various attacks. Hence it would be important to explore the impact of these hyperparameters on the results in the paper.\n\n- The paper measures the trade-off between perturbation budget and adversarial accuracy. While this can indeed give a more comprehensive picture of the performance of various methods, it is also important to note that some defenses are optimized for a certain target perturbation size (e.g., the perturbation budget used in adversarial training). To get a more comprehensive picture, it would therefore be useful to have multiple defense models trained with different hyperparameters.\n\n- For the transfer experiments, my understanding is that the experiments use only a single source model (one for CIFAR-10, one for ImageNet). Why is this the case? The effectiveness of transfer attacks depends on how similar the source and target models are. So utilizing a broader set of source models could yield a more comprehensive evaluation of transfer robustness.\n\n\nFurther comments / questions:\n\n- Why is MIM listed as a transfer attack in Section 3.1?\n\n- As mentioned in the paper, some of the randomization and transformation defenses were already broken in prior work. What additional insight can we derive from including them in the current benchmark?\n\n- Section 4.2 states \"[...] we do not evaluate PGD since PGD and BIM are very similar [...]\". Why do the authors prefer the term BIM considering that projected gradient descent has a long history in optimization?"}