{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1100", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper performs a large-scale empirical study by comparing different adversarial attack and defense techniques. From these experiments, they conclude that the robustness of different techniques varies based on perturbation budgets and attack iterations. They also found that adversarially trained models have, in general, the most successful defenses and the randomization based defenses are generally more robust for black-box attacks. \n\n+ The author presented a comprehensive background of adversarial attacks, which itself is a good contribution to the literature.\n+ I think the main contribution of the paper is the used of two curves: accuracy vs. perturbation budget and accuracy vs. attack strength curves to evaluate the attacks and defenses. I agree with the authors that such curves show a complete spectrum of weakness and strength of a technique, and future researches should adapt such evaluation.\n+ I appreciate the authors\u2019 effort to compare 16 states of the art attacks and defense techniques.\n\n- However, the study findings are not that novel. For example, it is intuitive that the robustness of a technique will vary with the perturbation budget and iteration steps. Also, PGD-based adversarial training is known to be state of the art defenses, and I am not surprised at all with their findings.\n\n- Since the authors used different defense models for CIFAR-10 and ImageNet, it becomes difficult to compare the model\u2019s performances across the dataset. \n\n- The title is somewhat misleading---there is no benchmark tool or dataset that the paper contributed. Their main contribution is a thorough empirical study. \n"}