{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents an evaluation of a number of kinds of classification models under various adversarial attack methods.\n\n\nFor classification models, the authors consider a collection of defenses ranging from those trained with robust optimization to those utilizing input transformations. The set of models includes those that have been previously shown to be broken (e.g. Song et al., 2018a, Xie et al., 2018, among others), and includes defenses designed for a wide range of threat models.\n\n\nFor attack methods, the authors include various attacks for a mix of threat models covering the white-box setting, black-box settings, and various allowed l_p norm perturbation bounds.\n\n\nThe study presented by the paper has unclear significance, does not present key information, and does not compare with key previous work. For these reasons I do not recommend acceptance.\n\n\nDetailed feedback:\n\n\n- The study is of unclear significance. From a security perspective, attacks on defended models are only interesting if the threat model is well-specified, and the attacker is able to dynamically adapt to the model within the threat model. Here, the attacks are blindly applied to the various defenses---as shown in [obfuscated gradients], this will not give a meaningful worst-case evaluation. Moreover, the fact that breaking the threat model will allow an attacker to easily bypass a defense is well known (e.g. Sharma and Chen 2017 [0]).\n\n\n- In a related note, in the conclusion section, the authors state that it is important to evaluate defenses at various attacker configurations (e.g. varying the number of steps and l_p perturbation allowed) to determine the superiority of one defense over another. However, in the security setting for adversarial robustness, the important metric is the accuracy in the specified threat model that is being defended against in the worst case.\n\n\n- The authors do not clearly state what the defenses' intended threat models are. It is therefore hard to evaluate the impact of the various attacks on the defenses in a meaningful context.\n\n\n- The authors do not discuss a number of previous related works. On the subject of transfer based attacks, Liu et al 2017 (ICLR \u201817) [1] study extremely familiar phenomena around varying the strength of the attack and its effect on transfer attack success. For how models stand up in various threat models, including those that they were not designed for, Kang et al [2] study this in depth as well and they are not cited. Finally, Khan et al 2019 [3] study randomization in the context of black-box attacks as well. \n\n\n[0] https://arxiv.org/abs/1905.09871\n[1] https://arxiv.org/abs/1611.02770\n[2] https://arxiv.org/abs/1905.01034\n[3] https://arxiv.org/abs/1905.09871\n"}