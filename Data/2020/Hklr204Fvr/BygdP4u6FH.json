{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\"Towards a Deep Network Architecture for Structured Smoothness\" proposes a new layer, dubbed FGL, specifically focused on structured smoothness.\nThe paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fMRI analysis. The writing is clear, the method is well formulated, and performance compared to relevant and recent benchmarks is strong. Thus most of the questions remaining are about the finer details of the proposed layer and method.\n\nWhat is the sensitivity of FGL to non-optimal group proposals for A? Given the authors reference Aydore et. al., do randomized (or perhaps meta-optimized) groupings perform poorly? Are there any potential ways to jointly-or-iteratively learn the groupings? Is Ward clustering the primary practical method for getting the groupings?\n\nCan the authors discuss a bit the relationship and potential tradeoffs between \"early\" segmentation architectures (such as FGL), and those often favored in semantic segmentation (\"late\" segmentation)? Is there a particular reason why \"late\" segmentation is particularly a poor choice in brain imaging?\n\nMy primary concerns come in relation to other methods for structured smoothness such as CRF/MRF, and the trade-offs and drawbacks of imperfect or downright wrong A, and alternative methods for practically finding good A matrices (potentially with an eye to applications outside fMRI). Addressing some or all of these in a section or two of the text body would potentially raise my score.\n\nSome small errors:\nOptimg -> Opting Introduction\nGuassian -> Gaussian section 3.2\n\nPotential useful references:\nCRF as RNN https://arxiv.org/abs/1502.03240\nEfficient Piecewise Training https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.html\nDeep Learning Markov Random Field for Semantic Segmentation https://arxiv.org/abs/1606.07230\nPixel Adaptive Convolutional Neural Networks http://openaccess.thecvf.com/content_CVPR_2019/html/Su_Pixel-Adaptive_Convolutional_Neural_Networks_CVPR_2019_paper.html"}