{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The contribution of this work lies in providing a library for working with the existing variants of infinite-width neural networks and avoiding the need to derive the NNGP and NT kernels for each architecture by hand. The authors have firstly shown performance comparisons between inferences between finite vs. infinitely wide neural networks. The authors then go into some implementation details with their library. The authors have provided the code and cookbook in the links provided in the abstract. On the overall, I like this effort which is timely.\n\nSome additional suggestions below:\n\nI would like to see an additional metric for performance comparison of probabilistic models, which is often used in the GP literature: mean negative log probability.\n\nIt would also be interesting to see how the posterior variance (e.g., Fig. 1 right) evolves over the entire space during training. \n\nI would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3.\n\nFor the summary of contributions, can you give the corresponding section number to refer to when you demonstrate each feature? For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper?\n\nCan the authors elaborate on the ease of expanding their library for the new developments in this field?\n\n\nMinor issues:\n\nPage 1: Gaussian Procesesses?\nPage 4: it\u2019s infinite?\nFig. 4: I would have preferred the indices to be placed as subscripts instead of superscripts.\nPage 8: it\u2019s order of dimensions?"}