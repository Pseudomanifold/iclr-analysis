{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work investigates compression-aware training and introduces a new hyper-parameter called \"Non-regularization period\". Basically, the paper proposes to apply weight regularization and compression less frequently so that they can use stronger regularization/compression, hence achieving high compression ratio and model accuracy.\n\nOverall, the idea of asynchronous regularization and compression is interesting and worth further investigation. However, I vote for a reject for now because\n(1) The paper is hard to follow and the writing can be significantly improved especially for the abstract and introduction. I felt rather confused when I first read the abstract and introduction. The authors frequently use the word \"weight update\", however it is unclear whether it refers to gradient update or the update caused by regularization/compression. Besides, the title is also a little confusing. There's little discussion about the choice of batch size and I don't really understand how weight regularization is decoupled from batch size after reading the paper. I suggest the authors to think more about the title.\n(2) The reason why such a training scheme improves model compression is unclear and needs further investigations. In the paper, the authors first interpret model compression as a way of inducing weight decay (and random weight noise). Particularly, the model accuracy is roughly constant over different value of NR period (as shown in Figure 3 and Figure 4) when weight decay is used. However, the results of model compression are quite different (according to Figure 5). The best performance is only achieved with very large NR period in the case of SVD and pruning. I think the authors should give some explanations for that difference. Also, I notice that quantization behaves more similarly to weight decay and it requires further discussion.\n\nTo sum up, I like the idea of asynchronous regularization/compression, but I'm not quite satisfied with current version of paper. I encourage the authors to improve their writing and add more discussions (see my point (2)) to the paper. I'm willing to increase my score if the authors can address my concerns."}