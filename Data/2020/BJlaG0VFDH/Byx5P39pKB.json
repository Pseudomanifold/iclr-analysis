{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies a model compression algorithm, where training consists in alternatingly updating a full precision weight vector using SGD for a number of steps N, and then compressing the weight vector to a quantized or low-rank representation. The authors present an empirical study of how the properties of this algorithm change as they change the number N.\n\nThe contribution is marginal, and the paper is hard to read.\nPeriodically compressing networks weights, rather than at every iteration, is not new. The paper is missing a \"related work\" section that embeds the proposed algorithm in the literature.\nThe experiments are small scale and not exhaustive and there is very little comparison to previous work and alternative methods.\n\nQuestions/comments for the authors:\n- When you say \"regularization\" you seem to usually mean \"compression\"? This was confusing to me.\n- What do you mean by \"asynchronous regularization\"? You seem to mean periodic compression?"}