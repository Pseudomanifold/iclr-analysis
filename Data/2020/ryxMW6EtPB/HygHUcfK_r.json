{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper has problems with clarity/polish and experimental design that are sufficiently severe\nto merit rejection by themselves.\n\nRegarding clarity/polish:\nI am generally not super picky about these things, but there does have to be some standard.\nThis paper looks very hastily put together, especially pages 7 and 8.\nThere are many typos and unclear statements.\nJust a few examples:\n\n> Generative Adversarial Networks (GANs) are powerful framework for (in the abstract)\n> be a good metric to evolution the difference (in the abstract)\n> In the past few years, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are impactful because it has shown lots of great results for many AI tasks, (first sentence)\n\n> It means that there is no an unanimous metric to represent the difference between the true data distribution and the generated distribution\nWhat does this mean? People have mostly settled on using FID for this.\n \n> It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by human eyes.\nIsn't this just restating the point made in the first sentence?\nRegardless, nobody really uses human evaluation anymore - so this is just not correct.\n\n>  It means that if the original generator and discriminator are random, it is difficult to confirm that the generator and discriminator can converge to the ideal conclusion by training with given data.\nBut this paper doesn't propose a way to solve that problem, so it's strange to mention this here in this way.\n\n\nThese issues would maybe be excusable if not for the totally inadequate experimental validation.\nA non-exhaustive list of methodological problems with the (single) experiment:\n\n1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs\nhave inter-run variance larger than the difference in score reported in Fig 1 and 2.\n\n2. The models have not been trained for long enough.\n\n3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which\nprobably leads to:\n\n4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,\nand rendering the third claim from the introduction (\"We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms.\") completely untrue.\n\nIn light of these other issues, I haven't checked the proofs.\n"}