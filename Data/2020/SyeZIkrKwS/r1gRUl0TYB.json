{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1718", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info.\n- The method obtained improvements over various networks (SuffleNet v2, MobileNet v2, ResNet 18) on ImageNet.\n\nMethods\n- Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention.\n- The GAP of the features gives the channel attention on each stage, and the method applies the dynamic selection of the kernels.\n- The number of channels in skip-connection shluld be the same because it should be elementwise multiplied with the channel attention acquired from GAP.\n- The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops.\n\nQuestions\n- According to Figure 4, it seems that the proposed add-on requires many parameters because it would include a FC layer for each block. But we cannot find the number of parameters in this paper.\n- The parameter $g_t$ is defined as 6. The experiment shows the ablation to the case $g_t$ =1, but what if we set the parameters to other numbers?\n\nStrong points\n- The proposed model achieved improvement with fewer Flops on large scale image classification dataset.\n- The method shows effectiveness when it is attached to various classification networks.\n\nConcerns\n- The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze-and-excitation network (SEnet, Hu et.al.). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet.\nThe author should clarify the difference and the strong points of the proposed block compared to SEnet.\n- Also, the reviewer cannot guarantee that the networks trained by the proposed method can transfer the knowledge to other tasks such as detection. \nThe reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre-trained backbone for the other tasks.\nRegarding this, the baseline methods (MobileNet V2, Shufflenet v2, ResNet)  are widely used as a pre-trained backbone for object detection, and the papers mention the CoCo object detection results using the pre-trained backbones from their method. The reviewer thinks that the experiment regarding this should be included.\n- The other thing is that the parameter increases. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'.\n\nConclusion\n- The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. \n- However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection).\n\n\nInquiries\n- Clarifying the difference between SEnet.\n- Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone).\n- Discussing the number of the parameter as well.\n"}