{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== Summary ===\nThe authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The dynamic kernels are obtained by a linear combination of static kernels where the weights of the linear combination are input-dependent (they are obtained similarly to the coefficients in squeeze-and-excite). The authors also include a theoretical and experimental study of the correlation.\nThe authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels.\n\n=== Recommendation ===\nThe experimental setup is rigorous but the current draft lacks some metrics that should be reported (as training times, parameter counts, memory requirements at training/inference) since the focus is on making CNNs more efficient.\n\nThe presented experimental results are satisfactory but the studied networks are not quite SOTA: they are much more competitive alternatives to ResNet and MobileNetv2. The correlation study is interesting.\n\nMy main issue with the paper is the lack of novelty. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against.\nHowever, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel.\n\nOverall, I am on the fence with this paper but slightly leaning towards rejecting it for the above reasons.\n\n=== Questions/Comments ===\n- Figure 5: how are the models constrained to have same FLOPS? Is it by changing the number of channels?\n- Consider adding training times for more transparency\n- Consider adding parameter counts in experiment tables\n- The related work subsection 2.3 is rather poor compared to existing work.\n- 'While model compression based methods' -> 'On the other hand, model compression based methods'\n- 'computing efficient' -> 'compute efficient'\n- 'values distribute' -> 'values distributed'\n- 'DETAIL ANALYSIS OF OUR MOTIVATION' -> 'Detailed analysis of our motivation'"}