{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper describes a data augmentation approach whereby noise is added to features (activations) late in a prediction network.\n\nThe value of this is based on the relationship between high feature weight and impact on classification decision.  By using this information to drive the augmentation noise, the augmentation can introduce more noise to more \"important\" features.\n\nThe performance is generally strong, but is somewhat worse than VAT+EntMin.  Additional discussion comparing these two approaches would be helpful. \n\nThere is some inconsistency in notation in the noise equation on page three. \\theta_{fc} is defined to be 128 x C, but \\bar{f} is R^128.  while f(\\bar{f} + noise) is used later.  It is unclear how R^128 is added to 128 x C without broadcasting or some other operation.  Detail should be provided here.\n\nOn page 5, the dropout probability is set to 0.8 \"because feature-based augmentation inherrently has its own regularization\".  I suspect this should be 0.2, not 0.8.  0.8 implied more dropout that the baseline of 0.5, and more regularization not less."}