{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors study the problem of estimating the Lipschitz constant of a deep neural network with ELO activation function. The authors formulate the problem as a polynomial optimisation problem, which is elegant. Subsequently, they utilise an LP hierarchy based on the Krivine-Vasilescu-Handelman\u2019s Positivstellensatz and suggest exploiting sparsity therein. The computational results are clearly not sufficient to apply this approach to real-world neural networks, but are still respectable.\n\nSection 3 (Theorem 2) is not original work, as leaving the theorem without a reference would imply: the authors cite Section 9 of Lasserre's 2015 book later, so they are clearly aware of this, and there are many application even within verification, e.g.,\nhttps://link.springer.com/content/pdf/10.1007%2F978-3-319-48989-6_44.pdf\nhttps://ieeexplore.ieee.org/document/8493559\n\nThe suggestions as to the exploitation of sparsity (Section 4) are not original work either. The authors could cite, e.g., JB Lasserre: Convergent SDP-relaxations in polynomial optimization with sparsity (SIAM Journal on Optimization, 2006), as one of the early proponents of the exploitation of sparsity.\n\nIn Section 7:\n-- The claim \"We observed clear improvement of the Lipschitz bound obtained, compared to the SDP method\" is not supported by the results the authors present. \n-- The authors do not present the run-time. This needs to be included, considering they imply that the key improvement over the traditional SDP is that this works with smaller variables and should be faster. \n-- The presentation of the experimental results should be improved, so as to follow the NIPS reproducibility checklist, or at least have error bars at one standard deviation and standard deviation in the table. \n\nOther than that, the paper is well written (modulo Section missing in \"Section 5\" at the top of Section 7), and I would recommend its acceptance. "}