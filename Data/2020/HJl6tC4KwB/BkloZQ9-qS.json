{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a very effective approach to modify an input image according to natural language description. The method is modified based on\u00a0ControlGAN with two novel\u00a0modules. The 1st is\u00a0Co-Attention Module (CoA) which is well-motivated and the ablation\u00a0study clearly shows the effect of this module. The second is the Detail Correction Module (DCM), which is only briefly described in the main paper with the most part described in the supplementary material. The effect of DCM is briefly shown in Fig. 8.\u00a0\n\nThe qualitative and quantitative results a\nre very impressive compared to\u00a0SISGAN and\u00a0TAGAN. In the ablation study, a few examples also show the\u00a0ControlGAN backbone architecture without either CoA or DCM generates results worse than the proposed method. However, I wonder what's the \"quantitative\" comparison with\u00a0ControlGAN backbone architecture without either CoA or DCM, or even both modules.\u00a0\n\nIn Fig.7, why oncat., Matched and Concat., Given generate structurally very different images? Is this typically happening? In Fig. 8, without CoA means CoA is removed from both the main architecture and DCM? In yes, what is used instead of CoA?\n\nIn general, the paper is tackling a hard and interesting task and the results are very impressive. My only concern is how\u00a0\"quantitative\" improvement is from CoA or DCM as compared to the backbone ControlGAN. Hence, I recommend for weak accept."}