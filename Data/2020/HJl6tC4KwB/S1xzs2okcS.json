{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# Summary #\nThe paper works on text-guided image manipulation. The authors proposed two modules, co-attention + detailed correlation, to better exploit the text information and maintain the semantic consistency on the synthesized image. The qualitative results look impressive.\n\n# Strength #\nThe qualitative results are quite impressive. The authors also point out the deficiencies of existing methods, suggesting future improvements.\n\n# Weakness/comments #\n1. The technical part of the paper is poorly organized/described. First, there is no general introduction to the pipeline. (The authors just refer to ControlGAN and show the overall framework in the appendix.) This makes it hard to understand the overall training objective, i.e., where/what are the generator and discriminator? There seems to be no introduction about the text features in Sect 3 and no text features are involved in the co-attention module. More importantly, for the two proposed modules, there are insufficient descriptions about what they are actually doing and why they will perform better than baseline methods.\n\n2. Co-attention mechanisms have been studied in other tasks that involve vision and language, such as visual question answering and captioning. However, the authors fail to refer to them. For example,\n\nJ. Lu et al., \"Hierarchical Question-Image Co-Attention for Visual Question Answering,\" NIPS 2016\nZ. Yu et al., \"Multi-Modal Factorized Bilinear Pooling With Co-Attention Learning for Visual Question Answering,\" ICCV 2017\n\n3. Since the main goal here is to change the attributes (or local detail) but maintain the global layout, the authors should discuss style transfer, which has very similar input (one image for content, and one image for style) and goal. The main difference is that, here, the style is given by text, not by a reference image.\n\n4. The authors should discuss the task (image manipulation) more. For example, if the task considers action attributes (from a standing horse to a running horse, or from a smiling face to a sad face), and if the proposed model can handle it.\n\n5. There is no description of the dataset. Note that, to train the model the authors need triplets of (I, S', I'), which are not directly provided in COCO and CUB. There is no description of the evaluation metric: how the similarity is computed?\n\n# For Rebuttal # \nSee the above comments.\n\nWhile the paper gives impressive qualitative results, the paper needs significant modification, especially on the technical part (e.g., Sect 3.). The current version is more like a technical report without any insights into what the proposed modules are actually doing."}