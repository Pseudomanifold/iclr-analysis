{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a method to solve the multi-person pose estimation problem. The authors design a bottom-up method and demonstrate that this bottom-up method performs better, especially under the circumstances where subjects have severe occlusions.\n\nIn particular, the authors utilize ConvLSTM to estimate the pose of one subject at each iteration. The choice of the recurrent network for the multiple person pose estimation is relatively novel. Designing a separate branch to determine the stop criteria is also well done under the proposed architecture. Especially I like the discussion and experiments on the ordering part, which solves the most critical problem applying the ordinal prediction method, i.e., LSTM, to this problem where the order does not really matter. The authors also come up with  new optimization tricks to deal with the limited GPU memory. The experiments are also reasonable: the proposed method achieves the best results among bottom-up methods on COCO and the overall best result on OCHuman.\n\nOverall, I believe this could be a good paper if the authors can address some of my concerns here:\n1. the motivation of the loop in the first LSTM is not quite clear to me. The authors demonstrate the feedback loop is important in the experiment and text, but the motivation is unclear. Moreover, if this loop is very useful in LSTM1, wouldn't it be natural to also include the feedback loop in LSTM2?\n2. based on what is claimed in Table 3, the optimal order is the learnt order, that is, u in Eq. 4 should be the same order as the order in Eq. 2. Then the overall architecture of the network consists of two consecutive LSTMs with intermediate supervision, which is intrinsically similar to stacked hourglass used in Newell\u201917. Therefore, can we expect better performance if we stack another LSTM on the proposed network?\n3. since the optimal order is the learnt order, have the authors trained the model multiple times to see if the learnt orders are the same? If they are different, can we still draw any conclusions?\n4. the performance on the COCO dataset is not so significant compared to the top-down methods. Though the authors claim that the COCO contains too many easy cases, there is no analysis about how the proposed method fails at these cases.\n5. missing literature.\nJin, Sheng, Wentao Liu, Wanli Ouyang, and Chen Qian. \"Multi-person Articulated Tracking with Spatial and Temporal Embeddings.\" CVPR 2019\nThis is also a fully differentiable bottom-up method. They achieved 0.68 AP on COCO.\nZhou, Xingyi, Xiao Sun, Wei Zhang, Shuang Liang, and Yichen Wei. \"Deep kinematic pose regression.\" ECCV workshop 2016\nSun, Xiao, Jiaxiang Shang, Shuang Liang, and Yichen Wei. \"Compositional human pose regression.\" ICCV 2017\nThese two methods work on single person pose estimation. They are bottom-up methods but they also consider the joint detection and association (bones) together as either a kinematic model or compositional model.\nIt would be great if the authors can also discuss these previous works.\n\nMinor comments:\nP in Eq. 1 should be \\textit{p}\nOn page 5 when HM is first time mentioned, it is not clear that HM abbreviates HeatMap\nOn page 7, \u201cCoco\u201d should be \u201cCOCO\u201d"}