{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a training framework that: \n(i) can efficiently handle catastrophic forgetting in a large number of tasks\n(ii) is robust to the ordering of the tasks\n\nThe high-level idea is to decompose learning parameters into two sets - one set that depends on the task and one set that is task agnostic. Hierarchal clustering is used to improve the efficiency of the training process by considering the decomposition of parameters at multiple levels (and not just 2). The paper shows improvements in terms of accuracy, stability, and order-robustness and provides ablation results (for various modifications of their proposed model) and considers a setup with around 100 tasks. The paper/idea is quite interesting and the results seem promising:\n\n* The results in figure 5: (d) and (e) are very interesting. It seems as if all the \"previous\" knowledge is being contained in the task-specific parameters. In general, I like the idea of being able to \"forget\" all the previous knowledge. I want to clarify one thing here: My understanding is that after training the model on tasks 1 to 5, the weights corresponding to task 1 are dropped (that is just the task-specific parameters tau and not the mask). Then before even one gradient update is applied, the model is re-evaluated on task 1. Is that correct?\n\n* Figure 7 seems to suggest that the drift is reduced by the proposed approach. What does the presence of multiple markers (of the same color) mean? For example, there are two green triangles in (b).\n\n====================\n\nBut, many things should be clarified for understanding the paper properly and for making a fair assessment of the claims made in the paper. I would be happy to update my score based on the authors' response to the following:\n\n* My biggest concern is the choice of baselines. The paper (rightly)  highlights that their work improves over many existing works as they provide a mechanism to \"retroactively update task-adaptive parameters\" for the previous task. But none of their baselines have this mechanism built-in. So while there is a clear advantage with the proposed approach, the comparison is unfair and the baselines should have considered approaches like GEM [0[ and A-GEM[1] while also provide a kind-of retrospective mechanism to correct the weights corresponding to the subsequent tasks. Without such a comparison, it is difficult to comment on the benefits of the approach.\n\n* On page 2, paragraph 3, the paper mentions that \"APD does not increase the intrinsic network complexity as existing expansion-based approaches do\". This claim seems to be loose since a new mask is learned for each task and needs to be persisted for all the subsequent tasks. So there is an \"expansion\"-like step involved. The authors should clarify this detail in the context of being memory efficient.\n\n* The authors propose a simplistic regularization approach (L2) to ensure that the shared parameters do not share too much across tasks. While it is good that a simple approach works so well, it would help if the authors discussed what do they think the reason is. EWC [2] and the authors' results indicate that regularization in the parameter space does not work as well as regularization in the function space. Thus the L2 regularization approach is not likely to work well.\n\n* Some parts of the paper needs to be reworded or clarified more. For example, since a per-task mask is being learned, there are two sets of weights being learned per task (the mask and the task-specific parameters). So there are more parameters to learn and not just the sparse task-specific parameters.\n\n* As I understand, the paper uses \"order robustness\"  to mean avoiding \"concept drift\" (or \"catastrophic forgetting\"). I might be missing something (in plain sight) but when I think of \"order robustness\", the order of tasks should not matter. This is somewhat different than avoiding concept drift.\n\n* Is the hierarchical clustering being done for each neuron (of the given model)? If yes, how does this approach scale to large neural nets? In general, how does the cost of doing the hierarchical clustering affect the training cost (of the model)?\n\n* Metrics: I am a little confused by the definition of the \"final task-average performance\" metric and could interpret it in at-least two ways. Could the authors please clarify this.\n\n* I do not understand some of the results in Figures 3 and 7, for the capacity of Progressive Neural Nets (PGNs). In general, PGNs add one new column (copy of base model) for each task. So the capacity of the PGNs should always be a multiple of intial model capacity. The results do not indicate that.\n\n* For the STL, the value of AOPD and MOPD suggests there is a good amount of variance when training the models. In this context, it would be helpful to know the variance associated with the other reported results as well.\n\n* Figure 6 is somewhat misleading as it does not account for the mask parameters that also need to be learned per-task.\n\n====================\n\nThings that should be clarified in the paper (but did not impact the score):\n\n* Is the attention (sigma) hard-attention or soft attention? \n\n* Is the attention applied per task (ie one scalar value per task) or layer or neuron?\n\n* Equation 2 seems to add a lot of complexity to the training mechanism (to correct for the weights of the previous tasks). Did the authors consider some other update/corrective mechanism that could be applied once a task has been learned? Please note that I am not criticizing the equation because it is complex. I am curious about the alternatives that the authors considered.\n\n* Are there any kind of mathematical guarantees when using equation 2? If not, why should it be a better alternative to approaches like GEM[0]?\n\n* Did the authors consider Piggyback like network for the remaining tasks as well?\n\n====================\n\nCertain important citations seem to be missing:\n\n* Works like GEM[0] and AGEM[1] fix the problem of \"unidirectional\" transfer of knowledge to some extent.\n\n====================\n\nSome minor corrections for the updated version:\n\n* Typos: eg \"catastropihc\", \n* In the section on Large-scale training, the STL model uses 100 times more params and not 10 times.\n\n====================\n\nReferences:\n\n[0]: GEM: https://arxiv.org/abs/1706.08840\n[1]: A-GEM: https://openreview.net/pdf?id=Hkf2_sC5FX\n[2]: EWC: https://arxiv.org/pdf/1612.00796.pdf\n\n"}