{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper claims to do imitation learning without expert demonstration using trajectories that are generated by suboptimal policies from other tasks.\n\nThe point of having an expert demonstrator is to help narrow the search for an optimal policy.  By taking the expert demonstration knowledge out of learning, to me, this is not retaining the benefit of imitation learning.  Thus, the paper is not about imitation learning, but rather about an optimization method that reuses data generated from multiple tasks.   Reusing trajectory data generated from multiple tasks to learn a policy of another task is not a novel idea.  If we could save all of the data regardless of whether if an optimal policy generates them or not, why not use them?  Less useful data may still contain useful information.  The better question is how to use them to learn policy efficiently.  If the motivation is to use trajectories from suboptimal policies from other tasks without expert knowledge, then I fail to see the motivation and the novelty of this paper. \n\nThe paper claims that the methodology self-supervises each action taken, judging how good it is for reaching a goal in the future without learning Q-values.  However, this was not realized.  The methodology gathers all trajectories that reach a goal into a set, and use behaviour cloning on the data of the set to learn a policy.  The sampled trajectories in the set could be suboptimal for reaching a goal, and there\u2019s little evidence that optimizing J_GCSL(\\pi) will learn an optimal policy based on these data.  Optimizing objective J_GCSL(\\pi) also does not take the long term effect of actions into account.  The gathering of trajectories and identifying the trajectory as goal-reaching is already a costly step, where no learning happens.  RL, on the other hand, would gather the data incrementally, learn, and act right away.  Also, it seems that the algorithm would require human knowledge to discern a trajectory as goal-reaching or not, which is contrary to self-supervision.  "}