{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a modification of the S4L method [1], combining self-supervised learning and semi-supervised learning. S4L introduces trains the network jointly using the cross-entropy loss on training data and an auxiliary task on all (labeled and unlabeled) data. The auxiliary task is to predict the rotation angle of a randomly rotated image. CRAP modifies S4L by conditioning the predictor of the rotation angle on the output of the classifier. \n\nFor me the main strength of this paper is the empirical evaluation The authors test CRAP on a wide range of image classification datasets and obtain strong results. The method is also quite different from other successful semi-supervised learning methods.\n\nThe main weakness for me is that the modifications from S4L seem to be fairly minor, and they are not very well motivated. For example, when I first read the method section, I thought that the idea of making the rotation angle prediction class conditional was to make sure that the classifier receives gradient both on the labeled and unlabeled data. However, in the best model, CRAP+, there is a separate classifier that is used to make predictions, and that doesn\u2019t get gradient from the unlabeled data.\n\nIf I understand correctly, the main differences between CRAP (or CRAP+) and S4L then is a slight modification of the auxiliary task (described in Section 4.2) and a modification of the architecture of the rotation predictor. In other words, CRAP is the same as S4L, but with a different auxiliary task and architecture. Given the large empirical improvement over S4L, I don\u2019t think the lack of differences is a huge issue. However, the current presentation doesn\u2019t provide a good motivation for the introduced changes, and the presentation can be simplified if CRAP is indeed a special case of S4L. I encourage the authors to comment on the differences between CRAP / CRAP+ and S4L and the motivation for introducing these changes in the rebuttal.\n\nBelow I describe some other, more minor issues and questions I have.\n\n1. The experiments are generally solid, and the results are good. However, CRAP introduces changes in architecture compared to S4L, and the other methods it\u2019s being compared against. Did the authors try to match the number of parameters for CRAP and S4L, or just tried to optimize the performance for both methods?\n\n2. As a suggestion, I would report results for CRAP+ on all datasets, and had a comparison between CRAP and S4L as a part of ablation study. I think this presentation would be more clear, as readers mostly care about the best performing version of the method. I don\u2019t insist on this change.\n\n3. For MixMatch [2], what results are being reported? In [2] the results on CIFAR-10 with 4k labels seem to be better than what\u2019s reported in the paper. \n\n4. The authors seem to provide a wrong citation for the fast-SWA method in the footnote on page 7, which should be [3]. The authors could also include the results of [3] for 4k labeled data in Figure 2 or Table 9.\n\n5. In section 5.2 fairly surprisingly S4L performs poorly on CIFAR and SVHN. It often performs worse than just training on the labeled data. CRAP outperforms S4L and other baselines by a large margin. At the same time, on ImageNet the difference is much smaller. What is the reason for that? Could it be that the hyper-parameters for S4L were not tuned well enough on other datasets?\n\n6. Overall, the method feels somewhat ad hoc. Can the idea be extended to other types of auxiliary tasks? What other tasks could we use? Can this method be extended to other types of data? \n\n7. GAN-based method were very popular in deep SSL before consistency-based methods. There the discriminator is a classifier that is trained for classifying labeled data bit also for an auxiliary task of discriminating between real and generated data on all (labeled + unlabeled data). [5] was one of the first papers to implement this and [4] achieved strong results. I believe CRAP and S4L are related to GAN-based methods in the sense that both  types of methods train representations for the classifier using an auxiliary task. So, it would be good to add a brief discussion of GAN-based methods to the related work.\n\n*Conclusion*\n\nFor me this paper is borderline. While the results are good, the changes compared to S4L seem minor, and, more importantly, not motivated very well. I encourage the authors to comment on the differences between CRAP and S4L in the rebuttal, and the motivation for these changes, and I am willing to update my score based on the response.\n\n[1] S4L: Self-Supervised Semi-Supervised Learning\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer\n\n\n[2] MixMatch: A Holistic Approach to Semi-Supervised Learning\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel\n\n\n[3] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nBen Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson\n\n[4] Good Semi-supervised Learning that Requires a Bad GAN\nZihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Ruslan Salakhutdinov\n\n[5] Improved Techniques for Training GANs\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen\n"}