{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# 1. Summary\nThe authors introduce a new pre-training procedure for image-text representations. The idea is to train the model on a huge collection of different image-text datasets and the use the model for downstream tasks. The difference between the proposal wrt the concurrent work is that conditioned masking is used: (i) Masked Language Modeling (MLM) conditioned on image; (ii) Masked Region Modeling (MRM) conditioned on text; and (iii) joint Image-Text Matching (ITM).\n\nI am on the fence for this paper given the balance between strengths and  weaknesses listed below. I am conservative here and decide for weak reject; but I am open for discussion, if the authors answer to my concerns detailed below. \n\nStrengths:\n* State-of-the-art results on several downstream vision-language tasks\n* Empirical work to investigate different ways to perform conditioned masking \n      \nWeaknesses:\n* Some parts of the method needs clarification (see point 2 below) to better understand the details and practical advantages of the method. \n* Limited novelty: the paper is an extension of BERT to the visual domain (see point 3 below)\n     \n      \n# 2. Clarity and Motivation\nThe paper reads quite well, although some points need to be improved:\n* \"Compared with LXMERT (Tan & Bansal, 2019) and ViLBERT (Lu et al., 2019) that use two streams (one Transformer for each modality), our UNITER model can learn joint contextualized ...\", why is this an advantage? Using two streams might also lead to learning context? Maybe an example can clarify my question.\n* End of Sec. 3.1 (and paragraph in Sec. 3.2): not clear how the model is training for ITM. What's the input and output? Why do you need a new symbol [CLS]?\n* Sec. 3.2 ITM: \"an additional special token [CLS] is fed into our model, which indicates the fused representation of both modalities\" - This is not clear. Why this special token is needed? Why is not needed in the MLM and MRM?\n* \"The scoring function is denoted as s\" -> please indicate in the main text what function you used\n* MRFM and MRC are clear, however the intuition of MRC-kl is missing. Why is this needed? What does it mean in practice to minimize such divergence (provide practical example)?\n* Combination of tasks (MLM + ITM + MRC-kl + MRFR) -> it is not clear how this is done in practice. Is the loss function composed (summed)? Within the mini-batch, the method randomly chooses which operation to do (e.g., MLM) for each sample? This should be clarified in the main text of the paper.\n  \n\n# 3. Novelty\nThe novelty of the paper is quite limited since it is an extension of BERT to the visual domain. The authors propose an empirical analysis of different ways to mask the visual input, however this might not be a substantial extension of previous work. In fact, recently there are many other papers (ViLBERT, VisualBERT, LXBERT, ...) working on similar topic with small differences. What it is missing in this paper is an understanding and intuition on the reasons why the conditioned masking idea should be better than the other visual masking ideas proposed in previous work.\n\n\n# 4. Experimentation\nThe main advantage of this paper relies on the extensive experimental analysis done on many challenging datasets reaching the state of the art on several downstream tasks.\nThe evaluation on both pre-training tasks and downstream tasks show that the method is working well in practice."}