{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a novel method for image-text representations called UNITER. The proposed method has been subsequently tested in many downstream tasks. A detailed ablation study helps to understand the role of each pretrained task in the proposed model.\n\nAlthough the empirical results are nice, performing the intensive set of experiments on many different tasks is definitely time-consuming and needs a lot of engineering efforts, the technical contribution does not seem significant to me. The paper modifies an existing pre-training procedure by conditional masking (Section 2). I agree this is well-motivated but it has little novelty and a similar idea is there in VQA (See \u201cDynamic fusion with intra and inter-modality attention flow for visual question answering\u201d). MLM and MRM are not new training procedure either, they are basically extending the BERT\u2019s training procedure with the consideration of multiple modalities.\n\nI have some questions for the authors:\n(1) What are the advantages of using single-stream transformer over two-stream transformer (page 2). I guess it leads to fewer parameters but I don\u2019t think this is a big problem.\n(2) Some visualization of attention weights would be helpful. \nMinor\n\u2022\tIn \u201cm \\e N^M\u201d (equation 1), what is N and M? \n"}