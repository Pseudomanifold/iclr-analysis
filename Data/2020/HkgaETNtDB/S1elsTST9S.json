{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces a new regularization technique \u201cmixout\u201d for fine-tuning BERT. Mixout technique mixes the parameters of two models \u2014 the pretrained model and the dropout model. Because it keeps pretrained model parameters in consideration all the time, it effectively prevent catastrophic forgetting. Empirical results show that mixout can stabilize fine-tuning BERT on tasks with small training examples (which has been shown to be difficult).\n\nI\u2019d like to accept this paper based on the extensive and detailed experiments and promising results. For example, the theory has been supported by experiment findings on handwriting dataset (computer vision) while the main contribution is on natural language tasks. The authors not only conducted experiments on tasks with less examples (the main focus of this paper) but also on a task with sufficient training examples. For each model regularization configuration, 20 random starts are used to report mean and best performance. This not only makes the results more reliable but also provides deeper insights.\n\nMinor clarification questions:\n\nFor 20 random restarts, are they the same across regularization setups, i.e., for each dot in Figure 3, is there an orange dot that has the same initialization?\n\nWhat will the extreme case, mixout(w_pre, 1.0), behave? According to Figure 1, it would always use w_pre and end up not learning on a target task. If so, would it introduce a cliff in Fig 4? \n"}