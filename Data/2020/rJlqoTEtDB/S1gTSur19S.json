{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes, analyzes, and empirically evaluates PowerSGD (and a version with momentum), a simple adjustment to standard SGD algorithms that alleviates issues caused by poorly scaled gradients in SGD. The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms.\n\nOverall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side. Specifically:\n\u2022\u00a0The convergence analysis assumes a batch size equal to T, the number of steps of PowerSGD. This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading. If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).\n\u2022\u00a0In Remark 3.4.3, the authors claim that another point of difference between their results and Yan et al.'s (2018) is that Yan et al. assume bounded gradients, an assumption that is not satisfied for e.g., mean squared error (MSE). But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem\nmin_\u03b2 (1/N)\u03a3_n (y_n \u2013 x_n \u2022\u00a0\u03b2)^2\nwith the minibatch gradient estimator computed over randomly chosen minibatches B:\n\\hat g = (1/|B|) \u03a3_{n \\in B} x_n (y_n \u2013 x_n \u2022\u00a0\u03b2).\nAs the norm of \u03b2 goes to infinity, so does the expected norm of the error of \\hat g. I'm not saying this is a particularly big \ndeal, just that it's not an improvement over Yan et al.'s result.\n\nThat aside, this seems like good work that could have a significant impact on practice.\n\nA couple of other minor points:\n\u2022\u00a0It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD. It would be nice to see some discussion (or at least speculation) on why that is.\n\u2022\u00a0Not all of the arrows in Figure 1 are pointing to the right lines.\n\u2022\u00a0In the abstract, it might be good to clarify that the exponentiation is elementwise."}