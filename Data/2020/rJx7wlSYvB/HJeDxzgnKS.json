{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed differentiable Bayesian neural networks (DBNN) to speed up inference in an online setting. The idea is to combine online vector quantization with Bayesian neural networks (BNN), so that only incremental computation is needed for a new data point.\n\nIt seems the idea of online codevector histogram, or online vector histogram, is not new, which is not surprising since the original codevector histogram work is from decades ago (1982). A quick search shows several related work in this domain, e.g., \u2018An Online Incremental Learning Vector Quantization\u2019. It would be better if the authors could clarify the differences if they want to claim the contribution.\n\nOne of my major concerns is the fairness in terms of comparison to BNN approaches. For example, in Table 2, BNN (shown as MU) is significantly slower than DU/DBNN and DNN. The authors mentioned that this is because MU predicts results for 10 batches of size 3, and therefore 30 times slower. Since DU and DBNN also uses MC-dropout, why is this not an issue for DU/DBNN. Such large overhead is also inconsistent with the description in Section 5.3, saying that the \u2018overhead of sampling weights is negligible\u2019. Could the authors elaborate on this?\n\nA related problem for clarification: it is mentioned before Section 5.1 that 30 samples are drawn from the BNN\u2019s posterior. Do you mean 30 feedforward passes of MC-dropout, as done in the MC-dropout paper? Also, the authors should have made it clear that they are using MC-dropout as a BNN.\n\nIn the introduction, the authors motivate the proposed DBNN by saying that BNN needs dozens of samples from weight distributions and therefore is rather inefficient. However, there are a lot of modern BNN that are both sampling-free and differentiable. For example, natural-parameter networks (NPN) parameterize both the weight and neuron distributions using natural parameters. Even the earlier work, probabilistic BP (PBP), as cited in the current paper, also counts as sampling-free. The claim of \u2018being differentiable\u2019 without acknowledging prior work is rather misleading.\n\nThe point above is also related to the MU baseline in Table 2. The issue of needing 30 passes can be readily resolved if modern BNN such as NPN (or PBP), which takes only one pass, is used.\n\nThe organization could be improved to make the paper more readable. It would be better if the problem setting of online inference is introduced at the beginning, followed by the overview of DBNN and then the OCH details. Otherwise, it is not clear what the focus of DBNN is, until Section 4.\n\nMinor:\n\nIt might be better to denote the weight as \u2018w\u2019 rather than \u2018x\u2019, to avoid confusion.\n"}