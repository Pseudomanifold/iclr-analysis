{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method for training Bayesian neural nets on a stream of non-stationary data. The authors propose a new way to approximate the variational distribution in the case where a memory (of past data) is used. The authors also use two forgetting mechanisms that allow the model to adapt to the changes in the data distribution.\n\nNote that contrary to many recent works on continual learning that focus on not forgetting previous distributions, the goal here is to predict the current data.\n\nI found this paper to be well written, the contribution is clear and the background material is well explained. The task is also important. I am not sure about the significance of the proposed approach. The motivation for providing a new memory update that takes into account the approximation error (from the variational approximation) is clear but I was not sure of the efficacy of the approach. Experimental results (Table 1) also show modest gains.  Experimental results using the adaptive method (Figure 5 & 6) are somewhat similar (e.g., why are other benchmarks not studied? gains are also modest).\n\n- In Table 1, what does \"bolding\" indicate?\n- In terms of other benchmarks, I was curious as to why you did not compare to \"The Population Posterior and Bayesian Modeling on Streams\" NIPS'15. As far as I understand this is also a Bayesian method that can accommodate non-stationary streams.\n- VLC is proposed for the case of continual learning without forgetting.  It seems like you could study your method in such a setting (e.g., using online multi-task learning).\n- In Section 4 (p.5 2nd paragraph), you mention that you assume that task boundaries are given. This seems to be a strong assumption and a significant change wrt to previous work. Could you perhaps add a practical example of this setup?\n- I think it would be worthwhile to provide the full proposed algorithm somewhere in the paper and also discuss its computational complexity explicitly."}