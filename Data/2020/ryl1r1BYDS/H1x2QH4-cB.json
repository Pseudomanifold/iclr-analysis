{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The main idea of this paper is to solve multi-agent reinforcement learning problem in dominance solvable games. The paper reviewed general multi-agent reinforcement learning and general norm-form game in game theory. The authors aim to recover multi-agent policies through independent MARL in norm-form dominance-solvable games. The paper states that one of solution concepts of dominance-solvable games is iterated dominance solution, which is different from Nash Equilibrium and may be more suitable under certain scenarios. Furthermore, the paper considers two common RL methods for control and learning policy: REINFORCE and Monte-Carlo policy iteration. The main contribution of the paper is to prove that both REINFORCE in binary action case and Monte-Carlo algorithms find the agents\u2019 policies converging to the iterated dominance solution. The interesting aspect of this paper is that iterated dominance solution based reward scheme can guarantee convergence to the desired agents policies at a cheaper cost in practical principal-agent problems. In appendix, the paper extended its conclusion to Markov games and three possible action cases. To the current status of the paper, I have a few concerns below.\n\n1.\tIt takes too much space for preliminary work and basic concepts, in Sec 1.1 (preliminary) and Sec 2 (MA-RL and Dominance-Solvable Games).\n2.\tThe notations are inconsistent and unnecessarily complicated. For example, for agent i \u201cits possible actions are the strategies in S_i\u201d (section 2); any action \u201ca \\in S_i\u201d (section 2,1); for agent i \u201cfor all s_i \\in S_l\u201d (Algorithm 1 line 2). It can be consistent to use the same notation to describe the same term. Moreover, \u201ca score per action, x_1, \u2026, x_{m_i}\u201d and \u201ceach agent starts with initial logits for x_1, \u2026, x_n\u201d. Formally, the corner mark in the same location should represent the uniform meaning. \n3.\tTypos: lemma 3.1 proof \u201cg = \u2026 (r_{s_h}-r_{s_h})\u201d should be \u201cg = \u2026 (r_{s_h}-r_{s_l})\u201d; above section 3.2 \u201cour proof of Theorem 3.1\u201d, should be \u201cLemma 3.1\u201d.\n"}