{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose AtomNAS, a neural architecture search (NAS) algorithm, with a new fine-grained search space and the dynamic network shrinkage method. The searched models achieve a new state-of-the-art result on the ImageNet classification task for mobile setting with restricted FLOPs. \n\n- The proposed method is novel and technically sound. In addition, the experimental results on ImageNet are impressive. However, the experiment section is not solid enough.\n\n- The authors do not include the searching cost and inference latency in Table 1. Different NAS papers have different objectives and searching cost. For instance, ProxylessNAS (Cai et al., 2019) and DenseNAS (Fang et al., 2019) focus on searching cost. They require only 200 and 92 GPU hours (with TITAN XP). However, the proposed AtomNAS takes 32 * 25.5 = 816 GPU hours (with V100). The authors only point out that DenseNAS uses more parameters.  It would be better if the authors can make the comparison more transparent.   \n\n- I wonder when given the same searching budgets as ProxylessNAS and DenseNAS, how well AtomNAS can perform.\n\n- The authors use only one dataset: ImageNet. I would like to see results on some other datasets or tasks. For instance, the authors may apply AtomNAS to other image classification datasets or finetuning the pre-trained models on object detection or semantic segmentation tasks.\n\n- In general, the paper is well written and easy to follow. I would encourage the authors to add legends to Figure 5 and Figure 6. While the meaning of each color is explained in the caption, it is not straight forward.\n\nIn short, the proposed method is interesting and the results on ImageNet are impressive, I weakly accept this paper and hope that the authors can make the experiment section more solid in a revised version.\n"}