{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This basic idea of this paper is to decompose the common building blocks of large network into atomic blocks, which equips NAS with more fine-grained search space. What's more, the authors propose a resource-aware search to reduce the computation and dynamically shrinkage the model to accelerate the learning. Retraining the final network is no longer needed. They achieve state of art on ImageNet under several complexity constraints.\n\nPros:\nNovel idea: the insight of this paper is that \"larger network building blocks can be represented by an ensemble of atomic blocks\". With this in hand, it can search the exact channel number through channel selection (i.e. atomic block selection, according to my understanding).\n\nEfficiency: Resource-aware selection and dynamical shrinkage of the model also make it more efficient in inference and training. \n\nCons:\nIt would be better if the author could provide some comparison on GPU time. Since FLOPs is only an indirect metric for speed evaluation. \n\nThe biggest problem of this paper is that experiment is not enough. It would be more convincing if experiments on other popular datasets (CIFAR10/100 etc.) or tasks (object detection, semantic segmentation, etc.) are implemented.\n\nConclusion:\nThis is an interesting paper with novel idea and efficient implementation. However, more experiments are needed to validate the utility of the proposed method. "}