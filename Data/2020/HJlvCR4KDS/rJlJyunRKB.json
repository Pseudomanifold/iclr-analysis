{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Interesting results and analysis but lack of novelty and details. \n\nThis paper proposed a novel text explanation method which extracts keywords that are essential to infer an answer from question. The authors proposed VCBS based on CBS and use Natural language inference to identify the entailments when the answer is yes or no. Experiment results show better mean opinion score 100 random samples results compared with the previous method and better VQA binary classifications when flipping based on the NLI results. \n\nDifferent from [Park et. al. 2018], who collected the explanations, this paper directly use the coco captions dataset as the source for the explanations. One of my major concern about this paper is it actually generate relevant captions with respect to the question and answer instead of real explanations. It's true that correlated caption can sometimes serve as the explanation when they cover the same concept coincidently but, we can not use these captions to explain the reasoning process of VQA. \n\nThe technique novelty of the proposed paper is also limited, the major novelty is the VCBS, which seems very similar to CBS. The only difference is VCBS adds relaxed parameters, which seems no technique novelty. Most annotations in Algorithms 1 is also not explained, making the readers hard to follow the actual content. The proposed model, although tied the vqa words with the explanation words, it suffers the same problems as PJ-X model, which didn't consider the VQA attention at all. \n\nThe experiment is also weak, considering the results is conduct on 100 samples, there might be significant variance. It's interesting the compared approach is learned based on a different dataset, which makes the results harder to compare. The NLI model results are interesting but for a more fair comparison, I would expect the proposed method compare with a model trained with VQA and coco caption dataset, such as VQA-E. \n\n"}