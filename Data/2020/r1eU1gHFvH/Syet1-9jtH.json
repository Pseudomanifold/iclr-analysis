{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "I have a lot of questions about the data used in the experiments. They are created according to the method explained in \u201cData design\u201d (p.2). It is also summarized in the last paragraph of the first section as follows: \u201dthere are 1/10 input bits that are always 1 for each class and these are the invariant bits, the 0s of each prototype are then filled in with a random mix of 1 and 0 of a known weight\u201d. What is the intention behind this way of creating data? How general are the data created in this way as well as the analyses based on them? It seems to me that the data and thus the analyses lack the generality needed for the purpose of understanding behaviors of neural networks on real tasks/data. \n\nThe same is true for \u201cNeural network design\u201d (p.3), in which 13 experiments conducted in this study are explained. I think their explanations are too condensed; each explanation is very short and it is hard to understand the motivation and purpose of each experiment, i.e., what is the hypothesis to be verified and in what way it is verified? \n\nIn Experiment-12, MNIST is used as data unlike other experiments, and they are modified as \u201cwith added 20 pixel invariants\u201d. What is the purpose of this modification? There is a statement in a footnote of p.5 \u201cNo LCs were seen in the standard MNIST runs\u201d, which agrees with the above concern about the lack of generality.\n\nAdditionally, I do not understand the statement in p.5 \u201cIncreasing the difficulty of the problem (by increasing n_x, \u2026\u201d. Why does the use of more training data make the problem harder? It should usually be the opposite; the smaller, the harder. \n"}