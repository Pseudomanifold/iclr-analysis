{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a procedure to control the output of neural decoder generation architectures via weak supervision. The approach employs structured latent variables as control states that can be constrained during training via posterior regularization, thereby encouraging the network to learn problem-specific knowledge without adversely affecting the expressiveness of the underlying architecture.  The paper presents evaluations on two database-to-text generation problems as well as part-of-speech induction, with comparisons against neural sequence prediction baselines.\n\nAs discussed, a consequence of the end-to-end nature of training standard neural decoder architectures is that it is generally difficult to control their output. Several methods have been proposed to address this either by modifying the architecture to impose a specific inductive bias, or by providing the network with auxiliary supervision. In this case, the proposed approach introduces latent structure into the architecture in the form of control states and then constraints the posterior over these latent states with domain-specific distributions. The results demonstrate that this approach outperforms several baseline methods in terms of several automatic metrics on two text generation datasets. Further, ablations show the advantage of posterior regularization. On part-of-speech induction, the method performs similarly to a state-of-the-art baseline in terms of perplexity (slightly worse, actually).\n\nThe problem is interesting and relevant. However, it relies upon hand-crafted constraints over the latent control states that would be difficult to design in many cases, an issue that the paper acknowledges. This, together with the fact that these constraints are task-specific, takes away from the generalizability of the underyling neural decoder architectures.\n\nThe discussion is opaque at times, making the paper a bit of a chore to read. There are also numerous typos/grammatical errors.\n\nADDITIONAL COMMENTS/QUESTIONS\n\n* The intro would benefit from a concrete example of the control one might like to impose on a network's output.\n\n* The database-to-text task is not terribly compelling as the constraints are encouraging the verbatim use of class labels.\n\n* The database-to-text evaluation relies upon automatic metrics. While these are commonly used for generation, it isn't obvious that they are the right metrics by which to assess the ability to control the network's output.\n\n* The description of the Ours+Force ablation is vague. What hard constraints were used and how?\n\n* The synthetic experiment is vague and doesn't contribute much.\n\n* Is there an L missing in the final PRLBO equation?"}