{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a method to instill prior knowledge into natural language processing models by introducing a latent variable which the authors call \u2018discrete control states\u2019 that has semantic meaning. To guarantee this, the authors extend the idea of an approximate Bayesian inference approach with posterior regularisation. Posterior regularisation allows the authors to control the meaning of the discrete control states. In section 5, the authors give two examples of posterior constraints that they used in different NLP tasks. In experiments, the authors demonstrate that their model with posterior regularisation consistently outperforms the baseline models. Some samples, drawn from the trained model are presented in the paper.\n\nI have no experience in NLP, and thus cannot properly judge the significance of the results section of this work. In my comments, I will therefore focus on the technical aspects of this work. Overall, I found the paper difficult to read. This is probably owed to the fact that this work encompasses amortized variational inference, conditional random fields, and posterior regularisation in about two pages of writing. It makes me wonder if some of the aspects of this work could be simplified in order to make it more concise.\n\nFurther comments\n- Section 3, last equation. It is not clear where the equality comes from. The authors claim that log p(y|x)  - KL(q(z|x,y)|p(z|x,y)) = -KL(q(z|x,y)|p(y,z|x), where the left-hand side of the equation comes from the PRLBO (as defined in the second equation in that section), and the right-hand side of the equation is the ELBO. I believe this equation should have an inequality with, i.e. the authors are optimizing a lower bound on the PRLBO. Expanding on the steps from the second to the third equation might clarify things.\n- In section 4 the authors explain that they model the approximate posterior q(z|x,y) as a structured conditional random field. I understand that in this setting the partition function can be calculated using dynamic programming methods as described in a footnote (which I can only partially follow). This allows the authors to calculate marginals for the approximate posterior and hence allows the authors to calculate the entropy of q(z|x,y) and the PR term. What is not clear to me is how the authors deal with the first term in equation (1). Is z sampled? How is back-propagation performed in this case, as z is discrete.\n- Sec. 7 baseline models, Model Ours+Force: Could the authors explain how they enforce hard constraints on the posterior?\n- What is the significance of treating z as a stochastic variable? What would happen if you leave out the KL term in the ELBO? What would happen if you use a deterministic auto-encoder only with posterior regularisation?\n- By now, it kind of folk knowledge in the community that in VAEs with a strong enough decoder, the encoder will be ignored. Does you approach suffer from this problem? How can it be counteracted?\nThe authors mention computation \n\nMinor comments\n- Authors need to number their equations\n- The top line of the sum and product symbols in equations are not correctly printed on paper. I printed the paper two times, and both times this was the case.\n"}