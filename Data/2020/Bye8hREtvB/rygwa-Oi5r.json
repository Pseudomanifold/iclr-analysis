{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes to learn a latent space for the PixelCNN by first computing the Fisher score of the PixelCNN model and then projecting it onto a lower-dimensional space using a sparse random matrix.\n\nMy first concern about this work is its novelty. Defining a feature space using the Fisher kernel of a generative model is a very well-known idea, and there is a large body of work around that. As the paper points out, the problem with the Fisher score for the recent deep generative model architectures is that Fisher score operates in the parameter space and the deep models have a very large number of parameters. The paper proposes to get around this problem by projecting the Fisher score onto a lower-dimensional space using random matrices. But I am not convinced that this random projection\u00a0can learn useful representations, which brings me to my second concern about the\u00a0evaluation metric. The paper uses the activation of the last layers of the PixelCNN as a baseline, which I consider to be a very weak baseline. Each activation at spatial position (i,j) only depends on the previous pixels and I believe they are not in general good high-level representations. For the evaluation, the paper only considers the FID score on the interpolated images and reconstructions. There are much better ways to compare the quality of unsupervised representations such as their performance on classifying images with a linear classifier as done in [1]. The paper would improve by comparing the quality of its latent representations with the recent unsupervised/self-supervised learning methods such as [1,2].\n\n[1] Data-Efficient Image Recognition with Contrastive Predictive Coding\n[2] Learning deep representations by mutual information estimation and maximization"}