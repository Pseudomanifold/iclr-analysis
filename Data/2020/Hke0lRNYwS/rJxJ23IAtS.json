{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper studies and evaluates variations on attractor networks (which have recurrence and converge to a fixed point) trained end-to-end by SGD on image completion or super-resolution tasks. Several loss functions are discussed and compared.\n\nThe only quantitative results against published state-of-the-art results (on  the super-resolution task) show that the proposed approach wins on some measures and tasks and loses on others (in fact it wins 25% of the time, within a pool of 4 methods...). While the authors claim superior performance against other recent attractor networks, no quantitative comparison was found in the paper. There were quantitative comparisons against the \"convolutional denoising VAE\" but that model was not previously proposed by other authors to be competitive nor was it defined properly (and it is not clear from its name and the description whether this is really a VAE,  since VAEs are generative models and are not trained to denoise). Overall, I am concerned that the experimental  part of  this paper does not warrant the conclusions of superiority. That being said, I like the approach and I believe that these issues can be fixed but I'd like to see those revisions before accepting the paper.\n\nThe experiments on toy problems like the Bar Task are not necessary. Better use the place for real comparisons against published state-of-the-art benchmark baselines. Otherwise (like with your own implementation of the CD-VAE) it is plausible that the comparisons will be biased (one usually works harder on our own method than on improving someone else's architecture for our task). \n\nSimilarly, the experiments on MNIST are not very informative and you might want to push them to appendices. Also, a more interesting comparison would have been with the convolutional version, where convnets do a LOT better than 1.5% error.\n\nThe comparison against Liao et al is not fair because they don't use convolutions, which we know can make a huge difference.\n\nIn addition to the experimental results  issues, there are several places in the paper which are unclear or possibly wrong. See minor points below.\n\nOne theoretical issue which bothers me is with the \"loss\" L_{\\Delta E}. There is no guarantee that it is lower-bounded, i.e. it can diverge which would not be appropriate for sure.\n\nThe training procedure which ends up being used is TD(1) but that procedure (in the context of the setup of the paper) has not been explained at all. It is really necessary to clarify that.\n\nIt should be clarified that the parameters are tuned by gradient descent on the loss (using backprop through time), or not?\n\nMinor points\n\nThe last paragraph of section 2 seems strange to me. Many ANs are also EBMs so the critique of EBMs (as  if this did not apply to ANs) probably needs to be removed or reconsidered. Also the last sentence of the paragraph is clearly wrong,  since EBMs can easily handle inputs (by opposition to the outputs which are to be predicted or constructed), e.g. see conditional Boltzmann machines or the ANs in the style of Equilibrium Propagation - Scellier et al 2017 (which by the way should probably be mentioned in the review, along with follow-up work).\n\nThe 'bipartite' structure is not clear. What are the two \"parts\" when you have more than 2 layers? Clarify.\nNote that the proposed sweep is inefficient compared to the one proposed for Deep Boltzmann Machines (Salakhutdinov et al 2009) which alternates between odd and even layers. The latter is also more biologically plausible than a feedforward-feedback sweep (consider that the differences in update times of different layers are very different depending on their depth, with your proposed scheme).\n\nDefine CBAN.\n\nPlease clarify how many iterations are needed for convergence in the various experiments. My own experience with ANs suggests that convergence can be quite slow, which is problematic both in terms of computational cost (compared to standard feedforward methods with reasonable depth) and in terms of memory. Hence the sentence in the conclusion stating that the computational cost of CBANs is no greater than DNNs is not clearly true until you demonstrate those aspects quantitatively.\n\n\n"}