{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The authors present a method for learning Hamiltonian functions that govern a dynamical directly from observational data.  The basic approach uses three networks: 1) an inference network (I'm not clear why this is not just called an encoder), that maps past observations to a latent p,q space in a VAE-like fashion; 2) a Hamiltonian network that governs the time-evolution of the system over this latent state; and 3) a decoder network that outputs the observation from the latent state.  In addition to introducing this basic formalism, the \n\nComments: I have mixed opinions on this paper, though am leaning slightly toward acceptance.  The overall notion of learning a Hamiltonian network directly is a great one, though really this is due to the Hamiltonian Neural Networks paper of Greydanus et al., 2019.  Although the focus in that work is on applying learned Hamiltonian networks directly to physics-based data, they also have an encoder-decoder network just using a classical autoencoder instead of a VAE.  So my first impression is that the benefits of the proposed HGN over HNNs in Figure 6 is really just an artific of this replacement.\n\nPerhaps because the authors also felt this was a marginal contribution, the paper's ultimate value may prove to be in the consideration of such networks for the purposes of normalizing flow models.  This portion seemed a little bit underdeveloped in the paper, to be honest, but overall the idea of parameterizing a normalizing flow with a Hamiltonian dynamical system seems like a good one (e.g., allowing for easier large-timestep inference).  But on the flipside, it does seem like the presentation here is rather brief, i.e., just defining the ELBO without much context or detail, etc.\n\nThus, while I'm very much on the fence on this paper, I think the marginal improvement over HNNs via a better encoder/decoder model, plus the realization that these methods are a good fit for normalizing flow models, altogether put the paper slightly above bar for me."}