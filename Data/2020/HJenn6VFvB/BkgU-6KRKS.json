{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system's initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling. Finally, a decoder network can use the system's phase space location at any rollout step to generate images of the system. The HGN can further be modified, leading to a flow-based model, the Neural Hamiltonian Flow (NHF).\nThe authors evaluate the HGN on four simulated physical systems, showing substantial improvements over the competing Hamiltonian Neural Network (HNN). Lastly, the NHF is shown to be able to model complex densities, which can further be interpreted in terms of the kinetic and potential energy.\n\nThis paper is a rare treasure. It tackles a well-motivated problem and introduces a, to my knowledge, completely new framework for embedding Hamiltonian dynamics in a generative model. This is hugely inspiring! The paper is a joy to read and includes very informative figures providing a high-level understanding of the proposed models. Accept is a no-brainer.\n\nThat being said, I have a few questions and suggestions for improvements. My biggest complaint is the evaluation of the NHF model. I would have liked to see a comparison to a state-of-the-art flow-based model in terms of density modelling. The authors state that the NHF offers more expressiveness and computational benefits over standard flow-based models, but this is never shown. While I am willing to believe the claim, it is not intuitive to me, and I would have liked to see experimental verification of it.\n\nFigure 6 needs a bit of love. It is quite challenging to read. Larger font sizes, conversion to vector format, and more distinguishable colours will help a lot.\nAdditionally, I think it would be helpful to have the derivation of the ELBO in Eq. (4) written out, e.g. in the supplementary material.\n\nAdditional questions:\n- In the experimental section, I am not sure what is meant by the deterministic version of HGN. Which part if the model is deterministic?\n- On p 6, it is mentioned that the Euler integrator results in an increased variance of the learnt Hamiltonian and that this can be seen in Fig. 6. How exactly is this seen in the figure?\n- How many epochs were HNN and HGN trained for to produce table 1? How do the convergence rates look, and how long time did they take to train?\n\nMinor comments:\n- p 5: Reference to \"Salimans et al.\" is missing the year.\n- p 6: There is a hanging ')' after \"as shown in Fig. 6).\"\n- p 6: \"reversed it time\" -> \"reversed in time\"\n- In the reference for Glow, \"Durk P Kingma\" should be \"Diederik P. Kingma\".\n\n"}