{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This work proposes an activation function that contain parameters to be learned through training. The idea is to give the learning algorithm more \"freedom\" to choose a good activation function, and hopefully better performance can be achieved.\n\nThe paper is well written, and the experiment results look reasonable. However, there are several key issues.\n\n1) as the authors stated, a \"good\" activation function should maintain the universal approximation property of the neural network. This seems not discussed for the PADE activation function.  Does (1) satisfy the conditions (i)-(v) listed in table I? Is there a rigorous proof? Table I seems to claim that the PADE based neural network satisfies (i), but there is no formal proof.\n\n2)  In order to avoid poles, the activation function used in this work is (2). How well can (2) approximate (1)? What is the potential loss? Perhaps there should be more discussion on this - preferably some theoretical supports.\n\nOverall, the reviewer feels that this paper starts with an interesting idea, but the developments on the theoretical side is a bit thin."}