{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper mostly reads well. It proposes to use statistics from previous activations to compute and adaptive scaling of the loss such that the amount of underflow is minimized. The scaling is defined per layer. Experiments are carried for various model sizes and datasets. \n\nIf anything I think the paper can do a better job at centralizing (maybe in an appendix) the gritty details (e.g. how the stats are computed etc). Unfortunately, the best way of doing this might be in the form of code or maybe pseudocode, but being quite explicit in all technical details. \n\nRight now this is mentioned in the text (same way as batch norm stats if I understood correctly, based on the current minibatch). Though is not clear how the variance on w is treated. How is the variance on dirac delta (backpropagated error) is converted into a scalar (that will be for the entire loss)."}