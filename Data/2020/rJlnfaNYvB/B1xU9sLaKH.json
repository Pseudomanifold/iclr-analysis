{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a method to train models in FP16 precision. The authors show that the key reason of training performance drop is the overflow or underflow of back propagation information. Instead of using a fixed value or dynamic value proposed by a previous work, this paper adopts a more elaborate way to minimize underflow\nin every layer simultaneously and automatically based on the current layer statistics. Experiment results on CIFAR10, ImageNet and Object Detection models are conducted to demonstrate the effectiveness of the proposed method.\n\nThere are some concerns about this paper:\n1. This paper tries to solve a very practical problem which is good, however, the stability of this method which is very important for real applications remains unclear. More networks such as VGG/ResNet/depthwise-conv based networks, more initialization methods (such as gaussian, xavier, kaiming), w/o bn layers,  and more tasks such as segmentation and detection with different batch sizes are strongly recommended to make this work more solid.\n2. In the experiments, it seems that dynamic loss scaling method works well too except on the configuration of SSD batchsize=8. Why dynamic loss scaling fails on this case? More detailed analysis are recommended to show the advantage of the proposed method.\n3. In  many experiments, it seems adaptive loss scaling with FP16 is even better than FP32, is this stable? Could we further improve the FP32 results if using dynamic / adaptive loss scaling? "}