{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper analyses a dataset of representations in the CA1 region of the hippocampus of a rat conducting a spatial plus maze task that switches between allocentric and egocentric versions. In the allocentric version of the task, the rat must always go from north or south arms to the west arm to receive a reward. In the egocentric version the rat must always turn right, regardless of whether they are starting in the north or south arm. These two versions of the task are switched after some period of time.\n\nThe researchers conducted demixed principal components analysis (dPCA) on the data, then compared the activity of the components to aspects of a tabular Q-learner. They argue that some of the components match variables used by the Q-learner. Following this, they examined the ability of both the tabular Q-learner and a deep Q-network to perform this task, and compared their performance to the rat. They find that the rat is better than the Q-learners at continually updating from the egocentric to allocentric task, and vice-versa.\n\nThe goals of this paper are fantastic. I really like the attempt to link hippocampal activity with RL representations. But, this study is very muddled, and there are very serious problems with the paper that render it inappropriate for acceptance to ICLR. The five most major issues are:\n\n(1) The choice of the demixing categories is missing a crucial category: space! Given the importance of the hippocampus for spatial representations, surely a big component of the variance can be explained by the animal's location in space. Why not include this? It might in fact be that some of the \"time\" components are just reflecting the usual spatial location of the animal during different components of the task.\n\n(2) The identification of components from the neural data with things like reward prediction errors, eligibility traces, etc., is all done in a qualitative manner with no statistical controls. This lack of quantitative assessment for some of the key claims in the paper is just not okay for an academic paper. Moreover, even the qualitative claims are underwhelming. The authors' claim, for example, that time component #1 is an eligibility trace is a real stretch, in my opinion (see more below in point 3).\n\n(3) The comparison between the animal data and the Q-learner is also done in a qualitative fashion that was extremely underwhelming. Fig 4 A & B were the most egregious. Are the authors really attempting to claim that the curves in 4A are clearly related to the curves in 4B? That's a real stretch, and to provide no quantitative assessment of this claim renders it completely unconvincing.\n\n(4) The fact that model-free reinforcement learning algorithms cannot adapt in changing environments/tasks has been known for a long time. As such, the result showing that the Q-learners cannot switch easily between the tasks is not novel. See, for example, this paper: https://www.sciencedirect.com/science/article/pii/S0896627313008052\n\n(5) Even if we accept the central claims from this paper, there is very little provided for machine learning researchers at ICLR to benefit from. What about the hippocampal representations makes the rats better at continual learning? What inductive biases or memory mechanisms might we glean from this work? Nothing like that is provided. As it stands, even being charitable, this paper really only speaks to a neuroscience audience, since even the Q-learning components are used only to understand the neuroscience data, not to think about how this could inform new ML systems or theories.\n\nIn addition to these problems, there are several small ones:\n\n- Was only one animal included in this analysis? That's never stated, but Figs. 5 & 7 seem to suggest that. Not only does this 100% need to be stated, but it's very problematic from a generality standpoint.\n\n- What type of recordings were these? How were individual cells identified (e.g. spike sorting)?\n\n- How was the Deep Q-net trained exactly? The authors say it was trained on the first 250 trials from the animal, but that's confusing? Was it not trained to perform the task itself? Also, where is all the info on memory buffers, hyperparameters, etc. There is no way to reproduce these simulations given the lack of detail here.\n\n- Some of the plots are confusing and hard to follow. For example, in figure 7 B and C, what determines the X-axis? What should I be looking for in the curves? "}