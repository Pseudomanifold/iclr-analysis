{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the privacy issue of widely used neural language models in the current literature. The authors consider the privacy implication phenomena of two model snapshots before and after an update. The updating setting considered in this paper is kind of interesting. However, the contribution of the current paper is not strong enough and there are many unclear experimental settings in the current paper.\n\nAccording to the current paper, the privacy implication seems to be defined in terms of general sequences in training datasets. If this is the case, I don\u2019t think such privacy implication is meaningful because our language models should memorize some general information to achieve their tasks. \n\nThere are some unclear settings in the experiments:\n1.In the experiments, why there are only 20000 vocabulary size for Wikitext-103 datasets?\n2.It is unclear how to construct canary phrases. \n3.After constructing the new dataset, the model is retrained or trained in the online way?\n4.Since the results for Wikitext-103 is not finished, the authors should remove the results on this dataset.\n5.What is the perplexity of the trained models?\n6.How to choose initial sequence in real data experiments?\n7.When you applying DP mechanism, how did you define the neighboring datasets, and how did you implement it (what is the clipping level, how did you calculate privacy loss for language models)?\n8.$\\epsilon=111$ seems that the model will provide no privacy guarantee according to the definition of differential privacy?"}