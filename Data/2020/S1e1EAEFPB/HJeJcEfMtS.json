{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nIn this study, the authors propose a new architecture for visualizing the latent space of a network. This architecture is achieved by appending a secondary decoder 'head' to visualize the latent space by reconstructing the inputs.\n\nIn summary, I found the paper to provide a simple architectural change for attempting to address this issue of network interpretability and improve transfer learning. The results all appear sensible and expected, but the experiments are somewhat weak. My primary concerns are:\n\n1. The network architecture described is not terribly novel as many papers have explored pairing an auto-encoder with a classification task. Thus, I don't find the methods to be much of a contribution. The main contribution of the paper is thus in the interpretation and analysis of the method.\n\n2. The results on network interpretability are all qualitative.\nThat is, I must make a judgement on the resulting reconstructed image. I find this unsatisfactory for providing any quantitative assessment of the performance of the method. In particular, I would want to see a quantitative assessment of this method and compare it with other network introspection techniques. Currently, there is no benchmark nor any other methods to compare against.\n\n3. The results on transfer learning are quantitative but weak.\nIt is not clear to me how other simple transfer learning methods may perform on the task presented. That is, I do not know how hard the actual task is. (Would retraining a logistic classifier  with SIFT features work just as well?) I would expect to see far more experimental studies to justify these claims. Additionally, I would expect to see comparisons with other transfer learning techniques to see how well this method fares.\n\nMajor Comments:\n\n- Why do the authors allow for the decoder to regularize the latent space? Why not just stop the gradients at the latent space to provide a microscope to visualize the latent space without effecting it?\n\n- This reference appears to be in the same spirit as this paper and probably should be cited.\nAlain, Guillaume, and Yoshua Bengio.\n\"Understanding intermediate layers using linear classifier probes.\"\narXiv preprint arXiv:1610.01644 (2016).\n\n\n- Figure 2, 3 and 4 provide very nice qualitative demonstrations of the reconstruction providing insight into how an image was misclassified, however some issues remain:\n  1) A human must interpret the reconstruction. This is a subjective process and is not systematic.\n  2) Can you see examples in cross-validated test images where the visualization would predict a misclassification? Can you see counter-examples where the resulting image does not make sense?\n\n- Figure 6 and 7 provide interesting results indicating that the unsupervised objective improves transfer learning across several classification tasks related to faces. The results are encouraging but I am concerned that (a) the task is too easy, (b) tuning the hyperparameter for \\lambda can be difficult. For (b), I would like to ensure that \\lambda was tuned independent on a third validation test (and not the test set whose numbers are reported). For (a), I suspect that an important benchmark is to just take set \\lambda = \\infty and then perform linear classification on the embedding. It would be interesting to see how well these numbers fare as a baseline compared to the reported numbers."}