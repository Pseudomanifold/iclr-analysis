{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Claims: The authors present perceptual regularization as a method for learning a visualization of deep representations for promoting interpretability and understanding of vulnerability to adversarial attacks. Second, they show their method can explain negative transfer to new tasks. Finally, they show that the representations learned with this regularization method transfer to unseen tasks better than without the reconstruction regularization.\n\nDecision: Weak accept. The authors tackle an important problem with a very simple regularization technique and show how it can aid interpretability in adversarial attacks and improve transferability through strengthening attention of features to new tasks and in the multi-task setting.  While the two problems are important, results on multiple datasets would be necessary to strengthen the paper. Section 3 on perception claims to give results on MNIST and CelebA datasets, but there only seem to be results on CelebA. For transfer in section 5, MNIST or SVHN should also be included by removing classes of digits and checking performance. The conclusion addresses the use of different types of generative models, but I would be extremely interested to see how VAE compares with a deterministic auto encoder, as they are known to have much better generative properties. \n\nFor Fig 6, it would also be nice to know how robust these numbers are to lambda, and include some variation for different values of lambda, or at least some discussion of how much tuning is required."}