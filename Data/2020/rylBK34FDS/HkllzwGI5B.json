{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper focuses on sparse neural networks. Typically, l1 regularization is the go-to strategy, however, it is not scale invariant. That is, all weights are affected by the regularization, not only those that are being driven to 0. l0 regularization is theoretically optimal, however, it is not smooth and has no gradients almost everywhere, so it cannot be used for training. As a compromise the paper proposes Hoyer regularization, that is the l1/l2 ratio. The Hoyer regularization has the same minima structure and leads to sparse solutions while being scale invariant, that is it does not affect all weights in the process. Additionally, the paper proposes structured Hoyer regularization. Last, it employs the said regularizations in deep networks: LeNet, AlexNet and ResNet on several datasets: MNIST, CIFAR, ImageNet.\n\nStrengths:\n+ The described method is simple, intuitive and straightforward. By applying the said regularization (~ \u03a3 |w_i|/sqrt(\u03a3 w_i^2)), one arrives at seemingly sparser solutions, which is verified in practice.\n\n+ The experiments are extensive and convincing. I particularly like that the authors have used their method with complex and deep models like ResNets, on large scale datasets like ImageNet.\n\n+ The presentation is generally clear and one can understand the paper straightaway.\n\nWeaknesses:\n+ The contributions of the paper are rather on the thin side. At the end of the day, Hoyer regularization is taken from another field (compressed sensing) and applied on deep networks. This is also witnessed by some moderate repetition in the writing, e.g., between the introduction and the related work.\n\n+ There are some points where the paper becomes unclear. For instance, in Figure 3 what are the \"other methods\"?\n\n+ In Figure 1 it is explained that the Hoyer regularization leads to minima along the axis. The gradients then push the models \"rotationally\". Could this lead to bad multiple local optimal problems? Is there any guarantee that any particular axis will generate better solutions than the other?\n\nAll in all, I would recommend for now weak accept. I find the work interesting and solid, although not that exciting."}