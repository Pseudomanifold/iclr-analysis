{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors present a method for the audio classification problem, using a pre-trained network (shared encoder) and creating adapters for each new task. This is an interesting alternative to transfer learning, and it would be great if minimal adapters can be designed to work with a baseline network.\n\nThe adapter network is of the same depth as the shared encoder network, and uses both its own output, and the encoder's output from the previous layer for its next layer. Using learnable gates, the channels of its previous layers can be selected to be on / off. The idea is to create a network that can be tuned for accuracy and cost (params and FLOPs) for on-device inference.\n\nSince the motivation behind the paper is networks that are suitable for on-device inference and multi-task adapters, I find the paper lacking in the following respect:\n\n1. Describing how exactly they will implement this architecture on-device. The channels that have been turned off by the gates, can have their parameters trimmed, and the next layer can ignore them. However, it would be great to get actual numbers in terms of latency, size, etc. from a model that is running on-device, with explanation of the work done to translate the training model graph to an efficient inference graph.\n\n2. Comparison with other on-device model compression techniques like quantization, distillation, pruning etc.\n\n3. Comparison with the transfer learning variant, where only the last few layers are trainable.\n\nWhile this is an interesting approach, it would be more convincing to know that this approach:\na) Translates to actual improvements in on-device inference, along with implementation details.\nb) Is better than / competitive with other techniques for compression and transfer learning.\n\nWith the paper as it currently stands, I would recommend consideration for a dedicated workshop, but weak reject for the main conference. I would be willing to be convinced otherwise, if the authors can fill in more details as mentioned."}