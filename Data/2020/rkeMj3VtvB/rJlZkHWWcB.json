{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an architecture to be used in multitask learning of audio tasks. They propose an encoder which provides input to layers at various depths. They employ a regularizer on the weights of the layers, in order to minimize the total amount of computation whilst having a shared representation pipeline to increase the overall performance of the system. What they are suggesting seems to improve the performance over a multitask learning baseline where the sharing mechanism is not employed (if I am understanding correctly).\n\nOverall, I found the paper little difficult to follow. Especially the experimental section is hard to follow as I found the figure captions lacking required information. For instance in Figure 3, I am not sure what the numbers under 'num. params.' section signify? I think you are choosing different values for the tradeoff parameter lambda, and then this corresponds to different accuracies? I don't understand why you are reporting different accuracy values when using the same Lambda value? Also on the figures I am guessing that different traces, different tasks. It would be better to plot the average trace as well. There is no mention of hyper-parameter search as far as I can tell from the experimental section. Also, I don't understand why you are not reporting the accuracy when lambda=0 in Figures 2,3? This would have helped us to better assess the whole potential of the proposed architecture. \n\n\nTo sum up, I think there seems to be value in this work, as the shared architecture seem to improve the overall performance when compared to a multitask learning baseline. This being said, I think very crucial information we are missing is the performance of the multihead baseline when it has the same number of FLOPS or trainable parameters as the proposed architecture. That is: You should train a multihead model baseline with the same number of trainable parameters as the suggested algorithm. I also think you should improve the writing, especially in the experimental section. Therefore I am currently suggesting a weak rejection. If you add the result of the multihead baseline with equal number of parameters with the proposed model, I would be open reconsider my decision.\n"}