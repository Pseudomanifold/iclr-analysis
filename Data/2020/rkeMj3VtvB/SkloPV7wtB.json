{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overview\n\nThis paper presents a model that can perform multiple audio classification task using a number of shared layers. The main motivation for the proposed model  is the fact that it consumes the same input (audio) to estimate various properties (like language ID, musical pitch, bird song) and therefore it might be useful to share some of the lower  level computations while also having some task specific parameters/layers. The main contribution of the paper is an additive cost term that learns to distribute parameters between the various tasks while penalising parameters/number of operations. \n\nThe main feature of the proposed model is task specific layers that are placed in parallel to the shared embedding layers that are used by all tasks. The task specific layers receive inputs from all the shared layers and the task specific layers in the layer below. The design is such that the task specific layers/parameters for each task are independent and therefore during inference only the computations for a given task need to performed (in addition to the shared computations). The cost function is then used to distribute parameters between these layers given a fixed compute budget.\n\n Although I think the idea and the area of application are extremely interesting and relevant, there are some shortcomings that should be addressed before the paper is accepted. My main criticism is the fact that the comparison between the multi-head model and the proposed architectures is not fair. The multi-head architecture does not have any task-specific parameters and contains N softmax layers for the N tasks considered, on top of the final shared encoder layer. This architecture is quite restrictive because not only does the multi-head layer have fewer parameters, it is also forced to share the same fixed-dimensional encoding across all N-tasks. It is therefore unsurprising that the multi-head architecture performs worse since it uses the same fixed-dimensional embedding in the final layer to represent information using for all N tasks, some of which are quite unrelated, for example detecting musical pitches and bird songs. A better baseline architecture would be to have a number of shared layers, followed by a number of tasks specific layers in series rather than parallel. This architecture might yield similar accuracies, would also have independent parameters for each task and the same cost function could then be used to distributed parameters between tasks. Secondly, the authors do not comment on the relationship between tasks. Some of the tasks like speaker identification and language identification are clearly related. However, detecting musical pitches, bird songs, instruments and environmental sounds etc are quite unrelated and it doesn't make sense for a single network to be good at performing all these tasks simultaneously. This is clearly reflected in the evaluation where the single-task accuracy is always better than any of the multi-task results for some tasks. The paper does not provide any insight into what tasks can be usefully shared. I think this needs to be addressed first before investigating the best method for sharing computation between tasks. Finally, I found the evaluation section quite difficult to follow. The figures should be improved for clarity and the main arguments and inferences drawn from these figures are not clear. Overall I think the proposed cost function has limited novelly and although the area of application is extremely interesting, the paper fails to address some important basic questions about the nature of the tasks. Furthermore, the baseline comparison isn't fair for the reasons outlined above. \n\nMinor Comments:  \n\nIntroduction\n\n1. \u201ccharacterized by a large number of parameters and floating point operations (FLOPs)\u201d, the authors mention floating point operations however all models that are deployed on embedded devices are quantised to have integer weights. Floating point operations are only performed during training. Given that this paper is about on-device inference, this sentence is a bit confusing. The authors should replace FLOPs with number of operations. \n2. \u201cthe audio embeddings might fail to capture all the information needed to solve all tasks\u201d, the authors here argue that the shared embeddings might fail to capture information related across all tasks, which is why I believe that baseline multi-head system should contain task specific layers/computations in series after the shared embedding layers. \n\nMethods\n\n1. I found the description of the gating mechanism quite hard to follow and had to read the section several times to understand what exactly is going on, even though the overall function is quite simple. The description should be improved for clarity. \n2. \u201cThe slope of the non-linearity s is progressively increased during training\u201d, what schedule was used to do this? The experiments later also do not shed any light on how the slope was changed. \n\nExperiments\n\n1. Why use 64 bins while computing the STFT? The standard practice is to use 40 bins (in speech and other audio tasks). Do any of the tasks require extra resolution along the frequency axis? Lowering the number of frequency bins is a useful way of reducing the input size and therefore the number of computations performed in the first/input layer. \n2. \u201cNote that the choice of the tasks used for the evaluation is consistent with the selected temporal granularity\u201d. I\u2019m not sure what temporal granularity means in this content. \n3. \u201cAs such, we do not consider speech recognition tasks, which generally require a much finer temporal granularity\u201d I\u2019m not exactly what is meant here. Temporal granularity in feature extraction? Or in the labels that are assigned to the input audio? This statement should also be clarified. \n4. \u201cThe number of parameters of the output softmax layer depends on the number of output classes.\u201d What is the sum of parameters in all output layers combined? Surely this is non-trivial compared to the number of parameters in the encoder (65k and 125k)\n5. Figures 2b,c and 3b,c should be enlarged for clarity. The text says that the curves should not start at 0 since they consider the number of parameters in the task specific layers, but many points on curve 2b look like they start at 0. Also Figures 2 and 3 are quite difficult to read. Its not the most clear presentation of the argument. The tables are much more clear though. \n\nSummary\n\nThe paper presents a novel model  for performing multiple audio related tasks using joint/tied layers. The main novelty of the paper is to present an additive term in cost function and a gating mechanism that penalises large models. I think the multi-head baseline against which the results are presented is quite restrictive since it doesn\u2019t have any task specific parameters and also has to share the same vectors for encoding all the information related to all the different tasks. Although the number of different tasks and experiments presented is commendable, the evaluation section does not present a convincing argument. The evaluation section also needs to be reworked in order to present the arguments and results more clearly. Finally, the authors should consider the relationship between the tasks considered and whether they expect all of them to be benefit from a joint/multi-task approach. \n"}