{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a method for multi-label text classification as a sequence generation task and using Bert model as an encoder. The paper shows that some improvement is obtained over methods on a collection of public and private datasets.\nThe paper's overall contribution and imapct seems quite limited. More concretely, some of the concerns regarding the paper are the following :\n1. The proposed method of using Bert as an encoder does not appear really novel as it has been proposed in other works such as XBert [1] and to datasets at a much bigger scale.\n2. The improvements over other methods is very little (around 0.5%) which can also be result of hyper-parameter tuning among other reasons.\n3. The authors should evaluate their method on much larger scale datasets in multi-label classification and also compare with other state of the art methods such as those available from Extreme multi-label classification repository [2].\n\n[1] XBert : X-BERT: eXtreme Multi-label Text Classification with BERT\nhttps://arxiv.org/abs/1905.02331v2\n[2] http://manikvarma.org/downloads/XC/XMLRepository.html"}