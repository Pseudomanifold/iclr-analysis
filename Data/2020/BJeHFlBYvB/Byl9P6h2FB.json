{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes sequence generating BERT (BERT + SGM) and a mixed model for Multi-label text classification (MLTC). The mixed model is an ensemble of vanilla BERT and BERT+SGM models. The methods are examined on three MLTC datasets in either English or Russian. The mixed model has a 0.4%, 0.8%, and 1.6% improvement in hamming accuracy (HA), micro-F1, macro-F1, and accuracy compared with vanilla BERT.\n \nEven though there are some accuracy improvements, it seems that most of the improvements are coming from the downstream task fine-tuning of BERT. Several questions and suggestions:\n \n1. The mixed model seems to have a better performance on most of the datasets over original BERT on HA and micro F1, while it fails to compete with original BERT on macro F1. This may be a sign that the mixed model is overfitting and cannot generalize to an unbalanced dataset. \n\n2. Next, it would be great to add more math deductive reasoning and model analysis. Can you show the dramatic difference between BERT with one more layer and  BERT + SGM? In figure 1, it looks like we have an attention as a final \u201cdecision maker\u201d, but does it make more correct decision compared with the normal BERT?\n\n3. Furthermore, it would be better to have more convincing experiment results. The mixed model performs well around 0.45. Can you interpret with more reasoning or just intuitions? Or is it just a random threshold that we get for this downstream task?\n \n "}