{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This is an empirical paper about the MLTC (Multi Label Text Classification) problem. They experiment on three different models. One method is the famous BERT model, which they use in the multi-label setting, by simply changing the activation function of the last layer of the original model to sigmoid. The second method, is BERT encoder combined with a Sequence Generation Model (SGM). For this one, they replace the encoder of a previous work (Yang et. Al.) with Bert encoder. And finally, the third model, is just an ensemble of the two previous mentioned methods: vanilla BERT and BERT+SGM.\nPros:\nThey have experimentally shown some improvements in the different MLTC metrics with (BERT+SGM) and mixed methods, compared to the vanilla BERT, specially, for data sets with hierarchically structured classes. \nThey have shown better performance of (BERT+SGM) rather than BERT for training with 3-4 epochs, however with higher (20) epochs, BERT will eventually win!\nCons:\nTheir experimental improvement for their mixed method, is not as significant for public data sets, less than 1% on average.\nThey have reported 0.4%, 0.8%, and 1.6% average improvement in miF1, maF1, and accuracy measure for public data sets. I believe this is not exact. \nFor example, the average set accuracy on public data sets is 0.8% instead of 1.6%. Or maF1 measure improvement on average is much less than 0.8%. For RCV1-v2 they have 0.55% improvement. But for the two other data sets they have lost 0.1% and 0.6% compared to the vanilla BERT. I am wondering how that comes to the average of 0.8% improvement on maF1 measure(?)\nOn one data set they have got 1.6% better accuracy but that is not on average, as how they have reported. \n\nI do not see much novelty in this work. They have combined two previous methods. Although it is always interesting to see that previous good methods can be combined to get better models, I expect a better performance improvement compared to the state-of-the-art methods for a paper that is empirically based.\n"}