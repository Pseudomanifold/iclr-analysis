{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "TL;DR: The paper gives interesting, theoretical results to adversarial training. The paper only uses linear classifiers, which are hardly the same problem as deep networks where adversarial attacks are problematic. Some conclusions from theorems can be vague or informal, and therefore are not very convincing. I vote for rejecting this paper since it is hard to claim it informs deep learning research (the motivating reason for doing adversarial training). However, I am not familiar with theoretical analysis of adversarial attack/defense, so I am open to counter-arguments.\n\n=================\n1.    What is the specific question/problem tackled by the paper?\nThe paper gives a theoretical analysis to the theoretically less-studied procedure of adversarial training, and shows properties of adversarial training in comparison to regular training, for both linearly separable data or inseparable data. The paper sheds light on some empirical behavior of adversarially trained networks, namely that they are more robust to outliers and lower in performance.\n\n2.    Is the approach well motivated, including being well-placed in the literature?\nI am not an expert of adversarial samples, so I am ill-equipped to judge the novelty of the paper. The research direction itself is well motivated, and in the realm of deep learning, it is posed as a first paper to theoretically analyze adversarial training.\nHowever, the authors only analyzed linear classifiers. This makes the results of the paper ill-suited for deep networks, whose non-linearity is arguably the reason why adversarial samples are such a problem. The motivation of the paper is thus greatly diminished. For linear classifiers, I do not know if there are existing work on their robustness when perturbation of samples are being trained on, but to be well-placed in the literature, the authors must either claim there is none, or cite those papers.\n\n3.    Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\nClaims and novelties in this paper include:\n(1) Adversarial training converges faster than regular training if samples are \u03b5-strongly linearly separable,\n(2) If samples are not \"\u03b5-strongly linearly separable\", adversarial training is robust to outliers, while regular training is not,\n(3) Confidence is low for all (training) samples if \u03b5 is large. \n\nOnly (1) seems to be sufficiently proved. I am not certain that this is a very useful result, and I am open to counter-arguments.\n(2) and (3) have steps that are vague and informal:\n\n(2) That regular training is susceptible to outliers is proof by example and people already know that. Also the claim relies on the assumption that pi^1 and pj^2 are on the same scale, while those samples that violate the decision boundary can have arbitrarily large pi^1 or pj^2. Since outliers are often the violators, and inliers often are not, a small number of carefully placed outliers can make ||p2|| quite large while each pi^1 can be very small. It is also worth noting the logic seems to boil down to \"N1>>N2, so inliers should overwhelm outliers, making the training robust\". The claim is not guaranteed.\n\n(3) I am not very sure if I understand it correctly, but the logic seems to be that the logits are bounded, so they cannot be too large, and so the confidence is low. However, the bound also involves the magnitude of w and x from the other class, so the final step of the proof is either unclear or the bound can indeed be quite large. Note that |wx| does not need to be very large for the confidence to be high (e.g. if logit is 5, the confidence is 1/(1+e^-5)=99.3%). The claim also relies on the assumption that epsilon is larger than the distance between the farthest points in the dataset, which is extreme since you can find an adversarial sample that can be considered to be simultaneously \"close enough\" to the two most dissimilar samples in the dataset.\n\nIn the end, the results are not very convincing or useful for informing deep learning research.\n\n=============\nTo improve paper:\n- Clarify motivation and how this would inform adversarial training highly non-linear classifiers;\n- Add related work for robustness to perturbation of linear models, or state that they don't exist;\n- Clarify weaknesses in the claims.\n\nEditorial changes:\nDefinition 2: \"logit\" <--- people call this probability estimates; logits are wTx\nSec. 4.1.2 needs to clarify what k in x_i^k means -- it's continued from proposition 1 which at first glance is irrelevant\n\n\n\n\n\n"}