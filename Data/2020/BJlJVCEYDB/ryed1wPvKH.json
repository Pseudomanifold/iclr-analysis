{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper builds a model of motivation-dependent learning.  A motivation channel is provided as an additional input to and RL-based learning system (essentially concatenated to state information), similar to goal-conditioned approaches (as the authors mention).  The motivational variables evolve according to their own rules, and are designed/interpreted as biological motivations such as water, food, sleep and work.  While the narrative is interesting, I lean towards reject as I believe it failed to deliver on what it promised.\n\nIn the first experiment, the satisfaction of these motivations are mapped onto a 4-room setting, where being in each room satisfies a motivation.  The choice to map the four rooms to biological drives is cute, but possibly confusing/misleading since this navigation problem really has nothing to do with these biological drives. A claim is that by providing the motivation as input to the policy, it is more robustly (across seeds) able to learn the \"migration\" (i.e. cycling) behavior among the rooms.  In a second example, a similar problem is solved involving navigation on a graph.\n\nThe final, most substantial example, is a policy trained to solve a simple, abstract version of a behavioral task. In this setting, a motivation channel was again used.  However, the motivation channel value is now fixed to one of two discrete values, essentially meaning it is simply a task-label variable, a paradigm that has already been applied in the context of simple models of neuroscience tasks, e.g. see Song et al. 2017 \"Reward-based training of recurrent neural networks for cognitive and value-based tasks\".  \n\nThere is a bit of a mixed framing overall as to whether it is being claimed that the \"motivation\" being passed as an input is a fundamental contribution to AI/RL (I think it is not), versus the computational modeling of biological motivation.  I think the people qualified to judge whether the computational model is a worthwhile model of motivation specifically are probably a narrower set of computational neuroscientists.  I do think there is value in the kind of computational modeling performed, involving establishing a relationship between training a neural network to solve a behavioral task and comparing this with real neural data.  This paradigm already becoming increasingly popular within computational neuroscience.  However, while I find the results slightly interesting, but not very significant, as someone interested in the biology of motivation, I question whether the nature of these contributions would be of broad interest at this venue.  \n\nMore fundamentally, I don't believe there is a meaningful ML/AI/RL contribution, and I have some issues with the presentation of the first two examples.  While I do like the narrative inspiring these problems, I find the implementations of the problems too simplified to really be meaningfully related to their inspiration (in terms of motivated behaviors).  Rather than really model motivation as part of the policy architecture, the authors have proposed a solution to modeling motivation that makes motivation a feature of the environment.  Essentially, the reward provided by the environment depends on an extra latent variable and by hiding this (in the cases where the policy does not see motivation inputs), it is quite likely that it becomes too difficult for the value function to predict what is happening (the environment has become partially observed).  This seems less a setting where motivation channels solve a problem, and more just an example of an environment that has more complex rules for generating rewards being more challenging to learn about, especially if latent variables are not available to the value function.  Critically, it has not been shown that motivational systems are useful for artificial agents, rather the tasks themselves have been designed to attempt to be models of biological motivation.  \n\nPersonally, I am interested in motivated behaviors and think that future AI developments should take note of this field, but again, the present work does not provide actionable insights into implementing an artificial motivation system.  At the same time, this work does not provide interesting enough neurobiological results for those to stand on their own either.\n\nMinor clarification:\n\n\"trained to perform in realistic tasks\" -- the task is very simple.  I would consider this a fairly abstract model of the task.  \n"}