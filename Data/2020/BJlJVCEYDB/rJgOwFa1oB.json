{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper presents a computational model of motivation for Q learning and relates it to biological models of motivation. Motivation is presented to the agent as a component of its inputs, and is encoded in a vectorised reward function where each component of the reward is weighted. This approach is explored in three domains: a modified four-room domain where each room represents a different reward in the reward vector, a route planning problem, and a pavlovian conditioning example where neuronal activations are compared to mice undergoing a similar conditioning.\n\nReview Summary:\nI am uncertain of the neuroscientific contributions of this paper. From a machine learning perspective, this paper has insufficient details to assess both the experimental contributions and proposed formulation of motivation. It is unclear from the discussion of biological forms of motivation, and from the experimental elaboration of these ideas, that the proposed model of motivation is a novel contribution. For these reasons, I suggest a reject.\n\nThe Four Rooms Experiment:\n\nIn the four-rooms problem, the agent is provided with a one-hot encoding representing which cell it the agent is located in within the grid-world. The reward given to the agent is a combination of the reward signal from the environment (a one-hot vector where the activation is dependent on the room occupied by the agent) and the motivation vector, which is a weighting of the rooms. One agent is given access to the weighting vector mu in its state vector: the motivation is concatenated to the position, encoding the weighting of the rooms at any given time-step. The non-motivated agent does not have access to mu in its state, although its reward is weighted as the motivated agent\u2019s is. The issue with this example is that the non-motivated agent does not have access to the information required to learn a value-function suitable to solve this problem. By not giving the motivation vector to non-motivated agent, the problem has become a partially observable problem, and the comparison is now between a partially observable and fully observable setting, rather than a commentary on the difference between learning with and without motivation.\n\nIn places, the claims made go beyond the results presented. How do we know that the non-motivated network is engaging in a \"non-motivated delay binge\"? We certainly can see that the agent acquires an average reward of 1, but it is not evident from this detail alone that the agent is engaging in the behaviour that the paper claims. \n\nMoreover, the network was trained 41 times for different values of the motivation parameter theta. Counting out the points in figure 2, it would suggest that the sweep was over 41 values of theta, which leaves me wondering if the results represent a single independent trial, or whether the results are averaged over multiple trials. Looking at the top-right hand corner I see a single yellow dot (non-motivated agent) presented in line with blue (motivated agent) suggesting that the point is possibly an outlier. Given this outlier, I\u2019m led believe that the graph represents a single independent trial. A single trial is insufficient to draw conclusions about the behaviour of an agent. \n\nThe Path Routing Experiment:\n\nIn the second experiment, where a population of agents is presented in fig 5, it is claimed that on 82% of the trials, the agent was able to find the shortest path. Looking at the figure itself, at the final depicted iteration, all of the points are presented in a different colour and labelled \u201cshortest path\u201d. The graph suggests that 100% of the agents found the shortest path. The claim is made that for the remaining 18% of the agents, the agents found close to the shortest path\u2014a point not evident in the figures presented.\n\n\nPavlovian Conditioning Experiment:\n\nIn the third experiment, shouldn\u2019t Q(s) be V(s)? In this setting, the agent is not learning the value of a state action pair, but rather the value of a state. Moreover, the value is described as Q(t), where t is the time-step in the trial; however, elsewhere in the text it is mentioned that the state is not simply t, but contains also the motivation value mu.  \n\nThe third experiment does not have enough detail to interpret the results. It is unclear how many trials there were for both of the prediction settings. It is unclear whether the problem described is a continuing problem or a terminating prediction problem\u2014i.e., whether after the conditioned stimulus and unconditioned stimulus are presented to the agent, does the time-step (and thus the state) reset to 0, or does time continue incrementing? If it is a terminating prediction problem, it is unclear whether the conditioned stimulus and unconditioned stimulus were delivered on the same time-steps for each independent trial. If I am interpreting the state-construction correctly, the state is incrementing by one on each time-step; this problem is effectively a Markov Reward Process where the agent transitions from one state to the next until time stops with no ability to transition to previous states.\n\nIn both the terminating and continuing cases, the choice of inputs is unusual. What was the motivation for using the time-step as part of the state construction?\n\nHow is the conditioned stimulus formulated in this setting? It is mentioned that it is a function of time, but there are no additional details.\n\nFrom reading the text, it is unclear whether fig 7b/c presents activations over multiple independent trials or a single trial.\n\nGeneral Thoughts on Framing:\n\nThis paper introduces non-standard terms without defining them first. For example, TD error is introduced as Reward Prediction Error, or RPE: a term that is not typically used in the Reinforcement Learning literature. To my understanding, there is a hypothesis about RPE in the brain in the cognitive science community; however, the connection between this idea in the cognitive science literature and its relation to RL methods is not immediately clear.\n\nTemporal Difference learning is incorrectly referred to as \"Time Difference\" learning (pg 2). \n\nNotes on technical details:\n\n- The discounting function gamma should be 0<= gamma <=1, rather than just <=1.\n\n- discounting not only prevents the sum of future rewards from diverging, but also plays an important role in determining the behaviour of an agent---i.e., the preference for short-term versus long-term rewards.\n\n- pg 2 \"the motivation is a slowly changing variable, that is not affected substantially by an average action\" -- it is not clear from the context what an average action is. \n\n- Why is the reward r(s|a), as opposed to r(s,a)?\n\nNotes on paper structure:\n\n- There are some odd choices in the structure of this paper. For instance, the second section---before the mathematical framing of the paper has been presented---is the results section. \n\n- In some sentences, citations are added where no claim is being made; it is not clear what the relevance of the citation is, or what the citation is supporting. E.g., \u201cWe chose to use a recurrent neural network (RNN) as a basis for our model\u201d following with a citation for Sutton & Barto, 1987.\n\n- In some sentences, citations are not added where substantial claims are being made. E.g, \u201cThe recurrent network structure in this Pavlovian conditioning is compatible with the conventional models of working memory\u201d. This claim is made, but it is never made clear what the conventional computational models of working memory are, or how they fit into the computational approaches proposed.\n\n- Unfortunately, a number of readers in the machine learning community might be unfamiliar with pavlovian conditioning and classical conditioning. Taking the time to unpack these ideas and contextualise them for the audience might help readers understand the paper and its relevance.\n\n- Figure 7B may benefit from displaying not just the predicted values V(s), but a plot of the prediction over time in comparison to the true expected return.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}