{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a method of learning a hierarchical communication graph for improving collaborative multi-agent reinforcement learning, particularly with large numbers of agents. The method is compared to a suitable range of baseline approaches across two complex environments. The initial results presented seem promising, but further work is needed to ensure the results are reproducible and repeatable.\n\nTo enable reproducibility, please include details of all hyperparameters used for all approaches in both domains. These should include justification of how the hyperparameters were tuned. Without understanding how these values were set I cannot support acceptance.\n\nTo ensure the results are repeatable, repeated runs of training should be completed and the variation in performance quantified in the results. These repeats may have already been performed as Table 3 and Figure 5 discuss average results, but if not they must be completed before the work can be published due to the known issues with high variance in performance that commonly occur in deep RL.\n\nI would also argue against the justification of excluding ATOC from the StarCraft II experiments as its performance in MAgent with 25 agents is comparable to the other baseline methods that were tested. However, this is lower priority than the issues above provided there is no significant change in the relative performance of methods when the variance across multiple runs is documented in all existing experiments.\n\nMinor Comments:\nThe following are suggestions for improvements if the paper is accepted or for future submissions. \n\nIn Section 2, centralised critic methods are grouped as \"communication-free\" however I don't think this is the best term to explain this approach as each agent has to communicate both its observations and actions to a centralised node (e.g. COMA) or all other agents (e.g. MADDPG). I also think this section should include coverage of other methods of utilizing graph neural networks in multi-agent reinforcement learning - e.g. \"Deep Multi-Agent Reinforcement Learning with Relevance Graphs.\" Malysheva et al. Deep RL Workshop @ NeurIPS 2018 and \"Relational Forward Models for Multi-Agent Learning\" Tacchetti et al. ICLR 2019.\n\nIn Section 3, the acronyms CBRP and HCOMM are used on page 4 before they are introduced in full on page 5 for CBRP and never for HCOMM. HCOMM is also not used in Figure 4 or in the text description of the method. I believe it is the module described in Section 3.2 but this should be made clearer.\n\nMany claims in the paper are worded too strongly and should be revised. In Section 2, it is claimed that DQN \"is one of the few RL algorithms applicable for large-scale MARL\" - However, there are now many successful applications of deep RL to multi-agent systems (some of which are cited earlier in this same section) that use a variety of algorithms other than DQN. It is also claimed in this section that \"DQN has excellent sample efficiency\" despite the sample efficiency of deep RL being a known issue, open research question and a barrier to its widespread use in practice. \n\nIn Section 3.2 the authors conclude \"Overall, our LSC algorithm has advantage in the communication efficiency\" despite in the same section noting two cases where ATOC has better efficiency (N_msg and N_b-r). I would suggest removing this sentence entirely as the paragraph above already contains a balanced account of the relative merits of each approach. \n\nOn pages 8 and 9 the authors make references to guarantees and in the Appendix to proofs that are not supported by theory only empirical results. Without supporting theory these words should be avoided.\n\nThe writing is also often informal to the detriment of presenting important information clearly. Notably, on page 3 \"due to explosive growing number of agents\" and on page 4 \"The overall task of the MARL problem can be solved by properly objective function modeling.\" The second of these, particularly the word \"solved\" is also related to the issue above of using the words guarantee and prove.\n\nFinally, the paper would benefit from a thorough grammar check. I note the following issues as simple changes that can be made to improve the readability of the paper:\n- In the abstract, the sentence \"but also communication high-qualitative\" does not parse. Perhaps this could be shortened to the brackets following? i.e. \"is not only scalable but also learns efficiently.\"\n- Page 2, \"or employing the LSTM to\" -> \"or use the LSTM to\"\n- Page 2, \"still hinder the\" -> \"still hinders the\"\n- Page 2, \"the communication structure need be jointly\" -> needs to be jointly\n- Page 6, \"policy module motioned below\" -> discussed below\n- Page 10, \"the map size are is 1920x1200\" -> the map size is 1920x1200\n- Page 10, \"We do not compare with ATOC because its poor performance in MAgent\" -> because of its\n- Page 10, \"we will to improve\" -> we will improve\n- Page 10, \"practical constrains\" -> practical constraints"}