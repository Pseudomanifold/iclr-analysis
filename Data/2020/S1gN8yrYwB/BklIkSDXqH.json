{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a new model-free + model-based algorithm, MBPGE, that trains a policy using a policy gradient algorithm on top of the learned models. Contrary to previous approaches, they use a true Bayesian distribution by means of the randomized anchorized MAP. Furthermore, they combine rollouts from the real environment and from the learned dynamics directly for the policy training, instead of relying just on the ones of learned dynamics, which induces a larger distributional shift.\n\nThe paper readability could be improved. First of all, the introduction spans over 2 pages. The readability could be significantly improved by splitting the introduction into introduction (containing the motivation) and related work. Secondly, the preliminary section spans more than their approach, which hints that the original contribution is too incremental. All of the actual method is just combining section 2.1 and 2.2. Lastly, section 3.1 is hard to parse. The authors extend the paper to 9 pages when there was no need for it.\n\nIn terms of contribution, it seems that the main contribution is to use the method presented [1] and replace the ensemble with the ensemble of [2] and use PPO [3] instead of TRPO [4]. There is no further insight in the paper, but the fact that the ensemble of [2] works better than the classical ensemble (fact shown in [2]) and that PPO works better than TRPO (fact shown in [3]). The other contribution is to combine samples from the real environment and the from the learned models to train the policy. This however has actually been done in the context of model-based reinforcement learning, see [5]. It would be interesting that the authors compared against different heuristics to choose the ratio between imagined and real rollouts (i.e., constant, annealed, fraction of improvement [1], etc\u2026). Overall, I don\u2019t think that the contributions of this paper are enough for publication. \n\nRegarding the experiment section I strongly believe that the comparison is flawed. The results they report on MB-MPO do not match with the ones in [6, 7]. For instance, half-cheetah does not learn at all while it is an easier environment than Walker and Hopper (and there\u2019s learning in those). Furthermore, given this they should also compare against ME-TRPO and ME-TRPO switching the TRPO with PPO, since this would be a fairer comparison. This section lacks also of a proper ablation analysis to identify how much each element of the proposed algorithm affects the performance the choice of Bayesian ensemble and heuristic to determine the amount of real samples and imagined ones.\n\nAt this stage, there is not enough contribution in terms of novelty nor delta in performance.\n\n\n\n[1] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, Pieter Abbeel. Model-Ensemble Trust-Region Policy Optimization.\n[2] Tim Pearce, Felix Leibfried, Alexandra Brintrup, Mohamed Zaki, Andy Neely. Uncertainty in Neural Networks: Approximately Bayesian Ensembling.\n[3] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. Proximal Policy Optimization Algorithms.\n[4] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. Trust Region Policy Optimization\n[5] Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine. When to Trust Your Model: Model-Based Policy Optimization\n[6] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, Pieter Abbeel. Model-Based Reinforcement Learning via Meta-Policy Optimization\n[7] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba. Benchmarking Model-Based Reinforcement Learning."}