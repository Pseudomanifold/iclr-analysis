{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary \n\nThis paper described a model-based RL method which uses learned dynamics models to augment the replay buffer used when training a PPO agent. \nSpecifically, the agent learns an ensemble of dynamics models, and then performs a PPO updates using a mixture of trajectories sampled from the true environment and trajectories sampled from teh learned dynamics models. \nThe fictitious trajectories are sampled from different dynamics models in the ensemble, which helps prevent the policy from exploiting the errors of a single model.\nThe proportion of real/fake trajectories are adapted using the ratio of the real vs. predicted rewards: if the average returns are similar, then the proportions will be approximately equal, but if they are very different then more trajectories from the real environment will be used. \n\nOverall, I think there are some interesting elements to this paper but it is currently not ready for publication. I recommend weak reject for three reasons: limited novelty, the experiments as they stand are not very convincing and the writing needs work. \n\n\n#### Novelty\nThe two stated contributions are: \n1. the use of principled uncertainty estimates in model-based RL (an anchored ensemble)\n2. the routine which adjusts the proportion of real vs. simulated trajectories for learning the policy.      \n\nIn my opinion, 1. is not much of a contribution since a number of works have already made use of different types of model uncertainty in the context of model-based RL. Here they use a slightly different version (the anchored ensemble, which is similar to the randomized priors used in DQNs) but this is more of a choice of implementation than a contribution in itself (a number of methods for estimating uncertainty exist, such as regular ensembles, dropout masks, BNNs, etc). The manner in which they use the uncertainty is to perform simulations with several different models to avoid overfitting to one, and this was already proposed in the ME-TRPO paper. \n\nThe idea of 2. is interesting and in general, adapting between model-based and model-free RL regimes during execution seems to have potential. However the current method seems currently under-explored and heuristic. The fact that it is sensitive to the \\alpha hyperparameter (Figure 2) is a bit troubling. Is there a principled way which does not require tuning this hyperparameter? Or reformulating things so that it is less sensitive?\n\n\n#### Experiments\n\nAlthough the experiments show some improvement, they are not that convincing. The improvements do not seem statistically significant for Swimmer and Walker, and the asymptotic performance is worse than PPO for Hopper. There is only half-cheetah where the algorithm shows a clear improvment. \n\nSince one of the claimed contributions is the use of the anchored ensemble, there should be an ablation experiment showing this gives an improvement over a standard ensemble, but this is not included.  \n\n\n\n#### Writing:\n\nOne issue with the paper is that it spends *much* too long discussing related work and preliminaries. \nThe first 5 pages are devoted to this and the proposed method is only introduced on page 6!\n\nSome detailed comments:\n- Paragraph 1 in the intro should be drastically cut. The sentence \"Recently...challenging tasks\" could be followed by references and then a sentence discussing the sample inefficiency and then moving on to the next paragraph. Most readers will be familiar with DQN, actor critic etc. \n- Same for paragraphs 2 and 3. A short discussion of the general idea behind model-based RL and its improved sample efficiency, the issue of the policy exploiting model errors, and some references are sufficient. \n- Similarly, section 2.1 is mostly unnecessary. It isn't necessary to detail the updates for standard algorithms such as PPO/TRPO unless they are useful for the proposed algorithm. \n- Section 2.2 is also too long. They main idea is very simple and is summarized in Equation 13. \n\n\n"}