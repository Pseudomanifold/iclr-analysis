{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** Summary ** \nThis paper proposes to improve the previously work DARTS in terms of the training efficiency, from the large memory and computing overheads. The authors propose a partially-connected DARTS (PC-DARTS) with two components: 1. Partial channel connection 2. edge normalization. To be detailed, they sample a small part of channels to perform connection and add edge normalization to eliminate the potential optimization problem. The results on CIFAR-10 and IamgeNet show the approach is effective, especially in ImageNet, the approach achieves SOTA results. \n\n** Strengths **\n1.\tThe research direction of reducing the training/memory effort of Neural Architecture Search is important, which is also very hot in nowadays. \n2.\tThe authors propose two components to perform the efficient search process, which are partial channel connection and edge normalization operations. These methods are reasonable to reduce the training effort. The authors are inspired by ShuffleNet or related research topics. \n3.\tThe approach is easy to follow and implement, the description of the method is also clear.\n4.\tThe experiments also show comparable results in CIFAR-10 and strong performance in ImageNet.\n\n** Weaknesses **\n1.\tThe operation of partial channel connection choses 1/k channels, the remain channels are directly added to the output. This operation somehow feels too straight to be reasonable, since the remain channel has larger weights (1.0) compared to previous weighted combination. Though the second edge normalization can eliminate little, but this modification still suffers from less careful design, for example, another \\alpha weighted combination? Besides, directly bypass is same as perform identity operation, is this right?\n2.\tThe motivation of edge normalization is somehow weak, as the authors are aware of this can also be applied to the original DARTS. From the ablation study, it also shows it works for the original DARTS, which makes the description of 3.3. to be not so convincing. Besides, in the first paragraph of 3.3, what does it mean that \u201cweight-free operations often accumulate larger weights\u201d compared to other operations? I feel the reason is that the weight-free operations are much easier to pass the gradients and easy to be trained. \n3.\tIn imageNet results, it seems P-DARTS significantly outperform PC-DARTS in terms of the search cost, and the accuracy is similar. This makes PC-DARTS approach to be embarrassing. \n4.\tOne general point is what the authors mentioned, indeed, for NAS, more training data involved in the training process is much more important compared to perform operations. Therefore, the advantage benefits from the less memory usage of 1/k selection and the more data in one mini-batch. This makes the design of current research directions to be different. Does it mean more training (longer time) and more GPU memory will significantly outperform current results? Even the SOTA approaches. \n5.\tMinor point: compared to ProxylessNAS which only samples two paths at each time, their method is much more efficient (though the binarization consumes much). What\u2019s the most advantage of PC-DARTS compared to their method? What if combine their approach with edge normalization?\n"}