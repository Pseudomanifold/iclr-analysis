{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper makes an interesting observation and tries to explain what causes it: architecture search methods tend to favor models that are easier to optimize, but not necessarily better at generalization.  I lean towards accepting the paper but there is some clear room for improvement.\n\nThe paper shows that NAS methods comes up with shallower but wider cells. \n- These cells are easier to optimize because they have a smoother loss surface and lower gradient variance. \n- Being easy/fast to optimize is favored by a NAS method because the models are typically not trained to convergence. Instead they are evaluated after a brief period of training. \n- This leaves the question: why are they smoother.\n\nBefore going into the comments I want to state that I am very happy to see that a paper providing an analysis of existing methods to enhance our understanding. This is very much needed in the architecture search community. \n\n==== Comments ====\n- Could you describe what exactly is plotted in Fig. 5, 6 and 7. Specifically what are the aces. . It would make the manuscript more self contained. This point is also one of the main reasons why I did not give the manuscript a higher score. I am unsure of what is plotted but I am giving this manuscript the benefit of the doubt because it is consistent with my own experience.\n\n- So far in the main text, most results focus on DARTS. It would be interesting to see the same consistent behavior is also present when comparing cells originating from different search spaces. (This is slightly different than the setting where the experiment is repeated on isolated search spaces). If this comment is unclear, please ask for a clarification.\n\n- Would it be possible to alter the conclusions by modifying the initialization of the weight matrix. It appears to be the case that the smoothness and the variance both depend on the eigenvalues of the weight matrices. If we could make them more well behaved we could potentially make the narrower architectures train faster?\n\n- Can you use a better term than common connection pattern in the abstract and conclusion. In general the abstract and conclusion could be written in a crisper and more to the point way.\n\n- Please update Table 1 to actually include parameter sizes. This would make the result more reliable. Also the adapted cells need to be explicitly provided. \n\n- Could you provide me with a better understanding of the difference between standard lipschitz and block lipschitz.\n\n==== More minor issues ====\n#) Correction required\nIn section 2, Zoph et al. is cited where it is argued that weight sharing could detrimental for the performance of nas methods. However this paper does not use weight sharing\n\n#) Corrections recommended\nXie et al. 2019 is said to be state of the art. In the actual publication the authors only claim to be competitive. I believe that this is a more balanced statement since those results are not always best on all dimensions (sometimes better on FLOPS but not parameters, unclear whether they would be faster on an actual device, on the large scale results they do lag behind in quality too.)\n\n#) My apologies for the slight digression but I do not think that using the Sciuto et al. paper is a good reference to discount weight sharing approaches or claim that random search is equally good. Some of their experiments are performed on a tiny search space with only 32 models. This gives random search a high probability of getting the right model, while a simple bias in the algorithm might cause it to be sub-optimal. That experiment does not show that random search is as good. Also on the PTB task, their reported results are worse than the open sourced implementation for DARTS. This means to me that this paper cannot be used as a reliable reference. \n"}