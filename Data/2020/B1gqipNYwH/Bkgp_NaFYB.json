{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the problem of learning suitable action abstractions (i.e., options or skills) that can be composed hierarchically to solve control tasks. The starting point for the paper is the (classic) observation that one skill should end where another can start. The paper then proposes a recursive algorithm for learning skills that obey this property. After finding some number of trajectories that reach the goal, the last few states of these trajectories are taken to define the initiation set for the ultimate skill and the termination set for the penultimate skill. The procedure is repeated, yielding a sequence (a \"chain\") of skills that extends from the initial state distribution to the terminal state. The fact that the number of skills is not defined apriori seems to be a strength, and the extension to trees of skills is neat.\n\nThe paper compares the proposed algorithm against state-of-the-art hierarchical baselines on five continuous control tasks of varying complexity; the proposed method outperforms baselines on 2 - 4 of the 5 tasks (depending on the setting considered).\n\nPerhaps the biggest limitation of the paper is that it ignores the exploration problem. As noted in \"Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?\", exploration is empirically one of the main benefits of HRL, yet the proposed method, by construction, cannot take advantage of this strength.\n\nI am leaning towards accepting this paper. While the idea is quite simple, it seems to work well empirically and, to the best of my knowledge, is novel. The simplicity should also make it a good launching point for future work in HRL. My main concern with the paper is that the poor performance of baselines on some of the tasks. For example, HIRO [Nachum 18] quickly solves AntMaze, while the baseline used in Fig 1 (HAC, which the paper claims is stronger than HIRO) gails to solve this task. I would be more confident about this paper if it added a comparison with HIRO.\n\nMinor comments:\n* \"fixed and costly hyperparameter\" -- Why is the number of skills \"costly\"?\n* \"Learning backward from the goal\" -- Two other closely related papers are Policy Search by Dynamic Programming [Bagnell 04] and \"Forward-Backward Reinforcement Learning\" [Edwards 18]\n* \"it may choose to ignore learned options\" -- Why should we expect that the optimal policy will use any skills, if it can be represented entirely using primitive actions?\n* [Jinnai 2019] -- This citation is repeated in the references."}