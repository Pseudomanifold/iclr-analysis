{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This is a positive review. Feel free to skip to the feedback.\n\nSUMMARY OF PAPER\nThis paper explains how to use cluster graphs to easily compute the asymptotic behaviour of any given correlation function (Definition 1) for deep linear networks. By \"asymptotic behaviour\" I mean that it upper-bounds correlation functions by c\u00b7n^s, where s is a nonnegative integer given by the particular cluster graph, and c a \"constant\" (which I think depends on the particular input, x, to correlation function, among other things).\n\nThe authors then conjecture (Conjecture 1) that these bounds transfer to deep nonlinear networks, and that they are tight:\n- Appendix C proves that these upper bounds also hold for deep ReLU networks, and 1-hidden-layer networks with smooth nonlinearity. This is also mentioned in page 3.\n- Section 2.3 empirically shows that these bounds are pretty tight. (in terms of the exponent, none of the theory here gives a value for the constant c)\n\nThe tool provided above is the main result. The authors then use it to provide some results about wide networks  \n- They give a different proof that for large width, the Neural Tangent Kernel stays constant during training. This is because its derivative wrt. time as a function of width n is O(n^-1), and thus 0 for n->infinity.\n- Using the ease of calculation from the tool, they approximate the change in the NTK over training time for any network. They do this using its value at initialization + a term that depends on n^-1. These results are numerically verified in Figure 1.\n- They present numerical evidence for the accuracy of this approximation to the change in NTK over time. \n\nThe authors spend the last 2 pages explaining how cluster graphs derive from Feynman diagrams (FDs), and why these help compute asymptotics.\n\nWHY I AM ACCEPTING THIS PAPER\n\nThe paper adapts FDs and cluster graphs, which is a potentially very useful tool for other wide-network researchers, and could accelerate research in this whole sub-field. It also shows their power by providing a surprisingly large amount of novel theoretical results.\n\nFEEDBACK\n\nAt a very high level, there is only one thing that I think isn't made quite clear by the presentation, and it should be. If I understand correctly, Feynman diagrams (or cluster graphs) are only used here to calculate correlation functions for deep *linear* networks. Then, other results establish that the width-dependent asymptotic behaviour for linear networks holds as-is for nonlinear networks, and these results with FDs constitute Conjecture 1. There are proofs for ReLU and 1-layer smooth networks, mentioned in pg. 3; and the experiments in the paper support it for common nonlinear deep networks as well. I think that asymptotics for linear networks transfer to nonlinear ones is an interesting result, which doesn't depend on FDs.\n\nWhat follows are details.\n\nIt is unclear to me whether cluster graphs are as \"powerful\" as FDs, i.e. whether the bound at the end of the Proof in page 8 is always saturated. Are there some cases in which you need to use FDs to get a tighter upper bound?\n\nIn Table 1 you should say that the values under \"lin. ReLU tanh\" are the fitted exponent s_C. This is not explained. Perhaps you  can mark the only 2 cases (in the 5th row) where the bound is not tight. It would be nice to know how much error remains between the fitted c\u00b7n^(s_C) and the empirical values.\n\nPlease explain x1 <-> x2 in eq. 8\n\nIn figure 1b, consider adding the finite-width limit prediction for the training dynamics. You have already done so for the prediction of the NTK during training in figure 5c, you could indicate it in the same way in 5b.\n\n\nTypos:\n\nFigure 2 caption: feynman -> Feynman\n\npg 8. anlytic evicence -> analytic"}