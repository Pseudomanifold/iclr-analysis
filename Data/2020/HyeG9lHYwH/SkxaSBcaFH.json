{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper aims to circumvent the quantization step and associated gradient approximations for compression algorithms that make use of entropy coding for compression. Entropy coding requires a probability mass function over discrete symbols. As an alternative approach the authors adapt the MIRACLE algorithm by Havasi et al. (2018), which was originally used to compress Bayesian neural networks, to work for lossy source compression with probability density functions over continuous latent variables.\nThe algorithm is based on taking several importance samples from the prior p(z) with the encoding distribution q(z|x) as the target distribution. The number of samples is equal to the exponent of the KL between the encoding and prior distribution, which can quickly grow to an uncontrollably large number. The index of the sample with the maximum importance weight is used as the code for the original image. If both sender and receiver have access to the same random sampler, the receiver can reproduce the sample by drawing a number of samples equal to this index, and decoding the last sample. Similar in spirit to Havasi et al, the authors take several steps to ensure that the KL divergence (and therefore the number of samples) does not become prohibitively large. The compression performance is evaluated by training the proposed model and the competing neural network-based method [1] on the Clic dataset, and evaluating it on a subset of the images of the Kodak dataset. JPEG is also used as a baseline. The authors show results achievable if coding using the relative entropy was perfect (denoted with \u2018theoretical\u2019), and the practically achieved compression performance (\u2018actual\u2019). \n\nDecision:\nWeak reject: although the idea of circumventing the quantization step required by the use of entropy coders is certainly valid and interesting, the results in the paper show that the resulting compression performance is worse than the competing method that does quantize. Moreover, the encoding time is significantly longer than this same baseline. \n\nSupporting arguments for decision:\nAlthough the motivation for circumventing the quantization step seems plausible, the authors show no evidence that the competing method [1], which does perform a post-training quantization step, actually suffers from it. The authors even state on page 8 that \u201c Most notably though, they only used the continuous relaxation during the training of the model, thereafter switching back to quantization and entropy coding, which, they show does not impact the predicted performance, and hence confirming that their relaxation during training is reasonable.\u201c If post-training quantization is reasonable, then this overthrows the entire motivation. More importantly, the \u201ctheoretically\u201d achievable results of the proposed method in seem only competitive in the low-bit rate regime and worse in the higher bit rate regimes. Even more, the \u201cactual\u201d practically achieved compression results are worse than [1] and also considerably worse than the \u201ctheoretically\u201d achievable compression results. The authors do provide a reason for why the \u201ctheoretical\u201d and \u201cactual\u201d results are so far apart, but are unfortunately not able to overcome this issue.\nIn the conclusion the authors honestly admit that the runtime of their method is much slower than the competitors (1-5 min for proposed method vs ~0.5 s  for [1] for encoding times). I appreciate that the authors mention this. The authors then state \u201cimproving the rate factor and the run-time does not seem too difficult a task, but since the focus of our work was to demonstrate the efficiency of relative entropy coding, it is left for future work.\u201d I do not think this paper demonstrates the efficiency of relative entropy coding, the results simply don\u2019t support this claim, and I therefore think that stating that the issues seem not too difficult to overcome is insufficiently convincing. \n\nThe quality of the empirical study can be improved. Another neural network-based compression baseline would make the empirical evaluation of the proposed method more insightful. Now we only see that the result is worse than [1], but it would be good to know how it compares to other baselines such as [2]. Furthermore, the paper does not show compression results aggregated over the entire Kodak dataset, but rather picks 2 images for the main part of the paper, and shows 3 in the appendix. Showing aggregate results gives a more robust estimate of the performance. Individual image results can just be put in the appendix. \n \n\nAdditional feedback to improve paper (not part of decision assessment):\n- In section 5 on the dependency structure of latents in the ladder VAE: is it really necessary to indicate the dependency structure with \u201ctopological structure\u201d? Seems unnecessary to me as dependency structure is a clear enough description already without making it sound overly complicated.\n- Page 9: \u201cFurther, our architecture also only uses convolutions and deconvolutions as non-linearities.\u201d Convolutions and deconvolutions are not non-linearities.\n- Fig 2: I\u2019m not sure if this figure is relevant enough for such a prominent placement in the paper. It doesn\u2019t discuss anything relevant to the contributions claimed in this paper. \n- I can\u2019t find a definition of O in line 3 of \u201cprocedure\u201d in algorithm 2 and in the return statement.\n\n\n[1] Johannes Ball\u00e9, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. ICLR 2018.\n[2] Lucas Theis et al.  Lossy Image Compression with Compressive Autoencoders. ICLR 2017\n"}