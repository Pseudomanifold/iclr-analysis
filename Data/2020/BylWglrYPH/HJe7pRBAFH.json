{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper investigates the idea of using symmetry and invariance in symbolic reasoning. In particular, it considers models where modeling symbolic symmetries through parameter-sharing help with generalization. The three tasks considered are: 1) rule learning: performs sequence-classification. 2) composition: performs sequence-to-sequence with structured input using encoder-decoder architecture, and 3) context-free language learning using memory. In the first two cases, the convolution (of single width) is used to benefit from the symmetry prior, and in the third task, convolution is applied to stack memory structure. In all cases, the proposed architectures were shown to outperform MLP and RNN.\n\nThe paper addresses an important area in deep learning, and the paper is accessible and easy to read. However, there are major issues:\n\n-- I found it challenging to identify a novel contribution. For example, the first task is simply using a single convolution layer followed by pooling for sequence prediction. However, using 1D convolution layers are somewhat wide-spread in NLP. \n\n-- The paper is oblivious to a large body of related work in the area of relational learning and invariant/equivariant deep learning. Here are some examples: Permutation invariant model for sets [1,2], and the link between parameter-sharing and invariance [3,4] is theoretically studied in several works. Note that convolution with a filter of width one followed by pooling is exactly invariant to the symmetric group. There are related works that extend these ideas to graphs [5,6], and relational learning [7]. Invariance has also been explored as it relates to memory [8]. Another relevant direction to discussions of the paper is the idea of attention in various architectures, such as transformers.\n\n-- There are vague or misleading claims. In particular, for some tasks, it is not clear why the proposed architecture addresses the targeted symmetry. For example, it is not clear why translation invariance in-memory models the structure in a context-free grammar. \n\n\n[1] Zaheer, Manzil, et al. \"Deep sets.\" Advances in neural information processing systems. 2017. \n[2] Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. \n[3] Shawe-Taylor, John. \"Building symmetries into feedforward networks.\" 1989 First IEE International Conference on Artificial Neural Networks,(Conf. Publ. No. 313). IET, 1989.\n[4] Ravanbakhsh, Siamak, Jeff Schneider, and Barnabas Poczos. \"Equivariance through parameter-sharing.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. \n[5] Kondor, Risi, et al. \"Covariant compositional networks for learning graphs.\" arXiv preprint arXiv:1801.02144 (2018). \n[6] Maron, Haggai, et al. \"Invariant and equivariant graph networks.\" arXiv preprint arXiv:1812.09902 (2018). \n[8] Kazemi, Seyed Mehran, and David Poole. \"RelNN: A deep neural model for relational learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018. \n[9] Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. \"Order matters: Sequence to sequence for sets.\" arXiv preprint arXiv:1511.06391 (2015).\n"}