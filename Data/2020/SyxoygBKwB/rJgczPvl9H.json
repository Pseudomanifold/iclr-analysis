{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles the problem of generating more realistic adversarial attacks for neural network policies, that can be generated in real-time and only require altering the current environment observation, not the agent\u2019s state (e.g., saved previous observations). To meet this goal, the proposed approach CopyCAT optimizes for a universal (i.e., input-independent) attack per action, that will lead the attacked policy to output that action.\n\nThe evaluations show that CopyCat\u2019s universal attacks are able to measurably alter agents\u2019 behaviour, compared to FGSM attacks. The paper does not compare to more complex, state-of-the-art approaches for generating adversarial examples, because it argues that these approaches take too long to create adversarial examples, which makes them infeasible for real-time attacks. The method used for evaluation is creative -- a policy is attacked to make its actions match those of a *better* policy, and the behaviour of the agent is considered successfully changed if it obtains higher reward than without the attacks.\n\nI found this paper to be well-motivated, and there is a clear contribution here, with the introduction of a universal attack that changes the behavior of neural network policies. However, I have concerns regarding the evaluation, as described below. The writing also needs editing; there are quite a few grammatical errors and confusing / unclear phrasing (e.g., \"In the latter\" is used incorrectly in a couple places).\n\nRegarding the evaluation, my first concern is that the experiments are all on Atari, in which the visual variation is extremely limited, compared to real-world observations. For instance, most Atari games typically only have a handful of colors. Because of this, I\u2019m not convinced that this approach of generating universal attacks would work on more realistic images, which limits the relevance of this paper.\n\nIn addition, it is too strong to claim that showing this approach is effective in the white-box setting means that it will also effective in black-box settings. These universal attacks are computed based on the training observations of the agent. When two agents are trained with deep RL, even if it\u2019s with the same algorithm but with different random initializations of the network, it\u2019s very likely that they will encounter different distributions of states during training. Thus, I would like to see additional experiments that evaluate how this attack performs in the black-box setting.\n\nFinally, there are other approaches that only require a forward pass through a neural network like CopyCAT does, for instance training a generative model to create adversarial attacks (e.g., AdvGAN [1]). Although previous work on this has largely focused on image classification, in essence a policy is just a classifier for which the output is images, so this line of work is worth mentioning and potentially comparing to.\n\nMinor comments / questions:\n- It's strange to refer to stacked observations as the \"state\" of the agent, since typically \"state\" is instead used to refer to either state features or a learned latent state. So claiming that prior approaches require changing the state of the agent is misleading.\n- Why are attacks with smaller l2 norm harder to detect? Is there a citation for this? It seems to me that limiting l-infinity norm is enough to make adversarial examples difficult to detect, and there's no need to limit both l2 and l-infinity. This is an important point, because the evaluations compare algorithms with respect to l2 norm.\n- Section 4 (Related Work) should be placed earlier in the paper, ideally right after the Introduction.\n- Regarding reproducibility -- where were the DQN and Rainbow agents obtained from? If they were trained in-house, what architecture and learning hyperparameters were used?\n\n[1] C. Xiao, B. Li, J. Zhu, W. He, M. Liu, D. Song. Generating Adversarial Examples with Adversarial Networks. IJCAI 2018."}