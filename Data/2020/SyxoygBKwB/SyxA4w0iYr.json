{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a targeted attack on reinforcement learning agents. The goal of the attacker is to add small perturbations masks to an agent\u2019s observations in order to encourage the agent to follow a specific (targeted) set of actions. Each mask corresponds to one specific action, and thus can be thought of as a universal perturbation to induce the agent into taking that specific action. The attack, named CopyCAT, is experimentally compared with current attacks in RL such as the FGSM attack of Huang et al.\n\nI would argue to reject the paper for two key reasons. First, the threat model is not well-motivated or clearly explained. Second, I am not convinced by the soundness of the experimental results because the FGSM baseline seems much worse than it should be. I am also concerned by the lack of ablation studies for the L2 regularization term in the author\u2019s loss function.\n\nThe threat model the authors propose is that the attacker should only have read-access to the agent that it wants to attack, and not write-access. In RL terms, this means that the attacker can only attack the current observation, and not the internal state of the agent. However, in the actual experiments, the authors use networks like DQN which take as input 4 frames at a time, and then the authors only apply the mask to the final (4th) state. This seems like a very strange threat model - if an attacker can already attack every 4th state by applying a mask to it, why can\u2019t the attacker just attack every state? In my opinion, the authors should reconsider the threat model, and perhaps focus more on the fact that they are generating universal perturbations for RL agents via action-specific masks, which does seem interesting.\n\nNext, I am worried about the soundness of the experimental results, especially the FGSM baseline. First, in Fig. 4, I do not think the L2 norm of the FGSM-L_inf attack should be on the x-axis - I would not expect the FGSM-L_inf attack to have low L2 norm at all, so it seems unfair to compare something that you explicitly regularize to have low L2 norm (CopyCAT) with something that has no L2 norm constraint.\n\nSecond, I do think that it is fair to have FGSM-L2 (I did not see this explicitly discussed anywhere in the text, which should also be addressed, but I assume this is an FGSM attack which is projected to a bounded L2 norm ball) in such a plot. However, it is worrying that the FGSM-L2 attack gets worse as the L2 norm of the allowed perturbation increases. Intuitively, allowing a larger norm attack should make it easier to force the targeted action; as a result, I am worried that the implementation is simply incorrect.\n\nThird, I would like to see ablation studies on the L2 norm regularization term added to CopyCAT. The intuitive explanation for why it was added (we should minimize energy) doesn\u2019t really make much sense to me - why do we want masks of low energy? Couldn\u2019t we just have a separate version of CopyCAT that just does L2 ball projections? Also, in Fig 4., it seems that the attacks with extremely low weight on the L2 norm regularization term perform better, so why have the term at all?\n\nFinally, I am confused why FGSM is so much worse. It seems to me that CopyCAT is very similar to FGSM, but it is computed over an entire training dataset as opposed to for one image. However, I would expect that FGSM on a single datapoint x to result in lower loss on that datapoint than the mask (which maximizes the loss over the whole dataset, not just x). I don\u2019t think it\u2019s a problem if CopyCAT performs slightly worse than FGSM, since the universality of the masks is the interesting feature; I almost think it\u2019s a problem if CopyCAT performs better. If it does indeed perform better, it\u2019s important to figure out which element of CopyCAT makes it perform better.\n\nSome additional feedback:\n- In the abstract, you should say what your contributions and results are.\n- I think having a section that very clearly outlines the differences (maybe by writing the two algorithms side-by-side) between FGSM and CopyCAT would be immensely helpful for understanding what your contributions are.\n- I think having more plots about how the masks transfer to out-of-distribution inputs would be really interesting. For example, you could explicitly start the agent off at states not seen in the data used for training CopyCAT and check if you can still force a specific action using the mask.\n- I would also add an additional baseline beyond just DQN vs Rainbow; while the two are different, they are trained to do similar things. I would also consider doing something like Randomly Initialized DQN vs trained DQN, since those have very different goals.\n"}