{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks. The new proposed method utilize the variational representation of $f$-divergence, which quantifies the difference between $\\mathcal{T}\\tau$ and $\\tau p$, where $\\tau$ is the parametric density ratio between the unknown behavior policy data and the target policy. If only if $\\tau$ is the true density ratio, the loss $\\mathcal{D}_{f}(\\mathcal{T}\\tau || \\tau p) = 0$. \n\nCompared with prior work (Nachum et. al 2019), the new proposed framework can generalize the undiscounted case $\\gamma = 1$, and the derivation for the new algorithm is quite simple and easy to follow. The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases. Moreover, I have two specific questions:\n\n- The choice of $f$-divergence. Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various $f$-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice).\n\n-The authors should also have a discussion that similar idea can be generalized to more general  distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship. I think there is a concurrent submission using MMD metrics).\n \n\nOverall I think this is a good paper and I recommend for acceptance. \n\nReference Papers:\n- Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-gan: Training generative neural samplers using variational divergence minimization.\" Advances in neural information processing systems. 2016.\n- Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017).\n- Nachum, Ofir, et al. \"DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections.\" arXiv preprint arXiv:1906.04733 (2019).\n- Anonymous, \u201cBlack-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning\u201d, submitted to ICLR 2020.\n"}