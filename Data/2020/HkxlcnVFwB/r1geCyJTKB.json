{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Main contributions:\nThis paper generalizes the recent state-of-the-art behavior agnostic off-policy evaluation DualDice into a more general optimization framework: GenDice. Similar to DualDice, GenDice considers distribution correction over state, action pairs rather than state in Liu et al. (2018), which can handle behavior-agnostic settings. The optimization framework (in equation (9)) is novel and neat, and the practical algorithm seems more powerful than the previous DualDice. As a side product, it can also use to solve offline page rank problem.\n\nClarity:\nThis paper is well established and written. \n\nConnection of theory and experiment:\nI have a major concern for the theory 1 about the choice of regularizer $\\lambda$. For infinite samples case, the derivation of theory 1 is reasonable since both term is nonnegative. However, in practice we will have empirical gap for the divergence term, thus picking a suitable $\\lambda$ seems crucial for the experiment. I think a discussion on $\\lambda$  for average case in experiment part should be added. And compared to Liu et al. (2018) which normalized the weight of $\\tau$ in average case, which one is better in practice?\n\nOverall I think this paper is good enough to be accepted by ICLR. The optimization framework can also inspire future algorithm using different divergence."}