{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors empirically investigate parameter divergence of local updates in federated learning with non-IID data. The authors study the effects of optimizers, network depth/width, and regularization techniques, and provide some observations. In overall, I think this paper study an important problem in federated learning.\n\nHowever, there are some weakness in this paper:\n\n1. The paper is nearly pure empirical. There is no theoretical analysis supporting the observations proposed in Section 4.1, which weaken the contribution of this paper.\n\n2. This paper only raises some issues in federated learning with non-IID data, and discusses the potential causes. No suggestions or potential solutions is proposed in this paper, which weaken the contribution of this paper.\n\n3. Since this is nearly a pure empirical paper, I hope the authors can make the experiments thorough. However, there are some experiments I expect to see but not yet included in this paper:\n\n    3.1. The authors only studies Nesterov momentum in this paper. However, in practice, it is more common to use Polyak momentum. I hope the authors can also study FL SGD with Polyak momentum in this paper.\n\n    3.2. In this paper, the authors assume that different workers has the same number of local data samples (in Definition 1). However, due to the heterogeneous setting, it is very likely that different workers have different numbers of local data samples, which could be another source of divergence. Furthermore, different numbers of local data samples also results in different numbers of local steps, which may also cause divergence.\n\n    3.3. [1] proposes a regularization mechanism (FedProx) to deal with the heterogeneity. Instead of studying weight decay, it is more reasonable to study the regularization technique proposed by [1].\n\n\n4. There are some missing details (maybe they are already in the paper but I didn't find them):\n\n    4.1. What is the definition of Adam-A and Adam-WB? And, what are the differences between Adam-A, Adam-WB, and vanilla  Adam? (and also, what is the \"A\" in NMom-A?)\n\n    4.2. When using Adam in federated learning, how are the variables synchronized? Note that for Adam, there are 3 sets of variables: model parameters, 1st moment, and 2nd moment. Due to the local updates, all the 3 sets of variables are not synchronized. When the authors use Adam in FL, did they only synchronize/average the model parameter and ignore the 1st and 2nd moments, or did they synchronize all the 3 sets of variables?\n\n\n----------------\nReference\n\n[1]  Li, Tian et al. \u201cFederated Optimization for Heterogeneous Networks.\u201d (2018)."}