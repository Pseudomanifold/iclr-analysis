{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThe paper presents an empirical study of causes of parameter divergence in federated learning. Federated learning is the setting where parameter updates (e.g. gradients) are computed separately on possibly non-IID subsamples of the data and then aggregated by averaging. The paper examines the effects of choice of optimizer, network width and depth, batch normalization, weight decay, and data augmentation on the amount of parameter divergence. Divergence is defined as the average cosine distance between pairs of locally-updated weights, or between locally updated weights and weights trained with IID data. The paper generally concludes that regularization methods like BN and weight decay have an adverse effect in the federated setting, the deepening the network has an adverse effect while widening it might be beneficial, and that adaptive optimizers like Adam can perform poorly if their internal statistics are not aggregated.\n\nI recommend that the paper be rejected. The main shortcoming of the paper is the lack of rigororous statistical analysis to support its conclusions. The paper contains a lot of raw data, but the discussion mainly highlights trends that the authors seem to have observed in the results, without quantifying the relative sizes of effects, how consistent they are across experimental conditions, etc. The writing is also quite unclear, to the point that I often didn't understand exactly what argument was being made.\n\nDetails / Questions:\nThe main problem is the lack of quantitative analysis of the trends the paper identifies. For example, regarding \"Effects of Batch Normalization\", there seem to be two claims made:\n1. Batch normalization makes things worse (somehow) in the federated setting\n2. Batch re-normalization still makes things worse, but not as much\nHow are these effects quantified? How large are they? Do they hold across all datasets, architectures, and optimizers considered? Ideally there would be a table summarizing each experimental manipulation, its effect on performance, whether that effect is significant, etc. Of course this requires some care because the paper is doing an exploratory analysis and there are many hypotheses to test; a good reference is [1].\n\nThe paper also relies heavily on parameter divergence as a measure of performance in federated learning, but I see no evidence presented that parameter divergence is predictive of test accuracy (which is presumably what we actually care about). Intuitively I can see how it might be related, but since divergence is basically being used as a proxy for accuracy, it is vital to show convincingly that the two are related. What do we gain by analyzing parameter divergence rather than simply comparing test accuracy?\n\nRegarding the \"steep fall phenomenon\": The paper seems to present this as an indicator that a manipulation performs poorly in the federated setting. But, isn't it a good thing if parameter divergence goes down? Why does specifically a sudden, sharp decrease in divergence indicate a problem?\n\nFinally, some improvements might be made to the experiment setup. For one, the case of completely-disjoint label sets in different local learners seems extreme to me. Wouldn't at least partial overlap be more common in practice? (This is not my area so I don't know). Experimenting with different degrees of overlap would be useful. As for network architectures, it would be valuable to look at a greater variety of standard architecture styles (e.g. ResNet, Inception, etc). I realize there are some experiments with ResNet, but the focus is mainly on the single-path VGG-like architecture. I do realize this is a lot of experiments to do.\n\nMinor points:\n* In the setting described as \"IID\" in Table 1 is not, the subsampled for each learner are not IID subsamples of the full dataset because they are class-balanced (if I'm understanding correctly)\n\nReferences:\n[1] Dem\u0161ar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7(Jan), 1-30."}