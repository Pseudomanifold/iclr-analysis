{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper experimentally studies the reasons for the slow convergence of the Federated Averaging algorithm when the data are non-iid distributed between workers in the multiclass-classification case. Paper performs extensive experimental study and observes that the main reasons for failure are connected to (i) the parameter divergence during the local steps, (ii) steep-fall phenomena when parameters on different nodes are getting close fast, and to the (iii) high training loss. \n\nMy score is weak reject. The paper provides extensive but unclear experimental results. Improving presentation would significantly improve the paper. For example, why in experimental and theoretical study different parameter divergence metrics were used, etc (see below), why different networks use different optimizers. \nMoreover, provided experimental comparison might be unfair. The learning rate is constant throughout all of the experiments, depending only on the optimizer, but not on the neural network architecture. This can affect the final results. \n\nConcerns and questions that should be addressed:\n1. The initial learning rates were not tuned properly. It is set to be the same for different neural network topologies, which might significantly affect the results. What did the choice of initial learning rates is based on? \n\n2. Why the parameter divergence metric in Definition 1 is not the same as in the theoretical study (Appendix B)? What is the intuition behind Definition 1?\n\n3. Why the divergence of parameters is considered only at the last layer? It seems to hide many important interactions in the other layers. \n\n4. Some important experimental details --- should be added:\n   - At which moment the parameter divergence is computed in the plots? Is it computed at the end of the local iterations right before synchronization? \n   - How the training loss was computed in the plots? before or after synchronization? on the local only or the global data?\n   - Which batch size was used? \n   - Improve the figure caption to detail the experimental setup. (e.g. in fig 3. the network architecture was mentioned only for one of the figures, include which optimized was used, etc)\n\n5. In experiments on Fig. 2. and Fig.3 (middle) what is the accuracy for IID baseline? Is the observed phenomena connected to the poor network architecture or to the non-iid data? \n\n6. In table 5 of the appendix, why experiments use Adam optimizer, but not Momentum SGD as in the main paper to compare the performance of ResNet14 and ResNet20?\n\n7. Better re-prase the definition of the steep fall phenomena, now it is not very clear: in the IID setting parameter divergence values are also sometimes reducing sharply; in the network width study parameters divergence doesn\u2019t experience sudden drop. Also, how does this phenomena (and parameter divergence too) connects to the training loss? \n\n8. Why for different experiments different baseline models are used? (NetA, NetB, NetC)\n\n\nOther minor comments: \n- Appendix B, first equation on page 13. (d_q)^t -> (d_q)^t_k; The size of gradient \\nabla_w [E ...] is different from the size of (d_q)_k. They cannot be added together.\n- page 7, last sentence of the first paragraph: what is the accuracy achieved with Batch Renormalization? Why the reason for accuracy gap is \u201csignificant parameter divergence\u201d? on fig. 3 \u201cparameter divergence\u201d is smaller than for the baseline.\n- Why the name of the section on page 7 is \u201cexcessively high training loss of local updates\u201d if later it is stated that it is actually smaller than for the IID case? \n- Defenition 1, line 4: \u201cthe then\u201d -> \u201cthe\u201d\n- section 3: \u201cA pleasant level of parameter divergence can help to improve generalization\u201d -> where was it shown?\n- section 4.2, paragraph 2: what is meant by \u201chyperparametric methods\u201d?\n- section 4.2, paragraph 3: \u201cquantitative increase in a layer level\u201d -> not clear what does it mean.\n- page 4, effect of optimizers: what do you refer to as \u201call model parameters\u201d? \n- page 5, last paragraph: Hinton et al... -> (Hinton et al\u2026). Use \\citet(\\citep) instead of \\cite. \n- why Dropout yields bigger parameter divergence if on Fig 2, right it actually helps? \n- Last line of the page 5. Where was this observed? \n"}