{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper contributes a new dataset for testing label corruption robustness. The authors attempted to make the dataset represent \"real-world noise.\" The hamartia is that they used the \"search by image\" feature from Google, so that \"similar\" images were what is determined \"similar\" by a convnet. In this way it is not clear their noise is any more real than what is seen in previous works (such as the Gold Loss Correction paper which used labels from a weak classifier as a source of label noise). The hamartia also makes their experimental findings and takeaways about \"real noise\" questionable, as it is not clear they are testing real noise but consequences of properties of convnet embeddings. However this paper still contributes a dataset, hence this paper still is some sort of contribution. However, with this flaw, it is not clear that it is enough for ICLR.\n\nSmall things:\n\n> robust learning is experiencing a reminiscence in the deep learning era\nrenaissance?\n\n>  ImageNet architectures generalize well on noisy data when the networks are fine-tuned. Comparing the first and second rows in Fig. 2, we observe that the test accuracy for fine-tuning is higher than that for training from scratch on both Red and Blue noise\nThis section should cite _Using Pre-Training Can Improve Model Robustness and Uncertainty_ (ICML 2019) since that is a main conclusion of their paper.\n\n> Inception-ResNet-V2 (Szegedy et al., 2017) is used as the default network architectures\nUnfortunately they're probably using some form of Inception to compute image similarities, which was how the \"real noise\" dataset was curated."}