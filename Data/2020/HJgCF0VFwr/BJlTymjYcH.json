{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a nonparametric score test to estimate the importance of network connections on the final output of Deep Neural Networks. They derive this by modeling the log-transformed joint density of each connection and final output in a tensor product reproducing kernel Hilbert space. They finally derive an asymptotic distribution of the proposed test statistics which only depends on the eigenvalues of the kernel. This importance test is applied to ranking the importance of each connection in Multilayer Perceptron Networks, and Convolutional Networks and sparsifying the networks by removing the least important connections. The method is applied post-training to fully trained networks but evaluated by retraining the sparsified network from scratch. The method is demonstrated in experiments on compressing three networks.\u00a0\n\nI believe this paper is a borderline accept. It provides a more statistically principled method to examine the importance of connections in a Neural Network and rank them compared to existing compression methods. Due to this ranking, there is a clear method to easily adjust the target compression rate. They are able to achieve high lossless compression rates. However, the benefits shown in the empirical results could be more convincing. It lacks baselines on more complex networks and could benefit from more empirical analysis of the theoretical benefits and properties of this approach.\n\nPros: \nThe method is able to maintain accuracy while achieving high compression rates on MNIST and CIFAR10. It does better than the baselines compared to the small networks. They show a capability for increasing generalizability by decreasing error rates.\u00a0\n\nIt provides a well derived statistically principled method to examine the importance of connections and rank them.\n\nIndicates an additional use to visualize the importance of features.\n\nCons:\nThe empirical results could be clearer. It lacks baselines for larger models on Cifar10. Could you compare it with the published results of other algorithms? How does it do on larger networks like Imagenet?\nComputational efficiency is mentioned but could be examined in greater detail. \u00a0How long does it take to run and how is that affected by model size?\n\nThe experimental setting is somewhat unclear. The baseline Louizos et al. (2017)\u00a0 was designed to optimize group sparsity/speed, but the experimental results here only examine the compression rate. Was the baseline run to optimize speed or sparsity?\n\nIt would be interesting to examine the correlation of importance score with actual impact on network performance. This might be done with a comparison with random pruning or pruning higher importance connections.\u00a0It might be useful to examine the performance of the networks after pruning nodes of differing importance without full retraining or just fine-tuning. It is unclear how important full retraining is in this method.\n\nIt would be interesting to visualize the importance of features at different depths in the deep convolutional networks.\n\nMinor suggestions: In the introduction, you mention l0 and l1 norm methods, but cite Han et al. (2015) which compares l1 and l2 norm and found l2 norm to be better overall.\n\ntypos:\nIn Abstract: nonparemtric scoring test\u00a0 ->\u00a0nonparametric scoring test\u00a0\u00a0\nIn 4.4:\u00a0sample averarge ->\u00a0sample average\nIn 4.5: ASYMPTOTICALLY DISTRIBUTION\u00a0 ->\u00a0 \u00a0ASYMPTOTIC DISTRIBUTION\u00a0"}