{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: This paper studies the problem of certified robustness to adversarial examples and proposes a new training method to obtain better certified robustness. This new method is based on using a double margin regularizer. \n\nDecision: I vot for rejecting this paper. I think the paper studies an important problem. However, I find some serious flaws in the experiments and also do not find the motivation and proposed idea convincing, Detailed under: \n\nExperimental concerns:\n\u2014 Evaluating on 200 examples seems very small. Most papers consider at least 1000 examples. \n\u2014 The paper compares all models based on *one* certified procedure. This is an incorrect comparison. Some certification procedures work better on some networks over another, and this does not reflect onthe actual robustness of the network. For example, ReLU stability paper reports 80% certified accuracy at \\eps = 0.3, but this paper reports 0% (possibly due to weaker attack). While some certification methods are generally weak, it\u2019s not necessary that they are uniformly weaker by the same amount on all networks. Hence this is an incorrect comparison. \n\u2014 Even if we use the results reported on the one certification method (ignoring the flaw above), the results only show in most settings that the double margin method ends up with a different point on the accuracy-robustness tradeoff. It\u2019s not clear why this tradeoff is better.\n\n\nConcerns with the motivation and paper overall: \n\u2014 Paper seems to emphasize the \u201cunified\u201d perspective of certified training and regularization. I find this unification very natural and not particularly novel/insightful. It\u2019s just a restatement of the objective. \n\u2014 I haven\u2019t verified the correctness of the propositions inthe paper. However, just the statement confuses me. \\lambda -> 0 is the regime where we don\u2019t care about robustness and only care about test accuracy. Why is this even interesting or relevant?\n\u2014 Finally, unfortunately while it looks interesting, I don\u2019t understand the motivation behind the double margin method. This doesn\u2019t seem to follow from rest of the paper. The authors mention below eq(9) that the gradient of the regularizer is the important term. Even if this is true, why does the double margin have a \u201cbetter\u201d gradient?\n\nIn light of the experimental concerns and generally weak motivation/description of the proposed regularizer, i vote for rejecting the current version. "}