{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper describes a self-adversarial method to train a GAN for text generation that circumventes the problems of mode collapse and reward sparsity. They replace the traditional binary discriminator with a comparative discriminator, which provides the generator with more frequent rewards that are not always restricted to the limited number of real training examples. \n\nThey show gains on synthetic and real generation tasks. \n\nStrengths:\n\nThe paper is well written with adequate details about the training objectives and the learning algorithm. I appreciate the human analysis conducted by the authors. The SAL method performs better than many strong baselines consistently across 3 datasets and on various metrics.\n\nThe authors present strong ablations, that are very insightful, particularly, regarding the role played by classifying sentences as indistinguishable.\n\nWeaknesses and Questions: \n\nIn general, some more description about the motivation of each metric would be helpful, rather than just stating that its from previous work.  \n\nHow is BLEU evaluated for this text generation task? Is the entire test set treated as a single reference? Do you generate the same number of tokens as the reference and then compute n-gram overlap between the reference and the prediction? What happens to the brevity penalty of BLEU?\n\nIn Table 4, does BLEU-5(F) denote only 5-gram precision, or is it the geometric mean of 1-5 gram overlaps?\n\nHow does NLL_gen serve as a measure of diversity for the synthetic dataset?\n\nFor the human evaluation, does quality mean grammaticality? Can simple memorized sentences be scored higher?\n\nTypos in Section 4. The authors refer to tables 17 and 18. Please fix. "}