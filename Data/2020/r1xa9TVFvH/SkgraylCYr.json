{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use the Neural Tangent Kernel (NTK) with the Upper Confidence Bound for stochastic contextual bandits. \n- The paper instantiates Kernel UCB (Valko, 2013) with the NTK and the novelty is limited from a theoretical point of view. \n- There is no experimental comparison with Neural Linear or Kernel UCB using a fixed kernel, (for example, the RBF kernel) or to methods like Thompson sampling that work well in practice even with non-linearities. \n\nDetailed review below:\n- Section 2.2 is not relevant to the paper and it might be more useful to use this space to explain NTK better. \n- Please explain NTK before instantiating the algorithm in Section 4. \n- The \"Efficient Implementation\" section in Section 4 is standard and done in all the linear bandit papers. Please acknowledge this or say how it is different. \n- The NTK description in Definition 5.1 needs to be clarified. At the moment, it is difficult to parse. Please give some intuition about it. \n- For the regret analysis, could you explain how the analysis is different from that of a fixed kernel in Valko, 2013. \n- What is the intuition for having a lower bound on \"S\", the norm parameter? Why is there no upper bound? \n- The width of the neural network depends on T^4. How does this affect the effective dimension \\tilde{d} in the worst case? Can it result in linear regret?\n- For Lemma 6.2, 6.3, please say that these are directly borrowed from Valko, 2013 and Abbasi, 2011. \n- From an experimental perspective, the width of the neural network is a constant wrt to T, K and L and clearly doesn't align with the theoretical bounds. Please justify why this is a valid thing to do? \n- As mentioned earlier, there is no comparison with Kernel UCB with a fixed kernel, Neural Linear or Thompson sampling, methods that work well in practice. \n- Finally, real-world experiments are necessary to show the benefit of using NTK in practice. "}