{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1050", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors study Neural Architecture Search, which aims to automate design of neural network models. Their approach consists in using a two stage algorithm:\n-\tA first Neural Network $f$ is trained for predicting the performances of sub-architectures. Then binary sub-graphs coding the sub-architectures are uniformly sampled (Bernouilli(0.5)), and their performances y are evaluated thanks to the first Neural Network.\n-\tThe graph sampling matrix A, which is indexed by the m sampled architectures and the Fourier basis of size $O(n^d)$, is built. Then the optimization problem $x^*= \\arg\\min_x ||y \u2013 Ax||$ is solved using Lasso. The largest Fourier coefficients are chosen to build an estimate g of f. Finally, computing minimum of g, the architecture is generated. \n\nSince $m << n^d$, the optimization problem is ill-posed. Theorem 3.2 shows that if A satisfies the restricted isometry of order s, then the sparse coefficients x can be recovered.\nThe algorithm is evaluated and compared to the state-of-the-art on various image classification tasks and on RNN.\n\n\nMajor concern:\n\n1/ I did not find the proof of Theorem 3.2 in the main paper and in the appendix, so I do not buy it.\n\n2/ The authors claim that their algorithm performs better than the state-of-the-art, but according to tables 1,2,3, I did not find significant differences of performances in term of test errors.\n\n3/ The key idea of the algorithm is not well explained. A one-shot NAS f is pre-trained. f is assumed to be well-trained. Then it is approximated with a Fourier-sparse Boolean function. Why using an approximation if f is perfect? Do you expect to reduce the needed number of sampled architectures?\n\n\nMinor concerns:\n \nEquation 3.1 is not clear. $X_S (\\alpha_l)$ does not depend on k. So all rows seem to be the same.\n\n"}