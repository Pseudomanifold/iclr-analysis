{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions:\nThis paper tackles the problem of One-shot Neural architecture search by proposing a new method. \nThe method consists mainly of new search strategy of the optimal architecture that is inspired by the recovery of boolean functions from their sparse Fourier expansions. As such, this work is an application of recent progress in the field of compressive sensing to One-shot neural architecture search. Given the problem formalism, the authors have also provides guarantee for the optimality of their method, i.e the method can recover the optimal sub-network of any given  a sufficient number of performance measurements.\n\nClarity\nOverall, the paper is well motivated and the technical content is good. That said the structure could be enormously  improved to ease the reading and the overall understanding. For example: better caption for Figure 2 explaining what is shown; presenting the pseudo-code directly in the method overview and spending the rest of the section explaining the method; showing the related work before the experiments; etc.\n\nNovelty\nThe main novelty in my opinion is the application of compressive sensing methods to One-shot NAS. This approach is significantly different from other One-shot NAS method that I am aware of mainly regarding the search strategy employed to find the best architecture. \nHowever, this work seems like an incremental improvement over Hazan et al 2018. To this regard, the only novelty that was the framing of One-shot NAS as a recovery of boolean functions from their sparse Fourier expansions is not new either.  That is said, I am open to be proven wrong on this point!\n\nResults\nThe experiment section is not self-content, the readers is refered a couple of times to other papers to get details that are  critical to reproducibility and understanding. \nOverall, the search strategy of CoNAS seems  parameter efficient, fast and competitive. Also, small ablation studies showing the effect of the different parameters of CoNAS were very informative and well-appreciated.\nHowever,  the search space is different between CoNAS and the others methods for some experiments, making it difficult to decide if the search strategy of CoNAS is definitely competitive compared to other methods or not.\n\nPoints of improvement:\n1 - Structure of the paper\n2 - Clarify novelty compared to HARMONICA ( not the application domain please)\n3 - demonstrate that with the same search space your method is competitive.\n4 - Does m=1000 in your experiments satisfies theorem 3.2? What is the value of d in your experiments? Can you provide supporting experiments that answer those questions?\n\nPreliminary decision:\nFor now, I will say *weak reject* \n\n"}