{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper provides a comparison of several uncertainty measures that have been proposed for neural networks. The goal of the work is to alleviate the lack of clear interpretability of softmax outputs as measures of uncertainty in neural networks for multi-class classification (in particular, here image classification). The authors propose the use of expected Bayes factors as aggregate measures of how much information is conveyed by a measure of uncertainty, and compares several proposed approaches based on this measure on CIFAR-10/100 and ImageNet-1k. The authors also discuss the application of uncertainty measures to detect mislabeled or ambiguous images, detecting out-of-distribution samples and adversarial examples. Passing by, the authors propose a measure based on the norm of the gradient to detect adversarial examples that seems to work well. \n\nThere are interesting ideas in the paper. The use of Bayes factors for measuring the quality of the uncertainty estimates, and the comparison of various existing methods on that measure seems useful. The experiments on various tasks also have their own merits.The extension to top-k uncertainty is also interesting.\n\nOn the other hand, I found the paper difficult to follow because the contributions are scattered over the paper and the appendices without clear link. A lot of space is devoted to the definition of the implied risk, which does not bring much to the overall interpretation of the results. The criterion for detecting adversarial examples based on the norm of the gradient appears at the end of the paper somewhat independently from what was before, and the definition and computation of the Expected Bayes factor is entirely deferred to Appendices, which makes the main paper not really self-contained.\n\nThe paper also lacks a conclusion. The various uncertainty measures seem to perform differently depending on the datasets, and it is unclear what the authors recommend in the end. \n\nOverall, it seems to me that with a bit of restructuration, the paper would be an interesting contribution -- for instance in terms of the assessment of dropout variance vs direct model entropy. It seems to me though that for now the paper lacks coherence and clarity in the message."}