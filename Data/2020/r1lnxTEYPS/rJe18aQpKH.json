{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Recent works have shown that out-of-distribution samples can have higher likelihoods than in-distribution samples for some generative models. To explain this phenomenon and to tackle the problem for OOD detection, this paper adopts \"typical sets\" for identifying in-distribution samples. Specifically, a \"typical set\" is a set of examples whose expected log likelihood approximate the model's entropy. For a Gaussian distribution, the paper finds that a single point typical set locates exactly in the \\sqrt{d} radius, which is usually favored over the high-likelihood origin. Then the paper uses the \"typical set\" to perform OOD for a batch of examples. Empirically they demonstrate competitive performance over MNIST and natural image tasks. \n\nTypical set seems natural for out-of-distribution detection. An important property is that, if one draws a large number of independent samples from the distribution, it is very likely that these samples belong to the typical set (basically Theorem 2.1).  However,  for small n, this property doesn't hold anymore, which leaves here a questionmark whether \"Typical set\" can be used for OOD detection in small n regime. As the author argues, for Gaussian distribution when n=1 the typical locations are those \\sqrt{d} radius points. But this doesn't justify the \"Typical set\". If the distribution is some non-Gaussian wired distribution, the typical locations doesn't seem to make sense at all.\n\nFollowing the previous argument above, the Typical set method requires to perform OOD for a batch of examples. In contrast, the Annulus method can be directly applied to one single test example. \n\nEmpirically, the Typically set doesn't demonstrate obvious advantages compared to the baselines. For both MNIST and natural image tasks, it seems that all methods behave similarly. For comparing such big tables, I would recommend adding a column showing the average ranks among all methods. Beyond that, standard OOD tasks usually evaluate methods using AUROC and AUPR (Hendrycks and Gimpel, 2017). Is it possible to also include such metrics ?\n\nTheorem 2.1 is confusing. It is beneficial to define what P is, and verbally state what the theorem conveys. "}