{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a valuable and detailed empirical study of the double descent behaviour in neural networks. It investigates presence of this behaviour in a range of neural network architectures and apart of identifying it as a function of the model size it also identifies it as a function of training time which I believe is novel. Overall I think the paper presents results valuable to the community. At the same time it has several issues that need to be addressed. If the issues are addressed in a satisfactory manner I will recommend acceptance of the paper. \n\nIssues and questions: \n\n** \"Such a phenomenon was first postulated by Belkin et al. (2018) who named it \u201cdouble descent\u201d and demonstrated it on MNIST with decision trees, random features, and 2-layer neural networks with `2 loss.\" This is not a correct statement. The authors cite works (Advani & Saxe 2017) and there is also \"Stefano Spigler, Mario Geiger, Stephane d\u2019Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart. A jamming transition from under-to over-parametrization affects loss landscape and generalization. arXiv preprint arXiv:1810.09665, 2018.\" that both identified this behaviour prior to the Belkin et al. (2018) work. There is even a much older line of work identifying the peak in the generalization error by Opper's group: \nhttp://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op03b.pdf (1995)\nSiegfried B\u00a8os and Manfred Opper. Dynamics of training. In Advances in Neural Information\nProcessing Systems, pages 141\u2013147, 1997.\nhttp://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf\n\nMoreover, as is only said in the supplementary material, Geiger et al. also tested on the CIFAR dataset, while the introduction of the present article only mentions previous experiments on MNIST. The fact that previous work also observed this in CIFAR should be moved in the introduction. \n\n** The authors claim to define the \"effective model complexity (EMC)\" and claim this as one of the main results of the paper.\n\nPresenting this as a new measure is strange, this exactly is what Geiger et all call jamming transition/threshold in Stefano Spigler, Mario Geiger, St\u00b4ephane d\u2019Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart. A jamming transition from under-to over-parametrization affects loss landscape and generalization. arXiv preprint arXiv:1810.09665, 2018. And very closely related to what Belkin calls \"interpolation threshold\" (to define interpolation threshold we could fix the model and vary the number of samples, Belkin et all fix number of samples and vary size of the model, but one is just the dual of the other). Giving it a yet completely new name seems to create more noise than value.\n\nThe relation between EMC and the jamming transition is discussed in the supplement. However, not in a accurate way. The authors say \"a \u201djamming transition\u201d when the number of parameters exceeds a threshold that allows training to near-zero train loss.\" but jamming is inspired by the physical phenomena when spheres are added into a finite volume box and the more spheres we have the harder it gets to fit them all in until it is not possible anymore. The analogy here is that fitting sphere in = reaching training error zero. Then number of spheres = number of samples. Thus the more samples the harder it is to get the training error to zero, leading to the jamming transition. Both in training and jamming the number of samples at which this happened depends on the details of the protocol/training, thus it does depend on things such as regularization.\n\nThe only aspect that I have not seen covered in the jamming analogy is the epoch-wise double descent.\n\nIn any case the discussion in the paper needs to be adjusted and these relevant relations to previous work corrected and moved from the supplement to the introduction.  \n\n** \"Informally, our intuition is that for model-sizes at the interpolation threshold, there is effectively only one model that fits the train data and this interpolating model is very sensitive to noise in the train set and/or model mis-specification.\" This intuition is correct I believe. However, it should not be called \"our intuition\" as it already appeared in the line of work by Opper cited above. \n\n** The authors present as another main result the fact that under comparable training conditions training with more data samples provides worse generalization, examples of this is also included already in the papers by Opper et al. cited above. "}