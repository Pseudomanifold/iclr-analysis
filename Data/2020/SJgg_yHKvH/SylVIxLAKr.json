{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper applies hyper-network idea to graph-neural networks and reports strong experimental results across different graph-learning datasets. More specifically, for each of three different GNN architectures (NMP-edge, graph isomorphism networks, and invariant graph networks) the paper identifies one particular MLP in the architecture and uses another network to assign the parameters of this MLP as a function of the input. What is novel here is that they use damping (i.e., averaging of the current input and the first input), so as to stabilize training. \n\nI have mixed feelings about this paper: although the theoretical contribution and novelty of the paper are minimal, using hyper-networks seem to significantly improve the performance of existing GNNs. That is why despite a lack of significant novelty in over 9 pages, I have voted for accepting this paper.\n\nAlthough there are typos and sometimes awkward sentences, the writing is generally clear. Since the proposed modification is minimal, the majority of the paper discusses the related works, where the changes are made to an existing GNN architecture. In addition to typos, there are some errors: for example, I think that the variant of the Invariant Graph Networks used in this paper uses matrix multiplication (along the first two dimensions) rather than element-wise multiplication stated in the paper. I also think that the paper should discuss why this use of hyper-networks in the architecture does not affect the invariance properties of the original networks.\n"}