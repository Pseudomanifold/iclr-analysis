{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "~The authors propose methods to apply hypernetworks to graph neural networks. They also introduce a Taylor approximation to one model with the hypernetwork to allow for stable training.~\n\nI think this work is a very interesting application of hypernetworks, but the contributions by the authors is limited in scope. Moreover, the paper is not clearly written and the results for improvement of the method are not conclusive.\n\nQuestions about the work in decreasing importance:\n\nHow many parameters do your hypernetworks have relative to the control models? How much does one need to tune the hypernetwork parameters?\n\nThe tables shown on pages 8 and 9 need considerable description that should make this work self-sufficient--someone shouldn\u2019t have to read another paper to understand your tables. \n\nTable 1 and Table 3:\n-What are the other Cormorant, Incidence, SchNet, and enn-s2s networks?\n-What are the Targets that you are predicting? Which one is challenging, or should we really care about improving?\n\nTable 4:\n\t-Almost all of your improvements are within the standard error of other models.\n\nTable 5:\n\t-Why don\u2019t these predictions have standard errors associated with them?\n\nI am still confused on the need for the Taylor approximation to the arctanh function. This needs to be explained in much further detail.\n\nWhat am I supposed to glean from Figure 1? What is BCH, POLAR, and LDPC ARRAY? What is the x-axis (EbNo(dB) and y-axis (BER)?\n\nIn section 3.1, what was the decision for the choice of hyperparameters and modeling decisions on page 4?\n\nWhy is \u201cc\u201d, the learned damping factor used throughout the paper learned as a scalar value, and not as the output of the neural network that employs a sigmoid activation as an output?\n\nIn the abstract and introduction, I am confused by the phrase: \u201cWe tackle this by combining the current message and the first message\u201d, especially since the word \u201cmessage\u201d has not been described in the abstract. Please clarify.\n\nOther notes that did not impact paper review:\n\nThroughout the paper, the word \u201cparameter\u201d is used instead of \u201cmetric\u201d, \u201cstatistic\u201d, or condition, e.g. from page 6: \u201c...the NMP-Edge architecture achieves state of the art performance on 9 out of 12 parameters, and in only one parameter it is outperformed by the original NMP-Edge model.\n\nIf the hypernetwork is changing due to input molecules or proteins, it would be great to show how the weights change to different .\n"}