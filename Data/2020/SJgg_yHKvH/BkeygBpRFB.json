{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors take four current graph neural network architectures: 1. The neural message passing networks of Jorgensen et al. 2. The invariant graph networks of Maron et al., 3. The graph isomorphism networks of Xu et al. and 4. The decoding hypernetwork of Nachmani & Wolf and extend them by adding extra layers to learn the wights of the network itself. Networks following this strategy are called hypernetworks and apparently they have shown to have good performance in other domains. \n\nIndeed, on the QM9 benchmark the modified architectures give promising results. Results are also presented on the MUTAG, PROTEINS etc graph classification datasets, but these are really too small to be relevant today. The authors also tried an error correcting code dataset, but the way the results are reported it is hard to understand how good they are.\n\nThe paper is a purely empirical study. Obviously the authors have put a lot of effort into experimenting with different architectures and improving the state of the art starting with four different architectures. However, the way the results are presented is confusing, so altogether the message is not clear. For example, there are two different tables for QM9. It seems like the NMP type algorithms are compared to each other in one table, and IGN type algorithms are compared to each other in the other table. However, it seems like the two sets of algorithms are not compared to each other, and even the units are different in the two tables. Even the units and the order of the targets is different in the two tables, which is super confusing. I don't know how well the GIN and DHN type networks would do on these benchmarks. So overall it is difficult to know what to make of all this."}