{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a means to uncover the modular structure of deep generative models of images using counterfactuals and presenting evidence for the fact that there are interpretable modules within current popular generative models. The paper is extremely well written with a good balance between mathematical notation and intuitive explanations. I think this paper should certainly be accepted as it provides an interesting and rigorous tool to understand the behavior and properties of deep generative.\n\nI have a few questions and comments:\n\n1) How early does this sort of modularity arise over the course of training? Does it vary for GANs versus beta-vae like models?\n\n2) I think it would be good to contrast this approach with ones that also have latent mixing strategies such as [1, 2] - these do not uncover latent modular structure, but can also produce hybridized images via linear latent space mixing unlike counterfactual assignment of latent vectors like in your work.\n\n3) From what I gather, this approach cannot produce hybridizations between two explicitly provided samples from a dataset, since the models you consider do not have an inference network like in BiGAN or ALI to get the corresponding z vector for a specific image. If this were available, it would be interesting to study influence maps estimated by taking the expectation over pairs of images rather than z-space vectors. \n\n4) Can we quantify modularity or the extent of disentanglement of internal representations under such a framework?\n\nReferences\n\n[1] Manifold Mixup - https://arxiv.org/pdf/1806.05236.pdf\n[2] Adversarial Mixup Resynthesis - https://arxiv.org/pdf/1903.02709v3.pdf\n\nMinor:\n\nProposition 2 - typo - \u201cthen and transformation applied to it\u201d"}