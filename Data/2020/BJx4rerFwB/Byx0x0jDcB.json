{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposed a weakly-supervised wMAN model for moment localization in untrimmed videos. Only the video-level annotation is available for training, and the goal is retrieving the video segment described by the sentence. The proposed model explored to utilize better context information and captured the relation between video and sentence/word via graph neural networks. In particular, instead of modeling the context information between the sentence and each video frame, wMAN tried to learn the representation with multi-level and co-attention, which considers all possible pairs between the word and the frame. The proposed model was evaluated on two publicly-available dataset and achieved reasonable results.\n\nPros:\n- Weakly-supervised method for video moment localization is a reasonable and important direction.\n- wMAN explicitly utilized multi-level context information between the sentence and the video frame, and used the graph neural network and the message passing to model the representation. I think this is a reasonable direction.\n- wMAN is evaluated with two publicly available datasets, and is compared with state-of-the-art methods and other \"oracle\" baselines. The performance is impressive and could be a better baseline for the future work.\n\nCons:\n- wMAN model the relation for all possible pairs of the word and the video frame. However, if the video is quite long, say 10 minutes, 30 minutes, or even few hours, will the method still be efficient and effective?\n- When building the relation between the word and the frame, is there any emphasis on verb, some particular word, or self-learned attention? For some particular word, say \"people\" and \"cup\", won't it have strong connection with many frames? But for some of the words, say \"hold\" and \"sits\", could it play a more important role?\n- Followed by previous question, in the qualitative results, it seems the boundary parts of the predicted video segments are less accurate. Is it because some of the words case these false positive results? What do you think the reason is?\n- Experimental results: I suggest the author to provide more ablation analysis to the experiment section. For example, the full model of wMAN works better than FBW on R@1, but worse on R@5 and R@10. Is there a particular reason about this? PE seems to be important for wMAN, and the authors provides few sentences analysis about this, but I don't think I fully understand this part. Another problem is that there is only few qualitative results, and in both these two examples, predicted results cover the GT segments. Is this always the case for wMAN? Why? Some failure cases could also be very helpful.\n- Less technical comments: The paper writing is fine to me, but I don't like the typesetting. I suggest to put the model figure more close to the methodology section and the qualitative results on page 8.\n\nOverall, I think the paper is marginal above the accept line."}