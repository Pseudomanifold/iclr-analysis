{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overview: \nThe authors proposed a weakly-supervised method to localize video moments given text queries.  The model builds multi-level relational graphs among pairs of word and video frame, and the graph is used to aggregate visual-semantic feature for each word and each frame. Then the attentive features are used to localize the sentence query in videos by calculating the similarity of words and frames. In summary, the proposed weakly-supervised Moment Alignment Network (wMAN) utilizes a multi-level co-attention mechanism to learn richer multimodal representations for language based video retrieval..\n\nPros:\n1. Significant performance improvement on Didemo and Charades-STA datasets. The authors achieved very good performance on both dataset, even higher than some of the full-supervision methods, such as CTRL and MLVI.\n\nCons:\n1. The overall novelty of the proposed methods is limited. Essentially, the key points of the model is hierarchical visual semantic co-attention.,which is proposed originally in [Hierarchical Question-Image Co-Attention\nfor Visual Question Answering], although the original application is VQA in image domain. So in this way, the novelty is only marginal.\n2. Paper writing can be improved. Figure 2 shows the overall structure of the model, however, the caption doesn't explain all the notations in the figure, such as WCVG, and the equations. Additionally, the reference is very far away from Figure 2, which makes the whole paper hard to read.\n3. For evaluation part, one important ablation study is missing: the number of steps T for message passing. This eval is important, as it shows the necessity of using \"multi-level\" attention.\n\nMinor comments:\n1. Make the caption of Figure 2 self-explainable, e.g. the meaning of LSE.\n2. There is a \"word-conditioned\" visual graph network, why not the other way, \"frame-conditioned\" semantic graph net and iterate over it?\n"}