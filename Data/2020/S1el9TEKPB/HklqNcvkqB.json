{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an algorithm to train neural networks combining sparsity and adversarial training. \nOn the sparsity inducing regularization, the paper proposes using proximal methods. \n\nOn the positive side:\n- The goal is of interest when it comes to real-world applications.  \n- There is an interesting analysis of the weights and how pruning / AT affects them. \n\n\nOn the negative side:\n\n- In my opinion, the introduction messes unstructured and structured. Weight sparsification is associated with structured while I do believe is unstructured. \n\n\n- Related work does not seem very comprehensive. The paper claims novelty on using RGSM to improve performance. How is this different from the formulation used in \"Learning the Number of Neurons in Deep Networks, Alvarez and Salzmann NIPS 2016\".\n\n\n\nExperimental settings:\n- One thing that got my attention is the threshold for pruning weights (1e-15). I think that is not a fair value. There are related works suggesting no loss in accuracy if the threshold is ~1e-5 (y. Sparse convolutional neural networks CVPR2015). I do believe the numbers would change drastically. \n\n- The comparison between ADMM and the proximal is unfair if using that threshold. The proximal has an implicit thresholding Eq. 6. \n\n- What is the goal for the ensembles of the small networks? where are the numbers?\n\n\nMinor stuff:\n- In sparsity and robustness, there are some works missing (as a typo). \n- On the method, there are two parts where I am confused. What is the aim for including all-around Eq. 4? The same with the theoretical guarantees. "}