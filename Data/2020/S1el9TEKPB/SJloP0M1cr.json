{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper presents an observation that Feynman-Kac formalism principled ResNet ensemble [1] yields sparser network weights compared to those of standard Resnet.  Based on this observation, the authors combine the ensemble model with Lagrangian based sparsification methods to obtain sparse and robust models. \n\nPros:\n- Obtaining sparse and robust networks is an important and challenging problem that could be of interest to a large audience.\n- The paper provides an interesting observation that EnResNet [1] yields sparser network weights compared to standard ResNet, and further leverage it to obtain sparse and robust models. \n\nCons:\n- The paper has limited novelty. It has already been shown in the original EnResNet paper [1] that EnResNet is more robust to adversarial attacks. Thus the only additional contribution of this paper is the observation that EnResnet is more sparse than standard ResNet, and combination of EnResnet with sparsification methods.  \n- The paper is not well justified on why one should use sparsifying techniques such as RVSM, RGSM with the Feynman-Kac formula principled EnResnet. The authors state that this enables sparsity to meet robustness; however, in all experimental results, the robustness actually decreases with the increase in sparsity which is opposite to the claims made in the paper. \n- The authors only report robustness on white-box gradient-based attacks. Thus it is not clear whether the method will generalize to black-box/gradient-free attack approaches such as NAttack [2].\n\nMinor comments:\n\n- Why didn\u2019t you report the accuracy on the clean examples? This is important in showing that the method generalizes well to clean examples while maintaining robustness.\n- Why use only 20 iterations to evaluate the attack? Will the model maintain its robustness with the increase in the iteration and epsilon? \n- Figures are not referenced anywhere in the text.\n- It would be better to put the results on CIFAR100 in the main paper rather than in the appendix.\n- Page 2, \u201clower cases\u201d  -> \u201clower case\u201d.\n- Page 2, \u201cRelated Works\u201d -> \u201cRelated work\u201d, it\u2019s better to have it as a separate section.\n\nOverall, while the paper provides an interesting observation, it has limited contributions due to lack of novelty. Further, inadequate experimental validation makes it difficult to see if the claims made in the paper are actually true. Thus I believe that this paper requires substantial improvements in order to be accepted to top-tier publication venues such as ICLR.\n\nReferences:\n[1] Bao et al. 2018, https://arxiv.org/abs/1811.10745 \n[2] Yadong et al. 2019, https://arxiv.org/abs/1905.00441"}