{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes using proximal steps as a replacement both for traditional operations in neural networks as well as strategy for expressing more exotic computations that are carried out in learning. Since proximal steps are the solution to a (usually simple) minimization problem, argmin differentiation techniques provide a general strategy for computing gradients with respect to their inputs.\n\nIn general, the techniques proposed in this paper are interesting and well motivated. The techniques provide strategies for deriving new and principled unsupervised learning objectives (deep CCA), reconceiving of and analyzing existing techniques (dropout), and support working with kernelized representations. Empirical evaluation on fairly small scale experiments demonstrates these techniques can productively be employed to develop new and powerful models\n\nConcerns:\nthere is, in my opinion, rather too much content for a conference paper of 8 pages. Fpr instance, the kernelization that the authors propose is rather orthogonal to the general techniques proposed, and its affordances are not analyzed empirically\nSome of the mathematical claims are too tersely described for most readers (keep your audience in mind!). One example occurs at the bottom of page three when discussing kernel warping; this relies heavily on previously published work, but adequate space does not exist to make the connections to the novelty of this work clear. This again suggests that a journal publication which offers authors and reviewers a more careful back-and-forth might be more appropriate.\n\nA claim and some missing citations:\n\n\u201cbut surprisingly underutilized in modeling deep neural networks\u201d - there has been a small amount of work, but it\u2019s not completely unsurprising since the theory of proximal mappings only really makes sense in the context of convex problems. Like many things in deep learning, it may turn out to be the sort of thing where we can ignore the conventional theory, but that\u2019s not a foregone conclusion. However, there are some other antecedents in the literature which do suggest that proximal-like operators do play an important role in the development of techniques in deep learning. Some relevant related material are the so-called \u201csparsemax\u201d and \u201csparsemap\u201d operations, which conceive of softmax (and marginal inference) in terms of a dual optimization problem. However, this work alters the dual form to recover sparsity inducing optimization problem; however, the techniques for deriving derivatives are the same and speak to the general flexibility of viewing the building blocks of neural networks as solutions to optimization problems rather than as less analyzable functions. Two relevant papers are: Martins and Astudillo (2016 ICML) and Niculae et al (2018 ICML).\n"}