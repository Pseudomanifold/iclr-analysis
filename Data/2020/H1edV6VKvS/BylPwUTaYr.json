{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes looking at many existing and some new layers on neural networks as proximal operations  (where a proximal operator is defined as  P_f(z) = argmin_{x\\in C} f(x) + 1/2 ||x-z||^2). Trivially many existing layers can be written in this way, but the model can be applied to novel fairly interesting setups. \n\nThe analogies and extensions of existing methods are overall very dense and hard to follow, specially with the amount of liberties and approximations spread throughout the paper. For example, at one point when talking about kernel spaces the paper suggests that a proximal operator computes X^-1 y and draws a close analogy to X y, \"modulo the exponent in X\", when in fact multiplying something by a matrix or its inverse tend to have quite the opposite effect (unless the matrix is its own inverse, which does not seem to be the case here). Similarly in the dropout section a b^2 is treated as the same as (a+lambda)^2 b^2, which again has quite different behavior (and IIUC the algorithm does not work with lambda=0).\n\nOverall it's very hard to believe that the proximal methods which claim to approximate and extend existing methods do indeed approximate and extend those methods, at least from reading the mathematics of the paper.\n\nThe experimental section does show that the methods perform reasonably well, and the two particular variants evaluated in the paper body proper are two of the better-justified one. I would strongly encourage the authors to substantially rework the paper to remove the more dubious equivalence claims (or, instead, strengthen them) and focus on simplifying the explanation and derivation of the things which do perform well.\n\n"}