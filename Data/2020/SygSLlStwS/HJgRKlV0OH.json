{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a novel approach to reformulate the meta-RL problem as model identification with a gradient descent based algorithm. Such innovation not only allowed us to perform meta-learning via supervised learning of the model, which is more stable and sample efficient, but also allowed us to leverage off-policy RL to learn the policy instead of meta RL.\n\nPros:\n1. Paper clarity. Although the submission had a few typos (I am not a native English speaker, but I'd encourage the authors to polish the writing of the paper), it's a very well-written paper overall. The flow and logic of this paper was clean, and the authors stroke a good balance between being focused about the core contribution of the paper, and reviewing related work and introducing sufficient preliminaries. As a result, I think this paper was accessible to both domain experts and the broader ICLR community. \n\n2. Novelty. This paper proposed a novel approach to reformulate the meta-RL problem as model identification with a gradient descent based algorithm. To the best of my knowledge, this was the first paper broke the meta-RL problem into a simpler meta supervised learning problem and an off-policy RL learning problem. Although each component of the proposed solution was not new, e.g., \"relabel\" was used in Dyna, MAML was first introduced in 2017, the combination of each component to address the meta-RL problem seemed to the novel to me. And I think the idea could be interesting to the ICLR community. \n \n\nCons:\nIt's weak accept rather than accept from me because of how the empirical evaluation were conducted in the paper, and I think the experiments conducted in the paper were a little bit weak (common for most ICLR submissions). Examples:\n\n1. Number of gradient steps is an important tuning parameter for MAML, it would be interesting to discuss number of gradient steps within the context of MIER.\n2. It might be useful to conduct some qualitative results to understand the model learned with MIER against the baselines, e.g., how well MIER adapt to the out-of-distribution tasks with simulated data points (examples o such qualitative studies could be found, say, in Finn et al., 2017).\n3. Given the fact that one major contribution of this paper was reformulating the meta-RL problem as model identification, it would be useful to conduct some quantitative study to help the readers understand the effectiveness of learning the environment model p(s\u2019, r|s,a) compared to ground-truth, and how the quality of the learned environment model made an impact on the overall performance of the model.\n4. Some implementation details of MIER were missing, I don\u2019t feel confident about how reproducible this research would be. For example, the specification of both environment and policy models were not discussed in the paper.\n5. In general, it would be useful to conduct more experiment results on more diverse data sets, say, in a supplement material.\n\n\nA few questions to the authors:\n1. Section 3.2: I assume the expectation should be taken w.r.t. p\u2019 rather than f?\n2. In Algorithm 1 & 2, how was the adapted context \\phi_T used to update policy \\psi? Was it as input to the model parametrized by \\psi? It might be useful to make it clearer.\n"}