{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n-------------\nThe authors propose an algorithm for meta-rl which reduces the problem to one of model identification. The main idea is to meta-train a fast-adapting model of the environment and a shared policy, both conditioned on task-specific context variables. At meta-testing, only the model is adapted using environment data, while the policy simply requires simulated experience. Finally, the authors show experimentally that this procedure better generalizes to out-of-distribution tasks than similar methods.\n\nMajor comments\n--------------\nMaking meta-rl algorithm generalize better outside of the meta-training distribution is a relevant open problem, and this work proposes nice ideas towards its solution. The paper is well-organized and easy to read. The idea of reducing meta-rl to a task identification problem is not completely novel since some recent works have been proposed in this direction (see later). Anyway, the proposed approach is interesting and seems (at least from the proposed experiments) effective. My main concerns follow.\n\n1. Though they attempt to address all relevant questions about the proposed approach, I found the experiments quite weak. Only two Mujoco domains are used for the standard meta-rl experiment, and only one of them (HalfCheetah) is used to test the out-of-distribution capabilities. Regarding the first experiment, MIER always performs comparably or worse than PEARL. What is the intuition behind this result? Does it suggest that MIER is paying additional sample complexity in \"in-distribution\" tasks in order to be more robust to out-of-distribution ones? On the other hand, the generalization experiments seem much more promising, but I would like to see more (at least the humanoid robot as well) to confirm that this result is not only a specific case of this domain. Furthermore, from Figure 3 it seems that MIER improves over PEARL even on in-distribution tasks, while it performed significantly worse in Figure 2. Why does this happen?\n\n2. Related to the previous point, I did not find any description of the parameters adopted in all experiments (learning rates, batch sizes, etc.). I do not believe I would be able to reproduce the results at the present time.\n\n3. The proposed method is somewhat related to other recent works [1,2]. In particular, [2] presents similar ideas, where the authors meta-learn a fast-adapting model (actually, a task encoder) and a shared universal policy conditioned on the task representation. The main focus is still to improve the generalization to out-of-distribution tasks. Can the authors better discuss the relations to these works?\n\nMinor comments\n--------------\n- In the introduction: \"Effective model training requires the validation batch to contain data corresponding to optimal behavior for the tasks...\". Why? In principle we could train a good model of the environment by running a sufficiently-explorative policy.\n- In the related works: \"Our method does not suffer from this problem since we use our model to train a model-free policy\". It is not clear why (though it becomes later) since simulating long trajectories from a learned model could lead to the usual divergence issues.\n- In Sec. 3.2: r in \\hat{p} should not be bold. Also, \"f\" in the subscript of the expectation was not defined (is it \\hat{p}?).\n- In Sec 3.4: there is a minimization over \\phi which however does not appear in the objective.\n- In the optimization problem at page 5: \\phi_{\\Tau} should probably be \\phi.\n- In Fig. 2, why is MIER run for less steps than the other algorithms?\n\n[1] Humplik, J., Galashov, A., Hasenclever, L., Ortega, P. A., Teh, Y. W., & Heess, N. (2019). Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424.\n[2] Lan, L., Li, Z., Guan, X., & Wang, P. (2019). Meta Reinforcement Learning with Task Embedding and Shared Policy. IJCAI 2019."}