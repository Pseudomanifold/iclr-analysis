{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes learning physical derivatives, the derivative of the trajectory distribution with respect to policy parameters. The proposed method estimates changes in  trajectories at a particular theta by using finite differences,\nthen fitting Gaussian Processes per timestep to generalize to new dtheta's. The paper then proposes techniques to robustify the process against noise in the system.\nTo deal with temporal noise, where trajectories are approximately equal up to a time shift, they simply\nestimate the optimal shift and use the shifted version to estimate. To address more complicated noise, they assume sensitivity of trajectories to noise is small relative \nto sensitivity to the parameters, and discretize the state space at a level such that trajectories that \ndiffer primarily due to inherent noise look the same at the discretized level, while perturbed policy \nparameters still lead to different trajectories. They then use the discretized trajectories to estimate \nthe finite differences.\n\nExperiments illustrate how the learned predictions compared to actual resulting perturbations and \nillustrate resulting trajectories on certain toy domains and a physical robotic finger. All experiments\nare done with very low dimensional state and policy spaces (1-3 dimensions each). \n\nWithout much more extensive experimental validation, this paper should be rejected. While I am not aware \nof any prior work on learning physical derivatives, the actual methods used are not novel in of themselves\nbeyond being applied towards learning derivatives with respect to the policy. As such, the method should be\nof practical interest in order to be accepted. With limited experiments on a single very low dimensional \ndomain and no comparisons against any alternative methods, there is little evidence demonstrating the actual \neffectiveness of the proposed method, especially on more complex domains and for downstream tasks.\n\nSuggested Experiments:\n\n- Stability in gradient estimation\nIt seems like it could require huge amounts of samples to be able to estimate gradients at parameters \nwhere for which the system is not very stable, as states in at later timesteps can easily change in \nhard to predict ways as the dynamics are propogated through time. This should be an especially big issue \nif we do not already have good stable controllers close to the desired solution and needed to actually \nconduct exploration in parameter space to solve a taask. I would appreciate more extensive evaluation \nacross multiple different (simulated) domains and assessing the effectiveness of gradient estimation \nalong random parameters.\n\n- Dimensionality of policy parameters and state spaces\nThe current experiments only involve very small parameter spaces. It would be important to see how well\nusing finite differences and GP regression scales with a higher dimensional search space, which can be \ndemonstrated on varying dimensionalities of an LQR system for example. It would also be important to see\nhow gradient estimation scales with high dimensional state spaces even with a small parameter space (like\nin the PD controller experiment in the paper). \n\n- Direct Comparison against learning dynamics models\nUsing the same data, compare (with same metrics as in table 1) physical derivatives estimated with the \nproposed approach against learning GP dynamics models and rolling out the perturbed policy with the learned\nmodel. Without a direct comparison against learning dynamics models and understanding what situations \nlearning physical derivatives provides better estimates, it is unclear when or why one would prefer to learn \nphysical derivatives in this way compared to a model based approach. \n\n- Quantitative results measuring costs of learned controllers\nDespite the name of the paper and a description of how to compute a policy gradient via physical derivatives, \nthere are no experiments involving such policy gradient updates as far as I can tell. While one advantage of the \nmethod (as well as model based approaches) is the ability to learn in unsupervised manner, it would be extremely \nhelpful to validate how well the physical derivatives are estimated in terms of how useful they are for a downstream \ntask, such as optimizing a controller for a cost function. Right now, experimental results lack any comparisons\nto other methods or any other way to assess the effectiveness of estimating physical derivatives. \nA comparison against regular RL policy gradient methods (or other model free algorithms) and model based RL \nwould give an idea as to whether the physical derivatives learned are actually useful.\n\nOther questions and comments:\n\t- Table 1: is this evaluating the accuracy of the physical derivatives on the shaking data that it was\n\t  used for learning, or on a validation set? If on a validation set, would the validation perturbations be\n\t  drawn from the same distribution as the training set?\n\t- The zero shot planning experiment in section 4.4 seems very contrived. It does not seem like a useful task\n\t  to adjust the parameters of the PD controller in order to reach a state that isn't the target. The figures \n\t  illustrating trajectories are also not very convincing and unclear. Two points are labelled source state and\n\t  target state, but it is not clear which is the intermediate state it is supposed to reach. In any case, most \n\t  of the trajectories seem to vastly overshoot the target? final state, and it is hard to assess how close the\n\t  trajectories end up being to the intended states from a 2d representation of a 3d space. Quantitative results\n\t  would perhaps have been more useful in illustrating the effectiveness of using physical derivatives.\n\t- What is the purpose of figure 4? It does not appear to be referenced in the text and it is not clear what is\n\t  being shown.\n\t\nOther notes not part of decision:\n\tPaper exceeds the 8 page recommended length\n\tLots of small typos in the text\n\n\t"}