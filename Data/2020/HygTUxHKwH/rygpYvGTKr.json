{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper proposes Qgraph, an algorithm that  addresses the problem of extrapolation error that appear in RL tasks with continuous action spaces. The authors describe a method to construct a graph from transitions generated by some policy. In this graph nodes correspond to states and (s, a, r, s\u2019, t) define transitions between these nodes. Then this representation is simplified and  used to compute Q-values using methods for tabular MDPs.\n\nThe related work section is missing several methods that attempt to address the same problem. Batch Constrained Q-learning (Fujimoto etal, 2018) introduces a formulation of Q-learning that constrains action selection with a generative model trained on a replay buffer in order to omit unseen actions. BEAR-QL (Kumar etal, 2019) describes a similar approach that uses a hard constraint based on MMD. It would be interesting to discuss connections with the recent work on off-policy batch RL.\n\nThe clarity of the paper can be improved. In particular, I have several questions regarding the method:\n1) How are node of the graph are constructed? Does one state correspond to a single node or several states are merged into a different node?\n2) How the actions are selected?\n3) What are the assumption regarding the initial state distribution? Does the set of initial states have to be finite?\n4) If two similar states appear in different branches of the graphs, are the corresponding nodes merged or not?\n5) If the considered environments are deterministic, what is the motivation for stochastic approximation of dynamic programming?\n\nThe approach has several major limitations. One of the main limitations of the approach is that it can be applied only to deterministic tasks. Although it is not stated clearly in the paper, it seems also requires to have a finite set of initial states. \n\nThe experimental evaluation is performed on a limited set of tasks and it is rather unclear whether the method can be scaled to higher dimensional control problems.\n\nOverall, I feel that the paper needs to be significantly improved. \n"}