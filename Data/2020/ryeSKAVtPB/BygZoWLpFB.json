{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors consider the problem of generating adversarial attacks on image classifiers. Going beyond the standard setting (where the goal is to flip the output label for any given input into an incorrect one), the authors propose to design attacks such that the output top-k predicted labels are randomly selected and ordered (and exclude the correct label).\n\nTwo attack strategies are proposed: one being an extension of the Carlini-Wagner (CW) attack, and one being an adaptation of the distillation approach of Hinton et al that \"hides\" information in the probability distribution corresponding to lower-ranked logits.\n\nThe paper makes an interesting first step in a new direction within the area of adversarial attacks. Unfortunately the paper is very confusingly written and requires a thorough revision, which I fear may be beyond the scope of the rebuttal period.\n\nFor one, neither attack algorithm is clearly described, leaving the reader to guess most of the details; clear pseudocode descriptions would help (since they are new after all). In particular, the \"knowledge-based adversarial distillation framework\" remains completely mysterious to this reviewer. P^AD seems to be...hand designed by manipulating Glove embeddings of category labels?? [If so, then what should one do if there is a completely new application where the labels do not have regular meaning?]\n\nMoreover, the experimental setup is confusing. In multiple places (starting from page 4 near Eq (5)) the authors refer to compute budgets such as \"9 x 1000\", which this reviewer interprets to be 9 steps of binary search over lambda with 1000 gradient steps over the relevant loss function. But how can this be a reasonable measuring stick when (a) the range of the binary search is never defined, and (b) the quality of the optimization not only depends on the number of steps, but also on other quantities such as the stepsize? Moreover, why should binary search over lambda be a good strategy at all? Since all the results are based on this setup I do not know how to evaluate the strength of the proposed results. "}