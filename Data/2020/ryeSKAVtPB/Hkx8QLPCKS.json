{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposed using a technique called adversarial distillation for crafting adversarial examples targeting the top-k predictions instead of the commonly used top-1 prediction in the literature. The authors generalized the Carlini-Wagner (CW) attack loss to the top-k setting and use it as a baseline comparative method.  Given the same number of iterations in the attacking algorithm, the experimental results on ImageNet show either better attack success rate or lower distortion measured in L1, L2 and Linfinity norms.\n\nAlthough studying adversarial attacks beyond top-1 prediction is interesting, I have several concerns about this submission.\n\n1. Novelty and contributions: In terms of technical contributions, generalizing CW loss from top-1 prediction to top-k prediction is rather straightforward. What is most interesting from the submission is that using the proposed adversarial distillation technique + top-k CW attack loss seems to make the attack more effective. But the authors did not provide a satisfactory explanation on why adversarial distillation can give a better attack. Most importantly, what can the research community learn from this attack? Will adversarial training with it gives more robust model?  The contributions seem limited without discussing and providing new insights in improving adversarial robustness.\n\n2. Lack of justification for the smaller perturbations: Since the proposed attack is basically the top-k CW loss + the adversarial distillation regularization loss, it is counterintuitive to see the perturbation norms of CW attack are consistently larger than the proposed attack, even for the L2 norm. Intuitively, adding an additional regularizer to the objective function will incur a trade-off. In this case, larger perturbation norm of the proposed should be expected. Please clarify and justify why the proposed approach will produce smaller norms.\n\n3. Are attack iterations sufficient? Based on Table 2, it's apparent that by increasing the number of iterations per each binary search, the performance of CW attack and the proposed attack becomes much closer. Can the authors provide more results with increased iterations and more binary search steps? I wonder there may be no performance gaps (or CW attack can perform better) if we increase the number of iterations and binary search steps. \n\n4. In page 2, the statement \"But, the three untargeted attack approaches are much better in terms of pushing the GT labels since they are usually move against the GT label explicitly in the optimization, but their perturbation\nenergies are usually much larger.\" is unclear to me. Are the authors suggesting untargeted attacks will have larger perturbations than targeted attacks? If so, the conclusion does not make sense."}