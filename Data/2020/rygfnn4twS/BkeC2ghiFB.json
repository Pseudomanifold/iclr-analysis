{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes a new method for quantizing neural network weights and activations that uses deep reinforcement learning to select the appropriate bitwidth for individual kernels in each layer. The algorithm uses a reward function that weights accuracy of the quantized model with latency, energy, and FPGA area, and leverages a high level and low-level controller to create quantized models that can take into account these factors. Compared to prior approaches taht only perform layer-wise instead of kernel-wise quantization, the quantized models can achieve better performance, or latency.\n\nWhile the problem of more effectively quantizing neural network weights and activations is interesting, I found this paper hard to follow and evaluate. The proposed hierarchical deep RL method mentions a number of tricks and design decisions that are not ablated, and the notation and text around the method were difficult to follow. There were also no baselines comparing to other approaches for performing kernel-wise quantization (e.g. random search, quantize based off variance, etc.). Without improved baselines and text I would recommend rejecting this paper.\n\nMajor comments:\n* Based on the analysis of kernel-wise results, it seems that a very simple strategy that chooses QBNs based on weight variance could be sufficient to achieve good performance. However, there\u2019s no comparisons to these kinds of heuristics or even to random search over QBN per-kernel. There are also no ablations that study whether the hierarchical-RL based approach beats a baseline that just directly predicts the kernel-wise QBNs independently.  Without these baselines, it\u2019s hard to know how well AutoQ works.\n* The text that details the hierarchical RL approach with multiple controllers that is core to the new AutoQ approach is extremely hard to follow, with many undefined symbols.\n\nMinor comments:\n* FIg 1: where in the neightwork do these weight kernels come from? What network/dataset?\n* You repeatedly state that ML experts obtain only sub-optimal results at selecting QBNs, can you cite something for this? What if you give experts access to additiional information like visualizations of weight distributions?\n* What\u2019s the tradeoff between using different QBNs for each kernel and specifying this number? Is there additional memory overhead?\n* Table 1: hard to read the exponent in kernel-wise math, add parentheses\n* Please use \\citep vs. \\citet where appropriate\n* Why not also search for kernel-specific activation quantization? \n* The notation in Fig 3 and surrounding text is really hard to follow, e.g. w_w, h_w, and indices into states (roundup could be ceil, names are confusing iRd, eRd?).\n* Eqn 2 comes out of nowhere\u2026 why optimize for log(accuracy) vs accuracy? How do you choose the user-defined constants? Decay factor?\n* Hardware overhead estimator: are these models accurate independent of QBN?\n* Why do you have to perform Gaussian augmentation? I didn\u2019t follow the re-labeling of transitions after Eqn. 6, is this needed?\n* Define \\mu when you first use it (Eqn 4?)\n* Is \\delta_a in implementation details the same as \\sigma around Eqn 4?\n* Table 3: Is there variance in these results based off fine-tuning? Could you include error bars or stderr on these estimates?\n* Figure 6: are the search spaces different for these different approaches? I.e. does AutoQ perform better due to the upper/lower bounds you set on QBN?"}