{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nSummary of the work\n\nThis paper proposes to automatically search quantization scheme for each kernel in the neural network. Importantly, due to a large amount of search space, hierarchical RL was used to guide the search.\n\nStrength\n\nFirst of all, I really liked the idea about more detailed search for kernel-wise configurations. It is interesting to see that different kernel settings has different best bitwidth. More importantly, it is great to provide cost model based estimations for cost/latency.\n\nThe authors also provided detailed discussion about weight bit distributions and average quantization bits among the layers. \n\nThe author provided implementation on real world FPGAs using bit-serial spatial architecture, which plays particularly nice with the algorithm. This something that should be highlighted in the discussion.\n\nWeakness\n\nFirst of all, I would love to see more detailed discussion about the corresponding hardware support implications. In particular, given the need of re-using computing resources, kernel-wise quantization has to use the bit-serial version of architecture(otherwise the MAC cannot be reused and we have to layout the entire network on FPGA), which may limit the applicability of the methods to higher number of bits.\n\nThe second potential weakness is the close relation between the method and HAQ. The method feels like a straight-forward extension(with DRL added). What would happen if you directly apply HAQ\u2019s method to the kernel-wise search space?\n\nFinally it would be great if the authors can clarify more about the FPGA setups(number of accumulators being used, whether there is reuse of compute unit in FPGA, or did you just layout everything spatially).\n\nQuestion:\n\nHow do you handle layers like BatchNorm(which normally need floating pt)?\n\nOverall, I find this paper provides interesting insights and solid evaluation and should be accepted to ICLR.\n"}