{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a method for network quantization. Similar with the work of \"HAQ: Hardware-Aware Automated Quantization with Mixed Precision\"(CVPR 2019), the proposed method is based on reinforcement learning. The contribution of the work is on the kernel-wise quantization, i.e., assigning different bitwidth to different kernels in one layer. And in the experiments, the proposed method clearly outperformed the state-of-arts of network-wise and layer-wise quantization methods.\nAlthough the high level idea is presented very well, some essential parts of the paper is a little bit hard to follow.  The motivation of using the hierarchical DRL is unclear.\n\nQuestions:\n\nWhy a hierarchical DRL agent is desired for kernel-wise quantization? Is it possible to modify the definitions of state and action of HAQ for kernel-wise quantization, which seems to be a much simpler solution for the task?\n\n\nWhat is the definition of iRd [L_i,K_j] in the Intrinsic Reward?\n\n\nIt seems the original HAQ used simple quantization way instead of LQ-Nets, which is different with the experiment setting in this paper. If I'm correct, does the change affect the HAQ's performance for fair comparison?\n\n\n"}