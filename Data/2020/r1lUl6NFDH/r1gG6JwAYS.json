{"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "A good paper that uses the Mirror Descent paradigm for learning quantized networks.  \nThough Mirror Descent is not their original idea, but using it in the context of learning quantized network is novel and interesting.  \nEmpirically, they showed better results than existing method, with comparisons with reasonable baselines including using relaxed projected gradient descent.  \n\nOverall, I don\u2019t have much concerns, but here are some more specific comment/questions (most relates to writing)\n\nIn the intro, it would be great to mention some past success on using MD, as opposed to just saying it\u2019s well-known. Also you mention MD can be used for more than quantization, but compression in general, it\u2019d be better to add that discussion, or remove this sentence. \n\nIn the beginning of Section 2.1, it'd be easier for the readers to make clear that the primal space corresponds to the quantized weights and the dual space corresponds to the unconstrained space in the rest of the paper.\n\nAt the top of page 3 you describe MD for the first time, but it\u2019s unclear to me how y^0 is handled.\n\nThe end of section 3 and section 4 talk quite a bit about STE, maybe it'd be clear if the authors can provide a concise description.\n\nAs someone not super familiar with NN quantization, this work seems like a good contribution.  My only possible concerns would be somehow comparisons to existing methods are not comprehensive enough (if this will be pointed out by the other reviewers)\n\n"}