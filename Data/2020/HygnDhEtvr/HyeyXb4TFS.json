{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes two modules to improve the performance of the Natural Question Generation task: (1) deep alignment network and (2) passage graph embeddings. The idea of generating passage graph is novel. The authors experiment with SQuAD and the numbers look good.\n\nI have a few questions regarding the model and experiments.\nFirst, a reasonable baseline could be using Transformer-based sequence to sequence model. Could you fine tune the embedding of CLS token and use that as a summary of the document? It seems that the construction of the passage graph is basically sparsifying a multi-head attention in the BERT model. I think you should justify why graph-structure is important in your experiment.\n\nSecond, if the Graph2Seq is particularly important for Natural Question Generation, the author should clarify it more. If the Graph2Seq model is generally applicable to replace the Seq2Seq model, the author should experiment with more tasks. The paper seems not well motivated.\n"}