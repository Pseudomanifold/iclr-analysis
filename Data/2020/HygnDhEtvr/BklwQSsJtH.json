{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The authors propose a Graph-to-Sequence Reinforcement Learning Model for Natural Question Generation, evaluated on SQuAD benchmark in for Question Generation. An interesting aspect of the work is related to the Graph2Seq model, and the use of the Reinforcement Learning to fine-tune the model. The latter stage seems to improve the structure of the answers considerably. An interesting use of RL algorithm and apparently a good choice of reward functions.\n\nQuestions: in the combined loss used in the RL run:\n1. Have you managed to have a successful run with gamma = 1?\n2. I understand that the L_rl factor is computed based on the sampling, and the L_lm is computed based on the top variant from the nbest list?\n\n"}