{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary & Pros\n- This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi-head architecture for preserving the diversity of the ensemble.\n- The proposed scheme provides the same advantages of the ensemble in terms of uncertainty estimation and predictive performance, but it is computationally efficient compared to the ensemble.\n- This paper experimentally shows that multi-head architecture performs well on MNIST and CIFAR-10 in terms of accuracy and uncertainty.\n\nConcerns #1: Novelty of the proposed method\n- The multi-head architectures have been widely used in various settings, especially multi-task learning. As the authors mentioned, it also used for online distillation [1]. Although its goal is different from this paper, just applying such multi-head architectures seems to be incremental.\n\nConcerns #2: Insufficient experiments\n- To evaluate OOD detection quality, ID/OOD datasets should be stated and various metrics (e.g., AUROC) should be measured like other literature, e.g., Table 2 in [2]. Such OOD detection quality is important to evaluate the quality of uncertainty estimation.\n- This paper provides experiments on only small-sized 10-class datasets, MNIST and CIFAR10. To verify the effectiveness of the proposed distillation method, other large-sized datasets should be tested, e.g., CIFAR-100, ImageNet.\n- There is no ablation study on the effect of the number of size of heads in Hydra. To achieve similar performance to the ensemble, how many heads are required?\n\nConcerns #3: Week efficiency\n- As reported in Table 5, in the case of CIFAR10, Hydra has 14x more parameters and 6x more FLOPs. Despite such a large number of parameters, the performance gain seems to be incremental.\n- A comparison with an ensemble with M=14 models should be tested because this ensemble has the same number of parameters compared to Hydra with M=50 heads. I think it might achieve good performance on the evaluation metrics.\n\nConcerns #4: Incremental improvements\n- Accuracy gain is too marginal even Hydra uses 14x more parameters.\n- ONE [1] might be a stronger baseline because ONE achieves 94% accuracy on CIFAR-10 using ResNet32 with only 2~3 heads while Hydra achieves only 90% even it uses 50 heads. Moreover, since ONE has multiple heads, uncertainty estimation is also available. So it should be compared with the proposed method.\n- In OOD detection tasks, Hydra underperforms Prior Networks on 5 of 8 datasets (note that PN (2.60) is better than Hydra (3.11) in the case of MNIST (test)). To overcome this gap, the proposed method requires more parameters.\n\n[1] Zhu, Xiatian, and Shaogang Gong. \"Knowledge Distillation by On-the-Fly Native Ensemble.\" Advances in Neural Information Processing Systems. 2018.\n[2] Andrey Malinin and Mark Gales. \"Predictive Uncertainty Estimation via Prior Networks.\" Advances in Neural Information Processing Systems. 2018.\n"}