{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper is focused on solving sparse coding problems using LISTA-type\nnetworks. It discusses the weakness of the ``no false positive'' assumption in\nprevious works and the weakness results in underestimated code components. The\nauthors propose a ``gain gating function'' to mitigate the weakness. Moreover,\nthe paper incorporates another ``overshoot'' gating function inspired by\nmomentum-based methods. Both contributions are supported with theoretical and\nempirical results. Numerical experiments show that the proposed model is\nsuperior to previous works especially in cases with high measurement noises or\nill-composed basis matrix. The paper is well written and easy to follow, and the\nempirical results are impressive.\n\nI really like the relaxation of ``no false positive'' assumption as in real world\napplication, learning-based algorithms may find it difficult to keep satisfying\nthe assumption while maintaining good empirical performance. And this relaxation\ncontributes to guarantee the convergence in a more real scenario.\n\nI think in (Chen et al., 2018), the support selection technique serves to\nmitigate the problem of underestimated code components as it bypasses the\nthresholding function for codes with large magnitudes. The empirical results\nalso show that in many cases LISTA with support selection has comparable\nperformance. I don't know if LISTA with support selection also suffers from\nthis underestimation problem severely.\n\nQuestions:\n\n1. The theorems in Section 3.1 hold without overshooting mechanism, i.e. eta==1.\nProp. 2 says that for one step of iteration, the optimal updates requires\novershoot. When the gain and overshoot mechanisms are combined, however, will\nthe theoretical convergence still hold?\n\n2. I don't have a very good understanding of why GLISTA performs so well given\nill-composed basis matrix compared to previous works. Could there be some\n(intuitive) explanation as to this based on the theoretical results?\n\n3. More details about the training process should be included. For example, I\ncan guess the loss function used for the training. But it is confusing not\nmentioning it at all in the paper. Some might think the objective funciton in\neqn (2) but the convergence analysis is with respect to the ground truth sparse\nvector. Also, the training scheme and some hyperparameter selection should also\nbe stated at least in Appendix.\n\n\nOverall I hold very positive attitude towards this paper due to its theoretical\ncontributions and good empirical results."}