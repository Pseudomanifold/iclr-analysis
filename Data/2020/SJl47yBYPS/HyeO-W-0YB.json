{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The main contribution of this paper is a normalization scheme to avoid saturating the squashing function typically used to constrain actions within a bounded range in continuous control problems. It is argued that algorithms like DDPG and TD3 suffer from such saturation, which prevents proper exploration during training, while maximum entropy algorithms like Soft Actor-Critic (SAC) avoid it thanks to their entropy bonus. The main reason behind the success of SAC would thus be its ability to keep exploring throughout training, by avoiding saturation. A second contribution is a new experience replay sampling scheme, named Emphasizing Recent Experience (ERE), based on the idea that most recently added transitions should be given higher weights when sampling mini-batches from the replay bufffer. Combining both ideas leads to the SOP (Streamlined Off-Policy)+ERE algorithm, which is shown to consistently outperform SAC on Mujoco tasks.\n\nAlthough this paper presents interesting insights and very good empirical results, I am currently leaning towards rejection mostly due to missing some important empirical comparisons, which hopefully can be added in a revised version.\n\nThe first key missing comparison (IMO) is to the Inverting Gradients approach from Hausknecht & Stone (2016), which the authors know about since it is cited in the related work section. Note that in that paper, the problem of saturating squashing functions preventing proper exploration was already mentioned, although not investigated in as much depth as in this submission (\u00ab(\u2026) squashing functions quickly became saturated. The resulting agents take the same discrete action with the same maximum/minimum parameters each timestep \u00bb). Their proposed Inverting Gradients technique was found to work significantly better than squashing functions, which is why I believe it should be an obvious baseline to compare to.\n\nThe other important experiments which I think need to be added are simply to implement the proposed normalization scheme within DDPG & TD3 to demonstrate its usefulness as a standalone improvement over existing algorithms. This would strengthen the claim that \u00ab algorithms such as DDPG and TD3 based on the standard objective with additive noise exploration can be greatly impaired by squashing exploration \u00bb. Without this comparison on the same benchmark, it is difficult to fully grasp the impact of this normalization.\n\nFinally, regarding the ERE sampling scheme, I would appreciate to see SAC+ERE as well, to (hopefully) show that it can benefit SAC too (since this second contribution is orthogonal to the SOP algorithm).\n\nMinor points:\n\u2022\tI would tone down a bit the claims for \u00ab the need to revisit the benefits of entropy maximization in DRL \u00bb, since better exploration has always been put forward as a major benefit (\u00ab the maximum entropy formulation provides a substantial improvement in exploration and robustness \u00bb, as written in \u00ab Soft Actor-Critic Algorithms and Applications \u00bb). To me, what this submission shows is essentially that naive implementation of additive noise exploration in e.g. DDPG is very bad for exploration, more than uncovering some novel properties of SAC.\n\u2022\tBelow eq. 1: \u00ab the optimal policy is deterministic \u00bb => should be replaced with \u00ab there exists an optimal policy that is deterministic \u00bb\n\u2022\t\u00ab principle contribution \u00bb => principal\n\u2022\tThe normalization scheme does not appear in Alg. 1\n\u2022\tIn Alg. 1 there are a Q_phi,i and a Q_phi,1 that should probably be Q_phi_i and Q_phi_1\n\u2022\tThe results from section E in the Appendix should be mentioned in the main text\n\u2022\tIn Fig. 4f the y axis\u2019 label is a bit clipped"}