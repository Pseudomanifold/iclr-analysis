{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n# Summary\nThe paper identifies a problem with TD3 related to action clipping. The authors notice that SAC alleviates this problem by means of entropy regularization. Given the insight that action clipping is crucial, the authors propose an alternative approach to avoid action clipping in TD3, which is empirically shown to yield the same results as SAC. Surprisingly, with this improvement, even several parts from TD3 can be removed, such as delayed policy updates and the target policy parameters. In addition, a straightforward-to-implement experience replay scheme is proposed, that emphasizes recent experiences, which propels the proposed algorithm to achieve state-of-the-art results on MuJoCo.\n\n# Decision\nThis is a great paper: accept. The proposed Streamlined Off Policy (SOP) algorithm is thoroughly evaluated, ablation studies performed, code made available. Nevertheless, there are a few suggestions below that may further improve the paper.\n\n# Suggestions\n1) It is said that entropy regularization leads to action not being saturated in SAC. I feel that this causal relation is very indirect. Maybe SAC with entropy just discovers better policies that do not go crazy between extremes? For example, if you would leave the entropy term but remove tanh saturation from SAC, don't you think you would also get a bang-bang policy? Adding such an ablation study could further strengthen the argument that entropy leads to no constraint violation, if it turns out true.\n\n2) The Emphasizing Recent Experience (ERE) replay scheme seems reminiscent of sampling according to a distribution exponentially decaying into the past. It is said that physically shrinking the allowed sampling range by dropping old experiences is better because then very old experiences cannot be used at all. It would be interesting to see a comparison to sampling according to exponential distribution from the replay queue."}