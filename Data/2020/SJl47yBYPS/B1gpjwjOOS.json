{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors investigate the role of entropy maximization in SAC and show that entropy regularization does not do what is usually thought: in the examples they investigate, where the output of the policy network needs to be squashed to fit in the action space domain, squashing would result in having only action at the boundaries, but entropy regularization maintains some intermediate values, hence exploration. From this insight, the authors replace entropy regularization by a simpler normalization process and show equivalent performance with their simpler Streamlined Off-Policy (SOP) algorithm. Then they introduce a second \"Emphasizing Recent Experience\" mechanism and show that SOP+ERE performs better than SAC.\n\nA good point for the paper is that the entropy regularization  study is very nice, more papers in the field should show similar detailed analyses of internal processes. But the paper suffers from a few serious weaknesses:\n\n- The TD3 mechanism goes beyond the Double Q-learning (or DDQN) mechanism of Van Hasselt et al: it takes the min over two critics. This should be explained properly.\n- the title, abstract and introduction insist more on SOP, but performance improvement seem to result more from ERE. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.\n\nAbout gradient squashing issues, the authors main mention de gradient inverter idea from this paper:\n\n@article{hausknecht2015deep,\n  title={Deep reinforcement learning in parameterized action space},\n  author={Hausknecht, Matthew and Stone, Peter},\n  journal={arXiv preprint arXiv:1511.04143},\n  year={2015}\n}\n\nThe authors should also probably also cite (and read the latest arxiv version of):\n@inproceedings{ahmed2019understanding,\n  title={Understanding the impact of entropy on policy optimization},\n  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle={International Conference on Machine Learning},\n  pages={151--160},\n  year={2019}\n}\n\n\nMore local points:\n- \"without performing a careful hyper-parameter search\": so how did you choose these hyper-parameters? I see what you mean, but this is a very vague and slippery statement.\n- I do not find the 23*4 images in Appendix B much useful\n- Fig 3 seems to be repeated in Fig 4. Can't you just remove Fig 3?"}