{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper \"Unsupervised Data Augmentation for Consistency Training\" marries two recent ideas of \n1. \"Data Augmentation\" (DA) from supervised learning: The authors explore various methods for \"DA\" mostly inspired by much recent work such as Random image transformations, Backtranslation, and TF-IDF based word replacement.\n2. \"Consistency Training\" (CT) from semi-supervised learning: CT tries to minimize the divergence between the output distributions of the classifiers that are produced by adding noise to the input.\n\nThe key insight in this paper is that, data augmentation methods that work well during supervised training should also work equally well as the noise distribution for consistency training on unlabeled data. The authors support this claim empirically through the experiments in table 1 and 2. \n\nThe paper is well written and the authors present extensive comparative and ablation tests to demonstrate that their proposed method works well with both low and high amounts of labeled data.  This paper should be accepted into the conference."}