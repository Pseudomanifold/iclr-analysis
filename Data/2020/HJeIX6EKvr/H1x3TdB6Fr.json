{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n\nThe paper exploits the training dynamics of deep nets: gradient descent fits clean labels much faster than the noisy ones, therefore early stopping resembles training on the clean labels. The paper shows that early stopping the training on datasets with noisy labels can achieve classification performance higher than when training on clean labels (on condition that the total number of clean labels in the noisy training set is sufficiently large).\n\nThe paper also makes the point that noise introduced during data collection are different from artificially generated noise through randomly flipping the labels of a clean dataset. The latter is often done in the literature. The \u201creal\u201d noise is more structured and therefore it is easier to fit and less harmful to the classification performance.\n\nStrengths\n\nThe paper is well written. The results are very interesting even though they are very intuitive and simple.\n\nWeaknesses\n\nThe idea seems to be known. For example,\nBetter Generalization with On-the-fly Dataset Denoising\nhttps://openreview.net/forum?id=HyGDdsCcFQ\n\nThe paper talks about early stopping. As shown by the above paper, it is also a function of the learning rate. Please comment on what happens in the case of small learning rate and early stopping.\n\nIt would have been great to prove the theorem for deep learning. The result is limited to linear models with large number of random features.\n\nThe paper does not make it clear under what conditions early stopping prevents the model from memorizing bad labels.\n\nThe paper focuses on classification. Will the claims hold for other task types such as object detection, segmentation, etc?\n"}