{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a masking process to improve the pruning of DNN. In addition, the algorithm proposes to automatically allocate the pruning rates over layers. \n\nOn the positive side:\n- I do believe the main contribution is automatically allocating the pruning rates over the network. \n\n\n\n\n\nRelated works:\n- How does this relate to methods using gating to prune in the presence of residual layers or BN?\n\n- Paper claims multiple times that related works need a train-prune-retrain process which is only valid for post-processing works. \n\n- I missed sparsity promoting works related to training from scratch where similar masks are implicitly used during training (even not explicitly stated) to maintain the zeros in the network. At least in [1,2] the training time is the same as the time used for training a network from scratch (same claim as in this paper). \n\n\nMethod:\n- The mask is trained using a sigmoid function and claims this will be representative of the relevance of the 'neuron' within the layer. How is this really related?\n- Why the initialization of the mask is to the mean? I think I am confused there. For initialization, I would argue all the neurons/parameters are relevant, right?\n\n- The claim that this method is 'much simpler' is a bit subjective. I do not see why. Please elaborate. \n- I am not sure if the claim of pruning filters and not considering the bn layer is correct. I would tend to think that, if pruning a neuron, the corresponding BN module should be modified (that is, propagating the zero to subsequent layers). \n\n- I do like the extension to residual connections. It would be great to have more and clearer details on how is this done. Would this also apply to the UNET type of architecture?\n\n- What is the intuition behind Eq 4 and how it is related to the relevance of a parameter? \n\n- The extension to multiple metrics is of interest, however, there is little detailed there. How is the automatic allocation done? This is repeated in section 3.3 but details are missing. \n\n\n- My understanding is that the regularization multiplier is affected by the learning rate, therefore their effect is lower as the training progresses. In this case, seems the opposite, right? (page 6 before 3.4). \n\n\n- The part with the sparsity budget is interesting. What guarantees are that the newly enabled neurons are actually useful? Could it be possible that the budget suggested is not the right for the task at hand and, therefore, the additional parameters are not really relevant / needed? \n\n\n\n\n\nExperiments:\n\n- There is loss with little sparsity when it comes to Imagenet (15 and 17% pruning) does not seem very promising even the training time is similar. \n\n- Seems like the experiments and comparisons to L1-pruning are not very surprising. It would be nice to have more comprehensive numbers. For imagenet, if using Resnet-50, would be easier to compare to other numbers. \n- In the imagenet comparison, L1-pruning is the version-A of the paper. What about the others or why that particular model?\n- How are the actual groups made? \n\nMinor details:\n- Please improve figure 1. It is not easy to understand. Same with Figure 3. What is seen in Figure 4. \n- I do believe the WideResNet-28-10 number of parameters for BAR is not correct. \n- Section 4.2 is a bit overselling. I do not see 'much-higher' parameter sparsity. The claims are mostly valid for VGG type of networks in this particular setting. \n\n\n\n\n[1] Learning the Number of Neurons in Deep Networks, NeurIps 2016\n[2] Compression-aware training of DNN, NeurIps 2017\n"}