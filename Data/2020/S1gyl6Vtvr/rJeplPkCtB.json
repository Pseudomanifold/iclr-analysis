{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a framework for training time filter pruning for convolutional neural networks. The main idea is to use a trainable soft binary mask to zero out convolutional filters and corresponding batch norm parameters.\n\nPros:\n+ The proposed method seems relatively easy to implement.\n+ The quantitative results in the paper indicate that MaskConvNet achieves performance competitive with previously proposed pruning methods.\n\nCons:\n- Writing of the paper could be significantly improved. See some examples below.\n- The main thing that bothered me about the method was the usage of hard sigmoid. If a mask component ever gets into one of the flat regions it won\u2019t be able to escape. The authors propose a workaround which they call \u201cmask decay update\u201d. This approach looks quite hacky and I\u2019m not sure how easy it is to make it work in practice.\n\nNotes/questions:\n* Abstract: \u201celegant support\u201d -> \u201csupport\u201d\n* Everywhere in the text: Back-to-back citations should have the form (citation1; citation2; \u2026)\n* Section 1, paragraph 3: \u201csuffer one\u201d -> \u201csuffer from one\u201d\n* Section 1, paragraph 4: \u201cabove mentioned\u201d -> \u201cabove-mentioned\u201d\n* Figure 1: The figure would greatly benefit from a detailed description. What\u2019s IF, OF and OF? The reader shouldn\u2019t be guessing. \n* Section 2, paragraph 3: \u201ccorresponded\u201d -> \u201ccorresponding\u201d\n* Section 3.1, paragraph 2: \u201cW \\in R\u201d \u2013 W is probably not a scalar value therefore it\u2019s in R^n. The same goes for the mask.\n* Section 3.1, paragraph 2: \u201cIt\u2019s easy to know ...\u201d \u2013 this sentence needs to be rewritten, e.g., \u201cOne can see that \u2026\u201d\n* Section 3.1, paragraph 2: \u201csparser\u201d -> \u201cmore sparse\u201d\n* Section 3.2, \u201cExtension to Multi-metrics\u201d: \u201cFLOPs\u201d are never defined in the paper. How is this quantity computed exactly? I\u2019m also not entirely sure how useful it is to introduce multiple lambdas \u2013 it seems that this case corresponds to a single lambda which is a sum \\lambda_i.\n* Section 3.3, paragraph 1: \u201cundersparsed\u201d, \u201coversparsed\u201d \u2013 not sure if these words exist. Maybe rephrase instead of introducing new terms?\n* Section 3.3, paragraph 1: \u201cvery laborious\u201d -> \u201claborious\u201d\n* Figure 3: Why not show points all the way to 0 sparsity?\n* Section 4.2, CIFAR-10: The authors mention that (Lemaire et al., 19) achieve better FLOP sparsity due to usage of Knowledge Distillation. From this sentence alone it\u2019s not clear how exactly KD helps. Why can\u2019t KD be applied in the proposed framework? I\u2019d appreciate if the authors could elaborate on this.\n\nI must admit that I\u2019m not an expert in the field of NN pruning but I\u2019m surprised that training-time masking of filters has not been tried before. Even if it\u2019s really the case I\u2019m not entirely confident that the paper should be accepted: the \u201cunsparsification\u201d looks more like a hack than a principled approach and the overall quality of writing needs to be improved. I\u2019m giving a borderline score for now but I\u2019m willing to increase it provided that the rebuttal addresses my concerns."}