{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n-------\nThis paper addresses the problem of few shot learning where the objective is to learn a model for T parallel tasks, each of these having few learning samples. The first contribution of the authors is to propose a PAC-Bayes bound over the cumulated risk over the T task. As expected with PAC-Bayesian theory, the bound involves a KL divergence between prior distribution over the set of hypothesis and a posterior that is obtained by learning. The prior is supposed to be shared against the tasks and the posterior is task specific. Prior and posterior distributions are trained by variational inference. An experimental evaluation is made on a synthetic data for regression and on an existing benchmark for classification.\n\nEvaluation\n--------\nMy main issue come rom the fact that the PAC-Bayes seems not valid since the prior is obtained after having seen the training data which is not allowed in the setting considered by the authors. For the variational inference, the authors seem to combine many existing idea to perform the inference efficiently. It is not clear if there is a particular contribution here, but at least combining the ideas in an appropriate way is already an interesting contribution. Evaluation is ok but restricted to only one benchmark for classification, the evaluation can maybe be extended. \n\nOther comments\n------------\n\n-In classic PAC-Bayesian theory, and in particular with McAllester's result, the priori must be chosen before having seen the data. This is required in the proof at some step to be able to exchange expectations over prior and learning sample respectively. As far as I understand the approach, the parameter vector \\theta that parameterizes the prior is trained iteratively over all the training instances of all the tasks which violated the assumption of the theorem.\n\nTo have data-dependent priors, the authors could consider the following papers:\n*E. Parrado-Hernandez, A. Ambroladze, J. Shawe-Taylor and S. Sun. PAC-Bayes Bounds with Data Dependent Priors. JMLR, vol 13, 2012. \n*G. Lever, F. Laviolette, and J. Shawe-Taylor. Distribution-dependent PAC-Bayes priors. ALT, pages 119\u2013133. Springer, 2010.\n*G Dziugaite, D. Roy. Data-dependent PAC-Bayes priors via differential privacy, NIPS 2018.\n*O. Rivasplata, E. Parrado-Hernandez, J. Shawe-Taylor, S. Sun, C. Szepesvari. PAC-Bayes bounds for stable algorithms with instance-dependent priors, NeurIPS 2019.\n\n-The paper integrates PAC-Bayes theory and Bayesian inference, note the following reference that can help to position the paper with respect to these two fields:\n*P. Germain, F. Bach, A. Lacoste, S. Lacoste-Julien. PAC-Bayesian Theory Meets Bayesian Inference, NIPS 2016.\n\n-For the variational part, as far as I understand, the authors combine many existing ideas that are related to Bayesian inference. I may guess that the way the authors combine the different things is somewhat novel, but it could be informative if the authors can more clearly identify what is novel here.\n\n-In the experimental evaluation on classification, the baselines used for 1-shot and 5-shot learning are different. This probably because the authors have selected the best competitors for each problem, but I think it would be interesting to evaluate the performance of all baselines on the two types of problems as for Simba.\nIt is no clear in the setup if the other competitors use the same base classifiers, otherwise it is important to precise it.\n\nI also wonder to what extend the results for 1-shot learning is dependent on the 4-layer CNN network used, having the result for other architecture would be interesting. \n\n-One interesting aspect of PAC-Bayesian theory is the fact that it can produce informative bounds. When the theory is corrected, it could be interesting to complete the experimental evaluation by providing some bound values to check the information provided by theory.\n"}