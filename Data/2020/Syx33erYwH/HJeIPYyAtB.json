{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, a multi-agent imitation learning algorithm for extensive Markov Games is proposed. Compared to Markov Games (MGs), extensive Markov Games (eMGs) introduces indicator variables, which means whether agents will participate in the game at the specific time step or not, and player function, which is a probability distribution of indicator variables given histories and assumed to be governed by the environment, not by the agents. Such a model allows us to consider asynchronous participation of agents, whereas MGs only consider synchronous participation, which is assumed in the existing multi-agent imitation learning algorithms such as MA-GAIL and MA-AIRL.\n\nThe contribution of this submission can be summarized as follows. From a theoretical perspective, the submission extends the theorems in MA-GAIL to those in eMGs, where most of them deal with Lagrange multiplier, its meaning, and properties. Followed by Theorem1 and 2, authors define an extensive occupancy measure, a natural extension of occupancy measures in MGs, and cast a multi-agent imitation learning problem into extensive occupancy measure matching problem in Theorem 3. For a practical algorithm, AMA-GAIL is proposed and shown to have a performance gain relative to BC and MA-GAIL.\n\nThe submission is highly interesting, but I think section 4 (Practical Asynchronous Multi-Agent Imitation Learning) and section 5 (Experiments) should be much clearly written. The followings are comments regarding those sections:\n- It seems that the key difference between MA-GAIL and AMA-GAIL is whether we consider the cost function when the indicator is 0 or not, but it\u2019s difficult to figure out just by comparing (3) (MA-GAIL objective) and (14) (AMA-GAIL objective) at the first glance.\n- Similarly in Appendix B, it\u2019s difficult to figure out the difference between MA-GAIL algorithm and AMA-GAIL except the fact that eMGs are assumed. I think some additional explanation is needed.  \n- How did you generate expert trajectories in eMGs setting? Suppose there is an agent taking an action \u201c1\u201d at time t, but it was not applied to the dynamics because the indicator variable of the agent is equal to 0 at time t. In this case, what would be stored in the expert trajectories? \u201cNull\u201d or \u201c1\u201d? If the agent cannot take an action in advance (before looking at its indicator variable), I think adding indicator variables in a condition of policy, e.g., $\\pi(a|s, i)=pi(a|s)$ if $i=1$, otherwise $\\mathbb{I}\\{a=\"Null\"\\}$, is mathematically rigorous.\n- Assuming that experts\u2019 trajectories include \u201cNull\u201d actions, how did you use MA-GAIL and BC with those trajectories? \n- The performance of BC seems weird to me since adding lots of training data reduces supervised learning errors and can also reduce covariate shift problems in BC since the theorem tells us that the regret is bounded by (error) * (episode length) ^ 2 in the worst case [Ross and Bagnell, \u201cEfficient reductions for imitation learning\u201d]. Such a tendency is empirically shown in MA-GAIL paper as well, i.e., performance of BC increases as the amount of expert trajectories increases. Is there any reason, BC shows poor performance in eMGs?\n\nThere are some minor comments:\n\n- In 2.1., $\\eta$ (initial state distribution) is not explicitly defined.\n- In 2.1., MGs assume each agent\u2019s reward function depends on other agents\u2019 actions as well as agents\u2019 own actions, but in the submission, rewards only depend on agents\u2019 own actions. This may be due to the asynchronous setting, but I think it should be mentioned in the paper.  \n- In Definition 1, null action is describe as $0$, whereas it was defined as $\\phi$ in 2.1.\n- In a sentence below Definition 1, $\\eta(i)=1$ -> $\\zeta(i)=1$.\n- Below (14), we don\u2019t have full knowledge of transition P, but we can sample from it (like black-box model). \n"}