{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method to initialize the bias terms in neural network layers, and argues that the proposed method improve the performance of binary neural networks (BNNs). The paper justifies the proposed method by analyzing the geometric properties of the ReLU and the hard tanh (htanh) activation functions, as well as by empirical results on the CIFAR-10 dataset using the (binary variants) of VGG-7 and ResNet. \n\nWhile closing the performance gap between BNNs and their full-precision counterparts is an interesting problem of practical importance, this paper has several limitations: \n\n(1) the analysis of geometric properties of ReLU/htanh is not sufficiently precise and clear;\n(2) the paper does not clearly present the connections between the htanh activation function and the straight-through estimator employed in back-propagating the gradients in training a BNN;\n(3) the experimental results are too limited on just one dateset, and only error rate on validation set is reported, however, lower error rate on validation set won't guarantee better performance on test set;\n(4) the presentation is imprecise and unpolished.\n\n\nMinor comments:\n\nSection 2: \n\"Tang et al. replaced replacing ReLU\" -> \"Tang et al. replaced ReLU\"\n\"many relaated works\" -> \"many related works\"\n\nSection 3: \nplease define the symbols used in Equation (1)\ntitle of Figure 2: \"behavior of ReLu\" -> \"behavior of ReLU\""}