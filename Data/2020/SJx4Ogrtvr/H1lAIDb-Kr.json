{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The paper proposes a method for bias initialization and shows that it improves training for BNN.\n\nI vote to reject the paper. Main points against are: (1) is no theory and very limited experiments (2) Bad writing.\n\nDetailed remarks:\n - The level of english is not good enough all over the paper, example: \"It is more common to use low-bit quantized networks such as Binary Neural Networks (BNNs)\u05f4 more common then what? (I also disagree on the scientific claim)\n- The authors claim that XNOR nets and such have \"memory occupation is significantly larger than the pure 1-bit solution like the vanilla BNN.\". While this is true for training, it is not true for inference which is in many cases where one needs to use limited hardware.\n- The paper main claim is the data equality and hyperplane equality are the main strengths of ReLU, but doesn't give any justification or even intuition into why this is the case. I am not convinced that these points are important, and the paper did nothing to try to persuade me.\n- Data point equality shouldn't hold for ReLU networks with non-zero bias initialization as well.\n- The experiments show promising results but only on cifar10 and only with the outdated BNN, also as a necessary baseline it would be important to show the effect of the bias initialization on ReLU networks. \n\n\nI believe the paper shows promising initial results but needs to strengthen them considerably. It also needs to improve the writing. A better justification for the method, even if it only at an intuitive level would help considerably."}