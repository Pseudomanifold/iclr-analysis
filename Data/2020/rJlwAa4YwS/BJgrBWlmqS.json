{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a technique for learning of lattice valued embeddings. The authors propose to learn the embeddings by gradient descent which naturally brings about the challenge of how to obtain non-zero gradients when the embedding space is discrete. To resolve this, they apply the \u201cCrypto-Lemma\u201d, a well-known result from information theory, which allows them to obtain meaningful gradients by using certain uniform perturbation of the embeddings. Since the resulting algorithm is hard to implement in high dimensions, the authors eventually replace the uniform noise with a small Gaussian noise. The paper is concluded with an experiment comparing learned and fixed lattice embeddings in terms of the average code length (\u201crepresentation cost\u201d) on a dataset of English sentences.\n\nI am currently leaning towards recommending rejection of this paper. The two main reasons are: (i) I am missing concrete examples of where application of lattice embeddings is more beneficial than simple continuous (or binary) valued embeddings; (ii) If the main application of this algorithm is supposed to be compression, then I am missing sufficient background section on alternatives to the presented algorithm and comparison to these within the experiments (I am admittedly no expert on the use of ML algorithms for compression, but a simple search reveals, e.g., [1] as a relevant baseline). Finally, since the paper is 10 pages, I am applying a higher standard then I would to an 8 page paper as instructed by the guidelines.\n\n\nMajor comments:\n\n- As alluded above, can you please clarify whether the main application of your algorithm is compression, or whether there are applications in which lattices have interpretative value or some other advantage not related to compression?\n\n- Can you please explain why is there no comparison to other compression algorithms (e.g., [1])?\n\n- Can you please provide more detail on how exactly you obtain the measurements in fig.3? E.g., how have you selected the reported hyper-parameters? Were the same hyper-parameters used for both yours and the straight-through model, or did each model run with its optimal hyper-parameters? Are the reported numbers an average over a larger number of random seeds? etc.\n\n\nMinor comments:\n\n- On p.1, line 5, \u201cof of\u201d -> \u201cof\u201d\n\n- On p.1, wasn\u2019t the phrase \u201cin another line of thinking\u201d supposed to be connected to the following, instead of appended to the preceding sentence?\n\n- On p.3, you say \u201cWe are interested in a machine learning application, but want to emphasize the representation cost for the objects being encoded as a first class metric to be optimized for.\u201d\nI am not entirely sure I understand what you want to say here. Standard variational inference (the type that is among else being employed by the cited VAEs) has a well known Minimum Description Length (MDL) interpretation. Can you please clarify why MDL is not \u201ca first class metric to be optimized for\u201d? \n\n- At times I felt like the paper is burdening the reader with unnecessary definitions. For example, why does the reader have to know what a \u201cfundamental Voronoi cell\u201d is when they already know the definition of a \u201cfundamental cell\u201d?\n\n- I like that you are distinguishing between probability distributions and their density/probability mass functions (PMF) as it generally makes reader\u2019s life easier in a paper like this. However, I would have liked if you have consistently stuck with the notation that you introduce (f for densities, p for probability mass functions) instead of using capital P in some places where density/PMF is appropriate (for example, compare the lower case p in eq.3, with the expressions for entropy and cross-entropy on p.5). On a related note, I find the choice of denoting both the encoder and density functions by f somewhat unfortunate.\n\n- In the second display after eq.8, should S be X? \n\n\nReferences:\n\n[1] James Townsend, Tom Bird, David Barber. Practical Lossless Compression with Latent Variables using Bits Back Coding. https://arxiv.org/abs/1901.04866"}