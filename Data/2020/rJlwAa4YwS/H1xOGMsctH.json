{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThe paper discusses discrete representation learning from a lattice perspective, where one performs a \"reparametrization trick\" during training and quantized learning over inference. This is different from other methods such as Gumbel trick in the sense that the quantization is done on lattices and the noise is uniform over primitive cells. The paper discusses some connections with VAEs, showing how one could interpret the VAE objective with \"Gaussian dithering\". While the theory is interesting and seems to be valid, I am not entirely sure how we could use it well in practice (apart from the proposed Gaussian dithering, which is very similar to what we already have with VAE reparametrization trick).\n\n\nQuestions:\n\t- In sec 2.3 an argument is placed for connection with VAEs. It seems that here U is sampled from Gaussian and therefore unbounded and not \"uniform\" within the primitive cell? It's not critical, but I wonder if the this argument could be more interested if better connected to lattices, ie the optimality of \"hyperspheres\" as a lattice representation.\n\n\t- Experimental-wise, I don't see how the proposed training approach is different from a regular VAE setup (maybe that is the point), and I am confused by the procedure in which you would do quantization for the \"hypothetical lattice\" experiments. Does the \"representation cost\" here mean the loss you get with the Gaussians? How do we obtain useful discrete representations in practice?\n\n\t- It seems helpful to have experiments on popular tasks for VAEs like binarized MNIST; this would make it easier to compare with more existing baselines. \n\n\t- It might be nice to restructure the paper, a lot of the paragraphs are very long and a lot of math uses one entire line while being very short (it seems the numbered equations are most useful). Some details can be put in the appendix to make the paper shorter.\n\nTypos: \n\t- introduction \"a line of research of autoencoders\" \\citep format.\n\t- Description for (3) \"first term\" and \"second term\" order?\n\t- Lemma 1: \"a uniformly distributed over\"\n\t- Grammatical errors across the paper; it would be nice to proof read them carefully for camera-ready version.\n\t\n"}