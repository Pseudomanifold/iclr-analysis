{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper builds a privacy-preserving training framework within a Trusted Execution Environment (TEE) such as Intel SGX. The work is heavily inspired from Slalom, which does privacy-preserving inference in TEEs. The main drawbacks of Slalom when extending to training are (1) weight quantization needs to be dynamics as they change during training, and (2) pre-processing step of Slalom to compare u = f(r) isn't effective as the weights change, and running this within TEE is no better than running the full DNN within TEE. In addition, Goten also makes the weights private as opposed to Slalom. Overall, this is a very important contribution towards privacy preserving training and the paper takes a strong practical and implementation-focused approach by considering issues arising due to memory limitations in TEE and the performance implications of default Linux paging.\n\nThe paper comes up with a novel outsourcing protocol with two non-colluding servers for offloading linear operations in a fully privacy-preserving way and does detailed analysis of the performance implications. Similar to a lot of other methods for training with quantization, the weights are stored and updated in floats while the computation is performed using quantized values. The experimental results suggest a strong improvement over the CaffeSCONE baseline. One drawback with experiments is the lack of comparison with Slalom for inference if Goten is assumed to be a framework for both training and prediction in a privacy-preserving way.\n\nAnother downside of the paper is that a few sections could be improved with their explanation, and there is quite a bit of redundancy in going over the downsides of Slalom and why it can't be used for secure training. For instance,\n- Section 1.1: \"Our results (referring to Section 4.2) show that CaffeSCONE\u2019s performance greatly suffer from the enclave\u2019s memory limit as it needs an inefficient mechanism to handle excessive use of memory not affordable by the enclave\". Here, it's not clear which mechanism is inefficient. Are we talking about mechanisms in CaffeSCONE for reducing memory usage while training and if so, are they somehow inefficient? Or does it mean to imply that we can't train a DNN fully within an enclave due to memory limits?\n-  Last paragraph of section 2.2 is unclear. \"CaffeSCONE guarantees the correctness of both training and prediction. Goten does not provide it as we present it due to page limitation, but we can resort to the trick used by Slalom\". What does the last sentence mean? Does Goten guarantee correctness during training and prediction or not? And what trick from Slalom are we referring to? The blinding trick used for privacy or the Freivalds' algorithm used for correctness?\n\nOverall a strong contribution with supporting experimental results, but the certain parts need further explanation or rewriting for higher rating.\n\nPros:\n- An important contribution in the direction of fully private DNN training and inference within a TEE. Draws inspirations from Slalom and mainly addresses the challenges left to extend the approach to training.\n- Motivation and reasons for why Slalom can't be used for training is very well laid out.\n- In addition to input and output activations, Goten also preserves the privacy of the weights.\n- Good baseline for comparison using CaffeSCONE.\n- Implementation factors considered and analyzed such as tricks as using SGX-aware paging instead of naive Linux paging.\n- Strong experiments and benchmarks\n\nCons:\n- Some sections are not explained well and unclear as mentioned earlier.\n- How does the inference performance of Goten compare to Slalom given the same privacy and correctness guarantees? This isn't clear from the experiments section.\n\nMinor comments:\n- \"Slalom\" is mis-spelt in line 4 of the abstract.\n- There appear to be typos and grammatical errors at many places in the paper. Further proof-reading might be helpful."}