{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. \n\nHowever, the following issues need to be resolved:\n1. The main method is based on published \"slimmable networks,\" such that the novelty is limited;\n2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar.\n3. Better ablation studies are required in Table 1. This table should be simplified. As the method cannot learn architectures but channel numbers, the only useful pairs of comparisons are those having the same architecture, such as  a pair of MobileNet vs AutoSlim-MobileNet.\n4. an important detail is missing: where does the AutoSlim start from? Does it start from a larger model than the baseline? In the set of \"500M FLOPs\" experiments, I see the size of \"AutoSlim-MobileNet v1\" (4.6M) is larger than \"MobileNet v1 1.0x\" (4.2M), this implies that AutoSlim start from a \"MobileNet v1 Nx\" and N > 1.0. What is exactly N?\n5. If AutoSlim starts from a larger baseline model with N times (N > 1.0) width, then the pruning baseline methods (AMC and ThiNet) should also start from the same larger models for fair comparison. In general, starting from a larger model and pruning it down can achieve a better accuracy vs. size trade-off.\n6. \"300 epochs with linearly decaying learning rate for mobile networks, 100 epochs with step learning rate schedule for ResNet-50 based models\", are baselines trained in the same way?\n\nMinor: \n1. missing captions in a couple of figures, e.g., Figure 5.\n2. \"the importance of trained weights\" vs \"the importance of channel numbers\" is trivial\n\n\n[1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. \"Understanding and simplifying one-shot architecture search.\" In International Conference on Machine Learning, pp. 549-558. 2018."}