{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a method to perform architecture search on the number of channels in convolutional layers. The proposed method, called AutoSlim, is a one-shot approach based on previous work of Slimmable Networks [2,3]. The authors have tested the proposed methods on a variety of architectures on ImageNet dataset. \n\nThe paper is well-written and easy to follow. I really appreciate the authors for structuring this paper so well. I have the following questions:\n\nQ1: In figure 4, the authors find that \u201cCompared with default MobileNet v2, our optimized configuration has fewer channels in shallow layers and more channels in deep ones.\u201d This is interesting. Because in network pruning methods, it is found that usually later stages get pruned more [1] (e.g. VGG), indicating that there is more redundancy for deep layers. However, in this case, actually deep layers get more channels than standard models. Is there any justification for this? Is it that more channels in deep layers benefit the accuracy?\n\nQ2: In \u201cTraining Optimized Networks\u201d, the authors mentioned that \u201cBy default we search for the network FLOPs at approximately 200M, 300M and 500M, and train a slimmable model.\u201d Does this mean that the authors train the final optimized models from scratch as a slimmable network using \u201csandwich rule\u201d and \u201cin-place distillation\u201d rule? Or are the authors just training the final model with standard training schedule? If it is the first case, can the authors justify why?\n\nQ3: In Table 1, \u201cHeavy Models\u201d, what is the difference between \u201cResNet-50\u201d and \u201cHe-ResNet-50\u201d? Also, why the params, memory and CPU Latency of some networks are omitted?\n\nQ4: In the last paragraph of section 4, the authors tried the transferability of networks learned from ImageNet to CIFAR-10 dataset. I am not sure how the authors transfer the networks from Imagenet to CIFAR-10? Is it the ratio of the number of channels? Can the authors provide the architecture details of MobileNet v2 on CIFAR-10 dataset?\n\nQ5: What is the estimated time for a typical run of AutoSlim? How does it compare to network pruning methods or neural architecture search methods?\n\nQ6: Can the methods be used to search for the number of neurons in fully connected layers? Are there any results?\n\n[1] Rethinking the Value of Network Pruning. Zhuang et al. ICLR 2019\n[2] Slimmable neural networks. Yu et al. ICLR 2019.\n[3] Universally Slimmable Networks and Improved Training Techniques. Yu et al. Arxiv.\n"}