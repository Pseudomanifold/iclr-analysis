{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a new algorithm (Optimistic Stochastic Gradient) for solving a class of non-convex non-concave min-max problem. The convergence theory is established for finding first order stationary point. The authors also proposed an adaptive variant of the proposed algorithm, called OAdagrad and showed an improved adaptive complexity.\n\n- It is not immediately clear to me why the update of Algorithm 1 becomes the algorithm in (Daskalaskis et al. 2017) when there is no constraint. Can the authors further explain this clearly? The two variables in (Daskalaskis et al. 2017) are updated with different signs, but here  z=(u,v)  (I assume, the authors should clearly define z too), it seems that u and v are updated with the same sign?\n\n- In terms of the theoretical contributions, aside from the presenting theorems, can the authors also comment on what is the key point to achieve the derived results? For example, in Algorithm 1, the only difference from stochastic extragradient method is T(z_{k-1}) instead of T(x_{k-1}), and the theorem achieves the same iteration complexity with weaker assumption.  Is this because the replacement term, or it is the proving technique improvement that can also be applied to stochastic extragradient method?\n\n- For Algorithm 2, why there is no projection operator like in Algorithm 1? Also, the algorithm design is a bit different from traditional AdaGrad as their H matrix is the historical average of all past gradient square (element-wise). Here the s_i is L_2 norm of historical gradient, there is no averaging (divided by k) operator. Can the authors elaborate on this algorithm design?\n\n- For experiments part, it is better for the authors to compare with more baseline methods such as Optimistic Adam. For Figure 1, can the authors put different batches in different plot so that we can directly compare the performances of different algorithms? It does not make sense that alternating Adam failed in training SA-GAN here but success in the original paper. Can the authors figure it out and provide the comparison on this?\n\nDetailed comments: \n\n- it is better to define m_t in or before Algorithm 1 to make it clear for the readers.\n- In Page 7, comparison with \u2026 paragraph, should be \\beta_1 = 0, \\beta_2 -> 1? If so, it is still a bit different with OAdagrad?\n"}