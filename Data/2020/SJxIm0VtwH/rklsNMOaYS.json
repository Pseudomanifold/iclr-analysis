{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes two methods named OSG and OAdagrad for solving stochastic non-convex non-concave min-max problems. In theoretical analyses, a convergence rate of $O(\\epsilon^{-4})$ and a much better rate are provided for OSG and OAdagrad, respectively, to find the $\\epsilon$-accurate first-order stationary point. Finally, the superior performance of the proposed method is empirically verified on training generative adversarial networks (GANs).\n\nClarity:\nThe paper is well organized and easy to read.\n\nQuality:\nThe work is of good quality and is technically sound. However, I did not verify the proof in detail.\n\nSignificance:\nA stochastic non-convex non-concave minimax problem studied in this paper is recently considered as an important class of the optimization problems because important machine learning problems such as GANs fall into this class and most past papers studied convex-concave min-max problems instead. A few studies [Iusem+(2017), Lin+(2018)] proposed optimization algorithms for this problem and derived convergence rates $O(\\epsilon^{-4})$ and $O(\\epsilon^{-6})$, respectively. On the other hand, proposed methods have several preferable properties compared to these methods. For instance, OSG exhibits a comparable convergence rate to [Iusem+(2017)] with a fewer per-iteration complexity and OAdagrad exhibits a much faster convergence rate $O(\\epsilon^{-2/(1-\\alpha)})$ depending on the parameter $\\alpha$ that is an order of the growth of the cumulative stochastic gradients norm. In addition, the order of $\\alpha$ is shown to be slow in general and certainly faster convergence rates are also confirmed in experiments on training GANs. Thus, experimental results seem consistent with the theory. \nSince a derived convergence rate of OAdagrad is potentially much faster than those of existing methods, I think the OAdagrad is one of the promising methods for training GANs."}