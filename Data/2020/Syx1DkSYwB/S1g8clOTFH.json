{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims at improving the computational cost of variance reduction methods while preserving their benefits regarding the fast provable convergence. The existing variance reduction based methods suffer from higher per-iteration gradient query complexity as compared to the vanilla mini-batch SGD, which limits their utility in many practical settings. This paper notices that, for many models, as the training progresses the gradient vectors start exhibiting structure in the sense that only a small number of coordinates have large magnitude. Based on this observation, the paper proposes a modified variance reduction method (by modifying the SpiderBoost method), where a 'memory vector' keeps track of the coordinates of the gradient vectors with large variance. Let $d$ be the size of the model parameter. During each iteration, one computes the gradient for $k_1$ coordinates with the highest variance (according to the memory vector) and an additional $k_2$ random coordinates. \n\nThe paper shows that in the worst case, the proposed method has the same gradient query complexity as the SpiderBoost variance reduction method. Assuming that the proposed method can track the sparsity of the gradient vector, the proposed method achieves a gradient query complexity which is $O(\\sqrt{(k_1 + k_2)/d})$ times that of the SpiderBoost method. The paper demonstrates the gradient query complexity improvement over the SpiderBoost method on MNIST and CIFAR-10 data set.\n\nThe paper presents novel results by utilizing the ideas from the field of communication-efficient distributed optimization. As far as the reviewer can tell, the results in the paper are correct. That said, there is quite a bit of room for improvement in terms of the writing of the paper. \n\nThe paper appears to have way too many typos. For example, \n\nIn Section 2.1:\n- Why is $k$ introduced?\n- $S$ denotes a random subset with size $k$ ---> $k_2$?\n- drawn from the set ${\\ell : |y_{\\ell}| < |y_{(k)}|}$ ----> ${\\ell : |x_{\\ell}| < |x_{(k_2)}|}$?\n- $rtop(x, y) = (0, 12, 0, 0, 1)$ --> $rtop(x, y) = (0, 16, 0, 0, 1)$\nIn Lemma 1: \n - while defining $top_{-k_1}(x, y)$, \".... if |x_{\\ell}| >= |x_{(k_1)}|\" ----> \".... if |x_{\\ell}| <= |x_{(k_1)}|\"?\nIn Section 2.2:\n - What are $g_0, g_1,..., g_{L-1}$? Shouldn't these be $\\phi_0, \\phi_1,..., \\phi_{L-1}$?\nIn A1: \n - right after (5), what is $\\tilde{x}_0$ in the definition of $\\Delta_f$?\n\nThe authors may also consider making the empirical evaluation more comprehensive by considering tasks from the NLP domain, e.g., language modeling. This would further help asses the utility of the proposed method."}