{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper introduces a sparse variant to SpiderBoost which reduces the complexity cost of updating gradient estimates by way of sparse updates. The authors prove that this variant incurs a negligible increase in worst case complexities as soon as certain assumptions are satisfied, and that when their algorithm captures sparsity correctly, they improve upon SpiderBoost's complexity.\n\nThis paper is clearly, and the experiments support the theoretical contributions. \n\nIn Figure 1, you report results as a function of gradient queries/N. Given Theorem 2, I assume that the graphs would look similar as a function of wall-clock time; can you confirm this?\n\nRecommendation: Accept. \n\nMinor comments and questions for the author:\n- I am slightly confused by the introduction of the rtop operator. Specifically, \n  1) What is the relation between k1, k2, and k?\n  2) You write that S is a random subset of size k. Should this be k2?\n  3) In your first example, should we have rtop(x,y) = (0, 16, 0, 0, 1), since for \\ell = 2, y_\\ell = 4, d-k1 = 4, k2=1? Am I missing something?\n  More generally, my understanding is that the rtop(x,y) operator randomly sparsifies y based on x, which essentially provides indication of where sparsity would be least harmful; when not sparsifying, rtop applies a rescaling that guarantees unbiased estimates. If this is correct, I would recommend making that intuition more clear early on in the paper, in order to improve upon the clarity of the paper.\n\n- You state that rtop is linear in y; since rtop depends on the random variable S, is the claim that E[rtop(x, y+y')] = E[rtop(x, y)]+E[rtop(x,y')] (which follows from unbiasedness)?\n\n- For your experiments, could you discuss how your choice of hyperparameters relates to the constraints in Theorem 1 and 2? \n\n- I believe Table 1 would be more impactful if it also included the initial entropy ratios at the beginning of training, rather than reporting those values below.\n\n- Other variance reduction techniques for minibatching focus on choosing the minibatches themselves with non-uniform sampling. Under such a sampling mechanism, do you foresee any complications to using SpiderBoost with Sparse Gradients, eg., decrease in overall sparsity?"}