{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method to learn locomotion and navigation to a goal location or through a set of waypoints for simulated legged robots. The contributions of this paper include 1) generalized experience, which is a data-augmentation technique to add more orientation-invariant experience, and 2) a latent representation to encode the state, the current location and the goal location. The paper compares the proposed method with a few baselines and demonstrates better performance.\n\nMy recommendation of this paper is Weak Reject. Although the method seems reasonable and the evaluation shows good results, I think that the paper can be improved for the following three reasons. \n\nFirst, it is not clear to me how the inference works if the goal is reasonably far away (e.g. can be reached in 10 steps) from the current position of the robot? Since the inverse dynamics model only outputs an action given the current position and the immediate goal (in the next time-step), how is the 10-step action sequence planned using the 1-step immediate goals? \n\nSecond, the details of data collection are unclear to me. I believe that the policy \\pi used for data collection plays an important role. Which \\pi is used? It would be clearer to present this part in the main text, not Appendix. If the data is collected from an RL agent which learns to walk to the right, how does the robot learns to turn when walking across different waypoints (Figure 7)?  \n\nThird, in this paper, the generalized experience is to add different initial orientations of the robot. I think that the similar effect can be achieved by reparameterizing (s, o, o') into polar coordinates: (s, \\theta, r), where (\\theta, r) is the intermediate goal location relative to the robot's current orientation and position. \\theta=0 means the goal is in front of the robot, and r is the distance between the goal and the robot. In this representation, all the generalized experience will reduce to a single (s, \\theta, r), which is invariant to the robot's orientation. This would already be a good latent space, without any learning. For this reason, I would suggest adding one more baseline to compare the proposed method against: using (s_t, \\theta_t, r_t, a_t, s_{t+1}, \\theta_{t+1}, r_{t+1}) to represent experience, then run the proposed method without generalized experience and latent representation. Will this baseline achieve similar or even better results? "}