{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the issue of scalability in distributed SGD over a high-latency network.\nAlong with experiments, this paper contributes two ideas: delayed updates and temporally sparse updates.\n\n- strengths: \n\n\u00b7 Clear-looking overall presentation.\n  Good efforts to explain the network problems (latency, congestion) and how they are tackled by delayed updates and temporally sparse updates.\n\n- weaknesses:\n\n\u00b7 While the overall presentation looks clean, the paper does not talk about the setting (one parameter server and many workers, only workers, etc).\n  This is arguably basic information, and the readers is left to understand by themselves that the setting consists in many workers without a parameter server.\n\n\u00b7 There is no theoretical analysis of the soundness of the proposed algorithm.\n  For instance in ASGD (that the authors cite), stale gradients are dampened before being used, which in turn is used to guarantee convergence.\n  In the proposed algorithm, there is no dampening nor apparent limit of the maximum value of t; such a difference with prior art should entail a serious (theoretical) analysis.\n\n\u00b7 Finally, using SSGD for comparison is not very \"fair\", as communication-efficient algorithms have already been published for quite some time [1, 2, and follow-ups (e.g. searching for \"local sgd\")].\n  At the very least a comparison with ASGD (cited) is necessary, as in a realistic setting latency is indeed a problem but arguably not bandwidth\n  (plus, orthogonal gradient compression techniques do exist, e.g. as in \"Federated Learning: Strategies for Improving Communication Efficiency\").\n\nquestions to the authors: \n\n- Can you clarify the setting?\n\n- Can you give at least an intuition why accepting stale gradient is correct (i.e. does not impede convergence)?\n  There is no theoretical limit on the value of t; can workers take any arbitrary old gradient?\n  So when the training is close to convergence (if it reaches it), i.e., when the norm of the gradients are close to 0,  the algorithm could then use very old gradients (which norms could be orders of magnitude larger) to \"correct the mismatch\" caused by the local training; is this correct?\n  To solve that issue, ASGD introduces a simple dampening mechanism (which is necessary in the convergence proof), which your algorithm does not have.\n\n\nThe related work section focuses on gradient compression techniques (which tackle low bandwidth, not latency) and asynchronous SGD (which is more prone to congestion, with a single parameter server),\nbut seems to overlook that sparse communication techniques already exist (this fact should at least be mentioned).\n\nThe idea of sparse communication for SGD exists since at least 2012 [1, Algorithm 3].\nA first mention of the use of such techniques for \"communication efficiency\" dates from (at least) 2015 [2].\n\n[1] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.\n    Optimal distributed online prediction using mini-batches.\n    J. Mach. Learn. Res., 13(1):165\u2013202, January 2012.\n\n[2] Sixin Zhang, Anna E. Choromanska, Yann LeCun.\n    Deep learning with Elastic Averaging SGD.\n    NeurIPS, 2015.\n\n\nI do not see a clear novelty, nor a proof (or even intuitions) that the proposed algorithm is theoretically sound.\nThe comparison with SSGD is arguably unfair, since SSGD is arguably not at all the state-of-the-art in the proposed setting (hence the claimed speedup of x90 can be very misleading)."}