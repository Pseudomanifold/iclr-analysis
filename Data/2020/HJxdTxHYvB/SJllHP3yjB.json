{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #4", "review": "The paper presents a new attack: Shadow Attack, which can generate imperceptible adversarial samples. This method is based on adding regularization on total variation, color change in each channel and similar perturbation in each channel. This method is easy to follow and a lot of examples of different experiments are shown.\nHowever, I have several questions about motivation and method.\n\nFirst, the proposed attack method can yield adversarial perturbations to images that are large in the \\ell_p norm. Therefore, the authors claim that the method can attack certified systems. However, attack in Wasserstein distance and some other methods can also do so. They can generate adversarial examples whose \\ell_p norm is large.\nI think the author should have some discussions about these related methods. \n\nSecond, I notice that compared to the result in Table 1, PGD attack can yield better results [1]. I hope to see some discussions about this. Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.\n\n[1] Salman, Hadi, et al. \"Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers.\" Neuips (2019).", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}