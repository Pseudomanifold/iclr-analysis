{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper claims that it identifies a fundamental issue in model-based reinforcement learning methods. The issue is called objective mismatch, which arises when one objective is optimized (for example, model learning objective) without taking into consideration of another objective (for example policy optimization). The author shows several experiments to illustrate the issue and proposes a method to mitigate it by assigning priorities to samples when training the model. \n\nThe issue of objective mismatch is not being noticed the first time. The paper \"Reinforcement learning with misspecified model classes\" has mentioned similar phenomenon. And other work such as https://arxiv.org/abs/1710.08005 also discussed similar issue. \n\nI disliked the way how the paper is motivated. The paper says \u201cin standard MBRL framework\u201d, what is the standard MBRL framework? I think there is no such standard so far. The claim saying that the mismatch is a crucial flaw in current MBRL framework is too strong. The paper at least completely ignored two broad classes of MBRL methods. The first is value-aware MBRL (which attempts to take into account decision making when learning a model), there are actually many MBRL methods are in this category. Some examples: value prediction network, Predictron, Value-Aware Model Learning. The second class of MBRL approach is Dyna. Several works (continuous deep q-learning with model based acceleration, Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains, Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation, Recall Traces: Backtracking Models for Efficient Reinforcement Learning, Hill Climbing on Value Estimates for Search-control in Dyna) show that even with the same model (hence the same model error), using different simulated experiences play a significant role of improving sample efficiency. It is unclear whether model-error plays a decisive role in improving sample efficiency. \n\nThe proposed method in section 5 lacks of justification, even a regular ER buffer is asymptotically on-policy, and hence it is indirectly linked with the control performance. It is unclear why introducing the weights based on expert trajectory can be helpful \u2014 it can be worse because it should be far away from on-policy distribution. "}