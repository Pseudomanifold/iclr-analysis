{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a supervised method for robust classification. By utilizing mutual information constrain on the encoder, supervised probabilistic constraint on the class conditional probability, and introducing a margin to the maximum likelihood, the proposed method can manage a high classification accuracy and detect the out of distribution data. \n\nThe motivation is good, the writing is OK. The structure of the paper needs to be refined.  The experiments are not very strong. Several concerns are listed:\n\n1.\tIt will be better if the author can show a structure map for Encoder, classification, class condition embedding, and mutual information evaluation networks, to clarify their relationships.\n2.\tIn Table 2, what is the accuracy for the rejection (outlier detection)?\n3.\tThe choose of the threshold for rejection is tricky. It will be better if the author can provide some rules for choosing the threshold which can be generalized to a new dataset.\n4.\tThe overall loss functions have three components, what is the contribution of different components to the final performance experimentally?\n5.\tThe architecture for encoder is large. Is it proper for Mnist which is a relatively simple dataset?\n6.\tThe comparison talked in the paragraph \u201cIs Fully Generative Model Necessary for Generative Classification\u201d, are these accuracies obtained from a comparable network size? It only makes sense if they are obtained using a comparable parameter size.\n"}