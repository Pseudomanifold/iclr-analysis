{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Overview:\n\nThe paper is dedicated to developing a regularization scheme for the provably robust model. The author proposes the MMR-Universal regularizer for ReLU based networks. It enforces l1 and l infinity robustness and leads to be provably robust with any lp norm attack for p larger than and equal to one.\n\nStrength Bullets:\n\n1. Using convex hull to enforce robustness, it is very reasonable and straightforward which highly aligns our intuition.\n2. The author provides detailed and conniving derivations and proof. And the results in table 2 do achieve the state-of-the-art provable robustness.\n3. It is the first robust regularizer that is able to provide non-trivial robustness guarantees for multiple lp balls. The lp balls don't contain any other. It also is one of the most straightforward methods among all potential similar methods.\n\nWeakness Bullets:\n\n1. I am very curious about the landscape or decision boundary analysis and visualization. For the author's MMR-Universal regularization, it should give the model a very \"good\" decision boundary which has clear marginal between any two categories. In my opinion, it is necessary to evidence to convince more readers.\n2. Try a few more classical CNN architectures. \n\nRecommendation:\n\nDue to the logical derivations and supportive experiment results, this is a weak accept."}