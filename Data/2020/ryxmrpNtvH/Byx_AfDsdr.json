{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Many NAS methods rely on weight sharing. Notably in ENAS, a single weight tensor is used for all candidate operations each edge of a cell. In this paper, the authors take a small NAS search space (64 possible networks) and train each network separately to obtain their individual rankings. They then examine how this ranking correlates when  the same network is trained as part of a super-net with weight sharing, as in NAS algorithms.\n\nGiven the prevalence of NAS algorithms, an examination of the potential pitfalls of weight sharing is very important and I commend the authors for that. There are a lack of typos and grammatical errors, which is nice too!\n\nI have two major issues with this paper however. Firstly, the scope is limited; everything is based on looking at 64 convnets trained on CIFAR-10, this makes it tricky to make any broad statements about weight-sharing in NAS. Secondly, the paper reads as a string of observations, and it is not clear what the takeaways are (it is hinted that one could reduce the search space for Figure 3, but this is not expanded on).\n\nA few chronological comments:\n\n- \"Population based algorithm () is another popular approach\" ---> \"Population-based algorithms are another popular approach\"\n\n- \"In this paper we try to answer\" --> A little confidence wouldn't hurt :)\n\n- \"Surprisingly\". I don't like the reader being told what is surprising/interesting etc. but maybe that's just me.\n\n- As mentioned above, more detail on search space pruning would be really nice. For instance, are particular operations e.g. conv5x5 neglected.\n\n- To clarify, when you have shared weights, but one candidate operation uses more parameters than another, do you just read off the weight tensor until you hit length? e.g. if convA uses X params, and convB uses Y params, are the parameters for ConvA weight(1:X) and convB weight(1:Y)?\n\n- Literature review is good. Figure 1 is nice and straightforward.\n\n- The figure and table captions could do with some more detail. For instance, in Figure 2 the caption should contain the take-home point of the figure.\n\n- \"there are some statistic information\" --> \"there is some statistical information\"\n\n- Figure 3 is nice. It looks like you can't tell what's good, but you can tell what's bad. A comparison of what architectures good v bad comprise off would be a nice addition.\n\n- Figure 5 confused me, as there is a lot going on. What do ordered and shuffled mean? Is it just whether you are mixing up your minibatch selections? \"The curve has obvious periodicity with the length of 64 mini-batches i.e. the number of child model\" doesn't make sense to me. Could you elaborate?\n\n- The accuracies in the plots look very low. ~80% for CIFAR-10 is really bad. Am I missing something?\n\nPros\n------\n- Good topic with a few interesting observations\n- Relatively well-written\n\nCons\n-------\n- Very limited scope. Only 1 dataset and only 64 models\n- The narrative is lacking, what are the key points that people using NAS should be aware of?\n\nI recommend a weak rejection for this paper. The topic is interesting, but I haven't been convinced through the limited scope of the experiments, or the arguments made what the real point is. Should I stop weight sharing with NAS? Should I prune my search space? etc. A few neat observations is nice, but there is a lack of cohesion. "}