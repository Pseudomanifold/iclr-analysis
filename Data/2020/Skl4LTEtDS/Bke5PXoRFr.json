{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes a method to progressively explore the action space for RL. The proposed method is called \u201cgrowing action spaces\u201d. The basic idea is that actions can usually be grouped by a hierarchical structure: the lowest level is the coarsest and higher levels gradually refine the action partition. This method effectively captures many RL settings, including multi-agent learning. One effective approach is to apply the action hierarchy. \n\nThen the paper performs experiments on both simple toy examples and a more complicated one, the Starcraft game. The simple toy problems is \u201cAcrobot\u201d and \u201cMountain Car\u201d with discretized spaces. A hierarchy of level 3 is considered. The experiments clearly demonstrate the effectiveness of the proposed methods. Also, the behavior of each level of the hierarchy is shown: coarse level of actions help exploration but cannot reach the highest value. \n\nThe paper then demonstrates extensive experiments on the game StarCraft. 50-100 units on each side of the game are used \u2014 which is much larger than previous papers. Further difficulties are introduced by randomizing the initializing location, and scripted logic controlled opponents. A 2-level hierarchy are tested and obtain consistently good results in all experiments. Further ablations are tested and explanations are provided. \n\nEvaluation: Overall, I like this paper. This paper is well-written, everything is clearly explained. The proposed method is novel and effective. \n\nSome minor comments:\n\t\u2022 What is the epsilon in Figure 1? \n\t\u2022 Using 3-level of hierarchy in the StarCraft game does not work well. You said the possible reason might be that the high level is pushing the limit of CNN. Did you try a different architecture that has a higher resolution?\n\t\u2022 The training curriculum is still mysterious. Why you pick a random l to start training? Maybe training level by level is better? By starting from the lowest level, you may be able to stop at any level. So you will have l different policies. In practice, you may find the best l this way.\n"}