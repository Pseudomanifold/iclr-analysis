{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the task?\nKnowledge distillation of BERT\n\nWhat has been done before?\nUnlike prior works such as Distilled BiLSTMSOFT (Tang et al., 2019), BERT-PKD (Sun et al., 2019) and DistilBERT, this work\n\ni) Do knowledge distillation at pre training stage also in addition to fine tuning stage.\nii) Student learns from all - embedding layers, attention matrices, hidden states, and final prediction layers. \n\nIn BERT-PKD, student learns from the [CLS]  hidden states of the teacher.\n\nWhat are the main contributions of the paper?\nNovel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models.\nNovel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages\nResulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation.\n\nWhat are the key techniques used to tackle this task?\nNovel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models.\nNovel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages\n\nWhat are the main results? Are they significant?\nResulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation with only \u223c28% parameters and \u223c31% inference time of them.\n\nResults show that three key procedures: TD (Task-specific Distillation), GD (General Distillation) and DA (Data Augmentation) are crucial for the proposed KD method.\n\nProposed distillation objectives - Transformer-layer distillation (attention matrices and hidden states), embedding-layer distillation and prediction layer distillation  are crucial for the proposed KD method.\n\nWeaknesses\nexperimental results were not easily comparable to prior work so it is hard to say if claims are well-supported experimental results\n\nQuestions\nDid authors try other values of lambda\n"}