{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary\nIn this paper, the authors curated two datasets: ImageNet-Vid and Youtube-BB in order to create human-reviewed perceptibly similar sets (Imagenet-Vid-Robust and YTBB-Robust). The obtained datasets are evaluated over 45 different models pre-trained on ImageNet in order to see their drop in accuracy on natural perturbations. Three detection models are also evaluated and show that not only classification models are sensitive to these perturbations, but that it also yields to localization errors.\n\nComments\nThe paper is clear, well organized, well written and easy to follow.\nThe authors present two novel datasets grouped in sets of perceptibly similar images and answer to the following hypothesis: Can the perturbations occurring naturally in videos be a realistic robustness challenge?\nThe thorough evaluation over the curated datasets shows pretty well that the changes in the model prediction are indeed due to a lack of robustness of the models themselves rather than the difference occurring from one frame to the other (occlusion etc).\nThe authors mention the curation was done with the help of expert human annotators. Details could be added as to how the annotators are considered experts and what process they went through (mturk? Handmade application to select the frames?).\nOverall I think the paper adds an interesting contribution, with the datasets themselves which can be used for image similarity tasks for example\nAlthough the contribution of the paper is important, it seems limited for the conference with no novel method proposed. \n\nTypo\nSection 3, l 4: using use -> using\n"}