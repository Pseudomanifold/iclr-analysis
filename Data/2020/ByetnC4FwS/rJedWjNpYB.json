{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper considers the task of extractive QA where the document is longer than the document encoder's size limit. (The conversational part does not play a big role in the method.) One solution is to look at a chunk of document tokens per time step. The paper proposes (1) a way to propagate information between time steps, and (2) a RL policy that selects how many tokens to skip when locating the next chunk. The method was evaluated on CoQA and QUaC, with gains observed when the document is much longer than the chunk size.\n\nThe observation about the answer's location in the chunk is eye-opening, and the need to select the right document chunk is presented well. The recurrent mechanism and chunk scores look correct.\n\nHowever, the paper has two potential weaknesses:\n\n1. The chunking policy looks too complex for the task and might also be incorrect.\n\n- Most of the time, the number of possible chunks (with 16-token strides) is small. One could score all such chunks for relevance at once without having to use RL to look at chunks sequentially. Efficiency-wise, scoring multiple chunks in a batch might be even cheaper than embedding a single chunk at each time step. This process of selecting relevant document sections is related to retrieval-based reading comprehension [https://arxiv.org/abs/1704.00051 | https://arxiv.org/abs/1808.10628 | https://arxiv.org/abs/1808.06528], where the relevant documents are retrieved for extractive QA.\n\n- In Equation 11, q_c contains parameters theta to be optimized, which I think makes the REINFORCE gradient incorrect.\n\n- Equation 13 is missing a term for the negative class (sum_c (1 - y_c) log (1 - q_c)), but this could simply be a typo.\n\n2. There are a few baselines that could have been tested:\n\n-  As stated above, instead of using a policy-based chunk selector, just score q_c on all spans, and use it to either select a span or to compute the span score q_c * p_start * p_end. This is similar to the sentence selector baseline but instead selecting chunks (which is more comparable).\n\n- A model with no recurrence but with RL chunking is missing.\n\n- One upper-bound experiment to try is to just select the span with the correct answer in the middle. This will indicate the amount winnable from doing better chunking.\n\nQuestions and comments:\n\n- Page 2: What is the chunk size for the plot?\n\n- Page 2: \"... the predicted spans are incomplete\" -- Does this mean the span is chopped in the middle?\n\n- What is the distribution of the actions that the policy takes? In particular, does it use the \"-16\" action at all?\n\n- Appendix A.4: Despite \"farmer roast\" not appearing, the answer chunk still has a strong prior (due to features such as similarity to the question and negative words).\n"}