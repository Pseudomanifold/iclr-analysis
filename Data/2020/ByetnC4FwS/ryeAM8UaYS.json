{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "One problem with modern Transformer-based text representation is the length limit. The authors propose to solve this problem by learning to chunk long documents with RL. The paper also describes a few other modules including answer extractor and a chunk scorer. The authors experiment their model on two conversational datasets.\n\nIn the introduction, the authors claims that the performance of models could be worse if answers are located far from the center of the chunk. One alternative approach to solve this problem could be to have smaller strides. Could you please demonstrate why learning to chunk is better than simply setting smaller stride in terms of performance? And it might be fair to take the stride as a hyper-parameter and tune it for better experiment results.\n\nAnother concern I have is about the experiment results. It seems that when the max sequence length is large, the gain of having a dynamic chunking system is marginal over the BERT baseline. \n\nIt's great to see that the model performs well on long documents (Table 3). But I am not sure if conversational datasets is the best to evaluate this model. As you mentioned in the introduction, some document-level RC datasets may contain very long documents (e.g. a wikipedia page). It'd be great to see how would this model perform on those datasets."}