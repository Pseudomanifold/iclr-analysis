{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a recurrent and chunking method for conversational reading comprehension (RC). The motivation is that the given documents in conversational RC tasks are usually long, but most previous approaches do not handle this problem properly. The paper introduces two methods. First, a recurrent module is used for better chunk selection. Second, in contrast to a chunking method with a fixed stride size, the model learns to chunk the document properly so that the answer span is not truncated across different chunks, which is trained with REINFORCE algorithm. The proposed method is evaluated on CoQA and QuAC, two conversational RC datasets, and the proposed method shows 0.4 and 0.5 F1 improvements over the BERT baseline.\n\n<Strengths>\n\nFirst, although the recurrent mechanism was used in several other papers (described in detail later), it is the first to be used in conversational RC.\n\nSecond, learning to chunk through RL has not been explored in the previous literature.\n\n<Weakness>\n\nOverall, I am not convinced that the contribution is significant. The contribution is narrow (focusing on how to chunk), the baselines are not strong enough, and the empirical improvement is marginal.\n\nFirst, recurrent mechanisms for selecting related chunk were explored previously, including \u201cNishida et al, Answering while Summarizing: Multi-task learning for multi-hop QA with evidence extraction. ACL 2018\u201d and so on, which are not mentioned in the paper.\n\nSecond, learning to chunk seems to be a marginal, focused contribution. Also, the reward is simply based on whether chunks do not truncate the answer span, rather than whether the document is chunked in a contextually meaningful manner. As the answer span is not across different sentences, isn\u2019t sentence-based chunking enough to resolve this problem? If it is not, then shouldn\u2019t the proposed chunking do something more than answer span based chunking?\n\nThird, I am not convinced by the baselines used in the paper.\n1) I believe that the BERT baseline uses a sliding window technique, but this paper does not mention it. Does the BERT baseline in the paper use it too? Also, the paper mentions that the number of chunks is fixed; is there a particular reason for it?\n2) For sentence selector, how many sentences are fed into the BERT model? Are multiple sentences concatenated, fed in parallel?\n3) More meaningful baselines would be: (1) chunking the document while making sure that the split doesn\u2019t happen within the sentence (as mentioned previously), or (2) chunking the document as (1), but do sliding window (e.g. if there are sentences A, B, C, D, and E and one chunk can contain 3 sentences, use \u201cA B C\u201d, \u201cB C D\u201d and \u201cC D E\u201d)\n\nLastly, the improvement over the BERT baseline is marginal (0.4 and 0.5 F1 on CoQA and QuAC, respectively, as mentioned above). In particular, \u2018max sequence length\u2019 is a hyperparameter, so only the result with the best hyperparameter should be considered.\n"}