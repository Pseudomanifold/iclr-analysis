{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a fractional generalized graph convolutional networks for semi-supervised learning. The authors design a new graph convolutional filter based on Levy Flights, and propose new feature propagation rules on graphs. Experimental results on multiple graph datasets are reported and discussed.\n\nPros.\n1. This paper presents a nice overview of three popular semi-supervised learning methods in Section 3.1, and presents insights regarding these models.\n2. A new graph convolutional filter is proposed. The motivation is clear, and the technical details are easy to follow.\n3. Experiments on five benchmark datasets are conducted. Both undirected and directed graphs are used in experiments.\n\nCons.\n1. The proposed method contains three major components: parallel FGS convolution, pooling, and residual block. Although some justifications are provided for such designs, it is difficult to justify the role of each component. In other words, it is unclear whether the performance gain is from the parallel structure, or the residual block. What would be the model performance without parallel structures? Also, given that FGCN has quite a few layers, the motivation of using residual blocks should be carefully justified. \n2. Many recent methods on graph neural networks are not discussed or included as baselines, such as [a-b].\n[a] GMNN: Graph Markov Neural Networks, ICML 2019\n[b] Large-Scale Learnable Graph Convolutional Networks, KDD 2018\n[c] SPAGAN: Shortest Path Graph Attention Network, IJCAI 2019  "}