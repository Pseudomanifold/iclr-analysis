{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "### Summary ###\n\nIn this paper, the authors focus on the problem of goal conditioned reinforcement learning. Specifically, the authors consider the setting where the agent only observes vision as input and the ground truth state is not observable by the agent. In this setting, it is hard to specify a reward function since the reward function has to compute rewards from images.\n\nThe authors first consider the setting of using a proxy reward function, and argue that false positive errors in the proxy reward function hurt the policy training significantly more than false negative errors. The authors demonstrate this empirically in a simple simulated robot arm reaching setting. Then the authors propose to use an indicator reward function that eliminate all false positive errors. The authors combine the indicator reward function with a mixture of goal relabeling schemes and a heuristic way of filtering out data with false negative rewards.\n\nThe authors evaluated the proposed method on 3 simulated robotic manipulation environments and one real robot environment. The results presented by the authors suggest that the proposed method performs better than baseline methods.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in learning goal conditioned policies from vision. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\n\nPro:\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative explanations and empirical evidence to support the crucial assumption that false positive rewards are worse than false negative rewards. The results for the main experiments are also easy to understand.\n\n2. The paper includes a fairly comprehensive set of ablation studies for each part of the proposed method in the appendix. The ablation study clearly illustrated the effects of balancing different goal relabeling schemes and filtering transitions.\n\n\nCon:\n\n1. I\u2019m not convinced about the magnitude of novelty in this paper. The indicator reward has already been used in HER[1], and the balancing of relabeling schemes seems like a direct extension of the various relabeling schemes proposed in HER. It seems to me that the only novelty of this paper comes from the filtering techniques for false negative rewards, which I do not think is enough for this venue.\n\n2. The experiment results are not very strong for the proposed method. In two of the three simulated robotics environments, the proposed method performs similarly to the indicator + balance configuration, which in my opinion is only a slight variation of HER. Therefore, I do not find the claimed advantage of the proposed method to be convincing.\n\n\nThe idea in the paper is well presented and carefully investigated. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement. Therefore, I would not recommend acceptance before these problems are addressed. \n\n\n\nReferences\n[1] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Advances in Neural Information Processing Systems. 2017.\n"}