{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose to apply HER to image-based domain, assigning rewards based only on exact equality (and not using an epsilon-ball). The authors also propose to (1) filter transitions that are likely to cause false negative rewards and (2) balance the goal relabeling so that the number of positive and negative rewards are equal. The authors demonstrate that these two additions result in faster and better learning on a simulated 2d and 3d reaching, as well as a rope task. The authors also show that the method works on training a real-world robot to reach different positions from images.\n\nOverall, the paper is a fairly straightforward and simple extension of HER, and the proposed changes seem to result in consistent improvements. Studying the importance of false positive vs false negative rewards is an interesting problem, but some of the details of the analysis are missing. Most of the writing of the paper is relatively clear, but there are a couple of assumptions that should be discussed more clearly. The analysis section seemed particularly confusing, as it introduces new notation and concepts (D without a subscript, goal observation set diameter) without explaining their significance. The experiments are relatively simple, making it difficult to judge whether or not the method will work on more challenging domains.\n\nIn more details:\n1. The related works section feels unnecessarily long. \n\n2. The analysis section is quite confusing to me and I do not understand its significance. It seems to say that reaching og rather than O+(og) is not very bad, if O+(og) is small. While this makes sense, it's not clear how the derivations of the paper add to this intuition. In particular, the paper derives an upper bound for t3/t1, which equals\n\n time to reach og / time to reach O+(Og)\n\nWhy is this \"an upper bound on the suboptimality of the policy\"?\n\n3. Can details of Figure 2 be given? This seems like a very important experiment, so it is a shame that it is not discussed or analyzed in detail.\n\n4. An assumption that is critical to this method's applicability is the assumption that no two states will receive the same observation. Without this assumption, the statement, \"the reward is positive only if ot+1 = og, which implies that f(ot+1) = f(og), or equivalently, st+1 = sg\" is false, and so therefore the statement, \"It should be clear that this reward function will have no false positives\" is also false. I do think that this is a reasonable assumption to make for some domains, but the authors should make that explicit.\n\n5. For the \"Indicator\" baseline, what is the probability used for relabeling with a future state? I assume p1=0, but what are p2 and p3?\n\n6. The ablations (Section C, Figure 5) indicates that only the filtering matters. I don't think this detracts from the author's submission, but it would be good for the authors to highlight the main paper. Currently, the statements, \"These experiments show that **both** Balance and Filter are important for optimal performance across the environments tested.\" seems unsubstantiated by the experiments--only filtering seems important.\n\n7. The authors should clarify the difference between the \"true reward\" and the \"original reward.\" What is the relationship? The last paragraph of Section 4 discusses false positive rates and says \"true reward is defined by O+(og) = {o | ||o \u2212 og||2 < epsilon}.\" However, two paragraphs earlier, the paragraph also discuses false positive rewards and says, \"under the original reward function r(st, sg).\" The experiment section then says \"Oracle uses the ground-truth, state-based reward function.\" Is the assumption that the \"true\" reward (defined in image space) is a perfect representation for the \"original\" reward (defined in state space)? Does this method work if this assumption is violated?\n\n8. (very important) How sensitive is the method to q0, and how was it chosen for the experiments?\n\n9. Figure 6 in the Appendix is very difficult to read.\n\nI would be inclined to raise my score if the authors\na. clarified some questions above\nb. conducted ablations to study the importance of q0, the threshold\nc. added experiments on more challenging domains (e.g. cube pushing, pickup) that demonstrated the effectiveness of the method on more domains"}