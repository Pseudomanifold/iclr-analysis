{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary\n---\n\n(motivation)\nCNN image classifiers tend to overfit to distractor patterns.\nPerhaps these patterns are spatially local, such that in most images signal is at one\nlocation while the noise models tend to overfit to is somewhere else.\nIf so, then generalization should improve if models are given additional\nsupervision (i.e., a mask identifying salient regions) that specifies where the signal is and is not.\nThis paper designs the Actdiff loss to realize this intuition.\n\n(approach)\nActdiff:\n1) The Actdiff loss requires a mask that highlights areas of the input\nimage which have signal and not distractor regions. It extracts features from\nthe original input image and its masked version then encourages the two features\nto be similar at every layer of the CNN using an L2 loss.\n\nActdiff is compared to 5 other methods including a reconstruction loss and Gradmask (previous work).\n\n(evaluation - synthetic dataset)\nA synthetic dataset is constructed for a simple binary classification task based on the presence of simple shapes.\nTwo patterns can predict the correct class at train time, but one of the patterns is removed at test time so additional information (masks in this case)\nis required to specify which pattern the classifier should use.\nAll the losses except Actdiff achieve at best 50% accuracy on the val set, but Actdiff gets 80% or more accuracy on 3 of 4 tested model variations.\nThis shows that Actdiff can effectively introduce the relevant masking information.\n\n(evaluation - Medical Segmentation Decathalon)\nThis dataset provides 3 segmentation tasks (Liver, Cardiac, Pancreas), including ground truth masks for those images.\nAll 6 methods outperform all the others at least some of the time.\nThe conclusion is that adding mask information using Actdiff doesn't improve segmentation performance.\n\n(evaluation - Multi-Site dataset)\nA final task tries to construct another synthetic dataset out of real X-ray images\ncollected at two different places.\nThe train set is largely from one place and the test set is mostly from the other place, and masks are constructed so Actdiff can try to eliminate this bias.\nActdiff causes a negligible increase in performance.\n\nThe paper concludes that signals CNNs tend to fit to in this type of data are not very spatially distinct.\n\n\nStrengths\n---\n\nThere is some novelty in the approach. The actdiff loss makes sense and masking + activation mapping have not been tried together before to my knowledge.\n\nExperiments follow a logical progression, starting by verifying the idea on a synthetic dataset, then moving to real data, and then evaluating on a half-synthetic dataset designed to debug the approach.\n\nExperiments average over many random initializations.\n\nThe paper embraces its negative result.\n\n\nWeaknesses\n---\n\nThe approach is not very compelling to me:\n* Implicit in this paper is that any information outside the mask is a distractor and any information inside is not a distractor. Why should the particular masks chosen for the experiments have this property? How can an expert know which features a model will find useful?\n\nThe paper's novelty is somewhat limited. The idea of regularizing using saliency maps has been explored and even applied to medical data like the MSD used here in Gradmask (one of the strong baselines this paper compares to). Activation matching is also common (e.g. [1]), though it has not been combined with masking before.\n\nWhile the weights applied to the various losses are provided, it's not clear how they were tuned. In this case there may be lots of competing losses, so it's important to tune the weights somehow to ensure the tradeoff between losses is optimal.\n\nThe results (and the conclusions) suggest Actdiff is not very effective at increasing generalization. Table 2 reports test results on all datasets. In that table, each loss outperforms all the other losses in at least two cases (a case is a model-dataset pair).\n\n[1]: Gatys, Leon A. et al. \u201cA Neural Algorithm of Artistic Style.\u201d ArXiv abs/1508.06576 (2015): n. pag.\n\n\nMissing experiments:\n\n* In the Multi-Site experiment, compare to what happens when the circular mask is applied to all images and not just those from one site. This is a necessary control to be sure that any benefits from masking are due to domain transfer and not other regularization effects. Either conclusion could be useful, but it would be nice to know.\n\nPresentation weaknesses:\n\n* In the synthetic dataset the model cannot tell the difference between correct and incorrect signals at train time. Therefore, I think there's no way for some of the baselines (plain classifier, autoencoder) to generalize correctly. Is that right? If so, it should be clear that comparisons to these baselines are not fair when discussing the synthetic evaluation in section 4.\n\nMissing details / points of clarification:\n\n* What is the Conv AE? I assume it is a CNN based autoencoder of some sort. A detailed description of the non-standard architectures would be useful for reproducibility, though probably only in the appendix.\n\n* What are the lambda hyperparameters? I assume these are weights on the corresponding loss terms, but this is never made explicit.\n\n* How does f(.) relate to the function o_l(.)? Is o_l(.) an intermediate step in f(.)?\n\n* The MSD dataset is not clearly described. Is this a classification dataset where classes are different diseases? What do the ground truth masks capture?\n\n* I think only Conv AE and UNet contain reconstruction losses. This presentation is a bit confusing since reconstruction loss was presented as another loss and it shows up in the tables implicitly based on the architecture being compared.\n\n\nSuggestions\n---\n\n* This paper would be a bit more convincing if it started with a concrete example of the problem illustrated on some dataset (e.g., maybe an example from Gradmask). That may also help drive intuitions later on in the paper.\n\n\nPreliminary Evaluation\n---\n\nClarity: The paper is fairly clear.\nQuality: Quality is mixed. Lots of relevant experiments are reported but they don't support clear conclusions and I'm not sure how well the models were tuned.\nOriginality: There is some novelty, but it is limited as discussed above.\nSignificance: I see limited significance.\n\nFor me this paper requires special scruitiny because it presents a negative result. Here are some factors that come to mind when thinking about whether to publish a negative result:\n* Is the approach compelling? - This approach is not very compelling (e.g., comments about limited novelty and lack of concrete examples to boost intuition).\n* Are the experiments thorough? - The experiments could be significantly more thorough (e.g., comments about tuning lambda).\n* Will readers learn something useful? - This paper may help researchers trying to leverage similar intuitions, but it won't be very useful outside this audience.\n* Does the paper present experiments that promote deeper understanding of why the approach failed? - This paper makes significant reasonable steps in that direction with sections 4 and 6, but I was still a bit dissapointed with the conclusions of these sections.\n* Does the paper discuss alternative approaches that were investigated? - Many alterantive approaches were considered and their performance reported.\n\nOverall I think this paper is close but fails to meet the bar because it does a bit worse than expected on most criteria above.\n"}