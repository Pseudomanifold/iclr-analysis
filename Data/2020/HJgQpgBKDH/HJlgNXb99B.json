{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new approach for handling the setting where the user is given (a) a small number, m, of clean labeled data points (i.e. assumed to be correctly labeled); and (b) a larger number, M >> m, of noisily labeled data points (note that while the paper refers to this as the weakly supervised setting, in this reviewer's opinion, that nomenclature usually refers to when just (b) is present).  The authors then propose a \"meta learning\" approach in which a \"meta model\"- the \"label correction network (LCN)\"- is trained on the clean labeled dataset to take as input a weakly labeled data point, (x,y'), and output the estimated correct label y_c, and then a \"main model\" is trained on these labels (the weakly labeled dataset with the labels from the LCN).\n\nThis paper follows a fairly standard approach for implementing the above procedure, so the bulk of the claims rest on the empirical evaluation in the experiments section.  Unfortunately, the experiments section leaves a lot of questions unanswered, and unfortunately does not support the claims in the paper well enough to merit acceptance in this reviewer's opinion.\n\nTo start, the paper considers three noise models for the weak labels: (A) uniform noise, (B) class-conditional noise, and (C) data-dependent noise.  (A) and (B) are commonly considered models, while (C) is a somewhat new one considered in this paper. \n\nit's important to note that in the majority of the settings, the proposed method seems to really only do better than other approaches in setting (C). In fact, while the authors claim that (A) and (B) \"are similar to one another\", and therefore relegate (B) to the appendix, an inspection of the appendix makes it clear that in setting (B), the proposed approach actually does worse relative to baselines (e.g. see Table 4 vs. Table 5)... regardless, the overall conclusion seems to be that the proposed approach really only does better in setting (C).\n\nThis then raises two questions: (1) is there any bias coming from the fact that the noisy labels in (C) are being generated from the same or similar model class as is used in the LCN to correct the label noise, as appears to be the case?  And (2) is the proposed approach actually doing anything useful, or is the conclusion just that training a model on a small amount of weakly labeled data does pretty well?  Going through these concerns in more detail:\n\nRe: (1): To generate the noisy labels in setting (C) (the \"WEAK\" setting in the paper), the authors use a set of \"weak classifiers\".  To start, there should be additional detail on what these classifiers are (see note about general lack of clarity in experiments section below).  However, from the provided code, it seems like the same pre-trained models are used to generate the weak labels as are used in the LCN to correct for this noise.  This seems to inject a significant bias, which would unduly advantage the proposed method in the one setting where it appears to do better than prior approaches.  Regardless, this is a concern that should have been addressed.\n\nRe (2): A second, more significant question is whether the large weakly-labeled dataset, and the main model trained on it, actually provides value over the LCN model trained on the cleanly labeled data?  I.e., is the proposed approach actually doing anything more valuable than just training a standard supervised model (e.g. the LCN) on clean data would do?  In more detail: the method works best in settings where the LCN is trained on clean data (1-10% of the main dataset according to Appendix tables, which is a significant amount of labeled data for a pre-trained model like that used for the LCN); however, is this just because this LCN model is being trained well enough to be a good discriminator?  Or is the proposed setting- where the LCN is then used to correct the labels of the larger weak dataset, and then a model is trained on this dataset- actually providing additional value?  Figure 5(a) seems to indicate that indeed the LCN model trained on a small amount of clean data might just do fine on its own... either way this is a key point that needs to be ablated / explored\n\nMore broadly, there are many unclear points in the experiments section that make the results hard to interpret.  For example the explanations around both Tables 2 and 3 are confusing and underspecified.  Table 3 is particularly poorly explained: what SSL (\"Semi-supervised learning\" I presume?) methods are used?  These are never mentioned?  What noise model is being used (I assume (A), \"UNIF\", since I see a \"p=0.6\", but the paper never says)?  Why are the number of clean labeled data points seemingly so arbitrary (ranging from 60 to 2.5k)?  What is \"MAE\"?  Etc.\n\nIn summary: this paper leaves open some key questions as to the value of the proposed approach, and the experimental setup in general, that would need to be answered more thoroughly for this paper to merit acceptance."}