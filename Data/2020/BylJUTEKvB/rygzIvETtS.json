{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel Cross-Iteration Batch Normalization (CBN) to address the limitation of BN in the case of small mini-batch sizes. Different from existing methods, CBN exploits the statistics cross different iterations to obtain more accurate estimates of the data statistics. Specifically, the proposed CBN uses Taylor polynomials to approximate the statistics using the information of multiple recent iterations. The experiments on both image classification and object detection tasks demonstrate the effectiveness of the proposed method.\n\nPlease see my detailed comments below.\n\nPositive points:\n\n1. The authors propose a novel method that exploits the information from different iterations to estimate the normalization statistics more accurately. \n\n2. Unlike existing methods, the authors use Taylor polynomials to cast the information of previous iterations into the current iteration. The proposed method is theoretically sound.\n\n3. The experiments on both image classification and object detection tasks show the superiority of the proposed CBN over the considered baseline methods.\n\nNegative points:\n\n1. The authors only conduct experiments on a single image classification model ResNet-18. More deep architectures should be considered in the experiments, e.g., DenseNet, MobileNet, etc.\n\n2. Several state-of-the-art normalization methods should be compared in the experiment of \" sensitivity to batch size\", including SN [1] and DN [2].\n\n3. One limitation of the proposed CBN method is that it takes much more computational complexity and memory consumption than the baseline BN method (See Table 6). It is not clear whether the performance improvement comes from the increased computational cost.\n\n4. It would be stronger to compare the inference time of different normalization methods.\n\n5. This paper considers the case of small mini-batch sizes. However, the authors only investigate 5 different batch sizes, such as (32, 16, 8, 4, 2). What would happen if the authors set the batch size to 1?\n\n6. The differences from a closely related work [3] should be discussed. This paper exploits the similar idea of computing the statistics of the current iteration by exploiting the statistics of multiple recent iterations.\n\n7. Some typos\n\n(1) In the experiment of Comparison of feature normalization methods, \u201cSimilar as the results ...\u201d should be \u201cSimilar to the results ...\u201d\n\n(2) In the section of Effect of compensation, \u201cThe train curve ... \u201c should be \u201cThe training curve ...\u201d\n\nSome References\n\n[1] \"Differentiable Learning-to-normalize via Switchable Normalization.\" ICLR, 2019.\n[2] \"Differentiable Dynamic Normalization for Learning Deep Representation.\" ICML, 2019.\n[3] \"Double Forward Propagation for Memorized Batch Normalization.\" AAAI, 2018.\n\n"}