{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper tackles the problem of batch normalization (BN) instability when using small batch sizes. As the fix, authors propose the following modifications to BN:\n 1) Use previous iterations statistics to virtually increase batch size\n 2) Correction via Taylor series linearization of of the previous iterations to compensate weight changes\n\n \nPaper shows in experiments on classification (ImageNet), detection (COCO) and segmentation (COCO), that the proposed method works on par with other normalization methods. Paper also shows that the proposed method does not work well for the beginning of the training and use \"burn-in\" period when standard BN is used instead of proposed CBN.\n\n\nQuestions: \n\n 1) Is it possible to use proposed method with batch size = 1? This could be a killer feature.\n 2) Why the CBN/BN memory footprints ratios for ImageNet and COCO (Table 6) are so different? (2.75 x for ImageNet vs 1.09 for COCO).\n It seems that for ImageNet there is no benefit of using CBN, because it takes 3x more memory than BN (0.66 vs 0.24 Gb), so one can just increase batch size 3 times for standard BN. \n 3) Paper reports results for \"validation set\", yet \"hyper-parameters were set by cross-validation\". Could you please specify, how was cross-validation done? \n 4) There is a practice of gradient accumulation:  doing multiple forward-backward passes, then apply optimization step once. Could you please comment, how such practice may possible interfere with proposed weight compensation? \n Wouldn`t it be benefitial to do similar practice for cross-iteration BN, so that no weight compensation be needed for such case?\n5) What are potential use cases of CBN compared to use  BN for big batches and GN for small batches, even bs = 1?\n \n\nOverall I think that paper is OK, but don`'t see practical applications where it is beneficial to use CBN instead of BN (for big batches) or GN (for small batches, even bs = 1).  I may be willing to increase my score, if my questions would be addressed. \n\n"}