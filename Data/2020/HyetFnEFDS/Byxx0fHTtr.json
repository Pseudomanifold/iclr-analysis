{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors proposed a method to optimize the topology of neural networks in a soft fashion. The main idea is to formulate the network as a complete graph (or a sequence of complete subgraphs), and to optimize the relative importance of each edge using gradient descent. The overall approach is similar to differentiable architecture search, except that (1) the continuous architecture is optimized wrt the training set (instead of the validation set), and (2) the learned architecture is never discretized at the end of training.\n\nThe paper is well-organized and easy to follow. The authors have also conducted controlled experiments to convincingly show that the method is leading to improvement. \n\nI'm a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable NAS to a search space analogous to the one used in [1]. In fact, the notion of soft topology has already been introduced in this prior work (Figure 2 in [1]: \"The aggregation is done by weighted sum with learnable positive weights w0, w1, w2\"), which was also optimized using gradient descent. A difference between this work and [1] is that whether the underlying graph is complete or randomly generated, but such a distinction is minor (we can always get densely connected random graphs by adjusting the hyperparameters of the graph generator).\n\nIn addition, I'm not sure whether the learned continuous \\alpha can be conveniently referred to as \"topology\". Note the mathematical definition of topology is discrete by nature. I believe the authors would need to either revise this terminology (e.g., by referring to it as \u201csoft topology\u201d, as a generalized definition of hard topology), or provide a way to induce discrete subgraphs from the continuous architecture. Sparsity regularization alone may not be sufficient as the non-zero \\alpha's are still real-valued.\n\nMinor issue:\nI like Figure 1 a lot. However, it seems the equivalence between the 3rd and the 4th sub-figures in Figure 1 can only be established for ResNet-V2 blocks, where there is no ReLU after each addition. It is not immediately obvious how this analysis can generalize to ResNet-V1 blocks (which still offers reasonably good empirical performance).\n\n[1] Xie, Saining, et al. \"Exploring randomly wired neural networks for image recognition.\" arXiv preprint arXiv:1904.01569 (2019)."}