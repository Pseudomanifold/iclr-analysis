{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an approach for improving teacher-student compression by introducing the assistant of GANs. A conditional GAN is trained for generating synthetic data. Then, the generated data combined with training data is used for knowledge distillation. Experiments on large random forests and deep neural networks demonstrate the effectiveness of the proposed method on data-augmentation. Moreover, an evaluation metric is proposed to evaluate s across-class diversity and intra-class diversity for generative models.\n\nPros:\n\n+ While using synthetic data of GANs to assist supervised learning has been shown to be failed in pervious works (e.g. [1] and also shown in this paper, this paper presents a new perspective to utilize GAN as a successful data-augmentation technique in teacher student paradigm.\n+ Experiments are conducted in several settings including different models (random forests, deep neural networks) and different datasets (images and tabular) to show the effectiveness of proposed method in various settings.\n+ The proposed GAN-TSC can be combined with standard augmentation to achieve higher performance as shown in the experiments.\n+ This paper is well written and easy to follow.\n\nCons:\n\n- Knowledge distillation, as a classical model compression technique, has been applied in deep convolutional models for several years. The CIFAR-10 dataset is too simple to evaluate this kind of methods. The author should try to conduct experiments on large scale datasets such as ImageNet, unless the reliability of the proposed algorithm would be very limited.\n- The proposed TSCScore seems to be similar with [1], especially when its novelty mainly lies in intra-class diversity compared with IS. It\u2019s necessary to discuss difference between TSCScore and [1].\n\n[1] Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. How good is my GAN? ECCV 2018.\n"}