{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This is a paper about a very interesting topic, involving both learning (in a supervised way) to induce a causal graph and taking advantage of it in a goal-conditioned policy.\n\nThis is clearly a timely topic and I loved the motivations of the paper. My main difficulty was with understanding the actual architecture and its motivation, but I believe this is fixable but is a serious impediment to being able to evaluate the paper, as it stands. I was a bit disappointed to see that training is mostly supervised (both providing the ground truth causal graph and an oracle policy as target) but on the other hand it is impressive to obtain these results with raw images as input and the comparative results are good.\n\nFirst, I would like to better understand the insight behind the architecture of F and several things would need to be clarified to enable reproducibility and making sense of the equations. I would start by suggesting to add an example illustrating why simply seeing a (state,next-state,action) triplet is sufficient to obtain a bit of evidence in favour of a particular edge of the graph. Since this is supervised learning of the causal graph, I imagine that the semantics of the node is predetermined, which is a bit disappointing (but doing otherwise would be understandably much more challenging). Second, I don't understand the structure of the causal graph C. What are the input nodes? action values? action x state cross-product? What are the output nodes? Effect variables? Why would C be NxN and not have different input and output dimensions? All this really needs to be clarified. Based on the 1st eqn of page 4 (PLEASE NUMBER YOUR EQUATIONS!!!) it looks like C is number of actions by number of actions, which does not seem consistent with any reasonable interpretation. The authors should also clarify how R is computed (if it is a straight difference of the encoder output, put up an equation for example) and how delta e is computed.\n\nThen in sec 3.2 the authors talk about a weighted sumn involving selected edges. I imagine this is soft-attenetion but it needs to be clarified with equations and explanations. What is the \"content\" associated with each edge e which gets averaged in the soft attention?\n\nI found a possibly interesting parallel between the focus of attention on one edge of the graph  at a time (figure 3) and the ideas of the \"Consciousness Prior\" (Bengio 2017, on arXiv) bottleneck (where only a small tuple of variables, corresponding to an edge here, is considered at each time step in order to reason, plan, decide etc). The fact that in the experiments this attention mechanism helps seems to support the sparsity of dependencies hypothesis underlying the consciousness prior.\n\nMy rating is weak reject but I am ready to upgrade with appropriate explanations answering the above questions.\n"}