{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors describe a method for endowing an artificial agent with causal reasoning  when completing goal-directed tasks. Causal reasoning knowledge is encoded in a directed acyclic bipartite graph or slightly more complicated \"master switch\" variation.\n\nEssentially the method is:\n  - train a model F to predict causal graphs (for which we have ground truth data) from trajectories generated with a heuristic,\n  - train an policy \\pi_G attending over the causal graph to solving tasks.\n\nAt inference time F an \\pi_G are frozen and are evaluated on similar tasks to the ones in training, where however the (unseen) causal graph is new. Thus the evaluation is in a meta-learning scenario, where the model has to learn how to learn to solve tasks.\n\nThe experimental setting is an agent controlling 5 or 7 switches in a simulated environment and observing 32x32x3 images of the environment.\n\nThe authors report significant improvements over an existing baseline (Dasgupta et al. (2019)). The key to the improvements is in the iterative prediction of the causal graph (for F) and in having attention over the causal graph (in \\pi_G).\n\nThe paper is describes a very nice new method and makes interesting points about the role that causal reasoning can play in modeling agents in goal-directed tasks. The paper is strengthened by improvements on existing baselines based on their intuitions.\n\nQuestions:\n\n* The authors claim that constructing an explicit causal structure, instead of a latent feature encoding, leads to better generalization in \u201clong-horizon\u201d tasks -- but they seem to only test on one task, fairly limited in complexity.\n\n* On page 1, the authors claim that empirical evidence suggests the lack of correct causal modeling is an important factor for lack of generalization, generation of unrealistic captions, and difficulties in transfer learning. This seems to be a bit of an overreach. It\u2019s possible that many empirical problems could be solved to a large degree by advances in machine learning without having to resort to explicit causal modeling.\n\n* On page 2, is the reward function r the same for all MDPs?\n\n* Could it be said from the start that pi_I is heuristic and described in section 4.1?\n\n* There seem to be strong assumptions on the structure of C that are only stated late in the paper. C is initially presented as an arbitrary acyclic graph, but from Figure 2 it appears to instead be a bipartite graph with N source nodes and N target nodes, and potential connections from any source node to any target node, where N is the number of actions. This should be explained early on. The \u201cmaster switch\u201d variation does not seem to exactly fit the mathematical descriptions from the \u201cMethods\u201d section.\n\n* \\hat{C}_H does not seem to be defined except inside Figure 2. It should be defined in the text.\n"}