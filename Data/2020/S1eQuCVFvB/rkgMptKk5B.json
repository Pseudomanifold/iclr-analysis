{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a way to measure a notion of surprise (disparity between prior and posterior) and using that as a classification rule. So, if the posterior for a class is larger than its prior, the method outputs that class label. In order to estimate the average prior for a pool of models, the method recommends building additional models to predict these \u201cpeer-priors\u201d, whereas the posterior is simply estimated by a maximum likelihood model (P(y=1) = Indicator(y=1)/K). The authors also propose a variant that directly tries to predict when the majority answer might be wrong. The authors show that such methods can do better than majority voting and some ensemble methods on a few datasets.\n\nThe paper looks at an interesting problem of when a majority vote response might be wrong by extending the notion of surprise previously defined for human labelers to machine learning models. The strong points of the paper:\n1. Simple and interpretable extension of a previously studied method. \n2. The method can be plugged into existing frameworks to replace majority voting.\n3. The results on the datasets considered seem good.\n\nHere are some of my concerns:\n1. The baseline of majority voting is fairly weak, since there are several models that take worker quality into account when aggregating responses. For example, the classic Dawid-Skene (1979) model. It\u2019s not very promising to see a model just beat majority voting.\n2. The presentation of results in Table 1 and 2 can be improved. Even though it\u2019s good to know how many predictions were corrected, it will also be good to see the overall accuracy numbers (like the ones Table 3).\n3. DMTS doesn\u2019t really have the same underlying machinery as HMTS, since it doesn\u2019t operate on the surprise measure, and putting them together in the same paper dilutes the focus. \n4. What are the weights in the weighted majority?\n5. The results in Table 3 show that HMTS is better than other ensemble methods but only marginally. Since HMTS uses more complex intermediate models (MLPs), I am not convinced whether the small improvement is from the proposed method or just more expressive models. For example, what would happen if the base classifiers in Adaboost were MLPs?\n6. How is minority defined in multi-class scenario?\n7. Minor formatting and grammatical issues: \u201clikely to the correct\u201d, \u201cmajority is tending to be correct\u201d, \u201cseemingly irrelevant topics\u201d, \u201cwhether adopting the minority\u201d, \u201caim to provide instruction to cases\u201d, \u201clinear regression\u201d (should be \u201clogistic regression\u201d since it\u2019s a classifier), and so on.\n\nIn summary, I think the paper has an interesting approach to an important problem, but with results that are only marginally convincing. I would have liked to see a more thorough empirical investigation to clearly establish the value of the proposed method. Based on these observations, I think the paper misses the mark and is slightly below the acceptance threshold for me."}