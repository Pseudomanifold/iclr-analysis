{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes two machine learning adaptations of the Bayesian truth serum approach to aggregating predictions from human experts. The first method proposed involves training two regression models for each classifier in the ensemble that predicts the proportion of other classifiers that assign the same label to a novel instance. The second approach is to train a binary classifier that, based on the features associated with an instance, determines whether the most common or second most common prediction made by individual ensemble members should be the prediction made by the ensemble.\n\nPros:\n+ The core idea of adapting Bayesian truth serum to ensemble prediction in machine learning seems sensible\n+ There is some evidence that the methods have an advantage over other common ensemble approaches in practice\n+ Although there are quite a few small English mistakes, the paper is well structured and generally quite easy to follow\n\nCons & questions:  \n1. The theorem statements and proofs are underwhelming. First they seem quite vague. Second, it\u2019s not clearly spelled out how the theorems relate to the presented method, and whether they really says anything useful about its correctness or efficacy. At face value they do not obviously analyse the correctness of the algorithm as claimed on pg6. \n2. The paper contains some simple experiments, but I do not believe they are an adequate enough evaluation of the proposed approaches. The most useful comparison are the results given in Table 3, but only three datasets are used, the margins are small, error bars are not provided, and no significance testing is performed. \n2.1 Standard practice when comparing multiple classifiers on multiple datasets would be to employ Friedman/Nemenyi post-hoc tests to determine relative performance of methods---see \"Statistical Comparisons of Classifiers over Multiple Data Sets\" by Janez Dem\u0161ar (JMLR, 2006).\n2.2 At minimum we expect Tab 3 to report results for all datasets used in the earlier experiments.\n2.3 The experiments compare with AdaBoost, Random Forest, and Weighted Majority. I feel that stacking is probably the most interesting baseline to compare with, as this is a method for learning how to aggregate predictions from ensemble members.\n\nOther: \nA. This paper does not directly deal with representation learning, so is only loosely relevant to ICLR.\nB. Not clearly unpacked why regression models need to be trained to predict \\hat{y}_i^j (Equation 4) when this quantity can be computed at test time without knowing the ground truth label?\nC. I do not understand the \"X out of Y\" description given in the caption to Table 1, which makes the results in this table hard to interpret. What is meant by \"classifiers' disagreement is high enough\"?\n"}