{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper explores the idea of removing the prior regularization, such as the KL regularization in variational autoencoders. In order to allow sampling from the generative model, the prior distribution is parameterized by a normalizing flow that is fit to minimize the cross-entropy between the marginal posterior distribution and the prior. I find the idea to be interesting. However, the paper, especially the experimental section, is in a very rough state, making assessing the contribution hard. I recommend rejection of the paper in the current state, but encourage the authors to finalize the paper.\n\nPros:\n1. The paper proposes a fairly radical change to the VAE setup.\n2. The results look sensible and in some aspects better than the results of VAEs and GANs.\n\nCons:\n1. As mentioned above, the paper is clearly unfinished. There are many formatting issues (too small figures; \\citet and \\citep being used incorrectly). Most importantly, the text is unnecessarily verbose. As an example, autoregressive flows are discussed in detail, yet not used in the experiments.\n2. Despite the verbosity, there is no discussion of the downsides of the removal of the regularization. For instance, I would expect the log-likelihood of the proposed model to be arbitrarily poor. Furthermore, it seems that the optimal encoder distribution in this setup is a delta-function, which makes the aggregate posterior a mixture of delta-functions and can lead to issues when fitting a prior distribution.\n3. I\u2019ve found the disentanglement results confusing. Beta-VAE usually requires beta > 1 for disentanglement, but this paper achieves better disentanglement for beta = 0 than for beta = 1"}