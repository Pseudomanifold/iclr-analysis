{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, the authors study VAEs which are trained with a loss that does not contain the typical KL divergence term.  Additionally, the authors focus on the case where the prior (latent) distribution is learned via a normalizing flow.  The authors empirically show that higher quality samples can be generated by VAEs without the KL divergence (than in comparison to those with the term) in this context; that the linear separability of the latent representations increases without the KL term, and that greater diversity can be obtained by conditional sampling.  \n\nThe observation that the authors report is intriguing and definitely worth further inquiry, but I feel that the paper in its current form does not present a coherent picture that other researchers can easily grasp and build upon.  The quality of the writing is a significant barrier toward understanding the deeper meaning of the paper.  As a result, I recommend declining this paper for publication in ICLR.  \n\n(1) The paper needs a much clearer focus.  For example, the title of \"empirical observations pertaining to ...\" should be replaced by something related to the actual claim of the paper\n(2) The abstract is too vague for experts or non-experts to get a clear indication of what is in the paper\n(3) Where does this work leave the relative performance levels of VAEs versus other generative models? \n(4) How broadly should the results be interpreted for VAEs?  Does it only apply to flow-based priors?  Or is it making a claim for a broader set of learnable priors?  This will also affect what is in the title and abstract.  Is it only being claimed for L2 loss (which then admits a 2-Wasserstein distance problem) or is it being claimed about a broader category of losses?  If so, what is the principle under which removing the KL terms is reasonable when it doesn't lead to a 2-Wasserstein problem?\n(5) Individual sentences of the paper are adequately written, but the writing of the paper currently feels like a long list of facts and details, which makes extracting the deep takeaway much harder.  It is unclear after reading this paper what I would tell another researcher or a student, though I feel that with a substantial revision there could be some very interesting things that could be told based a continuation by the authors of this work."}