{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The proposed work investigates the problem of learning hierarchy in RNNs. Authors note that different layers of the hierarchy are trained in \"sync\". The proposed paper suggests to decouple the different layers of hierarchy using auxiliary losses.\u00a0 The form of auxiliary losses used in the paper are of the form of local losses, where there is a decoder, which is used to decode past inputs to each level from the hidden state that is sent up the hierarchy, therebyforcing this hidden state to contain all relevant information.\u00a0\n\nClarity of the paper: The paper is clearly written.\n\nMethod: The proposed method\u00a0 ignores the gradients from higher to lower levelsin the backward pass,\u00a0 (because of this, the authors can also save some memory). In order to compensate for the lost gradients, authors propose to use local losses, and\u00a0we introduce an auxiliary loss term to force this hidden state to contain all information aboutthe last k inputs. The authors note that the hidden state from the lower level (to the higher level) should contain the summary of the past, and hence use a decoder network (which is simply parameterized) as a feedforward network which is used to decoder a \"past\" hidden state.\u00a0\n\nRelated Work Section: The related work section is nicely written. The authors have covered mostly everything. These 3 papers may still be relevant. (a), (b), (c).\u00a0 \u00a0(b) could be relevant for mitigating the parameter update lock problem as mentioned by authors in the introduction of the paper. (c) is also relevant as authors in (c) also consider using auxiliary losses for learning long term dependencies.\u00a0\n(a) SkipRNN:\u00a0https://arxiv.org/abs/1708.06834(b) Sparse Attentive Backtracking:\u00a0http://papers.nips.cc/paper/7991-sparse-attentive-backtracking-temporal-credit-assignment-through-reminding\n(c)\u00a0 Learning long term dependencies in RNNs using auxiliary losses\u00a0https://arxiv.org/abs/1803.00144\nExperiment Section: In order to validate the proposed method, authors evaluate it on copying task, pixel MNIST classification, permutedpixel MNIST classification, and character-level language modeling.\u00a0\na) Copying results show that the decoder network are essential to achieve decent results. This task though does not show the strength of the proposed method though as baseline also solves the problem completely. It might be interesting to actually scale the \"gap\" time in copying time step to something larger like T = 1000 or something.\nb) PIXEL MNIST classification: Authors use the pixel by pixel classification task to test the proposed method. Here, the proposed method performs comparable to the hierarchical RNN (but without using too much memory).\u00a0\nc) Character level modelling: Authors demonstrate the performance of the proposed method on language modelling task (PTB). These results are particularly not interesting, as the performance gain is very marginal. Also, may be using other language modelling datasets like Wikitest103 or Text8 might be more useful.\u00a0 As for the results, even unregularized LSTM performs better than the baseline in this paper. (For reference, see\u00a0https://arxiv.org/abs/1606.01305)\u00a0\n\nWhat authors can do to improve paper:\n- The problem considered in the proposed paper is very interesting to me. Though, the results are not (yet) convincing. It might be interesting to think about a task, where there are really long term dependencies like reading CIFAR10 digit pixel by pixel and then doing classification, where the authors can actually show the promise of the proposed method.\u00a0\n- It might also be interesting to know how are the original training cost objective is weighed against the auxiliary loss. Have authors tried any search over what kind of auxiliary loss performs well ?\u00a0"}