{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to compute static (context-independent) word embeddings from dynamic (contextual) models such as CoVe, ELMo, and BERT by pooling subword encodings with sampled sentences in each layer for each word and compares different pooling methods (max, min, mean, last) on word similarity tasks. Further, the authors provide analysis for the social bias in the produced embeddings with existing approaches. \n\nStrength: 1) The paper presents extensive comparisons with different experiment settings; 2) the social bias analysis is very interesting.\n\nWeakness:\n\n1) Novelty:  The techniques to compute static embeddings are quite simple, the authors basically test all combinations of pool methods and different numbers of sampled sentences. The technical contribution is very limited. \n\n2) Presentation: The figures are not very clear. The authors should at least prevent using the same line color for different settings in the same figure (Figure 1). It's hard to tell which combination of pool methods is the best. \n\n3) Experiments: The layers used to compute static embeddings are different in Table 1 and Table 2 for different tasks. It seems that the authors chose the best scores across layers for different tasks. It's better to compute scores using the same layer for different tasks and compare the layers on the averaged score across tasks. Otherwise, there's no take-away on which layer we should use in new tasks that do not have golden labels. The choice of using the floor(X/4)-th layer in the social bias analysis is also a bit of arbitrary. \n "}