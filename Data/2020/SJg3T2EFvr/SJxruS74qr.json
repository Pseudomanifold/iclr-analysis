{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces methods for distilling pretrained contextual representation to a static embeddings for the faster use. It proposes subword pooling and context combination, and its contribution is demonstrated with a suite of classic experiments for static word embeddings.\n\nMy score for this paper is weakly rejected because (1) the motivation of the proposed approach is not clear. The paper title is about \u201cdistill\u201d, but I can\u2019t find out what the methods try to distill, what is the objective; (2) the goal of the paper is to obtain a static embedding performing like ELMO or BERT. As we know, the power of the contextual embedding is demonstrated by different NLP downstreaming task like MT, QA and a bunch of text classification tasks. In this paper, I don\u2019t find those experiments; (3) this paper compared the results with word2vec and glove, but not clear which version to compare. And there are some re-embedding approaches that improve word2vec and glove such as \u201cWord Re-Embedding via Manifold Dimensionality Retention\u201d, the table in this re-embedding paper shows different version of glove and some of them are better than the proposed methods distilled from BERT.\n"}