{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work introduces very simple methods that use contextualized embeddings from large-scale pretrained language models like BERT to distill / generate static Word2Vec-style word embeddings. The motivation for doing so is twofold: (1) static embeddings are faster to use and easier to understand, which makes them more appropriate for some real-time or resource-constrained systems; and (2) we can then use a variety of analysis methods that have been developed for static embeddings to analyze their contextualized counterparts. The proposed method is very simple, just pooling embeddings of a given word type across multiple contexts (if the word is decomposed into subwords in a context, its subword embeddings are averaged), and there is extensive evaluation across different combinations of pooling functions, pretrained base model, and layer to distill. The authors demonstrate that these distilled word embeddings outperform Word2Vec/GloVe on various word similarity/ relatedness tasks and reveal interesting insights into these embeddings. Additionally, they investigate social bias in contextualized representations and reveal inconsistencies in existing techniques for studying social bias. \n\nOverall, I found this paper to be well-written and the bias experiments in particular to be compelling. However, I felt that there were too many obvious questions left unexplored, which I've noted below, and I didn't quite buy some of the explanations of the results. As such, I am a weak reject for now.\n\ncomments:\n- While mean pooling is simple conceptually and to implement, I do wonder if the authors could have gone further with the methodology. A natural continuation is to weight different contexts differently (e.g.,  upweight common contexts and downweight rare ones). Another is to extend the idea to multisense word embeddings (e.g., through some clustering over contexts). \n- One confounding factor that could affect the experimental results is the embedding dimensionality. The GloVe / Word2Vec embeddings are both 300d, while I suppose BERT's are 768d. Of course, it is hard to control for this when using pretrained public models, but perhaps PCA or some other dimensionality reduction technique could have been applied to BERT-derived embeddings before the comparison. As is, it is unclear how much of the improvement comes from the model size. \n- I would have liked to see the performance of the distilled static embeddings measured across a variety of downstream tasks (i.e., by plugging the embeddings into some task-specific model), not just word similarity / relatedness as these are not particularly insightful. Also, if the authors did this experiment, they could measure the change in the distilled embeddings before / after finetuning on the downstream task.\n- Fig 1 & 2 are hard to read and understand. In Fig 2, the entries in the legend don't seem to be explained anywhere. What are \"ADJ\" and \"PROF\"?\n- The result that social bias is not consistent across different layers of BERT is perhaps to be expected in light of other similar results (e.g., the bottom layers of ELMo and the middle layers of BERT seem to contain more syntactic info than others as shown by Hewitt & Manning 2019).\n- I wonder if the inconsistency in layer-wise patterns across different bias measures is not an issue with the bias measures themselves but rather with the assumption that distance metrics (e.g., cosine) are as meaningful with distilled static embeddings as they are with Word2Vec / GloVe. The latter approaches incorporate the inner product between two embeddings into their relatively simple training objectives, while the distilled BERT embeddings are computed from highly nonlinear Transformer layers. Perhaps for some layers these distance metrics are not meaningful? \n"}