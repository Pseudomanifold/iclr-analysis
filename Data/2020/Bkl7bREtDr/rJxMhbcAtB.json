{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies reinforcement learning for settings where the observations contain noise and where observations have long-range dependencies with the past. The proposed approach builds on the LSTM model and adds an aggregated memory cell, which decreases noise by allowing them to cancel at a high level.\n\nExtensive experiments are provided on two sets of tasks, the TMaze and the Minecraft tasks. The experimental results look convincing to me. However, as someone who is outside the area, it is difficult to understand the details of the paper. There are numerous observations and experimental design choices which are not clearly explained I think.\n- The current version (~ 10 pages) is significantly over length.\n- LSTMs are sensitive to noise: Is there an explanation for this observation? Also, is the claim still true by suitably regularizing the LSTM, etc?\n- TMaze Long-Short: What's the intuition for why this setting requires learning over long-term memory tasks?\n\nMore detailed comments/questions:\n- Intro P1: You start by talking about tasks that require long-term memory. Then you talk about full vs partial observations. What's the connection between these two?\n- Intro P4: This observation is interesting -- is there an explanation or intuition for what's happening here?\n- Figure 1: Why 68% confidence interval?\n- Aggregators: The 1/2 notation in the definition of m_t looks very confusing.\n- Definition of a_t, P5: what is FF_2?\n- TMaze Long Noise: By adding noise, do you mean that the observations are simply randomly sampled from {-1, +1}?\n"}