{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents an interesting idea based on introducing adversarial noise on real samples during GAN training. This novel approach may improve GAN training and have potentially large impact, but the paper in its current form is slightly below the standard of ICLR due to its lack of clarity.\n\nWhile it is very interesting to apply adversarial noise in real data, this approach is not clearly motivated or explained. For example, at the beginning of page 2, why \u201cAs a consequence, training will become unstable when generated distribution approximates target distribution because the gradient given by non-robust discriminator around real samples contains more adversarial noise\u201d? One may think that, on the contrary, the adversarial noise in generated samples would approximate that in real data when the generator distribution approximate the true data distribution. Similarly, after eq. 6, why \u201cConsequently, gradient given by discriminator may vanish when discriminator becomes stronger than generator without capacity constrained.\u201d? I would think the gradient would explode in this case given all the regularisation on gradients.\n\nIn addition, the term \u201cnon-robust discriminator\u201d has been used several times in important places, but is not clearly defined. Properties used to justify the approach, such as \u201csymmetric\u201d and \u201cbalanced\u201d need to be explained. For example, would it be possible to illustrate or measure the imbalance of discriminator?\n\nThe overhead of the algorithm needs to be stated more precisely. (after eq. 8) It is unclear to me that \u201cbackward propagation of Equation 5 with respect to x with negligible computation overhead\u201d. Would this require back-prop through the entire discriminator, which can be very deep thus costly? It would be helpful to provide an estimation of runtime overhead supported by experiments.\n\nIn algorithm 1, why is it necessary to perform discriminator update twice? How about skipping step 4 (eq. 12)? I think this may better mirror the adversarial update for generated samples, which only involves updating generator parameters once.\n\nIn section 3.4, I am not sure it is similar to unrolled GAN when the perturbation is zero. Unrolled GAN required backprop through discriminator update, which I don\u2019t think is the case here\n\nFinally, the experimental results are confusing. In Table 1, why the reproduced results are already much better than those in the original papers? A solid baseline is necessary for any further assessment. Further more, since the DCGAN and ResNet baseline models did not use recent regularisation approaches such as spectral-norm, it is hard to assess whether the proposed method can work without those existing techniques. It would be helpful to use at least a SN-GAN baseline.\n\nThe analysis also need to be improved. It is hard to interpret the histograms and L1 norms in Figure 5 as related to \u201cinformative gradient\u201d or \u201csemantic information\u201d as claimed in sectiion 4.4. Quantitative measurements such as correlation or mutual information may justify these claims.\n\nOverall, I think the idea presented is certainly promising, but needs further development to be qualified for acceptance.\n\nNit:\n\nThe 1-line derivation of eq. 6 can be incorporated into the main text.\n\nA transpose in eq.10 is missing.\n"}