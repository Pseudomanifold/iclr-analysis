{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents an ensemble method for reinforcement learning. Multiple policies conforming the ensemble are obtained by checkpointing a training policy at different times. The training policy is optimized by a \u201csingle-thread\u201d training procedure and a particular learning rate scheduling. The learning rate is cyclic (increasing and decreasing) and it is shown to be beneficial for obtaining high performance. The multiple policies from the ensemble are further filtered by a selection criteria during evaluation. Finally, the selected policies are used for action selection. The authors perform experiments showing the performance of their algorithm compared to some baselines.\n\nMy decision would be to reject this paper. The main reasons for this are: 1) the main contribution of the paper (page 4)  is poorly explained and lacks motivation. 2) The experiments although extensive in number of environments (for training) are poorly evaluated.\n\nSupporting arguments: \n\nEquation 2 needs more motivation. Why the authors have selected specifically such objective function? They could have chosen any other arbitrary loss function that enables ensemble diversity in a different way. Why the use of the L(s,a) and L\u2019(s,a)? They also introduce several parameters here that might make difficult to tune the algorithm e.g. beta, M, threshold. How do the authors compute P(s) ?  \n\nThe experiments are poorly evaluated in the following sense: \n\n- The authors provide in Figure 3 the results from evaluation of SEERL against the chosen baselines. However, in this Figure only 2 environments are evaluated. All the remaining Figures focus on the training performance (including the ones in the appendix) or unrelated to performance (Figure 4) .   Evaluation is really where one can see if a method is better or worse compared to another method. For example, see original DQN paper (Mnih 2015) to see a sensible evaluation procedure. Since  evaluation is the most important experimental analysis, the authors should devote their efforts in presenting a strong evaluation, even more, since their method highly relies on evaluation given that the ensemble consensus is only executed during evaluation (see Algorithm 1).\nAlthough many training results have been shown (see appendix) not many evaluation results are present. I believe this is crucial to show if their method is truly better than the proposed baselines.\n\n- Looking at training results SEERL seems to be highly unstable depending on hyperparameters and random seeds. For example, Figure 5(a) shows how sensitive the performance is when changing the number of policies in the ensemble. Similarly, in Figure 5 (b) one can see how much the performance varies by  slight changes in the initial learning rate. Furthermore, the training results in Figure 6 and 7 present plateaus in performance (as if the training procedure would have get stuck) and extreme drops in performance. Additionally, Figure 8, 9 and 10 show that SEERL is roughly under-performant in 6 out of 11 environments. \n\n- I believe the authors should also compare to standard baselines (e.g.  simple A2C / TRPO / PPO / SAC without ensemble).\n\n- Figure 2 (b) has no axis scale and therefore is of no use and cannot be understood.\n\n- How are the authors computing the average over P(s) in Equation (2)?\n\n\n\nMinor comments:\n\n- \u201cHowever, value function ensembles from different algorithms trained independently could degrade performance as they tend to converge to different fixed points and thereby have different bias and variance\u201d. This is only true if the objective is different even if the algorithms are the same.\n\n- The formatting of the paper seems to not follow the template since there seems to be no space between paragraphs (other ICLR papers do have such space).\n\n- The notation in Preliminaries is abused. P refers to transition dynamics but it is also used in P(A) for probability. Also, the \\epsilon is used instead of \\in. The log in equation 5 should be \\log. The advantage function in (5) is not defined.\n"}