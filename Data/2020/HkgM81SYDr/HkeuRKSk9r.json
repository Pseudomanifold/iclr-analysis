{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper proposed an ensemble method (SEERL) for model-free RL algorithms. The main advantage of the SEERL is that the training cost is small compared to other ensemble methods with multiple neural networks. SEERL proposed a dynamic learning rate schedule to generate multiple variants of policies during the training of a single instance.  The proposed method is applicable to both discrete action space tasks and continuous action space tasks.  \n\nDetailed comments:\n\n*Methodology:\nIt is not clear whether Eq 2 represents the training loss of the proposed method. Eq 3 is an indicator function, which has zero gradients almost everywhere.  The only useful term is the KL divergence term if the policies are trained with Eq 2. Does the author mean L(s, a) = L'(s, a) if the \"total error\" is greater than the threshold?\n\nThe motivation of using Eq 2 as training loss is not clearly justified or empirically demonstrated in the ablation study. What is the reason for using a threshold to select which policy to train? Is this loss shown to be more effective than just training the policies with the typical actor-critic loss?\n\nOne of the contributions claimed is the optimization framework for selecting the best policy. However, the ensemble approaches for both discrete action space (Sec 4.3.1) and continuous action space (Sec 4.3.2) are heuristic approaches. It is not clear how to justify those ensemble approaches are optimizing any objectives. \n\nTheoretically, the actor-critic/policy gradient approach is an on-policy algorithm, which means the sampled trajectory via behavior policy cannot be readily used for training current learning policy, without using techniques such as importance sampling. It is compatible with SAC since off-policy training can be directly applied, while the discussion on this issue has not been discussed. \n\n*Experiments:\nAccording to the reported results, the performance of SEERL is volatile. It fluctuates a lot comparing to the baseline approach.\n\nThe reported results do not seem to be sufficient to support the effectiveness of the proposed method. The baselines such as DDPG/TRPO/A2C are not state-of-the-art in terms of sample-efficiency. The comparison with SAC is valid.\n\n*Minor:\nEq 5. The definition of advantage function, missing a bracket, etc.\nParagraph 1, Section 5, citations of the baselines."}