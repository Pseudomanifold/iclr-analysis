{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary ###\n\nThis paper focuses on training an ensemble of policies in the reinforcement learning setting. Specifically, the author proposes a framework to train a collection of policies without the extra computation cost and sample complexity, and a method of selecting the best policies for ensemble.\n\nMost existing ensemble methods in reinforcement learning suffers from increased sample complexity or computation cost that scales with the number of agents being trained. The authors propose a method that uses cyclic learning rate to obtain multiple policies in a single training run. Specifically, the authors adjust the learning rate in a cyclic fashion, and save the policies each time the learning rate reaches its minima to obtain a collection of policies.\n\nThe authors then propose a way of selecting the top M policies for ensemble. The authors formulate the policy selection into a constrained optimization problem, where the cost function consists of a policy loss term that encourages the selection of well performing policies, and a divergence term that encourages the selection of diverse policies. After selecting the top policies, the authors use various ensemble techniques to get the final action from the policies.\n\nThe authors evaluate the proposed algorithm on the domain of simulated robot locomotion in MuJoCo and Atari game. The authors observe improved performance on some environments compared to the baseline.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in training an ensemble of polices without extra cost. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\nPro:\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative illustrations to help demonstrate the idea.\n\n2. The authors conduct a wide range of empirical experiments to support the effect of the idea. The authors test the idea on environments with both continuous action spaces and discrete action spaces. In the appendix, the authors include a fairly comprehensive evaluations of the proposed algorithm in 39 Atari environments and 6 MuJoCo environments. The authors also evaluate the proposed ensemble mechanism on top of multiple baseline algorithms.\n\nCon:\n1. I am not convinced about the benefit of the ensemble method as in many experiments presented in the paper, the baselines outperform the proposed method. In 7 of the 39 Atari environments, the baseline outperforms the ensemble method, and in 20 of the 39 environments, the baseline seems to be on par with the ensemble method. Therefore, it is not convincing that the proposed ensemble method really has an advantage over baselines.\n\n2. In many of the experiments, the baseline algorithms do not seem to be well-tuned and do not match the performance from the original papers. For example, in the MuJoCo experiments where the authors use SAC[1] as baseline, performance on 5 of the 6 environments has been reported in the SAC paper[1] and none of SAC baselines in this paper matches the performance in the SAC paper. In fact, the gap of performance is pretty large. Therefore, I would not trust the comparisons before the baseline algorithms are properly tuned and re-evaluated.\n\n3. For the Atari experiments, it would be better if the authors could compare to more recent baselines such as IQN[2] and Rainbow[3]. These algorithms perform significantly better than A2C so it is important to include these results.\n\n4. The choice of the optimization objective (equation 2) for ranking the policies seems a little arbitrary to me. Especially in many RL algorithms, the policy gradient loss for the policy and Bellman error for the value function are not normalized and therefore cannot be simply added together. It would help if the authors could provide some justifications or theoretical analysis to this ranking rule.\n\nThe idea in the paper is well presented and the authors include comprehensive experiment results. However, due to the mismatch between baseline algorithm performance presented in this paper and those presented in other papers, I\u2019m not convinced about the validity of many empirical results in this paper. Therefore, I would not recommend acceptance before these problems are addressed. \n\n\n\nReferences\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n\n[2] Dabney, Will, et al. \"Implicit quantile networks for distributional reinforcement learning.\" arXiv preprint arXiv:1806.06923 (2018).\n\n[3] Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018.\n"}