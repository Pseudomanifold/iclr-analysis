{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe MAML meta-learning objective is\n$\\mathcal L^\\text{MAML}(\\theta^\\star) := \\sum_\\tau \\mathcal L(\\mathcal D_\\tau^\\text{tst}, \\theta^\\star - \\alpha \\sum_{k = 0}^{K - 1} \\nabla \\mathcal L(\\mathcal D_\\tau^\\text{trn}, \\theta_\\tau^k))$\nwhere $\\mathcal D_\\tau^\\text{tst}, \\mathcal D_\\tau^\\text{trn}$ are test and train sets for task $\\tau$, $\\mathcal L$ is the regular training loss and $\\theta$ is the parameter of e.g. a neural network model.\nBy minimizing $\\mathcal L^\\text{MAML}$ it is possible to find a good\nuniveral initialization $\\theta^\\star$, i.e. one that generalizes well between\ntasks.\n\nThe authors propose a modification of the above update in the form of\n$\\mathcal L^\\text{GECCO}(\\theta^\\star) := \\sum_\\tau \\mathcal L(\\mathcal D_\\tau^\\text{tst}, \\theta^\\star - \\alpha \\sum_{k = 0}^{K - 1} g_\\tau^k(\\Psi) \\odot \\nabla \\mathcal L(\\mathcal D_\\tau^\\text{trn}, \\theta_\\tau^k))$\nwhere $g_\\tau^k(\\Psi)$ is a learned preconditioner for the gradients.\nThe specific model they propose for $g$ is a low rank model that also depends on\na parameter $\\nu$ which is set as the gradient of the previous gradient step\nupdated with the proposed preconditioner with $\\nu$ set to 0.\n\nThe authors propose that their preconditioner helps with the problem of noise in\nmeta learning and also reduces the number of steps required in the inner loop.\nThey claim to validate those propositions by presenting experimental evidence.\n\nI propose to reject the paper.\nThe proposed model seems somewhat arbitrary and is not well-motivated through\na priori theoretical reasoning.\nMany of the design decisions are not explained well.\nFor example the dependence on the context vector $\\nu$ is not well formalized.\nThe claims about the adaptivity gained through the GECCO model and the noise\nare not well motivated.\nFurthermore, there are in my view various inaccuracies (see detailed comments).\n\nDetails:\n- Abstract: The claim that gradient steps required for more samples in a task\nbecome more costly makes sense if we think full-batch.\nBut in that case there is no additional noise.\nSo the first two sentences already seem vague without providing a proper\nreference for what is truly being claimed here.\n\n- Abstract: \"We show that...\", perhaps specify in what way you show this.\n\n- Page 1: are are\n\n- Page 1: needs reference / reasoning for increase in noise with higher-order solvers\n\n- Page 1: what does higher-order solvers are \"required\" mean? when are they required?\n\n- Page 1: Hessian is not a higher-order solvers, it is a second derivative,\n\tperhaps you mean (quasi-)Newton here?\n\n- Page 1: \"so-called gradient steps\" perhaps _formally_ define \"gradient step\" early,\n\tif you refer to it so often (i.e. give MAML definition already in intro)\n\n- Page 2: In this paper, ... here you are just repeating what you already just explained.\n\n- Page 2: Our method converges fast and ... , in what context? / under what assumptions?\n\tSay the type of experiments this holds for.\n\n- Page 2: ...few-shot scenario, etc. , what do you mean by etc.?\n\n- Page 2: ... GECCO learns the update functions of the gradients, how do you define update functions?\n\tPerhaps better to say learns a preconditioner for the gradient step?\n\n- Page 3: $\\tau \\sim p(\\Tau)$, perhaps say something about stochastic assumptions here, is this iid?\n\n- Page 3: (nitpick) functionality of the model of interest, why functionality? $h$ is just the model / function, no?\n\n- Page 3: define / explain universal initialization\n\n- Throughout the whole paper: Many uses of articles where they do not belong.\n\tPerhaps ask a native speaker to proof read.\n\n- Page 4: immediately clarify that $\\Psi$ are the parameters of your preconditioner\n\n- Page 4: in Remark 2: Adam etc.\\ are adaptive methods whose motivation comes from\n\tapproximating the covariance matrix of the stochastic gradients and NOT approximating\n\ta Hessian. They are really quite different (especially when away from the optimum).\n\tThe adaptivity in Adam and so on is then based on the square root of the\n\t(diagonal) covariance approximation.\n\tIn quasi-Newton / natural gradient methods you would use the inverse of the\n\tHessian / Fisher.\n\tConsidering that your paper proposes a way to model something very similar to\n\ta preconditioner it would be perhaps be good if you could properly put your\n\tmodel into the context of ways to low-rank approximate\n\tcovariance / Fisher / Hessian matrices.\n\n- Page 4: Give an intuition for what $\\nu$ is supposed to be if you are going to\n\tuse it.\n\n- Page 5: Eq. equation 3\n\n- Page 5: Why do you need $\\nu'$ if you can just set $\\nu$? How is that an update?\n\n- Page 5: GECCO uses a context vector..., you have just been talking about this for\n\tthree paragraphs, why are you saying this again?\n\n- Page 5: Algorithm 1, line 7, shouldn't it be $\\tau_b$ instead of $\\Tau_b$?\n"}