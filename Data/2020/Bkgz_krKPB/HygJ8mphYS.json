{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper uses BERT in text generation via distillation. Specifically, on top of the common MLE training loss for generating tokens from left to right, this paper adds a distillation regularization term in the cross entropy form, in the hope that the likelihood of the masked token predicted by BERT (bidirectional) is close to the likelihood predicted by the autoregressive generation model.\n\nThe experiments are performed on text generation tasks like machine translation, text summarization, and image captioning. Specifically, it got SOTA on IWSLT14 German-English and IWSLT15 English-Vietnamese datasets. \n\nOverall, this paper presents a neat idea for using BERT in text generation. I would recommend a weak accept due to the following issues:\n\n1. The so-called SOTA results are not very surprising because this method uses BERT, which is pre-trained on huge extra datasets. Thus, one may argue that the comparison may be unfair.\n\n2. En-De (and En-Fr) are more important benchmark datasets in machine translation. However, the results in Table 3 are much worse than Ott et al. or Wu et al. The experiments can be more complete if WMT16 is used in training and checkpoint averaging is also employed.\n\nMinor suggestions: the text in Figure 2 are hard to discern. Please improve the quality of the figures.\n\n\n\n\n"}