{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper focuses on text generation. It proposes to 1) fine-tune BERT for conditional masked language modeling and 2) use the fine-tuned BERT as a teacher that provides supervision signals during seq2seq training. The paper is clearly written. The proposed approach lacks a strong motivation though; I would like to see more explanation on what problem is solved by adding the BERT teacher. The empirical results are okay but not remarkable. Thus I intend to reject the submission.\n\nApproach:\n- The main motivation as described in the paper is \"BERT\u2019s looking into the future ability can act as an effective regularization method, capturing subtle long-term dependencies that ensure global coherence and in consequence boost model performance on text generation.\" While I agree that looking into the future is useful, I don't see why adding BERT's future-dependent prediction to the objective is helpful. The ground truth label is also dependent on the future.\n- If the proposed approach does help \"looking into the future\", it should improve model performance with small beam sizes or even greedy search. It would be helpful to have this experiment.\n- The idea itself is very similar to label smoothing. Thus I'd like to see comparison with (uninformative) label smoothing in the experiments.\n\nExperiments:\n- How does changing \\alpha affect the results?\n- Section 4.5: Results in Figure 2 do not consistently support the claim that the proposed approach improves on longer sequences.\n\nMinor:\n- Equation 5: notation is loose. Loss should sum over examples. Unclear which words are masked in Y^u."}