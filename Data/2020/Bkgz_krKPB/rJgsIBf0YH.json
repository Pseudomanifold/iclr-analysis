{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a way of incorporating word distribution from a BERT model into the training procedure of a seq2seq model as a regularization for better text generation. The claimed advantage is that it will help train the seq2seq model by exploiting BERT's ability of \"looking into the future\". Experimental results show the model performance on machine translation and document summarization. \n\nMy major concern of this work is that it is not clear about the benefit of using a BERT as the teacher model. If the reason for performance improvement is from \"injecting future information for predicting the present\", one big missing part in the evaluation is using a bidirectional RNN as the teacher model. Then, the comparison with BERT as the teacher model will give us more information and also evidence about this work. \n\nSecond, although this paper explains the difference with the Masked Seq2seq pre-training from (Song et al. 2019), those two ideas are still very similar and the novelty of this part is not significant enough. Maybe I missed something. \n\nIn addition, the experimental results in section 4 are not strong enough to support the effectiveness of the proposed method. Specifically, is it possible to combine the proposed method with the Transformer (big) to show further improvement?\n\nOther comments on the detail\n\n- I think something is missing in equation 4\n- I am not convinced about the explanation associated with Table 4 in section 4.3. Unless there is a good reason (other than the test set contains some \"noisy\" example), I do not think modifying the test set to get some favorite results is appropriate. The bottom line is to run other competitive systems with the same internal split. \n"}