{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a simple and interesting strategy, DropEdge, to alleviate the over-fitting and over-smoothing in GCN. The logic is simple and clear and the paper is well-written. \n\nMajor concerns:\n1. After randomly enforce a certain rate of edges to be zero, how to preserve properties in the original complex network, such as degree power-law distribution, communities? If it was not necessary to preserve the properties, then what information should be preserved from the original graph.\n2. Randomly drop edges may result in disconnected components, how to handle disconnected components?\n3. Why do the authors use dimension difference as the measure to quantitatively evaluate information loss in Thm 1. More dimension reduction does not mean more information loss.\n4. As a follow-up concern for C1, graph sparsification makes more sense than DropEdge because it has clear information reserve targets while there is no target for the randomness in DropEdge.\n5. In Table 1 and Fig 2, why the improvements for more layers are bigger than those of the fewer layers?\n6. In Fig 2, why the trend of Reddit dataset is so different from others (the more layers the more improvements by applying DropEdge)? \n7. In Table 2, why there are the DropEdge versions for some methods while not for some other methods (e.g., FastGCN, ASGCN)? Why there is no result of GAT?\n\nMinor:\n1. Sec 3, \"notation\", \"\\mathbf{x}_n\" -> \"\\mathbf{x}_N\"\n2. Eq (1), \"\\mathbf{h}_n^{(l+1)}\" -> \"\\mathbf{x}_N^{(l+1)}\"\n3. What's C_l in the explaination under Eq(1)?"}