{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs).  Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors. Ultimately, the authors provide solid empirical evidence that, while a bit heuristic, their method is effective at alleviating at least partially the issues of overfitting and oversmoothing.\n\nI vote weak-accept in light of convincing empirical results, some theoretical exploration of the method's properties, but limited novelty.\n\nPros:\nSimple, intuitive method\nDraws from existing literature relating to dropout-like methods\nLittle computational overhead\nSolid experimental justification\nSome theoretical support for the method\nCons:\nMethod is somewhat heuristic\nMitigates, rather than solves, the issue of oversmoothing\nLimited novelty (straightforward extension of dropout to graphs edges)\nUnclear why dropping edges is \"valid\" augmentation\n\nFollowup-questions/areas for improving score:\n\nIt would be nice to have a principled way of choosing the dropout proportion; 0.8 is chosen somewhat arbitrarily by the authors (presumably because it generally performed well). There is at least a nice interpretation of choosing 0.5 for the dropout proportion in regular dropout (maximum regularization).\n\nAs brought up in the comments, edges to drop out to the graph's properties is an interesting direction to explore. While the authors state that they would like to keep the method simple and general, the method is ultimately devised as an adaptation of dropout to graphs, so exploiting graph-specific properties seems reasonable and a potential avenue to further improving performance. \n\np2: \"First, DropEdge can be considered as a data augmentation technique\" Why are these augmentations valid; why should the output of the network be invariant to these augmentations? I would like to see some justification for why the proposed random modification of the graph structure is valid; intuitively, it seems like it might make the learning problem impossible in some cases.\n\nDeeper analysis of the (more interesting, I think) layer-independent regime would be nice. (As a side-note, the name \"layer-independent\" for this regime is a bit confusing, as the edges dropped out *do* depend on the layer here, whereas in the \"layer dependent\" regime, edges dropped out do *not* depend on the layer).\n\nComments:\nFigure 1 could probably be re-organized to better highlight the comparison between GCNs with and without DropEdge; consolidating the content into 2 figures instead of 4 might be more easily parsable. Adding figure-specific captions and defining the x axis would also be nice.\n\nUse \"reduce\" in place of \"retard\"\np2 \" With contending the scalability\" improve phrasing\np2 \"By recent,\" -> \"Recently,\"\np2 \"difficulty on\" -> \"difficulty in\"\np2 \" deep networks lying\" -> \"deep networks lies\"\np3 \"which is a generation of the conclusion\" improve phrasing\np3 \" disconnected between\" -> \"disconnected from\"\np4 \"adjacent matrix\" -> \"adjacency matrix\"\np4 \"severer \" -> \"more severe\"\np5 \"but has no help\" -> \"but is no help\"\np5 \"no touch to the adjacency matrix\" -> improve phrasing\n"}