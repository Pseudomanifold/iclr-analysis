{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an approach to debiasing graph embeddings from known, given node attributes/metadata.\nSpecifically, the paper proposes to learn an embedding that is orthogonal to given node attributes, ensuring that there is no *linear* function which can extract the node attributes from the learned embedding.\n\n\nStrength:\n-\tThe paper addresses an interesting and relevant problem on debiasing graph embeddings.\n-\tThe paper presents a, to my knowledge, novel approach, to avoid the leakage of meta-data in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below)\n-\tThe paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below)\n\nWeaknesses:\n1.\tExperimental evaluation:\n1.1.\tThe paper only evaluates on the training set. Specifically, in experiment 1, the paper learns an embedding and supervises it to be orthogonal to given labels; this is good, but it also somewhat expected that this could be learned when tested on the same dataset. An important question is, if the model actually learned a generalizable embedding or just overfit to the training set. If the learned embedding is applied to new data, is it still not possible to extract political affiliation from it?\nI would suggest splitting the dataset in two part: One part to train the debiased embedding and one to train and test the Linear SVM.\n1.2.\tIt would be great if the authors describe better and quantify how they ensure in experiment 1 that the learned embedding of the MONET model is measured, and how this compares to the three baselines (e.g. a random or constant embedding would also be perfectly debiased).\n1.3.\tWhile the paper clearly states that the approach is restricted to linear relationships, it would be interesting to look at non-linear classifiers and see how well this works in practice, also in comparison to the baselines.\n1.4.\tFigure 3(c) visualizes that \n2.\tRelated work:\n2.1.\tThe comparison to related work could be improved. Specifically, a discussion relating this work to adversarial training, e.g. as in domain confusion networks [A] or in [5].\n2.2.\tThe authors argue that [5] is independent/concurrent work. I agree with the authors that [5] is sufficiently different to this work, but it should be discussed thoroughly as it has been published at ICML 2019. Unfortunately, the authors also miss to include in the references that it has been published at ICML 2019.\n\n\nWhile the paper explores an interesting direction and approach, there are several concerns which speak against acceptance (see Weaknesses above); however, I believe they can be a addressed/clarified in a further revision.\n\n\nReferences:\n[A] Tzeng et al, Adversarial Discriminative Domain Adaptation, CVPR 2017\n\n"}