{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "TLDR: split node embeddings into medatadata and graph structure, force them to be orthogonal.\n\nThe paper proposes to split node embeddings in a graph into two parts:\n1. graph structure embeddings: Es\n2. known node metadata embeddings: Em\nTo prevent Es from containing information about Em, the authors propose a scheme which puts Es into the Nullspace of Em through repeated SVD factorizations. This prevents linear classifiers that operate on Es to reliably predict information in Em.\n\nThe weakness of the paper stems from the proposed definition of debiasing. Just like two random variables can be dependent, but have a linear correlation coefficient of 0, in the proposed method the two embeddings may be linearly unrelated, but have a strong non-linear relationship.\n\nThis is an important caveat that should be highlighted in the papers' abstract, not burried deep on p4, under Theorem 2. \n\nIn fact, looking at Fig 3c information about party affiliation follows a XOR-like pattern in the PCA space. This means that a linear classifier will fail (indeed the linear SVM in Table 1 fails), but a non-linear one should work OK. Thus, contrary to the abstract, the proposed method doesn't remove the effect of arbitrary covariates, but removes a LINEAR dependence. \n\nThus the paper proposes to solve an important problem and proposes a partial solution, but overstates the results in the abstract and hides the true efficiency of the method.\n\nAction items ot correct the paper:\n- be more honest about the true result. Decorrelation does not imply independence.\n- redo Table 1 with strong non-linear classifiers such a Gaussian SVM or Random Forest to show how much is not filtered out by your linear decorrelation method\n\nFinally, contrast with the adversarial information removal [1] and  the information bottleneck [2], both of which also promise to remove non-linear dependencies. It may happen that the you method works better, even though it only guarantees no linear dependencies.\n\n[1] https://arxiv.org/abs/1505.07818\n[2] D. Moyer, S. Gao, R. Brekelmans, A. Galstyan, and G. Ver Steeg, \u201cInvariant Representations without Adversarial Training,\u201d in Advances in Neural Information Processing Systems 31, 2018\n\n"}