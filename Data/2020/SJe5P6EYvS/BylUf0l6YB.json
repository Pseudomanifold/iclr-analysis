{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes a novel LSTM architecture that adds several gating mechanism that gates the hidden state and inputs in between the LSTM update. The proposed model shows superior performance on smallish datasets including PTB,  Enwick8 and NWC.\n\nComments on the paper:\n\n1. The paper proposes an interesting architecture and it seems to show significant improvement in terms of performance for some language datasets. \n\n2. The paper is very well written, the motivation and formulation is clear. There are many analysis to understand the model (the strength and weaknesses).\n\n3.. One thing is that since this could take into account more context,  it seems that this model could potentially generate language / tokens with longer time dependencies. I wonder if the authors have performed any experiments on this and if they have seen any improvements on that front.\n\n4. Also, I am curious about the generalization ability of the model. Could the authors train the model on shorter sequences and test for generation with longer sequences and see how this compares with baseline models.\n\n5. The model seems to be related to Adaptive Computation Time (ACT) from Gaves et al. it would be nice to compare to the ACT.\n\n6. Another slight improvement in writing could be to hightlight the intuition (conclusion in page 8) at the beginning of the paper, this could help in better understanding the motivation of the paper.\n\n\nMinor comments on the paper,\n\n1. The link and the self-citations on page 4 are does not seem to be valid links and citations.\n\nOverall, a well-written paper, extensive analysis and good experimental result."}