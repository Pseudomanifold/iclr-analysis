{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper tackles the problem of context modelling within recurrent neural networks (RNNs). The authors propose an interdependent gating mechanism that enriches the coupling between inputs and hidden states. For an input x_0 and hidden state h_0; h_0 gates x_0 to create x_1; x_1 then gates h_0 to create h_1; this cyclical gating operation is applied for several rounds and it's output is fed into a recurrent neural network. For the next time-step, this process is repeated, with h_0 as the final h obtained after the final round of gating in the previous time-step. This results in the RNN processing a more contextualized version of the input tokens x.\n\nMain Contributions:\n1. A simple pre-processing step that contextualizes inputs for recurrent neural networks and significantly improves performance.\n2. An extensive evaluation of the proposed technique against previous works and on all relevant datasets.\n\nPros:\nThe paper is very well-written and clear. It motivates and explores the questions and issues surrounding this topic very well.\n\nCons:\nIt would be good to see how this performance translates to other RNN architectures such as GRUs. \n\n\nFinal notes:\nThis paper raises many interesting question:\n- What is the really going on with the gating mechanism? \nThe authors explore this question but the jury is still out on exactly what is going on here.\n- \"Mogrification\" as a general preprocessing step: could it also improve performance for transformer models?\n- Are there better ways to preprocess and gate the RNN inputs?\n\n--------\n\nReview Decision:\nIt is clear, well motivated, well written and represents a concrete contribution to the language modelling literature. Furthermore, most claims made are substantiated via thorough experimentation. Lastly, this work demonstrates that rather than relying on data and model scaling to improve performance; there is alot left to be done in tackling language modelling on smaller scale datasets. "}