{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors consider the problem of transfer learning by fine-tuning pre-trained contextualized embeddings (e.g., BERT, XLNet) for the task of question-question similarity of medical advice questions. Methodologically, they propose starting with standard pre-trained {BERT, XLNet} and double fine-turning; first with {Quora Question-Question pairs, Medical Question-Answer pairs, Medical Answer-Answer pairs, Medical Question Classification} and then with a Medical Question-Question dataset developed for this study. The basic finding is that first stage pre-training with medical QA performs better than with Quora Question-Question mappings \u2014 implying that in-domain is more important than in-task. However, this doesn\u2019t apply to the question classification task. As a byproduct, the small question-question dataset for medical advice is developed and released (and its development is described in good detail). Finally, there is some error analysis and discussion regarding generalizing to other settings.\n\nThe primary finding of this study is interesting, if not entirely surprising, in-domain related tasks result in better contextualized pre-training than out-domain data from the same task. In particular, this is unsurprising for a more technical domain such as medical. Additionally, the released dataset will be useful. However, in its present state, the scope of this study is too narrow to have impact within the broader community as it doesn\u2019t point to how to get this to work in general. Admittedly, this requires some definition of domains (probably around distributional similarity, etc.), task-similarity, amongst other properties \u2014 or at least a larger-scale empirical study along these dimensions to conceptually point to a potential theory. As the paper stands now, the most one can say is \u201cfor medical QQ similarity, first-stage fine-turning of pre-trained contextual embeddings with medical QA is better than general QQ.\u201d This might be impactful for medical QQ similarity, but isn\u2019t a significant methodological contribution, nor even really a question-answering contribution (as the title would imply), but the narrow statement said above. Thus, I recommend rejecting for ICLR.   \n\nA few less significant comments:\n\u2014 I wasn\u2019t able to match the performance numbers in the abstract to the paper. I am guessing somewhere in Figure 2?\n\u2014 One direction to go with the paper (IMO) is to follow the title and do various community QA tasks in multiple domains, thus making a contribution in this community (NLP/IR) both in terms of data and methodological interest\u2026and maybe even a new pre-trained embedding.\n\u2014 Another direction to go with this paper (IMO) is the transfer learning route. So far, theory in this space has been elusive (e.g., [Ben-David, et al., A theory of learning from different domains, Machine Learning 2010]\n\u2014 There has been a lot of transfer learning by fine-tuning contextualized embedding variants lately (admittedly, not a definitive paper yet). I would situate this work better within this space also. For example, https://ruder.io/state-of-transfer-learning-in-nlp/\n\nI hope the authors release the dataset and continue to pursue this direction by maybe first submitting to a medical NLP or question answering workshop/specific track, etc. However, I think significant additional development is needed before this work is ready for a top-tier conference.\n"}