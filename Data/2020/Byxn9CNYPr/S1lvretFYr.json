{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Authors studied the value of embeddings from medical specific text data in assessing question to question similarity. They compared different strategies in leveraging the domain specific data, namely on q-to-q, q-to-a, anwer completion and question classification sub tasks. Comparison was also done with previously published in-domain fine-tuned BERT family of models (Bio/Sci/ClinicalBERT). Sound experiments showed statistically significant improvements when medical-domain data is used for fine-tuning the embeddings.\n\nIn addition to the comparison and validation, the authors also promised to release a medium-sized new data set, which has 3000 question pairs with expert (doctors) annotation. The reviewer appreciates the value and cost in the dataset and believes it could be a good contribution to the Q&A research community.\n\nHowever the reviewer is not totally convinced about the conclusion in the paper. The experiments conducted in the paper are limited to medical domain, which is highly specialized and can't support the generalized conclusion: \"... that the semi-supervised approach of pre-training on in-domain question-answer matching (QA) is particularly useful for the difficult task of duplicate question recognition.\"  To have that conclusion, the reviewer would like to see the same comparison in other domains as well.  \n\nNote that in Section 8, authors did try to repeat the experiment to AskUbuntu data, but the result is not convincing. The authors themselves noted \"the improvement may not be statistically significant\" as well.\n\nAnother concern is that there is too little discussion and deep-diving about why only QA helped the task while other in-domain knowledge failed (as noted in Figure 2 and surrounding paragraphs). The experiments shown in Figure 2 suggests not all domain specific data can help embedding, some of them actually made it worse. The reviewer thinks that an investigation of why would add a lot of value to the research.\n\nThe suggestions are \n\n1) Make the claim that matches the evidence. We have two choices here: a) narrow the claims to medical domain, but that limits the scope of the paper too. b) more experiments to gather evidence from other domains too, that means substantial more work.\n\n2) Dig deeper into why only QA actually helped.\n"}