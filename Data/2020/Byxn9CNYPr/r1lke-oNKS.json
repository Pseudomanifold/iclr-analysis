{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors compare a number of different tasks and datasets for fine-tuning in the medical domain. The tasks/datasets are compared by applying a final fine-tuning step (on a new dataset to be released by the authors), and then reporting final performance on a test set. Their experiments show that pre-training a question similarity model on in-domain (in this case medical) information improved over out-of-domain pretraining.  \n\nThey define three pre-training tasks that they compare on different BERT models: 1. Classifying a question-answer pair as matched or not; 2. Classifying a beginning-ending pair of an answer to be a match or not; and 3. Classifying a question-category pair as a match or not. Their experiments show that only the first task helps improve their evaluation task, while the latter two actually decrease performance.\n\nFor the new dataset the authors plan to release, they asked doctors to generate 3000 question pairs by taking a patient generated question from HealthTap and 1) rewriting the question it in a different way 2) come up with a related question for which the original answer would be wrong.\n\n\nI believe that, while a better understanding of pre-training regimes is needed, the experiments in this paper do not offer enough insights to be interesting to the ICLR audience. I\u2019m leaning towards rejection.\n\n\nThe main positive finding, that training the same task with in-domain training data (on medical data-sets) is better than training on out-of-domain data (Quora questions), is not very interesting. The exploration of different tasks to adapt to a domain seems more interesting to me.\n\nHowever, while the set-up of of the different tasks and datasets is interesting, the paper does not offer a good explanation of why two of these tasks have failed. Are there maybe experiments that could be run to exclude obvious shortcomings (e.g. maybe these tasks are too easy and overfit in their training)?\n\nIt would also have been interesting to explore the relationships between the different tasks and look into different training regimes. For example, multi-task training with the different tasks might help prevent over-optimization for a single task and thus help generalize better to the downstream task.\n\nI have one concern with the new dataset used for evaluation, that I didn\u2019t see addressed in the paper. From what I understand, when creating the new dataset, the annotators were asked to create different looking questions with the same meaning, and similar looking questions that have a different meaning. I understand that these instructions were added to counter a tendency to have a correlation between lexical and semantic similarity. However, if these instructions were consistently implemented by the annotators there would be an easy way for a model to learn this relationship.  Have you looked into this possibility?\n\nLastly, while the qualitative analysis of the examples is an interesting starting point, it is hard for me to see if these really show meaningful differences between the models. It would have been more interesting to see a way of quantifying the differences between models instead of just considering a few examples.\n\nA few final remarks:\n\n  - I might have just missed this, but while you report variance for your results (which is great!) I don\u2019t remember seeing any information about how many times each experiment was repeated.\n  - There is little information conveyed in Figure 1, and it could be better used by adding to the discussions.\n"}