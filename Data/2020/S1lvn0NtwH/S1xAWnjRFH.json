{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThis paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another. Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation. Authors argue that ME bias could help the model to handle new classes and rare events better.\n\nMy comments:\n\nI very much enjoyed reading this paper. I support accepting this paper. It highlights one of the missing inductive biases in ML and proposes it as a challenge. As the authors also agree, ME bias is missing not just in DNNs. It is the issue of MLE. It would be good to have some non-NN results too. I see this is a challenge for MLE than DNNs.\n\n1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?\n2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.\n3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.\n4. Are the authors willing to release the code and data to reproduce the results?\n\nMinor comments:\n\n\n1. Page 3: second para, line 4: \u201cour aim is to study\u201d\n2. Page 5: last line: estimate for -> estimated for\n3. Section 4.2: 3rd line: \u201cthe class for the from\u201d\n"}