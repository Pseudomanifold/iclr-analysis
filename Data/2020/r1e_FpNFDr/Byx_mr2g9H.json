{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThe paper describes new norm-based generalization bounds that were specifically adapted to convolutional neural networks. Since convolutional neural networks do not explicitly depend on the input dimension, these bounds share the same property. Further additional improvement over Bartlett et al. \u201817 bound, is that this new bound depends on the sum of the operator norms of the parameter matrices, rather than the product.\n\nThe paper is clearly written and self-contained. I appreciate that the authors added a detailed comparison to Bartlett et al. \u201817 bound. However, the main result seems to be very incremental. The experiments are also very limited and not too convincing. Further empirical evaluation is needed to demonstrate progress. I would be willing to increase my score if the authors added a comparison to Wei and Ma \u201819, and more evidence was provided that the bound is tighter for typical convolutional networks found in practice (please see detailed comments below).\n\nDetailed comments:\n\nI see Wei and Ma \u201819 cited in the beginning only, but there is no further comparison. They also proved bounds with similar dependencies. How do the bounds presented in the paper compare to Wei and Ma bounds?\n\nWhat is the dependence of the constant C on \\eta in the bounds presented in Theorem 2.1? It is unclear what trade-off comes with eta and how the empirical risk term is balanced with the complexity term, since \\eta only appears next to the empirical risk term.\n\nThe authors demonstrate via a concrete example that there exists a setting (depending on epsilon), under which this new bound (up to constants) is tighter than Bartlett et al. bound. Three things remain unclear to me:\n - How do the constants differ? Is the bound presented in the paper tighter in absolute terms?\n - Is the bound tighter when the norms in the bounded are measured on typical trained neural network weights? An analysis of a few networks used in practice would make the comparison more meaningful (included the comparison to Wei and Ma).\n - Is the bound not worse than a VC bound in any (reasonable) setting? If not, is the bound tighter under typical settings when training standard vision networks?\n\nOther minor comments. In the introduction, the authors: \n - say that their bounds are size-free, which refers to the bounds not having an explicit dependence on the input size. In my opinion, this comes almost \u201cfor free\u201d when using convolutional neural network. Also, I think that size-free in the title is misleading, and should be replaced with input size-free.\n - mention that most recent bounds depend on the distance from the initialization instead of the size of the weights. This idea was first presented in Dziugate and Roy \u201817, which does not seem to be cited there.\n"}