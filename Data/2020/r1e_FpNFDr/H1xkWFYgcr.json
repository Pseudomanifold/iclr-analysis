{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers the generalization bound for deep neural networks, specifically, convolutional neural networks, which is one of the popular and crucial topics in the machine learning community, which has gathered a lot of attention.\n\nThe paper presents a generalization bound based on the number of parameters, the Lipschitz constant of the loss function and the distance of the final weights from the initialization, without dependence on the dimension of the input. The bound improves upon previous bound in some regimes when the size convolutional kernel is much less than the width of the network, which is a reasonable assumption. The paper also gives another bound which works for fully connected layers with an additional term that is linear with the depth of the network. \n\nThe paper has some nice ideas, but the contribution of the paper is not clear for me. The main theorems are based on previous results (Lemma 2.3). And the remaining work of the paper is mainly deriving the Lipchitz bound to be used in the theorem for various kinds of networks. I think this should be clearly stated in the paper.\n\nThe experiment part is not quite convincing. It is not clear from the figures that the norm decreases with the number of parameters in the network, which is claimed in the paper.\n\nThe writing of the paper also can be improved. The paper presents math, which is nice, but without much intuition explained.\n\nOverall I would not recommend this paper for admission."}