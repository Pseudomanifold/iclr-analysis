{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper extends the discriminator of GAN to use a distributional output (multiple scalars) instead of a single scalar. As a result, the trained GAN becomes robust to the mode collapse.\n\nPros:\n- The proposed method is clearly written and well-justified (e.g., Theorem 2).\n- Extension of the relativistic GAN [1] to the proposed setting is interesting.\n- The authors demonstrate that vanilla DCGAN architecture can generate high-fidelity (1024x1024) images.\n\nCons:\n\n1. An ensemble of discriminators?\n\nThe authors use multiple scalars to consider diverse factors of the realness. However, it is simply an ensemble of discriminators [2] in a spirit. As each discriminator focus on different factors, it is not surprising that the generator becomes robust to the mode collapse. Also, recent work on mode collapse (e.g., [3]) shows better results on the mixture of gaussian experiments even using a single discriminator. At least, the authors should compare their method with the ensemble methods and claim the advantage over them.\n\n2. Choice of the anchor distributions.\n\nThe choice of anchor distributions A_0 and A_1 are not specified. While the authors provide some partial results in Table 2, it would be worthwhile to clarify the experimental details and justify them.\n\n3. Role of each outcome u_i?\n\nThe authors claim that each outcome u_i corresponds to the different factors of realness. However, the role of learned u_i is not investigated. Also, one may enforce u_i to learn different factors by promoting diversity of them, e.g., decrease their cosine similarity [4].\n\nMinor comments:\n- The word \"support\" [5] is misused. The support itself means the set of non-zero elements, hence the authors should use the word \"outcome\" (or \"sample\") instead of \"support\".\n- The notation is not consistent. For example, the authors may use \"x \\sim p_data(x)\" (specify variable) or \"z \\sim p_z\" (omit variable), but not both.\n- Numbering is not consistent. For example, \"Tab.4.2.\" should be changed to \"Tab.2.\" for consistency.\n\n\n[1] Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard GAN. ICLR 2019.\n[2] Durugkar et al. Generative Multi-Adversarial Networks. ICLR 2017.\n[3] Xiao et al. BourGAN: Generative Networks with Metric Embeddings. NeurIPS 2018.\n[4] Elfeki et al. GDPP: Learning Diverse Generations Using Determinantal Point Process. ICML 2019.\n[5] https://en.wikipedia.org/wiki/Support_(mathematics)"}