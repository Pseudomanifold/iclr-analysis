{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The article presents an approach to reduce the precision of weights, activations and gradients to speed up the training of deep neural networks. The precision of these values is increased according to a dynamic schedule such that the original classification accuracy is reached after training.\n\nThe manuscript is in most parts well written and the addressed topic is of general interest for the research community represented at ICRL. Still, I recommend a weak reject, since the core idea of the manuscript, i.e. the dynamic switching between precision levels, is not shown to be a necessary condition for good classification results.\n\n\nMajor points:\n\u2022\tThe introduction does not give a clear statement about the novel contribution of the paper. Only the very last paragraph is specific about the paper.\n\u2022\tYour results support that step-wise increasing the resolution speeds up training without significant losses in accuracy. However, the impact of the gradient diversity, choice of p and threshold parameters on the performance of the trained networks are unclear. What is the isolated impact of every of these choices? According to Figure 2, pre-defined switching points between precision levels may also generalize between networks and datasets.\n\u2022\tThe description of the quantization scheme is not clear enough in order to reproduce the results:\no\tPlease give details about every step from FP32 to FPx values or cite appropriate literature.\no\tEquation 4 and 5: How are the scaling factors SC determined?\no\tPlease clarify the difference/relation between n and WL.\n\n\nMinor points:\n\u2022\tEquation 3: What does \u201crepresent. range(q^i)\u201d mean?\n\u2022\tText in Figure 1 and 2 is far too small and barely readable\n\u2022\tStep 5 in Algorithm in Section 3.3: What does \u201cp violates y more than gamma times\u201d mean? What is y?\n\u2022\tPlease clarify \u201cdistribution approach\u201d. Distribution of what?\n\u2022\tTable 1: For the baseline experiments, the precision is switched from 8 to 32 bits, for MuPPET from 8 to 12 bits (see main text). What is the motivation behind these different choices?\n\u2022\tDo you use any type of data augmentation?\n\u2022\tTable 3: Please clarify \u201ctheoretical limit\u201d. Does this limit include 12 and 14 bit quantisation. What do you mean by \u201coptimized quantization implementation\u201d in main text?"}