{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a learned image compression method that is able to be robust under a variety of tasks. The results aren't state of the art in terms of rate-distortion performance, but this paper has a very good analysis of the results, and has produced a very fast codec. In that sense, this is a very interesting paper that may lead to other fast methods (the other fast method they compared the runtime against - WaveOne never published a complete description).\n\nThis paper should be likely accepted, but the authors should town down the claims a bit. The results presented do NOT show that this method is better than the best hand engineered approach, despite what they claim. Even compared to BPG, which is NOT state of the art, the results are a mixed bag. \n\nWe would like to point out to the authors that the VVC codec has shown much stronger performance than BPG, and similarly the AV1 codec has surpassed the performance of BPG. Moreover, even Pik has also surpassed the performance of BPG, so just showing stronger performance than BPG is not grounds to make the claim that this method is superior to hand engineered approaches.\n\nMoreover, as I stated earlier, this method is not even better all the time, therefore weakening the claim.\n\nOn the positives:\n- the paper fully describes the architecture, unlike WaveOne\n- the runtime numbers are impressive (as far as I know, there is no faster published method)\n- the authors consider applications other than compression performance (such as classification performance in forensic analysis)\n\nOn the negatives, which I highly suggest that the authors fix if this paper is to be taken seriously by the community:\n- please be sure to explain that SSIM is computed in <RGB | grayscale>\n- please be more explicit about which loss is used during training for distortion (i.e., \"we use MSE for the training loss, but stop training when SSIM converges\")\n- please provide PSNR numbers for the method; and ideally MS-SSIM (in decibels) instead of PSNR\n- please add other neural compression methods to the graphs \n- please clarify that you create a file and decode a file for each image used to create the graphs (very important topic), as opposed to using the estimated file size\n- tone down the claims w.r.t. beating classical codecs"}