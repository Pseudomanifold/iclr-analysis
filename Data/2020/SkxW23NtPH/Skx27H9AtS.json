{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper the authors propose an end-to-end policy for graph placement and partitioning of computational graphs produced \"under-the-hood\" by platforms like Tensorflow. As the sizes of the neural networks increase, using distributed deep learning is becoming more and more necessary. Primitives like the one suggested by the authors are very important in many ways, including improving the ability of the NN to process more data, reduce energy consumption etc. The authors compared to prior work propose a method that can take as input more than one data flow graphs, and learns a policy for graph partitioning/placement of the operations in a set of machines that minimizes the makespan. This problem in principle is NP-hard as it entails both graph partitioning and graph scheduling as its components. The authors propose a heuristic that composes of two existing methods: graph neural networks are used to produce an embedding of the computation/data flow graph, and then a seq-2-seq placement network. The method is able to generalize to unseen instances.\n\nI vote for weak reject since some issues that I would like to see addressed by the author(s) are not. These include the fact that since the goal is to minimize the makespan,  scheduling within each machine the operations should be addressed in a better way. Also, while the objective J(\\theta) is reasonable, the distribution for the makespans could be very skewed (e.g., heavy tails over the dataflow graphs). Doesn't this affect the results? Finally, the novelty from a deep learning perspective is limited.\n\n- How do the authors address the issue of scheduling the operations within each machine? \n- How is \\mathcal{D} formally defined (i.e., the range of the mapping function)? Do you take into account the different number of machines, their memory footprints that can be significantly different, the different processing units they may have (GPUs, CPUs, TPUs)? Is the number of machines used for the partition automatically learned by the policy? That part was not very clear.\n- Since the authors compare with METIS, it is worth also comparing with Scotch https://www.labri.fr/perso/pelegrin/scotch/ that is also publicly available.\n- Can the authors comment on the scalability of their method as a function of n (number of nodes), and k (number of  devices)? "}