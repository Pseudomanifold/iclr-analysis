{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper combines predictive state representations (PSRs) with DPG and tests the overall performance on a TORC simulated task. Specifically, the authors propose to train a generalized value function (GVF) where the cumulant is how far the car is from the center of the road as well as the angle of the car. To train the GVF, the author propose to perform off-policy learning with importance resampling. To estimate the ratios, the authors propose to learn a discriminator that predicts which policy the actions came from, and show how to use this discriminator to estimate the likelihood ratio. For DPG, a small neural network is used for the Q function, and a linear policy is used. The authors demonstrate that their method outperforms DDPG-from-images on held-out TORC racing tasks, while not quite reaching the performance of DDPG-from-ground-truth state.\n\nUsing PSRs is a promising direction of work, but I found the contribution of this work rather obfuscated. It seems like there are two sources of novelty: (1) the use of a different number of continuation functions and (2) using a discriminator to estimate the importance ratio, but no details were given about these implementations. The paper would be greatly strengthened by reducing the amount of time spent summarizing prior work, and more thoroughly describing these contributions. Studying one of these contributions in more detail, rather than analyzing the performance of the final policies on a simulated task would also help make the hypothesis and insight of this paper more clear. In particular, I felt like these answers were unanswered:\n\n1. What parts of the algorithm are important to the good final performance?\n2. How important is it to use different discount factors?\n3. How does this work relate to prior work on off-policy evaluation?\n4. How important was it for a target policy to *not* be used?\n5. How was the discriminator trained (e.g. hyperparameters)?\n\nThe authors use ground-truth information when training. I am surprised that a supervised learning method trained with the ground-truth information was unable to recover the performance of DDPG-ImageLowDim. Can more details be given in this training procedure? Is it difficult to train a model to predict the current angle given the image? I find this a bit surprising.\n\nAlso, if an OU-process was used for exploration, how were the behavior policy likelihoods estimated using one-step backups? Since an OU-process isn't Markovian, it seems like this would require doing trajectory-level likelihoods, rather than on-step likelihood as suggested by Equation 3.\n\nMinor comments:\n - Why don't the other methods receive the last two actions?\n - The authors should cite [1,2] when referencing PSRs.\n - Please include legends and axis-labels to the Figures.\n - I don't understand the sentence, \"These predictions must be off-policy because if they were on-policy they would tell us no information to inform the agent how much adjustment is needed to make corrective actions to stay in the center of the lane.\" In particular, why wouldn't on-policy predictions be more useful to an agent?\n\n[1] Singh et al. Predictive state representations: A new theory for modeling dynamical systems. AUAI. 2004.\n[2] Littman et al. Predictive representations of state. NeurIPS. 2001."}