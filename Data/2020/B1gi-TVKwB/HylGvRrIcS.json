{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers the usage of learnt general value functions (GVFs) as representations for downstream control tasks in autonomous driving. The authors learn GVFs for hand-defined cumulant functions from off-policy data using TD-learning, using estimated importance weights to keep updates unbiased. A predictive representation (forecast) is chosen by the authors, which measures the \u201clane-centeredness\u201d and \u201croad angle\u201d of the car at various effective horizons. The authors perform an experimental evaluation on TORCS using images, where they demonstrate that control policies learnt using such a predictive representation generalize better than policies which learn end-to-end directly from images.\n\nThe use and discovery of predictive representations for RL, especially in the deep setting, is an interesting and important problem, relatively under-explored in the deep RL setting. The paper presents little technical novelty, however, the success of learning systems to perform control with predictive representations in this autonomous driving setting is of interest to the ICLR community. However, the paper suffers from lack of clarity and in presentation, in exposition and in the experimental section. Furthermore, I believe the experimental evaluation doesn\u2019t shed light on the performance of the predictive representation estimation, and leaves questions open as to whether the representation improves generalization or alleviates training problems. With these concerns in hand, I believe this paper holds promise, but in its current state is not yet ready for publication at ICLR.\n\nTechnical Contribution: \n\nThe use of predictive state representations / forecasts is to my knowledge a relatively under-explored setting in deep RL, so the success of such representations for autonomous driving is of interest. The primary technical contribution is techniques to use a continually learnt value function as the representation for a downstream policy. The adversarial density estimation procedure introduced in the paper is not novel, and has been studied previously extensively in the community, especially in inverse RL and exploration.\n\nPresentation / Clarity: \n\nThe presentation of the paper can be much improved, in detail and choice of language. Figures and their corresponding captions in Section 4 are difficult to parse. Most figures lack a legend, some lack axis labels, and captions are often non-informative. I found the exposition unclear / confusing at times, and would recommend the authors to revise the paper to improve readability in general, with focus on the introduction and experimental section.\n\nExperimental Evaluation: \n\nThe experimental evaluation is missing an analysis of the learnt predictive representation (GVFs). In particular, I would like to see a stronger analysis investigating whether the learnt value functions are accurate, how fast /effectively these representations adapt to changes in the policy, how accurate the estimated density function is (using the adversarial technique), and whether this importance sampling term is necessary in practice for GVF estimation (as it seems that the experiments are run in a more online fashion). An analysis of this form would lend greater support to the argument that the proposed policy evaluation scheme is effective and useful - and that the predictive representation is actually predicting the desired quantities. \n\nNo training curves are indicated anywhere, so I also have remaining questions as to whether the gain in performance presented by the authors is a function of the representation / policy generalizing better, or if the representation simplifies the RL optimization process, and thus enabling higher performance on the testing set. Furthermore, is the policy able to generalize better because of the choice of representation, or because the policy is of simplified form (linear) - would these hold for more expressive policies using this predictive representation? \n\nFinally, I would encourage the authors to include results for the comparison where the current cumulants are predicted from images as mentioned in Chen et. al 2015. \n\nThe experimental setup is well explained in the main text and in the appendix. \n\nSpecific Comments:\n\n1. Citations are incorrectly formatted throughout the paper\n2. In the first paragraph of Section 1.1, the bicycle model is never explained\n3. The paper does not define what a predictive representation is when it is first mentioned in Section 3\n4. The units for the X-axis for Figure 3 are not specified (also Figure 4, Figure 6)\n5. Equation 5 is missing a constant of 2\n6. As the authors deal with PSRs, the original paper introducing PSRs [1] should be cited.\n\n[1] Littman et al, \"Predictive Representations of State\", NeurIPS 2001"}