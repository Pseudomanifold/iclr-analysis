{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper focuses on using intrinsic motivation to improve the exploration process of reinforcement learning agents in tasks with sparse-reward and that require multi-agent to achieve. The authors proposed to encourage the agents toward the actions which changed the world in the ways that \"would not be achieved if the agents were acting alone\". The experiments are done with dual-arm manipulation.\n\nThe idea of guiding the agents toward the actions that they cannot do without concurrent cooperation is interesting. In this paper, it is presented by two types of intrinsic rewards: compositional prediction error and prediction disparity. The core component is the composition of these single-agent prediction model (f^{composed}). Although the formulation proposed is only based on intuition, the authors did enough ablation study to highlight the advantage of this loss function. \n\nAreas to improve:\n+ The objects used in the experiment are symmetric, it is good to open your study to the task in which the objects are asymmetric or even deformable.\n+ It is good to extend to the problem of multiple-agents (>2), while the order of agents who acts is important to compute \"the expected outcome with individual agents acting sequentially\" (for now, you only assume that the A will act first then the B acts later). \n\n"}