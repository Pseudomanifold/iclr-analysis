{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper introduces an interesting study that tries to explain why conditional text generation models with autoregressive decoders benefit from self-training on pseudo labels created from the same model. The paper introduces and verifies two hypotheses: 1) Decoding strategy: Since beam search is a biased estimator sampling using it doesn't reflect the learned distribution from the model and hence variations happen that benefit learning. (this partially help).\nAuthors verify that by replacing the beam search decoder by an unbiased sampling method for decoding. 2) Additional noise during training: The use of dropout adds discrepancy between training and inference which creates favourable regularization. Through smoothing of the latent spaces so similar semantic inputs get mapped to similar outputs.  Authors verify that using a toy task of summing two numbers. \n\nFollowing this intuition, authors introduce a new method of noisy self-training that even enforces hypothesis #2 (rather than increasing dropout which isn't practical to train). Authors experiment with word perturbation from (Lample et al. 18) and paraphrasing through translation back and forth.\n\nAuthors provide a thorough experimental section evaluating the noisy ST on Machine translation, low resource MT and Summarization the latter two latter are especially interesting to use ST rather than Back translation as target side documents might be hard to find (low resource MT) or quite challenging to recover the source side from (in case of summarization ).\n\nAdditional analysis experiments were performed to show the effect noisy self-training with regard to, increasing noise, number of available parallel data, size of ST samples generated from monolingual data.\n\nI am in favour of this work acceptance, overall the paper introduces very interesting insights to explain the usefulness of self-training for auto-regressive generation tasks and could inspire future work along this line in designing better ST algorithms and/or adoption of self-training in low-resourced generation tasks. \n\n\nQuestions to authors: \nAs explained in section 2, you preferred to model the ST task as an iterative process between pseudo training and fine-tuning was mainly chose for simplicity while providing equal results as shown in Table 6 (appendix). I wonder why joint training does provide lower results than separate training? I doubt this might be due to that it is given less time to converge compared to the 3 iterations of self-training. Can you provide more details about how this is done?\n\nFigure 4 (appendix), I was wondering if there's an intuition about the artefacts in the error heat maps of the toy task ( the patterns with -45 degrees slope)\n\n\nSuggestions to enhance readability:\nA lot of abbreviations finish by \"T\" this makes it quite hard to follow, I would suggest authors to remove \"PT\" and \"FT\" since they haven't been used much in the paper. \n\nLabels in Figure 1: especially early in the paper becomes hard to grasp, would be nice if you can add description to what each abbreviation means in the figure caption. \n\ntypo table 6 appendix: joint \"traing\"\n\n\n\n\n\n\n\n"}