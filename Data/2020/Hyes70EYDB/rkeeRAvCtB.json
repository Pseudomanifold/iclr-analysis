{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Interesting work and good contribution\n#Summary:\nThe paper demonstrated that by having an l1-norm based 2-class interpretability discrepancy measure, it can be shown both empirically and theoretically that it is actually difficult to hide adversarial examples. Furthermore, the authors propose an interpretability-aware robust training method and show it can be used to successfully defend adversarially attacks and can result in comparable performance compared to adversarial training.\n\n#Strength\nThe paper is well written and structured, with a clear demonstration of technical details. Compared with other works that tried to use model interpretation to help improve the model\u2019s robustness, the authors not only consider the saliency map computed for the actual target label but also the label that corresponds to the adversarial example. The proposed interpretability discrepancy measure is novel and has been proven effective to defend interpretability sneaking attacks that aiming to fool both classifiers and detectors and against interpretability-only attacks. Furthermore, extensive experiments have been done to prove the effectiveness of interpretability-aware training, which strengthens the claims of the entire paper.\n\n#Presentation\nGood coverage of the literature in both adversarial robustness and model interpretation.\nSome minor typos need to be fixed. For example, in the second last line of the caption of Figure. 2, one L(x\u2019,i) should be L(x,i) if I understand correctly. "}