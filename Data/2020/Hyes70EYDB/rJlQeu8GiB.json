{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "In summary, this paper studies if interpretation robustness (i.e., similar examples should have similar interpretation) can help enhance the robustness of the model, especially in terms of adversarial attacks. The study direction itself is interesting and very useful for the interpretation and adversarial attack community. Moreover, some promising results can be observed in part of the empirical study. However, this paper can be improved a lot as follows.\n\n1. This paper states several times that \"adversarial examples can be hidden from neural network interpretability\". It is not clear on the definition of \"hidden\" in terms of  \"interpretability\". Therefore, how this \"hidden\" is related and why this \"hidden\" is important are unclear too.\n\n2. Many details are missing, which makes the proposal suspicious. For example, the proposed method has a tradeoff parameter \\lambda. However, the settings and affects are not discussed at all. Without a clear setup, the reproducibility and applicability is in doubt.\n\n3. Some empirical results are overstated. For example, why 0.790 vs 0.890 and 0.270 vs 0.170  are comparable results? These results show the weakness of the proposed method. Further explanations can be provided. From the reported results, it could be useful to see results when the perturbation is even higher to check the limitation of the proposed method.\n\n4. Besides the clarification in the writing mentioned above, some typos or errors should be fixed, e.g., f_t'(x') - - f_t(x') >=0 in the proof of proposition 1.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}