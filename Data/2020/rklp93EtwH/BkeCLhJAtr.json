{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper mainly tackles the problem of heterogeneous tasks in meta-learning by proposing a new meta-learning framework ARML, which contains a module extracting relations across classes and a module representing meta-knowledge. When processing a new task, a graphical task representation is firstly constructed based on class prototypes, and then information propagation is conducted on a super-graph to find the most relevant meta-knowledge in the meta-knowledge graph. Ideally, the higher similarity between a prototype and a meta-knowledge node means the higher the correlation between a class and a specific type of meta-knowledge. In order to construct task-specific meta-learners, the authors utilize two auto-encoders to encode task representations with and without meta-knowledge graph. After that, a modulating function is applied to a set of shared parameters, which finishes the calculation of task-specific parameters. The authors empirically evaluated the proposed method on several datasets and it seems that ARML outperforms some compared methods.\n\nThis paper should be rejected. Firstly, the proposed method is not well motivated. It\u2019s true that tasks in meta-learning may be sampled from a complex (or multi-modal) task distribution, but why to represent a task as a graph? I think the relation between tasks can be simply obtained from instances (CNN embeddings). Secondly, it\u2019s hard to say the meta-knowledge graph can really capture knowledge with \u2018exact meanings\u2019 even though in some situations, a subset of nodes is activated and others are not.\n\nMain arguments\n1.\tThe whole framework is too complex and it\u2019s hard to say every module in the framework really works even ablation study is done.\n2.\tThe meta-knowledge graph lacks interpretability. From my perspective, it\u2019s just a set of learnable parameters without any exact meanings. Authors tried to analyze the constructed meta-knowledge graph by some experiments, but these discussions are farfetched.\n\nThings to improve the paper\n1.\tSimplify the proposed method.\n2.\tMake it clear why should we represent a task as a graph.\n3.\tSome most widely used benchmark datasets such as mini-imagenet and tiered-imagenet are not used. For a fair and convincing comparison, I suggest the authors test the proposed method on these benchmark datasets. Moreover, more methods should be compared."}