{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overall, this is a very interesting paper and, I think, would make a great addition to ICLR.  I find the ideas discussed in the paper to be stimulating and important to the community.\n\nThe authors are addressing a frequently overlooked and under-discussed aspect of RL: namely the instability of training that occurs when a Q-function becomes decreasingly familiar with areas of a state space while it focuses on a particular \"trajectory\" or solution to a task.\n\nThe authors add a measurement of uncertainty which is a simple heuristic.  It does require a couple additional hyperparameters.  One of which is not investigated in the experiments.\n\nThe experiments are ample and the approach is compared to a baseline and an alternative approach.  The experiments are informative and show the superiority of the proposed approach.\n\nComments:\n\nThe paper initially left me with the impression that the members were trained separately but Algorithm 1 looks like they are trained together.  This would have a significant impact on the number of training time steps required for training.  Please clarify.\n\nThe literature review is good but must add two relevant works.  One is MMRL by Doya et al.  I think their notion of a forward model is similar in spirit so it should be discussed.  Also, see the dissertation by DL Elliott entitled: THE WISDOM OF THE CROWD: RELIABLE DEEP REINFORCEMENT LEARNING THROUGH ENSEMBLES OF Q-FUNCTIONS.\n\nSection 3.3 didn't elucidate the topic for me.  It could be removed.  I think the description of the algorithm is sufficient.  Could be replaced by additional analysis of the uncertainty parameter from the experiments.  Also, what is the importance of the variable L in (5)?\n\nWhat does i denote in (6)?  I assume it's the index of the ensemble member.  Also, mention how the uncertainty values are initialized here.\n\nI feel that the paper is never really solidified in the mind until seeing (7).\n\nYour experiments indicate that the uncertainty value causes the \"preferred\" ensemble member to switch during the game of breakout.  That makes a lot of sense given the cyclical, for lack of a better term, nature of the game.  How about for tasks like the bipedal walker or the cart-pole task?  Would you expect, when training is completed, for a single member to dominant from the start position to the end of the trial?  If so, that's a limitation of the approach that doesn't diminish the contribution in my opinion.\n\nWould be interesting to see how the number of ensemble members used during a trial changes during training.  Does it increase/decrease?  Would love to see more analysis of this.  AGAIN, I don't think it's a negative thing if all are used but, eventually, one dominates.\n\nHow would using simpler Q-function approximators change the results?  Would it force them to decompose the task?"}