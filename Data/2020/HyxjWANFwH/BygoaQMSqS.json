{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the problem of accelerating the linear average consensus algorithm over networks and proposes a data driven approach by formulating it as deep neural network training. Specifically, the authors unfold the algorithm as a feedforward network where each layer represents a specific time and nodes are states at that time, and weights represent connecting weights (edges) in the network. Then the training was done by stochastic gradient descent. In the experimental validation, the proposed approach outperforms the baseline (Xiao & Boyd (2004) for deterministic and random synthetic networks. The proposed approach seems technically sound and interesting, but the paper may need to improve the following issues. First, it may need to show justification why the proposed approach achieved the better results and works well. Second, it needs to have more thorough experimental validation. Specifically, the baseline seems too old and not working well since the proposed approach is more than ten times better in errors, so more recent and appealing baseline (such as Ito et al. (2019)) would be required for better demonstration of the effectiveness of the proposed approach. Finally, the presentation of this paper might need more detail (see my comments below). \n\nDetailed comments:\n- It was not clear to me how training data were generated.\n- In incremental learning, what are the initial values of weights when adding another layer?\n- Why do you choose mini-bath size of 1? Have you tried larger sizes?\n- Why does only WS network have the error increase at k=5?  \n\nTypo:\npage 3: deep leaning techniques -> deep learning techniques\n\nTo AC: \nI do not know much about this area, but tried to my best to understand and assess the paper and this paper does not look  ready yet to publish in ICLR.  However, it is still possible that I did not understand the paper well.\n"}