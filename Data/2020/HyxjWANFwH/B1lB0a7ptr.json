{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents a deep-learning based variant of the linear average consensus algorithm over networks.\n\nAt a high level, starting from an initial set of states (scalars associated to each node) the goal is for all states to converge to their average via local exchange of information. Linear average consensus achieves this by (iteratively) updating each state variable by a linear combination of each node's state with those of its neighbors. The idea proposed in this paper is to learn -- in a data-driven manner -- the *weights* used in synthesizing this linear combination so as to achieve quicker convergence than those used in traditional methods (where the weights are pre-chosen and kept static throughout) for a given budget of iterations (say T). \n\nSuch a learning can be achieved by recognizing that the dynamics of the state updates can be \"unrolled\" to form a feedforward T-layer linear neural network; therefore, the weights of such a network can be learned using standard learning methods, given enough training samples of initial states (and their averages). The authors show improved performance of such methods over a baseline method proposed by Xiao and Boyd (2004).\n\nWhile the problem/research direction is interesting, I have several concerns about the setup, proposed method, empirical evaluations, and theoretical contributions:\n\n* The basic setup is not well-posed in my opinion.  Specifically, the authors never reflect upon the choice of window length (T), and in their experiments somewhat arbitrarily fix T=10 without justification. But why would one want to compute the average in exactly T rounds (no more, no less)? \n\nFor example, if the network is well connected (and the initial distribution is unimodal and peaky) then the mixing time should be very small -- so if a large and inefficient T is chosen, one would presumably converge much more slowly using this method. Likewise for the reverse case: if the network is very poorly connected (e.g. there is a bottleneck somewhere) the mixing time should be very large, and for an incorrect T, no matter how well the weights are trained one would never get good answers.\n\n* The algorithm used to train the network is puzzling. The \"unrolled\" dynamical system resembles an RNN (not surprising, since one can unroll many iterative methods into an RNN), but the authors train this incrementally layer by layer. Why not use other, more common RNN training methods (e.g., plain backprop with or without weight-sharing)? \n\n* The experimental results are in general not easy to understand. Figure 2 suggests that the learned weights jump around quite a bit from layer to layer without much structure (likewise with Figure 5). There are strange bumps in the convergence plots which suggest that the consensus error actually *increases* in some intermediate layer before decreasing. This again suggests that the method is not robust. Moreover, the test examples are synthetic, small networks and it is unclear whether the proposed method would scale at all to realistic networks.\n\n* Finally, the theoretical part is a bit confusing. I understand how the upper bound on r^T_asym is derived (basically, Holder's inequality repeated used) but don't know where the lower bound (and hence the claimed tightness) comes from. \n\nFor all these reasons, I recommend a reject."}