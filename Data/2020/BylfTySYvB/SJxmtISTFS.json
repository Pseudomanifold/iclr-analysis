{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a novel recurrent architecture called GATO. \nSpecifically, the authors focused on sequence modeling tasks and developed criteria for RNN models on such tasks. \nTha GATO model can resolve the vanishing/exploding gradient issue and is robust to initializations. \nEmpirical results show GATO can outperform LSTM and RNN in both synthetic datasets and real datasets.\n\nThe key insight of the proposed model is that only part of the hidden states is recurrently updated. \nThe GATO achieves this by adding the skip connection channel (or residual connection) along the temporal dimension.\nGATO summarizes the hidden states r_t by recurrently adding (transformed) r_t to s_t.\nThis idea also appears in many previous RNN models, such as highway RNN/LSTM, Statistical/Fourier Recurrent Units [1][2].\nSpecifically, the proposed GATO is a special case of SRU ( alpha=1). This limits the novelty of the paper and thus make the contribution marginal.\n\nAs for the experimental studies, the authors only provide comparisons with LSTM and GRU. There are a lot of advanced RNN architectures to address vanishing/exploding gradient issues, such as uRNN[3], oRNN[4], Spectral-RNN[5] and SRU/FRU [1][2]. It would be more convincing if the \nauthors could include these models into comparison.\n\nOverall I think this paper should be further improved before being accepted.  \n\n\n\n[1] Oliva, J.B., P\u00f3czos, B. and Schneider, J., The statistical recurrent unit. \nIn ICML 2017 (pp. 2671-2680).\n\n[2] Zhang, J., Lin, Y., Song, Z. and Dhillon, I., Learning Long Term Dependencies via Fourier Recurrent Units. \nIn ICML 2018 (pp. 5810-5818).\n\n[3] Arjovsky, M., Shah, A. and Bengio, Y., Unitary evolution recurrent neural networks. \nIn ICML 2016 (pp. 1120-1128).\n\n[4] Mhammedi, Z., Hellicar, A., Rahman, A. and Bailey, J., Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. \nIn ICML 2017 (pp. 2401-2409).\n\n[5] Zhang, J., Lei, Q. and Dhillon, I., Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. \nIn ICML 2018 (pp. 5801-5809)."}