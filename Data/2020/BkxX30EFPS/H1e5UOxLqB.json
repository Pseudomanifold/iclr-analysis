{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a set of losses to train auto-encoders using a stochastic latent code (PGA). The authors relate it to VAE and propose a variational variant of their framework (VPGA), but the initial intuition is distinct from VAEs.\n\nResults are presented on MNIST, CIFAR10 and CelebA and show both qualitative and quantitative improvements over VAEs.\n\nThe intuitions behind this framework seem sound and the authors added theoretical justifications when possible.\n\nThis paper seem to present a good idea that should be published, but is currently not clear enough. The writing of Section 3, especially section 3.1, need more work. The wording needs to be improved, maybe some notations are not needed, such as $\\hat{z} = h(z)$. There should be a clear description of what each loss is aiming to achieve. Currently, this is not clear. For instance, there are several mentions that h must map N(0, 1) to $\\hat{H}$, but loss (2) makes it map N(0, 1) to N(0, 1) (and I don't see other losses that would make it happen). It is likely I didn't understand it fully and a clearer section would help.\nAnother example of possible improvement would be calling the encoder and decoder with consistent names (for instance, Enc and Dec) could help, too. Currently, they are called encoder and decoder or generator in the text, f and g in the math, and theta and phi on figure 1.\n\nI am ready to change my rating after the rebuttal if the authors address clarity of section 3."}