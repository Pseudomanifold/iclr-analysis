{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, a hierarchical graph matching network, which considers both graph-graph interaction and node-graph interaction, is proposed. Specifically, the graph-graph interaction is modeled through graph level embeddings learned by GCN with pooling layers. While node-graph interaction is modeled using node embedding learned by GCN and attentive graph embedding aggregated from node embedding. \n\nSome concerns about the paper are as follows:\n\n1)\tThe novelty of the paper is incremental. The major contribution of the paper lies in the propose of multi-perspective matching function $f_m()$, which is somewhat similar to the Neural Tensor Networks proposed in [1] and utilized in [2] Although, in [2], the Neural Tensor Network is used to measure the similarity between graph-level embeddings.\n2)\tSome of the technical details of the paper is not clearly presented or well explained. \na.\tIn Eq. (7), attentive graph-level embeddings are calculated using weighted average of its node embeddings. However, it is not clear which node $i$ from the other graph should be used to calculate the weights (\\alpha_{I,j}, \\beta_{i,j}). Furthermore, it is also not clear why the attention score should solely base on a single node from the other graph rather than the information of the entire graph. \nb.\tIn Eq. (10), it would be better if the authors could provide more motivations about using Bi-LSTM aggregator. Especially, the embeddings to be aggregated are unordered. What are the two directions in Bi-LSTM in this case? What is the benefit of using Bi-LSTM as aggregator compared with LSTM aggregator or other aggregators? \n\nSome other questions to be clarified:\n1)\tWhy different similarity score functions are adopted for the classification task and the regression task? \n2)\tFor the classification task, the mean squared error loss is adopted. Why not using other more commonly used loss for classification task? \n\nSuggestions:\n\nIt would be better if the authors could empirically show the effectiveness of the Bi-LSTM aggregator.\n\nIt would be helpful if the authors could conduct some investigation on how the number of perspectives affect the performance of the model.\n\n[1] Reasoning With Neural Tensor Networks for Knowledge Base Completion\n[2] SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n\n\n\n"}