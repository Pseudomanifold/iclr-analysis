{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use the variational auto-encoder (VAE) to sample the network architectures. The VAE is applied to both one-short and gradient descent scenarios, and shows consistent improvement on different NAS tasks. The proposed method is reasonable, but I have two major concerns:\n\n- I wonder whether the VAE based approach will consistently converge to a good local minimum. It will be very helpful if the authors could provide robust analysis or at least the variations of testing errors.\n\n- I understand the motivation of VAE + one shot, but I am not very convinced by VAE + gradient-based. In the last paragraph in Section 4, the paper claims (1) VAENAS-G can increase the diversity of architectures, which can be also achieved by sampling the data set. Also, it claims (2) VAENAS-G helps to search for large models, which  I do not see experimental supports. \n\nDetailed comments:\n- Algorithm 1: it is confusing to use S_K and S_k without explanation\n- Table 1: most of the other methods provide test error with standard variations. To be fair, I'd see VAENAS's test error variation. \n- Tables 2 and 3: Why is VAENASNet (table 3) different from VAENAS-G and VAENAS-OS?\n"}