{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the training of WGANs with stochastic gradient descent. The authors show that for one-layer generator network and quadratic discriminator, if the target distribution is modeled by a teacher network same as the generator, then stochastic gradient descent-ascent can learn this target distribution in polynomial time. The authors also provide sample complexity results.\n\nThe paper is well-written and the theoretical analysis seems to be valid and complete. However, I think the WGANs studied in this paper are simplified too much that the analysis can no longer capture the true nature of WGAN training. \n\nFirst, the paper only studies linear and quadratic discriminators. This is not very consistent with the original intuition of WGAN, which is to use the worst Lipschitz continuous neural network to approximate the worst function in the set of all Lipschitz continuous functions in the definition of Wasserstein distance. When the discriminator is as simple as linear or quadratic functions, there is pretty much no \u201cWasserstein\u201d in the optimization problem.\n\nMoreover, the claim that SGD learns one-layer networks can be very misleading. In fact what is a \u201cone-layer\u201d neural network?\n- if the authors meant \u201ctwo-layer network\u201d or \u201csingle hidden layer network\u201d, then this is not true. Because as far as I can tell, the model $x = B \\phi(A z)$ is much more difficult than the model $x = \\phi(A z)$. The former is a standard single hidden layer network which is non-convex, while the latter is essentially a linear model especially when \\phi is known.\n- if the authors meant \u201ca linear model with elementwise monotonic transform\u201d, then I would like to suggest that a more appropriate name should be used to avoid unnecessary confusion.\n\nAs previously mentioned, the discriminators are too simple to approximate the Wasserstein distance, and therefore in general it should not be possible to guarantee recovery of the true data distribution. However, in this paper it is still shown that certain true distributions can be learned. This is due to the extremely simplified true model. In fact, even if the activation function $\\phi$ is unknown, it seems that one can still learn $A^* (A^*)^\\top$ well (for example, by Kendall\u2019s tau).\n"}