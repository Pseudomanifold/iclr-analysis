{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors study the generalization bound for GANs based on a new definition of generalization error where the distribution corresponding to the generator is assumed to be known for each generator (i.e., there is no empirical distribution for generators). For this generalization error, the authors give both bounds for a fixed generator and a uniform bound for a class of generators.\n\nIn my opinion, most of the theoretical results seem follow directly from standard tools in statistical learning theory and existing results on capacity bounds of neural networks. It seems that the authors do not introduce new ideas or techniques in the analysis.\n\nThe authors made comparisons with the related results in Arora et al (2017). However, since the generalization bound in Arora et al (2017) is based on a different definition of generalization error where empirical distributions are considered for both discriminators and generators, the comparison seems not fair."}