{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a new generative poisoning attack method against machine learning classifiers. The authors propose pGAN with three components to maximum the error of classification and guarantee undistinguished poisoning data for the discriminator. The experimental results show that the hyperparameter \\alpha significantly affects the poisoning data distribution and pGAN leads to specific error in a classification task.\n\nThis paper should be weekly accepted, considering the following aspects.\n\nPositive points: (1) The experiments seem solid. The overall performance with different parameters and the corresponding error type have been evaluated. (2) The error-specific and performance-control characteristics of pGAN seem to be interesting. (3) The paper is well organized.\n\nNegative points: (1) The authors should provide more justification on equation-3. Why do the authors directly average different loss for the discriminator and the classifer? (2) The function of the discriminator is not very clear, especially for the classification error test. Does the discriminator exclude the poisoning data according to certain rule? It would make more sense if the classification error measured from the data the discriminator selects. (3) pGAN can produce error-specific attack without sufficient justifications. Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency? (4) For the error-specific attack task, it would be better to provide an ablation experiment. For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \\alpha=0) or typical pGAN when they compare with the label-flip operation. Please explain which component contribute to the error-specific inclination.\n"}