{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper deals with the scalability of Binarized Neural Networks (BNNs) and their representation in Boolean logic. This encoding enables SAT solvers to reason about the underlying variables and query existential or counting clauses. The main contribution of this paper is the analysis of the architectural design choices of the BNN and modifications of the training procedure in order to improve the efficiency of SAT solvers. The modifications are simple but effective and the authors show consistent improvement on different benchmarks without the SAT solver timing out.\n\nOverall, I am leaning towards accepting this paper due to the improved empirical results compared to the baselines the authors build on. However, it does not seem that there is much novelty in the BNN architecture per se, but rather in the training procedure. The modifications are simple but effective. The paper seems to be well-written and easy to follow also from a reader not familiar with the literature in this area.\n\nI would be interested to have an answer to the following concerns:\n\n- It was not totally clear for me what is are the changes that the paper proposes in the block level and network level and if these modifications are present in their experiments.\n- Did you evaluate your method on other image classification datasets, such as CIFAR-10/-100?\n- The thresholding used to sparsify the matrix A^i and seeding the initialization (correct me if I am wrong) when retraining the BNN seems to be really similar to the Lottery Ticket Hypothesis [1]. It would be interesting to evaluate the sensitivity of your proposed method towards the initialization.\n\n[1] Jonathan Frankle, Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. In ICLR, 2019"}