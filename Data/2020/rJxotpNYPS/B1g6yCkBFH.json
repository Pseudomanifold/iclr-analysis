{"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduces a VAE that can be used in problems in which domain information is available at training time to increase the classification performances on unseen domains. The model can also be used in a semi-supervised setting if unlabelled data is available (even from unseen domains).\n\nAs the authors state, this is a quite common scenario in many interesting applications such as medical imaging. As such, I found the paper interesting to read (the paper is also written quite well).\n\nTo allow the classification of data from any domain, in the inference network the labels d are not used to infer the latent states, but only as an auxiliary loss that forces z_d to capture domain-specific information. However, this makes me wonder if z_d is needed at all? I wouldn't be surprised if the model performed equally well if domain information d was not passed to the model even during training, in which case z_x would also capture domain-specific information. To understand if this is the case, it would be very helpful to add a baseline model in which you use a version of DIVA in which z_d and d are not present both in the generative model and in the inference network.\n\nSince from the technical point of view the novelty of the model is limited, I would have liked to see a stronger experimental section to show the real-life applicability of the model:\n- The MNIST experiments are visually helpful to understand the disentanglement in the model. However you solve a quite simple task. What are the performances of a baseline classifier on this?\n- The malaria cell experiment is definitely more realistic and therefore interesting, and the results are quite convincing (despite being on quite low-resolution images). Since I consider the MNIST experiment a \"toy task\", I think the paper would greatly improve if a new real-world experiment was performed (e.g. other medical imaging datasets).\n\nOverall I liked the paper and I think it is relevant for the ICLR community, therefore I am voting towards acceptance. However, a more convincing experimental section is needed for me to increase the score.  \n\n\nSmall comments:\n- you should make clearer from the abstract that the goal of this paper is to build a domain-invariant classifier (as opposed to solving more VAE-like tasks)\n- in the beginning of section 2.1 you write twice p(x|z_d, z_x, z_d), without z_y\n"}