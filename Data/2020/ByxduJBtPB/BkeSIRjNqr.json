{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper describes situations whereby data augmentation (particularly drawn from a true distribution) can lead to increased generalization error even when the model being optimized is appropriately formulated.  The authors describe a somewhat intuitive example of how this can happen, and proceed to support this example and generalize with mathematical rigor.\n\nThe machinery of the analysis and the positive suggestion of X-regularization strike me as technically sophisticated and effective.\n\nThis is largely a theoretical paper.  The central example and description assume the availability of a true distribution to draw an arbitrary amount of examples from.  While CIFAR-10 is used to demonstrate the effectiveness of X-regularization, it is not compared to any other augmenatation / semi-supervised / regularization approach.  This makes contextualizing the contribution of this work more challenging.  General data augmentation techniques frequently do not draw examples form a true distribution (here the test distribution or a held-out development set) but rather manipulate real examples through some transformation function (like noise addition, rotation, scaling, warping, etc.).  This work does not address these conditions.  Rather the augmentation investigated here is more like self-training, where information from unlabeled examples are used in the training (i.e. regularization) of a model.  If i understand the technique correctly, assuming the eigenvector decomposition is relatively consistent under different samples of augmentation data, the regularization should be consistent whether regularized using the test set or some other unlabeled set.  While I think this is theoretically true, it would be useful to see this reflected on the CIFAR data, where the test data is unused during training at all.  \n\nThe contextualization of this work to related work -- particularly self-training and semi-supervised training -- is quite thin.  While acknowledged, the technical rigor that is brought to the main result is not used to contrast with other techniques.  Neither are there empirical comparisons to existing techniques in this space.\n\nOn balance, I find the theoretical investigation and explanation for \"how does this happen\" to be more compelling than the presentation of X-regularization.\n\nNote on \"Experience Assessment\": While I've done research on data augmentation and published in this space, the formulation and analytical tools employed in this paper I am less familiar with.   "}