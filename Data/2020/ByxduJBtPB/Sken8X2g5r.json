{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper provides some theory into the question of whether data augmentation can hurt test-set performance. To paraphrase the theory, data augmentation can hurt when it causes the model to learn a spurious local details instead of global structure, even if the augmented data comes from the same (predictive) distribution, and even if the true model lies in the hypothesis class of the learned model. Ironically these effects may be diminished in the large-sample regime, where data augmentation is less important in practice. Motivated by these issues, the authors propose \"X-regularization\" which requires that models trained on standard and augmented data produce similar predictions on unlabeled data. The paper includes a few experiments on a toy staircase regression problem as well as some ResNet experiments on CIFAR-10.\n\nReview: Overall I think this paper provides an interesting perspective on when and why augmented data can increase test error. Most of the theory deals with a simple convex problem, but some work is done to measure whether the insights carry over to more realistic settings. I think the main ways the paper could be improved are a) better explanation of connections between the theoretical assumptions and practice and b) better treatment of \"X-regularization\". For a) in particular, more discussion of which exiting data augmentation schemes fall under the theoretical framework would be useful. For b), X-regularization seems tacked on to the end of the paper, despite the potential strength of the contribution. It is not compared directly to existing semi-supervised or robust learning methods. As it currently stands I think the paper spends a bit too much time in the main text on the toy problem and theory, and could be bolstered by additional discussion of x-regularization. But, I think it's acceptable in its current form.\n\nSpecific comments:\n- In Figure 1 the green crosses use such wide lines that they look like dots at first glance. When the text referred to \"crosses\" I was confused. I'd suggest making them more obviously crosses.\n- \"the distribution of Px of the augmented dataset is potentially different than Stairs on the original dataset\" Up until this point, your setting was not specific to the \"Stairs\" problem so I'm not sure why you're mentioning it here. This sentence would make more sense to me if you removed the words \"Stairs on the\".\n- I think it would be useful if you provided some examples of data augmentation schemes (that are used in practice) that satisfy and do not satisfy the assumptions of your theoretical analysis.\n- I would also suggest that you provide some additional connections or discussion of the relation between the behavior of your setting (section 2) to the behavior of models we actually use in practice (neural nets). How much of the behavior do we expect to carry over? Why?\n- \"AT augments with imperceptible perturbations of training images with the corresponding correct target and thereby falls in our framework of covariate-shifted data augmentation.\" You should mention the assumptions implicit in this definition, namely that perturbing an image with a perturbation whose l_inf norm is less than some threshold will not change the target class. This is what connects it to your theoretical setting and so is an important detail.\n- Ma et al. (2018); Belkin et al. (2018) need \\citep\n- \"The assumption that we have \u03a3\" I think you are missing an \"on\" before \"\u03a3\".\n- IIUC General X-regularization essentially enforces that the distance between a model trained on augmented + non-augmented data produces the same predictions as a model trained on only the non-augmented data when both models are fed additional unlabeled data. If this is the case I would suggest clarifying this a bit. This bears some similarity to consistency regularization/the \"Pi-Model\" (Laine & Aila 2017, Sajjadi et al. 2016), except in those cases consistency is enforced between a model fed with augmented and non-augmented versions of the same image. In your case IIUC you train two separate models and use unlabeled data to compare them; in the consistency regularization a single model is trained on labeled and unlabeled data together.\n- \"Hence empirically, our X-regularization has lower standard error but higher robust error than RSL.\" I don't think you have empirical evidence that supports this claim."}