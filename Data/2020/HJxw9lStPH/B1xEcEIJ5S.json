{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors investigate how to use GAN for estimating the underlying density function of images. Generator network of GAN and its derivative are used for obtaining the density of data, and the authors analyze which is the point with high probability density and which is not.\n\nThe derivation of the probability density using Generator gradient seems to be okay, but this work highly depends on the belief that GAN produces the images from the same density function as the true one. Based upon the results, the authors claim that the result is uninterpretable (ex. the high probability of \u201c1\u201d in MNIST). It looks for me that simply the generator is not the one that makes data same as those from the true density function. Or even if the generator is okay, the gradient of the generator may not provide the gradient of true generator. Either of these can ruin the experimental results, but it was not confirmed that any of these malfunctions did not happen. Rather, the weird results are simply assumed to be correct.\n\nThe authors can provide results using synthetic data first from known distributions and confirm that their algorithm works as expected. The result that \u201c1\u201d in MNIST has higher density than others could be correct, but it should be confirmed as well. For example, the variations of \u201c1\u201d could be less than others. Maybe the authors have derived the useful equation, but unfortunately, the paper did not reach the threshold because of the presentation of results.\n"}