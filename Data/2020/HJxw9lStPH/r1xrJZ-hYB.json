{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper attempts to estimate densities corresponding to GAN models, and then study those density estimates to understand properties of natural image distributions. Understanding high-dimensional image distributions, and also understanding the distributions that GANs learn, is clearly an important topic worthy of study.\n\nUnfortunately, the actual quantity that the paper estimates is unclear to me, but it is certainly *not* the probability density.\n\nFirst of all, it was unambiguously proved by Arjovsky and Bottou (ICLR 2017, \"Towards Principled Methods for Training Generative Adversarial Networks\") that typical GANs, including presumably the ones studied here, *do not admit a probability density at all* with respect to Lebesgue measure. This is not merely a technicality: you're trying to estimate something which *does not exist*.\n\nBut, okay, per Arjovsky and Bottou the GAN distribution is supported on a (probably finite) union of manifolds (of possibly varying dimension). Then there exists a Radon-Nikodym derivative of the distribution with respect to some measure on its support. One has to be careful in picking the base measure for a union of manifolds of different dimensions, but suppose we make some reasonable choice, and call that the probability \"density.\"\n\nThe quantity P_d(x) given by (2) is still *not that density,* because G is *not* injective for typical GAN generators. Empirically, if you try to numerically invert x = G(z), you can find distinct z yielding essentially the same output image (e.g. https://arxiv.org/abs/1802.05701).\n\nBut, okay, pretend that P_d is the true density on the output \"manifold\" (which may not actually be a single manifold, but ignore that). As you note, points off the manifold should have density zero. Now, you don't say exactly how the regressor is trained, but presumably it's something like mean-squared error of log-\"density\" values. (Incidentally, this is something that should be in an in-PDF appendix as well as in the git repository you decided not to anonymize for submission.) Because you never give the regressor any examples with density zero, its behavior in extrapolating away from the output \"manifold\" is going to be essentially *arbitrary* based on the regressor's implicit priors.\n\nThis makes experiments like Figure 4 (right) and Figure 5 (left) *extremely* difficult to interpret. They're based on the arbitrary extrapolation behavior of a network whose training procedure we don't even know; even if we did, they presumably have very little connection to P_d, which isn't very connected to the true density on the output \"manifold,\" which isn't even *defined* for these out-of-manifold images.\n\nThe proposal of section 6 to instead estimate densities based on densities in Z is also problematic. It's easiest to see this if, instead of taking the latents to be distributed as Gaussians, you take the similarly or perhaps even more popular choice of taking the latent distribution to be uniform over a hypercube. Then literally every point would have the same \"density\" under your model. Images which the GAN cannot produce and hence should have a density of 0 will still probably be assigned that same density value by the density regressor. So your procedure would do...literally nothing. Doing it on a Gaussian prior means the outcomes are not uniform like this, but there is no reason to think they are at all meaningful: the same factors which alter the uniform latent density to the non-uniform output \"density\" will alter the Gaussian latent density in the same way.\n\n----------\n\nIgnoring all of this, the paper has I think two main takeaways:\n\n1. Likelihoods of high-dimensional distributions are unintuitive to interpret; \"simple\" images will probably show up as far more likely than \"complex\" ones, even if the \"complex\" ones are far more \"typical.\"\n\n2. Relatedly, generative models on images can assign very high likelihoods to out-of-sample images.\n\nBoth points are I think probably true, even if I don't at all trust this paper's demonstration of them, and very important to bear in mind when thinking about likelihoods for image-type distributions.\n\nPoint 1 is I think reasonable to expect by analogy to the high-dimensional Gaussian you brought up. I'm not aware of previous work explicitly highlighting it, but even if you had convincingly demonstrated it, I wouldn't have been super surprised.\n\nFor point 2, I again don't really trust your demonstration. I do trust it from Nalisnick et al. (2018)'s evaluation on models that actually have densities that can be straightforwardly-evaluated. But although you cite that paper as an arXiv submission, it was in fact published at ICLR 2019, which makes it a hard sell to call \"parallel to our work\" in an ICLR 2020 submission."}