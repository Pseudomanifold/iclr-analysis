{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary\n---\n\n(motivation)\nLots of methods produce attribution maps (heat maps, saliency maps, visual explantions) that aim to highlight input regions with respect to a given CNN.\nThese methods produce scores that highlight regions that are in a vague sense \"important.\"\nWhile that's useful (relative importance is interesting), the scores don't mean anything by themselves.\nThis paper introduces another new attribution method that measures the amount of information (in bits!) each input region contains, calibrating this score by providing a reference point at 0 bits.\nNon-highlighted regions contribute 0 bits of information to the task, so they are clearly irrelevant in the common sense that they have 0 mutual information with the correct output.\n\n(approach - attribution methods)\nAn information bottleneck is introduced by replacing a layer's (e.g., conv2) output X with a noisy version Z of that output.\nIn particular, Z is a convex combination of the feature map (e.g., conv2) with Gaussian noise with the same mean and variance as that feature map.\nThe weights of the combination are found so they minimize the information shared between the input and Z and maxmimize information shared between Z and the task output Y.\nThese weights are either optimized on\n1) a per-image basis (Per-Sample) or\n2) predicted by a model trained on the entire dataset (Readout).\n\n(approach - evaluation)\nThe paper uses 3 metrics with differing degrees of novelty:\n1) The bbox metric rewards attribution methods that put a lot of mass in ground truth bounding boxes.\n2) The original Sensitivity-n metric from (Ancona et al. 2017) is reported with a version that uses 8x8 occlusions.\n3) Least relevant image degredation is compared to most relevant image degredation (e.g., from (Ancona et al. 2017)) to form a new occlusion style metric.\n\n(experiments)\nExperiments consider many of the most popular baselines, including Occlusion, Gradients, SmoothGrad, Integrated Gradients, GuidedBP, LRP, Grad-CAM, and Pattern Attribution. They show:\n1) Qualitatively, the visualizations highlight only regions that seem relevant.\n2) Both Per-Sample and Readout approaches put higher confidence into ground truth bounding boxes than all other baselines.\n3) Both Per-Sample and Readout approaches outperform all baselines almost all the time according to the new image degredation metric.\n\n\nStrengths\n---\n\nThe idea makes a lot of sense. I think heat maps are often thought of in terms of the colloquial sense of information, so it makes sense to formalize that intuition.\n\nThe related work section is very well done. The first paragraph is particularly good because it gives not just a fairly comprehensive view of attribution methods, but also because it efficiently describes how they all work.\n\nThe results show that proposed approaches clearly outperform many strong baselines across different metrics most of the time.\n\n\nWeaknesses\n---\n\n\n* I'm not sure why the new degredation metric is a useful addition. What does it add that MoRF and LeRF don't capture on their own independently?\n\n* I think [1] would be a nice addition to the evaluation section as it tests for something qualitatively different than the various metrics from section 4. It would also be a good addition to the related work.\n\n\nMissing Details / Points of Confusion\n---\n\n* I think there's an extra p(x) in eq. 11 in appendix D.\n\n* I think the variable X is overloaded. In eq. 1 it refers to the input (e.g., the pixels of an image) while in eq. 2 it refers to an intermediate feature map (e.g., conv2) even though it later seems to refer to the input again (e.g., eq. 3). Different notation should be used for intermediate feature maps and inputs.\n\n\nPresentation Weaknesses\n---\n\n* In section 3.1 is lambda meant to be constrained in the range [0, 1]? This is only mentioned later (section 3.2) and should probably be mentioned when lambda is introduced.\n\n* \"indicating that all negative evidence was removed.\" I think this should read \"indicating that only negative evidence was removed.\"\n\n\nSuggestions\n---\n\n\"The bottleneck is inserted into an early layer to ensure that the information in the network is still local\"\nI'd like this to be explored a bit more. Though deeper feature maps are certainly more spatially coarse they still might be somewhat \"local\". To what degree to they loose localization information? My equally vague alternative intuition goes a bit differently: The amount of relevant information flowing through any spatial location seems like it shouldn't change that much, only the way its represented should change. If the proposed visualizations were the same for every choice of layer then it would confirm this intuition. That would also be an interesting result because most if not all of the cited baseline approaches (where applicable) produce qualitatively different attributions at different layers (e.g., see Grad-CAM).\n\n\n[1]: Adebayo, Julius et al. \u201cSanity Checks for Saliency Maps.\u201d NeurIPS (2018).\n\n\nPreliminary Evaluation\n---\n\nClarity: The paper is clearly written.\nOriginality: The idea of using the formal notion of information in attribution maps is novel, as is the bbox metric.\nSignificance: This method could be quite significant. I can see it becoming an important method to compare to.\nQuality: The idea is sound and the evaluation is strong.\n\nThis is a very nice paper in all the ways listed above and it should be accepted!\n\n"}