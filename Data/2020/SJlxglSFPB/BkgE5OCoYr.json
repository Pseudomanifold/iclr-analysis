{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper evaluates a variety of existing pixel-wise out-of-distribution detection methods in the task of semantic segmentation of road scenes. To do so, the paper introduces an evaluation protocol and applies it to two datasets (SUN and IDD) and two models (PSPNet and DeepLabV3+).\n\nStrengths:\n- The paper is well written with high quality visuals and plots\n- The paper studies an important problem\n\nWeaknesses:\n- The contribution seems to be rather incremental (evaluating existing methods on 2 dataset) and some related work might be missing\n- Although the analysis is well executed, it is not clear what the community learns from the paper\n\n\nAlthough I enjoyed reading the paper, I'd lean towards rejection of the paper. My main concern are as follows:\n\nIt is not clear what the community learns after reading this paper. Is the difference between different approaches significant? Are the class boundary pixels the biggest problem? The paper would benefit strongly from providing some indications of future steps, e. g. how to improve OOD in semantic segmentation.\n\nI'm missing a discussion between OOD and  and the prior work on uncertainty estimation in semantic segmentation (e. g. https://arxiv.org/pdf/1703.04977.pdf and the follow up works, or https://arxiv.org/pdf/1807.00502.pdf). It seems that uncertainty estimates for semantic segmentation could be applied to the tested scenarios off-the-shelf without the need for additional modifications. Is there any reason the paper did not include approaches for uncertainty estimation in semantic segmentation? In general, it would be useful to connect the tested OOD scenarios to other topics already studied in semantic segmentation such as: uncertainty estimation, outlier detection, and distribution shift in semantic segmentation. A paragraph drawing connections and highlighting the differences would make the paper stronger.\n\nOther comments:\n\nSection 3.2, \"Therefore only the car class as ID...\" I'm not sure I understand why car class is the only one considered ID for IDD. From Fig. 2, it seems that other classes such as bus, traffic light, pole, terrain, etc. could be also considered. Could the authors comment on this?\n\n\"The random normal noise is usually very easily detected by all methods, therefore Perlin noise images are used\". However, when looking at the results Fig 3 and Fig 4 it feels like Normal random noise is harder than Perlin noise. Could the authors comment on this?\n\n\"All OOD datasets used are mixed with Cityscapes evaluation sets\". Why it is important to add Cityscapes images to evaluation set? Wouldn't it be enough to use OOD datasets?\n\nOne suggestion of a plot that could jointly display the information from RQ1 and RQ2 would be to plot both of them in a one scatter plot (e. g. with ID IoU on x-axis and AUROC on the other).\n\nFigures 3 and 4 show results for 6 methods, while Table 2 only displays 3 scenarios for 2 models. Table 3 would benefit from including all 6 models and using the same labels as in Figures. Moreover, it would be interesting expand Table 2 by including the performance of the segmentation on in distribution classes from IID and SUN datasets in addition to Cityscapes results. "}