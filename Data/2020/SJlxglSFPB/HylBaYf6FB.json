{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "- This paper proposes a comparative study of out-of-distribution (OOD) methods for semantic segmentation. To this end, authors extend networks designed for OOD image detection to accommodate for the segmentation task. For evaluation purposes authors create a new dataset and use 3 well-known metrics, as well as a new proposed metric. \n- The paper is in general a bit dense to read and understand, since authors fail to explain many important details. Furthermore, the structure of the paper can be improved (e.g., related work is added at the end of the manuscript). \n- Technical contribution of this work is insufficient. Authors merely employ ODD classification networks to address the pixel-level OOD task. Nevertheless, the motivation of those changes are never detailed. Furthermore, some other important aspects are not explained. For example, typically classification networks are adapted for segmentation adding a decoding path, so that the output result is a map of the same size of the input times number of classes. However, how the probability map for the pixel-level OOD predictions is never explained. \n- Authors also mention that GANs and AEs are excluded to limit the scope of the paper. First, this reason is not convincing. Second, I believe that including all the significant works for the task-at-hand is more relevant. How these methods would work compared to the proposed networks?\n- Unless I miss something, the adapted OOD versions degrade the semantic segmentation performance of original networks. Why not to use the original versions instead?\n- Results are very unclear. Which dataset represents the OOD evaluation? Further, authors talk about results at image-level and pixel-level. Nevertheless, this is not detailed in the experimental section. Fig. 3 reports results of the different models at pixel-level. Where are the results of OOD at image-level? In addition, results are barely interpreted, and authors basically describe the values reported in the figures. I would appreciate a deeper interpretation of the results.\n- I am curious to know why the confidence OOD approach has much better performance with DeeplabV3+ than with PSPNet. While PSPNet is among the worst performing models with confidence (sometimes the worst), Deeplab + confidence is typically top-ranked. Do the authors have any insight on this?\n- There is no evidence that the proposed metric better models the OOD pixel-level performance than other standard metrics. Which are the results on this task achieved by mIOU instead? "}