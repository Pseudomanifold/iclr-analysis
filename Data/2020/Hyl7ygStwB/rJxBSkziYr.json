{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper explores the use of BERT to improve Neural Machine Translation (NMT) both in supervised, semi-supervised and unsupervised settings. The authors first show that using BERT to initialize the encoder and/or the decoder does not bring any clear improvement, while using it as a feature extractor performs better. Based on this finding, the authors propose a new approach to integrate BERT in NMT, named BERT-fused NMT, which incorporates BERT representations from the input sequence into the encoder and decoder attention mechanisms.\n\nI am ambivalent about this paper. On the one hand, the paper presents a thorough experimental evaluation, with strong baselines (often outperforming their original implementation) and results that can be interesting from different angles, and the reported improvements are consistent. However, the paper is rather poorly written and some important details are not adequately described, which left me with some concerns and an overall negative impression as I read through the paper. More concretely:\n\n- The paper is rather poorly written. There are many expressions that sound ungrammatical or otherwise unnatural to me (although I am not a native speaker myself) and, more importantly, the overall exposition of ideas is not sufficiently clear. I found the paper difficult to follow, and I was left with many doubts as I read through it. In addition, the style in which some results are presented is inappropriate for an academic paper (e.g. \"Obviously, our proposed BERT-fused NMT can improve the BLEU scores\"), although I understand that this was probably not intentional.\n\n- To make things worse, the paper is 10 pages long, and according to the CFP reviewers are \"instructed to apply a higher standard to papers in excess of 8 pages\". I think that the paper could be fit in the regular 8 page limit.\n\n- The pre-trained BERT models that the authors use were trained on different (and generally larger) training data than what they use for the NMT training (e.g. they all use Wikipedia). As such, the models that build on BERT are indirectly using this additional training data. How can we make sure that the reported improvements are not due to this additional data? What would happen if the same data was used for the baseline systems (e.g. through back-translation)? Also, please clearly state which pre-trained model you use for each specific experiment.\n\n- The treatment of subword tokenization is not given sufficient attention and raises some concerns to me. It seems clear that the authors combine different subword tokenizations for their proposed system (i.e. BERT and the NMT encoder/decoders use a different subword vocabulary). However, it is not clear to me how this is handled in the baseline systems that use BERT for initialization only, for which a mismatch in tokenization would be problematic.\n\n- I often find it difficult to understand what the authors did exactly for each of the reported systems. For instance, what is the difference between \"Standard transformer\" and \"Training NMT module from scratch\" in Table 6? I cannot see any yet the difference in BLEU is 1.5."}