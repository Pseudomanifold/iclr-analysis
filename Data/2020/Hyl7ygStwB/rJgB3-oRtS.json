{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\n\nThis paper discusses a method that effectively incorporates a (large) pre-trained LM, such as BERT and XMN, for improving the performance of NMT.\n \nThe motivation of this paper is rather straightforward and not novel; many researchers can quickly think of such an idea of incorporating the power of the recent (rapid) development of pre-training LMs into NMT.\nFrom this perspective, this paper is not very exciting. \nHowever, as described in the paper, we often fail to improve (or even degrade) the performance of NMT when we straightforwardly incorporate a pre-trained LM.\nThus, many researchers/developers might want to know a practical approach to integrate a pre-trained LM into NMT.\nThis paper provides a straightforward but smart way to incorporate pre-trained LMs, which is not trivial in the community.\nIn this sense, this paper might have a considerable influence on the community.\nI was a bit surprised by the apparent effectiveness of the proposed method since I also have attempted to apply pre-trained LMs to NMT and have not obtained a good result.\n \n \nExperimental results are mostly convincing; the authors conducted comprehensive and extensive experiments on many settings, such as supervised NMT with low- and hi-resource settings, a semi-supervised NMT setting by back-translation, document-level MT, and unsupervised NMT.\nThe results were also promising; the proposed method consistently outperformed conventional methods. \nI think these results are useful for many readers.\nMoreover, such findings also offer further insights for many researchers who aim to apply BERT to many other tasks, especially for text generation tasks.\n \n\nHere are my concerns about this paper.\n\n1, unclear explanations\nThe writing can be much improved. Readers might be able to guess, but several descriptions are hard to follow, or detailed explanations are missing.\nFor example, what is the exact operation of \"function cascade\"?  \nWhat is the difference between the \"Training NMT module from scratch\" and \"Standard Transformer\" in Table 6?  What is the main reason for the lower performance of (Miculicich et al. (2018)) than that of sentence-level NMT in Table 4?\n\n2, better comparisons\nI think the authors need to confirm another model setting for a fairer comparison, something like \"The proposed architecture with (fixed) random vectors instead of the BERT's contextualized embeddings.\nIt is because we sometimes observe the improved performance for the above model comparing with the original one.\nWe can interpret this improvement by the effect of increasing the weight parameters for injecting the additional random vectors to the original architecture.\nTherefore, I think the above model settings can improve the performance of standard Transformers, which can be a preferable counterpart of the proposed method.\nMoreover, the proposed method is closely related to the model ensembling since the method utilizes two separate models.\nTherefore, the authors should also report the results of model ensembling for better comparisons.\n \n3, less discussion for the experimental results\nI found minimal discussions about the results.\nFor example, in the ablation study, the authors only show (list) the observations of their results and no discussions.\nThe authors should provide discussions about how and why their method (architecture) can improve the performance compared with a similar (and current de facto standard) approach, like the fine-tuning setting that can often improve most of the other NLP tasks.\n\n"}