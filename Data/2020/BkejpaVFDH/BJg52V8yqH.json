{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose an operation that combines convolution and self-attention, which they call the Affine Self Convolution (ASC). This operation is used to replace the spatial convolutions in CNN architectures. The authors show that the proposed operation is translation equivariant and further introduce its roto-translational equivariant version. They additionally develop a group equivariant version of Squeeze-and-Excite. They conduct experiments on CIFAR10/CIFAR100 image classification and show some improvements compared to fully convolutional and fully self-attentional models (higher accuracy at a similar model size)\n\nI take several issues with the current draft.\n\n1) How is ASC different than MHA(Conv1x1(Q), Conv3x3(K), Conv3x3(V)) (where MHA is the multi-head attention function)? \nIt seems that the proposed operation is simply a convolution followed by self-attention (without batch norm or non linearity in between), since the affine map is simply a convolution with biases. This is obfuscated in the text.\n\n2) The experimental section is quite weak: \n- The authors do not compare against [1] which shares motivation with their work.\n- There is no mention of FLOPS/running times so it is hard to know the significance of the improvements, especially given the equivalence of ASC to MHA(Conv1x1(Q), Conv3x3(K), Conv3x3(V).\n- The authors only use the CIFAR dataset, which usually favors high regularization. This makes it hard to evaluate the significance of the roto-translational results.\n- The used architectures are far from SOTA.\n\n3) There are several issues with the claims made by the paper about related work:\n- \"We focus on self-attention in vision models, and we combine it with convolution, which as far as we know, are the first to do [...] While this is applied differently at each spatial location, we show that it is translation equivariant\". This is quite literally the focus of [1].\n- \"While there is work towards using attention in CNNs, the current models use them independently, sequentially, or in parallel.\" Given that ASC consists in applying convolutions and self-attention sequentially, I don't think this claim is valid.\n- \"Ramachandran et al. (2019), \u03b1(x, y) = softmax_y(score(f(x), f(y)+\u03b2(y\u2212x))). It can also be added to the neighbors score, after computing the score function, as done in Bello et al. (2019); Hu et al. (2019), \u03b1(x, y) = softmax_y(score(f(x), f(y)) + \u03b2(y \u2212 x))\". Actually, Bello et al. introduced the former formulation which was subsequently used in Ramachandran et al.\n- \"In parallel, Ramachandran et al. (2019) also show improvements using local self-attention with local relative positional embeddings over Bello et al. (2019) which use global self-attention with global positional embeddings\". What improvements specifically? Bello et al introduce relative self-attention over images with content-to-position interactions and show that this can work in combination with convolutions or by itself. Ramachandran et al uses similar content-to-position interactions with local self-attention and focus on fully attentional models.\n\nIn summary, the proposed method is of limited novelty, the writing obfuscates the nature of the method, the experimental section is unconvincing and some related work is characterized wrongly.\n\nMisc\n- \"Regardless, they are faster than the attention based models.\" What does 'they' refer to?\n\n[1]: Attention Augmented Convolutional Networks, Bello et al. ICCV2019\n[2]: Stand-Alone Self-Attention in vision models. Ramachandran et al NeurIPS 2019"}