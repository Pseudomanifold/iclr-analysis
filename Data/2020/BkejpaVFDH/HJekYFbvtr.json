{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThe paper introduces a novel layer type that fuses self-attention with convolutions. The mechanism (per pixel x and neighbouring pixels y in neighbourhood k) can be summarized as: 1) apply a different linear projection for each pixel y around x (there is an analogy to convolution) to get relative feature vectors for y wrt x; 2) add positional embeddings for each of the k elements in the kernel window (this is just a bias term for projection of 1), 3) apply local attention in kernel window around x. The only difference to pure strided, local self-attention is the fact that (K, V) feature vectors for all y are computed by a specific linear projection relative to x. This almost exact idea has been explored and is known as relative self-attention, see for instance [5]. The difference is that also V is subject to a relative linear projection. Local self-attention has also been introduced in various papers, e.g., [6]. The mathematical derivation of the idea is interesting, but at the same time suggests that the authors are doing something completely novel. I also feel that the complexity of their derivation is not warranted by the complexity of the actual mechanism. This might be misleading and hence makes it harder to establish the mentioned connections to prior work (e.g., relative self-attention). Finally, the presented results are not convincing at all. The approach is only tested on cifar and compared to a fairly poor baseline. There are no comparisons to empirical results from the literature, either. The proposed approach performs only marginally better at best.\n\nGiven that my assessment of the methodology is correct (please correct me if I am wrong) leading to a lack of novelty and the lack of strong empirical results, I vote to reject this paper.\n\n\nMajor issues:\n- Rephrase the claim to be the first to combine convolutions and self-attention as there have been multiple works on this subject before, e.g. [1], [2], [3], just to name a few. Maybe the first to fuse ideas of convolutions and self-attention into a single layer.\n- The derivation of their mechanism is introduced in a much more complicated way than necessary.\n- The authors basically introduce local, relative sel-attention \n- There is a complete lack of acknowledging prior works, e.g. [1-6]\n- Only marginal improvements over a single, not very storng baseline.\n- No empirical comparison to literature.\n\n\n[1] Attention Augmented Convolutional Networks. Bello et al 2019\n[2] Convolutional self-attention network. Yang et al 2018\n[3] QAnet: Combining local convolution with global selfattention for reading comprehension. Yu et al 2018\n[4] Pay Less Attention with Lightweight and Dynamic Convolutions. Wu et al 2019\n[5] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.  Dai et al. 2018\n[6] Image Transformer. Parmar et al. 2018\n"}