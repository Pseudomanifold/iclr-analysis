{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces the Affine Self Convolution that combines local connectivity, relative positional embeddings, and a convolutional filter. This form of local self-attention is spatially equivariant and is further extended to include rotational symmetries. Finally, a generalized form of Squeeze and Excite is derived that can be implied in a setting where the symmetries of some group need to be preserved. The experiments demonstrate that networks using affine self convolution use less parameters while attaining similar accuracies compared to using convolutional models.\n\nThe paper describes how to merge self attention with (depthwise) convolutions. This merge is executed in a sensible way and the derivations seems correct to me. The theory justifies that combined convolutions and attention could be beneficial and the experiments do verify that it is possible to train networks with affine self convolution to a reasonable accuracy. \n\nHowever, I recommend to reject this paper because:\n1) The paper does not clearly explain its contributions.\n2) The experiments do not conclusively demonstrate a significant practical advantage that justify the additional complexity.\n3) A significant part of the paper is dedicated to generalizing spatial equivariance to arbitrary groups, which seems unrelated to the contributions.\n\nMain argument\n\n1) The paper itself is not enough to understand how the proposed method compares to other local self-attention methods or adaptive filter methods. In the related work section various self-attention and data dependent filter methods are mentioned. It is unclear how the performance of those methods compares to the attention mechanism proposed in this paper.\n\nThe convolutional operation that is merged into the attention has a special form of sparsity because it is factorized in the feature dimension (depthwise convolutions). In convolutional models, depthwise convolutions have been used to increase performance while reducing parameter count. The paper only compares attention models with depthwise convolutions to CNN models with normal convolutions. It is therefore unclear what the increased parameter efficiently should be attributed to and whether the attention adds any benefit on top of the depthwise convolutions.\n\nFinally, it is unclear what the contribution is in the section on Group Squeeze and Excite (Group SE). It seems obvious that if an average over all spatial dimensions is taken as an input, the function will be invariant to any transformation of the spatial coordinate like translation and rotation. Therefore, the Group SE does not seem to differ in any significant way from normal SE. \n\n2) The reported accuracies on CIFAR are on par with the convolutional baseline. The authors argue that affine self convolution is more efficient in terms of parameters. However, as mentioned before this is not an apples to apples comparison because a method with sparse convolutions is compared to a model with dense convolutions. Other metrics that could give more information about the potential efficiency  of the method (flops, wall-clock time, etc.)  are lacking. There is also no details about how the affine self convolution can be implemented on modern accelerators which seems non-trivial.\n\n3) Due to the structure of the paper it is difficult recognize the contributions of the authors.  Local self-attention is derived within the context of an arbitrary group rather than just spatial transformations. However, it is unclear how generalizing the derivation to groups leads to any new insights. In fact, it seems obvious that any function that acts solely on a patch that is extracted using either spatial or rotational transformations will be invariant to those transformations. There is no special structure in the proposed attention mechanism that creates this symmetry or makes it easier to compute the function at every patch.\n\nPossible improvements:\n1) Re-organize the paper such that all group equivariance material is in its own section.\n2) Explain how the group equivariance affects the algorithm apart from operating on a different set of patches.\n3) Perform an ablation study. This might be somewhat complicated by the fact that two different methods are merged into one. Both the ablation towards vanilla self-attention as well as depthwise convolutions would be valuable.\n4) Provide more detailed information about the practical implementation of the attention mechanism: How efficiently can this be implemented on modern accelerators? How fast could it be in theory? What is the observed wall-clock time compared to CNNs?\n"}