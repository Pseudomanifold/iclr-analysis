{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The submission proposes an autoencoder architecture which combines two recent GAN-based architectural innovations, namely the progressive growing of the decoder architecture (as well as the encoder architecture in this case) and the use of the encoded representation to modulate the decoder via a feature-wise transformation mechanism.\n\nI think the overall idea behind the paper is valuable, but I don\u2019t think the submission meets the acceptance bar from a clarity point of view. I also have concerns with its characterization of the literature.\n\nClarity-related comments:\n\n- \u201cThis allows the layers to work independently of each other.\u201d This is an imprecise use of the term \u201cindependently\u201d. How can two layers work independently if one\u2019s input is the output of the other?\n- \u201cThe reconstructed samples could be re-introduced to the encoder, repeating the process, and requiring consistency between passes.\u201d The rest of the paragraph is built on the premise that this is a desirable property, but I\u2019m not sure I understand why this is a desirable property in the first place.\n- How to define \u201cdisentanglement\u201d in the context of representation learning is in itself an unsettled question as far as I\u2019m aware, but the submission uses the term without an intuitive or formal definition. What do the authors mean by \u201cdisentangled representation\u201d? What is measured by perceptual path length (PPL), and in which ways does PPL relate to the author\u2019s definition of \u201cdisentangled representation\u201d?\n- \u201c[...] a new autoencoder-like model with powerful properties not found in regular autoencoders, including style transfer.\u201d The term \u201cstyle transfer\u201d is overloaded; what do the authors mean?\n- The submission defines AdaIn as a way to combine \u201ccontent\u201d and \u201cstyle\u201d, and defines the style \u201cy\u201d in terms of mean and variance. In the context of the AdaIn paper, this makes sense: the instance normalization shifting and scaling coefficients are heuristically defined as the channel-wise means and standard deviations of a \u201cstyle\u201d stack of feature maps. However, in the context of this submission I\u2019m not sure this definition makes as much sense: the instance normalization shifting and scaling coefficients are the result of a linear projection of the latent representation and do not involve the channel-wise means and standard deviations of an external stack of feature maps; is this correct?\n- \u201cThis setup follows the same logic as that of Karras et al. (2019), but we do not require an ad-hoc disentanglement stack.\u201d Can the authors clarify what they mean by an \u201cad-hoc disentanglement stack\u201d?\n- Section 3.2 uses some notation for the encoder without introducing it first. I believe the only way to understand that \\phi(x) refers to the encoder network is to look at Figure 2.\n- Section 3.2 as a whole is hard to follow, in part due to the use of imprecise language (\u201cmutually independent\u201d, \u201crepresentation of those levels disentangled in z\u201d). At some point probability distributions are introduced (up until now the reader is operating under the assumption that the model is an autoencoder with no probabilistic interpretation), and mutual information is mentioned to justify an L2 reconstruction loss in z-space (which I would argue is an instance of mathiness that does not serve the reader\u2019s comprehension). Can the authors explain in plain language how layer-specific losses are defined and how the complete loss is obtained?\n- The submission presents model samples, but as far as I can tell the procedure for sampling is not provided. Unlike VAEs, autoencoders do not explicitly model the empirical distribution -- although reconstruction in denoising autoencoders is related to the score of the empirical distribution (Alain & Bengio, 2014). How are samples obtained from the trained model?\n\nLiterature-related comments:\n\n- The use of normalization layers to implement feature-wise transformation mechanisms is fairly widespread nowadays, but for instance normalization specifically the work of Dumoulin et al. (2017) pre-dates that of Huang & Belongie (2017). Both are cited by Karras et al. (2019) in relation to AdaIn (which is termed \u201cconditional instance normalization\u201d in Dumoulin et al. (2017)).\n- I disagree with the characterization of ALI/BiGAN as \u201chybrid models that combine the properties of VAEs and GANs\u201d: unlike AAE and AVB, which minimize KL-divergence terms in the VAE loss adversarially, the objective for ALI/BiGAN is purely adversarial. I would also include IAE (Makhzani et al., 2018) and BigBiGAN (Donahue et al., 2019) in the list of GAN variants that incorporate an inference mechanism.\n- The submission repeatedly asserts that GANs lack an inference mechanism: \u201cUnlike GANs, autoencoder models can directly operate on input samples.\u201d; \u201cTo work on new input images, GANs either need to be extended with a separate encoder, or inverted [...].\u201d; \u201c[...] GANs show good image quality, but have no built-in encoding mechanism [...]\u201d; \u201c[...] the problem with GANs is that they lack the encoder [...]\u201d. This is false: see for example ALI, BiGAN, and BigBiGAN. The problem in my opinion is elsewhere: the kinds of reconstructions these models yield are not suited to the downstream applications investigated in this submission, because they oftentimes fail to preserve low-level details.\n\nReferences:\n\n- Alain, G., & Bengio, Y. (2014). What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1), 3563-3593.\n- Dumoulin, V., Shlens, J., & Kudlur, M. (2017). A learned representation for artistic style. In Proceedings of the International Conference on Learning Representations.\n- Makhzani, A. (2018). Implicit autoencoders. arXiv:1805.09804.\n- Donahue, J., & Simonyan, K. (2019). Large scale adversarial representation learning. arXiv:1907.02544."}