{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper studies the problem of optimization for neural networks. It compares the optimization problem in parameter space with the corresponding problem in function space. In particular, it parameterizes function space using Fourier coefficients, so that the new problem is convex. When the \"disparity matrix\" (Jacobian) of the mapping from parameter space into function space has full rank, then critical points in parameter spaces are critical points in function space and hence global minima. The paper concludes by stating that \"an over-parameterized neural network is randomly initialized [...] will converge to a global minimum of zero loss in probability one\".\n\nI believe that the paper in its current form should be rejected. The main reason is that the second part of the paper is not rigorous, and the results that are shown do not imply that the optimization process will converge to a global optimum (contrary to what is explicitly stated). Indeed, while it is reasonable to assume that the \"disparity matrix\" should have full rank at initialization, this may not remain true throughout the training process. \nThere are also other technical problems, discussed below.\n\nSome comments:\n\n* The notation Q(.) is used inconsistently (Q(f), Q(w) and Q(\\theta)). The same is true for f_*, (f_w and f_\\theta).\n* I believe the disparity map is simply the Jacobian of the mapping w->theta.\nAlso, there are missing gradient symbols in eq. 8.\n* The \"canonical model space\" is not clearly defined. I believe it is simply L1(U) parameterized in a certain way, but this should be stated.\n* By considering the mapping into the truncated \"canonical model space\" (i.e., using a finite number of Fourier coefficients) then f_theta is not exactly equal to f_w. In particular, if f_theta has zero loss, then the same is not necessarily true for f_w. Thus, we cannot conclude that f_w is a global minimum. \n* The last statement in Theorem 2 is not clear (\"the trajectory [...] behaves in the same as those in typical convex optimization problems\"). From the proof, I believe it should mean that the disparity map never vanishes, but I don't understand why this is relevant (we would like for it to have full rank).\n* The main result of the paper rests on the claim \"When we use the gradient descent algorithms in Algorithm 1 to update the model, the chance to derive any new dead or duplicated neurons is extremely slim because it is unlikely for all parameters to simultaneously satisfy a large number of equality constraints\" but this is not rigorous and wrong.\n* Some typos: \"pointwise distint\", \"suface\""}