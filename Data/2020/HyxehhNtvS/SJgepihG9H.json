{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper looks at the neural net training problem in a \"canonical space\" which is parameterized by the Fourier coefficients of the function. This canonical space is a bijection of the function space L^1([0, 1]^K), and if we allow an epsilon approximation of the function we can truncate the Fourier coefficients so that the canonical space is finite-dimensional. The paper shows that in the canonical space, the training problem is always convex. Going back to the literal space (original parameter space for a neural network), it is shown that as long as a \"disparity matrix\" remains full rank, gradient descent will converge to a global minimum.\n\nI don't think this paper has anything new or non-trivial. I also don't think it's helpful to look at the canonical space proposed in the paper. In particular, it just transfers the difficulty of the problem into a disparity matrix, which we actually don't have control over. The paper claims that the matrix can be made full rank. This is not correct. Maybe one can prove it's full rank at random initialization, but I don't see how to prove this throughout training. The authors would need to provide a rigorous proof in order to claim this.\n\nIn fact, in non-convex optimization it's easy to arrive at a scenario where you \"only\" need some matrix to remain full rank in order to prove convergence to global minimum. One such example is the recent series of work on neural tangent kernel (NTK). There, as long as the NTK matrix stays full rank (actually, one needs eigenvalues bounded away from 0), one can show convergence to global minimum. However, to actually show this, one needs to apply stringent assumptions on the neural network architecture and to devote dozens of pages to the proof."}