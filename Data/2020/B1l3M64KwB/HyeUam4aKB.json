{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper investigates a list of methods to reduce the number of weights for deep RL architecture under the low-data regime. These methods include tensor regression, wavelet scattering, as well as second-order optimization (K-FAC). The experiments on the Atari games shows that by using tensor regression to replace the dense layer of the neural nets and using K-FAC for the optimization, one can reduce around 10 times of parameters without losing too much of performance. \n\nHowever, I have some concerns on the novelty of this work and therefore I\u2019m giving this paper a weak reject. Here are the reasons:\n\nTo begin with, leveraging tensor structure of the neural nets to reduce number of parameters while maintaining similar level or getting even better results have been done before, for example: Tensorizing Neural Networks (Novikov et al, 2015), Learning compact recurrent neural networks with block-term tensor decomposition (Ye et al., 2018) etc. Although the use of tensor regression might be new, the core idea is still to leverage the low rank property of the tensor and obtain a compression of the weight tensors. Moreover, why use Tucker decomposition specifically for the tensor regression? It has been proposed that using tensor train (TT) decomposition can also get very good results (see Garipov et. al. Ultimate tensorization: compressing convolutional and FC layers alike). Is it possible to investigate the use of TT decomposition for the dense layer of the deep RL architecture? Therefore the novelty for this aspect seems a bit weak for me.\n\nThe second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. For one particular game (demon_attack), this approach seems to outperform every other methods by a large margin. However the experiment shows that for the rest of the Atari games, there is a huge drop (45%) of performance. Therefore the significance of this approach is rather thin for me. Maybe some further investigation of the game demon_attack is needed to understand why using scattering in this game in particular gives such a huge performance boost. \n\nThirdly, as an approximation of the second order optimization, K-FAC does not really concern with the main theme of the paper, which is an investigation of potential weights reduction methods. It is great that the authors applied this techniques and seems to have great results. However, as the authors pointed out, K-FAC has been wildly applied in the deep RL literature, and the authors did not propose new extension for the K-FAC method, therefore the contribution of this matter is also quite thin. \n\nLast but not least, the writing of the paper is a bit clumsy, and I was having a hard time to figure out what exactly is the proposed method. I think this paper might need some rework on the writing to describe the idea of the authors in a more clear way for the publication. Due to these reasons, I\u2019m giving this paper a weak reject. \n\nSome writing comments and potential writing errors (did not affect the decision):\nPage 3, first line of \u201cTensor regression layer\u201d, the shape of the tensor X seems to be a typo. \nAlso here, the definition of <X, Y>_N in the paper is to sum over the dimension of I_1\u2026I_N, then the shape of <X, Y>_N should be K_1*\u2026*K_x*L_1*\u2026*L_y, without the I_N in the middle. \nAlso in this section, the authors mentioned Tucker decomposition for the tensor regression. However the phrasing of this sentence needs a bit rework. The usage of \u201cFor instance here\u201d, gives the readers a feeling that Tucker is just one possible way of doing this decomposition, but not necessarily the actual decomposition for the reported experiments. \nIn 2.3, there is a lack of definition for  \\Lambda_1 and \\Lambda_2. In addition, it would be better for the general readers to add a few definitions for the terminologies in this section. For example, \u201ccircular convolution\u201d, \u201cwavelet filter banks\u201d etc. I guess people with corresponding background will understand it with no problem, however I do find myself a bit lost in this section with these terminologies. \n2.4 line 6, \u201cwith A and. B smaller, architecture-dependent matrices\u201d. I think it should be \u201cwith A and B being\u2026\u201c\n3.1, line 5, \u201cThis is all the more pressing that\u2026.\u201d, I did not understand this sentence. \nIn page 6, line 3, there is a lack of definition for \u201ccompression rate\u201d. Is it the compression rate w.r.t only the last dense layer, or w.r.t the whole network?\nFigure 4 is lacking y-axis and x-axis labels. \n4.2, last bullet point, \u201cHowever, one must not forget that the conv layers one learns must be somehow be well adapted\u2026\u201d, I get what you are saying, but the sentence is a bit clumsy. \nTable 1 and 2, the row name \u201cAverage\u201d is lacking definition. \n\n Overall it is a good attempt to reduce the number of weights in the deep RL architecture, but I do think the novelty of this work is a bit thin and the three contributions were not tied together with the main theme of the paper. Therefore, I\u2019m giving this work a weak reject. \n"}