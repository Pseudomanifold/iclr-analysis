{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the problem of adversarial robustness. The paper shows that (Theorem 1) robust generalization error can be bounded in terms of the standard generalization error and a stability term, that does not depend on the labels. The paper also shows that for a simple classification problem involving learning the separator for a symmetric 2 gaussian mixture data, we can solve this problem robustly without additional labeled examples. The paper suggests that we can use unlabeled data to improve the robust generalization. Towards this the paper regularizer on the unlabeled data, that promotes stability in the model prediction. The paper evaluates this on Mnist and Cifar showing the better performance of the proposed regularization over PGD adversarial training.\n\nThe Theorem 1 in this paper is a triangle inequality on the loss ,and the observation about splitting the robust generalization into standard generalization error and stability, is not particularly new. The earlier work Zhang et al., 2019b show a similar result in their paper. They even propose and experiment with a similar regularizer (see eqs 3 and 5 in Zhang et al., 2019b). The exact implementation while can be different between these two, the paper does not currently compare with this and there is no evidence to prefer this regularizer over the existing one.\n\nThe Gaussian setting considered in this paper is quite simple and the techniques developed there are particular to the symmetric 2 Gaussian mixture problem. Given the other parallel works studying the same setting, it is good to also include a comparison of the exact results (such as sample complexity) for this setup.\n\nOverall I find the contributions of this paper to be not sufficient and cannot recommend acceptance at this stage.\n\nMinor:\nThe last line above theorem 4 and second line after eq 9 are written poorly.\n\n Zhang et al., 2019b  https://arxiv.org/abs/1901.08573"}