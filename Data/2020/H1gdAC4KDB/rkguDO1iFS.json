{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors study the sample complexity of adversarially robust learning with access to unlabeled samples. Theoretically, they consider the setting of Schmidt et al. 2018 (separating two class-conditional Gaussians) and present an algorithm which can learn a robust classifier with only a few labeled samples and a large number of unlabeled samples (circumventing the sample complexity separation of the original work). Then, the authors propose a modification of the VAT algorithm (Miyato et al. 2018) to train deep networks utilizing unlabeled samples. They find that, empirically, their algorithm achieves better performance compared to standard adversarial training on the labeled samples.\n\nOverall, the paper addresses an interesting problem, studying both a simple theoretical setting and a real-world empirical setting in which the authors achieve an improvement over prior work.\n\nUnfortunately, the paper is concurrent with two other works (which the authors acknowledge: Carmon et al. 2019, Uesato et al. 2019) which have already been accepted for publication at NeurIPS 2019. All of these works are very similar in spirit, proposing an algorithm for the theoretical setting of Schmidt et al. 2018 and an empirical algorithm for real-world settings. Moreover, these works improve over the current manuscript in a number of ways:\n-- The algorithm proposed for the theoretical setting is more general and is essentially the same as the algorithm used for real-world dataset.\n-- The experimental evaluation is significantly more extensive, performing additional ablations, and exploring the methods in more detail. The work of Uesato et al. 2019 is virtually a superset of the results in this manuscript.\n-- Both works collect additional images from an unlabeled and uncurated dataset (Tiny Images) and show that they can utilize them using their proposed approach to improve the state-of-the-art robust accuracy on CIFAR10.\n\nTherefore, given that: a) the results in the current manuscript are essentially a subset of the results appearing in Carmon et al. 2019 and Uesato et al. 2019 and b) these works will have already been published at NeurIPS 2019, 4 months before ICLR 2020, I am afraid I need to recommend rejection."}