{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper explores the application of RL to the problem of match plan generation for answering queries in Microsoft Bing.\n\nA potential strength of this paper is to draw attention to a new application of RL, one that potentially has significant social and financial impact. The paper provides some insights into how to cast the problem into the RL framework, in particular the definition of the parameterized action space.\n\nThe paper\u2019s weaknesses include:\n-\tInsufficient detail on some aspects of the problem specification, including a clear explanation of match plan generation, definition of the state space variables, and an explanation of what the transition function might correspond to.\n-\tThe learned policy is only tested on a held-out test set, not in a simulator.  This is not convincing  for RL applications, where the distribution of states changes as a function of the policy, so the estimate of the learned policy will be inaccurate on a policy collected from a different policy (unless correction is applied, e.g. importance sampling, which is not mentioned in the paper).\n-\tInsights as to how the findings from this new application of RL might inform other applications of RL.  This would be necessary for publication in a venue such as ICLR, where most participants & readers are primarily interested in new methods.\n-\tThe dataset on which training is done is not public.  This is not absolutely necessary, but would certainly increase the value of the paper for the ICLR community.\n\nAdditional comments:\n-\tYou seem to use a reward shaping function (bottom of p.6, \u201cReward design\u201d).  Is this just used for training (as a shaping signal), or also in testing the policy?   It is somewhat concerning that you need to add this explicit signal to modify the policy.\n-\tThe results on the games from Bester et al. (2019) do not necessarily strengthen the paper.  It\u2019s not clear what is \u201cyour algorithm\u201d that you test and compare to others, and how the games considered are similar/different than your target application.\n\n\n"}