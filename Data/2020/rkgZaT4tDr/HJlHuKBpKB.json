{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a parameterized action RL match plan generation method which extends the\nplan generation to the general case without any predefined knowledge. Key to address the problem\nare normalized softmax values of discrete actions to enable gradients backpropagation, parameter\nspace noise on parameters of the policy for unifying the exploration direction in both discrete and\ncontinuous spaces, and recurrent deterministic policies with prioritized replay buffer to accelerate\nand stabilize the training.\n\nThe paper is well written.\n\nStrength: This paper applied RL method on match plan generation. It shows superior performance than manual methods. \n\nWeakness: Although the experimental results are exciting, the method itself is nothing novel. I do believe this is a good paper but not sure it is the best fit to this conference. \n\nI am not an expert in this specific area. I'll leave the the decisions to other reviewers and ACs."}