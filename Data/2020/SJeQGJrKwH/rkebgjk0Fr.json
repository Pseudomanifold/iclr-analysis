{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an algorithm for discovering decision states in MDPs, in a task agnostic manner. The proposed method essentially generalizes the information bottleneck approach used in InfoBot, to an unsupervised setting. Whereas InfoBot recovers decision states in goal-conditioned policies by minimizing mutual information between goals and actions, the proposed approach (DS-VIC) does the same using implicit goals discovered by variational intrinsic control. Concretely, the authors propose adding a regularization term which constrains the mutual information between an episodic latent variable (having high mutual information to a final state) and each action along the (option conditional) trajectory. This objective is shown to be equivalent to a constraint optimization problem, which fits both a lower and upper bound on the VIC mutual information term. On MiniGrid environments, the approach is shown to yield somewhat interpretable decision states. In the footsteps of InfoBot, the authors then show that the resulting regularizer (the \u201clatent-action\u201d mutual information term) can serve as a useful auxiliary reward for transfer tasks with a hard exploration problem (transfer from goal navigation in small to large rooms).\n\nThe paper is interesting and provides some interesting theoretical results which adds to the body of work on variational intrinsic control, and unsupervised reinforcement learning. To the best of my knowledge, the derived upper-bound to the mutual information term used in VIC is novel and would be of interest to the community. The extension of InfoBot to the unsupervised regime, swapping extrinsic goals for inferred options is also intuitive and generalizes published work. \n\nThat being said, I do not think the paper is ready for publication at this point in time.\n\nOn the experimental side, the results are limited and not entirely convincing. Experiments are unfortunately limited to MiniGrid environments and MountainCar, whereas most recent work on empowerment (DIAYN, VALOR, etc.) has focused on more complex continuous control benchmarks. In addition, the experiments themselves are limited in scope and rely mostly on evaluating whether the mutual information term between goal and actions can be used to craft an auxiliary reward for downstream tasks, a task first derived in the InfoBot paper. Unfortunately, the results here are mixed, with the mutual information based reward statistically outperforming count-based bonus (which it builds on) only when the optimal hyper-parameter $\\beta$ (controlling the strength of the regularization term) is known. This somewhat breaks the narrative of unsupervised decision states. A much more compelling use case for the proposed regularization term would be improved data-efficiency on a downstreak task after pre-training with Eq. 6, following in the footsteps of DIAYN. Finally, as shown in the Appendix, the method seems rather brittle, requiring both a schedule on the size of the option layer and the strength of the regularization term, something which would be difficult to do in the absence of a downstream task.\n\nThere is also an important missing baseline which is glossed over. Instead of regularizing the mutual information between goals and actions $I(\\Omega, A_t \\mid S_t, S_0)$ one could simply encourage the low-level goal-conditioned policy to have high entropy. Indeed, one can show that  $KL[\\pi(a_t \\mid s_t, w_t) \\| \\pi_0(a_t)]$, with a learnt or fixed prior $\\pi_0$, is an upper-bound to $I({s_t, w_t} ; a_t)$: hence minimizing this KL (equivalent to maximizing entropy) would naturally prevent high mutual information between options and individual actions. As this term is already present in Eq. 6 it would be interesting to repeat the experiments, dropping the second term (minimality) but instead sweeping over the strength of the entropy regularization term $\\alpha$.\n\nWith respect to clarity, the paper could also be greatly improved. Decision states are never clearly defined to be those having high mutual information $I(\\Omega, A_t \\mid S_t)$. It is also not clear from the main text was is being plotted in Figure 3, requiring the reader to go through Appendix 4 to understand the visualization (without any references to this appendix in the main text). Similarly, I am not quite sure what to make of Figure 4 apart from the fact that different latent options yield different trajectories. See detailed comments below.\n\n\nDetailed Comments:\n(method)\n* It is rather disappointing that the reverse predictor uses privileged information, in the guise of x-y features. This represents quite a lot of prior knowledge about what we wish the options to encode. How does the method perform from the raw state?\n* Although mathematically elegant, I do not believe the sandwich bound explains why Eq. 6 helps uncover decision states. If we had an unbiased estimate of the mutual information between option and last state, then this would imply that an equality constraint on the VIC mutual information term would similarly yield decision states. This seems unlikely. An alternative hypothesis is that Eq. 6 works by injecting a soft prior, both via the temporal decomposition which aims to minimize $I(\\Omega, A_t |...)$ and its upper-bound $I(\\Omega, Z_t | \u2026)$ which bottlenecks state information. Testing this theory could help strengthen the paper.\n* It would be nice to spell-out that the standard \u201creverse bound\u201d employed by VIC cannot be used to estimate $I(\\omega, A_t \\mid S_t, S_0)$ as this would yield a lower-bound whereas we aim to minimize this term. I was almost tricked into thinking this was a simpler and valid strategy before realizing my mistake.\n* Footnote 7. High variance on $\\beta=1e-4$. Is it possible that too few seeds were used to estimate the standard error on the mean?\n\n(clarity)\n*\u201cDecision states to be points where the cart has velocity=0\u201d: wouldn\u2019t this mostly be restricted to the initial state?\nWhat exactly was done for DIAYN in relation to Eq. 8? The text from S4-Baselines seems at odds with the caption.\n* \u201cUpper bound is too tight\u201d. I don\u2019t think this is what you mean: tight would refer to how good the upper-bound approximation is, which is different from the constraint specifying an upper-bound which is too small.\n* Notation: Section 2.1 states that upper-case denotes random variables and lower-case denotes samples. Following this notation, equation should read e.g. $w \\sim p(\\Omega)$ and not $\\Omega \\sim p(w)$.\n* Notation: $p^J(s_f \\mid w, s_0)$. What does J refer to? J is not defined anywhere.\n* Section 4.1: Did not understand the sentence \u201cwe noticed that if an intersection is a decision state [...] having already made the decision.\u201d\n* Section 4.1: Did not understand what is meant by \u201cwhere trajectories associated with different options intersect\u201d? What does this mean concretely in MountainCar for trajectories to intersect? Furthermore aren\u2019t states with velocity=0 (mostly) restricted to the initial state?"}