{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper proposes an unsupervised method for discovering \"decision states\", defined as states where decisions affect the future states an agent can reach in the environment, based on the variational intrinsic control (VIC) framework that maximizes an agent's empowerment. This paper draws connections to many prior works such as VIC, diversity is all you need (DIAYN), and InfoBot and shows results on MiniGrid and MountainCar. \n\nMain Comments:\n\nWhile this is an interesting paper, I did not find the experimental section to be convincing enough for publication at this stage. Moreover, I am concerned by the novelty of the proposed approach, which seems very similar to InfoBot, the main difference between them being the replacement of the goals with options thus moving towards less supervision / use of prior-knowledge. However, if I understand correctly, this method still requires to specify a prior over the options, so it is not clear why DS-VIC would be preferable to InfoBot. If the empirical results showed a more robust and significant gain in performance on more diverse or complex tasks, I would be willing to reconsider my judgement regarding the significance of this work. \n\nMinor Questions / Comments:\n\n1. How do you define the final state S_f? Do you only consider the episodic RL setting? Do you consider S_f to always be after a fixed number of steps or whenever the termination function is triggered?\n\n2. Please include more information about what is represented in Figure 3 and the color scale. I am slightly confused by the interpretation of that plot because (1) it seems like the model does not detect \"all decision states\" (e.g. intersections) . that a human may consider while including others (e.g. corners, for which I do not agree that the agent should be incentivized to go even after learning from the reward structure that there isn't much to gain), (2) why is it that the for example the top-left figure has a rather nonuniform distribution across the rooms (is it influenced by the initial position of the agent?) and (3) the model doesn't seem to be very consistent about what it considers a to be a \"decision state\".\n\n3. Can you show similar plots for the MultiRoom environment? Perhaps those will shed more light into the learned behavior. \n\n4. Including all possible ablations to the objective in equation 6 would be helpful to tease apart the contribution of each term: variational control, information bottleneck, and entropy.\n\n5.  The results in Table 1 and Figures 6 do not show a significant gain in performance. Moreover, I suspect the other methods will converge to similar values soon after 8M steps. For a fair comparison, it would be good to show how the curves look after all (or at least . more of the models) converge. From Table 1, it actually seems to me that InfoBot encounters less penalty across all 3 models, even tho DS-VIC overperforms on the more challenging one. Moreover, in all 3 cases, at least one of the other methods seems to at least be close to the performance of DS-VIC so I am concerned that these may not be very challenging tasks for well-tuned SOTA methods. Plus, the comparison does not seem fair given that the the numbers are reported before the baselines converged. \n \n6. Did you pretrain the baselines for the same number of steps as DS-VIC? Please include more details about this stage and how you ensure the comparison was fair.\n\n7. It might be useful to include other baselines such as the curiosity-based exploration method from Pathak et al, 2017 . or universal value functions (Schaul et al. 2015)\n"}