{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The study of extremely over-parameterized networks (i.e. infinitely width networks) has become one of the most active research directions in theory deep learning. The key objects in understanding such networks are the conjugate kernel [1, 2] (CK defined in the paper) and the Neural tangent kernels [3] (NTK). The CK characterizes how the network looks like at initialization (connection to Gaussian processes as well) and the NTK is very useful to characterize the gradient descent training dynamics of large width networks in the kernel regime. Understanding properties of such kernels, in particular, their spectra distribution and eigenspace, could be potentially an important step towards a finer-gained understanding of generalization in neural networks.   \n\nThe main contribution of this paper is the development of the spectral theory of CK and NTK on boolean cube (similar or weaker results on uniform distribution in spheres and Gaussian distribution in R^n). More precisely, the authors show that, over the space of boolean cube, the CK/NTK could be diagonalized using the Fourier basis and the eigenvalues depend only on the frequency (i.e. the degree of the monomials); Thm 3.1. The authors also develop some computation tools to compute the spectra; Lemma 3.2.   Using the tools developed in this paper, the authors are able to clarify some of the interesting observations found by other researchers. Most noticeably, the authors show that the observation in [4] 'neural network is biased towards simple functions' is NOT universal. Whether this statement is correct or not depends heavily on the choice of activation function (e.g. Relu v.s. Erf) and hyper-parameters (e.g. weight variance, depths).  There are also some other interesting empirical findings: the optimal depth of a neural network depends on the complexity (i.e. degree in the boolean cube setting) of the function to learn, CK (i.e. training only the last layer) tends to be more useful for learning less complex functions, etc. \n\nOverall, this is a nice paper. I am leaning for a weak rejection for the reasons below\n1. The pool of readers of this paper might be small given that not many readers in the deep learning community are familiar with boolean cube analysis (this is the first ''deep learning - boolean cube analysis'' paper I read). \n2. Though the spectral theory developed in this paper is very nice,  I did not see a strong connection between this mathematical theory and research in machine learning. That is to say, I would hope to see the application of this theory to  some important open problems in machine learning that is hard to solve in the sphere or Gaussian distribution setting. Therefore, finding the right problem to apply this theory could significantly boost the impact of the paper and potentially attract more attention into the field of boolean cube analysis. \n\n\n\n[1] Amit Daniely, Roy Frostig, and Yoram Singer. Toward Deeper Understanding of Neural Networks:\nThe Power of Initialization and a Dual View on Expressivity. arXiv:1602.05897 [cs, stat], February\n2016.\n[2] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha\nSohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on\nLearning Representations, 2018.\n[3] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural Tangent Kernel: Convergence and\nGeneralization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. 00000\n[4] Guillermo Valle-P\u00e9rez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because\nthe parameter-function map is biased towards simple functions. arXiv:1805.08522 [cs, stat], May\n2018.\n"}