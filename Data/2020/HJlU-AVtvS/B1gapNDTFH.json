{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks' conjugate kernel(CK) and neural tangent kernel(NTK) on boolean cube. The eigenfunctions are identified and the eigenvalues are shown computable in polynomial time. Another main contribution of this paper is showing that the simplicity bias exists at least in a weak sense.\n\nI believe that this paper should be weakly rejected because it made more claims than what it can show in that the analysis doesn't work in real space, and the authors did not really show the simplicity bias. The following are my detailed comments.\n\nFirst, the whole analysis is based on boolean cube. Although the paper has shown empirically that in high dimension the uniform binary distribution is close enough to the uniform sphere distribution, it doesn't suffice to substitute boolean cube for sphere in real space. The spectral analysis in this paper is heavily due to working on boolean cube. The boolean cube is finite, which guarantees any inner-product kernel function $K(x,y) = \\Phi (<x,y>)$ can be diagonalized by finite many monomial functions, And there are only O(d) different eigenvalues, which enables efficient computation. These techniques are not easy to be transferred to real space. The experiment shows that the first five eigenvalues in boolean cube, sphere, gaussian is close, but key problems here are first, in practive the dimension $d$ could be smaller and second, sphere and gaussian have infinitely many eigenvalues while boolean cube has $2^d$ eigenvalues. The experiment cannot really justify that all eigenvalues are close (only first several are shown), not to mention the tail eigenvalues over the first $2^d$-th.\n\nEven if we assume that boolean cube is a reasonable choice, we should notice the goal of computing eigenvalues is to eventually show the inductive bias toward 'simple functions'. However, the authors failed to show it at least from the following perspectives:\n1) This paper did not show the trend of eigenvalues, but only the weak version of, for example, $\\mu_{2k-2} > \\mu_{2k}$. In the limiting case, it is more reasonable to fix dimension $d$ rather than the degree $k$.\n2) Working on boolean cube leads to limited complexity. The most complicated base function is restricted to $\\mathcal{X}_S$ where $S = \\{1, 2, \\dots, d\\}$. So the weak simplicity bias theorem actually only describes the relation among finite $d$ eigenvalues.\n3) No optimization arguments appear in this paper. Based on the spectral analysis, it is not rigorous enough to claim the networks are biased to simple functions, given that the target function consists of simple multilinear monomial functions. \n\nSince the boolean spectra is not a reliable measure, the further experiments under such a measure is therefore put under doubt.\n\nTo summarize, this paper definitely contains some rigorous analysis which I appreciate, but it made some claims that are not verified. More importantly, the boolean cube is not the appropriate domain which is hard to generalize to real space and the simplicity bias theorem in this paper is to some extent weak. Therefore, I suggest rejecting this paper in its current form."}