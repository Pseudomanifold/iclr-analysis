{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work, the authors tackle the problem of building dialogue systems that can handle diverse domains and exhibit both chit-chat as well as goal-oriented behavior. The authors describe these different functionalities as different skills. Their work proposes a dialogue model inspired by the Mixture of Experts architecture (Jacobs et. al.), which learns different skills through distinct parameters which are then mixed together in a soft fashion through learned weights. On a combined dataset consisting of MultiWOZ (Budzianowski et. al.), In-Car Assistant (Eric et. al.), and the Persona Chat (Zhang et. al.), the proposed Attention Over Parameters (AoP) model outperforms other baselines on F1, BLEU, SQL query accuracy, and Booking statement accuracy. On a Persona chat evaluation, the AoP also outperforms baselines on a consistency metric calculated via a natural language inference scoring model trained from a dialogue NLI corpus (Sean et. al.)\n\nWhile the problem the authors were tackling is an important one (namely combining chit-chat and goal-oriented dialogue systems), I had several problems with the proposed approach. First off, it is still not clear to me how different the AoP model truly is from a mixture-of-experts model, and in particular the differences presented in Figure 1 are not well-explained. \n\nMoreover, the AoP model does not seem like a novel or significant enough contribution. From my understanding, it simply feels like trying to learn separate decoders per skill and then just softly combining them through an attention weight. Why is having explicit decoders strictly necessary, rather than playing with the multiheads of the self-attention operation?\n\nIn addition, I had issues with the evaluation presented. While the model seemed to outperform some simplistic baselines (transformer and seq2seq architectures) on automatic evaluation, there was no human evaluation done. It is difficult to assess how good the generated outputs are from automatic evaluation alone. \n\nFinally, the authors claim that the model learns fine-grained skill decomposition, and they demonstrate this by fixing their attention weights and showing that they can generate very different output sequences. The only example of this claim is a (potentially) cherry-picked example, and without any larger scale quantitative analysis (with some sort of human eval), it is difficult to know whether the model is actually learning skill decomposition in its separate decoders."}