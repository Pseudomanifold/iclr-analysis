{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary: The authors of this paper proposed an attention over parameters based model for dialogue response generation. They assume that different tasks or topics require different models to generate responses. To model this idea, they design a model that can generate an attention vector based on historical dialogue history. This attention vector can then be used to get a combination of parameters that focus on the right topic/task to generate the next response.\n2. Overall assessment: This idea looks very interesting to me. It's simple but effective. However, this paper can still be improved in several aspects and it is not qualified yet to be published in a very competitive conference such as ICLR. I hope the authors can keep improving this paper and makes it a stronger publication.\n3. Comments:\n3.1 It seems not necessary to model the whole dialogue history. The next response usually mostly depends on the most recent user queries. Meanwhile, with a long sequence of words to encode, the model may not be very efficient. How well does the model work if only a subset of the dialogue history, such as the most recent one or two rounds, is modeled?\n3.2 Before formula (2), $Y_{:k-1} = \\{<SOS>, y_1, \\cdots, y_k \\}$, is it $y_k$ or $y_{k-1}$?\n3.3 How do the authors combine the three training datasets? This is very important information for other researchers to replicate this work.\n3.4 How to get the prior knowledge vector? It is given in the training data? What if we need to work on a dataset without such information? Will the model still work well? What if we remove the supervision over the attention vector in formula (8)? I suggest the authors conduct some experiments and analysis to answer these questions.\n3.5 From an engineering angle, how efficient is it to derive different decoder parameters for different instances and do the inference? As we know, having constant parameters at inference time enables us to do a lot of optimization and parallelizations.  How is the efficiency of the proposed model compared with existing ones?"}