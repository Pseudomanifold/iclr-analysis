{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThe paper proposes a new method to rank learning curves of neural networks which can be used to speed up neural architecture search.\nCompared to previous work, the learning curve model not only takes hyperparameter configurations into account, but by training it on offline generated data, it is able to model learning curves across different datasets.\nApplied to a simple neural architecture search strategy, the proposed method achieves higher speed-ups on image classification tasks than other methods from the literature.\n\nWhile the method seems interesting, I don't think the paper is ready for acceptance yet, since it a) misses some important details and b) the empirical evaluation is not sufficient.\nMore precisely the following points need to be addressed:\n\n - In section 3, the paper says that all automated methods follow broadly the same principal that the first model is trained to completion. This is not correct, commonly used methods, such as Hyperband (Li et al.) or BOHB (Falkner et al.), use successive halving (Jamieson et al.) which trains a batch of configurations for just a minimum budget and then already discards poorly performing configurations. I also miss a discussion of these methods in the related work section.\n\nHyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A.\nJournal of Machine Learning Research 2018\n\nBOHB: Robust and efficient hyperparameter optimization at scale\nS Falkner, A Klein, F Hutter\nProceedings of the 35th International Conference on Machine Learning\n\nNon-stochastic best arm identification and hyperparameter optimization\nK Jamieson, A Talwalkar\nArtificial Intelligence and Statistics, 240-248\n\n\n - Is the learning curve model updated during the optimization process with the new observed data? If not, is there any way the model can adapt to new data? Also, what happens if learning curves are fundamentally different to the training data, e.g if different learning rate schedules are used than for generating the offline data?\n\n - A simple baseline that is missing, is to use the last observed value as approximation for the final performance which often works competitively to more powerful learning curve extrapolation methods (e.g see Klein et al.).\n\n - The method is only applied for a simple random search strategy, however, in practice, one would use more sophisticated methods, such as for example Bayesian optimization. The question is, how effective is the proposed method in this setting, since the distribution of learning curves might be dramatically different and biased towards more well-performing configurations with almost identical learning curves.\n\n- I think the experiments would be more convincing, if the method shows strong performance when deploy in commonly used NAS/HPO methods, such as Hyperband or successive halving. This should be straight forward, since decisions which configuration is promoted to a higher budget can be made based on the model instead of just the last observed value.\n\n- What is delta in the experiments? How sensitive is this hyperparameter in practice?\n\n\nMinor comments:\n\n- Related to this paper is the work by Gargiani et al. which also models learning curves across datasets on offline generated data\n Probabilistic Rollouts for Learning Curve Extrapolation Across Hyperparameter Settings\n M Gargiani, A Klein, S Falkner, F Hutter\n arXiv preprint arXiv:1910.04522\n\n- Figure 5: visually it seems that all learning curves are almost identical, maybe it would be better if the plot could zoom in at least for the final stage of training."}