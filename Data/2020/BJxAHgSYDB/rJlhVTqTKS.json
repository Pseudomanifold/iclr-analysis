{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Learning curve prediction methods try to predict the final performance of a candidate model before it gets fully trained. In this way, learning curve prediction can act as a fast method for performance measurements in AutoML. Compared with previous approaches, the proposed method allows for transferring useful knowledge among different datasets.\n\nWhile the proposed method is simple, the authors have shown promising results. However, I have some concerns about the motivations and usage of the proposed method. Please see the questions below:\n\nQ1. Learning curve prediction is a general approach that can be combined with many zero-order optimization methods. Does this paper focus on learning curve prediction on general AutoML problems or just NAS?\n- If the authors want to target general AutoML problems, they should perform experiments with (1) more datasets (come from various domains) and consider more (2) search algorithms. \n- For (1), the authors can follow Auto-sklearn and Auto-weka, or some experiments on graph (e.g., GCN) or some experiments on language modeling. For (2), they can combine the proposed approach with Bayesian optimization or genetic algorithms. CNN has nice transfer learning ability, to show the wide applicability of the proposed approach is important.\n- If authors want to target at NAS, then the proposed method is not useful. Parameter sharing is a better method in NAS for fast performance evaluation. The authors need to compare with some recent NAS papers in CV (e.g., DARTS).\n\nQ2. \"Learning curve component\". Learning curves are naturally a sequence of data points. The proposed method simply does a maximum pooling over all positions, which ignores the sequence of data. Could the authors give some explanations? Is it better to carry an ablation study on this point?\n\nQ3. \"Architecture component\". How will the changes in the search space affect the proposed method? \n\nQ4. Could the authors random draw many architectures from \"NASNet search space\" and then plot their learning curve? It is better to show what kinds of curves will the proposed method likely to give early stops.\n\nQ5. Another main pitfall of the proposed method cannot offer a probabilistic estimation, which can be done by Baker et al. (2018). Since the model is early stopped, it is naturally that there are some uncertainty in the estimated ranks."}