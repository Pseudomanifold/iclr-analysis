{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "=== A. Summary ===\n\nThe paper proposes to hand-design (as opposed to learning) the 3x3 kernels in the very first layer of an image classifier CNN to be the well-known kernels that perform sharpening, embossing, blurring, and edge detecting, etc.\nAfter that, the network is trained to perform image classification (i.e. these kernels are kept frozen and only other layers' parameters are updated during training) on MNIST or ImageNet.\nThe authors argue that the hand-designed kernels are well-understood and showed qualitative examples of how they can be used to interpret what filters are often used to classify which class of images.\nAdditionally, the paper shows that the integration of the hand-designed filters almost kept the network accuracy unchanged.\n\n\n=== B. Decision ===\n\nReject.\n\nI vote for Reject because the work here, i.e. replacing the 1st layer 3x3 filters with the hand-designed ones, does not provide any new or surprising insights into what we have already known about the first layer kernels of CNNs or CNNs in general.\nIt has been since 2012 that Krizhevsky et al. showed the 1st layer filters learned during the training already learned Gabor-like filters and kernels that detector colors.\nThat is, they are already quite interpretable from human perspectives.\nCan't one reproduce this paper with the learned filters and draw similar insights?\nThe choices of the hand-designed kernels in the paper are arbitrary i.e. not supported by any reason.\n\n\n=== C. Suggestions for improvement ===\n\nInterpreting CNNs is an important quest and designing interpretability into CNNs is a viable approach!\nHowever, I suggest the authors seriously think about the motivation of the work i.e. focus on identifying what questions are not well-understood in CNNs and how your methods answer those questions."}