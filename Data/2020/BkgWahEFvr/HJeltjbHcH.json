{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors analyze the use of image transformation as a defense against adversarial examples, where a challenge is to prevent the deterioration of performance on clean images. To do this, they show that the softmax distributions for clean and adversarial images share similar \"features\", and therefore one can apply a trained distribution classifier which takes the softmax distribution to return the class label. This is as opposed to original approach of making a prediction for each sample (a random transformation of the input image) followed by majority voting.\n\nI have one major concern about their analysis, which makes me lean weakly towards not accepting the paper. I'm happy to change my score after discussions and further clarifications. In particular, I'm not convinced that the phenomena of softmax distributions appearing similarly for clean and adversarial images is not simply a result of the stochastic transformation converging to a fixed (or possibly neighborhood of fixed) distributions. Does the transformation retain information about the difference between classes? That is, while the divergence of softmax distributions from clean and adversarial images decreases as one applies more transformations, does the divergence of softmax distributions from clean images of two different classes also decrease?\n\nAs a non-expert in the field, I found the paper well-written. It motivates the idea well by identifying challenges in a promising technique (random tranfsormations) and provides decent background explanation.\n\nFigure 2 is an interesting example showing the relationship of the softmax distributions for clean and adversarial examples. This raises a few questions which are unclear to me:\n\n1. I'm a bit surprised at how remarkably bad transformation-based defenses seem to be in degrading classifier performance. Is it really the case that the MNIST model only gets 78% accuracy on class label 8 and 36% on class label 6? What about without the transformation?\n2. This phenomena of softmax distributions being similar seems to largely depend on the stochastic transformation T and form of attack. This is especially seen in Figure 2b, which suggests that T may be acting like a transition kernel which takes an arbitrary initial state and may have it converge to a fixed stationary distribution. What does the divergence look like for two class distributions of clean images over the number of pixel deflections?\n\nFor the defense, I'm also curious what happens if the model used the distribution classifier with random transformations at training time, instead of a separate softmax output layer.\n\nRegarding experiments, I only assessed their sensibility. Unfortunately, I don't know enough about the field to tell how significant the results are. \n\n1. For the choice of number of transformations and training set size for the distribution classifier, how were they chosen? They seem to vary arbitrarily across the datasets.\n2. It seems like the improvements appear only in single-step FGSM, which also tends to be the worst performing, where the other methods are iterative. What's the intuition for this?\n"}