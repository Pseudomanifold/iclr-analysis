{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\n\n===== Summary ===== \n\nOne strategy for defense against adversarial attacks is to perturb the input sample multiple times and then aggregate their corresponding outputs into a single output. One recent variant of this method (Prakash et al. 2018) shows using this type of defense mechanism degrades the accuracy on clean (unperturbed) inputs. This paper tries to mitigate the accuracy drop on clean inputs. It does so by learning an additional distribution classifier which takes as input the distribution of perturbed samples\u2019 outputs. The method is trained on MNIST, CIFAR10, CIFAR100 and using 4 different adversarial attacks. It shows significant improvement in several cases over standard input-perturbation methods.\n\n\n===== Strengths and Weaknesses ===== \n\n+ The paper picks an important problem, proposes a simple technique and achieves significant improvements in certain cases.\n+ The experiments are done on 3 datasets and 4 different adversarial attack techniques.\n+ The final experiment on end-to-end attack is interesting.\n\nConcerns regarding the main paper\u2019s description\n- From Figure 1, it seems that the distribution classifier does a binary classification for the distribution of each class separately. If this is the case, how is the final classification done (using the binomial distribution per class)? The figure suggests that the maximum probability of the binary classifications (bus for yellow class with 0.8 score) is taken. This would be strange since they are not optimized to compete with each other (e.g. as in a more proper cross entropy). In any case, the details of the distribution classifier is missing from the paper, and section 4 refers to the figure. The details should come in the main text using precise language and math formulation. \n\n- page 4 is very hard to follow since it tries to give a textual description to mathematical concepts. I think it is important to provide the mathematical definition such that 1) it is easier to follow 2) the concepts are unambiguously defined. This includes a definition of softmax distribution, joint distribution of output and transformation, marginalized distributions, clean intra-class distance, adversarial intra-class distance, and clean-adversarial inter-class distance.\n\n- page 4: it seems it is implied in the definition of \u201cclean\u201d images that these images are classified \u201ccorrectly\u201d by the network. Otherwise, in Figure 2.a, the clean row is uninterpretable not knowing whether the clean image was classified correctly or not.\n\n- page 4,5: what is a \u201ctransformation magnitude\u201d? It could refer to both the number of transformations and also the number of perturbed pixels. The behavior in Figure 3 suggests that the latter should be the case, but it\u2019s important to disambiguate.\n\n- figure 3 is counterintuitive. The support (possible distributions) should normally live on a simplex (due the sum-to-one probability constraint). In figure 3, it seems there is support in the full unit hypercube. In particular, we can see density along the horizontal/vertical line of 1-density for one class (i.e. non-zero density for others in this case)\n\n- page 6: \u201c[...] to train a distribution classifier on the distributions obtained from clean images only [...]\u201c. I cannot follow the argument at the end of the first paragraph that led to the proposed *training on clean images only*. I understand that from the qualitative demonstration of 8 different examples in Figure 4 the distributions of perturbed adversarial examples *might be* redundant to the perturbed wrongly-classified clean examples. However, even if it is redundant, why should it be harmful to train the distribution classifier with adversarial examples as well as clean examples?\n\nRegarding Experiments:\n- why is the evaluation done only based on accuracy of *correctly-classified* clean images and the recovery of *wrongly-classified* adversarial images? Couldn\u2019t it be that the baseline (majority voting) does a better job than the distribution classifier on the \u201cwrongly-classified\u201d clean images and the \u201ccorrectly-classified\u201d adversarial images? \n- I think it's still interesting and informative to train the distribution classifier using both clean and perturbed inputs to compare the performance.\n- page 6: why is there a need for applying a kernel density estimator followed by a discretization (e.g. for instance instead of a simple histogram)?\n\n===== Final Decision ===== \nMy current \u201cweak reject\u201d rating is primarily based on the unclarity of the main paper and secondary regarding the concerns for experiments. \n\n===== Points of Improvement ===== \nI believe the paper will become stronger if the proposal is described and motivated more formally.\n\n"}