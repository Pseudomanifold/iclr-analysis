{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposed a mechanism to freeze top layers after supervised pre-training, and re-initialize and retrain the bottom layers. For a model with n layers, when a separation index i is specified, the approach define layer 1~i as bottom layers and i+1~n as top layers. The proposed process enumerate all i from 1 to n-1, compute resulting validation errors respectively, and then pick the i with lowest validation error. The algorithm exhibited significant improvement on WSJ and some minor improvement on CHiME-4.\n\nThis work provides some new insight for training ASRs and the observations provide further data points for understanding the training behavior. The layer freezing trick however is relatively well-known, and thus leaving the novelty of the proposed idea to be limited at what layers they choose to freeze.\n\nIn algorithm 1 it describes the mechanism as having two loops while it really only needs one loop. The author mentioned they used a simplified version later in the text, and I\u2019ll suggest to update the algorithm block to make it clearer."}