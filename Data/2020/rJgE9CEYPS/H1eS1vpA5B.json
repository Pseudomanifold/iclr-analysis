{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors proposed a discriminability distillation learning (DDL) method for the group representation learning, such as action recognition recognition and face recognition. The main insight of DDL is to explicitly design the discrimiability using embedded class centroids on a proxy set, and show the discrimiability distribution w.r.t. the element space can be distilled by a light-weight auxiliary distillation network. The experimental results on the action recognition task and face recognition task show that the proposed method appears to be effective compared with some related methods. The detailed comments are listed as follows, \n\nThere are many grammar errors and typos in the current manuscript, such as \n-\tOur key insight is to explicitly design the discrimiability using embedded class centroids on a proxy set\u2026\n\nThe authors proposed DDL based on the principle of the intra-class distance and inter-class distance. How to avoid the imbalance of the dataset, namely some classes have the insufficient data?\n\nIn Eq4, the authors adopt a normalization method. How about the influence of the model if we ignore this normalization? Some ablation study whether or not the normalization is missing.\n\nThe implementation details of the proposed framework are unclear. For example, how to fine tune the network?\n\n"}