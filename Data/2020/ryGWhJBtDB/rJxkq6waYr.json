{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the properties of SGD as a function of batch size and learning rate. Authors argue that SGD has two regimes:  a noise dominated regime (small batch size) and curvature dominated regime (large batch size). Authors conduct through numerical experiments highlighting how learning rate changes as a function of batch size (initially linear growth and then saturates). The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size.\n\nThe two regime claim of the paper is not really novel. These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others). When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).\n\nThe interesting part in my opinion is the experiments on constant steps. Authors verify large batch size reduces test accuracy while improving train. I believe these experiments are novel and the results are interesting. Besides CIFAR 10, authors test this hypothesis in two other datasets while tuning the learning rate. On the other hand, contribution is somewhat incremental given observations made by related literature (Keskar et al and others).\n\nSome remarks:\n1) In Table 1, batch size 16k has effective LR of 32. However in Figure 1c SGD with momentum at batch size 8k uses an effective LR of 4. Can you explain this inconsistency i.e. why is there such a huge jump from 4 to 32 (in reality we expect the effective LR to stay constant in the curvature regime). I also understand that one is constant epoch and other is constant step. However 4 to 32 seems a bit inconsistent.\n\n2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)?\n\n3) Readability: Consider explaining what is meant by \"warm-up\", \"epoch budget\", \"step budget\" clearly and upfront.\n"}