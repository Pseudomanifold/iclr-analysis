{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "OVERALL:\nI think this paper is worth accepting.\nAll modern semi-supervised learning techniques use consistency regularization somehow,\nand this paper shows that you can get away with just using pseudo-labeling combined with some\nengineering to route around the main issue with pseudo labeling (which is apparently called confirmation bias,\nthough I hadn't heard that, and I don't like it as a name because it's confusing).\n\nNeither MixUp nor the idea of fixing some number of labeled elements in a minibatch is new,\nbut that's not the point - we thought one thing, and this paper suggests that\nwe were wrong about that thing - to me this is exactly the sort of paper it's good to have at conferences.\n\nI would change the framing slightly.\nYou're not showing that pseudo-labeling can be useful, because many techniques already incorporate a form of pseudo-labeling.\nInstead, you're showing you can get away without consistency regularization.\n\nA potential improvement:\nIf you add up this techique with some of the most recent SLL techniques based on consistency regularization somehow,\ndoes it do better, or are they both acting via the same mechanism?\n\nDETAILED COMMENTS:\n> , contrary to previous evidences on pseudo-labeling capabilities (Oliver et al., 2018),\nIt's not really contrary to the findings of that paper, since you've totally changed the\ntechnique compared to what's evaluated in that paper.\n\n> n (Berthelo et al., 2019) \nIt's Berthelot\n\n> and are the mechanisms proposed in Subsection 3.1\nDoesn't quite parse\n\n> Network predictions are, of course, sometimes incorrect.\nThis is a great line.\n\n> We use three image classification datasets...\nWhy not use SVHN, which is by now super standard for SSL papers?\n\n> , we add the 5K samples back to the training set for comparison\nwith the state-of-the-art in Subsection 4.4,\nThis is *allowed* from the perspective of reporting a valid test accuracy,\nbut if other papers don't do that, it kind of mucks up the comparison, no?\n\nFig 1 is nice, but why does the effect not seem to be symmetric about the\nblue and the red blobs?\n\n> architecture plays and important role\n\n\n> However, it is already interesting that...  and that future work should take this into account.\nThis sentence doesn't quite make sense\n\nRe table 4:\nI'm curious how e.g. MixMatch would fare w/ the 13-CNN network.\nI am surprised that the change from WRN -> 13-CNN matters so much.\n"}