{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nIn this paper, the authors uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposed an efficient spectrum visualization methods. The results obtained in the paper can shed some lights on the understanding of existing optimization algorithms (e.g., first order methods).\n\nWhile this paper is quite interesting, I kind of feel that it has limited value to the research community due to the following concerns.\n\n1)\tThe work is based on some assumptions, which is however not very reasonable.  Assumptions 1 and 2 are too strong \u2013 it is difficult to know and to guarantee that the elements of \\epsilon are i.i.d. Gaussian distributed, and it is not guaranteed that the true Hessian is of low rank. I feel these assumptions are key to the proof, but they make the impact of the work lowers on practical situations where we have no knowledge and guarantee on the true Hessian and its relationship with the empirical Hessian. Although the authors made some discussions on generalizing the assumptions, it is unclear from the limited discussions whether the same theoretical results could be obtained in the generalized setting.\n\n2)\tThe experimental verification of the theoretical results is not indirect and somehow not very convincing. Because the true Hessian is hard to obtain, the authors compared the empirical Hessian with respect to different sizes of the training data as a proxy. However, this is not convincing since we do not know whether the same results can still be observed if further increasing the data scale, and which is the trend with respect to the increasing scale. A better way would be to design a simulation experiments, in which we know the data distribution (and thus can compute the true Hessian). \n\n3)\t The experiments were only done on the CIFAR datasets \u2013 not at large scale and not for diverse applications. So it is hard to generalize the experimental results to the entire space of \u201cdeep learning\u201d (as indicated by the title). \n\n4)\tThe practical value of the work in not very clear to me. The authors only made limited discussions on this \u2013 it can be used to explain why first-order optimization algorithms work well in practice. However, this is what we already know. It would be much better if some practical guidelines can be derived which can improve (but not just explain) the optimization process of deep learning.\n"}