{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #643", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper uses a distillation method on the biaffine dependency parser to reduce the model parameters and parsing time. \n\nTwo obvious drawbacks:\n1.1 The method used is not novel. Knowledge distillation is absolutely not a new idea and has been widely used in NLP tasks. There is no comparison with existing work about distillation. It seems that this work directly borrows the methodology.\n\n1.2 Although speed of the parser can be increased by distilling, accuracy of the parser is injured too much that nearly 1 point on UAS compared with Biaffine baseline.\n\nModeling:\n2.1 For the technical part, this work only consider minimizing the loss between teacher and student distributions. In other words, it only takes advantage of \u201cwhat the teacher is capable of\u201d, just like in previous literatures. However, \u201cwhat the teacher is not capable of\u201d also matters. Maybe the authors can think about in this aspect, e.g., introducing a contrastive loss to penalize the wrong predictions of the teacher model, to improve the technical contribution.\n\n\nEvaluations:\n3.1 This paper only tests the distilling method on Biaffine parser which is not sufficient. It would be better to try other types of dependency parsers.\n\n3.2 The authors did not use FP32 or FP16 in the model training and inference stages. \n1. Train the model in full precision mode and inference in FP32 or FP16 mode,\n2. Training and inference in FP32 or FP16 mode.\nI expect to see such a baseline comparison.\n\n3.3 Compared to directly reducing the hidden layer dimension, there are other things such as reducing the number of hidden layers, reducing the size of the input embedding, maintaining or even expanding the hidden layer state (the size of input embedding is the main part of the model parameters), and layer parameter sharing. \nI expect to see the above mentioned empirical studies.\n"}