{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper applies model distillation to the Biaffine dependency parser (Dozat & Manning, 2016), showing that parsing speed and model size can be substantially improved without significant loss in performance. The paper is generally clearly written (though see comments below), and the results are generally convincing. On the other hand, there is not a lot of content, and not much novelty. Overall I am slightly positive towards this paper, mostly because of the important topic and the clean experimentation, in particular with multiple languages.\n\nComments:\n1. While the writing is fairly clear, the tables and graphs can be improved:\n- the upper part of table 1 is misleading. As the authors rightfully note, the speed is not comparable between different reported numbers due to architecture differences, etc. I would recommend showing only the two bottom parts, or reproduce some of the other papers on the same architecture. \n- The fonts are very small in all graphs, making them hard to understand.\n- Figure 3 is confusing (especially given the small font). It might be preferable to make the baseline parser the baseline numbers on which the proposed approach and the small model are compared against, instead of making the proposed method the baseline. \n\n2. I would appreciate more details about how the model was compressed to X%. The authors mention reducing the number of dimensions of each layer, but more details would be helpful.\n\n3. As the authors rightfully notice, their favorable results in Figure 2 are almost exclusively attributed to improved performance on Tamil. Adding standard deviations to Figure 2 would help appreciate the observed trends. Moreover, while experimenting with 8 languages is beyond standard practice in NLP, it seems adding more languages, in particular some with small datasets such as Tamil, would help better appreciate the nature of the proposed approach."}