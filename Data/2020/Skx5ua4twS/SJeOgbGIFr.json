{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to increase the efficiency of dependency parsers, in terms of both number of parameters and runtime speed, through knowledge distillation. More concretely, this paper distills the graph-based parser of Dozat and Manning (2017), into the same model architecture, albeit with fewer parameters than the original model. \n\nThe distillation objective is done locally on the distribution of the possible heads and corresponding arc labels for each token, using the standard distillation loss (i.e. minimising the KL divergence between the teacher's and the student's probability distributions). This distillation loss is then interpolated with the standard cross-entropy loss with respect to the gold labels. On a subset of the UD corpus, the approach leads to distilled parsers that are 1.21-2.26x as fast as the teacher model at test-time, with minimal performance loss (~1% in UAS/LAS decrease when using the fastest student model), and achieve slightly better accuracy than the teacher model when using the biggest student (80% of the teacher's parameters).\n\nOverall, despite the encouraging empirical findings and depth of analysis, I have several major concerns regarding the work, as listed below. Given these concerns, I am recommending a \"Weak Reject\" ahead of the authors' response.\n\n1. The paper fails to compare with, or even mention, highly relevant prior work on distilling dependency parsers (Kuncoro et al., 2016; Liu et al., 2018; full reference below). These two papers featured methods to extend the distillation objective to structured objects (trees in this case). In contrast, this paper is methodologically much more straightforward as it only performs distillation at the local word level, thus disregarding the global structural dependencies. \n\n2. Related to Point 1 above, the technical contributions of this paper are limited. The methods are very straightforward and are not particularly creative applications of knowledge distillation to dependency parsing, while the findings are also unsurprising. As knowledge distillation has been shown to work on various problems, it is not surprising that distillation also works for dependency parsing. The decoding speed-up is also relatively minor on GPU (1.21x, even when using the fastest, least accurate student model).\n\n3. Despite the reduction in the number of overall parameters, I am not entirely convinced of the \"greener\" aspect of the approach. The main reason is that training the distilled parser is a two-stage process, where one must first train the carbon-expensive teacher model, and then repeat the training process for the smaller student model. Naturally this two-stage training process results in more carbon footprint at training time, and training neural networks are known to be computationally expensive as they require multiple epochs on the same dataset. It is unclear whether the decreased carbon footprint by using the smaller model at decoding time offsets the increased carbon footprint at training time. In short, smaller number of parameters are not the whole story when it comes to carbon footprint.\n\nMinor questions and suggestions:\n1. I suggest using bold or underline to highlight the best and fastest parsers on Table 1, which would make the table easier to read.\n\n2. \"Bucilu et al., (2006)\" in Page 3 should be \"Bucila et al., (2006)\". The corresponding entry on the References should also be modified accordingly.\n\n3. Are the losses weighted equally in Eq. (3), or were there any interpolation factors used to balance the four loss terms?\n\n4. The Dozat and Manning entry on the reference should be in ICLR 2017 rather than ICLR 2016 (the ArXiv version was published in 2016).\n\nReferences:\n\nTimothy Dozat and Christopher D Manning. Deep biaffine attention for neural dependency parsing. In Proc. of ICLR 2017.\n\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, and Noah A Smith. Distilling an ensemble of greedy dependency parsers into one MST parser. In Proc. of EMNLP 2016.\n\nYijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin, and Ting Liu. Distilling knowledge for search-based structured prediction. In Proc. of ACL 2018.\n\n"}