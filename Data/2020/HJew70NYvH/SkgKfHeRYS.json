{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n# Summary\nThe paper proposes to use MCTS for fine-tuning a policy in continuous control tasks. Action selection is done by PUCT [42] and node expansion is done by sampling from the policy instead of trying all actions. Furthermore, the importance of good parallel implementation is highlighted. Results on MuJoCo tasks show a bit of a gain in performance but with quite high variance.\n\n# Decision\nThere are some concerns regarding the novelty of the proposed method. Furthermore, evaluations seem rather noisy to make a reliable judgement. Therefore, I currently refrain from recommending this paper for publication.\n\n# Discussion\n1) Being not an expert on MCTS, it nevertheless appears to me that it is a natural idea to extend it to continuous problems. A quick search reveals a number of papers, e.g.,\n\nhttps://ieeexplore.ieee.org/document/8401544\nhttps://tel.archives-ouvertes.fr/tel-00927252/document\nhttps://openreview.net/forum?id=SyiF5-23Z\nhttp://proceedings.mlr.press/v80/lee18b/lee18b.pdf\n\n=> Can you comment on what exactly is your contribution?\n\n2) Experiments\n    - I am not sure that showing the burn-in period of pure PPO learning in Fig. 2 is informative. Clearly, if one starts with a good policy, it can only get better from there. So, one can directly start with a PPO-pretrained policy.\n    - For the same reason, the switching ratio experiments in Fig. 3 seem superfluous.\n    - In MuJoCo experiments in Fig. 2, in the top-right two plots, there is quite high variance; in the bottom plots, on the other hand, the variance grows and shrinks. How can you explain such oscillating behavior of the variance among runs? How many trials were performed? The curves do not seem to be reliable.\n    - Evaluations in Figs. 4\u20135 are quite indecisive, except for Humanoid maybe. Perhaps, more representative environments could be chosen.\n    - The algorithm is not compared to any other approach. Are there no similar methods?\n"}