{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a hybrid approach that combines MCTS with policy optimization. The main idea is to use PO for policy improvement, then boostrapping for action selection. The proposed hybrid framework enables MCTS planning on continuous action problems. The method is demonstrated on continuous control tasks such as Humanoid with high-dimensional and continuous actions. Integrating planning into PO like this way is shown a large improvement over baseline methods.\n\nOverall, this paper pursues an interesting research problem. Integrating a planning ability to policy optimization would definitely be desired to achieve more data-efficiency. The idea proposed in this paper is quite straightforward. It simply proposed to use policy gradient algorithms to optimize the rolout policy and the action selection policy. In addition, some of the descriptions in the paper are unclear that makes it hard to understand. My main concerns are as follows.\n\n- The policy network is trained using a gradient-based approach. However, the original MCTS framework is expected to optimize a globally optimal policy. It would be more valuable if the authors pay some discussions for this limitation and demonstrate it in experiments. Probably if it is evaluated on a discrete domain first. \n\n- As said this hybrid approach not only aims to extend MCTS to continuous domains but could also be considered as a way of integrating a planning ability into policy optimization that helps data-efficiency. Therefore more related work and discussions, and drawing connections with planning embedding and model-based policy optimization are very helpful.\n\n- There are many technical details missing, hence making it difficult to understand: i) how a search tree can be built for continuous domains?, I do not see how the branching factor and the policy boostrapping are used to simplify a search tree construction for continuous problems; ii) how the description in section 3 is related to the proposed framework? iii) what is the meaning of the Number of actions per node, it is unclear which nodes? How the tree is constructed based on this number of nodes?\n\n- Experimental results are quite promising as expected. I wonder how TPO and PPO are compared in terms of the total computation time? "}