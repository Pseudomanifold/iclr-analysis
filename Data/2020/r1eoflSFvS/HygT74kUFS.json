{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tackles the problem of automatically finding the hyper-parameters of sparsity-inducing regularizers for network pruning. In particular, the authors focus on L^1 and L^2 regularizers. The method then optimizes the hyper-parameters by making use of the idea of cross-validation gradient: Computing the gradient of a validation loss w.r.t. the hyper-parameters. For the L^1 case, this requires deriving a differentiable version of the regularizer.\n\nOriginality:\n- As discussed in Section 2.2, the idea of gradient-based optimization of hyper-parameters has already been studied in the literature, with several existing methods relying on a cross-validation-based loss, such as Barrat & Sharma, 2018, and Luketina et al., 2016. Here, it appears from Section 3.4 that the authors essentially make use of the approximation introduced by Luketina et al., 2016. There is therefore little novelty on this aspect.\n- I acknowledge that, to the best of my knowledge, hyper-parameter optimization has not been studied in the context of network pruning. However, I do not consider this in itself as a sufficient contribution for ICLR.\n- This nonetheless required defining a differentiable version of the L^1 regularizer, as shown in Eq. 10. I must say that I was not aware of this way to encode the L^1 loss, although the authors do not seem to claim this as novelty. In fact, looking in the literature, I found that this seems to originate from the sparse coding literature. I would appreciate it if the authors could discuss the relevant literature on this topic, so as to justify that Eq. 10 indeed is equivalent to Eq. 1. Now, as this seems to be a standard approach, the novelty here also remains limited.\n\nExperiments:\n- The experiments show the good behavior of the proposed algorithm. However, I would be interested in seeing how far the final hyper-parameter values are from the initial ones, and how sensitive the results are to their initialization. Ultimately, if one needs to carefully initialize the hyper-parameters, the resulting algorithm might not be very useful. I would therefore also appreciate it if the authors provided the results obtained by keeping the initial hyper-parameters fixed.\n- For the ResNet-56 experiments in Table 1, the PP-1 and PP2 methods of Singh et al., 2019 seems to give similar accuracy but much higher compression rates as the proposed method. Can the authors discuss this? Why are the results of this method not reported for the other experiments?\n- Why are the FLOPs and Reduced values for the proposed method not reported in Table 1?\n\nClarity:\n- The general methodology is reasonably clearly explained.\n- However, the paper is poorly written, with many typos and grammar mistakes (e.g., Heisen instead of Hessian, weighted delay instead of weight decay,...)\n- Some of the mathematical terms are also not clearly defined: \n* Eq. 4 uses \\tilde{y}, but below the equation the authors used \\hat{y}\n* The beginning of Section 3 uses both \\tilde{W}^{l+} and W^{l+}, but both seem to indicate the same thing\n\nSummary:\nMy main concern about this paper is its novelty, as the method essentially seems to apply existing techniques in the context of network pruning. I also would like to have a better understanding of the influence of hyper-parameter initialization on the results."}