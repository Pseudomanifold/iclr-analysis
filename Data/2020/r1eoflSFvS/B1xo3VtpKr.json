{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: This paper presents a method for automatically tuning the L1 and L2 penalty terms of sparsity-inducing deep network training so as to maximize cross-validation performance.  The basic idea is to train a network with L1 and L2 regularization terms, and then use implicit differentiation to get the gradient of a cross-validation loss with respect to the corresponding lambda_1 and lambda_2 hyperparameters on these terms (since the L1 term is non-differentiable they use the standard optimization formulation to derive the gradients via implicit differentiation).  They show that the method \n\nComments: Overall I very much like the idea of using implicit differentiation of a cross-validation loss for automatic hyperparameter tuning, though this has been done in a very similar fashion by past work (e.g. in the cited Maclaurin et al., though see also e.g. paper).  The issue I have with this paper, though, is that neither the method nor results are sufficient to convince me that this approach provides either a theoretical or practical improvement in this case.  On the more mathematical/algorithmic side, the formulas for implicit differentiation only hold when W has reached an actual minima, which is almost certainly not the case in this setting.  Furthermore, the authors eventually just use an extremely crude approximation of the actual implicit derivative, replacing the Hessian with the identity, so it's unclear what their \"gradient\" term really even corresponds to here.\n\nOn the practical side, since the inner loop optimization needs to run a fair amount before the gradient can be computed, and since there are only two hyperparameters of interest (and it's not clear to me we even need the L2 term), it wasn't clear whether there was any real speed benefit to the proposed approach over simple grid search (and this speed benefit is really the only potential advantage of a gradient based approach over grid search).  No explicit timing results are presented in the paper, but the number of epochs used e.g. for CIFAR-100 are _well_ above what's needed for training a single model.\n\nFinally, in terms of the experiments, it's not at all clear to me the methods are outperforming the other approaches.  The authors bold their entries, but there is no metric in which they are better than all the others, and they could very well just indicate different points on the Pareto frontier.\n\nThere are also several typos in the paper, such as referring to the Hessian matrix as the \"Heisen\" and non-differentiability as \"non-derivability\".  "}