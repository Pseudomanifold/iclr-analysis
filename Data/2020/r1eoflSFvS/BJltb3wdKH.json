{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper addresses the issue of deep network compression. Both L1 and L2 norms on a validation set are used for regularisation to achieve sparsity and avoid overfitting. Auxiliary variables are used to address the issue of L1 term not differentiable. The proposed algorithm (together with a pruning step) has been evaluated on some deep networks with promising results. \n\nI recommend weak reject because of the following reasons:\n- The key contribution is the treatment of the L1 term. The idea seems not new [1]. But it could that using it for the network compression problem is new. Yet, the original contribution is limited.\n[1] A Fast Dual Projected Newton Method for 1-Regularized Least Squares (IJCAI\u201911)\n- While the values of \\lambda_1 and \\lambda_2 can be automatically learned, the starting epoch for pruning and the thresholding are needed to set manually and tuned.\n\nSome specific comments:\n\nSection 3.3:\nProvide some references and also explanation on the trick behind the use of the auxiliary variables W^{l+} and W^{l-} to improve a bit readability.\n\nFor the experimental results, PP-1 gives the best result for ResNet-50. Any comments?\n\nThe paper needs to be further polished. The following lists just some of those I spotted and there are more.\nSection 3.3\n\u201cw\u02dc is a vector which consisit of the elements of\u201d\n-> \n\u201cw\u02dc is a vector which *consists* of the elements of\u201d\n\nSection 3.4\nHeisen matrix -> Hessian matrix\n\n\u201cBecker & Lecun (1989) and LeCun et al. (1989) given the formula ..\u201d\n->\n\u201cBecker & Lecun (1989) and LeCun et al. (1989) *gave* the formula (equation?) ..\u201d\n\n\u201cand approximate it to a identity matrix\u201d\n->\n\u201cand approximate it to *an* identity matrix\u201d"}