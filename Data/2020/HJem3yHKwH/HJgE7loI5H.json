{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper suggests using ensemble of both full-precision and low-bits precision models to defense adversarial examples.\n\nFrom methodological point of view, this idea is quite straightforward and not novel, since there are already several works that applied ensemble methods to improve the robustness of NNs, including the Strauss et.al 2017 and (the following references are not included in the manuscript)\n\"Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong\nWarren He, James Wei, Xinyun Chen, Nicholas Carlini, Dawn Song\" \n\"Ensemble Adversarial Training: Attacks and Defenses\nFlorian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel\" .\n\"Improving Adversarial Robustness via Promoting Ensemble Diversity\nTianyu Pang, Kun Xu, Chao Du,  Ning Chen,  Jun Zhu \" ICML 2019\n\nThough these methods only considered combining full-precision models, the idea is the same in essence and let the low-bits networks involve into the ensemble is quite natural and straightforward. So I don't think the methodology contribution of this paper is enough for publication.\n\nWhen checking the empirical results, the compared baselines miss a very common-used and strong baseline PGD adversarial training. And also the performance of this ensemble is not significant. \n\nConsidering the weakness of the paper both in methodology development and empirical justification, this work does not merit publication from my point of view. "}