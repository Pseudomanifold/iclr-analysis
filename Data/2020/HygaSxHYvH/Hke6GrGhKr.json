{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents an application of masked language models for non-autoregressive machine translation. The presented work is closely related to cross-lingual representation learning models like XLM (Lample and Conneau 2019) where a masking function is applied to a source-target parallel sentence pair (dubbed TLM in their paper) and sequence-to-sequence variants where masking is applied only on the target side like Ghazvininejad et al 2019 and Lee et al 2018. The primary contribution of this work is the idea of concatenating source and target sentences into a single long sequence while masking only target side tokens, allowing a single transformer encoder to learn source-target interactions at every layer. At inference, this allows for different decoding strategies including autoregressive and non-autoregressive approaches by choosing different masking functions.\n\nI think closing the gap between autoregressive and non-autoregressive translation systems is extremely important and the problem being addressed is certainly important. The idea of concatenating source and target sentences and training a single transformer is sound - source-target interactions at every layer. However, I think this paper requires more work. 1) The technical and architectural contributions over Lample and Conneau 2019 and Ghazvininejad et al 2019 are fairly minimal 2) Ghazvininejad et al 2019\u2019s sequence-to-sequence model achieves better performance on WMT\u201916 En-Ro with half the number decoding revisions at test time 3) None of the decoding strategies are particularly novel or specific to the particular architecture, with the full unmasking variant having already been explored in Lee et al 2018 and the confidence-based variant in Ghazvininejad et al 2019.\n\nI have a few questions:\n\n1) Could one of the reasons for performance not being as good as Ghazvininejad et al be the absence of a distillation objective? Their improvements on WMT\u201914 En-De appear to be pretty substantial with distillation, especially when using fewer decoding revisions (not as significant on WMT\u201916 En-Ro however).\n\n2) Ghazvininejad et al report much better results with just 10 revisions, while you run twice the number, is this because of a difference in your confidence annealing parameters K(T) compared to theirs?\n\n3) In response to \u201cKaaliya Budhil\u201d, you say \u201cAlso, our formulation enables a broad generalization on decoding schemes including confidence-based (similar to [1]), left-to-right (or right-to-left) and many others, while [1] has only one decoding strategy\u201d - From what I understand, the decoding strategies you present can also be used by [1] ( Ghazvininejad et al) and also their confidence-based approach seems to perform the best.\n\n4) In the same response you also say \u201cNonetheless, we did not compare our performance to [1] since our MTM currently does not support a decoding with multiple target hypothesis lengths, while all results of [1] rely on such multiple-length decoding. This is not consistent and there might be a non-negligible gain by the multiple-length decoding, so we only compare with single-length decoding results of other related work for the sake of fairness.\u201d - Ghazvininejad et al report numbers with single length candidates as well (In Table 5 of their paper), you could compare with respect to those for a fair comparison.\n\nI would encourage the authors to continue working on this while experimenting with all of the bells and whistles of current non-autoregressive translation systems like multiple target length hypotheses and distillation using an autoregressive teacher."}