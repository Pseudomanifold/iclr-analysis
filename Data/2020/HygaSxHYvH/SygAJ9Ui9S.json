{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Overall, I personally really like the idea of masked translation model (MTM) with single transformer-encoder module and perform decoding in an iterative non-autoregressive refinement fashion. The source and target representations can interact with each other through a unified attention mechanism at every layer. The proposed MTM model can be seen as a unified framework where we can adjust the decoding strategies easily. \n\nHowever, I still have the following concerns about this submission:\n-- My first concern is the novelty of the proposed approach:\nThe idea of \u201citerative refinement based on mask prediction particularly in machine translation\u201d is not new. There are several approaches have been proposed (and even published) before this submission was made. For instance:\n\nGhazvininejad, Marjan, et al. \"Constant-time machine translation with conditional masked language models.\" arXiv preprint arXiv:1904.09324 (2019).\n\nGu, Jiatao, Changhan Wang, and Jake Zhao. \"Levenshtein Transformer.\" arXiv preprint arXiv:1905.11006 (2019).\n\nMansimov, Elman, Alex Wang, and Kyunghyun Cho. \"A Generalized Framework of Sequence   Generation with Application to Undirected Sequence Models.\" arXiv preprint arXiv:1905.12790 (2019).\n\n-- The training and inference choices are pretty similar. It seems that the biggest difference is that the proposed model uses a single module for both encoder and decoder. However, this particular model choice is also not new to the field where people already proposed to share part of the attention each layer.\n\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise coordination between encoder and decoder for neural machine translation. In Advances in Neural Information Processing Systems, pp. 7944\u20137954, 2018.\n\n-- Sharing the same model for both encoder and decoder should also bring several drawbacks. For example, it actually increase the computational time for every forward pass as we cannot pre-compute the encoder representation before hand. More importantly, the reason to merge the encoder and decoder into one model is not well justified. We cannot choose the particular architecture just because it is simpler. We also need to consider other real advantages such as quality, latency, memory usage, etc.\n\n-- My biggest concern is in the experiments.\nAs mentioned earlier, the particular choice of using a shared module for both encoder and decoder is not well justified by the experiments. It is essential to have a fair comparison between a encoder-decoder based model with mask prediction.\nThe paper only conducts experiments only on one direction of \u201cRomanian-English (Ro-En)\u201d which is not very enough to prove the author\u2019s claim. Prior work also used other datasets such as \u201cWMT En-De and IWSLT En-De\u201d.\n\nThe shown performance on Ro-En is also not convincing. Prior literatures (Ghazvininejad et, al. 2019; Gu et, al. 2019) show much higher BLEU score on Ro-En with similar iterative refinement process. Note that, the claiming of not using \u201cdistillation\u201d does not hold for Ro-En because this dataset is a bit easy and a refinement-based non-autoregressive model can easily match the autoregressive model\u2019s performance without distillation. However, for harder dataset such as WMT En-De it is not the case. It would be nice to have a fair comparison with prior works on En-De.\n\nAlso, the paper did not perform any speed/latency comparison using actual execution time on GPUs or CPUs. As commented earlier, MTM needs to recompute the encoder features for every refinement steps.\n\nMinor concerns:\n-- The paper\u2019s title is called \u201cmasked translation model\u201d, however, the model also predicts tokens even though the word is unmasked (depending on how we choose the decoding approach). It seems it is a bit misleading to use this title.\n\n-- How likely a wrong word that is unmasked can be corrected compared to mask-predict? \n\n-- Why the model is with 12 layers and 16 attention heads? It is not comparable with other previous work which utilized a norma, transformer-base parameters.\n"}