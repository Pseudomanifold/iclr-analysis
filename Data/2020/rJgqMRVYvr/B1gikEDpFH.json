{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes the notions of different privacy levels for different attack models, namely global and local meta-level and within-task level privacy for meta-learning. It proposes an algorithm for global within-task privacy. It provides privacy and utility guarantee of the proposed algorithm and experimental evaluations.\n\nThe proposed definitions make sense to me for the scenarios mentioned in the paper. The utility guarantee also seems interesting. I\u2019m a little concerned with the significance and novelty of the proposed algorithm (it seems like a direct application of a generic DPSGD algorithm) and the utility analysis. Maybe you can justify more on that part. I think the experimental evaluation can be made more complete, for example, you may consider:\n- a convex setting as was considered in the utility guarantee,\n- varying epsilon values. I think a utility vs. epsilon curve can better support your paper."}