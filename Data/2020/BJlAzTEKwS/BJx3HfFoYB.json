{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper proposes an ensemble method for reinforcement learning in which the policy updates are modulated with a loss which encourages diversity among all experienced policies. It is a combination of SAC, normalizing flow policies, and an approach to diversity considered by Hong et al. (2018). The work seems rather incremental and the experiments have some methodological flaws. Specifically the main results (Fig. 4) are based on a comparison between 4 different codebases which makes it impossible to make meaningful conclusions as pointed out e.g. by [1]. The authors mention that their work is built on the work of Hong et al. (2018) yet the comparisons do not seem to include it as a baseline. I'm also concerned about how exactly are environment steps counted: in Algorithm 1 on line 27, it seems that the fitness which is used for training is evaluated by interacting with the environment yet these interactions are not counted towards total_step.\n\nFurther comments: \n1. I don't agree with the first sentence in the abstract: there's nothing special about \"continuous control\" when it comes to deceptive rewards. Perhaps you should be more specific and refer to robotics-inspired environments or to \"commonly used Mujoco environments\".\n2. I don't know what \"non-convex continuous action spaces\" refers to. All the environments studied in the paper have action space [-1, 1]^n which is a convex set.\n3. I'm also not convinced that the existing environments have \"deceptive\" rewards--they were likely tuned so that learning on them is feasible.\n4. The discussion regarding exploration completely ignores well studied methods based on e.g. upper confidence bounds or Thompson sampling. Unlike diversity based approaches, these methods are theoretically motivated.\n5. In Section 2 you start with finite horizon MDPs but then present infinite horizon discounted set up.\n6. I think the point about having only a single critic would deserve more discussion: what policy is this Q-value of? Would having an independent critic for each agent help or make things worse?\n \n[1] Henderson et al. Deep Reinforcement Learning that Matters. (2017)"}