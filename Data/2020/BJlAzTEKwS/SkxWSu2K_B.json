{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "[Note to authors: I made profit of a small edit to add a new important comment, which starts with a star below]\n\nThis paper builds on SAC-NF, an extension of SAC with Normalizing Flows which is shown to improve exploration at a low cost (Mazoure et al., 2019). In this paper, the authors build a population of SAC-NF agents with different parameters for the NF, and use an attraction-repulsion approach to ensure diversity and performance of the population. The resulting combination is shown to be competitive to or better than state-of-the-art algorithms.\n\nThis is a nice paper with nice ideas, and the resulting ARAC algorithm shows state-of-the-art performance on difficult continuous control tasks. Thus I'm in favor of accepting it. However, it should be improved\u00a0before final acceptance.\n\nHere are a few random remarks:\n\nAbout related work, \"balancing the objective and diversity\" is also the central concern of Quality-Diversity (QD) algorithms (see e.g. Cully&Demiris for a survey).\n* Actually, in QD as well as Novelty Search (NS), the algorithms look for diversity in a so called \"behavior space\", which is also called \"outcome space in Goal Exploration Processes (GEPs), see e.g. \"O Sigaud, F Stulp (2108) Policy Search in Continuous Action Domains: an Overview, Neural Networks\", Section 4 for a unifying view. According to eq. (8) and (9), ARAC is looking for diversity in \"fitness space\", which in my opinion is weaker. I would be glad to see a comment on that.\n\nI had to have a look at (Rezende&Mohamed, 2015) and to read (Mazoure et al., 2019) to get the normalizing flow part. An effort could be made in the beginning of Section 2.3 to make the paper more self-contained.\n\nThe account of SAC corresponds to an early version of the algorithm. In the most recent one, the value function approximator has been removed (see \"Soft actor-critic algorithms and applications\").\n\nI had a hard time to figure out whether the \\beta of the NF had something to do with the \\beta_\\pi of the AR loss.\n\nFig. 4 is of much interest. I would be curious to see the performance of ARAC on Swimmer, as this benchmark has been shown to suffer from a deceptive gradient effect.\n\nReproducibility: \"See github.com\" => Can you be more specific ? :)\n\nThe caption of Fig 6 describes 3 things, but I can only see two curves.\n\nFig 7 seems to be a mere repetition of Fig 4. Late edit before submitting? ;)\n\nFig.8 suffers from a poor choice of colors: even magnifying a lot the pdf, I cannot tell which is TD3 and which is SAC.\n\nAgain, Fig.9 is the same as Fig. 2.\n\nIt would be nice to move Alg 1 to the main part if possible.\n\ntypos:\n\np3: without risk (of) loss of information", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}