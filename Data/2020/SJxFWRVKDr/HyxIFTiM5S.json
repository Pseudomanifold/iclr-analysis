{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors present an interesting idea: creating representations based on gradients\nwith respect to the weights to supplement information missing from the training dataset.\nThe idea if very well motivated and gets the reader excited.\nTheir explanation is clear from a high-level, however, the paper as a whole\nlacks rigorous justification. A lot of space is spent reviewing the role of gradients\nin training, what reconstruction loss is, and the basics of back-propagation --- however,\nmany of their own propositions (e.g. why constrain gradients?) are given without reasoning.\n\nThe paper could be made more clear by giving details and by changing the flow. For instance,\nSection 4 first gives a high-level explanation, then begins discussing specific experiments,\nand then returns to the method and provides some details. Additionally, it isn't until page 6\n(Section 5) that the authors introduce GradCon, one of the more promising ideas shared, this should\nbe a focal point in the paper and introduced early (and given more than two sentences for explanation).\nThe main proposal appears to be a modification of the loss function, but the paper may benefit from\ndiscussing implementation details (for example, during training vs. testing).\nFinally, the Figures (2 and 3 in particular) are not clear and need more explanation given in\nthe captions. Figure 3a and 3b tell an interesting story, but they're not easily digestable, nor is\nthe key take-away clear to the reader just by looking at the figure and caption.\n\nThe experimental results look reasonable and thorough, however the methods are sold on the\nidea of better representations for data missing from the training set, whereas the results\nare focused on anomaly detection. The method looks particularly promising at anomaly detection\ntasks --- the authors may have a more clear paper if they focus on this aspect.\nOverall, the paper presents a promising idea but it needs a more clear and rigorous presentation.\n"}