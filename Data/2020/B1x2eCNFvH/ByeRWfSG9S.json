{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper discusses a new strategy for deep semi-supervised learning that seems related to the deep label propagation method of Iscen et al. 2019, but is more scalable and has a different loss function. \n\nEach example is associated with a representation vector v_i and a label y_i. The authors' approach essentially works by alternating between two steps:\n\n(1) Representation learning: Updating the v_i (and other model parameters) where the loss is an addition of two terms: \n-standard supervised loss\n-term that encourages points with similar labels to have similar v_i\n\n(2) Label Propagation: Uses the representations v_i to compute nearest neighbors and propagate labels. The authors approach takes O(NM) where N is total number of points and M is number of labeled points and can be parallelized to O(NM/P).  This is in contrast to Iscen et al. 2019 which takes O(N^2). \n\nExperiments show that the authors' approach performs consistently better on ImageNet than existing approaches. With suboptimal preprocessing, it also performs comparable / slightly better than UDA (Xie et al. 2019) (The authors speculate it could do better with the preprocessing that UDA uses)\n\nI am not from this area but found the paper well written and easy to understand. \n"}