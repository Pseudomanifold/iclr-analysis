{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors proposed a new gradient-based architecture search method that tries to find more efficient alternatives starting from the pre-trained model. The approach is similar to DARTS (Liu et al., 2019) with a budget constraint, such as size and throughput. One major difference is to modify the update of architectural parameters, i.e., mixing weights of all candidate operations, to induce the sparsity rather than to keep the weighted sum of all possible operations. Another difference is that it starts from a well-defined architecture with pre-trained weights. It is simple to apply, but hard to tell. The recognized strengths and concerns are as follows.\n\n< Strengths >\n1. The proposed update method to induce the sparsity of the architecture during the search seems applicable to all gradient-based search methods, and more important for the budget-constrained search minimizing the discrepancy between the ensemble architectures during the search and the final architecture derived after the search.\n2. The setup to initialize with the well-trained model seems practically useful rather than starting the search with a random initial model.\n3. The proposed methods seem easy to apply.\n\n< Concerns & Questions >\n1. The algorithm does not seem efficient because it continues training iteratively while increasing the strength (\\lambda) of the L1 regularizer by the estimated cost until the budget constraint is met.\n2. No details to calculate the cost C_l0(\\alpha) for each architecture candidate (e.g., the throughput cost of the architecture with a 3x3 DS operation in layer 1).\n3. No number of operations (e.g., # FLOPs, # MACs) is reported. Since the throughput is strongly dependent on the underlying hardware, the number of operations also needs to be shown as a more general estimation of the model inference latency in various hardware devices.\n4. The models under comparison are out-dated. It needs to be compared with the latest models designed with computational efficiency in mind, e.g., EfficientNet (Tan et al. 2019a), MixNet (Tan et al. 2019b), MobileNet V3 (Howard et al., 2019).\n\n- Liu et al., DARTS: differentiable architecture search, ICLR 2019\n- Tan et al., EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML 2019a\n- Tan et al., MixConv: Mixed Depthwise Convolutional Kernels, arxiv:1907.09595, 2019b\n- Howard et al., Searching for MobileNetV3, arxiv:1905.02244, 2019"}