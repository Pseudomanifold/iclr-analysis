{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new architecture search method called \"DARC\" that utilizes a differentiable objective function. Since a naive formulation of architecture search is reduced to a combinatorial optimization which is not differentiable, the optimization requires much computational cost. To overcome this difficulty, this paper proposes a L1-norm relaxation and apply such relation in a layer-wise manner. The method shares a similar spirit with NAS, but the proposed model is more like \"model selection\" from a fixed candidates, and thus there is a Rademacher complexity guarantee. The effectiveness of DARC is justified by thorough numerical experiments.\n\nAlthough the idea is rather straight-forward, the effectiveness of the method is well supported by the thorough experiments. In particular, it works as a model compression method and shows a favorable performances compared with SOTA methods.\n\nThe pros and cons are summarized as follows.\nPros:\n- The proposed method is simple and rather easy to implement.\n- The numerical experiments show the proposed method gives favorable performances compared with the existing methods.\n\nCons:\n- The idea itself is rather straight-forward.\n- The theoretical analysis is instructive but its derivation does not require new techniques.\n- Compared with NAS, the proposed method should prepare a set of candidates which restrict the search space. This ensures generalization but limits its flexibility.\n\nMore comments:\n- I could not see how efficient the method is in terms of memory. It prepares several models in each layer, thus it requires large memories. Can it be performed on more large networks?\n- The setting of C_j affects the result. How did you set C_j in the experiments?\n\n"}