{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary\nModel validation curve typically aggregates accuracies of all labels. This paper investigates the fine-grained per-label model validation curve. It shows that the optimal epoch varies by label. The paper proposes a visualization method to detect if there is a disparity between the per-label curves and the summarized validation curve. It also proposes two methods to exploit per-label metrics into model evaluation and selection. The experiments use three datasets: CIFAR 100, Tiny ImageNet, PadChest.\n\nLimitations\nThe paper is very preliminary in nature. It does not compare with other related work. For example,  the task prioritization during training approach where tasks dynamically change priority or are regularized in some way. It will be good to see how the proposed approach compare with other related ones (three listed in related work) in the literature. \n\nOther approaches not mentioned in the paper can be more effective. For example, \nFocal Loss for Dense Object Detection, https://arxiv.org/abs/1708.02002\nmight automatically handle the problem to a large extent.\n\nHow much the problem is due to label imbalancing? If label imbalancing is one main problem, please first address it.\n\nThe proposed approach is not very interesting, brutal force and clustering. They are very straightforward.\n\nOverall, the quality is far below the bar of ICLR.\n\nComments not affect rating\n\nThe paper uses \"task\" which is not defined. It is confusing until much later in the paper that it just refers to \"class\".\n"}