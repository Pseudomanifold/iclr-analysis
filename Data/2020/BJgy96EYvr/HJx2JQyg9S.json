{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes methods for incentivizing exploration in multi-agent RL.  There are two approaches that are proposed, both framed as influence maximization (of either the state transitions or the decisions of the other agents).  The scaling to multiple agents is done via decomposing to pairwise interactions. This influence objective is the appended to the standard intrinsic motivation objective for single agent RL.\n\nThe proposed approaches are pretty elegant, and in a sense seem fundamental.  I'm not an expert in this particular area, so I don't know how novel these ideas are.  (See related work comments below.)\n\nThe empirical results seem quite strong, although (being a a non-expert), I can't tell if they're constructed to be good for the proposed approaches.  There isn't much discussion of limitations and/or experiments breaking the proposed approach.\n\nI found the related work discussion a bit incomplete.  Can the authors comment directly on related MARL work, such as Foerster et al., AAAI 2018?  What are the specific points of contrast?"}