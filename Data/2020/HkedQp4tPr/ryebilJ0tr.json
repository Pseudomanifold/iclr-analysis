{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a parallelized version of Scheduled Sampling, which is proposed initially to mitigate Exposure Bias. Exposure Bias occurs when sequence generation models are trained by teacher-forcing, where the t-th token, $y_t$, is forced to be correctly estimated given ground truths, $y_1, ..., y_{t-1}$. Scheduled Sampling randomly selects the previous sequence from generated one or ground truth. Since sequential Scheduled Sampling requires a long time to train, the authors propose a way to parallelize it without degrading the performance. Experimental results for dialog response generation, summarization, and image generation (as a grammar model on images) demonstrate that the proposed method is faster than the original Scheduled Sampling. Additionally, the performance gain is shown in comparison to baselines trained by teacher-forcing.\n\nMy first concern is that several papers are not cited, although they also address Exposure Bias:\n- Venkatraman et al., Improving multi-step prediction of learned time series models. AAAI, 2015.\n- Ranzato et al., Sequence Level Training with Recurrent Neural Networks. ICLR, 2016.\n- Zhang et al., Bridging the Gap between Training and Inference for Neural Machine Translation. ACL, 2019.\nThe second paper proposes MIXER, which is a method based on reinforcement learning. The training step can be performed without sequential sampling from the ground truth and predicted tokens. The third paper also randomly selects from a predicted sequence and the ground truth, but the selection is performed in a sentence-wise manner. \n\nRelated to the first concern, secondly, Scheduled Sampling itself is no longer the state-of-the-art method to solve Exposure Bias. It is unclear if the proposed method is competitive with the methods above. In addition, the empirical benefit in terms of training time seems to be small in comparison to them. The authors evaluated the proposed method using diverse tasks. Although it is good to see if Parallelized Scheduled Sampling is versatile, lack of comparisons to the existing methods except for the original Scheduled Sampling remains the superiority of Parallelized Scheduled Sampling unclear.\n\nFinally, the technical contribution is not so fascinating. Although parallelizing the original Scheduled Sampling is practically essential, the proposed solution seems to be a straightforward way. Theoretical arguments and proof also look obvious.\n\nNow I lean to reject this paper because of the concerns above. Since Exposure Bias is a fundamental problem for sequence generation problems as described in this paper, I would like the authors to revise their paper and submit it to another conference."}