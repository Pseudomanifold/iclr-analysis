{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a technique for scheduled sampling that can be parallelized. At a high level, the method is to do teacher forcing, followed by passes of sampling from the model. During each round, the output of each pass are mixed with some probability. The time complexity then scales with the number of passes as opposed to the length of the sequence. The authors show results on dialogue generation, image generation, and machine translation. The proposed method generally obtains performance on par with scheduled sampling, but is much faster. The paper is well-written and the method clearly explained. I am not super excited about the technical contribution of this work, so my score is for weak acceptance.\n\nSome feedback\n- It would be nice to put the task in the caption for Table 1\n- Figure 1 is underwhelming and doesn't really help illustrate how the method works, though its caption does.\n- Because the main selling point of this is efficiency, I would like to see how it compares to work on non-autoregressive sequence generation (e.g. https://arxiv.org/pdf/1902.01370.pdf), which should probably be in the related works as well."}