{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors demonstrate that, during training, there is a point during the early phase of training that leads stochastic gradient descent (SGD) to a point where the covariance of the gradients (K) has a lower spectral norm (smaller first eigenvalue) and improved conditioning in K and the Hessian of the training loss (H).\n\nThe authors experiments seem to verify that learning rate and batch size do play a part in the spectral norm of K and the conditioning of K. My one issue is that, while effects on K produced by higher learning rates are supposed to be \"good\", the authors do not directly relate this back to model performance. From my years of experience training neural networks, I have seen many scenarios in which higher learning rates result in worse performance, even after reducing the learning rate. Can this be related back to the author's claims? Under what conditions does a higher learning rate lead to these effects on K and H and will it always lead to better model performance?\n\n\nOther comments:\nIn definitions, it says that the eigenvalue of matrix A is \\lambda_A^i, however, later in the 4th part of the assumptions, the spectral norm of H is referred to as \\lambda_1^H. Is there a difference here? Typo?\n\nThe last part of the definitions where \\Phi(\\tau) is introduced should have a formal definition for \\Phi(\\tau) as \\Phi is initially does not take any parameter \\tau.\n\nIn section 4.1, \"further growth of \\lambda_K^1 K does not translate into an increase of \\lambda_K^1\" \\lambda_K^1 is repeated. Typo?"}