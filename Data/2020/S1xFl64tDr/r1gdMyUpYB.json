{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Abstract:\nIn this paper, the authors propose to hide information in phase of the input features. They proposed that if each layer of processing layer, sitting outside of the local unit, is phase preserving, then they can recover the phase back. They propose a modification to the most popular layers in DNN to satisfy that property.\n\nI think the general idea of the paper is interesting, but overall, the paper is very poorly written. It appears that it is written in a rush. \n\n*The abstract and introduction are poorly written. There are poorly written sentences in the text. Here are some examples:\n    + \"... We propose a generic method to revise a conventional neural network to boost the challenge of adversarially inferring about the input but still yields useful outputs. ....\"\n    + \"... given a transformed feature, an adversary can at best recover a number of features which contain at least another k \u2212 1 features which are different but cannot be distinguished from the real feature.  ...\" -- This is so central to the whole paper and it is not well-written. Personally, didn't understand this attack.\nPlease proofread your paper.\n\n*Why do you need both theta and b ?  It seems to me if \\| b \\| is comparable to \\| a \\|, it can destroy the information in original the a arbitrary bad, so it makes sense to keep the norm of b small. However, in the paper, it is suggested to use a random sample (I') and set b = g(I'). I don't see any ablation study in the paper. There is no free lunch, if you provide stronger identity preservation, there should be a compromise in the accuracy, and it seems to be the magnitude of the b is that compromise. \n\n* The whole idea of the GAN encoder is not well justified. What does it mean that the fake feature should contain information \"beyond\" a ? This very vague.\n\n* \"Inference attack 1 \" and \"Inference attack 4\" are the same; only the inference models used in each attack are different. I don't know why the author has separated them.   \n"}