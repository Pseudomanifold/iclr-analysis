{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors proposed \"observed mutual information\", by dividing estimated mutual information by the entropy of X. They claimed that the estimated mutual information has an error which increases as the entropy of X increases.\n\nHowever, the authors made a fatal error by misunderstanding the Asymptotic Equipartition Property (AEP), in the proof of Lemma 2. In information theory, AEP tells us that asymptotically, the cardinality of \"typical set\" is bounded by 2^{H(x) + \\epsilon}.  The typical set A(n)_\\epsilon with respect to p(x) is the set of sequences (x1,x2,...,xn) \u2208Xn with the property 2^{-n(H(X)+\\epsilon)} \u2264 p(x1,x2,...,xn) \u22642^{\u2212n(H(X)\u2212\\epsilon)} (See Section 3.1. in Thomas&Cover's book). However, the authors made a mistake by saying the cardinality of X is bounded by 2^{H(X) + \\epsilon}, which is completely wrong. \n\nThe wrong Lemma 2 showed that the error of mutual information estimation is proportional to the entropy H(x), which is the motivation for proposing observed mutual information. Therefore, the definition of observed mutual information is meaningless, let alone the remaining sections. The paper can be rejected without reading the remaining sections. "}