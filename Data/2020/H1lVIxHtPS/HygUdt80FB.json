{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a new method SARNet for multiagent communication which uses ideas from transformers as well as MAC to perform better than baselines. The model first extracts out important stuff from current information (broadcasted communication from other agents as well own private observation) and then extract important information from previous memories it has relevant to current step. The model is trained using centralized learning and decentralized execution approach. Then, paper empirically shows better results than baselines on OpenAI\u2019s multi-agent particle environment.\n\nThe paper is well written and the flow is easy to read. The SARNet is usually better in larger scale experiments but it is hard to say if it will generalize to other environments as all experiments are on multi-agent particle environment. Therefore, due to lack of clear positive signal from few experiments and issues mentioned below, I provide an initial rating of weak reject which I would be happy to increase once following concerns are resolved. \n\nFor a proper comparison, it would be good to compare the number of parameters as well as plots for rewards and convergence with respect to baselines. Are the parameters among the agents shared in any way? \n\nHow many runs were used for the final runs? From the paper, it seems like it is only one run which won\u2019t capture variance. [1] which came between CommNet and TarMAC is missing from baselines. Using [1] will allow SARNet to be extended to non-cooperative settings. Also, the setup optimizes global rewards, but [1] shows that individualizing the rewards helps in overall performance. It would be great to know if individualized rewards can help SARNet further.\n\nAn ablation that is missing from the paper is one where you only use memory unit and disable the attention mechanism. It would give a signal on how much memory helps. The paper is missing proper qualitative analysis on what agents communicate and if there are some sort of common signals there. Analysis like the ones in CommNet [2] paper would be helpful to understand what is being communicated. Surprisingly, in 4.4, MC-SARNet is performing better on #Captures \u2018N\u2019, any explanations on that. Do the titles of table in Table 3 need to be exchanged? \n\nAlso, how is the mean reward getting calculated in partially observable physical deception task? I didn\u2019t understand that which confused me about Table 2(b) and results for section 4.3. The gap between Average distance for CommNet and SARNet is huge for N=3, but SARNet performs better for N=6. It is hard to derive a conclusion based on just these two. What happens when N=9 is tested, does SARNet again beat the baselines. If it does, we can make a more solid claim about performance. Same goes for the rest of the tasks.Section 4.3 is only evaluated on one setting and Section 4.2 is only evaluated on N=3 and N=6.\n\nI find the metrics for the particles environment a little coarse which might be adding noise to the overall signal. Something like Traffic Junction provides clear signal on the improvements.\n\nTypos/Clarification:\n\n3.1 Through a structured reasoning\n3.1 the model interacts with memories -> Are memories shared in communication directly?\nPage 4: The key and value are broadcasted\nI might be missing something but are the dimensions of a_i^t and similar variables correct, it seems like they should R^{NxQ}\nTable 3(a) - Memory in predator-prey\n\n[1] Singh, Amanpreet, Tushar Jain, and Sainbayar Sukhbaatar. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018).\n[2] Sukhbaatar, Sainbayar, and Rob Fergus. \"Learning multiagent communication with backpropagation.\" In Advances in Neural Information Processing Systems, pp. 2244-2252. 2016."}