{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Parameter sharing (PS) is an important approach to speed up neural network search (NAS), which further allows the development of differential architecture search (e.g., DARTS) methods. However, PS also deteriorates the performance of learning models on the validation/testing set. \n\nThis paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super-net well. Finally, a search method is constructed for the supernet to find the desired architectures.\n\nOverall, the paper is too experimental. The method is an ensemble of existing approaches, i.e., every single component in the paper has been visited in the literature. Expect for experimental results, I do not see many general lessons we can learn from the paper. Finally, why the proposed method can be better than others is not well-explained and clarified. \n\nPlease see the questions below:\n\nQ1. Is NAS a method only for ImageNet? Can the method generalize to more applications/datasets?\n- While ImageNet is a good dataset for CV experiments, I think NAS should be a method for deriving architectures with certain requirements.\n- So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred.\n\nQ2. On motivation, could authors explain more about the difficulties of combining all these techniques? \n- Each method is brought from some other paper, what motivate authors to combine them together? What makes them believe this is possible?\n\nQ3. On presentation, could authors draw a figure of the search space in the main text and give an overall algorithm for Section \"3.2 COARSE-TO-FINE ARCHITECTURE SELECTION\". It is hard for a reader to see novelties there.\n\nQ4. \"We also use the swish activation (Ramachandran et al., 2017) and fixed AutoAugment V0 policy\" - are all other compared methods using swish activation and AutoAugment V0 policy?\n\nQ5. How about the search efficiency of the proposed method? Only the accuracy is reported in the paper.\n\nQ6. Could authors give STD (i.e., gray area to represent STD) in Figure 4, 5 and 7? Some curves are too close, I am not sure they are statistically different.\n\nQ7. How is the performance of the super-net? \n\nQ8. Could the authors add an ablation study on this point? -  \"The motivation is to improve all child models in our search space simultaneously, by pushing up both the performance lower bound (the smallest child model) and the performance upper bound (the biggest child model) across all child models.\" \n- It is important to avoid fine-tune\n- From the paper, I am not sure whether the problem is solved by changing the space or the proposed training method (See Q1)."}