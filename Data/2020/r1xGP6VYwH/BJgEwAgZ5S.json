{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method to optimistically initialize Q-values for unseen state actions for the case of Deep Q-Networks (DQNs) with high dimensional state representations, in order to step closer towards the optimistic initializations in the tabular Q-learning case which have proven guarantees of convergence. The paper shows that simple alternatives such as adding a bias to a DQN do not help as the generalization to novel states usually reduces the optimism of unvisited state-actions. Instead, a separate optimistic Q function is proposed that is an addition of two parts - a Deep Q-network which may be pessimistically initialized (in the worst case) and a term with pseudo-counts of state-actions that together form an optimistic estimate of novel state-actions. For the tabular case, the paper shows a convergence with guarantees similar to UCB-H (Jin et. al., 2018). This optimistic Q-function is used during action selection as well as bootstrapping in the n-step TD update.\n\nI vote for weak accept as this paper does a great job at demonstrating the motivation, simple examples and thorough comparisons for their proposed \u201cOPIQ\u201d model on simple environments such as the Randomized Chain (from Shyam et. al., 2019) and a 2D maze grid. While the experiments are in toy settings, the connection to UCB-H and the novel optimistic Q-function and it\u2019s training formulation make the contributions of this paper significant.\n\nHowever, my confidence on this rating is low as I have not gone through the theorem in the appendix and I may be wrong in judging the amount of empirical evidence required for the approach.\n\nWhile the paper does cover a lot of ground with important details and clear motivation, a lot of the desired experiments have been left to future work as mentioned in the paper. This, in addition to the experiments on just toy settings, is not sufficient to conclude that this approach may be applicable to actually high dimensional state spaces where pseudo counts do not work well. Ultimately, the proposed approach relies strongly on good pseudo-count estimates in high dimensional state spaces, which is still an open problem.\n"}