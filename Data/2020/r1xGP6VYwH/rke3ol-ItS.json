{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Optimistic Exploration even with a Pessimistic Initialisation\n================================================================\n\nThis paper presents an exploration algorithm based on \"optimism in the face of uncertainty\" via count-based bonus.\nThe authors observe that typical neural net initializations close to zero can be pessimistic, but show that augmenting a count-based bonus for acting and bootstrapping can overcome this.\nThe authors support their claim with an adaptation of a regret bound for the tabular case, and a series of didactic experiments with neural net models.\n\n\nThere are several things to like about this paper:\n- Exploration with generalization in Deep RL is a large outstanding problem, with few effective options and none really commonly used in the field beyond epsilon-greedy or Boltzmann.\n- This algorithm is reasonably well thought out, building on an established literature of exploration bonuses, but with a slightly different take on the structure of the bonus.\n- The paper is well structured, building from intuition to theory to toy examples in DQN setting.\n- Overall the algorithm appears to perform well against a wide variety of related variants (although that presentation is a little confusing / overwhelming).\n\n\nThere are several places the paper might be improved:\n- I don't think the authors make a clear enough case for why this method is *better* than the other optimistic bonus approaches listed... Yes there is a regret bound, but this is not as good as some other methods... Yes there are ablations... but they're not really clear about what the mechanism that makes this method better than others!\n- Although this algorithm is motivated by applications to *deep* RL, the key choice of the \"count\" (and thus the method for optimism bonus) is mostly sidestepped. It amounts to an essentially tabular bonus in the space of the hashing function... and it's not clear why this approach should work any better or worse than other similar approaches that the paper complains about. For example, if you used \"Randomized Prior Functions\" or \"Random Network Distillation\" with that same hashing functions you would likely end up with similar results?\n- The comparison to benchmark algorithms seems quite confusing and I'm not sure if it's really presented well. It might be good to focus on fewer comparisons at a time and push remaining less important ones to the appendix.\n- It would be great to get an evaluation of these algorithms on a standardized and open-source benchmark... and I think that bsuite could be a really good candidate for this paper https://github.com/deepmind/bsuite particularly the \"deep sea\" experiments.\n\n\nOverall I think this is a reasonable paper, and I expect it to improve during the review process.\nAt the moment I have to say that I don't think there is a clear enough case for why this method is preferable to other similar approaches, or enough insight into the pros and cons to accept."}