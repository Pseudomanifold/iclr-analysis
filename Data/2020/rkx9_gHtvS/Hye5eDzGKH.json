{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors show that a query classification system can be improved by providing additional information to the system. In particular, additional \u2018tags\u2019 are converted to question/answer pairs and the system can choose to \u2018ask\u2019 these questions to obtain the \u2018answers\u2019 before finalizing its decision. They use a deterministic function (based on information gain) to choose the next question, and train a reinforcement-learned controller to either ask another question or stop. The system is trained without actual interaction, relying on crowdsourced annotation tasks.\n\nThe authors apply their approach to two different domains: FAQ document suggestion and bird species identification, which seem to be well suited to test their approach. In both cases the authors use a heuristic approach to turn a collection of tags into questions for which annotators then provide answers.\n\nThe paper is generally well written and organized, and the few minor typos I noticed can be fixed with a spell-checker.\n\n\nI believe the paper is an interesting contribution and I\u2019m leaning towards acceptance.\n\n\nI think that the general question the authors are trying to answer is very interesting: can a system be improved by obtaining additional information from the user. The potential need to disambiguate the user query, and potential underspecification motivate this well.\n\nHowever, the way they set this up the task and created the data is limited though: the system can not dynamically identify an information need and try to fulfill it. It can only add knowledge about pre-defined \u2018tags\u2019 to the decision making process. For example, it is not clear to me if a similar improvement could be achieved by identifying a number of relevant tags beforehand and providing these in a single step. So while I am not entirely convinced that the annotation task is a close enough approximation of the real task the authors are trying to solve, I still think it is an interesting result. It might be an interesting additional baseline to train a classifier that has both the query and the relevant tags as input.\n\nIn addition, ideally I would have liked to see a more thorough analysis of the results. In particular, I would have liked to understand better how the additional information contributes to the classification. \n\nI would also like to better understand what value the policy controller provides over choosing a fixed number of questions. For example, how does the performance of the system differ for different numbers of fixed steps? What is the variance in the number of steps the controller chooses?\n\nMoreover, the description of the neural model is very short and could be expanded.\n\nIt would have been interesting to see an analysis of the kind of question the current approach chooses. In addition to that, other heuristics for choosing questions (e.g. increasing diversity) could have been evaluated in addition to information gain.\n\nLastly, the authors say that the code, data and experimental setup will be made public. Are there already concrete plans for doing this and when can it be expected?\n"}