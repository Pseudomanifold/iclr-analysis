{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to achieve generalization under appearance change by explicitly optimizing for invariance under the visual transformations present in the training data, by leveraging privileged information about which states should be treated identically. Results demonstrate improvements over both a separated RL+supervised version and the naive domain randomization approach.\n\nI have settled on a weak rejection, because although the results are reasonable, I feel that the approach is not particularly novel, requires significant access to privileged information, and is not sufficiently situated in the literature.\n\nThis is essentially a generalized visual place recognition problem, in which representations are sought that are invariant to visual changes in the environment such as texture and lighting. This is an extremely well-studied problem, and many important references are missing in the discussion of related work [1]. This problem has also been tackled with deep learning approaches [2,3,4] and so it is somewhat surprising that none of this work is referenced in a discussion of visual invariance.\n\nThe approach of using privileged information (image pairs that are the same place with different appearance) to train representations to be similar [3,4] is a very straightforward and not particularly novel auxiliary loss, and it is not surprising that it would improve generalization here. A more interesting and important direction is how to identify these points of invariance without supervised labels.\n\nThe results in Fig. 2 look very noisy and don't have error bars. Are these results statistically significant?\nThis paper could be improved with clearer experimental results showing some kind of monotonic improvement with the number of environments in Fig. 2, perhaps this could be achieved by running more experiments.\n\nThe paper would also be improved by considering unsupervised ways to provide the labeled pairs required for the method. For example, what about generating random transformations of the observations instead of using privileged information? The authors mention an adversarial approach; results from a technique like this would significantly improve my rating.\n\n[1] Lowry, Stephanie, et al. \"Visual place recognition: A survey.\" IEEE Transactions on Robotics 32.1 (2015): 1-19.\n[2] Chen, Zetao, et al. \"Deep learning features at scale for visual place recognition.\" 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017.\n[3] Arandjelovic, Relja, et al. \"NetVLAD: CNN architecture for weakly supervised place recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[4] Carlevaris-Bianco, Nicholas, and Ryan M. Eustice. \"Learning visual feature descriptors for dynamic lighting conditions.\" 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2014."}