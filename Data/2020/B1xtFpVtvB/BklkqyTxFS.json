{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Claims: They formalize generalization in a reinforcement learning setting by defining a distribution over POMDPs from which they can sample multiple POMDPs for training. The authors posit that using domain randomization causes overfitting to those domains, and propose an invariance regularization to add to the training objective that prevents this overfitting.\n\nDecision: Weak reject. While I believe the problem setting is important and the framework interesting to evaluate RL in a multi-POMDP or MDP setting and assuming a distribution over those multiple MDPs, the assumptions made in order to perform this invariance regularization are too strong. In Equation 1, in order to add the regularization penalty term, it requires that they have access to the transformation \\mathcal{T} or to observations that correspond across the MDPs. If the authors can explain why this is a reasonable assumption to make or explain how this assumption can be relaxed, I will consider changing my score. It is also unclear from the writing how this assumption is enforced. Is the initial state always the same across environments, so the agent is always performing the same actions to stay sync-ed across environments?\n\nCan you give some intuition as to why the agents trained on 100 and 500 environments are generalizing worse compared to ones that are trained on fewer environments? It seems one would expect the opposite to be true. "}