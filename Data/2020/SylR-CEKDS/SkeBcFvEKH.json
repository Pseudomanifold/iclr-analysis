{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper uses a deep neural network architecture (CNN + Transformer) to model logical translations of questions in the form of programs. The experimental setup uses the \"battleship\" game scenario, which is an interesting domain for questions because of the inherent partial observability present in the game. The paper is clearly written and well-presented.\n\nThe length of the submission is 9 pages in content. The call for papers states that \"Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.\" Given the major reservation I have regarding the current version (see below), and the need to apply a higher standard, it is hard for me to recommend acceptance in its current form.\n\nThe major reservation I have is that the paper currently sets up the reader to expect the data to contain real, human-generated questions. The abstract mentions \"human question asking\", talks about predicting \"which questions humans are likely to ask in unconstrained settings\"; on p.2 the paper states that \"the model can be trained from human demonstrations of good questions\"; and so on. Hence it came as a surprise to learn on p.6 that collecting human questions and translating them into programs is too laborious, so an automatic system was used to generate the questions instead. This isn't necessarily a killer for the paper, but it does require more justification, and more sign-posting early in the paper.\n\nOn p.3 there is some attempt at justifying the decision to use a simulator, which is that an existing paper has shown that it captures the full range of questions that people ask. I feel that this justification needs expanding.\n\nAnother issue with the paper is that it heavily builds on the existing work by Rothe et al., in fact so much so that it's difficult to fully appreciate the current paper without reading that existing work (which I haven't done, just for full disclosure). In fact, a summary of the current paper would be that it takes the existing work of Rothe et al., which is a rule-based question-generation system, and it \"neuralizes\" it by replacing the rules with a neural architecture.\n\nThat said, I think there is still a lot to commend in this work: the setting is an interesting one for question generation, the motivation for using programs is well-made, and the work appears technically sound.\n\nSome more minor comments\n--\n\nI wonder about the log-likelihood numbers on the synthetic questions. Doesn't this just show that a neural network can effectively reverse-engineer a synthetic grammar? I'm not sure how interesting that is. I also wonder about the decision to filter questions that score poorly according to the generative model used for training the network - doesn't this just add additional bias in favour of the model being evaluated?\n\nA similar comment applies to the decision to exclude ungrammatical questions from the uniqueness metric. I assume \"ungrammatical\" here means ungrammatical according to the pre-defined grammar. But that just makes it easier for the grammar-based system to perform well on this particular metric, no?\n\nI don't think EIG is defined anywhere in the paper.\n"}