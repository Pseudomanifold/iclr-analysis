{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors explore different ways to generate questions about the current state of a \u201cBattleship\u201d game. To do this, they introduce a neural network architecture with a convolutional encoder and a Transformer-based decoder and consider both supervised and reinforcement-learned training approaches. They evaluate the introduced methods in three different ways that demonstrate basic classification and generation abilities of the model.\n\nWhile the paper is generally well written and clear, I noticed quite a few grammar mistakes and typos. I expect these can be fixed in the final version.\n\n\nIn my opinion this work has several limitations and I don\u2019t think it is a strong enough contribution to be interesting to the ICLR audience. Thus I am leaning towards rejection.\n\n\nWhile the problem of question generation is quite interesting, this work is limited in several ways. Firstly the authors limit the domain of question answering severely by just considering questions about \u201cBattleship\u201d that can be expressed in a lisp-like language. They do not attempt to generate questions that are oriented towards a more complex goal (like winning the game), but instead use three tasks that are quite limited:\n\n- First they show the model works well on very simple toy tasks; in particular the tasks are 1. Identifying the colour with the least visible tiles, 2. Identifying the ship with a missing tile, and 3. Identifying the colour of the ship with both a missing tile and the least number of visible tiles. These are extremely simple classification problems and no real question has to be generated. While this is a good sanity check for the model, it is not an interesting result.\n- In a second task they try to show that the model can capture the distribution of human authored questions. However, they do not use human-authored questions but instead use importance sampling using a heuristic method from Rothe et al. My skepticism of this experiment comes from the fact that  the \u201cdecoderless model\u201d that does not take into account the board state performs similar to models that do (and even outperforms the model with LSTM decoder). I am not sure how meaningful this task is.\n- Lastly they show that using REINFORCE with a tweaked reward (a version of the energy function from Rothe et al.) produces new kinds of questions that have not seen before in the training data. This is not surprising and also not very meaningful without either an application or a thorough qualitative analysis of the generated questions.\n\nSo while extending the work of Rothe et al. with neural architectures seems like an interesting endeavour, the tasks the authors use in the paper are severely limited and do not demonstrate interesting behaviour.\n\n\nI believe this is not in scope for changes to this paper, but in general it could be interesting to evaluate the question generation in an end-to-end setup, similar to the reformulation framework used by Nogueira and Cho in \u201cTask-Oriented Query Reformulation with Reinforcement Learning\u201d and Buck et al. in \u201cAsk the Right Questions: Active Question Reformulation with Reinforcement Learning\u201d.\n\nNit: The colours chosen for the diagrams could be improved. In particular purple is quite hard to distinguish from the dark gray.\n"}