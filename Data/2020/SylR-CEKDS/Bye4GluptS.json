{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "[Summary] \nThis paper studies a very interesting problem - if machines can learn to ask the right questions to address or gain crucial information about tasks. I believe this is a very important yet under-explored problem. Specifically, this paper considers a setting where a neural network model is trained to ask questions by predicting a formal program. The network consists of a CNN encoder which encodes a partially observable state and a Transformer decoder that generates a program as a sequence of tokens. The experiments on a Battleship task show some promising results. \n\nSignificance: are the results significant? 3/5\nNovelty: are the problems or approaches novel? 3/5\nEvaluation: are claims well-supported by theoretical analysis or experimental results? 4/5\nClarity: is the paper well-organized and clearly written? 4.5/5\n\n[Strengths]\n\n*motivation*\nThe motivation for investigating machines' ability to generating questions to gain important information is convincing.\n\n*novelty*\nThe idea of utilizing learning models for generating questions and modeling human-generated questions intuitive and convincing. This paper presents an effective way to implement this idea.\n\n*technical contribution*\nLeveraging a context-free grammar (CFG) and reinforcement learning (RL) for question generation seems effective especially when it comes to generating unique and novel questions.\n\n*clarity*\nThe overall writing is clear and the authors utilize figures well to illustrate the ideas. Figure 1 illustrates the Battleship task and Figure 2 presents a clear overview of the proposed framework.\n\n*ablation study*\nAblation studies are comprehensive. The proposed framework consists of multiple components. The provided ablation studies (Table 2 and Table 4) help analyze the effectiveness of each of them.\n\n*experimental results*\n- The presentations of the results are clear. The conclusions are fairly convincing: \n- Experiment 1: learning to generate programmatic questions encourages learning rules for composition\n- Experiment 2: it is possible to capture the human-generated questions distribution as a conditioned language model.\n- Experiment 3: CFG+RL can produce more diverse and novel questions compared to supervised learning or RL.\n\n[Weaknesses]\n\n*novelty & contribution*\nOverall, I do not find enough novelty from any aspects while the overall effort of this paper is appreciated. \n- The problem is not entirely novel as [1-3] have explored asking questions with neural networks learning to generate programmatic questions.\n- The proposed framework leverages a CFG and learns with RL, while the former (program synthesis with a CFG) has been studied in [4-5] and the latter (program synthesis using RL) has been discussed in [4, 6-7].\n- Many setups considered in this paper have been explored in several neural program synthesis papers [8-12], which are neglected from this paper.\n- The conclusions presented in the experiment section are more or less expected (i.e. other works have presented similar results on slightly different problems).\n\n*oversell the motivation*\nI believe the authors oversell the key motivation (i.e. to enable learning agents to ask questions) a little bit. To be more specific, it would be more interesting if an agent is allowed to ask a question, take actions based on the answer, and then ask the next question. However, this paper investigates the case where a significant amount of human-generated questions are given, which could limit the learning in my opinion. Also, it is not clear to me how the setup evaluated in this paper can be extended to a setting that allows an agent to iteratively ask questions.\n\n*experiment setup*\nDo the models learning using RL get the reward based on just the ground truth questions (a sequence of tokens) or the execution results? \n\n*format*\nThe title & reference format looks wrong\n\n*reference*\n[1] Question Asking as Program Generation in NIPS 2017\n[2] Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding in NeurIPS 2018\n[3] The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision in ICLR 2019\n[4] Leveraging grammar and reinforcement learning for neural program synthesis in ICLR 2018\n[5] Learning a Meta-Solver for Syntax-Guided Program Synthesis in ICLR 2019\n[6] Neural Scene De-rendering in CVPR 2017\n[7] Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning\n[8] RobustFill: Neural Program Learning under Noisy I/O in ICML 2017\n[9] Execution-Guided Neural Program Synthesis in ICLR 2019\n[10] Neural Program Synthesis from Diverse Demonstration Videos\u2028 in ICML 2018\n[11] Learning to Describe Scenes with Programs in ICLR 2019\n[12] Learning to Infer and Execute 3D Shape Programs in ICLR 2019"}