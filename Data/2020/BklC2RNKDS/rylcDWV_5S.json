{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper focuses on verifying sequential properties of deep beural networks. Linear Temporal Logic (LTL) is a \nnatural way to express temporal properties, and has been extensively studied in the formal methods community. \nSignal temporal logic (STL) is a natural extension,  of LTL. STL specifications provide a rich set of formulations to encode intent for real valued signal over time. Formally proving STL formulae is intractable. But, it is possible to falsify such properties. This has been the main goal for various tools like Breach, and S-Taliro.\n\n\nPros :\nA  very  interesting avenue  explored in this paper, is using the syntax of\nSTL to formulate properties about multiple-MNIST, Safe RL and NLP applications.\nEven though the conversion from STL specifications to scalar valued function is a very well known technique.\n\nCons :\nIn my opinion, the paper lacks sufficient contributions in itself to be accepted at this conference. The idea of training\nfor robustness using intervals, has been well known for a while. The authors extend that to get conservative estimates of the level of satisfaction of the STL formula, and use that in the training process. Though training for robustness is an\ninteresting idea in itself, but the general opinion about using interval propagation to train networks is negative.\n\nOverall : Though the direction of this work is interesting but lacks sufficient technical novelty."}