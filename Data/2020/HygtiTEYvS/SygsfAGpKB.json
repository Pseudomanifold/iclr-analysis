{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a means to adapt to new state representations during reinforcement learning. The method works by learning a translation model that translates new state representations to old state representations. The authors evaluate the method on the MountainCar environment and show that the adaptation model is more efficient than training a new policy from scratch.\n\nMy concerns with this work are as follows:\n\n- I don't find the type of changes to state representations very useful (e.g. Section 6, velocity *=2, position /= 2) nor practical. Moreover, learning a translation model to recover the old representation seems easy given this type of simple perturbations. This perturbation is fundamentally different than those used to motivate the problem (e.g. \"a robot whose sensors break down\" or \"whose sensor outputs degrade\", in the introduction).\n- The authors only evaluate with one type of simple perturbation on one environment, hence I am skeptical regarding the generalizability of this work.\n- The premise of this work is to not store old transitions (Section 1, the paragraph \"the most simplistic idea is to...\"), however this model does store old transitions because it uses prioritized experience replay. In this case there is no argument against using this data to train the translation model.\n- A comparison that is missing from the paper is to fine-tune the existing model. I believe this is a more fair comparison in terms of sample-efficiency than training from scratch.\n\n\nOther comments:\n- For the title, to call the adaptation to slightly different state representations \"policy adaptation\" is a stretch.\n- There are a lot of tangential information in the introduction on things like sample efficiency and model-free vs. model-based RL. This is distracting.\n- The paper is excessively long."}