{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper discusses the inductive biases in generative models that tend to assign higher likelihoods to \"less complex\" images. In particular, a likelihood based generative model (like Glow or PixelCNN++) trained on a particular dataset has significantly higher likelihood for data that have lower compression ratios (e.g  with PNG). The authors propose a simple approach based on the likelihood ratio between a trained model and a \"prior\" model (based on existing compression methods) and demonstrate that this improves unsupervised OOD detection on certain dataset pairs. The idea is quite simple (and surprising it seems to work!), but it seems that better understanding of the proposed method could be achieved.\n\nQuestions:\n\nIf our goal is to perform OOD detection, then higher likelihood for test samples might not be an issue, as we can simply declare samples with higher and lower likelihoods as OOD?\n\nIt would seem that using L(x) might already distinguish some OOD samples from Figure 4? What are the AUC of OOD detection if we had simply used these? \n\nFrom Figure 4, the x axis is p(x|M) and is between zero and one, while L(x) should be -log p(x|M_0). Why would we expect to observe a highly linear correlation between the two (as opposed to linear with p(x|M) and exp(L(x)))? Visually it seems that a lot of the datasets have similar distributions in both cases -- wonder why the proposed S statistic improves OOD empirically? Does this still hold for continuous likelihood models like Glow?\n\nThe performance seems to depend on the \"prior\" chosen (e.g. FLIF seems to have much better performance than PNG); any insights why this is the case?\n\nTable 1: I suppose a reasonable comparison for AUROC is max(current, 1 - current)? If AUROC is very small, we can still obtain good classifiers with flipped predictions.\n\nIt seems strange that the black-and-white images and colored images are compared together (say I train on MNIST and test CIFAR10); it should be quite easy to detect even with existing approaches. I suppose it would be most interesting to see cases where existing approaches fail miserably (like CIFAR10 vs. CIFAR100/TinyImageNet); unfortunately the proposed methods does not seem to improve much for PixelCNN++.\n"}