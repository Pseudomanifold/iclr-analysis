{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper provides a method/metric for comparing sentence embedders, based on a nearest neighbor analysis. The method is straightforward: sample a sentence, embed the sentence and lots of sentences from a corpus, find the k nearest neighbors in the corpus to the sampled sentence, do the same for another embedder, calculate the overlap of the two sets of nearest neighbors.\n\nThe paper is commendably clear and easy to read.\n\nThe main problems I have with the paper are twofold: 1) it's not clear this is enough of a contribution for a top-tier ML conference; and 2) it's not clear what I do with the results.\n\nI think the idea of analysing embedders in this way is potentially interesting, but it feels like the paper needs more. This analysis could be a great section in another paper (e.g. one which proposes another embedding method); or perhaps it could be extended in some way, so that the current content only takes up 1/2 the space, and then there's 1/2 the paper showing how useful this analysis is, for e.g. building better embedders for particular tasks.\n\nThe abstract starts by talking about how embedders have been evaluated using various benchmarks, and hints at the idea that this new comparative approach could be an alternative. But the new method can't really be used for evaluation: I don't come away from the paper knowing whether embedding method A is better than method B, only that A is more like B than C.\n\nI think the problem with the paper as it stands is neatly summed up in the conclusion of the paper, which isn't a conclusion at all: it's just a mini-abstract. I'd like to know what readers should take away from the results, so that they can potentially build better embedders.\n\nI've given the paper a 1. rating only because I really don't think it's ready for a full ICLR paper, not because I think the method is uninteresting or useless. On the contrary, with some more work and thought about how the analysis could be used, this could be a potentially useful tool."}