{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Motivation: The authors propose the use of polynomials as the choice of activation function.\n\nClarity: The authors does not give a very clear introduction why using polynomial functions as activation functions is a good idea. Rather they say, \", we believe that the nonlinearity learned by the deep networks can provide more insight\non how they can be designed.\" a bit more intuition in the beginning of the introduction would make the paper a bit easier to read. \n\n\"We demonstrate the stability of polynomial of orders 2 to 9 by introducing scaling functions and initialization scheme\nthat approximates well known activation functions\"\n\nGenerally, a good technique is to motivate your design decisions before actually stating what those design decisions are. Atleast to me, it was not clear, why you need to introduce scaling functions and initialization scheme in the beginning of the introduction.\n\nScaling Functions: Authors note that the polynomials of order n can  explode, to circumvent this authors introduce scaling functions to dynamically scale the inputs.\n\n\"The purpose of our proposal is to explore newer nonlinearities by allowing the network to\nchoose an appropriate nonlinearity for each layer and eventually, design better activation functions\non the gathered intuitions\" \n\nHaving this somewhere in the introduction could help.\n\nIn general, the paper does not motivate well why use of polynomial activation functions is a good idea."}