{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors introduce learnable activation functions that are parameterized by polynomial functions. The authors learn the coefficients of the polynomials through backpropagation. In order to account for the large values that occur as the input goes to the extremes, the authors scale the output of the input to polynomial function by the maximum input value. The authors try a variety of initialization schemes for the polynomial functions and test their activation function on MNIST, CIFAR10, and CIFAR100. The results obtained show that their function is, at best, slightly better than ReLUs, however, they usually obtain about the same performance.\n\nThe authors do not obtain activation functions that consistently improve performance and do not offer any insight into learned activation functions. They show plots of what the learned polynomials look like, but there is no consistent pattern in their shape and the authors provide no explanation as to why the final shapes might be interesting in the first place.\n\nFurthermore, the paper is poorly written. For example:\n\nIn section 3, the authors use x_i to indicate an output of neuron i in a layer, however, x_i is bold. Is x_i a vector or a scalar? It is being used as input to a polynomial function, so it must be a scalar.\n\nIn section 4, \"On observation, Swish approximations are relatively more accurate when compared to ReLU and TanH.\" By what metric? Visual observation?\n\nIn section 4 they say \"we train each possible combinations (41^3 = 68921) of initialization for an epoch. We use same initialization for all the three activations.\" I am assuming 41 comes from a variety of coefficient values and \"for all three activations\" is actually all three coefficients of the polynomial.\n"}