{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes using parameterized polynomials as activation functions. A regularisation is proposed to stabilize training Experiments are performed and it is shown that polynomials perform well.\n--------------------------\nUnfortunately, I only had limited time reviewing this paper next to the other 6 I had to review and unfortunately i won't find more time before the submission deadline.\n--------------------------\nI am very conflicted with the premise of the paper. Using parameterized polynomial activation functions is hardly new and a very quick search revealed papers going back to the 90s. The paper does not seem to reference any of this prior work or the theoretical results that have been given (e.g. the unsurprising result that polynomial networks with finite depth are not universal approximators). The reason i don't mention a concrete reference is that I do not know which of the old references is the most important. \n\nThe paper does not seem to motivate why looking at parameterized polynomials should be important. In general, they are harder to use and have all kinds of degenerate cases. The paper proposes a way to bound the polynomial range of values and derivatives to alleviate the training problem, yet i miss why this should be important or better than any of the already developed solutions. Most polynomials learned seem to be quadratic or with very low higher order components, so it is not that the learned shapes are interesting.\n\nI did not have the time to assess the experimental results.\n\n\n"}