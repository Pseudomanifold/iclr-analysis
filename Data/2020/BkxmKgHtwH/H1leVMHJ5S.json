{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "With the recently growth in interest in adversarial examples for natural language, this paper takes an important step back in asking 'do the adversarial examples we generate actually respect invariances of meaning'. The authors first give a list of constraints which attacks against NLP systems might have to satisfy, and then evaluate 2 recently proposed attacks on NLP classifiers to examine whether they satisfy some of the proposed constraints. The authors show that the attacks actually make more errors of a type that are detectable by automatic grammar checkers, and that humans do not reliably confirm that the proposed attacks preserve the semantics of the original sentence. Further, humans achieve higher than chance performance when telling perturbed text from original text. Finally, the authors show that it is important to control how much tolerance is allowed when considering attack success.\n\nWhile I am very sympathetic to the aims of this paper, and I feel like it makes an important message (that there is no such thing as an imperceptible manipulation of text), I feel the execution of the paper is somewhat lacking, and the experiments need a lot of tightening up before the paper is ready to be accepted. Further, the writing of the paper is very imprecise at times which is an issue especially if this paper is setting out to standardise terminology. In addition, I feel like the authors are too willing to abandon hope of semi-automated measurement of constraint satisfaction. While it is true that human evaluation is the gold standard, many of the constraints the authors propose could be amenable to automation, and I feel like this deserves further discussion. For this reason, I feel like this paper needs one revision cycle before resubmission, but with tighter writing would be a good addition to the adversarial text literature. \n\nWe will start with the imprecise writing: this mainly concerns section 3:\n\nThe whole of section 3.1 uses the word 'morphology' in a very loose sense. The authors seem to mainly use 'morphology' as a synonym for 'surface distance', but in linguistics, morphology has a strict meaning; importantly, small surface changes to a word can dramatically change its morphological categories, or even its root word. To say, therefore, that small surface distances are 'morphology-preserving' is highly inaccurate.  I highly recommend not using the word 'morphology' for this kind of constraint. Further, the first sentence of this section: 'the attacker is willing to chance the semantics of the input as long as all changed sentences read the same to a human', is difficult to understand. By definition, if the semantics is noticeably different, the sentences no longer 'read the same' to a human. Perhaps the intended meaning is that humans are able to repair surface errors using context? \n\nEvaluating whether generated text fulfills some semantic specification is well-studied problem in NLG. The crafters of the message know exactly what semantic content they wish to convey. Depending on the form of the semantic representation, one can attempt to parse the output message back into a semantic representation, and compare this with the desired input semantic representation. Or, if the attacker has examples of output with the intended semantics, one can use well-beloved overlap based metrics like BLEU, ROUGE or METEOR to evaluate whether the generated text has the intended semantics. While all automatic metrics have their problems, they are used too widely to be ignored completely.\n\nThe non-suspicious constraint can also similarly be automatically measured. For instance, recent work on detecting computer-generated news articles have used large pretrained language models to discrminate between human-written and computer-generated articles [1], which is a technique which shows some promise. Therefore, claiming that 'evaluation of the non-suspicious constraint must be done by humans' seems wide of the mark.\n\nI feel the experimental section could do with a lot more examples. For instance, examples of each category of grammatical error for both models should be included, and examples of sentences which annotators judged to consistent have changed meaning. In addition, in Section 4.2, satisficing behaviour is well known in crowdsourcing surveys, and not including check questions to prevent this kind of behaviour can throw doubt on the conclusions that can be drawn. Also, one notes that when the intent of the question is reversed, the average human response is roughly 5-x, where x is the original human response. This, to me, shows that exactly how one asks the question is somewhat irrelevant. \n\n[1] Defending Against Neural Fake News, Zellers et al. 2019\n"}