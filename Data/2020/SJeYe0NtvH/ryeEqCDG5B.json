{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper targets on solving the dull and repetitive outputs in MLE training for neural text generation. The authors propose a new unlikelyhood training to avoid assigning much probability to frequent and repetitive words. The authors combine the proposed algorithms and beam search and state that the results improved over beam blocking and neculus decoding.  \n\nThe unlikelyhood training is to provide a set of negative candidates and minimize the probability of these tokens. This raises several practical issues: how to choose a reasonable $\\alpha$. \nThis set can be chosen as the previous tokens in the sequence. This is a reasonable choice, but the author does not state why the other choices are not working,e.g. Sharpening the distribution using temperature. A potential counter case is that there are similar words exists in the sequences, but the unlikely loss trends to distinguish these synonyms.  The other unlikelyhood training choice is called sequence-level set. However, it seems not sequence-level but just n-gram center.  A question would be why not chose the whole n-gram instead of just choosing the center of n-gram. Also, why a prefix is really needed is questionable. \n\n\nEq 8 seems wrong, why$i \\leq n \\leq j$\n\nTable 2 should have shown the original sequences on the repetition metrics to show it indeed make sense.ppl should be enough, acc seems redundant. It seems that unlikely training may be harmful to ppl, which is the common metric to evaluate generation quality. A better discussion should be made on this to explain why it performance or if ppl has some problem.\n\nTable 3 comparison may not be reasonable. As Nucleus sampling and beam blocking is not in training phase. This comparison is not really fair.\n"}