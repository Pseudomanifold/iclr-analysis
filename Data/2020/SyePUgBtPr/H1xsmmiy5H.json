{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "# Summary #\nThis paper works on essay scoring. Instead of treating it as a regression problem, the authors proposed to model it by pairwise comparison (i.e., who should be scored higher). They then introduced a way to infer the final score by comparing the test essay to multiple reference essays. The experiments result demonstrated the improvement against the baseline, especially when the training data is limited.\n\n# Strength #\nS1. The paper is in general well-written and can be easily followed. The ablation study is well-designed to show improvement.\nS2. The proposed idea seems to be novel for essay scoring and works well in both the limited and sufficient data cases, which may become the essential building block for future work.\n\n# Weakness or comments #\nW1. Beyond the issue of insufficient data, I feel that in general, essay scoring should not be treated as a regression problem. Indeed, the score may not follows a good metric: the difference between score 6 and 4 is not equal to score 4 and 2. This is similar to the problem of facial age estimation: the degree of appearance change between ages 1 and 3 is different from ages 61 and 63. The essay scoring problem might naturally be better modeled as an \"ordinal regression problem\" (or ranking) or classification problem, rather than regression. What the authors proposed can indeed be seen as ordinal regression by pairwise ranking, and the authors should thus discuss the literature. The followings are examples.\n\nZ. Cao et al., \"Learning to rank: from pairwise approach to listwise approach,\" ICML 2009\nL. Lin et al., \"Ordinal regression by extended binary classification,\" NIPS 2007\n\nW2. The related work can be much strengthened. The authors could discuss problems where learning pairwise ranking/similarity is beneficial, for example:\n\nF. Sung et al., \"Learning to Compare: Relation Network for Few-Shot Learning,\" CVPR 2018\nD. Parikh et al., \"Relative attributes,\" ICCV 2011\n\nThe final decision is made by aggregating the scores from references (based on the pairwise comparison or similarity), which is essentially a case of non-parametric models, which are known to work well in insufficient data. The authors thus may discuss techniques like support vector machines (regression), Parzen-window methods, etc. For example, Changpinyo et al. showed that with limited data, support vector regressions work much better than MLP for regression.\n\nChangpinyo et al., \"Predicting visual exemplars of unseen classes for zero-shot learning,\" ICCV 2017\n\nW3. The method is not learned end-to-end directly for score prediction. The authors should discuss about this.\n\nW4. [About testing/ evaluation] The authors do not include formulas for evaluation. Also, how many reference (known) essays the authors consider in making a decision for a test essay (e.g., Eq. (3)). Since there are multiple prompts, during testing, do the authors only select those known essays that are from the same prompt of the test essay to aggregate the score? Is this a fair setting comparing to other baselines (i.e., by knowing the prompt where the test essay is from)?\n\nW5. The explanation of Sect. 4.2 is not clear. How can a model overfit when using even more data? I think it might relate to imbalanced training, where there are more pairs with large score differences than those that have small score differences.\n\n# For rebuttal #\n1. Please discuss W1-W5.\n\n2. Besides using transfer learning (i.e., pre-train and fine-tune), the authors should compare to multi-task learning: i.e., directly learning the two objectives --- pairwise comparison and regression --- together.\n\n3. What is 10x in Table 3? Also, do the existing methods also use BERT for text embeddings? If not, the comparison might be unfair."}