{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method of automatic essay scoring (a document regression problem) by using pairwise comparisons. The paper proposes using siamese networks to compare the candidate essay with a set of anchor essays for every possible score interval to determine which score it is closest to.\n\nWhile the paper is well laid out I think there are some issues for the paper. The main issue is the limited novelty: using pairwise approaches to compare documents has an extensive history in document similarity problems and this can easily be used for score similarity. Other than using Bert pretrained embeddings and performing pretraining with the regression task (approaches which are known to work for text classification) there doesn't seem to be additional novelty in the paper. Important experimental details are also missing, such as how many anchors were used at test time, the sizes of the models,  the hyperparameters used and the loss used when training for regression.\n\nThere are also some peculiarities with the experimental setup. Why are only one layer of RNN/LSTMs explored? By softmax I assume the paper means a logistic layer or a 2-class softmax as their output layer for the pairwise model. The ablation results in table 5 also seem to imply the pretraining and other techniques have little impact on the model improvement which appears to mostly come from switching from regression to pairwise comparisons. Section 3.2.3 is commonly called pre-training rather than transfer learning. Section 4.3 is also incorrectly labelled few shot learning when instead it's an experiment with a reduced training set rather than truly giving only a few examples of each score notch. In the conclusion, the paper states that the pairwise model uses mutual information but that isn't what mutual information is usually taken to mean.\n\nFinally, there are numerous typos and grammatical issues in the paper.\n\nOther minor comments:\nP1: \"Computers also prevail\" -> \"Computers also prevail over\"\nP2: \"several problems remind\" -> \"several problems remain\"\nP2: \"power the of\" -> \"power of\"\nP3: \"same time semantically\" -> \"same time being semantically\"\nBelow eq3: \"in specific\" -> specifically\nEnd of P5: \"the the\"\nStart of section 4.2: \"It is not supervision\" -> Is is not supervision\nStart of section 4.4: \"from starch\" -> \"from start\""}