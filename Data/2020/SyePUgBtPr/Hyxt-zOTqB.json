{"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "What is the paper about ?\n*The paper proposes a new approach to Automatic Essay Scoring (AES) by pairwise comparison of essays using Siamese Network.\n*The method claims to mitigate the data requirement of existing deep learning based techniques for AES.\n\nWhat I like about this paper ?\n*Few shot results and ablation studies adds weight to the paper and makes the approach attractive.\n*Section 4.5.1 explicitly states the benefits over regression.\n\nWhat needs improvement ?\n*While application to AES is novel, the method itself is not very novel and to me seems like a derivative of BERTScore [1].\n*I am not very familiar with AES in general but I think that related work section needs improvement and the authors have missed out on few works that I found are closely related to their work. One such example is [2]. \n*Figure 1 doesn\u2019t explain the method very clearly and can be improved. What does text embeddings in the blue box mean ? Is that sentence level representation ? Is there more than one reference essay ? While it can be understood by reading the paper, all this is not clear from the diagram and defeats the purpose of having one.\n*Table 1 doesn\u2019t highlight which methods are actually proposed in the paper. The numbers in the table corresponding to other approaches are they taken from a paper or are from authors\u2019 own implementation of that method ?\n*Overall readability of the paper is low and has room for vast improvements. It is difficult to follow at times, especially the \u201cScoring\u201d section 3.3. The writing quality can also be improved. Lots of grammatical errors spread out across the paper.\n*What appears to be a simple method in the beginning turns out to be quite complicated approach considering the different dropping rates for training data and lots of other strategies to make it work. Improvement over existing methods like Skipflow or 10\u00d7(CNN+LSTM) is also not very significant.\n*Methods like Skipflow are missing from Table 4. How do those perform in low resource setting ?\n\nNits\n*Section 4.2 Line 2 It is not \u201csurprising\u201d.\n\nQuestions for Authors ?\n*Did the authors use BERTScore as an inspiration or was this a concurrent work ? Since its relevant to this work, can BERTScore be cited in the paper ?\n*Section 3.2.2 From my understanding the training data is generated in such a way that one essay is always better than the other as same score essays are not paired. In this case won\u2019t the model be biased to always predict a given essay as better or worse than the reference essay even though the quality is actually the same ? Is this taken care of by penalizing the distance of $p_i$ from 0.5 ?\n*Section 3.2.2 mentions \u201cTherefore, it is foreseeable that requiring RefNet to categorize identically rated essays will frustrate the model during training and impair the performance.\u201d Did you see this in practice ?\n*In Table 4, Was RefNet pre-trained ? If yes did you only use 10% and 25% of the data for pre-training ? \n\n[1] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text Generation with BERT. arXiv preprint arXiv:1904.09675.\n[2] Ronan Cummins, Meng Zhang, and Ted Briscoe. 2016. Constrained multi-task learning for automated essay scoring. Association for Computational Linguistics.\n"}