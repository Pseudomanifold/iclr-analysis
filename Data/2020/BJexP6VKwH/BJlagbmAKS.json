{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\n\nThis paper proposes domain adverarial approaches modified to address covariate shift \neven in the case of shifting label distributions. The paper is interesting and the authors\nare looking at an important problem, but the paper suffers several major misconceptions\nand the exposition is full of errors.  \n\nThe paper proposes a method called \u201cself training\u201d to \u201cestimate and align\u201d the  target label distribution\nand a \u201cprototype-based method\u201d for conditional alignment.\n\nThe paper purports to introduce a new problem: \u201cgeneralized domain adaptation\u201d\nbut actually this is just covariate shift. \nThey confuse the term \u201clabel shift\u201d and the colloquial \u201cshifting label distribution\u201d.\nMoreover when they describe the method formally they fail \nto even state the covariate shift assumption (that p(y|x) = q(y|x) ).\nAbsent this statement, the problem is unfdamentally underspecified.\nThey have stated only that p(y) \\neq q(y) and that p(x|y) \\neq q(x|y).\nThus the claimed contribution is to solve an impossible (more accurately, underspecified problem).\n\nThe proposed method leverages a \u201csimilarity classifier\u201d\nThe thing that the authors call a similarity-based classifier\nisn\u2019t well explained. It looks like an ad-hoc variation on a standard\nsoftmax prediction layer. \nHere, not that similarity means similarity between x and the label weights w,\nnot similarity between examples and each other.\nMoreover this classifier is trained as a classifier using a \nstandard cross-entropy objective on the target domain.\nThe paper lacks any formal justification for what this is offering\nthat we do not get from a standard classifier.\n\nThe next component of the model is \"Prototype-based Conditional Alignment by Minimax Entropy\u201d\nand it is also confusingly explained.\n\nThe paper attempts to make some reference to the theory of Shai Ben David\nwhich has been badly misapplied in the deep domain papers\n(see discussion by Johannson et al 2019 and Wu et al 2019).\nStrangely, the authors misattribute the theory to Zhao 2019.\n\n\nThere are some nice ideas in the paper and the experimental results \n(flaws in domain adaptation benchmarks notwithstanding)\nappear to be promising.\n\nHowever the paper is written too confusingly, is outright wrong in many places\nand runs the risk of badly misleading readers over even the most basic of definitions.\nI encourage the authors to give the paper a gut rewrite\nand do not believe that it can be published while resembling its current form.\n\n\u201conly aligns the covariate shift\u201d\n>>> \tBe more formal, not clear what it means to \u201calign the shift\u201d\n\tMoreover, note that owing to lack of shared support , it\u2019s not clear \n\twhat precisely the current methods (domain-adversarial) do\n\tor according to what principles they work.\n\n\n\u201cthe covariate shift needs to be minimized\u201d\n>>> \tThis is not a coherent way of describing the problem.\n\tThe covariate shift is a property of the data.\n\tYou cannot \u201cminimize the shift\u201d\n\n\u201clabel shift (p(y) \u0338= q(y))\u201d\n>>> \tActually this is a \u201cshfit in label distribution\u201d. \n\tNote that you can have a shift in label distribution \n\teven under the covariate shift assumption.\n\tLabel shift is the reverse assumption that p(x|y) = q(x|y).\n\n\n\u201cminimizing the label shift\u201d\n>>>\tAgain, this doesn\u2019t make sense. The practitioner doesn\u2019t get to choose the data\n\tthat they will face at test time. The \u201cshift\u201d refers to the data. \n\n\n\"Specifically, we assume p(x|y) \u0338= q(x|y) and p(y) \u0338= q(y)\u201d\n>>>\tThe problem described in this paper is called Generalized Domain Adaptation\u201d\n\tbut actually it is just \u201ccovariate shift\u201d this is not a new problem. \n\tThe use of new terminology for old problems and misapplication of old terminology,\n\te.g. \u201clabel shift\u201d make this paper a potential danger to readers \n\twho then will be confused in their subsequent interactions with \n\tthe wider literature on distribution shift.\n\n\u201cThese methods have achieved state-of-the- art performance on several domain adaptation benchmarks\u201c\n>>>\tIt\u2019s worth pointing out that benchmark SOTA is a dubious way to assess performance out of sample.\n\tThe point is that in supervised learning you get to know that your target is the same as your source,\n\tso it\u2019s ok to have a whole community smash the validation set and then see if we push the leaderboard on test data\n\tWith domain adaption, the relevant sample size is the number of shifts, no the number of images.\n\tHaving the community pound on 2-3 shifts tells us virtually nothing.\n\n\n\"our approach diminishes covariate and label shift\u201d\n>>>\tAgain this is not the accurate way to describe what you do.\n\tYou attempt to salvage classifier performance under these shifts,\n\tyou cannot \u201cdiminish the shift\u201d. \n\n\u201cRecently, Azizzadenesheli et al. (2019b) propose a regularized algorithm to correct shifts in the label distribution by estimating the importance weights using labeled source data and unlabeled target data. Lipton et al. (2018b) introduce a test distribution estimator to detect and correct for label shift.\u201d\n>>>\tThis is not exactly the right characterization of the related work.\n\tThe method proposed by Lipton 2018 is precisely what \n\tAzzizadeneshelli 2019 build upon (by adding a regularizer) \n\n\u201cConventional domain adaptation approaches \u2026 only align marginal feature distribution\u201d\n>>>\tAgain, the authors speak about \u201caligning the marginal feature distribution\u201d,\n\tbut this is confusing, Representations are aligned, not features.\n\t\n\n\u201cThis motivates us to align the conditional feature distribution, i.e. p(x|y) and q(x|y)\u201d\n>>>\tAgain these are not things that can be \u201caligned\u201d. They are properties of the data.\n\tThe entire paper needs to be re-written to be semantically coherent."}