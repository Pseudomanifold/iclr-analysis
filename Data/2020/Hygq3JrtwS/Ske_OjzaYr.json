{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the connection between sensitivity and generalization where sensitivity is roughly defined as the variance of the output of the network when gaussian noise is added to the input data (generated from the same distribution as the training error).\n\nThe paper is well-written and the experiments are very comprehensive. There are however 3 major issues with the current approach:\n\n1- Novelty: Novak et al. 2018 suggests a very similar notation of sensitivity and they show correlation with generalization. Even though the authors site this work, they don't discuss the connection very clearly. In light of that work, there is very limited novelty in this paper.\n\n2- Definition of test loss: Authors define the test loss to be cross-entropy but in almost all these tasks, what we care about is the task-loss which is 0/1 classification error on the test data and not the cross-entropy loss. These two loss behave very differently. In particular, the cross-entropy loss is very sensitive to the of variance of the output while 0/1 classification loss does not depend on it. Therefore, it is not surprising that there is high correlation between the output variance and the cross-entropy loss but it is not clear if this has anything to do with the test error.\n\n3- Using test data in the complexity measure: The goal of understanding generalization is not just to get correlation with the test error. One can always use a validation set to get a very good correlation. Even when we have limited data, we can always put a small portion of the data for validation without loosing much in the final performance. The main goal is to predict generalization without using any access to the distribution. In particular, we need properties that show how networks behave on new data instead of simply measuring a property on the new data. Therefore, using a measure that is evaluated on new data is not really helpful."}