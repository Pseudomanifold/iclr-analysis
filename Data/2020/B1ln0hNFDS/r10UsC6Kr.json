{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a method to \u201cdiagnose\u201d  a pretrained neural network, which is able to roughly quantify neural activations corresponding to object parts without part annotation.  An autoencoder is trained to reconstruct the features of intermediate layers of the pretrained neural network, where two tracks are introduced: an interpretable track contains disentangled filters for parts and an ordinary track produces the residual information (i.e. part-irrelevant features). The outputs of both tracks are added with a learnable weight $p$ to reconstruct the features of the original network. $p$ is claimed to measure the radio of parts\u2019 information in the features. \n\nPros:\n\nThe paper is clearly written overall with helpful schematic illustrations\n\nThe work is a good attempt to quantitatively analyze the interpretablity of several common CNNs.\n\nCons:\n \n1) Lack of novelty: The autoencoder as the main contribution is greatly based on previous works about  interpretable neural networks.  For example, mask layer is proposed by (Zhang et al. 2018c) and the loss of filter interpretability Loss_f is proposed by  (Zhang et al. 2019). \n\n2) Lack of comparisons with previous methods: The claim that the proposed method has a higher discrimination power than interpretable models is not supported or verified by experiments. Since the work is highly relevant to (Zhang et al., 2018c) and (Zhang et al., 2019), it\u2019s disappointing not to show any comparison with them in the experiment section. Since the location instability and classification error, representing the interpretability and discrimination power respectively, can be calculated by both the proposed method and the interpretable model, I will expect the detailed comparison w.r.t these two terms.   \n\n3) $p$ is the ratio of information through the interpretable track, and thus I can consider p in the interpretable models is implicitly set to 1.  It would be more convincing to show the proposed method is superior to the interpretable model when $p$ is set to 1. \n\n4) Since there is a tradeoff between the model interpretability and classification error,  it would be better to plot a curve by tuning the corresponding hyper-parameters and provide some analyses. \n\nIn summary, although this paper studies an interesting problem of interpreting pretrained neural networks , the technical novelty is limited, and comparisons with existing methods and sufficient ablation studies are missing."}