{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces an interesting idea of exploiting the low-rank structure of the value function to reduce the computation complexity of value-based RL algorithms. Instead of working in the reduced space, they focus to operate on the original space and reduce the computation by looking at few elements and inferring the rest.  They use a matrix completion/estimation strategy to infer the global structure from a smaller set of samples. They show empirical evidence of the low-rank structure in few classical control tasks (Mountain Car, Inverted Pendulum, Cart Pole), and provide an iterative procedure - Structure Value-based Planning (SVP), that is similar to value iteration but is able to exploit the low-rank structure to reduce the computational time.  They also provide a deep RL extension - SV-RL, that can be applied to value-based methods. They test the efficiency of their approach to Atari games. \n\n\nOverall this paper presents an interesting idea, that also scales to the deep RL algorithms. However, there are a few missing components that need to be addressed in order to fully support the claims.  Given these clarifications in the author's response, I would be willing to increase the score.\n\n1) Nature of the regularization for SV-RL.\nThe authors are proposing a form of regularization that enforces low-rank structure for the value function (and target Q-values in particular for deep RL agents).  The authors show that this form of regularization is helpful for improving the learning for low-rank tasks, and for tasks that have a high-rank the performance is worse.  This kind of regularization balances having a low-rank and small reconstruction error. However, shouldn\u2019t the regularization also depend on the size of the sub-matrix (the minibatch)?  However, I didn\u2019t find any experiments related to how changing it affects performance. Also, is the regularization related to due to the random projections (Johnson\u2013Lindenstrauss lemma)?\n\n2) It is important to note that SV-RL is limited to Deep Q-learning based techniques. So it can\u2019t be applied to any value-based method, especially when the samples in the sub-matrix are correlated.  \n\n3) Missing literature that exploits Low-rank structure for planning. There is literature on RL that is based on exploiting the low-rank structure for planning [1, 2, 3].  \n\n4) All the experiments are in deterministic environments? Is there a reason behind this? \n \n5) (Optional) This regularization introduces error in reconstructed approximate Q-values. It will be useful to have some analysis on how far it deviates from the optimal value function. There has been work in the field [4, 5] that I believe can be used to help derive an analysis of the kind of approximation error bounds that are being introduced here.\n\n\n\nReferences: \n\n\n[1] Byron Boots, Sajid M Siddiqi, and Geoffrey J Gordon. Closing the learning-planning loop with predictive state representations. The International Journal of Robotics Research, 30(7):954\u2013966, 2011\n\n[2] Pierre-Luc Bacon, Borja Balle, and Doina Precup. Learning and planning with timing information in markov decision processes. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence , pp. 111\u2013120. AUAI Press, 2015.\n\n[3] Sylvie CW Ong, Yuri Grinberg, and Joelle Pineau. Goal-directed online learning of predictive models. In European Workshop on Reinforcement Learning, pp. 18\u201329. Springer, 2011.\n\n[4] Klopp, Olga. \"Noisy low-rank matrix completion with general sampling distribution.\" Bernoulli 20.1 (2014): 282-303.\n\n[5] Recht, Benjamin. \"A simpler approach to matrix completion.\" Journal of Machine Learning Research 12.Dec (2011): 3413-3430.\n\n\n\n"}