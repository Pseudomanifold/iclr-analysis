{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes a method to learn bilingual word embeddings by modifying the Sent2Vec (which is based on word2vec) approach, and applying it to bitext. They evaluate on monolingual and bilingual word similarity, bitext mining, and a zero-shot document classification task where a classifier is trained on data in one language but evaluated on another (using their embeddings as features in both cases).\n\nI have the following concerns about this paper:\n\n1) For the sentence mining tasks - how come Ormazabal 2019 is not included? The baselines are weak and the task is not standard. One of the baselines used here is MUSE which is unsupervised (The refined version of MUSE was also not included in these experiments). There are fixed datasets that people have experimented with (like BUCC) that would help tie in your results here with the literature a little better. These could include LASER (which is already compared to for document classification) and \"Simple and Effective Paraphrastic Similarity from Parallel Translations\" ACL 2019, which like this paper, proposes a pooled token embedding approach and outperforms more complicated architectures. Neither of these approaches uses idf either (how much does idf help - is it really needed?).\n\n2) For the zero-shot document classification task, it should also be pointed out that LASER can handle many languages all at once. Can this approach also work well if all languages were trained jointly?. Also did you evaluate on XNLI for zero-shot as well? LASER does very well here. I do realize comparing to LASER is not really fair since it is trained on so much data, however the model is similar to previous versions of LASER that were trained on Europarl, which could also be used as the data for training your models for a more apples-to-apples comparison.\n\n3) Note that also the improvements on monolingual similarity using parallel data are well known. For instance \"Embedding Word Similarity with Neural Machine Translation\" (arXiv 2014).. Also even the base for the current state of the art models on SimLex-999, Paragram (TACL 2015), used paraphrases created by pivoting on parallel data.\n\nI think the main contributions of this paper are modifying Sent2vec so it can be used on bilingual data and using it to learn nice representations for words and sentences (and documents). I think that to be published, it should clearly outperform and/or have advantages over all previous works - and this is not clear from this paper in its current form. It is okay if it doesn't do the best on everything, but it is hard to tell how this work currently fits into the literature especially in terms of the sentence-level tasks which are a focus."}