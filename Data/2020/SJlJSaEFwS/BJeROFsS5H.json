{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "* Recommendation\nWhile the contributions in this work are not staggeringly innovative, they are well grounded in existing work and well supported by experiments. Therefore I think the paper should be accepted.\n\n* Summarize paper\u2019s major contributions\n\nThe authors aimed to improve on the task of cross-lingual sentence retrieval, by introducing a model with a modified objective function, which utilizes a cross-lingual loss. They demonstrated that this objective function led to large improvements on word-level representation tasks and cross-lingual sentence retrieval, and achieves competitive performance on a document-level task while being more computationally efficient. The authors performed an in-depth ablation study, to support their claims that the proposed model addresses some of the key problems with other existing approaches to cross-lingual representation learning (e.g. hubness). \n\n* Comments on the paper\n\nThis paper is exceptionally well written, organized, and clear. In addition to a solid introduction and related works sections, which frame the problem nicely, the conducted experiments thoroughly demonstrate the performance of the proposed model, as they evaluate on several granularities (word, sentence, document), as well as a robust ablation study and analysis. By the end of the paper, I am sufficiently convinced by the work and its contributions. \n\nMeanwhile, I believe the paper could benefit from more discussion or analysis of cases where the proposed model did not lead to improvements, in particular with Russian in the word-translation retrieval experiment, where TRANSGRAM outperforms the proposed model. Although the authors briefly note this in the Discussion section, there is unfortunately no conversation about why this may be. The proposed model becomes less convincing when I consider that it might only work for other agglutinative, English-like languages, and I wonder how this approach would fair with other morphologically rich languages similar to Russian, and non-agglutinative languages in general. \n\n* Minor corrections:\n\n- In Figure-1\u2019s caption, at the very end, there is a space missing between \u201csentence_(cross-lingual compotent).\u201d\n\n- Some missing colon\u2019s (:) throughout the paper when breaking from a paragraph to introduce a list (e.g. \u201cContributions\u201d in the Introductions)\n\n- The x-axis of Figure 2 and Figure 3 are unclear to me, and a bit difficult to read. How do I interpret \u201c10^-2\u201d as a corpus size? In other words, what are \u201c10^-2 amounts of data.\u201d Fix this. \n\nOverall, great work. I also appreciate the details about training both in the paper and appendix, that will be useful for those wishing to reproduce this work. \n\n* Questions for authors\n\n- Why do you think that TRANSGRAM outperformed your system on the word-translation retrieval experiment for Russian? Do you have any reasons to believe that the proposed model cannot extend well to other morphologically rich languages, or languages very dissimilar to English? "}