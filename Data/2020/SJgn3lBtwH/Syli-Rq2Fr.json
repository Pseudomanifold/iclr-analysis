{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors investigate pitfalls common to random embedding-based approaches to high-dimensional Bayesian optimization (HDBO). Each of several practical shortcomings is separately analyzed and, subsequently, addressed in straightforward fashion:\n\n  a. Large-scale distortions caused by clipping are handled by generalizing box constraints\n     (in the embedded space) to the polytope corresponding to the set of points that project\n     to the interior of the original search space.\n\n  b. Local distortions caused by defining squared Euclidean distances in embedded spaces\n     are handled by substitution for Mahalanobis distances.\n\n  c. Embedded spaces potentially failing to contain optima are handled by constructing an\n     estimator for the probability of this happening, which is then be used to pick better\n     embeddings.\n\n\nFeedback:\n    To the extent that I enjoyed reading this piece, I am not sure that it warrants publication at this time. Specifically, the degree of novelty on offer seems minimal and the empirical results are underwhelming.\n\n  1) Constraints on embedded candidate $\\mathbf{y}$ are naively defined in terms of a polytope, but this formulation has previously been deemed impractical. If nothing else, it would be good to clarify this matter: why did preceding works chose not to explore this direction and/or how did you make it work here?\n\n  2) Regarding use of Mahalanobis distances, evidence here (as provided in A.2) seems thin. Firstly, predictive MSE (Figure 6) seems like an odd choice of metric; log marginal probabilities would seemingly be more natural for GPs. Reporting of MSE is particularly suspect when results shown in Figure 7 indicate that the Mahalanobis distance based GPs are (markedly) overconfident. This issue is allegedly improved by marginalizing Mahalanobis parameters $\\Gamma$; however, the appropriate baseline here would be ARD with marginalized lengthscales (which, to my knowledge, is not shown).\n\n  3) Regarding ALEBO itself, Algorithm 1 states that acquisition functions were expressed in terms of an approximate posterior formed via moment matching against the Gaussian mixture formed by $m$ different samples of hyperparameters $\\Gamma$?  One usually pushes this uncertainty through the acquisition function as, e.g., $\\mathbb{E}_{\\Gamma}[\\alpha(\\mathbf{x}; \\Gamma)]$. What motivated this design choice?\n\n\nQuestions:\n - Does $\\mathcal{B}$ need to be sampled or can it chosen to maximize $P_{opt}$?\n - Marginalization of hyperparameters\n    - Did you jointly marginalize over all hyperparamerters or just Mahalanobis parameters $\\Gamma$?\n    - Why was a Laplace approximation used lieu of, e.g., slice sampling?\n\nNitpicks, Spelling, & Grammar:\n  - Various figures: 'NewMethod' -> 'ALEBO'\n  - Please report log immediate regret along with error bars\n  - To the extent that testing on e.g. \"high-dimensional\" variants of Branin and Hartmann-6 is standard, it isn't particularly convincing."}