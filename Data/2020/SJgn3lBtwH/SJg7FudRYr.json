{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This is a well-written paper and I enjoyed reading it. In summary the paper tries to address the following shortcomings of REMBO:\n\n(1)\tREMBO uses a random embedding to project a point in high dimensional space to a lower dimensional embedding space basing on the relation f(x) = f(Ay) with high probability, where x in D dimensions and y in d dimensions and d<<D. The problem of REMBO is that the relation f(x) = f(Ay) is only guaranteed with high probability and so, the embedding space may not contain an optimum. Second, When a point y  where Ay is outside the search space X, REMBO uses a projection to map Ay to its nearest point in X. This projection is not enough good. These observation are identified in paper of Binois et al (2018)as well.\n(2)HESBO is an extension of REMBO that avoid above restrictions by proposing a new random projection. However, as the paper mentioned, HeSBO have a limitation is that the probability that the embedding will contain an optimum can be quite low! \n(3)Besides, the paper also identify a new observation that linear projections do not preserve product kernels. \n\nThen, the paper proposes a new solution of BO to overcome these restrictions by using a Mahalanobis kernel to avoid (3). This kernel is a replace of ARD Euclidean distance to a Mahalanobis distance. To avoid (1), the paper use equation 1 (please find in the paper). To avoid (2), they use the projection P_opt. In all , I think the theoretical contribution is good enough.\n\nHaving said that, I am a bit disappointed that this paper does not talk about LineBO (ICML 2019).  LineBO is a good solution for high dimensions without any assumption on structure like low effective dimensionality. It uses even one-dimensional subspaces to solve high dimensional problem with the strong theoretical guarantee. It do not need to learn subspace, and so it avoids disvantages (1), (2) and (3) that cause due to the fact that the embedding may not contain an optimum as mentioned above. Thus, LineBO is a stronger contender to the proposed algorithm. I will lift my rating if the author provide their response to this point.\n\nAdditionally, the author should compare their method to the algorithm of Binois et al (2018) that solved very well disadvantages of REMBO by setting bounds to avoid (1). Moreover, because the problem of the paper is high-dimensional Bayesian optimization under the assumption of low effective dimensionality, they should compare to other strong algorithms under the same assumption such as SI-BO algorithm( NIPS 2013) that used active learning to learn the low-dimensional subspace instead of using random embedding like REMBO.\n\n\n\n\n\n"}