{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper suggests a new technique to analyze the implicit regularization caused by ReLU activations. They bound the generalization error by two terms: 1) one term that represents the distance between the trained network output and a piecewise linear function built based on the set of training points and 2) another term that represents the distance between the piecewise linear approximation and the desired target. The first term is bounded using a random walk type of analysis, which to the best of my knowledge is novel.\nI find this technique rather interesting and technically sound, although I do have a number of concerns and I'm at the moment more on the reject side, although I will re-consider my score if the authors can provide satisfactory answers.\n\nGeneralization to more complex activation functions\nIf I understand correctly, the interpolation technique between two points only works for ReLU functions. If one were to try to generalize the analysis to more complex non-linear functions by using a more complex interpolation schemes, wouldn\u2019t you then have a random walk in high-dimensions? If so, wouldn\u2019t that be a problem given the different properties of Brownian motion in high-dimensions?\n\nGeneralization to smooth activation functions\nAnother question related to the previous one is whether one could hope to generalize the analysis to smooth activation functions. I believe this is also a drawback of combinatorial techniques such as Hanin and Rolnick which have to rely on the discrete nature of the breakpoints.\n\nGeneralization bound is only derived for 1-d functions\nTheorem 2 is derived for each dimension independently while the generalization results in Theorem 4 are for 1-dimensional inputs. Where is the difficulty in generalizing these results to higher dimensions?\n\nPrior work on generalization of SGD\nI was really expecting a discussion about how the generalization bound derived in this paper compares to prior work, e.g.\nHardt, Moritz, Benjamin Recht, and Yoram Singer. \"Train faster, generalize better: Stability of stochastic gradient descent.\" arXiv preprint arXiv:1509.01240 (2015).\nKuzborskij, Ilja, and Christoph H. Lampert. \"Data-dependent stability of stochastic gradient descent.\" arXiv preprint arXiv:1703.01678 (2017).\nBrutzkus, Alon, et al. \"Sgd learns over-parameterized networks that provably generalize on linearly separable data.\" arXiv preprint arXiv:1710.10174 (2017).\nAnd many others\u2026\nFor instance the bound derived in Hardt et al. is also of the order O(n^-2). The bounds in Kuzborskij are also data-dependent and so are yours since your generalization bound depends on the density of the training points. Can you comment on this? What specific insights do we gain your analysis?\n\nNoise SGD\nMy understanding is that the authors assume that the noise of SGD is Gaussian. Although this is commonly used when analyzing SGD, there is evidence that the noise is actually not Gaussian, see e.g.\nDaneshmand, Hadi, et al. \"Escaping saddles with stochastic gradients.\" arXiv preprint arXiv:1803.05999 (2018).\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\nI feel this is worth pointing out and one could perhaps also extend this analysis to Heavy-tail noise. I would expect that the results would still hold in expectation but perhaps with a slightly worse probability.\n\nInfluence step size SGD\nUsing larger step sizes in the SGD updates increase the influence of the noise. I was expecting this to somehow be captured in your analysis but I fail to see where it appears. Can you comment on this? \n\nProof Lemma 3\nThe derivation of Eq. 10 does not seem completely justified in the proof in the appendix. The authors essentially prove that the length of the gradient gap is bounded by |S| but why is the coefficient \\omega distributed according to a normal distribution. It seems to me that you need the noise of SGD to be Gaussian for such statement to hold. Can you confirm this? If so, I think this needs to be clearly stated as an assumption since -- as pointed out above -- this is not necessarily true in practice.\n\nMinor: proof Theorem 2\nIt seems rather trivial but for completeness, you should write the proof of Eq. (6) in Theorem 2. \n\n\u201cA priori estimates\u201d\nThis is a terminology that is often used in the paper but never defined. What do you mean by \u201ca priori\u201d in this context?\n\nMinor comment\nI would move footnote 3 directly in the main step. I think it is important to point out that the steps of the random walk correspond to the breakpoints.\n"}