{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review of \"A mechanism of ... deep learning\"\n\nThis paper studies the generalization performance and implicit regularization of deep learning. In particular, the authors propose a novel technique called \"random walk analysis\" to study the nonlinearity of the neural network with respect to the input data points. Moreover, the authors prove that for a class of 1-d continuously differentiable functions, SGD can achieve O(n^{-2}) generalization error bound.\n\nOverall this paper is well written and easy to follow. The linear approximation with respect to input parameter space is also interesting and seems to be useful in the generalization analysis. Besides, I have the following comments and concerns.\n\n1. I am a little bit confused by the definitions \"Priori generalization estimates\" and \"posterior data distribution\". I would like to see clearer description in the introduction.\n2. I would like to see more discussion on Theorem 2 in the surrounding text, it is quite unclear to me why Theorem 2 is important and how it can be related to the \"implicit regularization\".\n3. I do not see the proof of (6) in Section 3.2.\n4. It seems that the generalization results in this paper are difficult to be generalized to high-dimension regimes. For example, if you assume that each entry of the training data point is generated from the uniform distribution in the interval [0, v], the density of training sample would be \\mu = n/(v^d), and the resulting generalization bound would be O(v^d/(n\\delta)), which is extremely large.\n5. It seems that the generalization results hold for any data distribution. However, it is widely known that if the training data is randomly labeled, the neural network trained by SGD cannot achieve small population risk, which contradicts the result in Theorem 4.\n\n"}