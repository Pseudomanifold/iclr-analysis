{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary: This paper presents an empirical study of different exploration techniques for model-based RL (specifically, iLQR) on a simulated manipulation environment. The experiments show the following results:\ni) None of the exploration techniques outperform the basic MBRL loop without exploration.\nii) MBRL loop w/o exploration outperforms MBRL with ground-truth dynamics on a single task, due to induced exploration by the imperfect model. However, it performs worse on generalization to new targets.\n\nDecision: Reject\n\nMain reason for the decision: I think the paper contributes an insightful observation (that an imperfect model induces exploration & curriculum in model-based RL, and works better than other exploration techniques in their manipulation domain). I am largely concerned about the lack of variety in experimental domains. The main contribution of the paper is an empirical evaluation of different exploration techniques (noise in action space vs. transition space vs. environment model; and exploration during optimization vs. environment interaction). However, the paper only presents experimental results on a single domain (simulated manipulation environment with 7 DoF). It would be helpful to compare the different exploration algorithms on a more diverse set of domains with different action spaces, transition dynamics, and task complexity.\n\nClarification questions / additional feedback:\n\n---Problem Motivation---\n1) It would be helpful if the authors elaborated on why there is a relative lack of prior work on exploration for MBRL (compared to model-free RL). Is exploration in MBRL more difficult (and if so, why?)? Is exploration less important in domains where MBRL is preferred over model-free RL (e.g., domains where environment interactions are expensive)?\n\n---Figure Presentation---\n2) Fig 2a: What is \u201cee dist\u201d? Also, the three learning curve plots don\u2019t seem to add much useful information to the paper, so they could be moved to the Appendix.\n3) Fig 2b: Why was \u201cbest reward seen so far\u201d used instead of \u201caverage reward over trajectories\u201d? Is it a more useful or fair evaluation metric, and why?\n\nMinor comments on grammar & style that did not impact the score:\n\u201cbut a relatively unexplored topic for model-based methods\u201d  --> \u201cbut relatively unexplored for model-based methods\u201d or \u201cbut *is* a relatively unexplored topic\u201d\n\u201clike the ground-truth simulation\u201d  --> \u201clike ground-truth simulation\u201d\n"}