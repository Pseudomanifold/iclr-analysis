{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates the effect of adding noise to action selection and transition dynamics estimation in model-based reinforcement learning (MBRL) for improved exploration. The authors use the iterative Linear Quadratic Regulator (iLQR) algorithm with their methods for inducing noise and evaluate the performance on a robotic arm simulator.\n\nI think the paper explores an interesting problem which does not receive a lot of attention (exploration in MBRL). However, given that it is an empirical paper, I don't find the empirical evaluation thorough enough and some of the results found seem questionable. Further, the paper is completely focused on iLQR on the robotic arm task, so it's not at all clear whether the results carry through to other algorithms or other environments.\n\nSome more detailed comments:\n- Intro: \"combines two extremely data inefficient techniques\": are you referring to model-free versus model-based? they're not really \"combined\" in RL, and \"extremely data inefficient\" seems rather strong.\n\n- Intro: \"In model free methods, exploration is usually done by adding noise to the action suggested by the optimized policy.\": not really, most of the interesting exploratory algorithms use more sophisticated techniques like intrinsic rewards, pseudo-counts, and many even have theoretical guarantees (see all the PAC-MDP work).\n\n- Sec 2.1: There's a whole line of related work missing here. For example:\n  * \"Near-Optimal Reinforcement Learning in Polynomial Time\", Kearns & Singh, 2002\n  * \"R-max \u2013 A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\", Brafman & Tennenholtz, 2002\n  * \"PAC Optimal Exploration in Continuous Space Markov Decision Processes\", Pazis & Parr, 2013\n  * \"Unifying Count-Based Exploration and Intrinsic Motivation\", Bellemare et al., 2016\n  * \"Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment\", Taiga et al., 2019\n\n- Algorithm 3 line 6: Where does \\hat{\\pi} come from?\n\n- Section 3: \"aim of reinforcement learning is to find a stochastic policy\" it doesn't have to be stochastic\n\n- Section 4.1, Action-maximum-entropy: You use $Q^{-1}_{a_t, a_t}$, what does this mean? Why does Q have two a_t subscripts?\n\n- Section 4.2, Transition-fixed-covariance: \"we set $\\epsilon^2_i = 1.0$\": this seems quite high. is there a reason why? did you try other values?\n\n- Equation (4): Is this a standard cost function or is it only specific to this work? If the former, add reference, if the latter, explain the choice.\n\n- Section 5: It's not clear what you mean by \"inverse kinematics\"\n\n- Figure 1: It's not clear where the targets are. It might help if you draw a trajectory from the initial to target position on each, otherwise it's hard to tell them apart.\n\n- Figure 2 (a): What's the difference between the three plots?\n\n- Figure 2 (a): The labels on the legend don't match anything that was previously introduced, so it's not clear what's what.\n\n- Section 5: What's MPC?\n\n- Table 1: There are a number of different hyperparameter choices for the different exploration methods you evaluate. How did you pick them? Did you do a sweep? Are the results averaged over multiple runs? How many?\n\n- Section 5.2: What is \"finite-difference approximation\"?\n\n- Section 5.2: \"We run the same set of experiments as above, but only with inner-loop exploration\": why?\n\n- Table 2: The algorithm with no exploration does *much* worse when using the ground truth dynamics compared with using the model in table 1. This is totally counter-intuitive: shouldn't the results be strictly better given that there's zero model approximation error?\nI'm not sure I buy the justification you gave in section 5.3. There is a >2x degradation when using ground truth without exploration for the 0.10m target, which is significantly worse than the degradation for the harder 0.45m target. If exploration really is the cause, I would expect to see the decrease in performance correlate with the difficulty.\n\n- Section 5.3.1: This section seems somewhat unrelated to the main thesis of the paper. If you do want to investigate this problem, it calls for a much more thorough investigation. Right now the investigation provided is somewhat superficial, which detracts from the paper.\n\n- Section 5.3.1: \"Thus, we conclude that... for a particular target\": Is this really that surprising? Ground truth dynamics are agnostic to the target, whereas the learned models build their estimates from collected trajectories, which are gathered using the current best policy, which is aiming to minimize the cost to the specific target.\n\n\nOne minor comment:\n- Section 5: \"We generate targets three distances 0.1m, 0.45, 0.8m away from the initial...\" would read better as \"We use targets at three different distances from the initial end-effector: 0.1m, 0.45m, 0.8m\"."}