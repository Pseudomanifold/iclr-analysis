{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Exploring by Exploiting Bad Models in Model-Based Reinforcement Learning\n============================================================================\n\nThis paper investigates the importance of exploration in model-based reinforcement learning.\nIn particular, the authors suggest that the *implicit* exploration generated through randomly initialized dynamics models can be competitive with, or even outperform alternative methods.\nThe main support for this claim comes through improved performance on a 7-joint robotic arm.\n\n\nThere are several things to like about this paper:\n- Highlighting the (potential) importance of exploration in complex model-based RL is important, especially since it is known to be a bottleneck in the field that usually relies only on reward shaping.\n- The observations that \"random initializations\" can provide some prior effect and that this can implicitly drive exploration is interesting.\n- The resultant algorithm is quite sensible and it generally appears to perform well.\n- The overall writing is reasonably clear.\n\n\nHowever, there are some places where this paper falls short:\n- The discussion of past work on \"efficient exploration\" in \"model based\" RL is sorely lacking, and in many places it is even wrong. In fact, it seems like *most* of the research on efficient RL has been done in the model-based setting... e.g.  E3, R-max, UCRL2, PSRL and more... Now I think that what you mean is model-based plus generalization but even then there are bounds for PSRL https://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension and more!\n- The connections to PSRL here are particularly interesting due to the argument that \"randomly initialized\" neural networks can approximate the posterior distribution... actually I think that there is some precedent of that in RL / exploration too e.g. bootstrapped DQN.\n- Actually I think that the authors mean to restrict their focus to \"model-based Deep RL\" but I'm not exactly sure how they draw the line there. And if the focus is specifically on the scalable Deep RL method, then there are even some questions about how scalable the model-based versus model-free approaches are... and you should address why this is such a focus.\n- I'm not sure that these experiments are very targeted in terms of insight, it would be nice to see some experiments that specifically target \"efficient exploration\"... potentially something like bsuite could help with that https://github.com/deepmind/bsuite.\n- I don't find the tables an easy way to parse data, some plots or bar charts could help to make this more clear!\n\n\nOverall I think this has the potential for good work, but there really are a lot of loose claims regarding exploration and model-based RL.\nBased on this, and some issues I have with the clarity/insight of the support for the claims I will tend to reject."}