{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper provides an analysis of a cross-lingual data augmentation technique dubbed XLDA, which consists of replacing parts of an input text with its translation in another language. Building on the mBERT approach, the authors show that at fine-tuning time it is beneficial to augment the training set of XNLI with cross-lingual hypotheses and premises instead of in-language pairs. For each language in XNLI, they show results by augmenting with each of the 14 other languages in the dataset, and show significant improvements over per-language performance.\n\nThe paper explores an interesting idea and shows that cross-lingual data augmentation works well. However, their analysis is limited to the XNLI and the Squad dataset, which do not cover a suitable range of tasks to fully conclude on the importance of XLDA for generally improving NLU tasks. It would have been interesting to show the effect of cross-lingual data augmentation for other GLUE tasks by augmenting the datasets with machine translation for instance. And also compare this model on these tasks to the monolingual back-translation approach, similar to https://arxiv.org/abs/1904.12848 . Applying XLDA with the latest open-source XLM models from the cross-lingual language model pretraining paper which obtain higher performance than the multilingual BERT would also make the results more convincing. While I share the excitement of using cross-lingual models to improve monolingual performance, I also feel like this paper lacks novelty and further evaluation to be accepted at ICLR, and would be more suited in a more NLP-focused venue."}