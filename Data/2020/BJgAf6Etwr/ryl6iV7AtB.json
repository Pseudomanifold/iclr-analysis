{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary: This paper proposes to augment crosslingual data with heuristic swaps using aligned translations, somewhat like what bilingual humans do in code-switching. I think this paper investigates a neat extension of the XNLI dataset, which is in fact the sort of thing it was created to enable! It also looks at SQuaD translations (but, I'd have preferred a bit more depth on one of these datasets over having both, but I understand why you made this rhetorical choice). \n\nYour augmentation extension to XNLI also uncovers a bunch of surprising results, like that code-switched utterances help models do better than monolingual ones! My main issue, if i had to find one, is that the paper doesn't try to offer (even possible) explanations for the unexpected results; maybe try to find space for more of these in a discussion section? Finally, this paper is really fun and well written, thanks for the effort! I'm going to leave a bunch of questions: it would be cool to see some in the final, but if they don't fit, you can consider them for a follow up.\n\nQuestions: \n-Are all \"portions\" full sentences? Did performance change based on which \"portions\" you swapped?  In the human code-switching literature, there are syntactic generalizations about what gets switched. If you analyze the swapping, you could figure out which parts of the sentence (say, verb phrases v. prepositional phrases, beginning v. middle v. end, etc.) mattered more for NLI performance. I'd love to know the answer to that question!\n-you say this: \"The BLEU score of the translation system has little effect on a language\u2019s performance as a cross-lingual augmentor. \" Any ideas on why?\n-you also say this: \"for every language a XLDA approach exists that improves over the standard approach\", what a tantalizing statement! Why did that happen?! \n-Are there any generalizations over whether typologically similar languages are better augmentors for each other than they are for really different ones? I feel like if you could redo your XLR method (fig. 4) by adding augmentors in order from most similar to least (or vice versa), and you might find the answer to this.\n-for XNLI, I'd love to see if you have differences by label (maybe in an appendix?)\n\nSmall Notes:\n-the text in fig1 should be bigger.\n-too many Ms and Ls, you had me chuckling at all the acronym puns!\n-define \"augmentor\" somewhere"}