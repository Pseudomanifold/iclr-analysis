{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi-supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper is very unclear and hard to follow. The authors make some claims that are not true, for instance, learning so called M1+M2 architecture end-to-end is hard, however, there are papers that successfully train such model. Moreover, the authors reported their results as SOTA, however, they missed many other papers with much better scores.\n\nRemarks:\n- The authors claim in the introduction that training a two-level VAE with a classifier (so called M1+M2 architecture) is not robust. However, there are papers that were able to train such model without any reported problems, for instance:\n* Louizos, C., Swersky, K., Li, Y., Welling, M., & Zemel, R. (2015). The variational fair autoencoder. arXiv preprint arXiv:1511.00830.\n* Davidson, T. R., Falorsi, L., De Cao, N., Kipf, T., & Tomczak, J. M. (2018). Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891. (published at UAI 2018)\n* Ilse, M., Tomczak, J. M., Louizos, C., & Welling, M. (2019). DIVA: Domain Invariant Variational Autoencoders. arXiv preprint arXiv:1905.10427.\n\n - The authors report SOTA results on MNIST (100 labels). However, there are papers that report better scores, for instance 2.6 in:\nDavidson, T. R., Falorsi, L., De Cao, N., Kipf, T., & Tomczak, J. M. (2018). Hyperspherical variational auto-encoders. arXiv preprint arXiv:1804.00891. (published at UAI 2018)\nIn this paper they used a VAE model (the M1+M2 architecture) that was trained end-to-end.\n\n- A rather minor remark, but I do not fully see a reason why the authors spent a lot of space on Section 2. I believe most of this text could be included in the Appendix.\n\n- Equation 8: The authors claim that this decomposition was introduced in (Zhao et al., 2017), however, it was done earlier:\nHoffman, M. D., & Johnson, M. J. (2016, December). ELBO surgery: Yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS.\n\n- Equation (16) is very vague and hard to follow. Moreover, the idea of using Optimal Transport is also hard to follow.  The authors present a very generic description of the Optimal Transport, and then refer to an algorithm (a pseudocode). It is very unreadable.\n"}