{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a reasoning-aware graph convolutional network for visual question answering. By incorporating feature-wise linear modulation to GCN, the proposed method performs visual reasoning with respect to the questions. Experimental results on several VQA benchmarks are reported and discussed.\n\nPros.\n1. Visual reasoning is an important and challenging research problem. This paper studies how to equip GCN with the visual reasoning capability.\n2. Three benchmark datasets are employed in the experiments. Details on experimental settings are also provided.\n\nCons.\n1. The major technical contribution of this paper might be the integration of FiLM and GCN. A similar idea has been discussed in [a]. \n[a] GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation. arXiv, 2019.\n2. Some technical details should be clarified. For instance, In Section 3.2, the node features v_i and v_j are fused using function Fuse(v_i, v_j). It's unclear what is the Fuse function.\n3. In experiments, important baselines are missing, such as [b-c]. Also, for VQA-CP v2 dataset, it would be helpful if the authors can show the detailed results for each category of questions, such as Yes/No, Num, and Other.\n[b] Self-Critical Reasoning for Robust Visual Question Answering, arXiv, 2019.\n[c] Overcoming Language Priors in Visual Question Answering with Adversarial Regularization, NeurIPS 2018."}