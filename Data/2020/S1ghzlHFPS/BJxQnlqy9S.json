{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper builds an interesting connection between Datalog rules and temporal point processes. The novelty of the approach is to factorize the latent state of LSTM into different blocks that represent three major interactions between temporal events, including: dependency, affects, and updates. The design of the node blocks within the hidden state allows the modeling of fine-grain structure of a given event type. Based on the Datalog program and the logic rules, the intensity function of the temporal point process can be formulated from facts in a database. The problem of enabling a flexible family of intensity functions is one of the most important topics in point processes, and a paper advancing knowledge in this area is certainly welcome.\n\nThe paper is in general well written. Section 2.2 can be more clarified by explicitly comparing the concepts of blocks and entities using \"mind(Alice)\" and \"body(Alice)\" before introducing the hidden state h_mind(Alice)(t). It took me some time going back and forth to understand these examples here. With respect to the design of the Datalog interface, it looks like it covers the assertion involving two arguments. Since these arguments affect the partition of the number of node blocks, it would be more clear to illustrate how to design the node blocks as the number of arguments increases (say beyond 2 arguments). In fact, if we know the number of entities in each event type, say the number of node blocks to partition is 3 per hidden state in advance, we can leverage three separate small LSTMs each of which has the private hidden state with the same number of nodes as that in one of the node blocks. Then, we can determine the interactions among these separate small LSTMs based on the logic rules, so it will be helpful to elucidate the additional advantages of partitioning these node blocks in the same hidden state. The proposed technique mainly considers how to incorporate the block design into the LSTM hidden states as a general sequence model. What is the unique characteristics of Neural Hawkes Process have been particularly exploited from this perspective? It looks like it can be applied to other LSTM-based approach as long as the predictions are functions of the hidden states. For the synthetic experiments, it is obvious that single Neural Hawkes process has more challenges to fit the mixture of processes. It will be more convincing to compare with a mixture model, like \"A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering\" with the proposed approach, and the same as in the real experiments. Also, a standard test-of-goodness fit like QQ-plot will also be more useful to improve the experiments."}