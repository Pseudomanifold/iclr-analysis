{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": " Summary: \n\n \nThe paper proposed to use Datalog rules to specify the design of the LSTM architecture for event data in continuous time. The LSTM module will be used to model the rate of the events. By incorporating Datalog rules, the paper aims to encode informed inductive biases into the model. \n\nComments:\n\n    After reading the entire paper, I think the main idea of this paper is to use sparse and structured weight matrices (called structural zeros\u201d in the paper) to substitute the dense weight matrices in the original LSTM, and to split the hidden state into blocks where each block refer to a different world\u2019s state.  \n\n\n         How to design the structured weight matrices and how to define the node blocks, it is informed by the  Datalog rules. This design, however, will lead to a huge weight matrix and a very  long hidden state once the types of events and number of entities grow. The proposed model will face a severe scalability issue. From this point of view, only \u201cstructural zeros\u201d weight matrices are not enough for an elegant model.                \n\n \n\n        How to smartly share parameters and how to control the number of parameters will be an interesting direction to explore. This submission touches on this a bit but not in a principled way. For example, in Eq. 18(a) and 18(b), the embedding vectors for the grounded predicate is a summation of the embedding vectors of the entities and the predicates. This final embedding is empirically validated or is based on some permutation invariant property? This needs more clarification or some references.\n\n \n\n \n        2.      The presentation needs to be polished. The current writing is not easy to follow. Especially for section 3. The architecture design needs to be clarified more. When I read this part, I felt a little difficult to map the Datalog rules to your model. \n        3        Since you are learning the vector embeddings for event types and entities, what are the advantages of this compared to the marked point process model, where the event types and entities are treated as discrete markers and are a much more parsimonious model. The Datalog rules can also be defined on the marker level by introducing a structured dependency structure over the markers. What are the potential benefits of learning the embeddings? The explanation is missing in this paper. \n\n       4     Lack of references. The proposed neural-symbolic architecture shares some similarities to the following papers: \n          (1) End-to-End Differentiable Proving \n\n          (2) DeepProbLog: Neural Probabilistic Logic Programming \n\n          (3) Neural Logic Machines.\n          \n        What are your contributions and differences in terms of the neural-symbolic architecture design?\n\n \n       As for introducing logic rules to guide event predication, this is not a new topic. Here is a list of references:\n\n \n         (1) PEL-CNF: Probabilistic event logic conjunctive normal form for video interpretation. \n          (2) A general framework for recognizing complex events in Markov logic. \n\n          (3) Learning Bayesian networks for clinical time series analysis. \n\n          (4) Logical Hierarchical Hidden Markov Models for Modeling User Activities. \n\n          (5) Slice Normalized Dynamic Markov Logic Networks. \n       5.         Lack of strong baselines. The paper only did a small-scale experiment study. It only compares a neural Hawkes process model. The experimental evaluation also needs stronger baselines. Specifically, methods that can handle continuous-time (e.g. marked point process) or probabilistic logic methods that can discretize time (as mentioned in the above references). The baselines are not quite strong and appear a bit arbitrary in the paper.\n"}