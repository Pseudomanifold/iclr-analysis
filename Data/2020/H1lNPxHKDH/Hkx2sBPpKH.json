{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper gives characterization of the norm required to approximate a given multivariate function by an infinite-width two-layer neural network. An important result is the relation between Radon-transform and the $\\mathcal{R}$-norm. This paper also shows application of the norm on some special case.\n\nI suggest this paper being accepted because it provides new insights into the approximation theory for neural networks. The perspective of norm constraint is different from the traditional approximation theory and may serve as a good contribution to the community.\n\nOne question is that: in section 4, the equation (19) is differentiated twice to get the equation (20) containing Dirac delta. Although this is intuitively correct, this seems not a strict derivation to my mathematical background. It would be great if the authors can show the strict definition and derivation presented here."}