{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper studies the function space regularization behavior of learning with an infinite-width ReLU network with a bound on the l2 norm of weights, in arbitrary dimension, extending the univariate study of Savarese et al. (2019).\n\nThe authors show that the corresponding regularization function is more or less an L1 norm of the (weak) (d+1)st derivatives of the function, and provide a rigorous formal characterization in terms of the \"R-norm\", which is expressed via duality through the Radon transform and powers of the Laplacian.\n\nIn addition, the paper provides a number of implications of this study, such as approximation results through Sobolev spaces, an analysis of the norm of radial bump functions, and a new type of depth separation result in terms of norm as opposed to width.\n\nOverall, this is a strong paper making several interesting and important contributions for our understanding of the inductive bias of ReLU networks. I thus recommend acceptance.\n\nA few comments:\n* is it possible to obtain precise characterizations of interpolating solutions in this setting (other than a mere representer theorem with ReLUs), as done in Savarese et al (2019, Theorem 3.3) for the univariate case?\n\n* perhaps the results of Section 5.1 should be contrasted with those of Bach (2017, e.g. Prop. 5), which only require ~ d/2 derivatives instead of ~ d here, albeit with stronger requirements, for essentially the same functional space (though the approximation result is obtained from an associated RKHS, which is smaller).\n\n* are the results on radial bump functions intended to provide insight on approximation or depth separation? what was the motivation behind this section?\n\nOther minor comments/typos:\n- after Prop. 1: \"intertwining\" appears twice\n- eq. (22): missing f in l.h.s.\n- eq. (23): is the first minus sign needed?\n- before Thm. 1: point to which Appendix\n- Section 4.1, \"In particular, this is what would happen ... d+1\": this should be further explained\n- Section 4.1, final paragraph, \"in order R-norm to be\": rephrase\n- Section 5.4, \"required norm with three layers is finite\": which norm? maybe point to a reference? Also, Example 5 could be explained in further detail\n- Section 5.5: what is an RKHS semi-norm? you'd always have ||f|| = 0 => f = 0 in an RKHS, by the reproducing property"}