{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In the paper, the authors propose a pipelined backpropagation algorithm faster than the traditional backpropagation algorithm. The proposed method allows computing gradients using stale weights such that computations in different layers can be executed in parallel. They also conduct experiments to evaluate the effect of staleness and show that the proposed method is faster than compared methods. I have the following concerns:\n\n1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2]. \n2) Does the proposed method store immediate activations or recompute the activations in the backward pass?\n3) In the experiments, the accuracy values are too low for me. For example, resnet110 on cifar10 is 91.99% only, it should be around 93%, an example online https://github.com/akamaster/pytorch_resnet_cifar10. \n4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.\n5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.  \n\n\n[1] Huo, Zhouyuan, et al. \"Decoupled parallel backpropagation with convergence guarantee.\" arXiv preprint arXiv:1804.10574 (2018).\n[2] Huo, Zhouyuan, Bin Gu, and Heng Huang. \"Training neural networks using features replay.\" Advances in Neural Information Processing Systems. 2018.\n"}