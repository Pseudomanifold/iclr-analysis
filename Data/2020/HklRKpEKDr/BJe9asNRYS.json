{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "1. Summary\n\nTeh authors propose to learn value functions that are a sum of utility (single-agent) and payoff (2-agent) components. The weights between all function are shared (common RNN). This stands in contrast to VDNs (only uses utility functions or centralized value functions. The authors evaluate on predator-prey, where they argue that e.g. VDN fails to learn.\n\n(Note that other work has looked at reward shaping to learn *decentralized* agents for that problem.)\n\n1. Decision (accept or reject) with one or two key reasons for this choice.\n\nWeak reject.\n\n- The comparison between different topologies is nice, but implies that the structure of the graph has to be fixed manually. This seems to be a severe and unscalable constraint. It would be better if the authors would propose and evaluate a method to determine / learn what the right graph structure should be.\n- Weight sharing between the various agent components makes the problem closer to a single-agent problem. What happens if the agents are decentralized and the (shared-weight) pairwise functions are separate?\n- Authors only evaluate on a predator-prey problem.\n\n4. Supporting arguments\n\nN/A\n\n5. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\n6. Questions"}