{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces the deep coordination graph for collaborative multi-agent reinforcement learning aimed to solve predator-prey tasks by preventing relative overgeneralization during the exploration of agents. \n\nIn general, this paper gives a detailed and comprehensible depiction of the Introduction, Related work and Background section. However, I have two concerns about their method and experiments. \n\nThe presentation of the \u201cMethod\u201d section is not clear enough to evaluate the contribution of this paper. Specifically: \n(1). In the \"Method\" section, your method incorporates three ideas: 1. restricting the payoffs; 2. sharing parameters; 3. allowing generalization; \nIn my understanding, both idea 1 and idea 2 come from VDN [4]. Idea 3 is not implemented in this work. Then, what is your contribution?\n\n(2). According to the announcement, the key benefit for your work is \"prevent relative overgeneralization during exploration of agents\". It is hard to access if the proposed method can prevent overgeneralization. Do you have some theoretical or empirical justifications?\n\n(3). I think it would be helpful if you could give a description according to your algorithm in the appendix. It would be better to draw a diagram to show what is your model.\n\nI think the experiments are too simple and not convincing. \n(1). I notice recent multi-agent reinforcement papers ([1], [2] closely related to your work) evaluate their work on the challenging set of StarCraft II micromanagement tasks and achieved the evident result. [2] was also submitted to ICLR 2020. \n \n(2). In fig.2, why did you only compare your model with VDN [4]?\n\n(3). In fig.3, why does the return value of QTRAN first decrease and then increase? From [3], it seems their return curve continuously grows. \n\n(4). In fig.4, why QTRAN fail to this task?\n\n[1]. Rashid, Tabish, et al. \"QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning.\" arXiv preprint arXiv:1803.11485 (2018).\n[2]. Wang, Tonghan, et al. \"Learning Nearly Decomposable Value Functions Via Communication Minimization.\" arXiv preprint arXiv:1910.05366 (2019).\n[3]. Son, Kyunghwan, et al. \"QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.\" arXiv preprint arXiv:1905.05408 (2019).\n[4]. Sunehag, Peter, et al. \"Value-decomposition networks for cooperative multi-agent learning.\" arXiv preprint arXiv:1706.05296 (2017)."}