{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Differentially private (DP) algorithms give provable privacy guarantees at the cost of a non-negligible loss in terms of utility. Motivated by such empirical observations, this paper proposes to lift the utility of the DP-SGD algorithm that is used to train differentially private DNNs by a mechanism called Laplacian smoothing. The authors claim that under the same privacy budget, the proposed methods enjoys better utility guarantees, measured in terms of optimality gap (convex cases) and stationary gap (norm of the gradient, nonconvex cases). \n\nOverall, the paper is well-written and easy to follow. However, I am not very convinced by the claim that the proposed DP-LSSGD algorithm enjoys better utility guarantees under the same privacy budget. Specifically, if we check the results in both convex and nonconvex cases in Table 1, it turns out the convergence rates (optimality/stationary gap) in terms of d, n and \\epsilon are exactly the same between the classic DP-SGD and the proposed DP-LSSGD. The only difference lies in the constants, which are fixed and do not matter too much. If we check the results more carefully, it is even not clear in the convex case the constant $\\gamma\\cdot D_\\sigma$ is smaller than $D_0$. From this perspective it is not clear to me what's the essential improvement here? \n\nThe main idea of the Laplacian smoothing is to precondition the original gradient descent by a fixed matrix $A_\\sigma$. The authors claim that the LS method could help to avoid spurious minima and reduce the variance of SGD. I am not sure about this point, since in the literature there are many data-dependent preconditioning methods, including AdaGrad, Adam, AdaReg, etc., but none of them is free of the local-minima problem. So it's not clear to me why a fixed preconditioning matrix will help here? The final DP-LSSGD algorithm is a combination of the original DPSGD algorithm with a preconditioning matrix given by $A_\\sigma$, similar to what is done in LSSGD. \n\nThe main contribution of this manuscript lies in Section 3, i.e., the theoretical analysis of the convergence rate of the proposed algorithm in both convex and nonconvex settings. The results (in terms of convergence rate) are the same as the ones in the non-private settings. Just a comment, with further assumptions, e.g., strong convexity in the convex case, the authors could potentially improve the $O(1/\\epsilon)$ rate to linear convergence of $O(\\log(1/\\epsilon))$. So the main take-home message here is that privacy does not sacrifice convergence speed, which is already known in the literature. \n\nExperiments comparing DP-LSSGD versus other DP variants are conducted. In Figure 5, it is shown that DP-LSSGD is indeed better than DP-SGD in terms of accuracy, but is slightly worse than DP-Adam. A combination of the Laplacian smoothing with DP-Adam gives the best result in terms of accuracy in most of the cases, which is a plus.\n\nOverall, I think there is indeed some novel contribution in this paper, but I feel it's quite incremental in nature, in terms of both theoretical analysis and empirical results. Hence I vote for a weak reject. \n\n\n\n"}