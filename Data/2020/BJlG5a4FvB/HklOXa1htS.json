{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper introduces LSSGD [1] in the privacy-preserving empirical risk minimization to improve the utility upper bound under the same privacy budget. The experiments show that the proposed method called DP-LSSGD outperforms the baseline method--DP-SGD [2].\n\nMy major concerns are as follows.\n1. The improvement of the utility upper bound is insignificant. Table 1 shows that the utility upper bounds of DP-LSSGD and DP-SGD have the same order.\n2. The novelty is incremental. The key method in this paper--LSSGD--was proposed in [1].\n3. Table 2 in [3] shows that DP-SVRG [3] outperforms the baseline method--DP-SGD. The authors may want to conduct more experiments to compare DP-LSSGD with other related work including DP-SVRG.\n4. The authors may want to provide the problem setup of the privacy-preserving empirical risk minimization in detail.\n\n\n[1] S. Osher, B. Wang, P. Yin, X. Luo, M. Pham, and A. Lin. Laplacian Smoothing Gradient Descent. ArXiv:1806.06317, 2018.\n[2] R. Bassily, A. Smith, and A. Thakurta. Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds. In 55th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2014) , 2014.\n[3] D. Wang, M. Ye, and J. Xu. Differentially Private Empirical Risk Minimization Revisited: Faster and More General. In Advances in Neural Information Processing Systems (NIPS 2017), 2017."}