{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nThe paper studies the Laplace approximation for Bayesian inference of a neural network. Specifically, it proposes a diagonal correction and a further low-rank approximation to the Kronecker-factored eigenbasis for more accurate approximation of the Fisher information matrix and better scalablility, respectively. The proposed diagonal correction is shown to have a smaller residual error in F-norm. Experiments are given to show that the proposed Laplace approximation makes more accurate uncertainty estimations. \n\nThe paper makes a certain contribution to existing Laplace approximations for the task in terms of accuracy and scalability. However, it is incremental and the novelty is a bit low, compared to many recent closely related works, for example,\n\nOptimizing Neural Networks with Kronecker-factored Approximate Curvature. 2015\nPractical Gauss-Newton Optimisation for Deep Learning. 2017\nFast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis. 2018\nA scalable Laplace approximation for neural networks. 2018\nEigenvalue Corrected Noisy Natural Gradient. 2018\n"}