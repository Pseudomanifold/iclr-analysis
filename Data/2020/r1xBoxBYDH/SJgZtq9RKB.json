{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Paper Summary:\n\nThis paper proposes to aggregate local information across channels in a convnet with the objective to emphasize / de-emphasize location in the feature map with a local gating for each position, shared across all channels.\n\nReview Summary:\n\nThe paper reads well and is technically sound. Experiments are performed over a few datasets. The approach could be related to other gating strategies and the results would be more convincing if the authors would take setup and accuracy from prior work. The use of the word \"attention\" is slightly misleading given it does not refer to the specific attention module which has become standard in recent years.\n\nDetailed Review:\n\nTo me, the contribution relies on three ideas:\ni. introducing gating maps that gates each feature map, i.e. sigma(g) * f\nii. sharing the gating pattern across channels. sigma(g) is the same for all channels.\niii. computing sigma(g) through recursive three computation, which allows very low parameter counts.\n\nI feel it is necessary to evaluate these ideas gradually with ablations and refers to related work on gating. For gating units, one can refer to GLUs introduced by Dauphin et al 2016. It would make sense to verify if ii. is a good idea, i.e. verify if GLU with sharing across channels perform better than without and later introduce recursive trees. Also verifying that computing gating potentials from point-wise convolutions rather than with larger kernels need to be done.\n\nI would strongly advise against using \"attention\" for anything which as no normalization across competing locations, \"gating\" is much more appropriate to describe your proposal.\n\nFor the experimental evaluation, the presented results are below the original wide resnet paper (BMVC-16) for Cifar10 (wrn err=4.81), Cifar100 (wrn 16-8, err=22.07) and SVHN (wrn16-8, err=1.54%) and none of the presented variant outperform this result. This poor performance casts doubts on the validity of the resnet experiments. Given that resnet/resnext networks are mostly compared on imagenet, I would suggest to also report at least one model architecture with the four variants on this set, with the baseline taken from another paper. Tiny imagenet also has a decent set of available results with a wide variety of architecture, with a lesser training cost.\n\n\nDetails: \n\nFor Figure 1. You could clarify how the module is connected to the whole network.\nDefine m in Eq. 1. \nDefine point-wise convolution at first mention.\nDefine PRelu and introduce equation.\n\nMissing references:\n\nLanguage Modeling with Gated Convolutional Networks, Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier, 2016.\n\nParsing natural scenes and natural language with recursive neural networks\nR Socher, CC Lin, C Manning, AY Ng, 2011."}