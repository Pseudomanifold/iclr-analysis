{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an attention mechanism that is designed to capture inter-channel relations. This is in contrast to other recent works (and the usual motivation for attention in computer vision) which focus on capturing long-range spatial dependencies. The approach at hand is based on recursive application of one-by-one group convolutions, which introduces only few extra parameters.  Experiments on CIFAR and SVHN show gains over a baseline without attention as well as two previously published attention methods. Several ablations are performed with the aim to provide insight into the attention method's behavior.\n\nOverall, I think that in its current state this paper should be rejected. Besides several issues with the presentation (which could potentially be improved in a future revision), my main criticism is that the approach is not well-motivated and not sufficiently justified experimentally: the reported results are on datasets consisting of 32x32 pixel (i.e. small) images, and results on ImageNet, the most standard classification benchmark, are missing.\n\nThe motivation given in this paper is that both inter-channel relations and long-range dependencies are important (I had trouble spotting this claim in the Cao et al.  (2019) paper which is cited in the introduction; it would be nice if the authors could elaborate on this?). Then, the authors focus only on the former without further exploring the latter. Intuitively, one would assume that long-range spatial context becomes more important as the input dimensionality increases?\n\nRegarding the presentation, I took issue with the following aspects:\n- The related work section is folded into the introduction and consists of a mere listing of current works on attention in computer vision. Some concepts like non-local networks are not introduced, which makes the listing hard to understand for readers that aren't already familiar with the literature.\n- The motivation regarding why one should forget about spatial dependencies is not clear. What I got out of it was that using inter-channel relations only produces good results, too -- but it's not clear to me why it should be, in principle, superior to exploiting both inter-channel and inter-location ones.\n- After reading section 2 multiple times, I still do not fully understand how the method works in practice. What I found missing was how C_0^m is applied back to the activations X that f is applied to. The text mentions \"recalibrate the input feature map by element-wise multiplication\", but I don't know what \"recalibrate\" should mean? In the end, are all channels simply scaled by a C^0_m? I think in addition to clarifying the textual description, it would be good to add X, Y, and the input to the following layer to Figure 1.\n- The result tables sometimes contain surplus information, e.g. the FLOPS column in table 1 and 2. It would be sufficient to say that the number of FLOPS does increases only by X% in the text?\n- I found Section 4.1 somewhat confusing. Based on my current understanding, if C_0^m is a 1xHxW tensor, presumably with different entries for HxW, how can the variance be 0? If it's the case that the method is bound to produce zero variance, then I would suggest not plotting the variance at all and merging the four graphs in figure 3 into a single one.\n- Section 4.2 was not clear to me either. Are the PReLU weights removed only after the network was trained? If not, figure 5(a) suggests that the weight in PReLU does not matter, which begs the question why this activation function is being used in the first place. \n- Figure 6 does not seem to support the claim of faster convergence speed."}