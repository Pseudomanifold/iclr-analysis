{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors propose a simple and highly parametrically efficient module named TAM, recursively encouraging neighbouring channels to collaborate in order to produce a spatial attention map as an output. \n\nThe paper offers a good read and diagrams are informative.\n\nBelow are comments for improvement and clarification.\n- The authors claim the effectiveness of capturing inter-channel relations based on performance but it would be great if they can illustrate inter-channel relations.\n\n- Also, since the authors mentioned other works of attention modules (NLNet, GCNet, BAM, CBAM), it would be good to compare with them (although they compared with CBAM)\n\n- In Figure 6, on ResNet-50, TAM shows faster convergence and lower Top-1 Error but on ResNext-50, it is hard to see the difference. Any comment on this?\n\n- It would be interesting to see any segmentation task and performance comparison by imposing nonlinearities between neighboring channels and considering inter-channel relations. "}