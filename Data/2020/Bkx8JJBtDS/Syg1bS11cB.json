{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "TITLE\nCounting the Paths in Deep Neural Networks as a Performance Predictor\n\nREVIEW SUMMARY\n\nPAPER SUMMARY\nThe paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network. The paper demonstrates a correspondence between the proposed measure and the empirical performance in a set of experiments. It is argued that more effective neural network structures can be found by maximizing the proposed measure: This is demonstrated by a novel modification of a densenet architecture that increases the proposed measure and leads to a modest performance gain.\n\nQUALITY AND SIGNIFICANCE\nI really enjoyed reading this paper since it aims to give a better understanding of the relation between neural network archtecture and performance, for which there is really not much theory. And while I understand the motivation behind the simple measure of path complexity proposed, I am not convinced that the measure really is very informative. The paper only provides weak empirical justification and no theoretical justification. Demonstrating an improvement on CIFAR-10 with a relatively minor change of the densenet architecture is interesting and worth noticing. But I am simply not convinced about the explanation, and as a demonstration of the particular improvement the experiment is too limited (as the improvement in accuracy is marginal).\n\nCLARITY\nThe presentation is clear and the paper is in general easy to follow. Experiments are described in sufficient detail and source code is provided, which is commendable!\n\n\nFURTHER COMMENTS\n\nI am not sure why you refer to number of layers etc. as \"hidden\" hyperparameters. Why \"hidden\"?\n\n\"with continuous mathematics\" Unclear\n\n\"The goal of DNN models...\" This is not the goal, I think. The goal must be something along the lines of making correct classification on new data (generalization). This section describes your interpretation of how an effective NN works.\n\nI am not sure how your claim regarding \"increasing the dimension of the representation\" is supported by the reference (Vapnik, 2013).\n\n\"comply with many of the assumptions behind complex networks\" What you mean by this is unclear to me. Is there some technical definition of \"complex network\" that I am not aware of? If so, provide a reference. (To my knowledge, the term \"complex network\" usually refers to any network with a non-trivial topology.)\n\nAt first read, the definition in eq. 1 is confusing. What happens when layer i+1 has fewer nodes/channels than layer i? Then the definition of \"paths\" seems inappropriate. Consider explaining this up front.\n\nThe claim that the number of paths quantifies quality (in some sense) is not justified well enough in my view.\n\nThe assumption that Ni+1 = Ni*(1+alpha) seems unjustified.\n\nIn fig. 2 I would have liked to see all the data from the experiments including results for different initialization and learning rates. Also, in this experiment, can you justify using the same weight decay in all settings?\n\n\"...the optimal variance of the weight initialization\" I think \"optimal\" is too strong here.\n\nIf I read fig. 2 correctly, the optimal accuracy and Z are not the same (although it seems there is some correspondence)\n"}