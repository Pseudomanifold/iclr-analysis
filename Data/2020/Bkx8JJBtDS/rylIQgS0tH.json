{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a novel quantity that counts the number of \u201cpath\u201d in the neural network which they argue is predictive of the performance of neural networks with the same number of parameters. The paper shows a continuous approximation Z for the proposed path counting model and provides a close-form solution on how the channel number in the architecture should be chosen. The experiment investigates the correlation between Z and generalization of model and also compute Z on a number of popular architectures to show that Z correlates with the empirical performance observed. Finally, the paper proposes a small modification to the DenseNet architecture which increases Z and shows that this change improves generalization with no extra parameters.\n\nI think understanding the influence of the network architecture is a valuable direction for deep learning that is currently under-explored and the approach proposed seems very reasonable. Some existing works that the authors that might want to discuss are [1] and [2]. [1] proposes an architecture dependent quantity that can be optimized to improve generalization and [2] explores using different classes of random networks for neural networks. [3] proposes a quantity that predicts generalization albeit with a very different approach.\n\nI have a number of concerns regarding the motivation, experiments and overall writing quality, so I am currently leaning toward rejecting this paper:\n\n    1. The justification for paths in section 2.1 is extremely hand-wavy. This is the core mathematical object the paper studies, and if theoretically rigorous justification is difficult, then at least a detailed heuristic justification or an illustration should be provided. The paper claims the proposed quantity provides an increased understanding of generalization but I do not see sufficient support for this claim in the current draft.\n\n    2. The paper argues that neural networks is a complex network and make another loose connection to path entropy, a concept from physics. I believe making connections to physics is great, but this is not a common knowledge for the ICLR community and warrants more elaboration.\n\n    3. Figure 2, 3 and 4 \u2018s points (especially the converged points at the the top) do not track the predicted Z closely. In fact, in some cases, the networks with smaller Z performs the best (Figure 3). If this is due to randomness, then the experiments should be repeated for statistical significance. Further, different time steps should be colored differently to increase readability.\n\n    4. In my opinion, the paper positions itself to be a heuristic paper with empirical verification rather than a theory paper. As such, I think the paper needs much more empirical evidence to support the claim. For example, table 1 is missing many entries which makes comparison hard. Given that in the toy models the performance sometimes does not track the prediction closely, I would like to see more results on realistic models and more thorough analysis on the results (especially when the prediction fails to explain the observed results).\n\n    5. The proposed method, like the paper points out, requires non-trivial understanding of the architecture. This limits its utility. One of the largest potential I see is using it for architecture search but this need for understanding makes it difficult. Is it possible to derive an algorithm to compute Z for arbitrary DAGs? i.e. NASNet or randomly wire neural networks.\n\n    6. For methods of improving Z, only one possible modification is made to a single architecture (DenseNet). This is not very convincing evidence for the effectiveness of Z. I would like to see such modification made more more architectures such as a vanilla Resnet or wide resnet. Some proof of universality would strengthen the argument.\n\nMinor comments that did not affect my assessments:\n    - Typographical Errors: inconsistent usage of double quotation, page 3 bottom \u201cfeeedforward\u201d.\n    - Figure 5/6 look hand-drawn and are extremely confusing. I encourage the authors to remake them with either PowerPoint, Google Slides or TikZ and add more details + clarification.\n\nReference\n[1] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Tan et al. 2019\n[2] Exploring Randomly Wired Neural Networks for Image Recognition, Xie et al. 2019\n[3] Predicting the generalization gap in deep networks with margin distributions, Jiang et al. 2019\n"}