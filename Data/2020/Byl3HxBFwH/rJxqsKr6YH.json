{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe paper proposes a method for sequential and adaptive selection of training examples to be\npresented to the training algorithm. The selection happens in a latent space, based on choosing\nsamples which are in the direction of gradient of the loss in the latent space. Two selection\nstrategies are investigated: nearest neighbor and interpolation followed by generation. Results on\nshown on MNIST, CIFAR10 and IVUS (Intravascular Ultrasound) datasets.\n                                                                \n                                        \nDetailed comments:                      \nThe proposed method works in two stages. First a VAE is trained using unannotated samples. In the\nsecond stage, hard examples are found, in every iteration, in the latent space of the VAE and used\nfor sequential training. The sampling is done using the gradient of the objective function in the\nlatent space. The method makes sense, however the choice of space in which the sample selection is\nbeing done is not well motivated or validated. The space could have been the original image space\n(although given the high dimension, it would probably not work), or could have been any intermediate\nfeature space. Why was the space chosen to be the VAE latent space? Would it be possible to \ndemonstrate some benefits of doing so, theoretically and empirically? \n                                        \nThe experiment section is relatively weak. The datasets used are relatively small and in two out of\nthe three datasets, the method does not improve. The dimension of the latent space is also\nsurprisingly small (2). While the main body of the paper describes the method with VAEs, the\nexperiments for the CIFAR10 dataset (where the results were in favor) were done \\alpha-GAN. "}