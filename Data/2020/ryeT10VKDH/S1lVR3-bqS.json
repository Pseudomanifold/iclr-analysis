{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThis paper tackles the problem of transferring a policy from source to target MDP, which differ in the state transition function. The idea is to add an additional cost that is the KL divergence between the trajectory likelihood under target policy (being learned) and target dynamics and the trajectory likelihood under the source policy (assumed optimal and deterministic) and source dynamics. The intuition is that the target policy will learn to match the state distribution of the optimal source policy. Results on MuJoCo locomotion robots with varying physics show that the proposed method performs better on target than warm-started RL or learning from scratch. \n\nI think the problem of transferring knowledge from one task to another in RL is very important for RL to be applicable to more real-world scenarios.\n\nConcerns / Questions\nLine 7 of Alg1 is confusing because it refers to a \u201ctarget task model\u201d, but in Assumption 2, it says only a model of the source transition function is needed. I think it makes sense that only the source transition model is needed because the target next state is given by experience. \nI think the combined assumptions of a) access to expert behavior (same as DAGGER) and b) that the MDPs differ only in dynamics functions and c) access to the source transition model are rather strong. I think (b) is a special case of transfer learning - a lot of transfer learning is concerned with changing reward functions as well, which this method wouldn\u2019t apply to. I think this could be made more clear in the paper. It would be good if all these assumptions were made clear and discussed.\nI think the related work section is missing important areas of research in imitation learning and meta-reinforcement learning. For imitation learning, the approach strikes me as bearing similarity to PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings (Rhinehart et al.), and the topic of imitation learning should be discussed in general. For meta-RL, mentioning that it shares the same goal of transfer and citing a few main works (e.g. Duan et al. 2016, Wang et al. 2016, Finn et al. 2017 etc) would be good.\n\nWriting Suggestions\nSome terms used throughout the paper are quite unclear (e.g., \u201cunsupervised RL\u201d, \u201cintrinsic adaptation reward\u201d, \u201csupervised reference trajectory tracking\u201d). I suggest standardizing and defining terms early to avoid unnecessary confusion.\nWriting the Bellman operator and the value function equations in Section 2 don\u2019t seem very relevant as they are I think never used again?\nSections 3.1 and 3.2 are quite difficult to understand on first read (e.g., what does \u201cpoint-wise local trajectories\u201d mean?). \nI find Section 3.2.1 a bit misleading, \u201cThe optimization is more akin to supervised learning\u201d - I agree the KL minimization is essentially imitation learning, but you are still doing policy search in addition to it?\n"}