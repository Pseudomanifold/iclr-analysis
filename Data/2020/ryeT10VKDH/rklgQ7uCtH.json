{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "\nThis paper addresses the actively studied problem of efficiently transferring policies across domains in reinforcement learning. Authors propose a framework to transfer policies between tasks in domains with significantly different state transition. The proposed algorithm is based on a policy adaptation mechanism, with the idea that provided that a source optimal policy of a task is available, that policy is adapted to derive the optimal policy of the target task at a low sample complexity. \n\nThe paper is well written and the proposed algorithm is novel and original. The algorithm is very clearly described and theoretical bounds are provided. The performance is evaluated experimentally on a set of tasks, showing promising results.  \n\nCons:\n- Some important previous  works are missing from the related work section. For example, [1, 2] are also concerned with transferring policy across task that share the same domain. \n- The experimental section could benefit from comparing to other transfer RL baselines such as [1].\n- Assumption 2 should be discussed; any suggestion on how to model the KL approximation if the assumption does not hold?\n\nMinor comments for clarity:\n- some acronyms are introduced without ever being spelled out: D-RL, TL, MDP, PAC\n- page 3, equation before (2), is \\gamma = 1?\n- the use of s' for the optimal state, a' for optimal action and then r' for total reward is quite confusing. Specially because s' is in general used to describe a subsequent state in the literature. \n- page 7, first line 'discount' instead of 'discout'\n- font on figures 1 and 2 is very small\n\n1. Barreto et al, Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement, 2018\n2. Ma, Wen, and Bengio Universal Successor Representations for Transfer Reinforcement Learning, 2018"}