{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThe paper presents a hierarchical approach for learning a policy to navigate indoor environments. It consists of a two cooperative policies consisting of a planner which provides sub-goals to the executor achieves the sub-goal. The executor provides a summary of it's trajectory back to the planner which helps the planner decide the next sub-goal better. They show results on Habitat for the task of point goal navigation. \n\nMy overall rating is mostly based on the points raised in the section below around experiments and and analysis. \n\n- Strengths :\n    - Instead of using a single way communication from planner to executor, they also provide a summary vector from the executor to the planner that gives feedback about the given sub-goal.\n    - The paper uses a hierarchical approach for point goal navigation and gives some theoretical explanation on how a long rollout horizon has negative impact on the exploration efficiency. Even though the proof was only for the 2 actions, they ran an empirical experiment to show that in the case of point-goal navigation task, as the horizon increases, the exploration efficiency begins to rapidly decline as evident by the normalized geodesic distance.\n\n- Weaknesses:\n    - Clarity: The paper is very light on details in section 3. Both the algorithms are mentioned in the supplementary but should be mentioned in the main paper. Also some of the notations are not mentioned clearly. For instance, what does the terminal (t_i) mean.\n    - Experimentation: The paper compares the proposed approach with Savva et al 2019. But the comparison is not fair.\n        - (1) the number reported in the paper are not the best numbers from Savva et al. The best numbers come by just using the depth sensor so showing results by using just the depth sensor modality might be better. Or replicating the experiments with all the different sensor modalities (RGB only, Depth only and RGB-D) will be better.\n        - (2) The approach mentioned in Savva et al doesn't build an egocentric map, so either endowing that approach with egocentric map, or removing that from the proposed approach will be a better evaluation.\n    - The authors also don't substantiate the reason for not using SPL enough in the form of citations? The paper mentions that maximizing on this metric as an objective can result in undesirable behavior but Savva et al doesn't maximize SPL reward as part of the objective. The metric is only used at evaluation time.\n    - The authors claim that the proposed approach is specially useful in the case of sparse reward, but their reward definition suggests that it's not a sparse reward task. \n    - Analysis and ablations: Even though it has been shown empirically that this hierarchical nature helps, it'd still be good to tease out the gains properly. Like mentioned before, are the gains simply coming from an agent having access to an egocentric map. Is the egocentric map even important for the planner. Giving some insights about what the planner has learnt and what the kind of sub-goals it generates for the executor would be helpful.\n    - Did the authors try simple baseline such as using a random policy for sampling sub-goal. There are other heuristic based baselines such as always generating a subgoal within k meters of the current position in the direction of the target.\n    - For pointgoal navigation, is the egocentric map all that useful? If the agent is building the egocentric map as it executes a policy, how does the planner plan ahead based on parts of the map that it hasn't discovered. Does it help in situations where it went down the wrong path, and has to come back?"}