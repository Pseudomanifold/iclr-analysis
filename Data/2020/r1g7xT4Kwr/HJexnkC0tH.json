{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a hierarchical learning method for the Pointgoal navigation task. It consists of two agents, a Planner and an Executor. The Planner agent proposes subgoals to the Executor which learns to reach the subgoal and passes back information to the Planner. Results on Gibson dataset in the Habitat environment show that the proposed method outperforms PPO LSTM and SMT baselines on RGB-D Pointgoal navigation task.\n\nThe authors motivate the method by stating that there is a need to plan across long time horizons with sparse reward signals. They state in their contributions that their method significantly improves sample efficiency. However, the experimental task setup and empirical results do not justify any of the above claims.\n\nSample efficiency: A simple classical map and plan deterministic baseline achieves a success rate of 0.976 on the validation set [2] and an SPL of 0.89 on the test set [1] for the RGB-D Pointgoal Navigation task without any training. This is much higher than the success rate of 0.81 reported by the authors using 30 million frames. \n\nSparse reward and long-time horizons: The task involves intermediate rewards equal to the reduction in the geodesic distance at each step. This is not a sparse reward task.\n\nThe current set of results do not empirically justify the significance of the proposed method as there exists a deterministic algorithm that performs much better than the proposed method without requiring any training data. The proposed method is specifically designed for the Pointgoal task and it requires the depth channel as input to build the map which is given as input to the Planner. The method can not be trivially applied to any other navigation task or RGB setting in the Pointgoal task. The theoretical result is trivial. It is widely known that the exploration sample complexity grows exponentially with the time horizon of the task. \n\nDue to the lack of empirical evidence to justify the effectiveness and significance of the method, I vote for rejecting the paper.\n\nOther comments and suggestions:\n- In the list of contributions in the introduction, I suggest changing the first contribution to a hierarchical RL approach \u2018for navigation\u2019 as it is not a generic HRL approach for any task.\n- The theoretical result in Section 3.1 is not required in my opinion. It could be replaced with Algorithms in the Appendix.\n- I can see why optimizing for SPL can lead to undesirable behavior, but there's no harm in reporting SPL even if you do not optimize for it, as your baselines are also not optimizing it. I would recommend reporting SPL as it is fairly popular metric and it helps in comparing with prior methods. \n- I suggest tackling RGB variant of the Pointgoal task or other navigation tasks such as Room, Object or Image Goals by modifying the proposed method.\n\n[1] https://evalai.cloudcv.org/web/challenges/challenge-page/254/leaderboard/839\n[2] https://github.com/s-gupta/map-plan-baseline"}