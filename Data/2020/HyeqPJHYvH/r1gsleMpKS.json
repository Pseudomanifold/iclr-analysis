{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions: this submission proposes a video pixel generation framework with the goal to decouple visual appearance and dynamics. The latent dynamics are modeled with a latent residual dynamics model. Empirical evaluations on moving MNIST show that the proposed residual dynamics model outperform MLP or GRU. On more challenging KTH and BAIR datasets, the proposed method achieves on par or better quantitative performance with previous methods, and have nice qualitative results on content \"swap\" and dynamics interpolation.\n\nAssessment:\n- To my knowledge, the proposed model is novel for video generation.\n- The proposed method is supported with strong quantitative results and qualitative analysis, ablation on Moving MNIST shows that the proposed latent residual dynamics model outperforms MLP and GRU baselines.\n- The authors might be interested in related work on video generation with decoupled appearance and dynamics models, such as [1]. It would also be interesting to see evaluation on more challenging datasets, such as Human3.6M.\n- Question: how does the proposed inference framework make sure to decouple appearance with dynamics? Can y_i not encode the appearance information?\n\n[1] Minderer et al., Unsupervised Learning of Object Structure and Dynamics from Videos. NeurIPS 2019."}