{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content - hence the name \"laconic\" image classification. The motivation for this task is to compare human and machine performance in a task that deviates from the standard ImageNet setup. To make different content reductions comparable, the authors measure the (approximate) entropy of an image via its PNG-compressed file size. As image transformations, the authors utilize quantization, downsampling, cropping, and a combination of the above. The authors find that convnets with higher accuracy are also more robust to these perturbations, and that humans perform well on the minimum-entropy examples of the networks (but not vice-versa).\n\nOverall I find the comparison of human and machine performance interesting and hence recommend accepting the paper. However, there are multiple directions that could possibly strengthen the core experiment. Hence I only give a weak accept at this point. Concretely, these directions are:\n\n- Do the results in the paper change if a different entropy measure is used (e.g., JPEG compression)?\n\n- As suggested by the authors, training networks to be robust to the \"laconic\" image perturbations could be an interesting direction. For instance, standard data augmentation with the proposed perturbations would be a relevant baseline.\n\n- To aid replicability and to compare the performance of different human test subjects, it would be interesting to conduct the experiment also on a crowdsourcing platform such as Mechanical Turk (as a complement to, not a replacement for, the university population in the paper).\n\n- Also to add replicability and to make it easier to compare different human accuracy evaluations, it would be good to measure how well the annotators perform on the unperturbed images and on a simple noise transformation (e.g., Gaussian noise).\n\n- It would be good to know how approximate the entropy measures in the paper are, e.g., to understand why humans perform worse in the \"combined\" perturbation setting.\n\n- How did the results from the control group and the open / online evaluation differ?\n\n\nIn addition, I have the following suggestions for improving the paper:\n\n- For the related work section, the authors may find the following papers on robustness of convnets to distortions interesting:\n\n* Manitest: Are classifiers really invariant?\nhttps://arxiv.org/abs/1507.06535\n\n* Exploring the Landscape of Spatial Robustness\nhttps://arxiv.org/abs/1712.02779\n\n* Spatially Transformed Adversarial Examples\nhttps://arxiv.org/abs/1801.02612\n\n* Semantic Adversarial Examples\nhttps://arxiv.org/abs/1804.00499\n\n- It could be helpful for the reader to see some example images of the different transformations in the main text.\n\n- Section 6: \"[...] a bias in texture for images trained on the ILSVRC dataset [...]\" - should this be \"classifiers\" instead of \"images\"?\n\n- Section 6: \"[...] would be interesting to explore in future\" - insert \"the\" before \"future\"?"}