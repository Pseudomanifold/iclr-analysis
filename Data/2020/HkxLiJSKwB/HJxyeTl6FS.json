{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an extension of Value Iteration Networks that uses graph neural networks to perform value iteration over graphs of arbitrary size and connectivity. The paper demonstrates how graph networks can be used to perform value iteration both in their original form and with attention, and then compares these variants to several baselines across three domains (grid worlds, irregular graphs, and motion planning). Across all these comparisons, the proposed method tends to perform better than the alternatives and with better data efficiency.\n\nOverall, I think that this is an interesting extension to VINs and a natural use of graph neural networks. However, I think the paper is somewhat limited by the fact that it only performs experiments with imitation learning and not with full RL. Additionally, I think the paper suffers from a lack of clarity in some places and issues with the evaluation. I thus lean on the side of reject, but might be willing to increase my score if these issues can be addressed.\n\nMy main concern is that the method is not evaluated in the context of actual RL or even in environments where the model would potentially provide an improvement over symbolic approaches. Specifically, In the imitation learning setting, this means that the network is essentially just learning a shortest-paths algorithm (I\u2019ll also note that this has already been done for smaller graphs as a toy example in Battaglia et al. (2018)). This then raises the question: why do we need to learn a shortest paths algorithm at all? Why not just use a traditional symbolic shortest paths algorithm (as is actually done to generate all the training data in the paper)? The answer in the original VIN paper was that by training the VIN module with a policy, the agent could learn to exploit additional perceptual information that might not be so straightforward to incorporate in the symbolic computation of value iteration (e.g. see the \u201cMars\u201d domain from the VIN paper). It does not seem to me that any of the experiments in the present paper test on graphs where the reward function is unknown and a function of the input features. Thus, it does not seem to me that the paper sufficiently justifies why the method is better than alternative symbolic methods.\n \nI also found the paper somewhat hard to read. It is a bit overly verbose in some places while not providing enough detail in other places. For example, I do not think it is necessary to go into so much detail about GPPN, Generalized VIT, GCN, or MPNN (probably these details can be included in the appendix). It is of course important to briefly describe what GPPN and Generalized VIT do in relation to the proposed method, but this could be done in the main text in only 1-2 sentences each without including any math. This would make it much easier to identify which equations are actually relevant for the proposed method. However, I thought that other parts of the paper weren\u2019t particularly clear: for example, it\u2019s not clear to me what the graphs look like in the Baxter experiments.\n\nFinally, I appreciate the extensive experiments and comparisons to baselines. However, given that only single numbers are reported, I find these results almost impossible to interpret. I would like to see all results reported with error bars over multiple training runs over the algorithm (ideally at least 3). Otherwise, I do not know whether the difference between (for example) 99.8 and 99.5 is meaningful (Table 2).\n\nSome additional comments and questions:\n\n- The original VIN paper did include results on an irregular graph (WebNAV). It would be nice to see some discussion of this result in comparison to the proposed method.\n- How would this method scale to very large graphs?\n- The paper should include a discussion of other uses of graph networks in RL, such as [1-5].\n- It would be nice to see a comparison that uses max pooling rather than sum pooling. I suspect the reason why the GAT variant works so well is because it is picking out the maximum value, so it would be good to see a comparison of this explicitly.\n\n[1] Khalil, E., Dai, H., Zhang, Y., Dilkina, B., & Song, L. (2017). Learning combinatorial optimization algorithms over graphs. NeurIPS 2017.\n[2] Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., & Battaglia, P. W. (2018). Relational inductive bias for physical construction in humans and machines. CogSci 2018.\n[3] You, J., Liu, B., Ying, Z., Pande, V., & Leskovec, J. (2018). Graph convolutional policy network for goal-directed molecular graph generation. NeurIPS 2018.\n[4] Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L., Kohli, P., Battaglia, P. W., & Hamrick, J. B. (2019). Structured agents for physical construction. ICML 2019.\n[5] Wang, T., Liao, R., Ba, J., & Fidler, S. (2018). Nervenet: Learning structured policy with graph neural networks. ICLR 2018."}