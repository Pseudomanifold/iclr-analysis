{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes using graph neural networks for learning to plan on general graphs. The proposed method is able to learn from optimal plans and generalize to unseen graphs. Empirically it outperforms several existing baseline approaches on both regular graphs (2D lattices) and general graphs.\n\nThis paper should be rejected because (1) the contribution is incremental, similar ideas has been explored in previous papers; (2) the experiments are too toy to demonstrate the practical use of the proposed method.\n\n==============================================================================================\n\nMain argument\n\nThe idea of this paper is not novel enough. As its predecessors, VIN[1] contributed the idea of embedding the value iteration algorithm into the neural architecture; and GVIN[2] extended the same idea to the general graph domain. This paper instead tries to improve GVIN by replacing the value iteration update with the graph neural network message passing update. The only contribution is showing empirically that graph neural networks are somehow more suitable than the previous parametrizations of update operators. Moreover, the experiment part of this paper is not convincing. Both synthetic domains I and II looks toy and domain III looks very similar to domain II (please let me know if I am wrong). Domain III is more practical but it doesn't look obvious to me why using learning is better than using traditional shortest path algorithm such as Dijkstra? From my understanding Dijkstra would be more efficient and effective. This experiment is not a good example of a potential real-world application of the proposed method.\n\n\n==============================================================================================\n\nWriting, soundness and organization of the paper\n\nThe writing of this paper is acceptable, but the organization can be improved. Using more than 4 pages to introduce the background knowledge is way too much. The writers should focus more on their own work.\n\nThe authors keep emphasizing the idea of invariant to graph isomorphism throughout the paper. It is a property of the proposed architecture but correct me if I am wrong, it doesn't occur to me that VIN and GVIN lack such property. It also confuses me when the author says in the last paragraph in the first section, \"These models are known to be invariant to graph isomorphism, therefore they are able to have a generalization ability to graphs of different sizes and structures\". I don't really understand the rationale behind this. Can the authors explain this in the response?\n\n\n\n[1] Tamar, Aviv, et al. \"Value iteration networks.\" Advances in Neural Information Processing Systems. 2016.\n[2] Niu, Sufeng, et al. \"Generalized value iteration networks: Life beyond lattices.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018."}