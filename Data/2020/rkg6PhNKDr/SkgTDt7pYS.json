{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy.\n\nThis paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don't think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers.\n\nGiven that, I vote for a rejection."}