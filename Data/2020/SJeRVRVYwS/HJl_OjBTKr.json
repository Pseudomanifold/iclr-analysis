{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents experiments on neural summarization of text, using two types of sequence-to-sequence models, on a corpus that has been created from both an existing data-set, and a newly created data-set. The authors propose a new metric to measure the \"factuality\" of the generated summaries, which the proposed approach improves.\n\nI find the paper very confusing to read, and the experiments unconvincing. My main issues are:\n- The description of \"Our SD dataset\" is concerning: \"We do not perform explicit pre-processing, and thus the papers do not follow any standard format ...\" - this makes me wonder if the data can be trusted, please describe this more carefully. Can you make the data available so that other researchers can reproduce your results?\n- You claim to achieve \"sizable improvements over a strong baseline\": I have no idea how well a strong baseline on this dataset should perform because there are no reference points of your implementation, data preparation, and choice of hyper parameters on other well-understood datasets or tasks, as far as I can tell.\n- I find the description of the experiments confusing: you claim to use multi-task training, transfer learning, as well as co-training - but from what I can tell, the two methods are essentially the \"GROVER\" method with extra text markers, and an auto-regressive application of a seq2seq model (which is auto-regressive itself in the case of the STORY model)\n- There is a concern about novelty: \"unlike these techniques, our task here is automating science journalism\" - this in itself is probably not a contribution, however the \"RA\" metric could be an important and novel contribution\n- I must admit I am not sure I understand the RA measure from your description. Could you at least describe what the range and ideal value for RA is? Is it 0-100 with 100 being best? Because it is a fraction that you multiply by 100 to get \"points\" or scores? Where do you get the p_hat from in the evaluation? Do you measure them \"empirically\" by counting? Or do you get them from the trained model? \n- It is good to show example, in this case I think it would be better to show shorter excerpts and more conditions, and explain exactly what the reader is supposed to see from each example.\n"}