{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a novel dataset, which summarizes a scientific paper into a corresponding press release (i.e., Science Daily), for textual summarization task. The proposed transfer learning setup improves an automatic evaluation metric (i.e., ROUGE) as well as a newly proposed automatic metric for checking the factuality of the summarization model. \n\nI really appreciate the dataset itself and the way they introduce the problem (e.g., factuality, language style difference). The dataset would be a valuable dataset for abstractive summarization. However, the way that they validate the hypotheses is not convincing, and most of the empirical validation is not reasonable to me. Please find more details below. \n\nOverall, the paper is easy to follow except for the RA evaluation part. In terms of presentation, it would be better to follow by adding some real examples of the Science Daily text. Readers may be interested in seeing how the press release is different from other summaries. \n\n\nMy major concern is the lack of scientific/linguistic consideration of the task. First, it would be better to highlight the major difference of the task compared to existing textual summarization and tackle some of them in your model design. For example, the text used in press releases will be much easier words for public readers as described in the paper. Then, how does your model address this issue? If the summarizing the papers to press release is different from summarizing the papers to abstract or title of the paper, then how does your model address this? For the issue of factuality (which I don\u2019t get convinced much though), which parts of transfer learning do you think it is tackled by? Given the ROUGE and RA scores, I don\u2019t find any evidence of how factualities are improved by the model you proposed.\n\nSecond, textual summarization is not always \u201cneural\u201d textual summarization. If your focus is only the \u201cneural\u201d textual summarization, please clarify how the \u201cneural\u201d component makes it different from general textual summarization. Otherwise, I highly encourage authors to do more literature survey on textual summarization, scientific paper summarization, textual abstraction, and more. Summarization of scientific papers has been actively studied for many years, in various ACL venues (e.g., ACL, NAACL, EACL, EMNLP). Some works focus on the content of papers, while others focus on the meta-information of papers such as citation networks. Here are some of them:\n\nCoherent citation-based summarization of scientific papers. ACL 2011\nA supervised approach to extractive summarisation of scientific papers. CoNLL 2017\nScisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. AAAI 2019.\n\nIf you treat textual summarization as a machine learning problem, it is fine. But, please understand the nature of the problem first and then design your model accordingly. \n\nAnother weakness of this work is the lack of novelty in the proposed methods. The only contribution made in the proposed model is adding the separation tokens to distinguish the source and target, and two different datasets. First, I don\u2019t think the hypothesis made by authors is correct: encoders and decoders will focus on the tokens they specified and distinguish them accordingly. This may or may not be true but there is no evidence provided in the experiment. Second, another hypothesis in the PART model does not make sense at all: dividing press releases into three equal parts (i.e., highlights, body, conclusion). This is the commonly believed writing scheme in the press release, but how the equally divided pieces of text could be corresponding to each category? It would be nice to annotate some of the press release text and validate your hypothesis. \n\n\nIn Tables 3 and 4, why do think PART outperforms the BASE (or AB)? Is that because of the separation tokens? If so, it would be better to add another simple baseline to do the exact operation in a different way. For example, making two objectives (i.e., multi-task learning) for both the press release and paper abstract but sharing the internal representations of encoder and decoders might be one way. Basic separation of a long document by tokens is not multi-task learning. \n\nIn RA evaluation, the description of the current human evaluation on summarization is moved to Section D in Appendix. But, why? For me, that is the most important motivation and background of the RA method. But, once I read the Appendix, I still don\u2019t get convinced of the necessity of RA evaluation and the reason why the existing human evaluation is not feasible for your task. Similarly, there is no qualitative analysis (i.e., human evaluation) on the output summaries. Also, please consider moving a few output text into the main paper for better understanding. By the way, you use the acronym RA in the title of the section but the full name is firstly introduced at the end of the whole section. \n\nThe scores of RA in Tables 5 and 6 and the partial agreement of RA scores with ROUGE scores are not convincing at all to support the conjecture \u201cRA could be a good measure for the generalizability of transfer learning experiments in summarization\u201d. \n\nThe points discussed in the Discussion section sound reasonable and helpful but any shreds of evidence are not given, especially about the conceptual/logical accuracy.\n\n\n"}