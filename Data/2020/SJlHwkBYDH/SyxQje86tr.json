{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors apply the Nesterov Accelerated Gradient method to the adversarial attack task and achieve better transferability of the adversarial examples. Furthermore, the authors introduce a scale transformation method to provide the augmentation on the model, which also boosts the transferability of the attack method. Experiments are carried out to verify the scale-invariant property and the Nesterov Accelerated Gradient method on both single and ensemble of models. All experiments turn out to be a positive support to the authors' claim.\n\nHowever, one small drawback of this paper is that the author does not claim any comparison between the Nesterov Accelerated Gradient Method and other momentum methods (e.g. Adam, momentum-SGD, etc). This experiment is somehow important since it shows the better transformability is obtained from 1) Nesterov Accelerated Gradient Method only, or 2) all momentum method, which is significant for further research.\n\nAlso, in the setting of the Scale-Invariant Transformation, the authors forget to address that what if the attacked network has an input normalization. Does it mean to downsample the value of each pixel in the input image? If so, is the equation $S_i(x) = x / 2^i$ better to be $S_i(x) = [x / 2^i]$ where $[]$ means casting to the nearest integer? \n\nOne more question of this work is:  The Nesterov Accelerated Gradient method is known for its proveable fast descent property comparing to the traditional Gradient method. Do you observe any speed-up during your training? "}