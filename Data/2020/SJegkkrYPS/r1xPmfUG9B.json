{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper investigates methods to train neural networks so the final network has sparse weights, both in convolutional layers and in fully connected layers. In particular, the paper focuses on modifying the training so that the network is first trained without sparsification for a certain number of epochs, then trained to be increasingly sparse, and then fine-tuned with a fixed sparsity pattern at the end.\n\nWhile I find the overall approach of the paper interesting, currently the experiments are not systematic enough to derive clear insights from the paper. Hence I unfortunately recommend rejecting the paper at this point. I hope the authors find time to conduct more systematic experiments for a future version of the paper.\n\nConcretely, the following would be interesting experiments / questions:\n\n- How effective is the proposed training method on architectures other than ResNets?\n\n- What happens if the \"pruning era\" is made longer, started substantially earlier, or started substantially later? Currently it is not clear if the epoch 30 - 50 pruning era is (approximately) optimal and how much performance varies with begin and end of the pruning era.\n\n- Due to the small variation between some of the methods, it would be good to investigate how robust the ordering is when the experiment is re-ran with different random seeds etc.\n\n\nIn addition, I have the following suggestions:\n\n- The authors may want to remove or enhance the adversarial robustness evaluation. Currently the authors only evaluate robustness against FGSM, but it is well known that iterative attacks such as PGD are more effective.\n\n- Instead of \"intra-epoch pruning\" or \"intra\", the name \"combined\" may be more clear for the combined method.\n\n- In the description of the experimental setup, it could be good to specify what GPUs were used (since this lead to the smaller batch size).\n\n- It could be helpful for the reader to discuss how predictive results on Tiny-ImageNet are for results on ImageNet.\n\n- In Table 2, it would be good to add context by comparing to prior work with sparsity level 60% and some of the compression-focused methods from Table 4.\n\n- In the comparison to Mao et al. (2017), it could be good to clarify that they also work with ResNet models on ImageNet.\n"}