{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the question of model evaluation and reproducibility in machine learning research, specifically deep learning research, and designs and tests an extensive system for evaluating and comparing models. Users specify their evaluation parameters through a text file, and this is used by the runtime, which can be interfaced through a UI or the command line, in order to carry out the evaluation. Several experiments shed interesting light on various aspects of model, framework, and hardware performance.\n\nHypothetically, if I were to design a new model and wish to evaluate its performance relative to existing SotA models, I would potentially use this system to run all of the models, including my own. That would mean that I need to \"upload\" or otherwise integrate my model into this system, and it was unclear from my reading of the paper how easy such a process would be. Similarly, I would wish to maintain similar training and evaluation conditions for my model, e.g., the same pre and post-processing, and that would involve \"extracting\" those steps from the system for use during training. I would also like to understand whether or not this is feasible and easy given the system's design.\n\nIn section 3.1, the authors write \"The hardware details are not present in the manifest, but are user-provided options when performing the evaluation.\" An example of how this operates would be useful in the paper.\n\nAs far as experiments go, I'm not sure what the main takeaway is from section 4.1. To me, the takeaway that pre-processing is important and existing models are sensitive to pre-processing is not a new finding. The results from Table 1 could certainly be obtained without the use of the proposed system, and though there would be some scaffolding involved, I don't think that the coding would not be particularly difficult. Is the takeaway that the proposed system makes it easier or faster to evaluate the effects of different types of pre-processing? Wouldn't this be most interesting at training time?\n\nI find the experiments in sections 4.2 and 4.3 interesting. In section 4.2, I'm not sure if figure 9 includes enough information or description to conclude that \"GPU instances in general are more cost-efficient than CPU instances for batched inference\", and some more detail here would be useful.\n\nGenerally, I believe that the work is well-motivated and timely, the authors seem to have done a good job in citing related work (though admittedly I don't know much about this area), and the results are supportive of the claims of the system's usefulness."}