{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a unified approach to specify, evaluate and benchmark different ML methods.\nWith the main goal of enforcing repeatability and faireness when testing different methods, authors propose\nan open source runtime on which 1) specify the model, 2) describe the workflow and 3) evaluate the benchmark \nof several ML algorithms and frameworks.\nThe core of the work is the definition of the so-called \"model evaluation manifest\" which consists of a formatted\ncollection of descriptive information where both hardware/software and framework versions are specified, along with\nthe set of tasks to be carried on as well as the data sources to test the methods against.\nOnce the manifest has been created, the desired hw/sw configuration is deployed on Amazon and the specified models are benchmarked.\nThis benchmarking offers several insights on the evaluation of a given ML model, by stressing out the importance of aspects that can severely bias  the final outcome of the model (e.g., pre-processing tasks, different hardware configurations or normalization of the data).\nTo describe the workflow, authors use an image classifier on a given hardware as a running example, and play with different  preprocessing methods to measure their impact on the final accuracy of the model.\n\nSome details are not well specified/clear in the work:\n1) Data exploitation. There is the possibility of testing different methods on own datasets. Given that the deployment is run on Amazon instances, what are the requirements (e.g., data must be on S3 and so on). \n2) The manifest can be injected with python scripts that, running in a container, perform the desired operations (preprocessing). It is stated that \"parameters are passed by reference\". So if you pass a \"mutable\" object (\"env\", I guess) you need to bind it to the outher scope. How this is accomplished? (globals?)\nInstead, if you pass an \"immutable\" object (\"data\", I guess), you cannot rebind the outer reference nor mutate the object. So, what's the meaning of \"passing by reference\"?\n3) Privacy and anonymity. When performing debugging, system, framework and model level profiling information are collected on a tracing server. Is this server  part of the platform?"}