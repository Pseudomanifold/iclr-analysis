{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a variant of Adam, named as SAdam, and establish a data-dependent O(log T) regret bound. The key idea is using a faster decaying yet under controlled step size to exploit strong convexity. Some experiments are carried out to demonstrate the effectiveness of the proposed algorithm. The idea seems interesting, the writing is well-written, and the analysis seems correct (I did not fully check all steps, but the key steps seems ok to me). \n\nProbs:\n1. The proposed SAdam is an effective variant of Adam designed for strongly convex functions. The algorithm is a natural extension of Adam, and SC-RMSprop could be regarded as a special case.\n2. The authors establish a data-dependent O(log T) regret bound for SAdam, and as a byproduct, they present the first data-dependent logarithmic regret for SC-RMSprop. The authors also fix a small bug in the analysis of AMSgrad. The theoretical result is the key technical contribution of this paper.\n3. The experimental results shows that Aadam can be used to minimize strongly convex functions, as well as neural networks, which is believed to be non-convex.\n\nCons:\n1. As the authors mentioned in Remark 2, the main limitation of their analysis is that the role of the first-order momentum is unclear. Although the first-order momentum can accelerate the convergence in practice, proving this in theory remains an open problem. Is there some contribution on this aspect? \n\n2. The 4-layer CNN in Section 4.2 is a bit small. It would be better if the authors test their algorithm on larger and more popular neural networks.\n\nIn summary, this paper contributes the theoretical studies of ADAM-type algorithm, although the algorithm is somehow incremental. To me, a bit surprising result is that the step size originally designed for strongly convex functions also works well for training CNN.\n"}