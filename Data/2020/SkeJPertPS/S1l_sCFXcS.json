{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach to building random forests that are\nbalanced in such a way as to facilitate domain adaptation. The authors\npropose to split nodes not only based on the Information Gain, but\nalso so that the sizes of each set passed to left and right children\nare equal. Another extension to the standard random forest training\nprocedure is the use of a collaborative term subtracted from the\ninformation gain over the source domain. This term encourages\nalignment of the source and target domains in the leaves of trees in\nthe forest. Experimental results are given on a range of standard\nand open-set domain adaptation datasets.\n\nThe paper has a number of issues:\n\n1. There are some problems with clarity, and the English is somewhat rough\n   throughout. These problems are not terribly distracting, but the\n   manuscript could use more polish.\n2. I don't see a detailed discussion anywhere about the\n   hyperparameters used for fitting the random forests. How many trees\n   are used? What is the max depth? These parameters should be\n   discussed and included in the ablations in order to appreciate the\n   complexity/performance tradeoffs.\n\nThis paper has some interesting ideas in it, and the experimental\nresults are excellent. I would encourage the authors to move salient\nmaterial from the supplementary material to the main article and to\nprovide a more thorough discussion of the complexity of the models\n(the structural parameters of the trees/forests)."}