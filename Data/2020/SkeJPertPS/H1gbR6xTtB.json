{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper introduces a method for domain adaptation, where each domain has noisy examples. Their method is based on a decision tree in which the data at each node are split into equal sizes while maximizing the\ninformation gain.  They also proposed a way to reduce domain alignment. Their method is tested on several noisy domain adaptation settings and performs better than other baseline methods. \n\nPros:\nTheir idea to utilize a decision tree for domain adaptation sounds novel. \nExperiments indicate the effectiveness of their method.\n\nCons:\nThis paper is not well-written and has many unclear parts. \n1, The presentation of the problem set is unclear throughout this paper. In the abstract, they mentioned that they tackle the situation where both source and target domains contain noisy examples. However, they did not define the exact problem setting in any section. I could not understand what kind of problem setting motivated their method, which makes it hard to understand their method. \n2, How they actually optimized the model is also unclear. From Eq 1~4, it is hard to grasp how they trained the model. \n3, In open-set domain adaptation, simply minimizing domain-distance can harm the performance. How does the method avoid this issue? It was also unclear. \n4, Experimental setting seems to be wrong and unclear. In Openset1, they say that \"The labels from 1 to 10 of both source and target domains are marked as the known class, and all data with label 11\u223c20 in the source domain and label 21\u223c31 in the\ntarget domain are used as one unknown class\". However, Saito et al. (2018) used 21-31 classes in the target domain as one unknown class. In addition, \"According to Saito et al. (2018) the target data of the unknown class is not used in training, \", they used the 21-31 classes for training in an unsupervised way. How is this method used to detect unknown class? Is there any threshold value set for it?\n5, The experimental setting is unclear. In 4.4, \", we use only 10% of training samples\", does it mean 10 % training source examples or target examples? This setting is also unclear. \n\nFrom the cons written above, this paper has too many unclear parts in the experiments and method section. I cannot say the result is reproducible given the content of the paper and the result is a reliable one. They need to present more carefully designed experiments. \n"}