{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a method for converting sentence pair classification tasks and sentence regression tasks into span into span extraction tasks, by listing all the possible classes (entailment, contradiction, neural) or the discretized scores (0.0, 0.25 ...) and concatenating them with the source text. With this formulation, one can train a BERT-based span-extraction model (SpEx-BERT) on classification, regression, and QA tasks without introducing any task-specific parameters. The purposed SpEx-BERT model achieves moderate improvement (0.3 points) over the BERT-large baseline on the GLUE test set when fine-tuned on intermediate STILTs tasks (Phang et al., 2018).\n\nStrengths:\n- Extensive finetuning/intermediate-finetuning experiments on a range of NLP tasks.\n- The paper is mostly well-written and easy to follow.\n\nWeaknesses:\n- This paper presents a lot of experiments. But it seems that the most useful / head-to-head comparison against the BERT model are the last 2 rows in Table 2 with the GLEU results, where the improvement is moderate.\n- The idea of expressing various NLP tasks (including textual entailment and text classification) as question-answer has been well-explored in decaNLP (McCann et al., 2018). It would be nice if the authors could elaborate more on how the proposed method differs from theirs.\n\nOther comments/suggestions:\n- Likely typo in abstract: \"fixed-class, classification layers for text classification\"\n"}