{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work presents an autoregressive model for generating symbolic music.  Unlike existing models for symbolic music generation, this work explicitly models the musical hierarchy of pitches, chords, voices, measures and movements. Rather than letting a model learn these concepts directly from symbolic music, this model builds in this hierarchy of concepts as a prior. Overall I think the presented model is very interesting, however I found the model description extremely difficult to follow and it would be hard for a reader to try and reproduce this work independently since there are many machine learning and music ideas at each level of the encoder/decoder stack, and not all of them are presented in enough detail to be understood clearly. \n\nSection 2 briefly mentions ancestral sampling and the NADE model. It also references the CocoNet model which is trained to reconstruct partial scores. However, its not clear to me what this discussion has to do with the objective function used for training. What does x correspond to in the hierarchy of musical objects considered in this paper? Is it a measure? Given that that is the objective used to train the network, the connection with the encoder-decoder structure needs to be more clearly and explicitly stated. \n\nThe paper mentions that they use the pitch embedding from Yan et. al. (2018), however they don\u2019t provide any details for how these embeddings are calculated, what their dimensionality is etc. From Section 3, its also not clear how many pitches are considered. Predicting the pitch and tie values uses 2 layer MLPs with an output softmax, but its not clear what the sizes of the MLPs or the output softmax layers is for each case. It would also be helpful to illustrate the duration encoder with a figure. The text states that during training, the order of pitches in a chord is randomised to account for the fact that chords are unordered sets of pitches (and ties). Does this turn out to be sufficient? What is observed at inference time which the chord decoder? The note embedding is an element-wise product of the pitch and tie embedding. I don\u2019t follow the argument that the multiplication is helpful in \u201cbreaking the linearity of the sum aggregation used by the chord encoder\u201d. Why is this necessary and does it turn out to be crucial? What happens if the embeddings are simply added? \n\nI found it difficult to follow the description of the duration encoding. The pitch, chord and duration encoder descriptions would greatly benefit from a figure/schematic/block diagram showing explicitly how the score in musicxml format is first processed and then how each of these objects is processed via the encoders. \n\nIn the voice encoder, a bidirectional GRU is used to process the chord-duration tuples. If the objective function uses an auto-regressive structure then wouldn\u2019t the bidirectional GRU have access to events that it shouldn\u2019t see? From the discussion in Section 2, its not clear how this masking is performed and how the objective function is set up during training. \n\nThe evaluations are in the form of a listening test, however the subjective results are not very inspiring. Do the authors plan to release any code for this work? \n\nOverall, I found the description of the proposed model to be difficult to follow. I think each of the components would benefit greatly from more figures and block diagrams. The lack of associated code for the model make this work quite hard to reproduce. I am also not quite sure exactly what the training objective is, though I understand its somehow related to the NADE and CocoNet. I think the paper should be reworked with special attention to making the low-level details more explicit. \n"}