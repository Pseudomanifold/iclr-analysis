{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a method for generating polyphonic music in a hierarchical manner. The hierarchy is inspired by the hierarchy used in scoring western music. \n\nOverall I like the idea and I think it has merit. However, the writing and exposition of the author's methods still requires a lot of work, as it is rather difficult to follow how the different parts of the system work together. Finally, the subjective listening tests report a like ratio of 17.7% for the model's pieces, compared with a 40.5% ratio for pieces composed by humans. Although I appreciate their honesty and wouldn't expect the model to outperform trained human composers, it would be good to find a better metric to measure the model's performance. The audio samples provided seem reasonable, but if one were to only look at the like ratio, there's no indication that the model produces anything reasonable.\n\nSome more detailed comments below:\n- In the abstract you say \"capable of generating long polyphonic music given number of measures, up to hundreds of measures.\" What does this mean? Are these measures given to the model to condition it, or that the model can generate hundreds of measures?\n- First paragraph of background: \"if all conditional distributions are compatible\". What does it mean to be compatible?\n- In Figure 2, it would be useful to have the RHS as a concrete example relative to the score on the LHS.\n- Top of page 4: \"c is the input to the decoder\": what does this c represent? what are its dimensions?\n- Pargraph on page 4 starting with \"All decoders are autoregressive models:\" is not at all clear. \"lower level\" is used five times, but it's not clear what it means.\n- Sec. 3.1.1: Does your representation have fixed quantization of measures? Otherwise, why do you need the ties?\n- All of sec 3.1.1 is very hard to follow. A diagram would be useful.\n- Duration encoder: \"every possible digit\": it's not clear what you mean by digit.\n- Duration encoder: \"all digit embeddings for all prime bases\": what does \"all digit embeddings\" mean? seems like a really large number.\n- Duration decoder: What do you mean by \"we consider the dependency between these digits\"?\n- Sec. 3.1.3: \"a sequence with each step\": what does \"step\" mean in this context?\n- Within-beat position signal as extra input: \"whose value is wrapped to [0, 1)\": what does this mean exactly? Can we have two notes occurring at distinct times within the same measure, yet still map to the same b_i? Why is that desirable?\n- Cross-beat note splitting: \"a note crosses multiple beats, which bypasses circles of the positional signal.\" It's not clear what this means.\n- All of the \"Voice encoder\" and \"Voice decoder\" sections are really hard to follow, again, a diagram would help a lot. Some issues:\n  * what are the Ws? they've never been introduced!\n  * \"after summing up memory matrices...context get aggregated\": where is this happening? what do you mean by synchronous measures? multiple voices?\n  * Again, it's not clear what c is, nor what M is.\n  * What's LayerNorm?\n  * \"implements the last term on the RHS of the equation 9\": how?\n  * \"chord decoder which implements the first term on the RHS of the equation 9\" how?\n- Sec 3.2: \"then the model attends back into the memory matrix\": what does this mean?\n- Sec 4.1: \"Our dataset has 893 pieces\": how many total measures?\n- Sec 4.2: You used letters to refer to many of the hyperparams discussed here, please use the same letters here.\n\nMinor corrections to improve the writing:\n- Abstract: s/we construct a inter-measure/we construct an inter-measure/\n- \"Music, largely, is a form of art with no tangible existence\" is a weird way to start the paper...\n- Intro: s/more effortless/less effort/\n- Bottom of page 1: s/However, as the rhythm becoming more complex/However, as the rhythm becomes more complex/\n- Below Fig 1: The refernce \"(ran, 2004)\" seems incorrect, it doesn't appear in the bibliography.\n- \"instead of a simple word embedding lookup\", they're not really simple...\n- Top of page 4: s/it is omitted from the our model/it is omitted from our model/\n- Sec 3.1.1: s/a chord is a set of pitch-tie tuple,/a chord is a set of pitch-tie tuples/\n- Duration encoder: s/we directly encodes/we directly encode/\n- Cross-beat note splitting: s/making human musicians easy to count/making it easier for human musicians to count the beats/\n- Sec. 3.2: s/representation, a plenty of neural/representation, plenty of neural/\n- Sec 4.2: s/The final loss is then a average/The final loss is then the average/\n- Sec 4.3: \"with measure length 32, 64, and 128, respectively\": you don't need \"respectively\" here.\n- In Fig. 4, the label \"Figure 4\" is missing. Also, place the legend in a space where it's not blocking the important pieces of the plot (in fig (b) in particular)."}