{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This manuscript proposes a new approach to fitting Markov Random Fields (MRFs).  The general structure of the algorithm is amenable to many MRF structures and can be fairly straightforwardly applied to learning on a wide variety of problems.  The theoretical analysis supports that the algorithm is reasonable.  Experimental results show strong results on several different MRF models, albeit on relatively small problems.\n\nI am giving this manuscript a weak accept.  The approach, to me, seems novel in fitting MRFs.  However, the theoretical claims and their limitations need to be more realistically discussed, and the empirical results need to be shown on larger and more complex datasets.\n\nFirst, on the empirical results, there is a large literature on fitting RBM models, including many on scaling to much larger models.  Given that AdVIL actually diverges on the logZ estimation as the number of iterations goes up makes me worry about the efficacy of this approach on larger RBMs (Figure 2 uses v=64 and h=15 while a common RBM on MNIST is v=784 and h=500, a huge difference in scale).  As a large literature shows that estimating partition functions or normalizing constants gets much harder as the dimensionality goes up, I worry about this strategy and I think the manuscript would be greatly enhanced by looking at more common model sizes from the ML literature.\n\nAlso, to me, the classic MCMC+SGD is as much a black box as the proposed technique.  I realize that many of the top performing MCMC adapt specifically to the problem, but much of this is transferable between systems and can be put in simple sampling schemes.  Table 1 should be updated to include these typical techniques, because it is not clear that the proposed system actually outperforms the typical PCD-1 scheme (especially given in Figure 3).  Or succinctly, it should be made clear why I should use this over an MCMC approach.\n\nTo address these concerns, I would like the authors to answer how their proposed algorithm works in larger MRFs and do a more complete analysis compared to more traditional strategies.\n\nSecond, the theoretical claims are nice, but the manuscript should be revised to address the limitations of the theory.  In particular, Lemma 1 is extremely strong, and I disagree with the assessment that it is \"much weaker\" than the typical nonparametric assumption.  It seems that as the optimization gets close to the solution, this is essentially the exact same condition.  The authors need to clarify how exactly this is different, and dive into the practical implementations.  This also seems like it would get increasingly difficult as the number of hidden and visible units increases, so they should address how this Lemma holds as the theory scales.\n\n"}