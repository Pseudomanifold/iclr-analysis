{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The work proposes using variational distributions to model the model the inference of latent variables and model the partition function building on NVIL, thereby providing an algorithm that would work on general MRFs for both inference and learning. Since the two terms in the NLL are opposite in sign, it is a minimax operation and GAN like adversial training can be used. The paper shows providing tighter results to estimate the log partition function and comparisons on the digits dataset and Anneal importance sampling.\n\nThe paper builds on NVIL by using two variational distributions for the NLL and how to solve the parameter estimation problem. I think this strategy can be tested more extensively on more types of general MRFs and more rigourous experimentation and that the community will benefit from reading from these ideas. The paper contains some theory behind the work and experimental analysis of Advil including the sensitivity to parameters. Advil shows promise compared to the competing methods in some of the problems.\n\nQuestions and suggestions for improving the paper\n1. Is this applicable to multivalued nodes or just binary problems? Or are any modifications needed?\n2. Inference can be done in general using approximate methods like variational message passing, QBPO among others that don't depend on graph structures either, how does this work compared when those algorithms are used with simple gradient descent while training the parameters?\n3. The paper states that the algorithm is convergent if the variational encoder approximates the model well. How would you define good approximation?\n4. It would be good to add more details of how the GAN framework/adversial training is used.\n5. Would it be possible to compare Advil to ALI in some of the experiments?\n6. Fig. 2 d was not clear to me as why to expect the plot we see. The NLL flattens out with no progress.\n7. It would be helpful to the reader to understand the comparison using persistent contrastive divergence. Why is it not used in other experiments. The paper says the main comparison point is NVIL but different experiments either mention ALI, VCD or PCD which is confusing.\n8. It would be nice to see the time comparison between this learning parameters and other methods.\n\n\n\n"}