{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a solution to the large scale query-document retrieval problem. The proposed method was shown to be a better alternative to the classic information retrieval approach such as BM-25 (token marching + TF-IDF weights). The proposed method is based on two separate transformer models which has computational benefit over one cross-attention model. For fast training, they have also used the sampled softmax. For pre-training tasks, Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki LinkPrediction (WLP) were studied.\n\n\nThe paper is written well, easy to follow and well-motivated. However, there is a major technical problem in the proposed method. In the proposed approach, the query embedding (q_emb) and the document embedding (d_emb) train separately by two transformer models (two towers --- Query-tower and Doc-tower). After that, the similarity was measured through a dot product. Two embedding models are, therefore, represented by separate vector space representation. Applying dot product to find the similarity does not make much sense to me, as the embedding is not comparable in two different vector representations. \n"}