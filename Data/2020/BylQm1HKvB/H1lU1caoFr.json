{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a framework for emergent communication system using internal reflection, similar to obverter technique in Batali 1998 and Choi et al., 2018, while not requiring explicit supervision. Despite the nice idea, the dire quality of writing makes it extremely difficult if not impossible to understand the proposed model and the results. Also, some of the experimental results seem unnecessary. In Table 3., for example, (1) in \u00a74.4.1 the authors merely list the rules without giving an overall conclusion. (2) I'm not certain what's the significant of the results in Table 3., as these are messages from one trained instance, and I'm sure a different training run will yield different messages.\n\nAlthough I am strongly leaning towards reject, I am willing to discuss with my co-reviewers if they have a different opinion.\n\nCons\n- There doesn't seem to be a proper baseline in any of the experiments conducted.\n- The writing generally is unclear, full of ungrammatical sentences and extremely hard to read: e.g. see examples below. \n     - \u00a73.2: \"A language system can emerge by a pure machine, excluding the human viewpoint.\"\n     - \u00a73.2: \"The structure, characteristics, and limitations of the messages generated by the machine can be determined.\"\n     - \u00a73.2: \"The agents as a whole have a movement to create a system in which languages is unsupervised and one conceptual pact is formed.\"\n     - \u00a73.3: \"Receive an image and generate a message based on self-knowledge.\"\n     - \u00a73.4: \"This formal rule was from the human perspective\".\n\n- The evaluation section is very rambling and contains unnecessary parts that need not be included, e.g. convergence of mean squared error loss between GRU output and latent space z.\n\nQuestions\n- The internal reflection function part, the core of this paper, is not properly explained until the end. For example, in \u00a73.3, \"when the comparison result is lower than the threshold value\": what is the \"comparison result\"? How does one compare a message and another message? Please elaborate on this further.\n- In the evaluation section \u00a73.4, the authors keep referring to \"dream\": \"correct answer rate of Dream\", \"We entered the message into an agent to evaluate the reconstructed Dream\". But the authors don't provide a formal definition of \"dream\" except providing a citation to Ha and Schmidhuber, 2018. Are the readers expected to know what this means? \n- In \u00a73.4, \"Jaccard coefficient were applied to test the message similarity between labels\". How does one apply Jaccard index to compute similarity between labels? This seems an important part of evaluation but not explained at all.\n"}