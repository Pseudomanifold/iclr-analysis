{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tackles the problem of learning optimal policies robust to system failures. This problem is modeled as an MDP in which there is some unknown process (potentially an adversary) which may terminate the agent at any point, and exact a termination penalty on this agent. The paper proposes to approach this problem as a two-player game. The paper provides a number of theoretical arguments to show that the two-player game may be solved via a value-iteration approach.\n\nOverall, the paper lacks sufficient significance (there are no demonstrations of the practical performance of the algorithm) to merit publication in its current state.\n\nAdditional comments:\n-- The ability of the adversary is very unclear to me.  Is it Markovian?  I don't think so, because then the back-up value iteration approach would not work.  The use of \\Omega after Eq 2 is also unclear.  Can the authors please clarify the mechanism in which the adversary operates?\n-- The paper lacks practical experiments.  I encourage the authors to either submit to a more appropriate venue or add experiments to the paper."}