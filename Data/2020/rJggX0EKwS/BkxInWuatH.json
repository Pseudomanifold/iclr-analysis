{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper studies the norm of hidden activation of each layer and the norm of weight gradient of each layer for deep ReLU neural network. By using concentralization property of the random initialization, the paper derives their expected values and high probability range when the network width is sufficiently wide. The results are correct and the paper is easy to follow. However, the result has been given in previous work. I do not recommend the acceptance.\n\nThe result presented in this paper has been covered by a recent work [1]. Please refer to Section 7.1 for the forward part and Section 7.3 for the backward part.\n\n[1] Zeyuan Allen-Zhu, Yuanzhi Li and Zhao Song. A Convergence Theory for Deep Learning via Over-Parameterization"}