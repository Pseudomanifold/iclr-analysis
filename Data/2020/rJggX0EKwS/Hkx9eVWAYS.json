{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows that under He initialization and for sufficiently wide network, (1) the norm of the activations of a L layered ReLU is preserved w.r.t the input across layers and (2) the norm of a weight matrix gradient at different layer is only dependent on the norm of the top-layer error and the input, because the norm of back-propagated gradient is approximately preserved. \n\nThe paper is clearly written and easy to read and the proofs are quite straightforward. That being said, the results are not surprising and from my point of view, the overall novelty of this paper is a bit marginal for top-tier conference like ICLR. \n\nThus I vote for rejection. "}