{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies initialization techniques for deep ReLU networks from a theoretical standpoint and derives finite layer width concentration bounds to show that with the He initialization scheme, deep ReLU networks preserve the norm of the input sample during a forward pass and the norm of the gradient with respect to the output during a backward pass. The concentration bounds also suggest lower bounds on the width of the ReLU layers. The authors verify their theory with experiments on synthetic data.\n\nWhile I believe the finite sample concentration bounds for networks initialized using the He initialization are valuable, it seems to me that this work is incremental in terms of understanding the He initialization. The techniques used in the paper are also fairly well known Chernoff bounding techniques and concentration of Gaussian random vectors and matrices.\n\nThe authors claims on explaining overparameterization are also overstated in my opinion. While the authors are able to obtain a lower bound on layer widths in order to preserve norms during forward/backward passes at initialization, the lower bound is only dependent on the input dimension and not the size of the dataset, which is the relevant quantity to decide whether a model is under/over parameterized. Even in the authors' bounds for finite datasets, what I can surmise from their results (it would be better to explicitly state it if that is one of the goals of this paper) is that the width of each layer needs to be atleast log(N) where N is the size of the dataset. This is hardly overparameterized. \n\nFurthermore, the authors do not explain how studying the properties of the initialization might help understand generalization at minima. Since gradient descent based techniques seem to prefer solutions that are close to initialization, the analysis in this paper might be a useful starting point in understanding generalization.\n\nThe authors could also consider how adding BatchNorm layers and/or Residual connections affect the He initialization scheme, and whether initialization matters for those techniques. Finite width concentration bounds for initialization of networks using Batchnorm/Residual connections could be useful. \n\nTo summarize, I do not see how the authors claims about explaining overparameterization (even at initialization) can be made. Without those claims the contribution of this paper is incremental and does not warrant publication at this time. I am willing to adjust my score if the claims about overparameterization are stated explicitly and make sense."}