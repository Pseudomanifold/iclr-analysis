{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work considers random parameter initialization in neural networks (In particular the initialization presented in He et al.) and develops non-asymptotic bounds for the norms and gradients of neural networks during initialization. The authors show that the norms of the outputs and gradients (for gradients, under a different assumption on the dimension of the matrix) remain constant through the different layers. The results presented differ from previous work in that they give nice concentration bounds for such output and gradient norms. In addition the authors prove results in the case of infinite samples under the assumption that they arise from a finite dimensional space.\n\nOverall the presentation is clean, but the results presented have the following issues.\n\n1.Very similar in nature to previous results, for example the results presented in [1] Theorem 5.4 give very similar concentration results and use very similar mechanisms.\n\n2. All the results for the infinite stream coming from a finite dimensional subspace do not address the fact that the training points usually do not lay in a linear subspace of dimension d << n. Further, there is a stringent requirement on d in such results and they fail to hold for even \"fairly\" small d.\n\n3. In Theorem 2, the argument that the output through a layer of a rank-d linear subspace remains within a rank-d linear subspace does not seem correct, is it possible that it remains in the union of subsets each of which lies in a subspace of rank d?\n\n4. The proofs for the gradients make assumptions that deviate from the initialization previously introduced. The work of Glorot et al. for example discusses the tradeoffs between the two assumptions and suggests the balancing between maintaining the input and output variance distributions.\n\nDue to the following issues I choose to reject this work at this time.\n\nBelow are additional minor typos or issues in the paper.\nOn page 6, the sentence that starts with: \"it is beneficial for the width\": Suggesting to use neural networks of constant width seems a bit impractical.\n\nThe citation of Arpit et al. from 2017 seems possibly wrong since the paper cited discusses memorization and does not focus at the initialization of neural networks.\n\n\nTYPOS:\npage 1 last sentence before equation 1:  back backward? \n\npage 1 last sentence: repeated words: as as the the\n\npage 1 last sentence: property --> properties?\n\npage 2 section 2 paragraph 1: both these papers --> both papers, both of these papers?\n\npage 4 sentence after the proof sketch paragraph: the sentence is difficult to read\n\n[1] \"Stochastic gradient descent optimizes over-parameterized deep relu networks\", 2018, Difan Zou, Yuan Cao, Dongruo Zhou, Quanquan Gu\n\n\n"}