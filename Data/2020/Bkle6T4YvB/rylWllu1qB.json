{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this work, the authors propose a way to transfer a pre-trained English BERT model to a new language within a short amount of time. The key insight is to map English embeddings to the foreign language and have separate embeddings for both English and the foreign language. The resulting bilingual LM is evaluated for zero-shot transfer learning on two tasks: XNLI and dependency parsing.\n\nPros:\n- The authors provide good details into their hyperparameter settings and about how the obtain the foreign language word embeddings.\n- By leveraging existing pre-trained models, they\u2019re able to do pre-training for their bilingual LM within 2 days.\n\nCons:\nI find that a key comparison point in this paper is missing, which is Bilingual BERT trained on just the two languages that are being considered for their RAMEN system. This is not a fair comparison while mBERT which is trained on 100+ languages is not.\nAll comparisons are not fair since a simple baseline of just training mBERT on two languages with monolingual data and with a shared WPM is not evaluated here.\nThe proposed system has an unfair advantage over mBERT since it\u2019s initialized from BERT/RoBERTA and fine-tuned only on two languages. Hence most of the parameters are used for just the two languages while mBERT uses the parameters for 104 languages.\nGiven this unfair comparison, I\u2019m not sure if we can draw a meaningful conclusion from all the experiments. \n\nRating justification:\nGiven the lack a fair comparison between the bilingual and multilingual BERT models, I don't think the conclusions are insightful.\n"}