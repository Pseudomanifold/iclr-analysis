{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "*Summary of the contributions:*\nThis paper deals with convergence, of the hidden layer of a 2-layers Relu Network trained with Weight Normalization which consists in decoupling the direction and the magnitude for the pre-activation layers. The authors show, under mild assumption, the linear convergence (with high probability) of the training loss in the over-parameterized setting.\n\nConceptually the result says that using random feature selection (the last layer is fixed, initialized with Rademacher random variables), with a large enough network, the hidden weights do not need to move much from their initialization to get arbitrarily small training loss. \nThus, the analysis of the whole dynamics is reduced to a perturbation analysis of the dynamics around the initialization. (I would like to emphasize that even though the final argument ends up to be a \u2018simple\u2019 strong convexity convergence proof, I acknowledge that the proof of such result is far from being straightforward.) \n\nThe contributions of this paper are the following: in the setting introduced in [15] (the ref numbers correspond to the ones in the paper) the authors prove the linear convergence of the training loss for a large enough output layer of a 2-layer Relu network with Weight Normalization. Regarding to [15] the difference is that the authors study Weight Normalization. They are able to get better bounds on the over parametrization, and provide an analysis that highlights two training regimes: a fast one (V-dominated) and a stable one (G-dominated) that could explain the benefit of weight normalization. \n\n\n*Decision:*\nI am leaning for a weak reject for the following reasons: \n1. The main issue to me is that this work builds up on the work by [15] without addressing the issues risen by [12]. One important takeaway from [12] that the \u2018lazy training\u2019 regime studied might not be a desired behavior and might not happen in practice. Thus, I think that extending the work form [15] to WeightNormalization would be interesting if the phenomenons described by the theory are backed-up by some practical evidence.\nFor instance : \u201cwe believe that G-dominated convergence actually is common but that it emerges at later stages in training, after the weights have adopted their directions in the V-dominated regime and improves stability.\u201c is something that the authors could check in practice. \n\n2. I found the discussion regarding the phase transition a bit unclear. The magnitude of the weights v_k is treated as it was alpha the coefficient used to parametrized their initialization. Then since the phase transition depends on the magnitude of \\alpha, it is argued that the model naturally transitions as \\|v_k\\| increases (which is the case because of the discrete gradient updates).\nHowever, to me to make that phase transition argument you would need to show that the scaling of the matrices at time actually depends on the norm of the current iterate v_k(t)\n\n3. Finally, regarding the proof of Theorem 4.1 and 4.2, I found it way more clear in [12]. There is a key subtle bootstrap argument that is not explicitly stated in the proof of Theorem B.1 and B.2: The condition R\u2019 < R  (stated in Lemma 3.4 in [12]) is the key argument that leads to a sufficient bound on the overparametrization. \nI put some remark regarding the clarity of the proof in the minor comment section. I think it would improve the quality of the work but did not have a major impact on my grade. \n\n4. The presentation is slightly misleading. The fact that the last layer is not optimized is slightly mentioned once and the author make it appear just like is was done without any loss of generality  \u201cWe consider the case where the ck\u2019s are fixed. This does not alter the capacity of the network since the gks are trainable (ck and gk are interchangeable analytically).\u201d. Moreover, the omission that Theorem 4.1 and 4.2 are only true with high probability makes even that fact even more unclear. I think that this aspect (last layer is fixed) should be clearly emphasized in the revision. \n\n\n\n*Questions:*\n\u201cwe believe that G-dominated convergence is common\u201d, can you provides evidence that G-dominated and V-dominated  stages occurs the way you describe it in the discussion ? \nSame for the claim \u201cThe direction of the weights changes rapidly at the earlier stages of training when \u03b1 is small, and G-dominated convergence ensues as \u03b1 grows, leading to improved stability.\u201d Particularly the link between your theory, the norm of v_k and alpha. (c.f point 2. of the decision section)\nI think that it is important to emphasize that in your work c_k are randomly sampled and then fixed (it is much more clear in [15], but I do not think that the reader should know the related work to realize that). You seem to argue that it is without loss of generality when you say \u201cWe consider the case where the ck\u2019s are fixed. This does not alter the capacity of the network since the gks are trainable (ck and gk are interchangeable analytically).\u201d\nbut this statement seems wrong since there is a relu activation between c_k and g_k (thus you cannot interchange negative c_k with negative g_k). Can you comment this ? \nCan you clarify the bootstrap argument mentioned in section 3. of the decision section ?  More precisely, can you justify the sentence \u201cthis contradicts one of Lemmas B.7, B.8 at time T_0.\u201d in the proof of Theorem B.1 ? \n\n\n\n*Minor comments: (do not need to be addressed in the rebuttal)*\n- In the proof of Theorems B.1 and B.2 the statement \u201cclearly T_0>0\u201d should justified. This statement is true but a continuity argument that should be stated. However, the dependance in t could be easily avoided using the proof technique from [12] (Lemma 3.2) where the result is actually stated for any weight close enough to the initialization.  \n- The title of [12] has changed and one author is now missing. (since you cite the NeurIPS version you should then cite the right title with all the authors) \n- I would not say that  \u201csmaller alpha leads to faster convergence\u201d since the optimal step size depend on \\alpha. \n"}