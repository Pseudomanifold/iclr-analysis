{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks. Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. \n\n+The experimental results are impressive. The trained model is robust (at Madry\u2019s PGD level), and the total training procedure is fast (6 min for CIFAR-10 and 12 hr for ImageNet).\n\n+ The method is simple, and I guess reproducible. \n\n+The paper shows surprising facts of a well-known method.\n\n+The paper is generally well-written and easy to follow.\n\nI do have some concerns of the work\n- The paper is empirical and the techniques are combinations of previous methods. Even for the surprising fact that Random+FGSM, it has been discussed in several previous papers, for example,  ICLR 2019 Defensive Quantization: When Efficiency Meets Robustness  https://openreview.net/forum?id=ryetZ20ctX. So the main contribution of the paper is limited to show RFGSM works well when combined with optimization tricks like cyclic learning rate. \n\n-In previous methods claiming random+FGSM can train robust model, their method seems to be slightly different from Alg 3 in page 4 of this paper. The alg in this paper seems to be identical to Madry\u2019s implementation of R-FGSM, which is shown not robust to PGD attacks. See discussions in https://openreview.net/forum?id=rJzIBfZAb and https://openreview.net/forum?id=ryetZ20ctX. I would like the authors to clarify their method to resolve such conflicts and make it clear how R-FGSM can be as robust as PGD as in table 1. \n\n-The first two paragraphs of section 4.1 seem to be inaccurate. One important trick in the \u201cadversarial training for free\u201d paper is to replay each minibatch m times. It is hard to say how much nonzero initialization helps. According to \u201cuniversal adversarial training\u201d (https://arxiv.org/pdf/1811.11304.pdf). It may help, but cannot compete with Madry\u2019s PGD training when defending against PGD attacks. \n\n-I am not sure if using a larger norm 1.25 * \\epsilon is a fair comparison. A baseline of PGD training bounded by 1.25 * \\epsilon would help. \n\n-Could the authors combine table 4 and 5 for easy comparison of robust accuracy and training time? Did the authors try the optimization tricks on ImageNet for the baseline free adversarial training method?\n"}