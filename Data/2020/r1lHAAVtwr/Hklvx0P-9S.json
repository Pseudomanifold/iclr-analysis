{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "## Summary\nThe paper tackles the problem of promoting diversity in the weights of deep neural networks. The problem is interesting and useful. The paper argues that hierarchical learning and hyper spherical learning are important in addressing this problem. The paper provides experiments on CIFAR-10 and CIFAR-100 where the improvements of using such regularization is visible but not sufficiently significant.\n\n## Contribution of the paper\n1. The paper proposed a regularization to training neural networks with discrete angular distance metric on the weights.\n2. The paper shows improved performance on CIFAR-10 and CIFAR-100.\n\n## Overall feedback\nI found the paper is well motivated and the proposed approach to be interesting. But I found the experimental validation a bit confusing. The improvments of the proposed approach also seems quite marginal. The contribution of different regularization terms is not understood clearly as well. So I am leaning towards rejection.\n\n## Detailed feedback and questions for rebuttal\n1. The writing could be improved significantly. I had a hard time to find exactly what different regularization terms are, e.g., E, H, L2. The paper could be more clear by clearly stating these regularization equations.\n2. Please capitalize \"eq. (1)\" to \"Eq. 1\".\n3. It seems there are three regularization E, L2 and H. But different tables show different combinations. For example, Table 1 has E and E+l2 while Table 2 has E and E+H and Table 3 has only E+H. Can you provide full results on all datasets on E, E+l2, E+H? Without seeing the full results it is hard to draw any conclusions.\n4. Please correct the text \"resnet-100\" to \"resnet-110\" assuming you are using resnet-110.\n5. It seems E+H improves marginally over E. Can you elaborate the explanation about it?\n6. Why E+l2 improves so much (+2%) on CIFAR10?"}