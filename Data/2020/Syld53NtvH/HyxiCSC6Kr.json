{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes some expected bounds in order to improve the robustness-accuracy trade-off on standard CNNs. The bounds numerically improve (Gowal et al), thus this method works numerically well.\n\nMy main concern is about the fact that several assumptions seem relatively constraining or are not quantitatively justified. Furthermore, I'm wondering if such bounds could be applied to Neural Tangent Kernel works. This could be useful and it would seem to correspond better to the setting of this work.\n\nPros:\n- The performances are quite good.\n- In expectation for random NNs, the proposed bounds are tighter.\n- The beginning of Section 4 conducts an interesting numerical study that tries to validate those bounds.\n\nCons:\n- Some assumptions seem pretty unrealistic to me. For instance, the fact that the neural network should have Gaussian i.i.d weights: it is thus surprising that this technique works in real-life settings. As far as I understood, this assumption is not done in (Gowal et al). Did the authors check that their CNNs had Gaussian weights? I'm not convinced by the paragraph at the end of section 3.2, yet a simple histogram to validate this claim would convince me.\n- A major difference with (Gowal et al) is that: here, the bounds are in expectation whereas the bound in (Gowal et al) are deterministic. In this paper, there are some approximation assumptions (e.g., large input, large number of hidden layers) without non-asymptotic arguments. For instance, I was sort of expecting some concentration inequalities that would allow to quantify how large should be the input dimension $n$ or the width $k$.\n- Most of the proofs are long and complicated, and several equations of 7-8 lines could be summarized with up to 2-3 lines maximum. I had a hard time to understand the proof of Theorem 1, which is simply some algebra. This could be improved.\n- I'm curious of the imagenet performance: couldn't this technique be easily applied to AlexNet, which is nowadays simple to manipulate? Is there a technical issue to do so?\n- Is the assumption 1 really an assumption? given a,a' and b,b' there always exists m such that a>=b-m and a'<=b'+m, like m>=max(|a-b|,|a'-b'|). Am I wrong? I think I do not understand this assumption...\n- How simple is it to extend those theoretical results for NNs to the case of CNNs?"}