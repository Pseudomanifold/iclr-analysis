{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper conducts extensive experiments of Lipschitz regularization on GAN models with different loss functions and observes that typical GAN setups are insensitive to the choice of Lipschitz constant. And it claims that Lipschitz regularization just scales the neural network output. What's more important in GAN training is to restrict the domain and interval of attainable gradient values of loss functions, for which the paper proposes so-called ''domain scaling'', because it can avoids the gradient vanishing or exploding problem. \n\nQuestions:\n1. I get confused with the domain scaling (11). If we use very small alpha, the M in Corollary 1.1 will correspondingly become very large (reciprocal relation)? Then how can domain scaling restrict the interval of attainable gradient values?\n\n2. I think the narrative of the paper is not very good and thus confusing about the most significant point of it. There are too much trivial derivation in the main text before Section 4.3 . But it talks little about the proposed domain scaling. I think the authors should spend more time explaining how (11) works."}