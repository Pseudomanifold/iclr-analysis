{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\nThe present work investigates the mechanism by which Lipschitz regularization influence GAN training. It makes the observation that under scale invariant activation functions and optimizers (for instance, RELU and RMSProp), the value of the Lipschitz constant in GAN variants like WGAN or spectrally normalized GAN only results in a multiplicative constant of the generator output. Therefore, it argues that the role of the commonly used 1-Lipschitz penalty is not to ensure small discriminator gradient, but rather to restrict the relevant regions of the loss function and the size of the gradients. To support this claim, it is shown that in the limit of small Lipschitz constants the size of the domain of attained discriminator outputs shrinks. For large Lipschitz constants, however, the performance of all GAN variants other than WGAN detoriates, which is explained with the fact that the relevant domain of the loss function is not restricted any more.\n\nDecision\nI believe that it is an interesting pursuit to better understand the role that Lipschitz regularization plays in GAN training and I much appreciate the thorough experiments provided by the authors. However, in the present form, key points of their analysis too vague (see examples below), which is why I vote for rejecting the paper\n\nIt is unclear why the paragraph above Hypothesis 1 implies that \"the choice of loss functions contributes little\", as claimeed just before the hypothesis.\n\nHypothesis 1 inadequatly summarizes the introduction of Liptschitz regularization which was largely motivated by the fact that if the data and generated distribution are mutually singular (for instance, because the data is concentrated on a low-dimensional structure), loss of the optimal discriminator does not depend on the choice of generator. In the original WGAN paper, WGAN is motivated as computing an approximate minimizer of the wasserstein distance between the data and the generated measure. As is evident from the Kantorovich duality that changing the Lipschitz constraints on the discriminator only amounts to applying a scaling to the resulting distance.\n\nThe paper sometimes mentions the role of Lipschitz regularization and sometimes the role of the choice of Lipschitz constant. However the interpretation of WGAN as approximating the Wasserstein distance readily shows that the Lipschitz regularization can matter even in cases where the exact choice of Lipschitz constant does not. For instance, it claims \"The equivalency between tuning Lipschitz constants and domain scaling (Eq.11) implies that the performance improvements of various Lipschitz regularized GANs (Table 2) originates from the restriction of the loss function\" Tuning the Lipschitz constant is equivalent to rescaling of the loss function. However, imposing a Lipschitz constraint at all is not equivalent to restricting the output range of the discriminator, since a function of arbitrary small range can have arbitrary large Lipschitz constant. Thus, it can not be concluded that the improvement due to Lipschitz regularization is due to a restriction of the loss function.\n\nSuggestions for improvement\nThe observation that spectral regularization often amounts to rescaling the entire network and that this in turn can be seen as a restriction of the relevant domain of the loss function might still be a useful observation, but as outlined above important parts of the conclusions are unclear in the present form."}