{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper study the effect of Lipschitz regularization in GANs. They first show empirically that the choice of Lipschitz constant doesn't affect too much the training. They confirm this observation by showing that the Lipschitz constant only affect the scale of the output and the gradient of the discriminator. They then show both experimentally and theoretically that the choice of Lipschitz constant also affects the size of the domain. Finally the paper show that instead of choosing the Lipschitz constant we can instead use a parameter $\\alpha$ that scales the output of the discriminator.\n\nSome of the observations are quite interesting, however the organization and the poor writing of the paper makes the contribution unclear. Thus I think this paper should be rejected.\n\nMain argument:\n- According to the author when the loss is linear, the choice of Lipschitz constant has no effect, because it only scales the gradient of each parameters by some constants which the authors argue is corrected by methods such as RMSProp. I still believe this could have an effect at the beginning of training when $E[(k_{SN}^mg)^2]$ has not \"converged\" yet, and could still suffer from vanishing or exploding gradient as mentioned by the author. In particular, this would make the different layer of the discriminator learn at different speeds, and this would scale down or up the gradient of the generator. Thus it might be necessary to slightly adjust the learning rate at the beginning of training, especially the gradient of the generator. Can the author discuss how they chose the learning rates in their experiments ? I'm not sure the empirical observations would hold if the authors were to use different learning rates for different choice of $k_{SN}$.\n- The idea of introducing some hyper-parameter $\\alpha$ to scale the output of the discriminator, is not entirely novel. For example for NS-GAN this is similar to changing the temperature of the sigmoid which is already a known trick to modify the smoothness of the prediction. So I'm not sure how much this is a new contribution.\n- The claim that the loss function should be strictly convex for better convergence but linear to avoid mode collapse, is weakly supported in the paper. In the current state I don't think there is enough evidence in the paper to claim that.\n- It would be nice to run the experiments with different seeds and report the mean and standard deviation.\n\nMinor comment:\n- Notation are a bit confusing in equation 5, should be clarified that this the RMSProp update of the weights for the layer at depth m, if the loss function is linear.\n"}