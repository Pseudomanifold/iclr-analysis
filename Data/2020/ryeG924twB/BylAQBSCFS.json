{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "1. Summary\n\nThe authors apply MARL to principal-agent / mechanism design problems where selfish agents need to be incentivized to coordinate towards a leader's (collective) goal.\n\nThe leader is modeled as a semi-MDP with event-based policy gradients and modules to model/predict followers' actions. The leader sends messages to followers, an \"event\" is a pair (timestep, message of leader to a follower).\nA `termination' menas that an agent should stop executing the previous selected action; the leader signals as such to the agent.\nWith this modeling step, the authors formulate an event-based policy gradient, which considers models for which goal to send to followers and when.\n\nThe authors compare this approach on 4 environments with M3RL, which also solves (extensions of) principal-agent problems.\n\n2. Decision (accept or reject) with one or two key reasons for this choice.\n\nWeak accept.\n\n3. Supporting arguments\n\nThe approach seems sound and conceptually related to a multi-agent generalization of STRAW https://arxiv.org/pdf/1606.04695.pdf, where a planner predicts / commits to an action-plan for a single agent.\n\n4. Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.\n\n5. Questions"}