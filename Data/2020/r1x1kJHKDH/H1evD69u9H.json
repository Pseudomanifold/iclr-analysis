{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Here is my summary of the paper:\ni) a latent representation needs be learned in a self-supervised way, due to the shortage of labels; \nii) this latent representation should nevertheless efficiently support supervised learning;\niii) this is hardly the case in mainstream approaches as the latent representation aims at compression, not making any difference between informative and uninformative parts of the example (image).\niv) assume that one can determine the informative and uninformative parts;\nv) enforce the latent representation to focus on the reconstruction of the informative parts; \nvi) validation: this latent representation is more efficient, classification-wise\n\nA very interesting research programme, indeed. \n\nUnfortunately the devil is in the details: I incline to think that the paper is premature for publication at ICLR:\n* Assumption iv) is the weakest one; the argumentation (repeated parts; but not the background - though the background is repetitive in general) is inconsistent.\nI find it hard to consider that a part is informative/uninformative per se; (e.g. even the background can be informative if the goal is to discriminate natural from interior scenes.\n\nSuggestion: It might be a good idea to use the notion of saliency to define the informative parts (e.g. to define the loss weights involved in ${\\cal L}_w$; see for instance \"What do different evaluation metrics tell us about saliency models?\" (ArXiv 2017) and references therein.  \n\n* Step v) is not very clear. I understand that a grid of cells is defined on the image, and to each cell is associated the \"prototypical (repeated) patterns\" that appear in the cell (boolean presence/absence; continuous code). But a simpler alternative would be to break the full image into cells; to learn a vanilla latent code on these cells; the repeated ones might be better encoded just due to their higher number; (it is true that the background also would matter); or you use the saliency score as weight on the loss. \n\nDetail: I would omit the comparison between infant cognition and the NN training, where the former one needs incomparably less examples; the comparison suggests that the infant is an independent entity, which is debatable -- think of a feral child. "}