{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a VAE architecture that tries to represent only interesting parts of an input in its latent space. These interesting parts of an image supposedly resemble patches in the observation space. The proposed model makes it also necessary to sort of redefine the reconstruction loss. The goal of the model is to find latent representations that are more useful (compared to standard VAE models) for downstream tasks.\n\nThe paper suggest to learn a binary mask in latent space that indicates 'interstingness'. The concept can be generalized to multiple 'interesting things', the number of which is upper bounded by the user (see ablation study). These binary indicators not only identify interestingness, but also 'sameness', i.e. 'interestingness' is resembling the fact that an identified concept appears repeatedly.\n\nAs a first comment it seems that the paper is clearly tied to the problem of learning representations of visual data -- it seems to be not easily applicable to arbitrary domains?\n\nI think this is overall a nice idea. I also like that the ultimate goal of the architecture is 'downstream' usability (classification results in this case).\n\nNonetheless, the model is formulated in a generative so what's its generative capacity? I'm missing this aspect in the paper (or Appendix at least). Also, in terms of comparing it with alternative approaches, why is the betaVAE choosen? Conceptually a VQ-VAE seems to be much more closer to the described modelling assumptions (this may potentially not directly obvious). Finally, I'm uncertain about the actual impact of your model, compared to the additionally introduced weighting scheme of the reconstruction loss in section 3.4. This seems to have an important effect, make your model and betaVAE look very similar performancewise. So what does your model actually bring to the table? I'm missing investigations along that question.\n\nSpecific remarks:\n* Why use a NN for the downstream classification task. Why not something much simpler, like a linear classifier?\n* What's the structure of this NNclassifier? Section 4 mentions fully-connected layers, but 4.2 talks about 'added conv layers'?\n* You repeatedly miss the article \"the\"."}