{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces an algorithm based on the VAE for training a feature map in an unsupervised way, and later assesses the quality of said feature map in supervised classification tasks. The trained feature map is a fully-convolutional neural network, which thus extracts features that relate each to some spatial path of the image. The latent formulation forces the model to cluster these spatial patches into groups. Patches assigned to the same group see their features averaged and only copies of this average are given to the decoder. The rationale is that this mechanism guides the model into focusing on patches that display some kind of repetition across the dataset, which are more likely to contain useful information for classification. Here the VAE is used as a proxy for training the feature map, rather than as a generative model. Several experiments are provided comparing the classification performance when using the feature map trained with the proposed model PatchVAE and its variants compared to the more traditional beta-VAE.\n\nWhile the paper presents an interesting construction on the VAE loss, I see a few issues with it which in my opinion should block its acceptance, mostly concerning the experimental results. I however believe they can be addressed and I will revise my decision if they are.\n\nFirst of all, the tables of results (1, 2, 3, 4 & 5) all present extensive accuracy result, but lack any indication of uncertainty or confidence intervals. Where they obtained by a single run each? Are they an average over several runs with different random seeds? This should be clearly detailed in the text of the paper, and if possible indication of the variance of the results should be displayed. As this paper stands, I cannot say if the results of PatchVAE are indeed significantly different from those of beta-VAE, or if the difference is actually within the margin of error of the experiments.\n\nSecondly, the experiments show that the feature maps learned by PathVAE and beta-VAE are both significantly outperformed by a supervised training, but there is no commentary about that. In which context could PatchVAE be used in practice? If it is not yet good enough to be used in production, do you have any suggestions about where to direct future research to further improve it? Also, how does PatchVAE compare to the self-supervised tasks that are in Section 2?\n\nThird, your ablation study notably does not talk about beta_app. How was its value chosen for the experiment? Does it have a lot of impact on the results? This seems to be an important parameter to get right.\n\nFinally, is a paragraph in the paper that needs to be clarified: part 3.2, paragraph \"training\": I don't think this is the posterior Q^A that is assumed to be a Normal distribution N(0; I), but rather its associated prior. Similarly for Q^V."}