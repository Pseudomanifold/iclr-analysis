{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzes some of the optimization challenges presented by a particular formulation of \"short-and-sparse deconvolution\" and proposes a new general purpose algorithm to try to alleviate them. The paper is reasonably clearly written and the experimental results seem impressive (as a non-specialist in this area). However the experimental investigation on real data does not compare to a baseline, which seems like an essential part of investigating the proposed method. If this were corrected I would recommend an accept.\n\nI would like to make it clear that I have little experience in this area and am not familiar with relevant previous literature, so I was only able to roughly judge novelty of the ideas and how the experimental results might compare with existing approaches.\n\n\nMajor comments:\n\nFor the experimental investigations on real data there was no baseline presented. Are there no task-specific algorithms that have previously been developed for deconvolution of calcium signals, fluoresence microscopy and calcium imaging? At the very least it would be helpful to compare to one of the other general purpose methods used for figure 6.\n\nIt seemed like a lot of the paper is taken up with a recap of the results presented in Kuo et al. (2019), and it didn't always seem clear exactly what the relevance of these results were to the present paper. In particular it seemed strange to me to devote so much space in section 2 to the ABL objective given that it is not used (as far as I can tell) for the proposed algorithm.\n\n\nMinor comments:\n\nThe abstract says \"We leverage... sphere constraints, data-driven initialization\". Is the effect of these actually investigated experimentally?\n\nIn the abstract, \"This is used to derive a provable algorithm\" makes it sounds to me like this will be done in the current paper.\n\nIn the abstract, a reference for the \"due to the spectral decay of the kernel a_0\" claim would be helpful.\n\nIn the notation section, the definition of the Riemannian gradient seems a little sloppy mathematically: strictly f needs to be defined on $\\reals^p$ (or an open subset of $\\reals^p$ containing $S^{p - 1}$) in order for the right side of the gradient expression to be defined.\n\nAt the start of section 2, it would be helpful to give a one-sentence description of the unifying theme of the section.\n\nIt might be helpful to explicitly state that \"coherence\" is related to the strength of temporal / spatial correlations to aid developing the reader's intuition for the meaning of $\\mu(a)$.\n\nThroughout the paper the problems that multiple equivalent local optima (due to shift symmetry) cause come up repeatedly. What happens if this symmetry is removed straightforwardly by adding a term to the cost function to encourage a particular value of $a$ to be the largest?\n\nIn section 2.1, for \"It analyzes an ABL...\", it's not completely clear what \"It\" refers to (I presume Kuo et. al (2019) from context).\n\nMarginalization in my experience refers to a \"partial summing\" operation, whereas just above (4) it is used to refer to a \"partial maximization\" operation. This seems non-standard to me, but is this usage standard in this field?\n\nI didn't understand the relevance of the sentence \"Under its marginalization... smaller dimension p << m.\" to the present paper. The sentence also seemed vague and difficult to understand if you were not already familiar with this result. It should also have a reference to justify this claim.\n\nIt wasn't clear to me whether figure 1 were schematic \"rough intuition\" diagrams intended merely to be suggestive, or provably showing the type of behavior that happens in all cases. Also, what are the axes, generic \"parameter space\", I guess? I also didn't follow why (a) appears projected on to a plane while (b) and (c) appear projected on to a sphere(?)\n\nIn section 3, under \"momentum acceleration\", it would be helpful to justify the claim that \"In shift-coherent settings, the Hessian... ill-conditioned...\".\n\nIn figure 4, the axis labels are much too small to read. In figure 5 (b), the red poorer results obscure the green better results.\n\nIn figure 4 (d), is it fair to say that the main convergence speed improvement for homotopy-iADM is in going faster from \"quite near\" the optimum to \"really near\" it, rather than getting \"quite near\" it in the first place? If so, isn't the latter more often what's relevant in practical applications?\n\nIn figure 5, it's a little confusing to switch color meanings between (a) and (b).\n\nReweighting seems to have a dramatic positive effect in figure 5. Is it worth investigating its effect in the other experiments as well, for example in figure 4?\n\nIn figure 6 (b), is there any reason not to compare against standard black box optimizers like vanilla SGD, ADAM, etc (possibly with projection to satisfy the sphere constraint if necessary)? Would they perform very badly?\n\nTypo \"Whilst $a_0$ nor $x_0$\" should be \"Whilst $a_0$ and $x_0$\".\n\nWhat does the square-boxed convolution operator in (8) mean?\n"}