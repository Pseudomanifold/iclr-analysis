{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper propose to compress deep neural network with SVD decomposition, which however has been published 6 years ago in this paper to decompose FC layers:\nXue, Jian, Jinyu Li, and Yifan Gong. \"Restructuring of deep neural network acoustic models with singular value decomposition.\" Interspeech. 2013.\nFor convolutional layers, Tucker decomposition, which is high-order SVD, is apparently a better choice. (Kim et al. (2016)) They should at least compare their method with this one.\n\nIn their deduction of full-rank-low-rank model joint training,  W_r^{(l)} = U_r^{(l)}{U_r^{(l)}}^T W^{(l)} is used without any explanation.  Since U_r^{(l)}{U_r^{(l)}}^T = I_r, W_r^{(l)} will be first r rows of W^{(l)} or first r cols of W^{(l)}.  It can not be treated as approximation of W^{(l)}.  In other words, the foundation of their training is wrong, which makes their experimental results unconvincing.\n\nThey conduct their experiments with CIFAR-10/100, which is too small for VGG-15 and ResNet-34.  A larger dataset would be better.\n\nThe writing of this paper is not good. For example:\n1. In proposition 2.1, what does \"y\" represent.\n2. For Error-based criterion, what is its difference with selecting rank according to singular values?\n3. What is P^{(i)} and M^{(i)} in complexity-based criterion?\n\nIn conclusion, i will give a weak reject."}