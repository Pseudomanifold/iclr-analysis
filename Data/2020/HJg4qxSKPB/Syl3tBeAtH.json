{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper aims to explain the regularization and generalization effects of data augmentation commonly used in training Neural Networks. \nIt suggests a novel measure of \"rugosity\" that measures a function's diversion from being locally linear and explores the connection between data augmentation and the decrease in rugosity.\nIt further suggests the explicit use of rugosity measure as a regularization during training to replace need for data augmentation.\nThe paper is very well written and both the positive and negative findings are clearly presented and discussed. \nCons:\n- The main contribution of the paper, in my view, is the suggestion of using rugosity as a explicit regularization for training Neural Networks. Nevertheless, all the results in the paper show a negative impact of this on the test accuracy which is contradicting to the proposition. \nThis result has been discussed in section 5 but without much evidence to the explanations mentioned. The connection is very interesting but I believe further work is needed to explain those negative results on test accuracy. \n- The difference in finding (Table 1) between the CNN and ResNet networks can be more discussed. \n- Additional tasks (like regression) or even toy examples can be useful in further explaining the connection between rugosity and generalization to test data. \n "}