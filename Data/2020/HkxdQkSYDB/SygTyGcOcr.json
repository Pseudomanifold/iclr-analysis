{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper addresses the problem of coordination in the multi-agent reinforcement learning setting. It proposes the value function factorization similar to independent Q-learning conditioning on the output of the graph convolutional neural network, where the graph topology is based on the agents\u2019 nearest neighbours. The paper is interesting and has some great ideas, for example, KL term to ensure temporal cooperation consistency. However, the paper has its drawbacks and I feel obliged to point them out below. I vote for the weak acceptance of this paper.\n\nOne of the main drawbacks of the paper is that it is extremely hard to grasp. Even the Abstract and Introduction are hard to understand without having a pass over the whole paper. The authors often use vague terms such as 'highly dynamic environments' or 'dynamics of the graph' which make it hard to understand what they mean. The paper would benefit from a more precise language. Some of the important notions of the paper are used before they are introduced, which make the general picture very hard to understand and to relate the work to the existing research.\n\n'Related Work' section seems to be missing some recent work applying graph neural networks to multi-agent learning settings:\n\u2022 Malysheva, Aleksandra, Tegg Taekyong Sung, Chae-Bong Sohn, Daniel Kudenko, and Aleksei Shpilman. \"Deep Multi-Agent Reinforcement Learning with Relevance Graphs.\" arXiv preprint arXiv:1811.12557 (2018).\n\u2022 Agarwal, Akshat, Sumit Kumar, and Katia Sycara. \"Learning Transferable Cooperative Behavior in Multi-Agent Teams.\" arXiv preprint arXiv:1906.01202 (2019).\n\nMy questions to the authors:\n\n\u2022 In section 3 you mention 'a set of neighbours ..., which is determined by distance or other metrics'. Can you elaborate on that? What are these metrics in your case?\n\u2022 Just before the Section 3.1, you say 'In addition, in many multi-agent environments, it may be costly and less helpful to take all other agents into consideration.' Have you run any experiments on that? In the appendix, you show, that making the neighbourhood smaller negatively affects the performance, but what if you make it bigger? Ideally, I would like to see an extended version of Figure 8 and 9 in the main part of the paper since they are very interesting and important for the claims the paper makes.\n\u2022 At the end of Section 3.1, you mention the soft update of the target network. Later, in 3.3, you say that that you do not use the target network. Can you elaborate more on that?\n\u2022 In Equation 4, is it a separate KL for each of the attention heads? If yes, this is not clear from the formula.\n\u2022 It will be useful to see the ablation experiments for all of the testbeds, not only for Battle.\n\u2022 Why do you think the DQN performance drops in the second half of the training in Figure 4 for all of the runs?\n\u2022 Have you tried summation instead of the mean aggregation step?\n\nI will put comments for particular parts of the paper below.\n\nABSTRACT\n\n>>> ...environments are highly dynamic\n\nWhat do you mean precisely here?\n\n>>> ...graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment\n\nWhat is the 'dynamics of the underlying graph'? What is the 'graph of the multi-agent environment'?\n\n>>> 'coordination is further boosted'\n\nNot sure that 'boosted' is the right word here.\n\nINTRODUCTION\n\n>>> '...where mutual interplay between humans is abstracted by their relations'\n\nNot sure what it means.\n\n>>> we consider the underlying graph of agents...\n\nThe agent graph has not been introduced yet.\n\n>>> DGN shares weights among all agent(s) making it easy to scale\n\nWhat do you mean precisely by 'easy to scale'? Can you support this claim?\n\n>>> We empirically show the learning effectiveness of DGN in jungle\n\nNeeds a reference to the testbed.\n\n>>>  ... interplay between agents and abstract relation representation\n\nWhat is 'abstract relation representation?\n\n>>> We consider partially observable environments.\n\nWhat do you mean precisely by that? What is the MDP formalism most suitable for your problem statement? What is objective under your formalism?\n\n>>> However, more convolutional layers will not increase the local region of node i.\n\nWhat do you mean by that?\n\n>>> As the number and position of agents vary over time, the underlying graph continuously changes, which brings difficulties to graph convolution.\n\nWhat kind of difficulties?\n\n>>> As the action of agent can change the graph at next timestep which makes it hard to learn Q function.\n\nWhy does it make it hard?\n\n>>> DGN can also be seen as a factorization of a centralized policy that outputs actions for all the agents to optimize the average expected return.\n\nIt would be useful for the reader to compare your approach with all the others type of the value function factorization. To me, your approach looks like a more sophisticated version of independent Q-learning, is that true?\n\nMinor comments:\n\n* In 3.2 it would be very helpful to put the dimensions for all of the variables for easier understanding.\n* The brackets in the equation 3 are not very clear (what are you concatenating across?)\n* In section 4, when describing an environment you say 'local observation that contains a square view with 11x11 grids'. What is the total size of the environment?\n* The performance plots for Battle include ablations before the ablation subsection is introduced. This is a bit confusing.\n* All figures/tables captions should be more detailed and descriptive.\n* \u2018However, causal influence is not directly related to the reward of environment.\u2019 Should be \u2018of the environment\u2019.\n\n"}