{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a mutual information term into the training objective of message passing graph neural networks.  The additional term favors the preservation on information in a mapping from an input edge feature vector e_{i,j} to a weight matrix f(e_{i,j}) used in computing messages across the edge from node i to node j.  A variational lower bound on the mutual information is used in training.  Impressive empirical results are given for chemical property prediction and relation prediction in knowledge graphs.  I have no real complaints other than I might recommend citing the original work on infomax:\n\n\"Self Organization in a Perceptual Network\", Ralph Linsker, 1988."}