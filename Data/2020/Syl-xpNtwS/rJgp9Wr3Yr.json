{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper investigates the information bottleneck approach in learning representations in reinforcement learning. The authors propose 1) combining actor-critic objective and IB loss; 2) deriving optimal target distribution of the composite objective; 3) optimizing a lower bound of target function and using SVGD for optimization; 4) empirically show that the proposed method achieves promising results; 5) empirically show that there exists E-C process in RL; 6) discuss the relationship with MINE.\n\nPros:\n1. Well organized paper with a clean presentation.\n2. The technical contribution is enough (composite loss, optimal target distribution, using SVGD to solve lower bound, comparing with MINE)\n3. The experimental results are promising.\n\nCons/Questions:\n1. In Sec A.2, deriving the target distribution by KKT condition seems more rigorous (P_phi should be non-negative and sum up to 1). Using gradient equal to zero gets the same results is because the constraints are not active here (the optimal solution is not on the boundary of simplex).\n\n2. If not use the lower bound of the target distribution (use P rather than U), is there any method to solve this?\n\nOverall, I found this paper is interesting, with good theoretical contributions and promising empirical results."}