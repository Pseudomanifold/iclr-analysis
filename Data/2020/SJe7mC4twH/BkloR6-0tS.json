{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers the problem of achieving robustness against adversarial attacks in a white box (as well as \"black box\") setting. The key idea is to use quantization layers within the neural network and represent the intermediate network features in terms of their codebook centroids. The paper also explores multi-head quantization where the features are first projected via transformation matrices and then quantized. Experiments are provided on MNIST and Fashion-MNIST datasets and show promising results. \n\nPros:\n1. The idea of quantization within neural networks, while not a new idea, however looks interesting from the perspective of achieving adversarial robustness.\n2. Experiments demonstrate some interesting trend.\n\nCons:\n1. The paper lacks any key insights into why quantization might be a better idea than standard schemes that seek robust-balls within which the perturbations are ineffective (see for example Parseval's networks, or convex outer adversarial polytopes, Cisse et al, or Wong and Kolter resp.). Intuitively it may be true that using quantization offers better robust feature regions, however, won't that be sometimes an overkill and may lead to poor performance? \n\n2. The paper is perhaps very unclear in certain parts. Some notable sections that I found hard to follow are listed below.\na. The Figure 1 is too cluttered and incomprehensible to me. The inputs are shows as very thin arrows and I believe they matter more than the (too many) backward arrows. This figure does not go well with the explanations in the subsequent sections.\nb. The paragraph in Section 3.1 \"Overview\" looks out of context and too cluttered. There is no formal introduction of the proposed method, which I would have expected to be in this paragraph. \nc. The section 3.4 on \"Optimization loss\" is too confusing as well. It is unclear to me what is going on. How are the centroids learned, how do you handle the non-differentiability of argmin, etc. What is the stop-gradient attempting to do? I assume this is to handle the gradient of the assignment step, perhaps use the standard straight-through operator in that regard?\n\n3. There are several technical errors in the paper. Some listed below.\na. What precisely is N_E in (3) ? The notation previously introduced in (1) is N_B. Is it the same as N_E? Or am I missing something here? \nb. The operator I in (3) should be a matrix with dimensions nwh x n_c, such that I(j,i*) = 1 as per the condition in (3). Otherwise I(N_F(x))Q won't match in dimensions. \n\nc. Won't you lose a lot of key information by hard quantizing the x against Q? While, this may allow robustness to perturbations, it looks to me to be detrimental to performance, esp. on large and complex datasets. Also, using the L2 norm to compare x to Q in (3) is also perhaps problematic, is the assumption of x and Q are in a Euclidean space, justified? If so, how?\n\nd. I could not quite follow the idea of multi-head quantization as suggested. Not sure if the authors are aware, but product-quantization is a very standard approach in quantizing input features for nearest neighbor retrieval problems. Typically, the inputs are first projected to subspaces of lower dimensions, before being quantized. This is also the case with recent multi-head transformer models. However, as proposed, the projection matrix W is of the same dimension as the input (dxd), so there is no low-dimensional projection in this step. It appears that all this operation is doing is some linear transformation of the input space, finding the max correlated rows in W with x, and then selecting those dimensions from z_e, on which the quantization is applied in (5). It is unclear to me if this is the right way to do the multi-head quantization or what would this approach lead to. Also, as is written, the multi-head quantized feature will be K times larger than the input feature. I would also add that the hadamard product in (4) will set a lot of data dimensions to zero. In the worst case, say softmax is a delta function, you would have only a single dimension in z_e_i as non-zero. Then, attempting to quantize such a vector with Q in (5) does not make a lot of sense to me.\n\n4. The experimental results, while showing some benefits on two datasets, are disappointing in showcasing the breadth and depth of the proposed scheme. To be specific, only two small scale datasets (MNIST and Fashion-MNIST) are used in the study. Why not CIFAR, SVHN, etc. or large scale models such as ImageNet? Also, only two adversarial attack methods are compared, (FGSM and BIM), why not more recent approaches?  There are no comparisons to other sanitization or defense techniques. \n\nOverall, this paper has some novelty in its core idea in the context of adversarial defenses, however, the exposition lacks conviction, and the experiments are lackluster.\n"}