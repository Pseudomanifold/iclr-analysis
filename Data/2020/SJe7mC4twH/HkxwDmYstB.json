{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The paper proposes to add a 'quantization layer' to defend against adversarial examples. Unlike previous work where the input is quantized, the paper proposes to quantize somewhere between intermediate layers. The proposed layer projects the layer's input on to different directions and quantizes each projection according to a code. Numerical experiments on MNIST and Fashion-MNIST are provided.\n\n- The approach is very similar to previous work such as VQ-VAE, and the novelty seems to mostly lie in the application of the technique to defend against adversarial examples.\n\n- The paper is not very clear about training. The layer requires projection matrices (P) and the codes (Q), but does not really explain how these are learned (even if that means referring to precise sections of other work such as the VQ-VAE paper). Some insights as to how easy this type of model is train as well as details would help clarify this.\n\n- The proposed model ends up have two separate outputs: one through the quantized path, and one through the non-quantized path used for training. The paper could do a better job at explaining how this can be used. Before 4.1.1, the paper mentions that experiments focus on the quantized path for accuracy. Does that mean the paper is suggesting to only keep the quantized path for inference in general, or are there any upside in leveraging the non-quantized path? The presentation as is is a bit confusing.\n\n- The experiments are not all very compelling:\n\n\t- Regarding the distributions of pixels/concepts, the paper claims 'the distribution of pixels exhibits much more significant distortion than the distribution of learned concepts'. Although this is an interesting part of the analysis, Figure 2 (i) uses a large $\\varepsilon$ value which makes it harder to understand the impact in the more usual regimes of lower values, (ii) fails to provide adequate evidence that the difference is 'much more significant' (which is vague anyway).\n\t\n\t- \"Q-base\" is often better than \"Q-large\", both with and without adversarial training. This is a bit counter-intuitive and goes against the narrative put forward in the paper around multiple quantization heads. However, the discussions only mention \"Q-large and Q-base\" as a whole and do not provide any insight into this.\n\t\n\t- Generally, it is hard to make sense of the results - the proposed methods often make adversarial training worse, although not always. The benefits seem to be stronger when not using adversarial training.\n\t\nMisc. comments:\n\n\"the non-quantized path is introduced ... to regularize the quantization operation\": what does that mean in this context?\n\"typically imperceptible *to* human visual inspection\"?\n\"and the other is the Non-quantized output\"\n\"online available\" -> \"available online\"?\nFGSM does not 'scale the \\ell_infy norm of the gradient'?\n\"regarded as among the most effective defense methods ()\": missing reference\n"}