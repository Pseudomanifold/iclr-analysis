{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Writing\n1. Not easy to follow. Not well written.\n2. The definition of quantization is not clearly clarified.\n3. The calculation of Q in eq.(2) is not clarified.\n4. Format errors: \nFor example, \u201cThe resulting defense is regarded as among the most effective defense\nmethods ().\u201d (an extra brackets) in Section 2.2.\n5. Citation error:\nIn Section 2.2, the Pixel-Defend is wrongly referred to Defense-GAN.\n \u201cSamangouei et al. (2018b) propose PixelDefend...\u201d  while the reference of PixelDefend is \n\u201cPouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. In ICLR (Poster). OpenReview.net, 2018b.\u201d\n\nExperiments\n3. Only FGSM adversarial training is used as a comparing method in the experiment. Comparisons with other relevant or prevalent defensive methods should be reported. (input quantization methods [1, 2], Defense-GAN[7], Pixel-Defend[3], and so on)\n4. FGSM training [4] is shown to be not general to new attacks, while PGD adversarial training [5] is stronger than FGSM training and is more robust against different attacks, which is a better choice for comparison.\n5. Results on Larger dataset like cifar10?\n6. The experimental results require more explanations.\nFor example, in Table 1, Q-large obtains much better performance than Q-base under large perturbations on MNIST, however on F-MNIST, it seems not, why? F-MNIST includes more details I think, and Q-larger is assumed to \u201ccan find more rich concepts that reflect the similarity and difference between samples\u201d, thus Q-large should be regarded to perform better than Q-base.\nMeanwhile, when combined with adversarial training, Q-large performs worse than Q-base, why? \nIn table 2, Q-large drops to 16.95%, which is not evident to demonstrate the effectiveness of the method. Meanwhile, the results in white-box attacks are not consistent with the results in black-box attacks.\nIn summary, I am confused by the inconsistent results on the different datasets and under different attack scenarios (white and black). Meanwhile, some results can not support the argues in section 3. Thus, I hope that the authors can give more explanations.\n7. Since the quantization step introduces non-differentiability in the Q-path, thus the gradient-based attack methods cannot perform well on Q-base or Q-large. It can be regarded as a case of obfuscated gradients [6]. BPDA[6] was proposed to overcome obfuscated gradients, so it is a better method to evaluate the robustness of the quantization layer.\n\n[1]\tJacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. ICLR, 2018\n[2]\tWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In NDSS. The Internet Society, 2018\n[3]\tSong, Y., Kim, T., Nowozin, S., Ermon, S., & Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. ICLR,2018\n[4]\tIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR , 2015\n[5]\tMadry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. Towards deep learning models resistant to adversarial attacks. ICLR, 2018\n[6]\tAthalye, A., Carlini, N., & Wagner, D. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML, 2018\n[7] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers againstadversarial attacks using generative models. ICLR , 2018.\n"}