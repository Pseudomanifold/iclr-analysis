{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The classic transformer network is designed to capture the dependencies along one dimension. Applying the transformer to spatiotemporal data creates the challenge of modeling the attention in a computationally efficient way (time, location, variable). This paper investigates different ways of implementing 3D attention and extensively evaluates the performance of them. \n\nThe proposed problem is a practical problem and I think the main contribution of this paper is valuable. However, there is always a trade-off between having access to the entire history and the ability to be used in streaming settings. The need for the processing of the entire history of time series increases the latency of the algorithm. Also, the authors should report the actual run-time of the algorithms on the real data (beyond Table 1 and compared to the baselines).\n\nThe main criticism of the experiments is that the datasets are very small. For example, NYC-Traffic has only 186 time series which is considered to be of the toy-scale. Also, if you have run the experiments and not copied the reported data, make sure to indicate that in the paper. Citing a paper in a table usually means the numbers are reported from the paper.\n\nThe ways that the authors decompose the attention tensor reminds me of the works in different tensor decomposition algorithms in spatiotemporal data [1, 2].\n\nThere are minor typos in the paper too. For example, see the first in-line equation in Section 3.1.\n\n[1] Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions and applications. SIAM Review, 51(3), 455-500.\n[2] Bahadori, M. T., Yu, Q. R., & Liu, Y. (2014). Fast multivariate spatiotemporal analysis via low-rank tensor learning. In NeurIPS.\n"}