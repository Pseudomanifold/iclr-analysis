{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. It is very well written and the claims are theoretically sound. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the \"decision boundary tilting\" effect caused by the normalization. Overall, this paper has done solid work to reveal an interesting phenomenon. If it is true, this finding will impact almost all DNN models. \n\nMy concern is that this phenomenon is just another effect of \"gradient masking \" (as pointed out by Athalye, et al.). Batch norm is a well-known technique to avoid overfitting, without batch norm the network can be easily trained to be saturated with almost zero gradients, demonstrating a false signal of \"robustness\" to noise. The random noise and real-world corruption experiments are definitely helpful to clear this doubt, but only partially. My concern remains because of two obvious signs of  gradient masking: \n1. The accuracy on PGD-li (epsilon=0.031) attacks are suspiciously too high (20% - 40% Table 3/4). For this level of attack, the acc should be nearly zero. This is likely caused by the gradient masking effect, considering the cifar-10 networks were trained for longer time with larger learning rate (150 epochs, fixed lr 0.01). Training on MNIST is much easier to get zero gradients.  \n2. The weight decay discussion is not helpful at all, on the contrary, it confirms my concern on the gradient masking effect. In Table 8, the robustness was increased ~40% by just using large weight decay. This is not the \"real robustness\", and can be easily evaded by adaptive attack (see Athalye's paper).\nWith the above two concerns in mind, I doubt the phenomenon revealed in this paper is just \"one can easily train a saturated model without batch norm\" or equivalently \"it's hard to train a saturated model with batch norm\". It is hard to say if this is a bad thing for batch norm.\n\nI am quite surprised that the authors ignore this completely. Here are a few things that can be done to rule out the possibility of gradient masking. The masked gradient can be identified by: 1) One-step attacks perform better than iterative attacks; 2) Unbounded attacks do not reach 100% success., etc (see Section 3.1 of Athalye's paper).\n1. Including FGSM in the experiments and show the same trends as PGD-li. \n2. Show two networks have similar gradient norms.\n3. Apply cw-l2 attack, and show batch norm has forced large perturbation.\n\nTwo other suggestions:\n1. Summarize the different angles/steps taken to verify the phenomenon, somewhere before the experiments.\n2. Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness."}