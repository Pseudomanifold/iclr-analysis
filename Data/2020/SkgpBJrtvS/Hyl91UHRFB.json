{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper combines a contrastive objective measuring the mutual information between the representations learned by a teacher and a student networks for model distillation. The objective enforces correlations between the learned representations. When combined with the popular KL divergence between the predictions of the two networks, the proposed model shows consistently improvement over existing alternatives on three distillation tasks. \n\nThis is a solid work \u2013 it is based on sound principles and provides both rigorous theoretical analysis and extensive empirical evidence. I only have two minor suggestions.\n\n1, From Section 3.2 to Section 3.4, it is not clear to me that on the model compression task, are both the proposed contrastive loss and the loss in Eq. (10) used?\n\n2, The \u201cDeep mutual learning\u201d, Zhang et al, CVPR\u201918 paper needs to be discussed. I\u2019d also like to see some experiments on the effects of training the teacher and student networks jointly from scratch using the proposed loss. \n"}