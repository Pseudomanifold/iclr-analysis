{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I do not necessarily see something wrong with the paper, but I'm not convinced of the significance (or sufficient novelty) of the approach. \n\nThe way I understand it, an independent assumption on internal representation is relaxed by capturing correlations between them and higher order dependencies between them using a different objective function(distance measure), the problem then becomes nothing but trying to minimize another distance metric between teacher and student networks on an intermediate layer. \n\nComparing with original distillation method (KD) I'm not sure how significant the improvement is. And technically this is just a distance metric between the internal representation or output of the student and teacher. Sure it is a more involved distance metric, however it is in the spirit of what the distillation work is all about and I do not see this as being fundamentally different, or at least not different enough for an ICLR paper.\n\nThe experimental results also suggest only a marginal improvement compared to other methods. It would be helpful to also include the variance of each experiment i.e., it was mentioned that the results were averaged by repeating 5 experiments to make sure the proposed method consistently better than others.  Rightnow, It is hard to compare with other approaches. The paper gives off a feeling that this method is not novel. Why this particular distance metric between the representations? Why not just L2?\n"}