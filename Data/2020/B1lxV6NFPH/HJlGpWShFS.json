{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "\nThe paper introduces a new encoding for cell structures to improve efficiency of neural architecture search methods.\nAs a second contribution the paper proposes Bayesian optimization with an ensemble of neural networks as probabilistic model for the objective function.\nBoth contributions combined show superior performance on the Nasbench101 benchmark as well as competitive performance on the DARTS search space.\n\nWhile the paper identifies an important problem - encoding of architectures - I do not think the paper is ready for acceptance.\n\nAbout the encodings:\n\nFirst, using different encodings to enable better architecture search has been investigated by others before.\nFor example, Ying et al. also provided different encodings of the adjacency matrix for Nasbench101 besides the used binary encoding and it seems that different methods work well with different encodings.\nAlso in the work by Kandasamy et al. they presented an encoding for architecture, such that Bayesian optimization can be applied.\n\nSecond, the encoding described in the paper lacks some intuition.\n\n - How does enumerating all paths and encoding them as a binary vector convey more information than just using the adjacency matrix?\n\n - Lead isomorphic graphs, which for example occur in Nasbench101, to the same encoding?\n\n - It seems somewhat counter intuitive to use a large binary vector (more than 18000 dimensional vector for the DARTS space) as encoding for Bayesian optimization which is known to struggle with high dimensional input spaces.\n\n\nAbout the Bayesian optimization strategy:\n\nThe proposed probabilistic model for Bayesian optimization seems straight forward and simple. Also here, previous work (Snoek et al, Springenberg et al., Perrone et al)  already proposed to use neural networks and ,in order to be more convincing, the paper should include a comparison to these methods.\nFurthermore, the paper should clarify how the diversity in the neural network ensemble is enforced. Are the neural networks trained with different random initialization? How does it compare to the method proposed by Lakshminarayanan et al. which showed better performance for neural network ensembles based only on different random initialization?\n\n\nMinor comments:\n\n\n- In the Nasbench101 paper other Bayesian optimization strategies (e.g SMAC, BOHB, TPE) showed strong performance. The results would be more convincing if these methods are included in the comparison.\n\n- Following the empirical protocol by Ying et al. the results would be easier to parse if the Figure 3  could report the log test regret on the y-axis. I am also missing a figure that shows the robustness of the method across independent runs.\n\n- How are invalid architectures in the Nasbench101 (e.g architectures that violate the max edge constraint) treated in the experiments?\n\n- I think the paper is missing the following references:\n\nSimple and scalable predictive uncertainty estimation using deep ensembles\nB Lakshminarayanan, A Pritzel, C Blundell\nAdvances in Neural Information Processing Systems, 6393-6395\n\nScalable hyperparameter transfer learning\nV Perrone, R Jenatton, M Seeger, C Archambeau\nAdvances in Neural Information Processing Systems, 6845-6855"}