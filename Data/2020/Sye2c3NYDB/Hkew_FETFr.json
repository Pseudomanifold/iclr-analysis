{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents some follow-up work on the lottery ticket hypothesis (LTH). The iterative magnitude-based pruning (IMP) used in the original LTH paper by Frankle and Carbin (FC19) failed to find winning tickets on large-scale vision networks without modifying the training procedure. This paper describes a change in the learning rate schedule that allows the authors to find subnetworks that outperform randomly reinitialized winning tickets. The authors name these subnetworks \u201cboosting tickets\u201d, and demonstrate that boosting tickets train faster. \u009dThe authors then use these boosting tickets to speed up adversarial training. They propose to use FGSM-based adversarial training to find the boosting tickets (since this part is computationally intensive) and then train these obtained boosting tickets using the more expensive and better performing PGD-based adversarial training. \n\nThe idea of using winning tickets for adversarial training is quite interesting. However, given the speedup times (49%), the simplicity of the idea (combining two existing things, adversarial training and pruning via IMP) and the limited number of experiments, the overall contributions seem to be minor. The findings presented regarding the slight change in the learning rate schedule are not very novel, nor give any new surprising results. A lot of the things that are presented in the paper also appeared in the original paper FC19. Further, it looks like the authors misunderstood what the LTH says and what the definition of a winning ticket is (see below).\n\nOverall, the paper presents very incremental and low-impact work. The experiments should be expanded considerably (and corrected accordingly, given the correct definition of a winning ticket) in order for me to recommend acceptance.\n\nDETAILED FEEDBACK\n\nWINNING TICKETS, Section 3: \u201cIn particular, we show that boosting tickets are winning tickets, in the sense that they outperform the randomly initialized models\u201d. Based on this sentence, I believe that the authors seem to think that a winning ticket (subnetwork + initialization) is the one that outperforms the same subnetwork when it is randomly reinitialized. However, this is incorrect. Frankle and Carbin define it as a subnetwork+initialization that can be trained at least as fast to the same or higher accuracy as the original (!) network.\n\nTRAINING TIME. FC19 (v3 on arxiv) contains graphs demonstrating that winning tickets train faster. This submission does not have an explicit comparison how their new learning rate schedule affects the performance of winning tickets (!) compared to the learning rate warmup done in FC19. \n\nRANDOM TICKETS. There is no experimental evidence that boosting tickets do much better than random subnetworks when trained in an adversarial way (potentially even more impressive computational savings...).\n\nLEARNING RATE SCHEDULE, Section 3. The only difference in the learning rate schedule is that a different schedule is used for pruning and for training. One of the key properties of the LTH is that the subnetworks  found via IMP can be trained under *the same* training procedure. In my opinion, introducing some unprincipled/theoretically unjustified learning rate tricks has none to little impact on future research.\n\nSCALE, Introduction: \u201cAlthough FC19 show that winning tickets converge faster than the full models, it is only observed on small networks, such as a convolutional neural network..\u201d. While it is true that in FC19 the experiments were performed on relatively small networks, Frankle et al. 2019 have another paper containing experimental results on large-scale networks (all the way up to inception on imagenet). Further, I would like to highlight that this submission only contains experiments on wide Resnets and VGG-16. If one of the suggested contributions is doing IMP at scale, why are the empirical results limited to these relatively small networks? \n\nPRUNING RATIO, Section 3.3. It looks like the pruning ratios are tested only for one shot pruning. Could the authors please explain their choice, and elaborate on the reason for this particular experiment.\n\nUNSTRUCTURED PRUNING, Section 2: \u201cOne of the limitations of the LTH\u2026winning tickets are found by unstructured pruning.\u201d I cannot see how the submission addresses this. Same holds for the rest of the paragraph.\n\n\nOther minor comments:\n\n- \u201cWe observe the standard technique introduced in FC19 for .. does not always find boosting tickets\u201d. I am not sure how this is different from what was already pointed out in FC19, and \u201cwe observe\u201d suggests that the contribution of observing was done by the authors in this submission.\n - \u201cboosting effect\u201d in the third paragraph in the introduction is undefined.\n - none of the plots contain error bars.\n - In section 5 under \u201cAccelerate adversarial training\u201d a future research direction is suggested: combine \u201crecycling the gradients\u201d idea with the idea proposed in this paper to speed up overall training time. This seems like a trivial application rather than a research idea. What did the authors have in mind here?\n - Calling the model \u201cMadry\u2019s\u201d in all the tables seems extremely unfair to all of his (more junior) collaborators. \n - The comment under Figure 5, \u201cWhile a wider model always boosts faster..\u201d, seems unsupported by the figures."}