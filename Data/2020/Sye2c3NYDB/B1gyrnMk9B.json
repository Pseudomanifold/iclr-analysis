{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work presents a neat discussion on how to mitigate the huge cost of lottery ticket training. Specifically, it finds a setting that produce boosting tickets which can converge faster than lottery ticket. Moreover, it proves that pruning after FGSM and PGD can produce nearly same boosting ticket, which will largely save the adversarial training time. \n\nDespite the above insights, I still have several reservations.\n\n1) In the paper it says \u201cWe observe the standard technique introduced in Frankle & Carbin (2019) for identifying winning tickets does not always find boosting tickets.\u201d It will be very depressed to see the boosting ticket only exists in small learning rate since small learning rate can only produce a weaker performance compared to lr=0.1 (roughly 92% vs. 93%). I still want to see results of the same setting (warmup, epochs) except changing learning rate to 0.1. The author should show some bad case under large learning rate setting.\n\n2) The boosting tickets are only compared with winning ticket rewinding to epoch=0 (use the initial weight). Actually some work [1][2] has shown that winning ticket rewinding to epoch=k (using the weight after training k epochs) can outperform the original weight. Rewinding to k epoch has a benefit that the pruned model do not need to train such a long time as the inherit weight already have some knowledge. This is also another \u201cboosting\u201d effect. I expect the author to compared the boosting ticket with wining ticket under the setting that rewinding to k epoch (k is set to different values like 5, 20, 60, and final epoch). When rewinding to k epoch, the learning rate scheme should also follow that one. \n\n3) Pruning ratio is a important setting to winning ticket and boosting ticket. However, Sec. 3.3 could not give a direct conclusion on how the pruning ratio affects the boosting ticket. Also, the pruning ratio of experiments on Figure. 2 is not clearly explained. I expected to see the final accuracy of boosting tickets that pruning ratio equals to 30%, 50%, 70% and the same pruning ratio while using random weight.\n\nConsidering the rebuttal time is limited, the rewinding experiment in (2) should at least give the results when k=60 and being equals to the final epoch, and also give the whole training time of these settings (the searching time plus the time of training the subnet for both wining ticket and boosting ticket while k is set to different values.)"}