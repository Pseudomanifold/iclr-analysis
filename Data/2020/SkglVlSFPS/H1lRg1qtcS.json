{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose to combine planning methods like MCTS with an ensemble of value functions to a) estimate the value of leaf nodes of the search tree and b) use the ensemble estimate of uncertainty to guide exploration during MCTS search. \nThe MCTS rollouts are also used as optimization targets for the value function.\n\nI believe this is a clear reject. On the one hand, the paper needs signficiantly more work on the writing and clarity. On the other hand I have several worries on the method and evaluation side. \n\nRegarding the presentation of the paper:\nOverall, the paper seems quite rushed. This is not a strong reason for rejection but should be improved in a future version. For example, punctuation and sentence structure is often wrong, the paper has only slighlty over 7 pages, a citation is undefined on p.7 and images and whitespace is formatted wrongly on occasion (e.g. top of page 6).\nMore importantly, on the content side, the experimental section is sufficiently clear and well written, however, the method description needs more detail and background information. The paper relies on several prior works which are referred to but not described (E.g. MCTS , the sampling mechanism by Osband et al. which they are using but not describing, the 'mask' from Osband et al which they are using but not describing).\nFurthermore, the algorithm itself is not described in sufficient detail:\n- How does the 'soft-penalization' work?\n- How exactly does the mechanism \"similar in fashing to\" Thomson sampling work?\n- Are you learning a model or do you have access to the true transition function?\n\nRegarding the method:\nI can't say anything definitive about the method as I'm not entirely clear how exactly it works. However, I have several worries that might need addressing:\n- It seems to me that the method relies on access to the _true_ transition and reward function and not on a learned model. This is a big difference to much of the prior work they compare against. This also makes the comparison against any pure model free method like PPO much less meaningful.\n- Similarly, manually avoiding dead-ends and loops is a very strong assumption \n- Also, being able to distinguish and use a fixed ratio of \"solved\" and \"unsolved\" episodes is a strong assumption. \n- The one main contribution seems to be a new way of how \\phi_a(x) is defined. Their particular choice needs a clearer motivation. Furthermore, if there is more contribution and differences to prior work, highlighting them more would help the reader understand the contribution. \n- As the work makes several strong assumptions regarding the environment and access to the model, significantly more work (e.g. ablation studies) is needed to clearly show which assumption and feature of the algorithm is important for performance (and ideally also why). For example (but that's just a first idea): To understand the impact of their choice of \\phi vs. their planning architecture, it would be be interesting to maybe train PPO using an exploration bonus based on \\phi. This would allow disentangling the contribution of: Access to the true model, \"discrete-environment-tricks\" like penalizing dead-ends, and exploration incentivication of \\phi. "}