{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors further study the lottery ticket hypothesis formulated by Frankle and Carbin. They demonstrate that the sparsity pattern corresponding to a lottery ticket for a given initialization can be uncovered via low-cost training. By doing so, they propose a method to: 1) first identify the lottery ticket efficiently and 2) exploit the sparsity of the resulting network to train it at a lower cost.\n\nThe contribution is however a bit incremental in my opinion. The original LT paper was not focused on efficiency, and it is not a far stretch to try to find the tickets sooner during the training. On the other hand, the experiments are well conducted (especially 4.3) and even if the original idea is simple, it is of interest to see it tested as clearly. All in all, I found this paper convincing and worth reading, and I think it should be accepted.\n\nPositive points:\n- The literature review is sufficient and present with great clarity the latest results.\n- The problem tackled is of great interest and has potentially impactful applications.\n- The authors focus on hardware friendly types of pruning.\n- The paper is well written and enjoyable.\n- The algorithm used to compute the EB tickets seems a bit ad hoc, but in my opinion sufficient as a first approach.\n\n\nNitpicking:\n- Not sure why Max(Q) > eps is in the while condition (return if Max(Q) < eps should be sufficient)\n- The treatment of the mask distance in figure 3 is confusing. It is not obvious why the authors are plotting 1-distance, and the legend of the figure suggest that a mask has a distance of 1 with itself. Recommend plotting d instead of 1-d and invert the color bar instead if they feel so inclined (yellow=0).\n- Abstract: \u201cconsistently exist across models and datasets\u201d a bit of a strong claim as only cifar is used.\n"}