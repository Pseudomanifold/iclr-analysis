{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors propose a methodology to discover new categories in an unlabeled dataset with the help of a label one. The authors propose the following methodology. First bootstrap some features using self-supervised learning on labeled and unlabeled data. Then transferring the knowledge of the labeled data to the unlabeled one by supposing that the representations of both are similar, the similarity being a rank statistic. Then using this knowledge a joint supervised-unsupervised objective.\n\nQ1. The paper is about discovering new visual classes. Section 2 mention that the number of classes C^u must be known \"a priori\". How do you tackle this limitation? Is there any heuristic, similar to the one found in the clustering literature, that could help?\n\nQ2. Have other losses been investigated for the clustering head? Such as the triplet loss, or deep clustering loss rather than BCE?\n\nI would suggest to report standard deviations as the experiments were repeated 10 times on random train-test splits.\n\nI propose a weak accept. The paper is well written, the methodology is original, the experiments are convincing and the authors will release publicly the code. My main concern is Q1, which is eluded. Second, it would be nice to have an experiment illustrating the impact of the rank k when transferring knowledge. Is k=5 always good? If not, what could influence the best value?\n"}