{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors study the behavior of simple neural networks at initializations. Particularly, the authors show that at initializations, neural networks tend to be functions with high class imbalance.\nFurther, the authors show that how such conclusion would be reached with or without a bias term, with different number of hidden layers, with change of activation functions. \nThe work is fairly interesting, yet the motivation is less clear.\nThe conclusion that study of such initializations can help understand the generalization power is not convincing.\nDespite that neural networks at initializations are biased towards low entropy functions, it\u2019s not clear with training on a dataset with an optimizer, how much we can conclude about the generalization power.\nOverall the paper is well written.\n\nBelow are some more detailed comments:\n1) In the Introduction and the first paragraph of Section 2, the authors motivate by describing how important it is to understand the inductive biases. Yet the study is about the behavior of a random initialization. It would be nice to tie these two better; or motivate from another angle, other than the inductive bias. To my reading, the sentence \u201cwhat the inductive biases \u2026 are, and how they arise\u201d is not well supported because I still don\u2019t follow what the inductive biases are from reading the paper except that a random initialization likely be low entropy functions.\n\n2) In Figure 3(a), 4(b,c,d), there is a spike at the mid-point of t. Though not as high as the extreme points, this is contradictory to the main conclusion. It would nice to add discussions.\n\n3) It would be nice to add experiments to study how such bias at initializations would impact the model training.\n"}