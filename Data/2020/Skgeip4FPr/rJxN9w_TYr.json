{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the a-priori bias of a feed-forward neural network when the weights are initialised uniformly at random and independent of the network architecture. The paper claims that this initialisation leads to biases towards low entropy functions when the input and output are binary values. \n\nThe paper starts with a single layer perceptron without the bias term, and generalises the analysis to networks with multiple hidden layers and ReLU activations. The proposed approach seems rigorous, but I have a hard time to follow the paper as many of the important results are presented in appendix. In addition, the analysis is based on a feed-forward neural network with binary inputs and a single binary output, it is not clear whether these results can be generalised to architectures of practical importance such as convolutional/recurrent neural networks. Overall an interesting piece of work that contributes to the understanding of deep neural networks.\n\n\nMinor comments:\n\nSection 4.2: \n\"as as predicted by Eq. 1\" -> remove duplicated \"as\"\n\nSection 5.2: \n\"some interesting recent work\" -> \"Some ...\"\n\"produce.At first sight\" -> add a space before \"At\"\n\"If there is not bias\" -> \"If there is no bias\""}