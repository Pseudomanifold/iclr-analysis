{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission proposes a method to perform neural rendering. From a set of images taken of a static object under constant illumination, the proposed method first selects the four viewpoints nearest to the requested novel viewpoint. Then, the images corresponding to those viewpoints are blended together using an encoder-decoder network to produce the novel view image. The key contribution of the submission over previous work is the handling of view-dependent effects such as specular highlights. Those view-dependent effects are first removed from the nearest retrieved images using the proposed EffectsNet. The resulting estimated diffuse images are then re-projected to the target viewpoint and view-dependent effects from this viewpoint are added before blending the images together.\n\nThe proposed method improves the robustness of existing neural rendering methods to materials that are not roughly diffuse. \n\nThe process to select the 20 reference images is based on a coverage scheme that is never presented, hindering reproducibility. Would it be possible to describe this scheme? Similarly, the hyperparameters used for the adversarial loss are not explicitly stated, are they exactly the same as the cited Pix2Pix? Would the source code be shared publicly?\n\nI would have appreciated an ablative experiment where the proposed pipeline is kept as-is, but a state-of-the-art intrinsic decomposition technique (as discussed in sec. 2) was applied instead of EffectsNet.\n\nI would recommend using subsections to prevent confusion in references (for example, sec. 6, p. 5 referring to sec. 6).\n\nIn fig. 6, the quotient image is hard to interpret as it is not linear. A colormap representing the percentage of error wrt. the ground truth might be easier to interpret.\n\nEffectsNet is used for both removal and addition of view-dependent effects, which makes part of the paper confusing. Maybe adding the mathematical symbols of eq. 1 to fig. 2 might help the reader to understand the training steps?\n\nThe impact of the regularizer weight of 0.01 (p. 5) is not discussed. I suspect this value might be important, as too much regularization might give underestimated view-dependent effects and too little might provide strong and incoherent effects. An analysis of this hyperparameter would be welcome.\n\nIn my opinion, the proposed abstract is slightly hard to read, maybe it would benefit from being shorter?\n\nMinor details\n- p. 3 \u201cAn extensive overview is give[n] in [...]\u201d\n- p. 8 I believe the \u201cz\u201d of the unit \u201cHz\u201d is wrongly stylized as a mathematical variable.\n"}