{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a method for rendering of reconstructed objects. At its core, it works by distinguish between diffuse images and \"view-dependent effects\" (in essence, every element related to the scene instead of the object like specularities). The approach leverages two distinct neural networks. First, EffectsNet, which infers the view-dependent effects from an image and their depth map. Second, CompositionNet, which learns to combine multiple warped view into the final image and to remove the related artifacts. Results show improvements with respect to previous methods, especially for temporal coherence and dealing with a low number of images.\n\nThe proposed approach is well described, and does seem to fix a few issues with current methods. The comparisons are extensive and comprehensive for what I can tell. The results on static images are not significantly better than previous work, however, and the need to train the whole pipeline for every object is quite a limitation. Apart these considerations, I do not have much complaints about this work. The few others I came up with are provided below.\n\nI am not completely sure here, but it seems like CompositionNet could be used with other approaches than the one described in the paper, in particular with IBR. How would it improve the performance of previous approaches, especially regarding temporal coherence?\n\nFor the EffectsNet training, we have to assume that two random images from the training set can be reprojected onto one another without loosing too much of the surface. If the views provided by these images are too different, than the number of pixels actually providing a loss to the network will be very low, which would be detrimental to the learning process. Could you comment on that? Is there a quick fix to this issue?\n\nOne thing that was not discuss was about the effects integrating a neural network into the rendering pipeline have on the output resolution. Classical methods are not really limited on this regard, but neural networks cannot easily provide high resolution outputs. I understand that EffectsNet is not directly used as image (only to subtract the \"view-dependent\" parts), but could there be issues if we use a resolution of, say, 2048x2048 instead of 512x512? Or even higher? Given the aim of the paper at targeting high quality rendering, such consideration has important practical consequences.\n\nIn summary, this paper does a reasonable job at improving the current IBR methods. The idea of making the network learns the scene effects instead of the whole object appearance is sound and could probably be translated to other related problems."}