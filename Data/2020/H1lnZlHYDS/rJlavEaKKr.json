{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studied the convergence and global optimality of GAN, when the generator is parameterized as wide neural networks and the discriminator is solved to approximate optimality in each iteration. It claims to establish the first global convergence and global optimality in training GANs when the generator is parameterized by neural nets. \n\nI'm not sufficiently familiar with the optimization literature, thus cannot determine the significance of the global convergence result. However, once established the optimality result follows naturally, although under strong assumptions. As such I don't have strong opinions either favoring acceptance or rejection.\n\nQuestions and comments:\n1. Can you clarify on the significance of your global optimality result? It appears to be Lemma 4.3, but before Lemma 4.3 you mentioned \"the convergence result is discussed in Lin et al. (2019), which is concluded in the following Lemma\". Furthermore, given your assumptions that L(\\theta, f) is strongly concave in f and the function class \\mathcal{F} is convex and compact, how is your contribution different from the analysis of (S)GDmax in Lin et al (2019)?\n3. The global optimality result is established under the assumption that both the model and data distribution have bounded densities w.r.t. the ambient space. This is rather strong and does not reflect the typical use cases of GANs. "}