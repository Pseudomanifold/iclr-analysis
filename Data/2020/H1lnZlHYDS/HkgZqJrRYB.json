{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work considers GANs in IPM type with the generator represented by a two layer network. Under certain assumptions, they establish the global convergence and global optimality of training GANs. \n\nMy major concern is that the bound in Theorem 4.8 may not be meaningful. Specifically, \nthe derived upper bound on the total variation distance relies on the distance $\\|\\theta^{*} - \\theta_{0}\\|_2$ ($\\epsilon_{K}$), which maybe large in the over-parameterization setting. For example if $\\|\\theta^* - \\theta_0\\|_2 = O(\\sqrt{m})$, the upper bound is in the order of $O(1)$. I would like the authors to further discuss on this issue, i.e., provide a upper bound of $\\epsilon_{K}$ and show that the bound of the total variance distance is indeed meaningful.\n\nAnother concern is that Assumption 4.1 may be too strong. Once the structure of generator network is fixed, the assumption 4.1 gives constraints of the parameter set. For example, if the generator is a linear function from D dimension to D dimension with parameter $\\theta$ and uniform distribution noise, $p_{\\theta}$ is in proportion to $\\det(\\theta) ^{-1}$. In this case the gradient of probability density is not bounded.\nThe authors would like to briefly conduct some discussion on this assumption and provide some examples.\n\nSome typos:\nIn Assumption 4.5, it seems that the first and second assumptions contradict with each other.\nIt seems that the equation before Section C holds trivially.\n\n"}