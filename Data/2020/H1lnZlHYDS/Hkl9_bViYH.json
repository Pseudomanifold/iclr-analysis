{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors consider the convergence of adversarial algorithms for neural networks training. They assume that for each generator an oracle is able to compute the globally optimal discriminator and the generator is a two-layer network.\nUnder such assumptions,  they prove that gradient descent steps (on the generator parameter space) converge to a generator that reproduces the data distribution. The proof is obtained by mapping the distributions into a Reproducing \nKernel Hilbert Space and showing that on a stationary point the discriminator takes a constant value over any sample X = g(Z). \n  \nMy main concern is about the clarity of the exposition and the assumptions made on the generator. The main idea should be explained better in the introduction by stating explicitly what assumptions are needed. In particular, what are the net advantages or differences between the proposed method and the cited existing approaches that also look at the problem from the generator's perspective (i.e. an optimal discriminator is given by an oracle for each values of the generator parameters)? The authors say that in those works global convergence 'requires stability assumptions'  but do not specify what is the relationship with their assumptions.\n\nAs for the assumptions on the generator's architecture, it is not clear whether the two-layer form of g_\\theta could be expressive enough for any practical application. The simple form of g_\\theta seems to be a key element in the proof of the \nmain theorem but the authors do not say if and how their idea generalises to more complicated cases.\n\nQuestions:\n- how strict is the assumption that \\cal F is a convex set. Would it possible to add an intuitive explanation of why the absolute value can be be removed in this case?\n- why the vector parameters a_k can be kept fixed and only  the optimisation over \\theta is considered? \n- what are the differences between the regularity assumptions made by Hsieh2019 and the assumptions made in the paper (regularity of f and form of g)?  Is the form of the generator considered in Hsieh2019 similar to g_\\theta used in the paper?\n- is the specific form of the chosen kernel function implied by the architecture of g_\\theta? Has such kernel function already been used in the past? How easy is to prove that it defines a RKHS? \n"}