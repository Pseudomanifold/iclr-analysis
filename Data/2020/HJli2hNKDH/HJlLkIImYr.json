{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Claims: This paper studies instances where observational overfitting is occurring in reinforcement learning, and separates it from other confounding factors contributing to poor generalization. They specifically study observational overfitting with LQR and deep neural networks, and show implicit regularization naturally occurs in this setting.\n\nDecision: Accept. The paper presents relevant bounds on overfitting depending on the dimensionality of the signal and noise. The framework and definition of family of MDPs are interesting and useful, and the focus on the implicit regularization provided by overparameterization when dealing with high dimensional observations with correlated noise is important and useful to decouple from varying dynamics. \n\nSection 3.2 needs more explanation of the experiment, how the train test split is created and if this is on the original rendered images from OpenAI gym or with correlated noise -- I've never seen such stark generalization gaps one the original environments. Are all of these experiments done with the proprioceptive state? Are the experiments in 3.3 then done with learned deconvolutions to generate the 84x84 images? Why this instead of using the rendered images themselves (as f) with added noise parameterized by \\theta?  Fig 8 is also not well explained, there are two sets of lines of each color in the plots with no label as to which is which, I assume these are train and test? Perhaps a plot that shows the generalization gap as the y-axis would be more clear."}