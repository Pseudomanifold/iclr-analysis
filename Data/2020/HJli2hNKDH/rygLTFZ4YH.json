{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThe paper proposes a method for measuring generalization error in reinforcement learning (RL). The paper proposes to disentangle the observations into the features that relevant and not-relevant for the policy and then perturb the non-relevant features while keeping the relevant features constant. If the agent has learned to generalize, then perturbing the non-relevant features should not change the RL score. \n\nComments:\n\n1. The paper addresses an important question in RL:  generalization in the observational space. It is not trivial to define generalization in RL as this differs fundamentally from a SL or unsupervised learning setting.  The paper proposes a metric to measure this generalization error and this can be applied to non-toyish environments.\n\n2. The paper is clearly written and well-motivated. \n\n3.  I do like the proposed method. However, I also see some shortcomings of the method.  \nThe paper proposes to disentangle the features into relevant and non-relevant features. While this might be easier for certain task, it might be much more difficult for other tasks. The relevant features may be some implicit priors that are not easy to extract, for example, the fundamental physics of an environment. I am not sure how this can be addressed in a complicated environment where the relevant features are sampled from an implicit prior.\n\n4. The experiments on CoinRun i think is the most relevant ones. However, in this environments, it seems that although the observational features are quite different (rendering of the environment), the underlying physics or moves/ actions are very much similar. It would be nice to see a more complicated environment where the underlying physics or composition of actions can be different."}