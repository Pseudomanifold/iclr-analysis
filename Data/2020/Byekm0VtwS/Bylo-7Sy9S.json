{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a way of training neural nets on analog-circuit based chips, which are cursed with uncertainties. Such uncertainties are deeply rooted in the way neural nets are implemented on such chips. Take [c = a x b] as an example. In order to perform this operation, one can set the electric potential to a and the conductance to b and c will be the output current. The problem here is that we cannot set the conductance precisely, which often encodes the weights of a neural net. This implies we cannot precisely program a neural net into these chips. This paper proposes to train a neural net with the presence of such noise, by treating weight as a random variable during training. The experimental results based on simulation suggest this is a much better strategy than programming a neural net into chips imprecisely. \n\nOverall, this paper touches upon an important research problem towards running neural nets on neuromorphic computing chips, which is how to deal with the underlying uncertainties. The proposed algorithm is reasonable and the experimental results look encouraging. However, I would like to ask a few clarification questions. Given authors\u2019 response, I will be willing to adjust my score. \n\n(1) For the baseline, have you tried randomly jittering the network weights after every training iteration in a way that is \u201cblind\u201d to the source of the uncertainties (i.e. conductance)? I would like to understand in what degree modeling the noise helps. If this works out, it implies, (1) we do not have to pay much cost in sampling; (2) there is a simpler way to train neural nets that behave robust when deploying onto neuromorphic computing chips despite the uncertainties. \n\n(2) Is there any intuition behind replacing every weight after every k epochs with new samples (Sec. 2.3)?\n\n(3) The paper does not mention the overhead of estimating the loss with n feed-forward passes dramatically slows the training process. I assume it will slow down training by n times? \n\n(4) There is a comparison between retraining and fine-tuning. Despite being less accurate, is fine-tuning faster to train in terms of the actual training time?\n\n(5) Here I quote the paper \u201cThe UATS performs better when the neural network has more layers\u201d (Sec. 3.2). I cannot find an empirical comparison that supports this claim. \n"}