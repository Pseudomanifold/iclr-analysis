{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper studied reinforcement learning from demonstration. Given a set of \nexpert demonstrations, this work provides a policy-dependent reward shaping objective that\ncan utilize demonstration information and preserves policy optimality, policy improvement,\nand the convergence of policy iteration at the same time, under the assumption\nthat expert policy is optimal and stochastic.\nThe main advantage of the proposed method is that the reward shaping function\nis related to the current policy. \nA practical algorithm based on theoretical derivation is provided. The authors conducted sufficient experimental results to demonstrate that the proposed method is effective,\ncomparing with a set of advanced baselines.\n\nI recommend acceptance: \nPrevious works on RLfD usually empirically incorporated a regularization\nto the RL objective, while those works didn't discuss whether this\nregularization will lead to sub-optimal policy or not. This paper discussed\nhow to use the demonstration information to do exploration and maintain\npolicy invariance at the same time, with a relatively strong assumption.\nUsing the framework from SAC, the\nalgorithm is shown to converge to the optimal via policy iteration, in\ntabular case. This work also developed a practical expert policy\nsupport estimation algorithm to measure the uncertainty of\nexpert policy. Utilizing the adversarial training framework, \nthe explicit computation of expert policy is avoided. \nThe authors conducted sufficient experiments to demonstrate\nthe effectiveness of the proposed method, compared with the state-of-the-art in RLfD. \n\n\nTechnical concerns:\nThe stochasticity assumption of expert policy in Asm. 1 can be contradicted\nwith that expert policy is optimal in policy invariance proof.  \nThis paper works on a problem of infinite horizon discounted MDP. \nAccording to Puterman [1994], Theorem 6.2.7, there always exists a\ndeterministic stationary policy \\pi that is optimal. Or intuitively,\nif we find the optimal value function via Bellman optimality equation,\nthe optimal policy is acting greedily (deterministic). The provided theorems are\nnot compatible with the MDP where only deterministic optimal policy exists.\nIt is not clear that in what type of MDPs the optimal stochastic policy exists and\nit can satisfy Asm. 1. \nCould the authors clearly specify the applicable problem settings?\n\nIf the asm 1 is satisfied, what is the necessity to incorporate the\nindicator function in Eq 4.? Since p(s) > 0 for all s, following strong stochasticity policy.\nFor any trajectory \\tau, p(\\tau) = p(s)\\Pi_t p(s_{t+1}|s_{t}, a_t)\\pi_E(a_t|s_t) > 0. \n\nThe proof of Theorem 2 is similar to the proof in Proposition 1, [1], though in a\ndifferent context. It would be better to have a citation?\n\nExperiments:\nIt would be more convincing to show the performance of behavior cloning policy using\nexpert trajectories. \n\n[1] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014.\n\n"}