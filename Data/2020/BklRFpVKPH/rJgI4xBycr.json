{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to mix reinforcement learning and imitation learning to boost the learning of an actor critic architecture. The authors use a regularized reward function that minimizes the divergence between the policy of the expert and the one followed by the agent. They use demonstrations obtained from a trained agent and experiment their method on several mujoco tasks. \n\nI have many concerns about this paper. First, The state of the art is missing important pre-deep-learning references such as: \n\n1. Direct Policy Iteration with Demonstrations: Chemali and Lazaric\n2. Learning from limited demonstrations, Beomjoon et al.\n3. Residual Minimization Handling Expert Demonstrations, Piot et al.\n\nThen, they make a mistake by saying that DQfD only considers transitions from the expert as self-generated and placed in the replay buffer. DQfD actually uses the same additional structured classification loss than Piot et al. [3] (except that they use boosted trees instead of deep networks, DQfD and Piot et al. are the same algorithm).\n\nAlso, the proposed solution here is equivalent to regularizing the MDP with a KL divergence  w.r.t. to an initial policy that would be the one of the expert. It is already studied in several works and more generally it comes with some assumptions on the policy update. It is generally studied in \n\n4. A theory of regularized MDP, Geist et al\n\nThey actually propose exactly the same framework as a special case in the appendix of that paper. \n\nIn addition to not be very novel, I think the method has some flaws. The authors use demonstrations coming from a pre-trained network which is known to make the imitation learning part much easier. Especially if it comes from an RL agent using similar deep RL algorithms (which is the case here). Finally, they only test on mujoco tasks which are very specific tasks with deterministic dynamics and very dense rewards  around states visited by the optimal strategy so initializing with an expert policy that is learned from demonstrations of a similar network of course helps. I would be more impressed by experiments on stochastic environments and sparse rewards. \n\nFinally, there is a concurrent work submitted to the same conference. Of course the authors could not know but I\u2019d like to have their impression about how their work is different.\n\nhttps://openreview.net/forum?id=BJg9hTNKPH&noteId=BJg9hTNKPH"}