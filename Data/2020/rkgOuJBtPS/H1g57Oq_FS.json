{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper explores a method for correcting typos in a \"worst-case\" way, in which worst case means against, for example, all mistakes of edit distance k for specified k. To me, this is an interesting problem because 1) typos do not work for word vectors, which is the most common input in downstream NLP tasks, 2) minimizing edit distance is a combinatorially hard problem, so all brute force methods are infeasible at scale. \n\nThe paper provides a nice discussion of these issues, and proposes a graph-based method in which all words + their typo versions form a graph, and either a word is corrected by a representative word which is the most popular amongst connected components, or a softer version of this is used. The paper also describes a tradeoff between stability and fidelity (which I think is better understood as performance degradation) and discusses how this approach that really is task agnostic. \n\nOverall, I think the scheme is simple to understand and intuitive as to why it works well. Table 1 also looks like a nice collection of tasks, and the baselines seem reasonable. The main concern I have is I don't see how the combinatorial nature is reduced. If you are given a corpus and someone writes a typo, what is the procedure? Is that typo already stored in a database, in which case the database is exponentially large in memory? Or do you need to compute edit distances in order to create this graph to begin with, having the combinatorial issue there? If the combinatorial issue is not resolved then I don't see why this is better than just a nearest neighbor lookup where the distance is edit distance, which otherwise I would suggest as another important baseline. (If the authors can show this is a misunderstanding on my part, and/or include a much more in-depth discussion of the computational complexity of each scheme, I will increase my score.) \n\nMy other main critique is that some things are a bit vague. For instance, I don't understand how the sentence embedding is created, though it is discussed--the exact procedure is vague. Also, I don't really understand equation (5). What is the std taken over? What is the random variable? It seems like the first term should be conditional on the accuracy of a typo-free scheme, since the task can be arbitrarily hard. \n"}