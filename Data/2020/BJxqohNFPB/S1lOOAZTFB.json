{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a method to improve the image-to-image translation. By utilizing the CG-based Synthia and embedded edge maps, it has shown some effects on the Cityscapes dataset.\n\nThis paper is not novel. Generally, it just utilized additional sources to help the generation. It is difficult to generalize to another dataset/domain as the CG-based set is not easy to get. The model side has very limited novelty. The pipeline and motivation are similar to previous work for domain adaptation for generative models and bridging domains:\n1. Unsupervised Image-to-Image Translation Networks\n2. Adaptation Across Extreme Variations using Unlabeled Domain Bridges\n\nThe experimental study is weak. Only the Cityscapes dataset is included. It is good to see if this method can work on others, like ADE20k and COCO-stuff. Also, some strong baseline, such as SPADE, is ignored. "}