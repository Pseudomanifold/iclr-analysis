{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary: \nThis paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The paper provides promising experimental results on CIFAR-10 by obtaining state-of-the-art certified accuracy while also simultaneously improving clean accuracy. The paper is overall well-written and the algorithm is clearly described. \n\nImportant questions to be answered: \nI find the need to clarify my understanding and request for more information in order to make a decision. \n\n--Methodology/motivation for the method: I am trying to understand abstractly what the proposed layerwise training is trying to optimize. To be concrete, let's compare to the relaxation of Wong and Kolter (which this paper uses in the instantiation of layerwise adversarial training). What's the exact difference?\n\nOne way to view this is the following: The same training objective, but a different way to optimize. The new proposal to train involves freezing weights until one layer iteratively starting from the input layer. It is possible that this kind of training provides some inductive bias in finding better solutions. Is this an appropriate understanding? \nHowever, the paper\u2019s experimental results unfortunately change the certification procedure. In other words, they haven\u2019t evaluated the same training objective as that of Wong and Kolter. Hence, it\u2019s not clear if the gains are from the better networks, or better certification method, or network being better suited for certification by the method used. The phrase \u201csame relaxation\u201d is not appropriately used. Their certification procedure uses a different (and tighter) relaxation. \n\n\n--Effect on latent adversarial examples: I am unable to understand why this training procedure would reduce the number of latent adversarial examples. The definition of latent adversarial examples seems to suggest that it\u2019s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. However, the proposed layerwise adversarial training procedure involves replacing the actual set S_i with the convex hull C_i when freezing things till below i-1. I do not follow how the proposed method tries to make C_i = S_i. Implicitly the optimization objective does try to make C_i small because the bounds being optimized are tighter when C_i is small. But this is true even for normal certified training, and not sure what changes in the new training procedure.\n\n\nSpecific experimental results that would help: \n--Certified accuracy on using the same LP based certification procedure used in Wong and Kolter with the new layerwise trained networks\n--The paper\u2019s own certification procedure (a combination of previous methods) on the network from Wong and Kolter or a note on why that doesn\u2019t apply (if it doesn\u2019t)\n--The paper currently provides only one data point to suggest this training method is superior. Would be good to try SVHN or MNIST. MNIST is perhaps \u201cessentially\u201d solved for small \\eps. But would be good to see if the training method offers gains at larger \\eps. In general, would be good to see more consistent gains. \n--The paper reports results on first 1000 examples of CIFAR10 test set. From my personal experience, there is a lot of variability in the robustness of test examples when evaluated on 1000 random test instances. Especially since the paper doesn't take a random subset, it might be good to make sure the gains are consistent on some other subset. The Wong et al. baseline is evaluated on the entire test set for example, and hence might not be a fair comparison? What's the Wong et al. accuracy on just the first 1000 test exampes\n\nOverall, I am leaning towards accept but need some conceptual and empirical clarification from the authors (detailed above). \n"}