{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper was very clearly written and easy to follow. Kudos to the authors. In particular, the experimental evaluation section was exceptionally clear. Thanks to the authors for making the paper so easy to review. The \u201cMain Contributions\u201d section was excellent as well as it allows the reader to quickly understand what the paper is claiming.\n\nThe introduction & related work section was very clear, and seemed to quickly get the reader up to speed.\n\nMinor critiques:\n\n- It\u2019s not clear to me that the network size is actually as impressive an improvement as is implied. Barring an extensive hyper-parameter search that demonstrated that this network architecture is the smallest possible that could achieve the presented results, I strongly suspect that applying techniques from papers like EfficientNet [1] or MobileNet would allow the authors of Mirman et. al (2018) to reduce the number of parameters required to achieve their results. I don\u2019t think this takes away from the paper, though- the results are strong despite that. I would encourage the authors to weaken the claims that the only better network is 5 times larger. \n\n- In general, I would have liked to see more evaluation- e.g. I would have liked to see more results with a variety of perturbations (2 through 8, not just 2 & 8), and on a variety of datasets. \n\nQuestions to the authors:\n\n- How robust is the algorithm to architecture choice? \n- What happens if you change the test set? E.g. instead of evaluating on the first 1000 images, what if you evaluate on another random subset? Does that make a difference? I\u2019m concerned that the subset of the test set the authors are using for evaluation isn\u2019t representative of the entire test set. \n- Is the current architecture the largest network that can be run? I would be interested in seeing how network size affects the performance of your technique. \n- What hyper-parameter tuning did you do? What other network architectures did you try? \n- How do the comparison methods compare in terms of training time/machines used? E.g. do all the methods reported in Table 1 use similar amounts of computing power? \n\n\nOverall, this is a great paper, with some interesting results presented in a tight, clear manner. While I would like to see more experiments on larger datasets- e.g. ImageNet- the results seem solid and absolutely worthy of publication.\n\n[1]: https://arxiv.org/abs/1905.11946v2\n[2]: https://arxiv.org/abs/1704.04861"}