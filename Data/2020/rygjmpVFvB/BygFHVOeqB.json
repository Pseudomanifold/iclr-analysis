{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary\n\nThis paper provides an interesting application of GAN which can generate the outlier distribution of training data which forces generator to learn the distribution of the low probability density area of given data. To show the effectiveness of the method, the author intuitively shows how it works on 2-D points data as well as the reconstructed Mnist dataset. Additionally, this approach reaches a comparable performance on semi-supervised learning and novelty detection task.\n\nPaper Strengths\n\n1. The idea of this paper is novel, and the implementation of this method is easily interacted with any GAN model. Also, due to its concise structure compared to the existing method, it saves more computational memory and is time efficiency.\n\nPaper Weaknesses\n\n1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images. \n2. The model seems to be sensitive to the hyper-parameter \\alpha, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?"}