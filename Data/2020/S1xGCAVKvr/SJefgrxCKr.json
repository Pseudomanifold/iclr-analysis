{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors build on the 'learning to learn' work, that aims to leverage deep learning models with optimization algorithms, most commonly with recurrent networks.  The goal is to utilize meta-learners that can adapt the optimization update strategy based on data/experience.  The authors aim to tackle problems that often arise in such meta-optimization schemes, such as covergence and generalization problems.  The paper is overall well-written, and several experiments are presented\n\nBuilding on previous work, the authors propose some variations to the meta-learning schemes and architecture.  I comment on these below.\n\n- The authors remove the bias term based on the intuition that for the meta-learners, removing the bias can lead to better convergence.  This is supported by a set of experiments in Fig 3a (but with only one learning rate?).  This experiment shows an extraordinary difference in accuracy between including and not including the RNN bias.  I think, because this difference is substantial (along with papers that do use the bias in meta-learners showing good results overall), more evidence/ablations are required.  \n\nCan we can be sure that if the bias is non-zero (as in other reported works), we should expect a worse performance?.  Clearly this change seems to speed up the process, and in the example explained in the paper it makes sense, but could there be examples where lack of bias might lead to worse results?\n\n- The authors claim that one of the problems of learning-to-learn algorithms is the late-stage optimization strategy.  While previous works introduce a weighting for the meta-optimizer's loss function (usually binary), the authors extend this to continuous monotonically increasing functions in order to weight the late stage steps of the algorithm more.  The authors propose compensating small loss difference happening during late optimization with a larger weight.   This naturally brings the question of which function should be used, as showing that any function will do seems difficult in practice. Have the authors tried any functions that failed or led to overfitting in comparison to other works?  Could one argue that if the loss difference is not large then perhaps the network can be more prone to overfitting at least in some cases?\n\nFinally, the authors introduce embedding sharing between coordinates (aggregating hidden states), and also propose fine-tuning (sampling 5% of training data 5 times for warm-up).  Sharing hidden state information is expected to be useful (and has been often employed in literature), similarly to fine-tuning.  Also, 5% of dataset and 5 times seems to be an arbitrary choice - probably more related to the experimental section rather than the method itself.  \n\na question on fine-tuning:  if fine-tuning was used for the proposed method, have the compared methods also been pre-trained to provide fair comparisons?  This is particularly relevant to fig 4B where only the fine-tuned method slightly overperforms SGD.\n"}