{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper presents several improvements over the existing learning to learn models including Andrychowicz et al. (2016) and Lv et al. (2017). Specifically, this paper analyzes the issues in the original learning to learn paradigm (L2L), including instability during training and bias term issues in the RNN. It proposes a new loss based on weighted difference for improving the meta-optimizer in the later stage of optimization. It also proposes information sharing between RNNs for each coordinate. Finally it presents how to fine-tune the meta-optimizer on new task. \n\n\nPros:\nReasonable technical improvements to fix some issues of the learning to learn framework\n(1) reduce the number of parameters in the meta-optimizer significantly\n(2) improve the stability of the meta-optimizer.\n(3) improve the generalization to new tasks and datasets.\n\n\nCons\n1. The novelty is not good enough and the method does not seem to be solid enough. Except for the technique in the *structure of each recurrent unit\" section, other techniques are tricks that are hard to tell why they could work. That said, I think the experiments should verify each of the proposed components, and see their roles in the proposed method.\n\nAlso, it is claimed in the Abstract and the paper that the proposed method *successfully converge to optimal solutions\"... I think this claim is quite unprofessional. How do you know it converges the optima?\n\n\n2. With respect to the experiments:\n\nThe experiments are only done on the MNIST and CIFAR10 datasets, which are small-scale datasets. Current meta learning model has achieved advancement on more challenging datasets, e.g., Mini-Imagenet and CIFAR1000.\n\nFor the experiment section of comparison to baseline methods. It would be fair to compare all the methods with removing bias or not removing bias setting. As the author mentions that \"the maximal bias term of LSTM in Lv et al. (2017) is 0.41, and maximal bias term of LSTM in Andrychowicz et al. (2016) is 0.31; this, consequently, leads to bad performance\". It would be interesting to know that if the bias term of the two baseline model are removed, how is the performance difference compared to the method proposed by the authors? \n\nHow does the number of parameters of the meta optimizer scales with the problem size? That is, how does the number of parameters in the meta-optimizer grow with increasing the number of image classes in the problem at hand, e.g., 20 classes, 50 classes, etc?"}