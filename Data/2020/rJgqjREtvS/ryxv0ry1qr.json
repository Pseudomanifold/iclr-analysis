{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a single image super-resolution method. The discussion refers to sparse coding as motivation and how such coding can be achieved within a neural network using activation functions, and then it slides into iterative soft thresholding and ends up with the observation that ReLU and the soft nonnegative thresholding operator are equal (paper should have pointed that they are equal with slight difference around the bias term). This argument is not new, this conclusion has already been known and analyzed extensively in (Papyan et al., 2017a).\n\nThe proposed network is not much different from what several other SR methods used before (see NTIRE 2019). It is basically composed of a collection of single layer residual units that share the same parameters. As expected, there is also a skip connection from the input to the last layer. The only slight variation is that the activation function ReLU for the residual layers are arranged before the convolutional layers. \n\nTraining details are missing. \n\nThe paper does not provide any comparisons with the top-ranking methods in the NTIRE 2019 single image super-resolution challenge leaderboard. "}