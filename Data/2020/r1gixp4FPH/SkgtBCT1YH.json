{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "1) In the statement of theorem 1, what do you mean by \u201cwith probability one\u201d and \u201cconvergence in expectation\u201d together? The inequality (7) does not have any random variable anymore after taking the expectation. Need to explain more clearly this part. \n\n2) Basically, all the results of this paper is based on the (or close to) strongly convex property. However, numerical experiments show for some non-convex functions, specifically for deep learning problems. It is unclear what kind of loss function the author(s) are using for training classification problems on MNIST and CIFAR-10. This could be softmax cross-entropy but not quadratic. \n\n3) The theoretical results in this paper are not strong. The interpolation setting could make all solutions of the component function are the solution of the total loss function. In this situation, we know that stochastic algorithms could take advantage because of \u201cautomatic variance reduction\u201d. \n\n4) The result in this paper is quite incremental from the one in Vaswani et al 2019, \u201cFast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron)\u201d. More discussion is needed if the author(s) think that it has significantly improvement. \n\n5) It is true that Nesterov SGD is very efficient for training neural networks and MaSS may have some effect in practice. However, theoretical part needs to be improve. I would suggest to analyze for nonconvex problems or using the assumptions which are verifiable and reasonable for neural network. \n\nThe work may be potential, but in order to convince people to trust this algorithm, rigorous theory must be provided. Some experiments in the paper are not representing all scenarios that MaSS may not work. \n"}