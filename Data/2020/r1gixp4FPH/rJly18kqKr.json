{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper shows the non-acceleration of Nesterov SGD theoretically with a component decoupled model. Moreover, the authors introduce an additional compensation term and derive a novel optimization method, MaSS. MaSS is both theoretically and empirically proved to outperform Nesterov SGD as well as SGD.\n\nPros\n1. It's amazing to see the great improvement introduced by the compensation term into the theoretical result of MaSS. Moreover, the authors generalize the setting of square loss function to other convex loss functions.\n2. The encouraging result in Table 1 in EMPIRICAL EVALUATION shows the consistent outperformance of MaSS over SGD and Nesterov SGD regardless of the changing learning rates.\n\nCons\n1. The discussion on why the zero eignvalue can be ignored in Section 4 is insufficient. \"(stochastic) gradients are always perpendicular to W^*\" seems not that obvious.\n2. The empirical result merely involves two settings of learning rate: 0.01, 0.3. I suggest a wider range of learning rates to show the outperformance of MaSS.\n\nSome typos\nLast line of the first paragraph in INTRODUCTION: there is a redundant \"can\". 7th line of the 5th paragraph in INTRODUCTION: there is a reduntant \"the\" after \"In this case\".\n    "}