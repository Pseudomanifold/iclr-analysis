{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper is interested in music generation, leveraging a 2D representation of the music data. \n\nThe idea of using a 2D representation is quite good, though here very specific of the considered type of music. The authors might want to discuss related approaches , considering 2D general representations of music [1, 2]. The originality of the proposed approach is to use dilated convolutions to capture the long range dependencies in a GAN framework. \n\nThe paper however looks premature for publication at ICLR, for several reasons:\n\n* The thorough discussion of the dataset might be put in supplementary material. Instead, the reader would like to know whether the authors considered data augmentation of their relatively small dataset (820 tunes). Likewise, the description of the architecture could be put in a supplementary section, or on github for reproducibility.\n\n* The authors did not justify the choice of the specifics of the architecture, such as the number of filters, layers or the presence / absence of batch normalization (part 3.1). The reviewer would like to see how the results vary with respect to those parameters. It would also be interesting to see what the generated images look like before the tanh (part 3.1.2).\n\n* The assessment of the results is hard to interpret; the reasons why the Frechet distance varies depending on the models and the phrases combinations should be discussed.\n\n* The gold standard for evaluating music is based on the human assessment of the generated music (involving naive, advanced and expert people), as done in [3], cited; having a human being evaluate and compare the tunes would help to gain insight into the generation. In the same perspective, it would be appreciated to put the results on a website for the reader to assess the quality of the generated music. \n\n* The contraction of the support (as displayed in Fig. 3) suggests that there might be some mode dropping with the GAN; this should be studied in depth. \n\n* The authors consider Magenta and FolkRNN as baselines; the reviewer suggests to also consider e.g. [4]  (although not not exploiting the specifics of the style), to comparatively assess the proposed approach.\n\n[1] \"Onsets and Frames: Dual-Objective Piano Transcription\", Hawthorne et al., 2017.\n[2] \"TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer\", Huang et al., 2018.\n[3] \"DeepBach: a Steerable Model for Bach Chorales Generation\", Hadjeres et al., 2017\n[4] \"Music Transformer\", Huang et al., 2018."}