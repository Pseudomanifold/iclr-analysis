{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper describes a (DC)GAN architecture for modeling folk song melodies (Irish reels).\nThe main idea of this work is to exploit the rigid form of this type of music --- bar structure and repetition --- to enable 2-dimensional convolutional modeling rather than the purely sequential modeling that is commonly used in recent (melodic) music generation architectures.\nThe ideas in this paper seem sound, though they primarily consist of recombining known techniques for a specific application.\nThe main weaknesses of this paper are in the evaluation (see below), and while I understand that evaluating generate models for creative applications is difficult and fraught territory, I don't think the efforts taken here are sufficiently convincing.\n\n\n\nStrengths:\n\n- The paper is clearly written, and the authors have taken great care to describe the unique structures of the data they are modeling.\n- The proposed architecture seems well motivated, and matches to the structure of the data.\n\n\nWeaknesses:\n\nThere are three components to the evaluation, and each of them are problematic:\n\n- The first evaluation (Fig 2) compares the average Frechet distance between phrases generated by different models, and within the original dataset.  Some brief argument is given for why Frechet is a good choice here, but it still seems quite tenuous: what does this distance intuitively mean in terms of the data?  How should the scale of these distances be interpreted / what's a meaningfully large difference?  How concentrated are these average distances (ie, please show error bars, variance estimates, or some notion of spread)?\n\n- The second evaluation (Fig 3) uses t-SNE to embed the generated melodies into a 2D space to allow visual inspection of the differences between distributions produced by each model.  While this might be a reasonable qualitative gut-check, t-SNE is by no means an appropriate tool for quantitative evaluation.  The authors at least did multiple runs of t-SNE, but this hardly amounts to compelling evidence.  Moreover, combining all data sources into one sample prior to running t-SNE induces dependencies between the point-wise neighbor selection distributions, which seems undesirable if the eventual goal is to determine how similar each model's distribution is to the source data.  A better approach might be to create independent plots for each model's output (with the original data), but I'd generally advise against using t-SNE for this kind of analysis altogether.\n\n- The third evaluation (Fig 4) measures the amount of divergence from the key (D) in terms of note unigrams.  This evaluation is done qualitatively, and the histogram is difficult to read --- it may be easier to read if the octave content was collapsed out to produce pitch classes rather than pitches.  If, however, the goal is to actually measure distance from the target key, one could do this quantitatively by comparing histograms to a probe tone profile (or otherwise constructed unigram note model) to more clearly characterise the behaviors of the various models in question.\n\n\nAt a higher level, there is no error analysis provided for the model, nor any ablation study to measure the impact of the various design choices taken here (eg dilation patterns in Figure 1). \nThe authors seem to argue that these choices are the main contribution of this work, so they should be explicitly evaluated in a controlled setting.\n"}