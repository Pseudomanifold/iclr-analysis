{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper attempts to introduce a neural network which structure which is inspired by use of episodic memory in the brain. Unfortunately, it is very difficult to discern what the precise contribution is, either from the methods sections or the experiments, and a few competing goals of the paper seem to be at odds.\n\nStarting high-level, I still am not clear what the paper's fundamental goal is. In the abstract, there is the stated goal that a challenge to understanding the results of cognitive experiments on animals is \"incomplete access to relevant circuits in the brain\", and that ANNs can \"model neural circuits\". This is a fine goal \u2014 using a computational model of the brain as a means to better understand its latent processes when analyzing experimental data \u2014 but it is worth bearing in mind that actual models of the brain do not have much in common with typical deep learning architectures, even recurrent networks. However, the following sentences seem to completely contradict this goal! Instead of building a low-level model of the brain, the goal is to build an ANN which mimics the high-level cognitive behaviour. This is also a fine goal \u2014 mimicking humans or animals in behaviour at particular tasks makes sense, and may even help us learn about high-level cognitive function \u2014 but it is not at all clear what this has to do with recording \"the electrical activity of the animals neurons\". To me these seem like fundamentally different goals, with different types of models required: a biophysical model for the former, probably, and a deep learning model for the latter.\n\n\nUnfortunately, I do not understand what model is being introduced. Section 2.1 introduces a variant of a GRU, but does so without defining any relevant notation, and without defining the standard GRU in their notation by which we could compare differences. Instead, we are given a description of a simple RNN (in a continuous-time formulation), and then the modified GRU (in a discrete-time formulation). Why is the standard GRU not presented, if that forms the basis for modification? What are these modifications? It seems like the primary change is injecting gaussian noise in Equation (4), but the text claims the primary change is the use of a \"threshold-linear\" (I assume this is the same as rectified linear) activation on the output. Also, what is the function f() in equation (4)? The text says \"The function f(x) = x is a linear function\" \u2014 which is, in fact, the identity function. Why is the identity function here?\n\nWhich part of this modified GRU corresponds to episodic memory?\n\nIf this is not the episodic memory, where is it? There are no further equations anywhere in the paper. The entire high-level description in section 3.2 of a \"replay buffer\" is far too vague to constitute an explanation. How is this buffer used? An algorithm block, at the very least, would be necessary. From the text, I have no idea how to implement this method.\n\nExperiments: the correlation to the animal behaviour in Figure 3 is quite good. However, this is a very small setting \u2014 is this all the data available from Roitman & Shadlen, or similar studies?\n\nAlso, multiple times the paper points out the goal is not to maximize performance, but rather to mimic the animal data. I think instead it would be good to re-define \"performance\" as correlation, or some other measure, relative to the animal behaviour: \"good performance\" means the left and right columns of figure 3 nearly match. This can be quantified!\n\nHow well do other algorithms, without the episodic memory or the modified GRU, perform at the task in figure 3? What is a meaningful baseline here \u2014 do other machine learning methods perform meaningfully differently from the proposed method?\n\nFinally, I am confused by the experiments on the final two pages. It is stated it takes 18,000 trials for the agent to achieve the baseline accuracy rate. How many trials does it take the monkeys in the original study? Is it possible to also include the monkey performance in figures 4 and 5, is matching the monkey performance is the goal?\n "}