{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an adversarial defense method that is a saliency-based adversarial example detector. The method is motivated by the well-known fact that saliency maps and adversarial perturbations are having similar mathematical formulations and derivations. By using model-based saliency maps rather than gradient-based ones, it seems to detect hard attacks with smaller perturbation size as well. As far as the authors mentioned, the proposed method is simply using different techniques to derive saliency maps compared to the previous methods.\n\nOverall, the intuition and motivation of this paper are from the previous works and the main contribution is to use another (powerful) saliency map extractor for learning an adversarial detector. Although the overall results are improved from the previous methods, the proposed method is lack of novelty. \n\n- For SMD (Saliency Map Defense), what is the reason that the input image is not used together? computational issue? performance degradation? \n- Is it possible to train a single detector that can handle all different adversarial attacks?\n- Would the distance between saliency maps from different attacks be small? How does the saliency map change under different attacks?\n- Have you tried any other powerful saliency maps other than Dabkowski & Gal (2017)?"}