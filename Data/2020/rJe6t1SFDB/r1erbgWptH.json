{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a unified representation of query-response pairs for response selection in chit-chat dialogues. The representation includes the following four core components: syntactic embedding by graph convolution network, sentence representation from BERT, knowledge embedding based on entity linking to Wikipedia, and context representation with memory networks. The experimental results show the effectiveness of each proposed representation in both automatic metrics and human evaluations.\n\nThe main contribution of this work is to combine various different representations into a single end-to-end model architecture for dialogues response selection. However, each of the proposed ideas is not novel enough. And many parts of the paper are not very clearly written. The experiments also need to be further improved to support the contributions of this work.\n\nPlease find below my major concerns:\n- The motivation of each proposed component is not very clear. I suggest to highlight this more in the Introduction.\n- It would be better to emphasize the contributions of this work in Related Work section. Now the section just lists the related studies with no explicit comparison to this work.\n- I'm not convinced that the proposed methods are novel enough to be presented at the conference. I don't find any significant contribution of this work from the original work for each core component.\n- I'm curious how much the long range dependency problem affects to the dialogue response selection task in general. The experimental results show that the model with 'Bi-GRU & GCN' helped to improve the performances. But there should be an additional configuration only with 'Bi-GRU' with no 'GCN' to see the impact of the dependency parsing.\n- It's a bit confusing why the BERT embedding is considered as a knowledge module along with the actual KB representation with Wikipedia.\n- I'm wondering what if the Bi-GRU in the syntactic module is replaced with BERT. In the experiment, the BERT only model already achieved higher performances than the Bi-GRU+GCN only. It would be also interesting to see the performances of BERT+GCN.\n- It's not clear how to get the KB embedding in Section 4.2.2. Did you take the title of Wikipedia article or also with body texts for each entity? More details are needed.\n- This model uses Stanford CoreNLP for dependency parsing and entity linking. I guess there might have been some errors from CoreNLP models due to the characteristics of chit-chat conversations. I suggest to add some analysis to show the impact of the pre-processing errors to the overall performances.\n- It's not clear why DSTC2 dataset is used in the experiment. I don't think this dataset is appropriate for response selection. I suggest to use DSTC7 Track 1 dataset at https://github.com/IBM/dstc-noesis instead.\n- It would be great to compare the model performances with other stronger baselines on Persona Chat."}