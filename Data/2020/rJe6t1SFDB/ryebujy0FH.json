{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a model for dialogue understanding, named AMUSED (A MUlti-Stream vector representation method for USE in natural Dialogue). The method has three main components to understand a sequence of dialogue utterances. A) syntactic component based on dependency parse of utterances. B) knowledge base module based on entities linked from the utterances using off-the-shelf entity linker. C) memory network module which conveys information from previous utterances. The method uses a triplet loss, which compares the correct question - answer pair, with the negative example (question - randomly sampled answer) pair to train the network parameter. \n\nI don\u2019t think this paper is strong enough for ICLR as it is now because (0) the proposed method lacks novelty (1) the evaluation task is not well motivated (2) the experimental result doesn\u2019t support comparison to existing methods, (3) the paper is not very clearly written. \n\n(0) The proposed method lacks novelty: the paper basically combines many existing ideas and stacks them together for the task of answer retrieval for conversational question answer dialog. While the paper argues using \u201cmulti-head attention\u201d as contribution at the end of introduction, this is more or less standard practice these days. Overall, I don\u2019t see a technical novelty for this paper. \n\n(1) I don\u2019t think \u201cNext Dialogue Prediction Task\u201d is very well motivated, and this is main evaluation measure of this paper. It\u2019s not very realistic scenario, and proposing this new task makes the work hard to compare with existing work. The authors should motivate this task more clearly. \n\n(2) The experimental result doesn\u2019t support valid comparison to existing methods. Moreover, important experimental details and ablations are missing. While the paper puts together many modules, the experiments don\u2019t justify those modules. For instance, what was the benefit of using KB? Table 1 does not isolate its effect as it\u2019s conflated with memory network component. I\u2019m skeptical whether it does anything. Similarly with GCN. The paper should present the model\u2019s best performance, and best performance - GCN component to show its effectiveness.  The paper is also missing many details: for example, for knowledgeable module, how frequently entity mentions were detected and used? (Because it must have an entity name embedding in GLoVE vocabulary?) \n\nHuman evaluation section also needs more rigor. It isn\u2019t clear how many examples were manually evaluated, and how reliable were the annotators. How would you describe \u201cexpert linguists\u201d? Any measure of inter-annotator agreement? \n\n(3) The paper is not very clearly written, leaving readers confused. For example, \u201cdistribution bias\u201d at the end of the second paragraph of page 1 should be better explained and defined. So is \u201cInteresting response retrieval\u201d at the end of Section 1. What is \u201cMulti-stream\u201d?  Table 2 is really confusing, from what I understand, the last two rows are of the same system on different datasets. \n\nOther comments:\nIn section 4.1., the paper talks about how syntax information have been helpful. I think it\u2019s a bit dishonest to stress it here, given most state-of-the-art NLP models (pre-trained LMs) doesn\u2019t use any syntax. \nThroughout the paper, capitalization is inconsistent. \nThe paper uses \u201cnext sentence prediction\u201d , \u201cnext dialogue prediction\u201d interchangeably. Please be consistent.\nPrecision@1 isn\u2019t the accurate or intuitive description of what\u2019s happening. "}