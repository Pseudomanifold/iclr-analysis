{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Contributions: \n\t-Authors have proposed a methodology to optimise high dimensional functions in a derivative-free setup by reducing the sample complexity by simultaneously learning and optimising the low dimensional manifolds for the given high dimensional problem. \n\nAlthough, performing dimensionality reduction to learn the low dimensional manifolds is popular in the research community, the extensions made and the approach authors have considered seems to be novel.\n\nComments:\n\t\n\t- Authors have talked about the utilization of domain knowledge on the geometry of the problem. How feasible is to expect the availability of the domain knowledge? Authors have not discussed the downsides of the proposed method if the domain knowledge is not available, and a possible strategy to overcome the same.\n\t- Authors have said that they are specifically interested in random search methods. Is there any motivating reason to stick to the random search methods? Why not consider other sample efficient search methods? \n\t\u201c\u2026\u2026.random search scale linearly with the dimensions\u201d, why one should not consider other sample efficient methods that grow sub-linearly as against random search? \nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. International Conference on Machine Learning, 2010\n\t- Please derive Lemma 1 in the appendix for the sake of completeness.\n\t- I am missing a discussion about manifold parameters like \u201c\u03bb\u201d in the important equations. \n\t- Authors have made a strong claim that neural networks can easily fit any training data, but it may be not be true for many datasets.\n\t - Authors have claimed that they have fast and no-regret learning by selecting mixing weight \u03b2=1/d. Author might want to discuss more on this as this is an important metric.\n\t - \u201c \u2026. total time spent on learning the manifold is negligible\u2026. \u201d \u2013 any supporting results for this claim.\n\t - \u201c\u2026\u2026.communication cost from d+2k to d+2k+kd\u2026 \u201d \u2013 curious to know if there is any metric like wall-clock time to talk about the optimisation time.\n\n\t- Authors have restricted the comparisons to only three important methods, but it is always comparing with other baselines in the same line. Authors should consider Bayesian optimisation as it is a good candidate for the performance comparison, even though the researchers are interested only in random search methods (just like CMA\u2013ES).\nKirschner, Johannes, Mojm\u00edr Mutn\u00fd, Nicole Hiller, Rasmus Ischebeck, and Andreas Krause. \"Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces.\" arXiv preprint arXiv:1902.03229 (2019).\n\n\t- It is seen from the results that the proposed method is not performing better for low dimensional problem like \u201cSwimmer\u201d function. But according to the initial claim, method was supposed to work better in low dimensional problems. Is it because of the fact that the problem space is not drawn from high dimensional data distributions? \n\t- \u201c\u2026..improvement is significant for high dimensional problems\u201d \u2013 It will be better if the authors compare their proposed method with some more derivative-free optimisers that are proven to be good in high dimensions (like high dimension Bayesian optimisation).\n\t-  \u201cThe no-learning baseline outperforms random search \u2026\u2026\u2026.\u201d \u2013 this statement is not very clear, does it mean like the proposed method works only when the problem is reduced from higher dimensions to lower dimensions and not on the lower dimensional problem itself?\n\t- \u201cPerformance profiles represent how frequently a method is within the distance T of optimality\u201d \u2013 Any thumb rule considered for the choice of T?. Can we think of any relation with standard metrics like simple regret or cumulative regret that are used to measure the optimisation performance?\n\t- \u201cAlthough BO methods typically do not scale\u2026\u2026 \u201d \u2013 Authors have made a strong assumption here. In the literature, we see active research in the context of high dimensional optimisation.\nRana, Santu, Cheng Li, Sunil Gupta, Vu Nguyen, and Svetha Venkatesh. \"High dimensional Bayesian optimization with elastic gaussian process.\" In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2883-2891. JMLR. org, 2017.\n\t\n\n\nMinor Issues:\n\t\u201cImprove the sample complexity\u201d may not convey the meaning very clearly to the readers, something like \u201cImprove the sample efficiency\u201d or \u201cReduce the sample complexity\u201d would add more clarity.\n\tInconsistency in the terms used in Proposition 1 and Proposition 2. What does \u201cI\u201d signify in the formula? Was that supposed to be \u201ct\u201d?\n\tEven though the constants are mentioned in the appendix, it is always better to mention the constants used in the algorithm like \u201c\u03b1\u201c as step size for quick understanding.\n\t\u201cFollow The Regularized Leader (FTRL)\u201d is more appropriate than \u201cfollow the regularized leader (FTRL)\u201d\n\t\u201cnoise table in pre-processing\u201d \u2013 Should it mean something relevant to the paper? \n\t\u201cWe use widely used\u2026..\u201d \u2013 may be consider rephrasing the sentence here\n\t\u201ctreshold\u201d \u2013 Typo in Table 1\n\tY \u2013 Axis in Figure 2 is missing\n\tAppendix B :\n\t\u201cWe also perform grid search \u2026. \u201c would look better\n\tMuJoCo Experiments \u2013 is the parameter space continuous and what is the search space considered for n, \u03b1 and \u03b4. Do we deal with smaller search spaces in every problem? Any other way of searching the parameter space to further improve the efficiency? \n"}