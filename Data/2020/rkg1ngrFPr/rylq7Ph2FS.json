{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper formulates a connection between the Fisher information matrix (FIM) and the spectral radius of the input-output Jacobian in neural networks. This results derive the eigenvalues' bound to theoretically study the convergence of several networks.  Here the upper bound further improves the upper bound of FIM derived in (Karakida et al., 2018). \nThis is a very interesting and useful direction of applying information matrices to study the initialization of deep networks. \n\nI suggest the weak acceptance of the paper. After addressing the following remarks, I can adjust my reviews. \n\n1. There are some typos, such as see[?] in page 7, the main theorem on page 6 should be written mathematically with a remark. \n\n2. What is the major technical difference between this paper and Karakida et al., 2018? \n\n3. Here the model is given by conditional probability is defined by a neural network. \nThe author may also be interested in implicit models, such as normalization flows and generative networks. \nIn this case, the Wasserstein information matrix, (Hessian of Wasserstein-2 loss), may be very suitable to be studied. \nSee:\n\n\"A. Lin, W.Li, S.Osher, G. Montufar, Wasserstein proximal of GANs, 2018.\"\n\n\"W.Li, G. Montufar, Natural gradient via optimal transport, 2018.\"\n\n"}