{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyses the training behavior of wide networks and argues orthogonal initialization helps the training. They suggest projections to the manifold of orthogonal weights during training and provide analysis. Their main result seems to be a bound on the eigen-values of the Fisher information matrix for wide networks (Theorem on pg 6). In their experiments they train Stiefel and Oblique networks as examples of manifold constrained networks and claim they converge faster than unconstrained networks.\n\nCons:\n- Page 6, the main theorem of the paper, Theorem (bound on the fisher) doesn\u2019t have a proof.\n- Fig 1. What\u2019s the overhead wall-clock time of manifold constraint?, on cifar10 the two manifold don\u2019t have the same rate. Euclidean on cifar10 has higher test accuracy. Test accuracy after 200 epochs is below 90 and below 60.\n- There are claims in the paper for providing explanations by making connections to Neural Tangent Kernel but it is mentioned only in the discussion section and they reiterate previously known results.\n- Fig 3: is the training plot for cifar10 in this figure the one in figure1? Where is the training curves for svhn? Where should we see the rate of reduction in training loss for these methods?\n- Section B.4:  To show that FIM and NTK have the same spectrum you need \\nabla^2 L to be identity which is only true for L2 loss function. This does not apply to other loss functions such as cross-entropy."}