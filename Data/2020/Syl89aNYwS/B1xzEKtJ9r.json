{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Given a model and an image, the proposed method generates perturbations of the image, such that the model output\u00a0in intermediate layers of the network does not change. Their method first generates such perturbed images, and then uses those to\u00a0generate a saliency map in order to interpret the classification.\u2028\u2028The idea is overall interesting, but its relationship with many other methods is not adequately examined.\u00a0Significant portion of recent literature is ignored, especially studies on counterfactuals and decision\u00a0boundaries in the context of image classification.\u2028\u2028For numerical results, they compare their results with raw scores generated by three other methods, but the\u00a0results are quite thin and disorganized with respect to adversarial robustness and interpretability.\n\nYour proposed computational method possibly\u00a0needs to be changed, too.\u2028\u2028 Details are explained below:\u2028\u2028\n\nAuthors have cited the paper on \"Sanity Checks for Saliency Maps\", but have not used it to verify their own\u00a0results, not with respect to saliency map, and not with respect to computational cost.\u2028\u2028Several recent methods on saliency maps have not been considered, for example:\u2028\n1. Certifiably robust interpretation in deep learning\u2028\n2. Interpreting Neural Networks Using Flip Points \n3. Explaining Image Classifiers by Counterfactual Generation\n\u20284. Counterfactual Visual Explanations\u2028\n\nIt's not clear how your proposed saliency scores relate to methods that consider\u00a0counterfactuals and\u00a0the\u00a0decision boundaries.\u00a0For any image, there is a counterfactual image closest to it. One can also compute the closest image on the decision boundary of the model.\u00a0Such image can reveal\u00a0which pixels should be changed in order to change or to keep the classification.\u00a0The relationship between the\u00a0current method and those methods needs to be established.\n\u2028Assume that for an image, you compute the closest image to it on the decision boundary and let\u2019s call the distance between them r (distance measured in l1 or l2 norm). Then, consider a ball centered at that image with radius r. All images inside that ball would have the\u00a0same label as the original image and you can obtain all of them only by solving one optimization problem for finding closest point on the decision boundary. Would this be a less expensive computation compared to solving your proposed decoy optimization many times?\n\nThere are many hyper parameters in your optimization problem, and it seems that for each image, hyper-parameters should be tuned separately. However, if you\u00a0seek the closest image on the decision boundary, your results would be independent of hyper-parameters. \n\nAs you have mentioned, your optimization problem seems quite hard to solve, but no details are given on how long it takes to solve it for a single image. You have mentioned sometimes you cannot solve it if c is not chosen well. This information needs to be reported. Is it solvable at all with a large mask? How many trials did it take to obtain your saliency maps?\n\nYour optimization is nonlinear, non-convex, non-differentiable, \u2026. . How did you deal with non-convexity? Do you arrive at the same solution each time you solve it? These are vital information to about an optimization method.\n\nProposition 1 is quite obvious. Its discussion seems obvious, too, given how you have defined the Z. In your proof, you have considered a single layer network, i.e. a simple linear transformation. It might be better to explain your proof for Proposition 1 as an observation and use it as the motivation on how you have defined Z.\n\u2028Overall, you are taking a data-driven approach to interpret the classification of an image. The additional computational cost of your proposed method needs to be compared to other methods that are not data-driven. For example, how does your computational cost compare with the references above?\n\nOn page 12, one equation is referenced with ??.\n\nAt the beginning of section 3.4, you are using a standard optimization technique to augment a constraint as a penalty in the objective function. You can explain it as such, instead of saying we solve an alternative formulation. See Numerical Optimization by Nocedal and Wright.\n\nThe necessity to transform the variable via tanh, instead of performing projection is not explained. Does it make the process faster?\n\nStrategy for choosing a good initial value for c (in the penalty term) and how to increase it is not explained well. How many iterations does it take to find the solution?\n\nAny classification model is defined by its decision boundaries, so interpreting the\u00a0classification without considering the decision boundaries seems inconclusive.\n\nIn summary, it\u00a0would be best to compare your results with other methods, some of which are mentioned above. It would also be necessary to justify the additional data-driven cost that you are prescribing and to explain why your goals cannot be achieved otherwise with less expensive computation.\n\n\n"}