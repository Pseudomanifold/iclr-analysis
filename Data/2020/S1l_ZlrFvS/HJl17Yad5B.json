{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces SANE, a new approach for explaining image similarity models by combining a saliency map generator and an attribute predictor. In this way, the method is not only able to highlight what regions contribute the most to the similarity between a query image and a reference image, but also predict an attribute that explains this match. During training, SANE jointly optimizes the attribute prediction of the query image and maximizes the overlap of the saliency map of the image similarity and the attribute activations.\n\nI think the paper addresses a very interesting problem that has been commonly overlooked. There are many recents works on the explainability of neural networks for images classification and other similar tasks, but very few have addressed this problem for image similarity. It is also novel and interesting the addition of an attribute predictor in the system which provides additional information that cannot be captured by the saliency map alone. Finally, the paper also does a big effort presenting a quantitative study of how SANE is able to explain similarity models.\n\nHowever, I would also like to raise a couple of issues/questions regarding the method and its technical contribution:\n\n- I would suggest the authors to include a brief explanation of the architecture used for the attribute predictor since it will help to understand how the attribute activation is computed. I am assuming that a Fully-Convolutional Neural Network is being used, where the output of the last convolutional layers has as many channels as the numbers of classes. Is this correct?\n\n- Why using softmax + L1 loss to train the multi-attribute predictor? Aren't there other activations and losses better suited for multi-label classification, such as sigmoid + binary cross entropy loss, where there's no need to divide the ground-truth labels?\n\n- In order to match image similarities with attribute descriptions the authors propose matching similarity saliency maps with attribute map activations. This is done by first computing a saliency map for the similarity between a query image and a reference image, computing the activation maps of the ground-truth attributes of the query image, then finding the attribute activation that best matches the saliency map, and finally minimizing the distance between the saliency map and the attribute activation using an L2 loss (cf last paragraph Section 3.1). My first question is: how is the best matching attribute match? I missed this explanation in the paper and, to my understanding, this is a very crucial step. My second concern is that I don't see why the attribute activation map should be matched with the similarity saliency map since not all the regions highlighted in the similarity map might describe the attribute. For example, if we're comparing two images  containing a jacket and both contain the attributes \"zipper\" and \"black\", the saliency map might highlight regions of the zipper and where the black color is present, but the activations of the attribute \"black\" should not be enforces to match the regions of the zipper. Could the authors explain the intuition behind this design choice?\n\n- A final minor comment: as I mentioned before, the introdution of attributes in the explanation process is a very interesting contribution since they provide the user an explanation that is a step closer to a description in natural language. However, this comes at the price of needing attribute annotations at training. In order to overcome this problem, the authors suggest using an attribute discovery method when no attribute annotations are provided. My question therefore is: how are these automatically discovered attributes gonna be useful in order to provide a description, given that they are not associated with any word or concept?\n\n\nAlthough the paper proposes a very interesting approach for explaining image similarity models, I also have some concerns that I think should be addressed before its acceptance. Therefore, my initial recommendation is weak reject."}