{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I Summary\nThis paper proposes a novel method for image similarity models explanation, introducing Salient Attributes for Network Explanation (SANE). The method identifies attributes that contribute positively to the similarity score, thus explaining the important image properties, and pair them with a generated saliency map unveiling the important regions of the image. The method combines three major components: \n- An attribute explanation model\n- A saliency map generator where three \"black box\" algorithms are tested (sliding window, RISE, and LIME) and one \"white box\" (Mask)\n- An attribute explanation suitability prior is computed by the weighted combination of the TCAV scores of an attribute, its confidence score and the matching of its activation map with the generated saliency map\n    \nUsing the saliency maps as supervision for the attribute activation maps seems to improve attribute explanations. The obtained explanations help users understand the model's predictions and build trust.\n    \n\nII Comments\n\nOverall the paper is well written and presents an interesting method for explaining image similarity models. However, from a writing perspective, it can be hard to follow as the paper lacks story-telling as to why such or such methods were chosen/implemented.\n\n1. Content\n- While this work is conceptually interesting, the technical novelty and contributions don't stand out as much as they could. What is the context in improving image similarity explainability? I believe examples in industry or medical could be found to highlight the story of the paper. Why a method is used over another? (TCAV, Mask etc, what lead to this choice?)\n- In 2. Related work, Saliency-based explanations, the paper refers to white-box models but does not offer explanations as to why Mask is chosen over other methods (gradcam, guided backprop etc).\n- In 3.3 TCAV is mentioned, as far as I know, the method works with concepts as images against random images. Here attributes are used as the concepts, how are the random counterparts selected? Moreover, the section on TCAV should be in the related work, whereas how it is used for this specific case would be described in 3.3. \n- In eq 4, \u00e2 is mentioned but s is used.\n- In 4.2 there is a small user study to verify if the explanations were useful, the study is a nice addition, I really like this kind of results! It would be even more interesting if it compared the results with other baselines.\n- The \"discovering attributes\" part in the appendix is promising, this is something that could be referred to in the conclusion.\n\n2. Writing \n- Intuitive and well-described explanations are given in most paragraphs with examples (3.2, Manipulating similarity scores, the button example) which give a good understanding of the problem. This led to a better comprehension of the challenges.\nSmall typos, did not impact the score:\n- section 3. l 7 explanation -> explain\n- section 4.2, results, l 5 effects -> affects\n\nIII Conclusion\nThe idea is interesting and seems to yield good results, especially in the appendix with the discovering attributes methods. The paper could sell itself a little better with more context/applications where it could be used."}