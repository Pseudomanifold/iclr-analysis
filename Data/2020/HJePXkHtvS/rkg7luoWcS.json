{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an algorithm two learn both classifier and out-of-distribution sample detector. Instead of learning softmax weights, the proposed approach learns to project the inputs to a latent space, where each class is a Gaussian distribution. Out-of-distribution samples can be detected by the distance between the learnt representation and centers. The proposed approach can be viewed as generalization of Gaussian discriminant analysis and one-class classification. The proposed approach is technically sound, and the experiments do show some improvement over previous algorithm on out-of-distribution detection, especially on tabular datasets. However I think there are some weaknesses of this paper\n\n* The novelty is a little thin. The proposed algorithm is based on just a modification of the learning objective, and there are no theoretical analysis of why the proposed approach can work better.\n* Experimental result is somewhat weak. Improvement on the image datasets seems marginal, especially on the SVHN dataset. I also doubt if classifying Cifar10 against TinyImageNet or LSUN challenging enough, because these datasets are fairly different. I am interested in whether the proposed approach can detect novel classes, such as training using only 9 of 10 classes on Cifar10.\n\nAnother question: does learning classifier as well as centers need additional optimization techniques, like special initialization?\n"}