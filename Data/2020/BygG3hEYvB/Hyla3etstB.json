{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: this paper introduces 2 new entropy-based metrics that can be used to explain what happens in hidden layers of image neural nets. These metrics are information discarding (how much original info from the features is flowing through a particular layer) and reconstruction uncertainty (how much can be  recovered to the original input)\nOverall, it is an interesting idea and it looks less ad-hoc than other methods (gradient based, inversion based etc).  The  initial ideas and approximations are well explained, but applying it all to neural nets is not that clear- I did find it hard understanding how it all works together, and an algorithm for computing all the metrics would in a table would be much easier to follow.  I also find it hard to draw any conclusions from observing the values of the metrics (please see below) on different architectures (see comment #4).\n\nDetailed questions/comments\n1) How is delta_f computed? Is it a variance of activations values over the batch? Or something else\n2) How is concept defined? It seems that the paper goes under the assumption that there is a magic subset f  (in a hidden layer), but where is it coming from. For a trained nn, how do you define what this f is?\n3) How does the proposed method apply to non image settings, for example a simple binary classification with dense features. What are the concept features, where is the ground truth segment etc\n4) Figure 3: what conclusions do we draw from it? Which modification of res net was giving better performance? Should we look for networks with higher SID, harmonic mean of SID and reconstruction or something else How do you practically use the metrics you propose to chose the best architecture. The only thing you say is that deep DNN has higher SID and RU. Struggling to understand what use this is. \nFrom the appendix, Section A is actually more interesting and illustrative than what you put into experiments in the main paper. \n5) Experiments didn't really compare your proposed method with any of alternative methods. A couple of illustrative pictures with what your method shows and gradient based shows for example would go a long way\n6) How do you train a reconstruction network to ensure minimum required efficiency. What is the architecture you chose for it?\n7) If these proposed metrics are correlated with generalization (which is not clear yet), do you  think it can be introduces as a new regularization during training.\n\nMinor: \n- I really would suggest to have separate captions for each figure, instead of one long caption and top left, top right etc. It is really hard to parse\n- In appendix instead of such a large dump of pics, chose a couple that really illustrate that you are trying to show, and add the explanations from alternative methods (gradient based for example) to demonstrate the benefits of your method \n\n"}