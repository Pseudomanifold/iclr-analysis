{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript proposed to represent the embedding matrix as a small set of anchor embedding and sparse transformation. The paper is trying to be general-purpose, end-to-end trainable, and able to incorporate domain knowledge. Experimental results show that it is possible to compress the embedding in the proposed way without much loss of accuracy.\n \nThe authors propose to find anchor embedding by several methods, such as frequency, clustering, or random sampling. The sparsity on the transform is imposed by L_1. Although I get the basic idea and I am familiar with many of the techniques, it is unclear to me what is the main focus of this paper, and the technical contribution is quite vague. Why is the large embedding matrix a problem? Besides the low-rank form proposed, are there any other ways to compress it? This paper is not well motivated at all. Therefore, I think this manuscript is not ready to publish in its current form."}