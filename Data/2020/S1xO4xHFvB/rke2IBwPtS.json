{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper explores the use of replicating neurons across and within layers to compress fully connected neural networks. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes. \n\nStrengths: a lot of nice experiments with clearly advantageous results are given.\n\nWeaknesses: One obvious baseline missing is sparse compression, which can be achieved using either l1 regularization, or hard thresholding + fine tuning, both of which are easy to implement and appear in several works, e.g.\n\nScalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization (Yang, Ruozzi, Gogate)\nTraining skinny deep neural networks with iterative hard thresholding methods (Yin, Yuan, Feng, Yan)\n\n... many others just via googling ... \n\nAlso, I think this work should be compared with compression schemes that work via kronecker product, which seem very similar to this scheme (but where the kronecker matrix is binary to produce replication)\n\nCompression of Fully-Connected Layer in Neural Network by Kronecker Product (Zhou, Wu)\n(more via google)\n\nOne obvious advantage of replication over kronecker product is lower complexity, but nonetheless, the methods belong in a similar family.\n\nOtherwise, I think the work makes sense, the idea is nice, and the results show promise!"}