{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a novel way to determine automatically the quantization level at each layer of \ndeep neural networks while training. The quantization can result in a more compact network requiring\nless memory for storing the parameters (weights), with minimal drop in accuracy. \nIf I understand correctly, the main idea in the paper is utilizing a continuous representation of the quantization using a continuous cursor, and a new differential approach from Neural Architecture Search (NAS), to create a differential learning algorithm for the problem of finding the optimal quantization. \nOn the positive side, the authors present very good experimental results on popular networks like ResNet, \nand improve on other method's compression accuracy, with little to no reduction in accuracy. \nHowerver, the paper is not clearly and carefully enough written to convey the author's results which makes it a borderline paper in my opinion. \n\n\nIn my understanding, compressed (quantized) networks can have different advantages besides a more compact representation of the network: \nFor example, this can also reduce training time because all operations are performed with words with fewer bits - but the authors focus only on the compression ratio and do not discuss this issue. In fact, it seems that their algorithm actually requires more training time than standard training of neural networks, because you start with training w with full 32-bit representation, and then train with different quantizations simultaneously. \nIt would be good if the authors describe clearly how do different methods (theirs and the others mentioned) compare to each other in terms of savings of different resources (training time, compression, inference/prediction time for the trained networks etc.) \u00a0and which of these is more important (e.g. the latter in mobile devices).\n\nAnother practical issue is that using a different number of bits for each layer may complicate the design and software/hardware implementation of a network, compared to say allocating 4-bits or 8-bits to each weight. It is correct that you can reduce the overall number of bits, but you may reduce it even further if you allow a different number of bits for each individual weight. I assume that there is a price in power/memory/speed etc. to this non-homogenous property, and it may be better to get a slightly lower compression ratio provided that there is more uniformity in the network weights. \u00a0This is not a critique of this particular paper, but of the entire framework of flexible quantization. \n\nThe English of the manuscript could be greatly improved. For example: \nPage 2: \"..multiple bits for different layers..\" - I assume the authors mean \"..a different number of bits used for different layers ..\"\nPage 2, bottom: \"The authors (Wang et al. 2018) presented ...\" - the sentence is long and unclear. \nMany sentences starting with 'And' - for example: \"And a Gumbel softmax function ...\" (page 3) \n\nThere are a few vague and non-informative statements which do not contribute to the understanding of the field and the authors' contribution. For example: \n- Page 3: \"A typical search method is random search, however, its efficiency is not ideal.\" (does efficiency refer to computational efficiency here?)\n- Page 4: \"The reason why we add regularization item to the loss function is because the regularization can prevent overfitting to some extent\" (I thought that here the main purpose of regularization is to get a trade-off between accuracy and compression). \n- Page 6: Regarding parameters choice: 'a rather optimal set of them is chosen ..' - this seems quite arbitrary and it is not clear how to choose parameters for the authors' method for future architectures and datasets. \n\n\nThere are also terms that are known to expert but could use a short explanation/reference - for example: \n- Adam optimizer \n- Cosine annealing\n- Gumble Softmax\n\nThe mathematical equations are definitions are not clear enough and contain errors: \n- Eq. (1): there is (x',y') twice in the equation, but in the sentence afterward there is also (x,y). \nThere are D_T, D_V with and without tilde. \nThe space of maximization of w for a given quantization is not defined (I'm assuming all weights vectors w\nwhich can be represented using the number of bits in the quantization). \n- Eq. (3) - the loss is loosely defined - what is 'parameter size'? the number of bits for all the weights in each layer? \n- Eq. (4) - is w or w_k the full precision weights? why are x and y in the interval [0,1]?\n- Eq. (6) - c_i represnts the i-th layer, but d_1,d_2, a_1,a_2 are fixed. Are they also different for different layers? \u00a0 \nAlso, a_1 and a_2 are the boundaries of the cursor - are they set to [0,1]? or [0,32]? \n- The authors defined Conv - the convolution operation - but what are W_1 and X? vectors? how is the convolution\ndefined precisely? it is also not common in math papers to use '*' for \u00a0product\n\nIt is not entirely clear to me how exactly the authors define a differentiable loss and a gradient for the quantization part Loss_q.  When a cursor parameter changes (e.g. c_i from 2.5 to 2.6) then the loss is defined by quantization to the two consecutive integers (in that case 2,3 ) and there is a continuous mixture parameter between them which can be changed smoothly, which is nice. But at some point, the parameter will reach 3, and then the quantization will use the two integers 3 and 4 - the fact that 3 has weight 1 in the loss (and 2,4 have weight zero) makes the loss continuous, but is the loss differentiable at this point?  some explanation is needed here on if this is a problem and how is it avoided/solved. \n\nAlgorithm 1: The description in Figure 1 can be made more precise. The algorithm seems to alternate between \noptimizing the cursor c, and optimizing the weights w for a given value of c. \nThere are also missing details and parameters like gradient step size.\u00a0 What does Grad-C * L_C mean? is the gradient multiplied by the loss?? or only applied to it? \nIn the sentence \"Quantize the network ... to update the loss\" - which loss is updated and how? L_V, L_T? something else? \nA detailed description of the algorithm and parameters or the actual code used by the authors would improve the understanding of their method. \n\nSection 4.1: Figure 2 compares the loss of one-integer vs. two-integers quantization scheme. The authors argue that\u00a0their two integers scheme is better because it is smoother. But the loss for one integer is actually lower - so why wouldn't it be better to use this one? \u00a0\nwill the two-integer method eventually reach a lower loss? \n\n"}