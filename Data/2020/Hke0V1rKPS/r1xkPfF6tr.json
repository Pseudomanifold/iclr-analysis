{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nIt was previously observed that models that were more robust to adversarial perturbation had more interpretable jacobian. The authors attempt to train for interpretable jacobian in order to improve the robustness of the model.\nThis is done by employing a GAN-like procedure where a discriminator attempts to distinguish between the transformed jacobian matrix (fake images, equivalent to generator) and real images.\n\nExperiments indicates that this improves robustness compared to unprotected models and approximately similarly to models trained with adversarial training.\n\nComments:\n* The motivation given for this line of research is the cost of adversarial training (2nd paragraph of Section 3)\nNo experimental comparison is given with regards to the time it takes to train a model with adversarial training, versus the time it takes to train a model with JARN. It is also important to note that this introduces additional complexity (needs to choose an architecture for the discriminator, tune proper learning rates, etc...), which is not mentionned.\n\n* Why not test simpler jacobian regularization method as proposed by other papers (see below). Proposition 3 of Simon-Gabriel et al. shows that results similar to adversarial training can be obtained, and they don't need several iterations like adversarial training, nor do they need to train an additional discriminator like your method.\n\nOpinion:\nThe paper provides an interesting proof of concept for a method, showing that it is feasible. It however doesn't make the the case for why it is a good idea. Discussion and comparison to very significant related work is missing and experimental measurement of any advantages of the proposed method vs. adversarial training is lacking. I think that these aspects should be improved before the paper is ready for publication.\n\nTypos:\nLine 11 in Algorithm 1 -> The label is wrong, i assume it's \"Update the discriminator f_disc to maximize L_adv\"\n\nRelated works that needs discussing:\n- Drucker, Lecun 91, \"Double backpropagation increasing generalization performance\" for other regularizer on the jacobian, discusses generalization rather than robustness.\n- Simon-Gabriel et al., \"First-order Adversarial Vulnerability of Neural Networks and Input Dimension\"\n"}