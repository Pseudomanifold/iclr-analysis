{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper authors propose a framework for learning in multi-agent systems. Main contribution of the proposed framework is that it learns the communication structure between the agents, which improves the perfomance. In order to impose a structure authors use a graph neural network, which enables the agents to share information and rewards. They empirically show that their method is able to outperform Multi-agent Deep Deterministic Policy Gradient (MADDPG), Trust Region Policy Optimization (TRPO) and Deep Deterministic Policy Gradient (DDPG) on Waterworld, Multi-Walker and Multi-Ant tasks.\n\nI would recommend the paper to be accepted since the authors came up with an innovative solution to the difficult problem of how to coordinate in a multi-agent setting although they have a limited experimental setup. In order to improve the exposition I would recommend the authors provide more evidence along the lines of how the proposed method deals with indefinite number of agents problem. How does the performance change as more agents added to the task. They chose 4 in their experiments why that number was chosen? How does the gap between baseline methods vs. SMAL change as the number of agents increase?"}