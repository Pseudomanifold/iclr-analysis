{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new way to perform multi-agent reinforcement learning via using a graph neural network like method to pass messages from agents. They show performance improvements over baseline techniques such as TRPO, DDPG, and also multi-agent algorithm MADDPG.\n\nThe problem is very interesting and I do like the experimental domains such as multi-walker. However, the experimental results lack comparison against a simple baseline and it is unclear how the proposed SMAL algorithm would perform against basic multi-agent RL techniques.\n\nThere are also various inaccurate or debatable claims in the paper.\nIn the first paragraph, the authors state that single-agent RL learns to fulfill goals without modeling the behaviors of others. This is clearly not the case as the most basic model-free approaches such as DQN agents may indeed learn to model other\u2019s behaviors inside their policy networks.\n\nA basic centralized multi-agent system would have action spaces N times A where A is the action space of a single agent. Learning would take observations from all agents and act jointly and in a coordinated manner (both centralized actor and centralized critic). This system however is not scalable to a large number of agents. However, it is still a good baseline to compare against. For example. How would a centralized multi-agent DDPG system work on multi-walker? e.g. with dimensions 4*24 ? I think it would be surprising if SMAL would be better than this baseline, what are authors thoughts/views on this?\n\nFirst paragraph of page 2 suggest that SMAL is able to learn +ve and -ve weights to suggest whether an agents are collaborators or competitors. However, from sections 4.1.1 and 4.1.2, it appears that those bounds on v_ij are actually set by hand.\n\nPositive/negatives weights on an edge is hardly a good indicator of collaboration/competition. I do not see a relationship between the two. Typically, collaboration or competition arise out of the reward function, e.g. whether it is a zero-sum game or a general-sum game.\n\nIn addition, the claim in 4.1.2 that it is ok to zero-out messages from \u2018competitors\u2019 are dubious. Why would it not be beneficial to infer additional information from messages of other adversarial agents?\n"}