{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper applies ideas from message passing over a global graph between agents to perform multi-agent reinforcement learning. The algorithm SMAL is demonstrated on a suite of continuous control domains.\n\nThe paper is imprecise, unpolished and doesn't reflect the state-of-art fairly. It does not support as significant contribution. The current approach conditions the policy on the messages from other agents and therefore is a centralized controller. Experiments make comparison to methods that either support decentralized execution (MADDPG) or are single-agent RL techniques (TRPO) which is unfair. No comparison to other methods like [1,2] that learn to communicate between agents or even the introduced environments in those works. The claim that \"messages and rewards are both necessary for collaboration\" is unjustified from the experiments given the environment comes from [3] which did not allow any communication between agents and showed scaling to 10 agents compared to 4 agents in the current work.\n\n[1]: https://arxiv.org/abs/1605.07736\n[2]: https://arxiv.org/abs/1812.09755\n[3]: https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5"}