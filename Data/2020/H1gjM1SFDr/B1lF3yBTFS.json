{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the L0-Sparse Subspace Clustering (L0-SSC) of high dimensional noisy data. The goal of L0-SSC is to cluster the data according to their low-dimensional subspace structure by minimizing the number of the nonzero elements of a matrix Z such that X = XZ for data matrix X.\n\nCompared to the existing work of L0-SSC, this paper proposes noisy L0-SSC to handle noisy data. Moreover, it proposes Noisy-DR-L0-SSC to improve the efficiency of noisy L0-SSC by projecting the high-dimensional data onto a low-dimensional space by random projection and then performing noisy L0-SSC on the obtained low-dimensional data. The correctness of both noisy L0-SSC and Noisy-DR-L0-SSC is proved by showing that the subspace detection property holds. The idea of using random projection to improve the efficiency of noisy L0-SSC seems interesting, and the experimental results support the correctness of noisy L0-SSC and Noisy-DR-L0-SSC by demonstrating that the proposed methods achieve the promising result. Some questions are as below:\n\n1 Please include mean and deviation in the results of Noisy-DR-L0-SSC in Table 1 and Table 2. Because random projection is used for dimension reduction, mean and deviation are expected in the results to avoid performance fluctuation due to randomness. \n\n2 Can the authors show the performance of Noisy-DR-L0-SSC with different p-values (i.e. different dimensions after random projection)?\n\n3 Can one try sparse random projection in the framework of Noisy-DR-L0-SSC so as to further improve its efficiency? For example, does the correctness of Noisy-DR-L0-SSC still hold if we use OSNAP (Jelani Nelson and Huy L. Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. FOCS 2013)? If does, the contribution would be more sufficient.\n\n4 L0 minimization problem is an NP-hard problem, so in general case, the Proximal Gradient Descent (PGD) in Section 5 only renders an approximate solution to the optimization problem of noisy L0-SSC and Noisy-DR-L0-SSC. It could be better if the authors can provide a bound between the approximate solution by PGD and the optimal solution so that there is a guarantee that the gap between the approximate solution and the optimal solution is bounded. \n\n5 Since the correctness of noisy L0-SSC and Noisy-DR-L0-SSC are proved in terms of the subspace detection property, it could be better if a measure directly related to the subspace detection property is used to measure the performance of the proposed methods.\n\n6 In the randomized analysis of noisy L0-SSC, the assumption is adopted, i.e., data in subspace are i.i.d. isotropic samples. In Remark 5, this assumption is regarded to equivalent to the assumption of data are i.i.d. samples uniformly distributed on the unit sphere up to a scaling factor. The scaling factor, i.e. square root of d_k, can be easily removed in the current analysis of noisy L0-SSC so that almost the same correctness of noisy L0-SSC can still be obtained under the assumption that data are i.i.d. samples uniformly distributed on the unit sphere. The assumption of the uniform distribution on the unit sphere has been widely used, e.g. in L1-SSC Soltanolkotabi & Cands (2012) and noisy L1-SSC Wang & Xu (2013). Slightly reformatting the current results of L0-SSC under randomized models and presenting the correctness of L0-SSC under the assumption of the uniform distribution on the unit sphere can make a comparison to existing works easier. \n\n7 C_{p,p_0} in equation (31) is defined in the appendix (equation (55)). Please define it before it is used.\n"}