{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper explores the problem of simultaneous learning for topic modeling and survival prediction.  Their contributions are: (1) integrating two different topic modeling approaches with survival models for joint learning and (2) showing results on two medical datasets with some brief analysis of what topics are recovered.  This is an interesting task to be using with topic modeling.  I appreciate that the approach could be used in medical systems where interpretability of models is very important.\n\nI think the high-level idea of the algorithms is fine, but the main drawback is that the experimental section needs to be expanded on, perhaps with larger datasets and more analysis.  Unless more experiments are provided, I might lean towards rejection.  I think it's ok that some of the empirical results are inconclusive, but it should lead to more quantitative error analysis.  I definitely think the smaller size of the datasets could be one factor in the negative results, which is why I think the authors should try experimenting with a larger dataset.\n\nMore specific comments and suggestions:\n- Each dataset is small (371 and 1981 data points respectively).  This may be too small to effectively train some of these neural models, which I believe may be one reason why they underperform, especially on the pancreatitis data.  In order for the experiments to be more conclusive, maybe you can switch to a different dataset.\n- Related to the size of the data, I suspect that many of the differences in the table are statistically insignificant.  Can the authors please specify which of the results are significantly better than the others?\n- While I liked the examples in Section F of the appendix, they are difficult to compare between models, and not all of the models are included.  More quantitative analysis is needed to really understand how the models\u2019 learned topics differ in terms of coherency, cohesiveness, interpretability, etc.  Providing really detailed quantitative comparisons might strengthen the analysis section of the paper.\n- I liked the preciseness of the descriptions of different algorithms in the background section.  They were all explained nicely and easy to understand.  However, the background section is a bit too detailed at times.  Authors may consider condensing a bit more."}