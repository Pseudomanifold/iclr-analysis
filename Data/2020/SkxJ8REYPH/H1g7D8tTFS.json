{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors verify the effect of BMUF[1], which is called slow momentum in this paper, on computer vision and natural language processing tasks with different kinds of local optimizers.  They also provided the theoretical convergence guarantee of BMUF.\n\nThe literature survey of this paper is quite good and the experimental results are convincing. However, they should modify their claim that \"BMUF is a special case of SlowMomentum\".\nIn classical block momentum version of BMUF, the  update formula is:\nu_{t+1} = \\beta u_{t} + \\alpha (x_{t,0}-x_{t,\\tau})\nx_{t+1,0} = x_{t,0} - u_{t+1}\n\\beta is called block momentum and \\alpha is block learning rate\n\nin this paper, the update formula becomes:\nu_{t+1} = \\beta u_{t} +  (x_{t,0}-x_{t,\\tau})/\\gamma_{t}\nx_{t+1,0} = x_{t,0} - \\alpha\\gamma_{t}u_{t+1}\n\nObviously this two formula are equivalent. BMUF is a general framework, which can work with different kinds of local optimizer. The author should not narrow down the definition of BMUF as BMUF with SGD as local optimizer and \\alpha=1. Actually, \\alpha=1 is used in all experiments of this paper.\n\nIn conclusion, the organization and writing of this paper is satisfactory, the experiments and theoretical proof is valuable for respective researchers. They should clarify that SlowMomentum is same with BMUF with classical block momentum. I will give a weak accept to this paper.\n\n [1] Kai Chen and Qiang Huo. Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5880\u20135884, 2016."}