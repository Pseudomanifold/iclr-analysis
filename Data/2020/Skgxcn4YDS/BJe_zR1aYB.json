{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a new NN architecture designed for life-long learning of natural language processing. As well depicted in Figure 2, the proposed network is trained to generate the correct answers and training samples at the same time. This prevents the \"catastrophic forgetting\" of an old task. Compared to the old methods that train a separate generator, the performance of the proposed method is noticeably good as shown in Fig 3. This demonstrates that the new life-long learning approach is effective in avoiding catastrophic forgetting.\n\nThe motivation of the paper is clear. The comparison to old methods seems fair. The proposed method is clearly different from previous methods.\n\nOne weakness of the paper is in the experimental results especially in section 5.4. The statistical significance of the results in table 5 is missing. As the authors have discovered, the performance is highly dependent on implementation. In addition, the resulting performance might have a high variance. From the data, it is hard to argue that LAMAL is better than MBPA++ by a significant margin.\n\nI recommend having a more elaborate figure instead of Figure 2. The structure of the network might be particularly interesting for people who are not in this field. \n\nOverall, the results are very interesting and worth a publication."}