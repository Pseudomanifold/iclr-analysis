{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the problem of lifelong language learning. The core idea underlying the algorithm includes two parts: 1. Consider the NLP tasks as QA and then train a LM model that generates an answer based on the context and the question; 2. to generate samples representing previous tasks before training on a new task. \n\nIn experiments, the authors demonstrate the efficiency and effectiveness of the proposed models based on the following perspectives:\n1. Compare the proposed method with existing baselines. However, it seems that keep real data is missing from Table 3. \n2. Preliminary studies with 3 tasks and study the task oder.\n3. Performance of the training epochs.\n4. Hyper parameter tuning gamma. \n\nThe weak points of this paper are as following:\n1. The assumption of modeling all tasks as QA might be strong;\n2. The baseline from using real data is missing;\n3. There are many components that are missing from the discussion, such as the complexity of the language model, etc. For instance, when the model complexity is high,  TopK sampling could be expensive.\n"}