{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary\nThe authors propose solving data poison bi-level optimization using the meta learning formulation. \n\nDecision\nOverall, I think the proposed meta-learning formulation seems legit, but I found the formulation of metalearning is hard to parse and the experimental comparison is not persuasive. Hence I recommend for weak reject.\n\nSupporting argument:\n1. The written can be improved in describing the meta learning formulation. Usually there is a clear definition of meta-learning training/testing tasks and training/test examples in each task. \n2. 16/255 is quite large for CIFAR10. What happen in other radius? Can the approach work in other constraints? \n3. In figure 4, the variance of the success rate, target loss are quite large. I wonder whether this means the approach is not stable.\n4. Since it is the first clean-label poisoning attack on networks trained from scratch, it might be worth comparing the approach to other setup where there are some baselines.\n5. The arrangement of figures are far away from text.\n6. In Algorithm 1 Line 5, I don't get the part why there are M models instead of M-th model. In Lines 9, 10, the formulation is not clear. What's the reason for Line 11?\n7. It is not clearly to me why there needs to be M models to consider. What's the performance with different M?\n\n\nAdditional feedback:\n1. Footnote should go after period. \n"}