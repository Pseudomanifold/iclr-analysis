{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a sparsification technique that seeks edges contributing negligible amounts to the performance of a network. \n\npros)\n(+) This paper is written well but needs more clarity.\n\ncons)\n(-) This paper did not cite modern pruning/sparsification methods published recently.\n(-) The proposed method has only compared with some outdated methods.\n(-) Only LeNet-5 and VGG-16 have been used to validate the proposed method.\n(-) This paper lacks any analysis of why the proposed methods would work well. Specifically, on the formulation of the sparsification method, it is hard to find sufficient backups why the authors did like that.\n\nComments)\n- How would you guarantee using the infinite feature selection method could give proper score in general?\n- How did you determine theta_l for each layer?\n- The major problem of this paper is the experimental section. This paper only compared with outdated methods, so it is hardly verifying the effectiveness of the proposed method compared to other methods. \n- It is necessary to involve ResNets as one of the baselines."}