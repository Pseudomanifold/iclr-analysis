{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a transformer block with higher-order interactions. More precisely, instead of computing a dot product between a query vector and a key vector, 2-simplicial attention computes scalar triple product. Instead of computing a weighted average of value vectors, 2-simplicial attention computes the weighted average of tensor products of value vectors. The resulting architecture has improved representation power which is demonstrated using experiments on bridge BoxWorld environment.\n\nThe major gripe I have is a lack of discussion in the main paper of how precisely, 2-simplicial attention can help solve better bridge BoxWorld. I did notice some discussion of this in Appendices F, H and I. But I believe the reader would be better served by having such a discussion in the main body. More generally, what kind of tasks can 2-simplicial attention address better? Answering/discussing this question can go a long way in making the paper more valuable.\n\nIs it possible to evaluate the improved expressivity of 2-simplicial attention on real-world datasets/tasks?"}