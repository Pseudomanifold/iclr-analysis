{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This in an interesting paper as it tries to alleviate the burden of hyper-parameters tuning for exploration strategies Deep Reinforcement learning.\nThe paper proposes an adaptive behaviour in order to shape the data generation process for effective learning. The paper considers a behaviour policy that is parametrized by a set of variables z called modulations: for example the Boltzmann softmax temperature, the probability epsilon for epsilon-greedy, per-action biases, ..\nThe author frame the modulations search into a non-stationary multi-armed bandit problem and proposes to adapt the modulations according to a proxy to the learning progress. The author provides thorough experimental results.\n\nComments:\n\n- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL. \n- The proposed proxy is simply the empirical episodic return. It is not well explained in the paper how this proxy correlates with the Learning progress criteria.   \n- The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories. How this proxy incentives the agent to explore poorly-understood regions? In other terms, how this proxy help to tradeoff between exploration and exploitation ? \n-  The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017) cited in the paper. \n- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me. They estimate a certain probability at time step t by empirical frequency based on data from previous time steps. But as the parameters change during the learning, the f_t\u2019(z) at time t\u2019 < t is not distributed as f_t(z). This introduces a biases in the estimate.\n- I appreciate the thorough empirical results and ablation studies in the main paper and the appendix. They are really interesting. \n- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper. Is it a baseline with the best hyperprameters in hindsight? \n-  From the plots of learning curves in appendix, the proposed methods doesn\u2019t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ? "}