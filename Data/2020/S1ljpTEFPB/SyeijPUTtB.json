{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a guided attention-based strategy for sparsifying deep neural network models. The goal is to achieve both model compression and minimal loss in accuracy. The novelty of the technique seems to be the use of a combination of (group-)sparsity penalties and a variance promoting regularizer while training the neural network. Experimental results on MNIST and CIFAR (10/100) are presented.\n\nUnfortunately, the current manuscript is confusing to read, makes limited contributions, and is incomplete in many places. I include an incomplete list of caveats below, but it should be clear that the paper cannot be accepted without a major rehaul.\n\n* The authors use the \"attention\" nomenclature throughout the paper but it is unclear to this reviewer why this choice is made. As far as I can tell, the main formulation (1) is just regular cross-entropy minimization with a couple of regularizers thrown in -- no attention-type mechanism is used here -- so the basic formulation is of unclear novelty.\n\n* The groups in the group-sparsity penalty are not clearly defined anywhere. \n\n* The entirety of Section 4 (which seems to be a main contribution of the paper) is rather confusing. What is the vector V and what does it have to do with the main formulation (1)? What does \"Update gradient\" in Algorithm 1 referring to? What is the purpose of Section 4.3?\n\n* What does a \"sparsity level of 60%\" mean in the experiments? Are the authors referring to compression ratio here? Unclear since never defined..\n\n* The main (only) theoretical statement is Proposition 1, whose detailed proof is claimed to be in an \"Appendix\" but no such appendix is provided. Moreover, Proposition 1 does not seem to be a mathematically sound statement.\n\n"}