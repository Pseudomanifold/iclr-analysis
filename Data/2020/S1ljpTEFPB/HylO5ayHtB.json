{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed Guided Attention for Sparsity Learning (GASL) to learn sparse model in neural networks. The authors demonstrate through experiments that their methods achieve state-of-the-art sparsity level and speedup with competitive accuracy.\n\nQuestions:\n1) In the abstract, the authors claim \u201cOur work is aimed at providing a framework based on an interpretable attention mechanisms...\u201d. How is your method interpretable? I think the paper needs more justifications on this point.\n\n2) The results of this paper would be more convincing if the authors could include experiments on ImageNet datasets, as in [1], [2]. \n\n3) In section 4.2, why is \u201c the choice of V^r should be highly correlated with V\u201d?\n\n4) In section 3.2, why is variance loss related to attention mechanism? From my view, you simply add a loss function to penalize the variance of weights in a group. This part is confusing.\n\n5) Also you said that the detailed mathematical proof for proposition 1 is put in the appendix. But where is the appendix?\n\n6) For the experimental results, your method fall short of Sparse Variational Dropout in terms of clean accuracy. It seems to me that your method may not be so effective.\n\n7) There seems to be some typos in the current version:\nSection 4.1 last line, \u201cfrom now one\u201d should be \u201cfrom now on\u201d\nSection 4.1 something is wrong with the last line in equation (5). \n\n\nIn general, the paper is not clearly presented. Many details are needed to clarify the contributions. The paper writing needs to be improved, as there exist many typos in the current version and the presentation is confusing. Therefore I recommend weak reject for the current version of the paper.\n\n\n[1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015.\n[2] Learning Structured Sparsity in Deep Neural Networks. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li. NIPS 2016.\n"}