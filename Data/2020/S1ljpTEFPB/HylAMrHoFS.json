{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "# Summary\n\nIn this paper, the authors propose an approach to induce (structured) sparsity in deep neural networks during training. \n\nThe proposed approach builds heavily on Structured Sparsity Learning (SSL) [1], which in a nutshell pioneered the use of a group-Lasso regularizer [2] to induce sparsity at the level of filters, input channels, layers and filter shape in deep neural networks. The main contribution presented in the manuscript is the inclusion of an additional regularizer in the SSL framework (Eq. 3) which, for each layer, encourages the collection of $L_2$ norms of each group\u2019s weights to have a large variance, with each group thus acting as a \u201csampling unit\u201d in the calculation of the variance to be maximized for each layer.\n\nExperimental results are provided for MNIST, using an MLP and LeNet-5-Caffe as models, and CIFAR-10/CIFAR-100, using VGG-16 as the model.\n\n# High-level assessment\n\nFrom a methodological perspective, the manuscript\u2019s main contribution relative to prior work such as SSL [1] is the inclusion of the regularizer described in Eq. (3), which aims to penalize solutions for each group-wise $L_{2}$ norms have a small variance. The addition of this regularizer is however largely heuristic, lacking a theoretical motivation in the manuscript. \n\nRegarding the empirical assessment of the contribution\u2019s performance, I believe that the experimental results presented in the current version of the manuscript do not provide sufficiently clear evidence that the proposed approach significantly outperforms existing baselines. Most importantly, the reported results for some baseline approaches appear to be in contradiction with those found in their respective original publications, though the reasons for this disagreement are not discussed in the manuscript.\n\nFinally, I found the paper to be severely lacking in clarity, to the extent that I unfortunately felt it got in the way of properly understanding some of the low-level details of the proposed approach. Sections 3 and 4, where the method is introduced, often make use of imprecise mathematical notation and vague textual descriptions that make it difficult to assess the soundness of the proposed approach. In particular, it is for me unclear how exactly Section 4 relates to Eq. (3), what the role of Proposition 1 is precisely and what its statement exactly refers to.\n\nBecause of these reasons, I cannot support acceptance of the submission in its current form. However, I would encourage the authors to thoroughly revise their manuscript to address those issues.\n\n# Major points\n\n1. In Section 3, the authors refer to [3] to justify the inclusion of their variance-inducing regularizer (Eq. 3). In particular, the authors claim that:\n\n\u201cThe power of the variance as a regularizer has been investigated in [3].\u201d\n\nHowever, unfortunately I am unable to see the relationship between [3], which aims to minimize the (unknown) risk by minimising instead a surrogate upper bound of the risk that explicitly takes the bias/variance trade-off into consideration, and the approach proposed in this manuscript.\n\nIn particular, in [3] the authors propose **minimizing** (not maximizing) the variance of the **loss** (not of the parameters) with respect to the **empirical data distribution** (not parameter groups) in the context of convex optimization (not non-convex models like deep neural networks).\n\nTo this end, I would encourage the authors to clarify exactly how the results of [3] support their proposed regularization scheme.\n\n2. In Section 3.1, it is claimed that by reparametrizing $\\lambda_{v}$ as $\\lambda_{v} = \\alpha \\lambda_{s}$, the number of hyperparameters is reduced by, which seems to imply tuning $\\alpha$ is unnnecessary. However, this is not immediately obvious to me, given that the results in Table 4 suggest that the performance of the proposed approach can vary substantially with respect to $\\alpha$.\n\n3. In my opinion, the exposition of the proposed approach throughout the manuscript is, at times, confusing.\n\nFirstly, the paper frequently refers to the proposed approach as an instance of \u201cattention\u201d, including the manuscript\u2019s title. However, I must admit I fail to see the connection to attention, in the sense of the methods referred to as related work in Section 2. If the authors feel their paper is indeed related to attention, I would encourage them to elaborate on this connection further, clarifying how exactly their method is related to those approaches cited in Section 2.\n\nSecondly, Section 3 could be improved by more clearly separating original contributions from prior work. Related to this, SSL [3] appears to be cited \u201cout of place\u201d, as I believe it would be more appropriate under the \u201cgroup sparsity\u201d subsection. Also, given the emphasis on structure, I believe the choice of groups should be discussed in greater detail since, at the moment, only one choice is used, with no justification, and it is only discussed as part of the experimental setup in Section 5.\n\n4. In my opinion, the current version of the manuscript suffers from a severe lack of clarity.\n\nSome of these issues, prevalent throughout Section 3, are somewhat minor, in that with some effort they can be \u201csecond-guessed\u201d from context. Examples include:\n\na. The function $G$ in Eqs. (1-3) is never defined properly. Likewise, the function $H$ in Eqs. (1, 3) is also never defined, though a footnote claims it\u2019s the identity function. However, in that case, what is the role of $H$ in the exposition?\n\nb. The authors refer to the regularizer in Eq. (1) as the inverse of $\\Psi$, whereas I believe they meant the reciprocal.\n\nc. Eqs. (2, 3) use a normalization factor $\\vert G(W^{l}) \\vert$ which is never defined. It is claimed that \u201c... is a normalizer factor which is in essence the number of groups for the $l_{th}$ layer \u2026\u201d. Does this mean it is equal to $M$ and, if so, why use two different terms for the number of groups in a layer?\n\nd. The notation $( )_{l}$ used in Eq. (2) is not defined properly. Instead, I would recommend using the notation in [3], explicitly indexing the weights and number of groups per layer.\n\ne. The function $\\Psi$ as defined in Eq. (3) does not match its textual description. Eq. (3) describes the sum of variances (across groups in a layer) of the group-wise $L_{2}$ norms of the weights. Instead, the authors refer to this regularizer as \u201c... simply the variance of the group values for each layer, normalized by a factor ...\u201d. \n\nHowever, the lack of clarity is, in my opinion, more problematic for Section 4. In brief, I fail to see the connection between Section 4 and minimization of Eq. (1), because of the following issues.\n\nFirstly, I find the definition of $V(\\theta)$ in the first line of Section 4.1 puzzling. As I understand it, it would appear that $V(\\theta) = \\theta$. However, in this case it is unclear to me: (i) what the purpose of defining $V(\\theta)$ is and, most importantly, (ii) what is the relation between maximizing the variance of $V(\\theta)$ and minimizing (maximizing the reciprocal) of Eq. (3). In particular, regarding (ii), Eq. (3) appears to concern the variance of $L_{2}$ norms, whereas $V(\\theta)$ seems to involve the model weights directly. As a minor point, the authors should probably define $\\vert \\theta \\vert$ explicitly, though I take this to mean the number of parameters at some unspecified level of granularity (group?/layer?/model?).\n\nSecondly, I do not find the statement of Proposition 1 to be sufficiently precise to allow me to verify its correctness. In particular, I believe the statement \u201c... does not enforce any upper bound on the variance vector $\\hat{V}(\\theta)$\u201d should be rephrased in mathematically precise terms. This imprecision, linked to my confusion regarding what $V(\\theta)$ is meant to be exactly, makes it difficult to evaluate the proof of Proposition 1 as it stands now.\n\nThirdly, the proof of Proposition 1 refers to an appendix that, to the best of my knowledge, has not been provided for review. It would also be helpful to clarify what parts of the proof are an original contribution, seems to the best of my knowledge the derivations shown appear to significantly overlap with [4].\n\nFinally, I do not fully understand the relevance of Proposition 1 relative to the rest of the manuscript, though this is likely caused by all of the issues regarding clarity in Section 4 described above.\n\n5. Some aspects of Algorithm 1 remain unclear/unjustified, which is likely caused (at least in part) by the lack of clarity throughout Section 4 as argued in \u201cmajor point 4\u201d above.\n\nFirstly, it is not immediately clear to me how to estimate the mean of $V^{r}(\\theta)$ in practice in Algorithm 1 and, relatedly, what the assumed source of stochasticity for $V(\\theta)$ is. Secondly, the if-else condition in Algorithm 1 does not appear to be largely heuristic. Finally, I would encourage the authors to prove precisely how Algorithm 1 contributes to minimizing Eq. (1) and its relation to the regularizer described by Eq. (3).\n\n6. The experimental results do not convey clearly the benefit of the proposed approach in terms of accuracy nor sparsity. \n\nIn particular, results across the three reported experiments (Tables 1 and 2) appear to be largely mixed, with competing approaches often outperforming the proposed approach. Most importantly, the authors report no error bars to illustrate the effect of random variability due to factors such as random seeds, making it difficult to assess the statistical significance and reproducibility of the otherwise apparently small relative differences reported between approaches.\n\nFinally, the results regarding the trade-off between sparsity and accuracy reported in Table 3 are largely incomplete, incurring risk of \u201ccherry-picking\u201d. I would strongly encourage the authors to complete these results by reporting results for the other two experiments (MNIST/CIFAR-10) as well as for a much wider regime of sparsity values, ranging from 0% to 100%.\n\n7. Some of the reported results for competing approaches appear to be, at first glance, substantially worse than in their respective publications.\n\nFor example, [5, Table 1] reports an error of 0.75% on LeNet-5-Caffe trained on MNIST, with sparsities per layer of 67% - 98% - 99.8% - 95%, for Sparse Variational Dropout. Instead, this submission reports the same error, but much lower sparsity rates of 66%-36%-59%-75%, respectively. Similarly, [6, Table 2] reports that Bayesian Compression prunes more than 99% of all parameters while maintaining an error of approximately 1%, which again seems at odds with the reported sparsities per layer of 60%-74%-89%-97% in Table 1 of this manuscript. While not as directly comparable due to small differences in the experimental setup, similar discrepancies in attained sparsity rates appear to also be present for CIFAR-10 and CIFAR-100 (e.g. [5, Figure 3]) as well as other baselines (e.g. [7, Table 1 and Figure 3]).\n\nI believe these apparent discrepancies should be discussed and justified, adjusting the experimental setup to match the original publications if necessary.\n\n# Minor points\n\n1. The submission would benefit from being proof-read for English style and grammar issues.\n\n2. The authors should clarify what exactly is meant by \u201cspeedup\u201d throughout Section 5.\n\n3. Given the focus of the manuscript on structured sparsity, more choices for the definition of groups should be explored in the experiments, following [1].\n\n# Proposed improvements\n\nI would encourage the authors to address the major points listed above. While they might be numerous enough to constitute a major revision, I nonetheless believe they would substantially strengthen the manuscript.\n\nIn particular, I encourage the authors to rewrite the manuscript with a focus on clarity, to better motivate their method both theoretically and in relation to prior work, to carry out additional experiments to highlight the benefits of the proposed approach and to clearly discuss the sources of discrepancies between the reported results for the baselines and the original publications.\n\n# References\n[1] Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" *Advances in neural information processing systems*. 2016.\n[2] Yuan, Ming, and Yi Lin. \"Model selection and estimation in regression with grouped variables.\" *Journal of the Royal Statistical Society: Series B (Statistical Methodology)* 68.1 (2006): 49-67.\n[3] Namkoong, Hongseok, and John C. Duchi. \"Variance-based regularization with convex objectives.\" *Advances in Neural Information Processing Systems*. 2017.\n[4] Wang, Chong, et al. \"Variance reduction for stochastic gradient optimization.\" *Advances in Neural Information Processing Systems*. 2013.\n[5] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational dropout sparsifies deep neural networks.\" *Proceedings of the 34th International Conference on Machine Learning*-Volume 70. JMLR. org, 2017.\n[6] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" *Advances in Neural Information Processing Systems*. 2017.\n[7] Louizos, Christos, Max Welling, and Diederik P. Kingma. \"Learning Sparse Neural Networks through $ L_0 $ Regularization.\" *International Conference on Learning Representations*. 2018."}