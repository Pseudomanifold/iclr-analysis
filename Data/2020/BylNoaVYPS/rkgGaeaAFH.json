{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a reasonable and natural way of modeling\nopponents in multi-agent systems by learning a latent space\nwith a VAE. This latent space will ideally learn the strategy\nthat the opponent is playing to inform the agent's policy.\nWith fixed opponents, the results across many tasks are convincing.\n\nMy one concern with this modeling approach is that it will start\nbreaking down if the opponents are *not* fixed as this\npotentially makes the agent more exploitable.\nThe opponents could learn to send adversarial sequences to\nthe opponent model that make it appear like they are playing\none strategy but then they could change strategies at\na critical point where it is too late for the agent to recover\nor perform optimally.\nThis type of exploitability has been explored in the game\ntheory community in [1,2] and the references therein.\n\n[1] Ganzfried, S., & Sandholm, T. Game theory-based opponent modeling in large imperfect-information games. AAMAS 2011.\n[2] Ganzfried, S., & Sandholm, T. Safe opponent exploitation. TEAC 2015."}