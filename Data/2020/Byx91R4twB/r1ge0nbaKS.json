{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a class-conditional GAN model for video generation called DVD-GAN. The generator uses a single latent variable and uses ConvGRU modules and ResNet blocks to generate N frames. The model uses a dual discriminator, with one discriminator that discriminates individual frames, i.e. an image discriminator, and one that discriminates the whole video. This is similar to the MoCoGAN model, with the main difference being that the video discriminator operates on a smaller resolution video, thus reducing the dimensionality of the input to discriminate. The model is used to generate videos after being trained on the large-scale Kinetics-600 dataset, which contains multiple examples and has a lot of variability across videos. The main contribution of the paper is to successfully train this large GAN model on the very large-scale Kinetics dataset. The samples from the model are very visually appealing and are qualitatively  superior to any previous video prediction model.\n\nWhile the paper mostly focuses on scaling up current models, it achieves significantly better qualitative results than previous models on a very challenging dataset, and therefore I believe it should be accepted as it is a significant advance in the field which probably will lead to follow-up work based on the model proposed here. \n\nHowever, there are a number of things that could be improved/minor comments:\n\n- Further details about the generator should be included in the main body of the paper, only having a figure to describe its architecture is not enough when the model and how to scale it up are key contributions.\n- The authors introduce a FID score for video which is similar to FVD, but FID is only used to report results in one experiment, while FVD is used for the rest of the experiments. Since the community uses FVD and there is a publicly available implementation of this metric, I'd suggest that the authors also include FVD scores in Table 1 to help reproduce the results. This is important since the FID metric is not explained thoroughly in the paper and small implementation details matter when using these metrics.\n- The related work section is missing many references to video prediction models, please add them to the paper. Some examples include:\nDecomposing motion and content for natural video sequence prediction. Villegas et al. ICLR 2017\nUnsupervised Learning of Disentangled Representations from Video. Denton and Birodkar. NIPS 2017\nPredrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. Wang et al. NIPS 2017 \nPredRNN++, ContextVP, ...\n- Additional metrics for the BAIR experiment, including LPIPS and SSIM. While FVD correlates well with human judgement, LPIPS does so as well and provides another evaluation of the model. Furthermore, metrics such as SSIM as used in SVG-LP can help better understand how well do the models cover the ground-truth sequence for given context frames.\n- Qualitative results for the BAIR experiment. Since most current models are not trained on Kinetics, qualitative samples for the BAIR dataset would help to qualitatively compare current methods to DVD-GAN.\n- In practice people have found that it is very difficult to train BigGAN-like models on images and videos. A common difficulty is that training diverges after a number of iterations, with the model starting to show mode collapse and big oscillations in terms of FID scores. Since the models are trained for a big number of iterations, have you observed these kind of issues with different hyperparameter configurations? If so, did you find any strategies to address it?"}