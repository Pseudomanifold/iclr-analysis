{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary:\nThis paper presents a method for training a generative adversarial network on high resolution videos and complex datasets. They propose decomposing the discriminator in Adversarial Networks into a spatial and temporal discriminator similar to previous works, however, the temporal discriminator downsamples the input using average pooling before forwarding it through the network. In experiments, the presented method outperforms previous state-of-the-art methods in the used metrics. In addition, the videos generated from the Kinetic-600 dataset in a non-conditional setting are the most realistic looking up to date. \n\n\nPros:\n+ Best generated video quality out there (In the Kinetics-600 dataset)\n+ Numerically outperforms baselines in all datasets\n\nWeaknesses / comments:\n* What is the message of the paper?\n- It is not clear to me whether this paper is about a new method or if it\u2019s about an empirical study of the network size for video prediction algorithms. If the paper is about a new method, then the novelty is lacking. Decomposing the discriminator into space and time discriminators was originally proposed by MoCoGAN. Therefore, the only difference I can see is that the input to the time discriminator passes through a downsampling function (average pooling). On the other hand, if the paper is about showing that a bigger video model generates better video, the paper is lacking a more in depth evaluation. It seems that the size (frame resolution and number of frames) is only done on the Kinetics-600 dataset (Table 1). However, it is not clear to me whether there is a conclusion to be made from these results about the model size. The FID seems to favor smaller models and IS favors bigger models? But the last (biggest) model does not increase the IS? I am not sure what to make of these results. In addition, there is no such results on the other datasets (UCF101 and BAIR) or on the frame conditioned experiments for Kinetics-600. Do the authors have any comments or clarifications on this?\n\n\n* Lacking proof that the network is not just overfitting to each class.\n- It seems that the authors train separate models for each video class. The generated videos are the best I have seen in such complicated dataset. The authors claim that the fact that the Kinetics-600 dataset is large automatically removes the concern of overfitting. However, I would like to see  evidence that this is true. Can the authors do something like sample a video and find its nearest neighbor in the training data and compare them to see if the model is simply memorizing the training set? Something like this could serve as evidence that the network is not simply memorizing the training data.\n\n\n* FVD Evaluation on Robot Push (marginal improvement based on FVD paper)\n- In Table 3, the authors present quantitative comparisons for frame-conditioned video prediction with state-of-the-art methods. The authors of the FVD paper state the following: \n\n\u201cIn Figure 5 it can be seen that when the difference in FVD is smaller than 50, the agreement with human raters is close to random (but never worse).\u201d\n\nThe difference between DVD-GAN and SAVP / Video Transformer is below 50. Does this mean that this result may not significant? It seems that basically the top 3 methods are performing similarly. Can the authors comment on this?\n\n\n* Comparing 11 frame conditional generation with 16 frame unconditional generation (DVD-GAN)?\n- In Table 4, the authors are comparing DVD-GAN-FP, Video Transformer and DVD-GAN. DVD-GAN-FP clearly outperforms Video Transformer. However, the authors make a comment about the DVD-GAN result being much better than the other two and that \u201cthe synthesis model\u2019s improved performance on this task seems to indicate that the advantage of being able to select videos to generate is greater than the advantage of having a ground truth distribution of starting frames.\u201d. However, the DVD-GAN model generates 16 frames, and not 11 like the other 2. Is this comparison correct? Also the other 2 methods (DVD-GAN-FP and Video Transformer) are constrained by what they must predict given input frames, does this fact affect the comparison / conclusion by the authors?\n\n\n* Claim of diversity but no evidence in the video prediction setting (frame conditional generation).\n- The authors claim that their method generates diverse videos, however, there is no palpable evidence that this is true. Something like showing examples of different videos generated given the same condition should be good or per-frame evaluations like PSNR and SSIM with error bars of the N number of samples generated given the same condition could also serve as proof that the generation is diverse.\n\n\n* Video results only provided for Kinetics-600\n- It looks like the authors only provide actual video files for the un-conditional generative model for Kinetics-600. Could the authors provide videos of the frame conditioned experiments as well (UCF101, BAIR and Kinetics-600)?\n\n\n* Argument in paper: \u201cThe synthesis model\u2019s improved performance on this task seems to indicate that the advantage of being able to select videos to generate is greater than the advantage of having a ground truth distribution of starting frames\u201d\n- I do not fully agree with this argument. Optimally, a frame-conditional model should identify all objects/background observed in the input frame and object/background dynamics if multiple frames are given. Given the optimally identified factors, video generation/prediction should be much better or the same as a purely generative model. The fact that the experiments in this paper show that the conditional model is not as good as the non-conditional model is most likely due to the model and/or objective function and not the generation scenario. Do the authors have any comments on this?\n\n\nConclusion:\nThis paper does indeed present the best video generation up-to-date given the dataset difficulty. However, the paper itself has multiple issues as stated above. Please try to address these in the next revision.\n"}