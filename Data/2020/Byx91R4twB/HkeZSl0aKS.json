{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper tackles the problem of efficient video generation. The authors present a Dual-Video-Discriminator Generative Adversarial Network (DVD-GAN) composed of an image-level spatial discriminator and video-level temporal discriminator. DVD-GAN achieves state-of-the-art results when benchmarked against the FID, IS and FVD quantitative metrics. Compared to previous video generation works, DVD-GAN is the first model to present compelling qualitative results on the UCF-101 and Kinetics-600 dataset.\n\nMotivation:\nEfficient video generation using GANs remains a significant challenge as it exacerbates all the issues associated with image generation using GANs. There are also increased memory and computation costs due to the 3D nature of video and the requirement for temporal modelling.\n\nMain Contributions:\n1. The authors propose to spatially down-sample the input into the video-level temporal discriminator. \n2. The authors propose to temporally sub-sample video frames for the image-level spatial discriminator\n\nSecondary Contributions:\n1. The authors set a benchmark for video generation on the Kinetics-600 dataset. This is significant due to the scale and complexity of this dataset in comparison to other datasets in the video generation literature.\n\n----------------\nPros:\n+ The paper is fairly well-written and clear\n+ The paper recognizes and tackles a significant challenge in video generation when using GANs.\n+ The ablation studies and experiments offer clarity with regard to the performance of the proposed model on different datasets and with different sampling strategies.\n+ This paper presents compelling results for high-resolution (i.e. > 64x64) video generation on complex datasets.\n\n\nCons:\nThe main weakness of this paper is that it does not represent a significant advancement in our understanding of video generation using GANs. The novelty of DVD-GAN is also limited with respect to prior video GAN literature. DVD-GAN appears to be a straight-forward application of the BIG-GAN [5] family of models to video generation. Subsequent experiments and results are tailored for this particular model with little to no applicability to prior models or generalization. This is a significant weakness given that this paper's main contribution is a discriminator component for video GAN architectures.\n\nOther Observations:\n- Dual Video Discriminator GANs were introduced in MoCoGAN [1], the naming of this model may mislead readers into thinking that this is the first such discriminator architecture.\n- Subsampling of the discriminator has already been explored in TGANv2 [2].\n\nPoints of Improvement:\n- Given that the video discriminator is the main contribution, it should be benchmarked against previous video generator models in the literature such as TGAN [3], MoCoGAN [1] and TGANv2 [2]\n- Given that the reproducibility of these results (at this moment in time) is difficult outside of a few organisations, can the authors provide a more thorough exposition of the efficiency aspects of their proposed discriminator. Currently, the best argument for the efficiency claims is the 58% reduction in pixels processed per video but what does this mean concretely, with respect to memory, computation and time efficiency?\n\n\nFinal notes:\n- Authors should clear up the confusion with class-conditional vs unconditional UCF101 results by clearly labeling class-conditional vs unconditional results. Furthermore, they should provide a detailed section on how the UCF101 metrics were achieved, it is known that video GAN metrics are highly sensitive to even the decision of when to normalize inputs to the evaluation network (see Appendix B of [4])\n\n- As impressive as the results are on Kinetics-600, they ultimately are dampened if they cannot be reproduced or built upon. The original Kinetics-600 dataset (based on youtube clips) is no longer available as youtube has either blocked, geo-blocked or removed a significant portion of this dataset. In light of this, do the authors plan to provide access to this dataset or should we place less significance on the Kinetics results?\n\n-------\n\nCurrent Review Decision:\nAlthough this paper does present state-of-the-art results for all video generation datasets and carries out thorough experimentation to demonstrate this. The DVD-GAN model rides significantly on the backbone of the BIG-GAN model to achieve this. The Dual Video Discriminator by itself does not provide a novel contribution for a conference such as ICLR as it has already been proposed [1] and discriminator subsampling has been explored in different forms previously [2]. Crucially, the claims on efficiency are not sufficiently explored and it is unclear what they are and how they would translate to other video GAN models in the literature. As such, I lean towards rejecting this paper in its current form.\n\n\n----------------\n[1] - https://arxiv.org/abs/1707.04993\n[2] - https://arxiv.org/abs/1811.09245\n[3] - https://arxiv.org/abs/1611.06624\n[4] - https://arxiv.org/abs/1909.12400\n[5] - https://arxiv.org/abs/1809.11096\n"}