{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a new loss function for dealing with label noise, claiming that the loss function is helpful in preventing overfitting caused by noisy labels. Some experiments show the effectiveness.\n\nThe theory part is unclear to me. (1 Why the derivative of Eq. (1) is Eq. (2)? The notation of f, f_\\theta, and f_w has been abused. The notation is confusing without explanation. It seems Eq. (2) is not correct and the following is not convincing. (2 Why a small gradient will slower the model to fit the data? This is not clear and maybe not true. (3 The assumptions \\hat{p}+\\hat{k}+\\hat{l}=1 is very strong to me. The events should be dependent. This makes all the theoretical analyses pseudo and not convincing at all. The authors may spend more effort to make the part clear, reasonable, and convincing.\n\nMany claims are very ambiguous. For example, \"the key point is that it always holds on some degree\", on which degree and why always holds? \"In some cases, it even leads to a better memorization phenomenon\", in which cases and why lead to better memorization phenomenon?\n\nSome claims are even wrong. For example, \"Traditional methods in label noise often involves introducing a surrogate loss function that is specialized for the corrupted dataset at hand and is of no use when one is not aware of the existence of label noise\" This is just for some specific methods, not for the most of them.\n\nTypo: \"We verify that . Therefore,\"\n\nOverall, I cannot understand why the proposed loss function works and cannot recommend acceptance for the current version.\n"}