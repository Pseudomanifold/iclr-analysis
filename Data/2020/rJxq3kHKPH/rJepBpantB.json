{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: The paper focusses on the problem of noisy labels in supervised learning with deep neural networks. The paper, in turn, proposes an early stopping criterion for handling label noise. The early stopping criterion is dependent on a new loss function that is defined as the log of true label + weight on a reservation option? The paper shows that when the labels are corrupted then the propose early stopping criterion does better than early stopping criteria obtained via the validation set. \n\n\\The first section of the paper establishes that when label noise is present in the dataset, then there are three stages to training a deep neural network. \nThe learning stage where the highest accuracy on the test set is achieved. \nThe gap stage where test set accuracy goes down. \nMemorization stage corresponds to when a deep neural network memorizes corrupt labels and test accuracy goes completely down. \nI cannot understand figure 1(a). The y-label says accuracy but it seems that the plot is about loss. What dataset was this and what architecture of DNN was used? The plot shows that the DNN achieved a 100% accuracy in 5 epochs. Is this result meaningful? Before establishing a hypothesis based on this should the hypothesis not be tested on multiple datasets.\nThe paper says that these stages are persistent across multiple architectures and datasets and as proof the paper says \u2018we verified that\u2019. Why can\u2019t the reader see the experiments? By across datasets does the paper mean MNIST and CIFAR? By across architecture does the paper mean the two architectures mentioned in the appendix one each for MNIST and CIFAR respectively? \n\nThe paper makes the assumption that label noise is symmetrically corrupted. Why and where does such an assumption hold? What happens to the proposed method if that is not true. \n\nAssumption 2: During the gap stage the model has learned nothing about the corrupt data points. \nHow is that even possible? \n\nEquation 1: So the loss function proposed is log(f(x)_y + (1/o) f(x)_m+1) .  What is y here? The true label? Why is y called a point mass? Is this different from the cross-entropy loss + log loss on m+1 ?\n\nI do not understand equations 2 to 5. \n\nFor figure 3 again what datasets were used?\n\n\u201c Making random bet will help with making money and a skilled gambler will not make such bets\u201d \nWhy does making random bet help with making money? If random is good how can a skilled gambler exist in such a game? What is this skill?\n\nk denotes the sum of probability of predicting anything that is not y or m+1 (it does not denote prediction). \n\nIn the experiments section what was the symbol for the rate of corruption changed from epsilon to r. Are they different? \n \nWhat is nll? \n\nIt seems that gamblers loss best shines when the corruption rate is as high as 80% . That is 80 percent of the data is corrupted. Does this mean that if I trained with only 20% of the non-corrupt data I would still get a 99% accuracy on MNIST (even without gamblers loss)? A comparison of this sort would have been useful.  \nOne astonishing result the paper presents is that with gambler\u2019s loss even with 80% corrupt labels a 94% test accuracy is possible on MNIST dataset. I think this is significant, this raises the question that is it required to label all the data points ina dataset to achieve high accuracy or is it possible to achieve just as much with only 20% of the labels?"}