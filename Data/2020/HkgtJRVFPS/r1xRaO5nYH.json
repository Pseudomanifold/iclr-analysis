{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper shows how to train an autoencoder that preserves topologically\nrelevant distance across varying length scales.  It presents a new\ndifferentiable loss term for topological distance between input and latent\nspace.  The topological loss has a number of nice features: compatibility with\nminibatch training, and extension to higher input and latent space\ndimensionality fairly easily.\n\nSince this trainable topological loss can be applied in more general scenarios\nthan simply auto-encoders or visualization, I think this well-written article\nis of general interest and worth publishing.\n\nThe main ideas and related work are presented clearly in sections 1--4, and the\nexperiments compare TopoAE with a variety of low-dimensional visualization\nmethods.  Embedding quality is evaluated with a wide variety of metrics on real\nand synthetic datasets highlighting the preservation of global and local\ntopology.\n\nMy only real issue, easily fixable, was that after the minibatch stability\ntheory (Sec. 3.3), I really wanted to know what minibatch sizes were used in\nthe experimental figures, and spent twenty minutes before admitting defeat.\nPlease include the actual TopoAE minibatch size when presenting the\nvisualization figures in the article and appendices.\n\nThe appendices contain a wealth of useful reading and experiments, and the\nprovided source code was clearly organized and useful to browse.  I did not go\nline-by-line through the appendix proofs."}