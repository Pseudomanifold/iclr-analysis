{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work discusses how to set the projection size for each head (head size) in multi-head attention module, especially Transformer. Theorem 1 is interesting, which points out a lower bound for the head size. The proposed method is to decouple the dependency between the head size and the embedding size. The experiments show that the proposed method is able to achieve comparable performance to BERT with fewer training cost.\n\nThe lower bound for the head size is a valuable result. However, the novelty is very limited. To decouple the dependency between the head size and the embedding size is not a novel point. In BERT/Transformer, it is set d_q=d_k=d_v=d, which is not a strict constraint. The only constraint in attention is to have d_q=d_k to allow dot product. Therefore, the proposed method is more like a tuning of hyper-parameters."}