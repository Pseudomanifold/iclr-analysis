{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents an attempt to reduce the memory complexity of Transformers. The authors call their model the Reformer. It presents a LSH based self-attention mechanism, along with reversible adaptation of Transformers. The Locality sensitive hashing scheme reduces complexity from L^2 to L which is pretty neat. \n\nTackling the quadratic complexity of self-attention is indeed an important and nice direction. I think the LSH based attention quite novel and is a natural solution to reducing the complexity of the self-attention module.  However, I think the technical description could be improved as the current form is quite confusing and difficult to parse.\n\nThe experiments are a little on the weaker side. Authors presented results on imagenet, enwiki and a synthetic task. I am mainly concerned if the Reformer works on tasks such as machine translation or other NLP tasks. The paper does not present much evidence that the effectiveness of LSH is broad and versatile.\n\nMy current vote is a weak accept, based on some preliminary understanding and the general novelty of the idea. \n\nI do have some questions/issues/comments:\n\n1) Given that there is some form of QK sorting, how is it possible to mask the future? Is this because tokens are sorted within buckets?\n2) Can the authors clarify what \"Causal masking on the Transformer is typically implemented to allow a position i to attend to itself.\" mean?\n3) I'm a little confused about how the sorting is being done. Can this be done in an end-to-end differentiable manner?\n4) Can the authors present some results on other tasks? While neat, I think other tasks (e.g., MT or QA) can be investigated to further ascertain that the LSH attention works well. Current experimental results are not too convincing.\n"}