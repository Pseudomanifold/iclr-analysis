{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper is about continual learning on NLP applications like natural language instruction learning or machine translation. The authors propose to exploit \"compositionality\" to separate semantics and syntax so as to facilitate the problem of interest.\n\nIn summary, the current manuscript is clearly not ready for publication. The writing is not good, as I cannot see clearly the backbone of the paper. Honestly, I got very confused by the presented contents. What\u2019s the problem/motivation? No preliminary background knowledge? What\u2019s the classic or na\u00efve method to solve the problem of interest? What\u2019s the main advantage/novelty of the presented method compared to that classic/na\u00efve method?\n\nPlease see the detailed comments below.\n\nIn the Abstract, you mentioned \"One of the key skills \u2026 ability to produce novel composition\u2019\u2019. Do you imply your method can continually learn new compositions? If so, how does that reflect in the technical parts and the experiments?\n\nThe paragraph before Section 3 might be redundant.\n\nMany typos exist. Such as the word \"iuput\u2019\u2019 in the 1st paragraph of Page 4.\n\nWhat\u2019s the problem of S2S-CL? Increasing input number n and output number m?\n\nWhat does the word \"COMPOSITIONALITY\u2019\u2019 mean in Section 3.2? Also, what\u2019s the relationship between the last two equations of Page 4?\n\nHow do you defend the simplifications adopted in the first Equation of Page 5?\n\nThe notation \"{0,1}^{Uxn}\u2019\u2019 usually represents a binary matrix of size Uxn. It is not suitable to use them to represent a matrix containing one-hot columns.\n\nAt the beginning of Page 6. Why entropy regularization can be introduced via L2 norm on the embedding matrixes p and f? Also why that L2 norm regularizations can `achieve disentanglement\u2019? Please provide the detailed proof or the reference. \n\nWhat are the detailed settings of the demonstrated experiments?\n"}