{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper addresses the problem of many-to-many cross domain mapping tasks (such as captioning or text-to-image synthesis). It proposes a double variational auto-encoder architecture mapping data to a factored latent representation with both shared and domain-specific components. The proposed model makes use of normalizing flow-based priors to enrich the latent representation and of an invertible network for ensuring the consistency of the shared component across the two autoencoders. Experiments are thorough and demonstrate results that are competitive or better than the state-of-the-art.\n\nDecision:\nThis work is a good example of meticulous and well-executed neural network engineering. It combines well-known ideas (variational auto-encoders, normalizing flow priors, invertible networks) into an effective and working solution for a complicated problem. The model is shown to bring improvements in the state-of-the-art over several metrics and benchmarks. The manuscript is well written and easy to follow (provided some technical familiarity with variational inference). It includes all the necessary details for understanding the method. Related works appear to have been discussed and compared properly, although I cannot assess if important works on cross-domain mapping are missing. For these reasons, I recommend this work for acceptance without reservation.\n\nAdditional feedback:\n- Above Eqn 5: K- divergence --> KL divergence\n- The code could have been cleaned up and better organized, for easier reproducibility and reuse. "}