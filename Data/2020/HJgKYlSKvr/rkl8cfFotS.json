{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "SUMMARY: A new modification to the rendering mechanism in a differentiable renderer  to generate 3D images, trained in an unsupervised manner on 2D images of faces\n\nCLAIMS:\n- train a generative model in an unsupervised way on 2D face images,\n- by generating 3D representation of [shape, texture, background] and feeding that to a differentiable renderer\n- modify the rendering mechanism to be differentiable wrt vertices of the triangular mesh (in addition to the texture)\n- curb the problems of training by using shape image pyramid, object size constraint\n\n\nLIT REVIEW:\nWell done, sufficient summarization of past work. But not \"first\":\nit would be pertinent to mention the ICCV 2019 paper \"HoloGAN: UNSUPERVISED LEARNING OF 3D REPRESENTATIONS FROM NATURAL IMAGES\" (https://arxiv.org/abs/1904.01326), which tackles the exact same problem. It does not use a triangular mesh representation, or a differentiable renderer, instead it uses a 3D feature representation and a neural renderer. However, the problems tackled are very close to avoid mentioning this paper.\n\nHence, it might not be good to claim\n- in the abstract: \"the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way\",\n- in introduction: \"For the first time, to the best of our knowledge, we provide a procedure to build a generative model that learns explicit 3D representations in an unsupervised way from natural images\",\n- and similar claims in other places.\n\nAlso, Pix2Scene (https://openreview.net/forum?id=BJeem3C9F7) also has similar ideas, although they tackled primitive shapes and not faces.\n\nDECISION: This paper has very promising results.\nAlthough it is limited to faces, which the community knows is something GANs are good at modeling because of the inherent structure, it is nevertheless a relevant piece of work in modelling 3D scenes in a graphics way and then training using adversarial learning. I am particularly impressed by the renderings of the depth and texture, and would be interested to explore that area further.\n\nHowever, it is more pertinent to check how the model performs objects more complicated than faces. A very simple experiment is to try this on ImageNet images, which are also centered and aligned. This would help investigate the possibility of extending this method to more complicated objects than faces.\n\nI would suggest to maybe put more focus on the fact that you have used the traditional graphics pipeline and integrated that into adversarial learning, as opposed to dealing with just weights and biases. That is indeed significant (in my opinion).\n\nKnowing that most GAN training time is spent in overcoming a lot of failures, it would be great if the authors can summarize the failure cases and elaborate on the experiments performed to overcome those failures. This was briefly touched upon in Section 7, but it would be great if they could elaborate more on them possibly in the appendix.\n\nIt would be great if the authors can share their code, there was no mention of any possibility of this.\n\nADDITIONAL FEEDBACK:\nPage 5:\n...an added constrain*T* on m in the optimization...\n...fooled by an inverted dept*H* image...\npage 6:\n...rendered <remove>attribute and</remove> depth, attribute and alpha map..."}