{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes modification to Deep CFR and introduce a simplified baseball environment to evaluate the modifications. The rigour of and detail in documenting the empirical evaluation is not currently of the standard I would expect for publication at ICLR. I will detail suggestions for improvement below, but my most pressing concern for discussion in the rebuttal is with regard to the 2nd conclusion - If the team has learnt a Nash equilibrium as initial strategy, then how can performance of either team be improved by only one team further adapting their policy as shown in Table 5 lower avg2 by the difference between No Adapt (0.61) and Guest Adapt (0.51)? If I have interpreted the authors results correctly, this demonstrates that the initial policy is not a Nash equilibrium.\n\nSuggested Improvements:\nIn the related work section, the narrative of the paper would be clearer if the authors introduced why the meta-learning literature is being reviewed. I would also recommend weakening the claim that \"instability of the training process\" is \"the critical issue in MARL\" to just noting it is \"an issue\" and request the authors justify the claim that this \"is particularly severe in a competitive environment\". Why is this more of an issue in fully competitive environments than it is in general sum games?\n\nIn Section 3.1 the authors claim they need to switch to TD learning from tree search due to stochastic state transitions but there are forms of search that can accommodate stochastic transitions so this claim needs to be removed. Perhaps the authors can motivate this change in another way? This section also introduces 2 further simplifications of the domain (1) learning strategies for half-innings and applying them to the whole game; (2) agents knowing their opponents true action chosen and not the noisy observation (e.g. pitchers target location instead of actual pitched location) and (3) \"both agents know each other's strategies\"  - these should be included in Section 2.1 when the domain is described.\n\nSection 3.1. closes by stating \"In principle, their average strategies will gradually converge to the Nash-equilibrium strategy.\" This is very weakly argued, to make such a claim the authors should provide evidence that their environment and modifications to Deep CFR meet the requirements of the theory where this guarantee was proven. \n\nSection 3.2. notes that the learning rate is \"manually determined\" but the precise methodology of tuning hyperparameters is not provided and no values of settings used for any hyperparameter are included in the paper. Without details of the methodolofy it is unclear if a rigorous empirical evaluation was performed and without the precise hyperparameter settings used the results are not reproducible.\n\nSection 4.1. notes \"It is very interesting to observe that the learned Nash-equilibrium strategy is actually quite similar to real-life baseball strategies.\" Setting aside the issue regarding whether a Nash equilibrium has been learnt, this is a subjective opinion not an rigorous empirical observation. The same applies to the comment \"The simulation results are very similar to real-life baseball games\" on page 9  Can you support these claims that the learnt strategy is similar to real-life by comparison to the data collected from the MLB Statcast?\n\nIn Section 4.2. there is further imprecision in the discussion of results. For example \"as the strategy adaption mechanism is employed, the WP of most teams \u2026 actually decreases.\" As this mechanism is a core contribution of the paper, this evaluation needs to be more rigorous. Is there a statistically significant difference caused by using the proposed mechanism? \n\nAll results presented should include quantification of variation as well as average values and the number of repeats these averages are taken from should be clearly documented. The inclusion of a limited subset of teams in empirical evaluations (e.g. Table 4 only including teams 0,1,2,3 and 12 and Table 5 only including teams 5 and 13) should be clearly justified.\n\nAll references to papers published in conference proceedings or journals should cite the published version of the paper and not the arxiv version. All references should include the full publication venue and not abbreviations (e.g. Zhang and Lesser, 2010 is currently listed as just AAAI). References to online resources should include a note of the date accessed.\n\nMinor Comments:\n1) The word \"purpose\" is used frequently in place of \"propose\" (e.g. line 7 of the abstract)\n2) Page 2: \"plays the-best-of-three game\" -> plays the best-of-three games\n3) Page 2: \"that worth further study\" -> that are worth further study\n4) Page 3: \"To simply the game\" -> To simplify the game\n5) Page 3: \"listed in C\" -> listed in Appendix C\n6) Page 8 (and recurring): Initial speech marks are the wrong way around, this looks like a Latex error.\n7) Page 10: \"we would relief\" -> we would relax\n8) Page 10: \"in the real life\" -> in real life\n"}