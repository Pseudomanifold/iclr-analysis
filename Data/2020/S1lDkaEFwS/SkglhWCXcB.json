{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to introduce randomness in a classifier\u2019s predictions to mitigate black-box attacks that rely on gradient estimation through finite differences. The intuition behind the defense is correct: finite differences rely on the outputs of the neural network being non-deterministic and accurate to estimate gradients near the test points being attacked.\n\nHowever, the threat model chosen in this paper is not well justified: the adversary cannot be forced to use a particular strategy. Unlike what is suggested in Section 3, estimating gradients through finite differences is not the only strategy available in the black-box threat model (this is later mentioned in Section 6). In this case, the adversary could for instance decide to adapt by instead mounting a black-box attack that relies on transferability. Because Figure 2 shows that the defense does not provide robustness in the white-box setting, this suggests that other forms of black-box attacks that either (a) rely on transferability or (b) are label-based only would still evade the model. This limitation should be addressed to understand how applicable the defense strategy is in a realistic deployment.\n\nPutting this aside, it is not clear from Figure 3 that an adaptive strategy was evaluated in the limited black-box setting that is considered here (the caption of Figure 3.b only describes a \u201cwhite-box\u201d adaptive adversary), or that the defense is effective. The attack success rates are high for many graphs and increase as the adversary averages over more runs. Moving forward, increasing further the highest number of runs would help appreciate the limitations of the approach: it is currently set to at most 100, which is low. \n\nAs far as organization is concerned, a lot of real estate is spent on background material, and few experimental results are presented to support claims made in the introduction. Addressing the above comments would probably require compressing background material a bit. \n\nPage-by-page details:\n\n1/ An attack is always adversarial by definition, \u201cadversarial attack\u201d is a tautology. \n\n2 / What do you mean by \u201csuccessful attacks\u201d?\n\n2/ What do you mean by \u201cstrongest\u201d loss?\n\n2/ Having a perturbation limited to be small does not guarantee it won\u2019t impact the semantics of the input, even in the vision domain. Have you verified that the perturbations that you chose left semantics unperturbed?\n\n2/ It is best to avoid making broad statements such as \u201cWe use the l2 perturbation penalty as this type of attack results in the strongest attacks\u201d because they are unlikely to hold across datasets and models.\n\n7/ Figures are difficult to parse (e.g., does T and U stand for targeted and untargeted?)\n\n7/ Distillation was already shown to be vulnerable to black-box attacks in [7]\n\n8/ Gradient masking was introduced in [7] prior to [25].\n\n8/ It would be good to justify the following statement (see my comment above): \u201cAlthough this is a valid attack vector for even black box models, we do not consider this type of attack in this work\u201d"}