{"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed another variant of BERT, called BERT-AL, which can deal with arbitrarily long inputs. The authors constructed the proposed method by combining the segment-wise BERT with the multi-channel LSTM. The authors validated the proposed method on the text summarization task and achieved higher performance than existing works. Their proposed method can be combined with other transformer-based approaches, such as XLNet or RoBERTa.\n\nThe paper tackles a relevant topic that is interesting to the attendees of ICLR, which is, \u201cHow do we get the overall information of a document, which has a number of sentences?\u201d.  However, the paper has several issues that I will try to cover in the following.\n\n(1) (No sufficient results which support authors\u2019 claim) \nFirst: In the introduction, authors claim that the model for NLP tasks whose inputs are usually long texts should be able to take arbitrarily long inputs so that the model can extract important information from the entire long documents. However, there is no concrete experimental example, where the existing methods (standard BERT or BERTSUM) fail to extract information.\nSecond: In the discussion of the experiment section, the authors claim that the proposed method BERT-AL can be parallelized and runs faster than existing methods. However, there is no comparison with existing methods about the running time. \n\n(2) (Clarity about the proposed method)\nThe presentation of the proposed method is slightly confusing. There is no concrete explanation about the learning procedure. Are any pre-training and fine-tuning procedures the same with those of the standard BERT?\nThe main part of the proposed method is to combine the segment-wise BERT with the multi-channel LSTM. How is the pre-training procedure conducted with parallelized computation? A detailed explanation should be shown.\n\n(3) (Clarity about the experimental results)\nIn table 4, the experimental results are shown. There are results of the baseline 1 & 2 (existing methods), the proposed method, and the existing method (BERTSUM). There is no discussion about the comparison of the proposed method with BERTSUM. It seems that BERTSUM\u2019s result is superior to the proposed method\u2019s result. Please explain more details of those results.\nThe paper includes an explanation about the dataset and the task but does not include the experimental environment (used machines and the time to conduct the experiment). For the fair evaluation of results, we need the information on the overall experimental environment.\n\n(4) (Presentation)\nThe presentation around mathematical formulas is a bit confusing. Please fix the following.\n- Subscripts of the formula should be \u2018\\mathrm\u2019\n- \u2018<=\u2018 should be \u2018\\le\u2019\n- The \u2019n\u2019 of the right-hand side of equation (5) is \u2019N\u2019?\n- The multiplication \u2018*\u2019 should be \u2018\\times\u2019"}