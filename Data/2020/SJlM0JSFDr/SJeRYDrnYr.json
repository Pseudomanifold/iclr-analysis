{"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The authors provide a theoretical analysis of deep Q-learning based on the neural fitted Q-iteration (FQI) algorithm [1]. Their analysis justifies the techniques of experience replay and target network, both of which are critical to the empirical success of DQN. Moreover, the authors establish the algorithmic and statistical errors of the neural FQI algorithm.\nThen, the authors propose the Minimax-DQN algorithm for the zero-sum Markov game with two players. They further establish the algorithmic and statistical convergence rates of the sequence of action-value functions obtained by the Minimax-DQN algorithm.\n\n[1] Martin Riedmiller. Neural fitted Q iteration\u2013first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pp. 317\u2013328. Springer, 2005.\n\nThe strengths of this paper are as follows.\n1. This paper is theoretically sound. The authors establish the convergence rates with detailed proofs step by step. \n2. It is the first theoretical analysis that provides the errors of the neural FQI algorithm with a ReLU network. This analysis provides a rigorous approach to understand deep q-learning algorithms.\n3. The authors propose an extension of DQN for the zero-sum Markov game with two players. They further analyze the convergence rates of the sequence of action-value functions obtained by the proposed algorithm.\n\nMinor comments:\n1. Page 2: In Notation, \"$\\|f\\|_{2,v}$\" may be \"$\\|f\\|_{v,2}$\"\u3002\n2. Page 3: In the 2th line of Section 2.2, \"$\\{d_j\\}_{i=0}^{L+1}$\" may be \"$\\{d_i\\}_{i=0}^{L+1}$\".\n\n"}