{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes two hierarchical models for the challenging task of question generation from paragraphs. One is the hierarchical BiLSTM model with selective attention, which learns to attend to important sentences and words from the paragraph. The other is the hierarchical Transformer architecture. The two hierarchical models both learn the hierarchical representations of paragraphs. Extensive experimental evaluation on the SQuAD and MS MARCO datasets shows that the hierarchical representations to be overall much more effective than their flat counterparts. \n\nThe paper is reasonably well written; one can easily follow and understand most technical aspects. However, I have some following major concerns about the paper:\n(1)\tThe idea is not novel, because the use of hierarchical structure to model long text is straightforward and has been widely studied in natural language processing tasks, such as dialogue [1,2] and document summarization [3,4]. The authors should explore the differences among these tasks and maybe provide a related work for the utilization of hierarchical structure in NLP [1,2,3,4].\n(2)\tThe experiment is not sufficient, the authors should compare their model to the SOTA baselines, such as the paragraph-level methods [5].\n(3)\tHow much data and how many volunteers did you use for human evaluation? The authors should give a detailed description for how to conduct your human evaluation.\n\n[1] Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, Joelle Pineau. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models.\n[2] Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma. Hierarchical Recurrent Attention Network for Response Generation\n[3] Yang Liu, Mirella Lapata. Hierarchical Transformers for Multi-Document Summarization.\n[4] Xingxing Zhang, Furu Wei, Ming Zhou. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization.\n[5] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation with maxout pointer and gated self-attention networks.\n\n"}