{"experience_assessment": "I do not know much about this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the problem of question generation from paragraphs. It introduces two models that incorporate learning of \"hierarchical\" representations -- one LSTM-based, and one Transformer-based. The paper shows that these models yield higher performance than their \"flat counterparts\".\n\nThe basic idea of leveraging hierarchical structure in language is of course sensible, but it's not clear to me how well-motivated or original this particular instantiation of hierarchical representation is, or what we have learned based on the comparisons reported.\n\nThe overall motivation, both for the problem in general and for this particular approach, should be improved. The fact that question generation is popular is not adequate argument for its importance -- the paper should argue for why this problem matters. As for this particular approach, by certain definitions of \"hierarchical\", one could say that many or most models these days incorporate hierarchical representations, so this paper needs to be much clearer about how its instantiation of hierarchy is different. Part of the problem here is that the exact methodology is obscured by the non-optimal structure of the writing. Even after a couple of passes, I'm fuzzy on the precise model details. In what precise way is this model's instantiation of hierarchical structure different from the general hierarchical structure of, for instance, processing text starting at the word or character level and producing token-level representations as well as sentence-level representations? \n\nEven more problematic is the lack of detail about the baselines to which the proposed models are being compared. The paper's claims about the value of this hierarchical approach hinge entirely on the comparison with these baselines (notably, there is no comparison to the current state of the art, presumably because these methods do not compare favorably). So the value here would be in what we learn about how a particular instantiation of hierarchy can improve over the lack thereof. But because I'm not clear on the precise differences between the baselines and the proposed models, I'm unable to assess whether this comparison is in fact a comparison of the presence of hierarchical representations, or something else -- for instance, is it possible that the \"hierarchical\" models simply have more parameters? Or have access to richer features? It's not clear what factors have been controlled for and what have not. There is also no analysis or discussion to assist us in understanding what we have learned.\n\nAnother area without adequate methodological description: what was the nature of the human evaluation? Was this crowdsourced, or simply performed by the authors? On how many questions was this human evaluation performed, and how were they selected? If the humans were not the authors, how were they asked to do the ratings?\n\nOverall, I think the paper needs to improve its clarity with respect to motivation and methodology, before it can be ready for publication."}