{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method for question generation given long paragraphs. The authors propose to encode the paragraph with a hierarchical architecture first encoding sentences in terms of constituent words, then paragraph in terms of constituent sentences. They experiment with both a bi-lstm and a transformer based architecture. The question decoder is a soft-attention model and the attention mechanism is specifically tailored to take into account the hierarchical nature of the encoder. The paper's main contribution is to propose two hierarchical architectures as well as a novel way of attending over the inferred hierarchy of representations during decoding. The empirical results suggest that the hierarchical versions of the architectures outperform the \"flat\" ones.\n\nThe main weaknesses are (1) the novelty of the proposed methods is low (2) results are hard to compare with previous work and some experimental details are omitted. Together, they put the paper below the acceptance bar.\n\n(1) Hierarchical encoders have been used under different architectural twists in a plethora of different seq2seq settings. The proposed hierarchical attention mechanism consists in re-weighting the word-level attention probability by the sentence-level attention probability, hence the hierarchical nature of the attention. Simply put, the probability of attending over word w given decoder state h_t, is just p(w|s, h_t) p(s|h_t), where s is the sentence containing word w and p(s|h_t) and p(w |\u00a0s, h_t) are just standard soft-attention. Something close has already been proposed in https://arxiv.org/pdf/1602.06023.pdf, but it's not cited in the paper.\n\n(2) Experimental results do not report any previous published baselines, but just baselines proposed by the authors. I'd love to see experimental results by https://www.aclweb.org/anthology/D18-1424.pdf. I don't know if their results are directly comparable. There's no information about how the human experiments have been performed, how many annotators have been used and what are the instruction given to the human annotators to judge syntax, semantics and relevance.\n\nThis paper could be improved by strengthening the experimental evaluation and reporting the relevant references to previous work:\n\n1) add reference to https://arxiv.org/pdf/1602.06023.pdf, and to the plethora of hierarchical attention models in the literature not cited in the paper.\n\n2) compare the scores of the model with https://www.aclweb.org/anthology/D18-1424.pdf. Especially report the results under different splits of Squad corresponding to previous work.\n\n3) Add more details on the human evaluation.  \n\n"}