{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe authors propose a method for learning hierarchical policies in a multi-task setting built on options and casts the concepts into the Planning as Inference framework. They claim that the method brings several advantages such as:\n-Stabilize end-to-end mtl,\n-Leads to coordination b/w master policies,\n-Allows fine-tuning of options\n-Learn termination policies naturally\n\nThe proposed approach derives the reward Eq.6 by extending the graph to options for Levine\u2019s tutorial. Eq.6 is simply the extension of the reward of maximum entropy RL to the options framework. The ideas presented in the paper are interesting, but I have concerns about the scalability of such an approach. Please see the detailed comments below.  Additionally, please note that although I have marked a weak reject, I am open to adjusting my score if the rebuttal addresses enough issues.\n\nDetailed Comments:\nA primary weakness of this approach is that it seems like there is one network that learns the options and is shared across all task (that would be the prior) and then there is a task-specific network for all options (posterior), wouldn\u2019t this be very difficult to scale if we want to learn reusable options over the lifetime of an agent? If there are n tasks, do you need to use n different networks? \n\nThe authors assume that all options are present everywhere i.e. I \u2286 S. I think the work could benefit from removing this assumption.\n\nThe authors mention that unlike (Frans et al., 2018), they learn both intra-option and termination policies: there is definitely more work that aims to learn both the skill and its termination. It would be more complete to cite additional references here that learn both of these or rephrase this sentence. \n\nIt does not seem clear why \u201cterm 1 of the regularization is only nonzero whenever we terminate an option and the master policy needs to select another to execute.\u201d This doesn\u2019t seem true as this is a ratio of the two probabilities and not just the instantiation of the random variable. \n\nThe results in moving bandits alone are very convincing. However, in Taxi (2b) distral+action seems to be as good as/even better MSOL. In directional Taxi (2c) Distral(+action) manages to reach the same final performance (if we care about that), can you please comment on this.\n\nSome parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? if yes, then it is obvious that their method could improve over learning on 12 tasks with one set of network. Please clarify. \n\nOne major concern is that the only high dimensional experiment is a swimmer and it is not immediately clear how much do we gain there. Distral is relatively closer in performance to both MSOL and MSOL frozen. I would recommend evaluation in a variety of high-dimensional domains such as other instances in Mujoco, and visual domains. In particular, the proposed ideas would make a stronger case if the baselines included other multitask hierarchical agents such as [4] for example. A discussion including some of the missing relevant related multi-task literature would also be helpful [1,2,4,5,6].\n\n[1] Mann, T. A., Mannor, S., & Precup, D. (2015). Approximate value iteration with temporally extended actions. JAIR, 53, 375-438.\n[2] Konidaris, G., & Barto, A. G. (2007). Building Portable Options: Skill Transfer in Reinforcement Learning. In IJCAI,\n[3] Andreas, J., Klein, D., & Levine, S. (2017). Modular multitask reinforcement learning with policy sketches. ICML\n[4] Tessler, Chen, et al. \"A deep hierarchical approach to lifelong learning in minecraft.\" Thirty-First AAAI Conference on Artificial Intelligence. 2017.\n[5] Ammar, Haitham Bou, et al. \"Online multi-task learning for policy gradient methods.\" International Conference on Machine Learning. 2014.\n[6] Mankowitz, Daniel J., Timothy A. Mann, and Shie Mannor. \"Adaptive skills adaptive partitions (ASAP).\" Advances in Neural Information Processing Systems. 2016."}