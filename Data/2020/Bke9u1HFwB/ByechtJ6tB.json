{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency?\n\nIn this paper, the authors revisit the model-free baselines used in recent model-based reinforcement learning algorithms. And after more careful tuning of the hyperparameters, the model-free baselines can obtain comparable and much better performance using the same number of samples.\n\nThe paper is well-written, and the experiments are well-designed to support the claim.\nHowever, the research contribution of the project is limited to image-space discrete RL tasks, and does not cover the wide-range other RL. In terms of the novelty, the proposed algorithm is not fundamentally different from Rainbow. \nTherefore I tend to vote for borderline for this paper and am willing to increase the scores if more improvement is updated.\n\nBesides, I would like to thank Mr. Ankesh Anand for mentioning the Hasselt et al. (2019) [1] paper, which is indeed very similar in terms of the topic discussed and the methods used to evaluate the algorithms. \nI have also read Hasselt et al. (2019) before this submission, but I think it would be fair to say the two papers are relatively concurrent.\n\nPotential improvement:\n- It will be great if the authors also extend the discussion to current reinforcement learning algorithms that are applied in continuous tasks from states. In [2], similar conclusion is observed in continuous control tasks, where SAC [3] / TD3 [4] perform substantially better than many of the state-of-the-art model-based baselines.\n\n[1] van Hasselt, Hado, Matteo Hessel, and John Aslanides. \"When to use parametric models in reinforcement learning?.\" arXiv preprint arXiv:1906.05243 (2019).\n[2] Wang, Tingwu & Bao, Xuchan & Clavera, Ignasi & Hoang, Jerrick & Wen, Yeming & Langlois, Eric & Zhang, Shunshi & Zhang, Guodong & Abbeel, Pieter & Ba, Jimmy. (2019). Benchmarking Model-Based Reinforcement Learning.\n[3] Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n[4] Fujimoto, Scott, Herke van Hoof, and David Meger. \"Addressing function approximation error in actor-critic methods.\" arXiv preprint arXiv:1802.09477 (2018).\n"}