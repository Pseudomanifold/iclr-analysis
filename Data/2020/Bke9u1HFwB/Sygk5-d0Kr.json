{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper presents a data-efficient version of the Rainbow DQN by Hessel et al. (2018) that matches the performance of a recent state of the art model-based method by Kaiser et al. (2019) on several Atari games. Particularly, the paper empirically shows that a simple hyper-parameter tuning, in this case increasing the ratio of number of training steps to the environment interactions as well as decreasing epsilon-decay period, can result in significant improvements in sample efficiency of the Rainbow DQN agent. They show that their method (which requires significantly less computation) can outperform the model-based variant on half of the games tested, while performing worse on the rest. \n\nOverall, I believe that this paper is below the acceptance threshold due to lack of 1) novelty, 2) significance and 3) depth of analysis.\n\nThe observation that more training updates with the agent\u2019s existing experience results in  sample efficiency in DQN method has already been shown empirically by Holland et al. (2018) which has also been cited by this paper. Additionally, more recent work by Hasselt et al. (2019) which is not currently cited, explicitly addresses the same problem as this work by tuning the hyper-parameters of the Rainbow DQN to achieve significant sample efficiency, outperforming Kaiser et al. (2019) in 17 out of 26 Atari games tested. In addition, their work gave a rather detailed motivation and analysis of their findings, proposing and testing several hypotheses for how and when model-based methods could outperform replay-based model-free variants. \n\nIn comparison to prior work, the current paper has a more limited scope and significance. Hence, I believe more work would be needed to warrant acceptance.\n\nHessel et al., 2018: Rainbow: Combining Improvements in Deep Reinforcement Learning https://arxiv.org/abs/1710.02298\nKaiser et al., 2019: Model-Based Reinforcement Learning for Atari https://arxiv.org/abs/1903.00374\nHolland et al., 2018: The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces https://arxiv.org/abs/1806.01825\nHasselt et al., 2019: When to use parametric models in reinforcement learning? https://arxiv.org/abs/1906.05243\n\n"}