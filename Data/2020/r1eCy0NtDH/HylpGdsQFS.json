{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper discussed the property of the NTK with the increasing depth L with the help of the Edge of Chaos initialization. The authors show that if deep neural networks are not properly initialized, the NTK can have a large condition number, which leads to the poor performance of training and generalization. Moreover, the authors also introduce the conditions that make the neural network trainable by decreasing the convergence rate to a nearly constant kernel w.r.t the depth L by using the specific Edge of Chaos initialization as well as different activations and use residual connections. Experiment results show that the theoretical results are well aligned with the practice.\n\nDetailed Comments:\n1. At the paragraph after Lemma 1, the authors claimed that f_t(x) is entirely given by f_0(x), which means a generalization error of order O(1). This is a little inaccurate I think, as NTK can only characterize the training process and does not directly indicate generalization. Also, as t to infinity, the coefficient of initialization f_0(x) can be arbitrary small and thus the f_t(x) is entirely decided by initialization is not accurate as well. To analyze generalization with NTK, we need a little more, see [1]. This is not some core issue, but I think the authors should make the description more accurate.\n2. The authors should explain what richer limiting NTK means at the end of the Sec 3. It is unclear how the convergence rate of the NTK related to the richer limiting NTK.\n3. A typo in the first paragraph in Sec 4, ouerselvs to ourselves.\n4. In definition 1, the second-order derivative of \\phi is defined via the indicator function 1_{A_i}? It\u2019s better to use another notation like \\mathbf{1} or \\mathbb{1} to make it more clear.\n5. I think for clarity the authors should give a formal definition of ANTK, or if it is unnecessary, better directly use the original NTK. Also, it is better to show the invertibility of the NTK is equivalent to ANTK more formally before Proposition 2. This conclusion is not so straightforward at the first view. Meanwhile, I think the invertibility of ANTK can just indicate a bad condition number of NTK? I don\u2019t think it directly related to the invertibility of NTK.\n6. There are several typos in the appendix. Please go through the appendix and fix them.\n7. In the proof of Lemma 3 and Lemma 4, I don\u2019t get why |a_l| < l + |a_0|. It may depends on the property of q? For sufficiently large q, if a_0=0, a_1 = q + O(1) which is not necessarily smaller than l=1? But to get a_l/l bounded, it is not necessary to use this. Also, I think it\u2019s better not omit the constant in the dynamic system of Lemma 3. \n8. Better use the Stirling\u2019s approximation rather than k to infinity in the calculation of k!/(k-\\alpha-1)!\n9. The lemma number of Appendix C is in a chaos. And I don\u2019t quite get the idea behinds Appendix C. Is it relevant to the contents in the main text?\n10. I also feel the content in Appendix D is unnecessary for this paper.\n\nOverall, this paper is interesting and gives a unified perspective on the recently developed NTK and Edge of Chaos initialization. It also sheds light on the impact of different activation function on NTK by generalizing the results of [2]. I feel this paper can help the communities have more understanding on the properties of deep neural networks. However, this paper can be better if the authors can polish their paper and reorganize the appendix part.\n\n[1] Arora, Sanjeev, et al. \"Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.\" International Conference on Machine Learning. 2019.\n[2] Hayou, S., Doucet, A. & Rousseau, J.. (2019). On the Impact of the Activation function on Deep Neural Networks Training. Proceedings of the 36th International Conference on Machine Learning, in PMLR 97:2672-2680\n"}