{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the limiting behavior of neural tangent kernels when the depth grows to infinity. They show that the obtained limit kernels are trivial (a constant) unless one uses 'edge of caos' initialization, in which case they are close to the identity. The authors compare the convergence for different activations, showing a slower convergence (hence better propagation) for some piecewise smooth activations compared to ReLU. For residual networks, the 'edge of caos' behavior is claimed to be in place regardless of the initialization.\n\nThe detailed characterization of these limiting kernels for different activations and initializations is interesting. Yet, the work is quite incremental and of limited significance, in that such EOC initialization was known to be important for controlling propagation of both activations and gradients with such networks, so it is no surprise that the NTK has similar properties, given that it basically consists of the sum of similar gradient covariances.\n\nsome comments:\n- title: isn't the \"mean-field behavior\" already subsumed in the definition of NTK?\n- contribution 4.: \"more suitable\" seems a bit strong if it's just a log(L) factor?\n- after lemma 1: generalization may not provide a strong argument here unless further discussed: the constant kernel is obviously bad for learning anything, but the EOC limiting kernel is also pretty bad for predicting anything outside the training data\n- section 3: typo \"ourselves\"\n- prop 3: specify that phi is the relu\n- the proof of prop 3 should be more detailed. Also, it is not obvious that you are giving the correct formula for the NTK of resnets\n- Table 1: is it meaningful to compare performance with such low accuracies?"}