{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "*Summary*\nThis paper provides the analysis of three algorithms in the context of minmax convex concave games: Simultaneous stochastic subgradient method, Simultaneous gradient with optimism and Simultaneous gradient with anchoring. These three algorithm are first analysed in continuous time with an ODE perspective and then leverage these intuitions and techniques to analyze the discrete time versions.\n\nI think that these contributions are of interest of ICLR community, however, I have some concerns regarding the presentations of the results. \n\n*Decision*\nI vote for a weak accept that could move to an accept if the authors improve the presentation of the paper:\nThe paragraph \u201cRegularized dynamics and convergence\u2019 in Section 3.1 in quite hard to follow. This subsection is basically the proof of the convergence of the continuous version of  GD-O. Stating the result before the proof would help the reader to understand where the authors want to go. \nVery same point for The subsection 4.1\nFor the stochastic version of SimGD-A a new parameter $\\epsilon$ is introduced without any comment or description on why it is necessary. \nA FID above 40 for MNIST is very far from standard results (that are below 1). Thus I am not convinced by the practical advantage of Anchored Adam on these models that have performances results very far from the standard ones.  (for instance between 20 and 25 for CIFAR10 is very reasonable). It is maybe because you do not use convolutional layers in your architecture. I think that using a DGAN architecture (for instance the one from the pytorch tutorial https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) would give the expected results (i.e. FID close to 0).\n\n*Questions*\n\n- When you cite Lassale Principle you mention that if $z_\\infty$ is a limit point of $z(t)$ then starting at $z_\\infty$, you stay at a constant distance to $z_*$. But in the bilinear example you give any point in the circle is not a limit point (size no dynamics converge to it). I guess when you said limit point you meant adherent point ?\n- Is the condition $\\epsilon$ only necessary for the proof ? Does $\\epsilon =1$ work in practice ? \nIf no, what is the best value for $\\epsilon$ ?\n"}