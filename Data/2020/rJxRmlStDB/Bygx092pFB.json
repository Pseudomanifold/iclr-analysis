{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies how to extract/select suitable training data from comparable \u2014rather than parallel\u2014 corpora. The idea sounds reasonable.\n\nMy major concern is about the evaluation: it didn't compare with any existing work. Actually there quite a few papers  on mining parallel sentences from comparable corpora such as Wikipedia, as shown below. Seems the authors are not aware of those works and didn't review and compare with them. Without such comparisons, it is difficult to judge the effectiveness of the proposed method and the quality of this work. \n[1] Finding similar sentences across multiple languages in Wikipedia, Proceedings of the Workshop on NEW TEXT Wikis and blogs and other dynamic text sources. 2006.\n[2] Method for building sentence-aligned corpus from wikipedia, 2008 AAAI Workshop on Wikipedia and Artificial Intelligence (WikiAI08). 2008.\n[3] Extracting parallel sentences from comparable corpora using document-level alignment, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.\n[4] \"Improving machine translation performance by exploiting non-parallel corpora.\" Computational Linguistics2006.\n[5] https://www.aclweb.org/anthology/W04-3208.pdf\n[6] https://openreview.net/pdf?id=ryza73R9tQ\n\nMinor issues:\n\t1. \"For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100k merge operations is applied.\" Most papers on neural machine translation don't use such a large BPE size, which is likely to lead to better performance. It would be better to use the same setting as previous work for fair comparisons.\n\n\t2. \"In the case of SS-NMT, both tasks \u2014data extraction and learning NMT\u2014 enable and enhance each other, such that this mutual supervision leads to a self-induced curriculum, which is the subject to our analysis.\" Similar idea, mutual boosting between data selection and model training, has been explored in the following paper, although not for machine translation. What's the difference between these two papers?\nLearning to Teach, ICLR 2018."}