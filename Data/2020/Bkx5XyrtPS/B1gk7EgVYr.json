{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "Summary: \n\nThe paper shows a deep linear network has no spurious local minima as long as it is true for the two layer case for any convex differentiable loss. \n\nComments: \n\n1) I understand that there exists some work on deep linear network recently. However, they seem to be only for theoretical purpose. Most of the current practical problems do not consider this kind of network for training. If it has high impact in practice, then people are starting to use it. Could you please provide more reasons why we need to care about this impractical network? \n\n2) It is still unclear about the contributions of the paper. Why \u201cdeep linear network has no spurious local minima as long as it is true for the two layer case\u201d is important? And what we can take any advantage from here? What if there exist some spurious local minima for the two layer case (which is widely true)? \n\n3) The paper looks like a technical report and seems not to be ready. \n\nThe results are quite incremental from the existing ones. The contributions of this work to the deep learning community are still ambiguous. \n"}