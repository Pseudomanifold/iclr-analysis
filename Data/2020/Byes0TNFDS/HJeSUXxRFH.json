{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed Entropy Penalty (EP) training, based on Information Bottleneck (IB) to make the trained model generalize beyond the IID assumption that is satisfied by usual training and testing datasets.\n\nFirst, a loss function derived from IB is provided Prop. 1. Then they argue that minimizing this loss over lower layers is better Eq. (5) and (6). They further assume the hidden layer is Gaussian, and use squared l2 loss as entropy penalty.\n\nThey theoretically analyze a different loss function Eq. (7) under two simple cases, where optimal solutions have closed forms, and find that the optimal solutions contain smaller weights for non-discriminative features (p_i there).\n\nExperiments on coloured versions of MNIST are conducted to show that the proposed Entropy Penalty achieves better results than several baselines.\n\n1. There is no definition of a robust or non-robust feature, but just a vague description by a simple example (camel appearance). I suggest before presenting the method, providing examples like p_i the synthetic cases, to give a sense of what kind of features are (non-)robust.\n\n2. The Gaussian assumption of the hidden layer is quite restricted as mentioned. And why only apply EP on the first layer rather than all the hidden layers?\n\n3. The objective in analysis Eq. (8) looks different from Eq.(7). Why not using the original objective?\n\n4. The synthetic examples are not convincing enough. For example, what are the solutions of other methods, and how can we see the EP solution is better than others for these examples. More comparisons are needed to give a sense of the advantage of EP.\n\n5. Why is randomly assigning colours considered as a distribution shift? It is not clear to connect what the synthetic examples try to deliver and the coloured version of MNIST here. \n\nThis paper shows that EP based on IB, together with the Gaussian assumption of the hidden layer can learn robust features as shown in synthetic case analyses and coloured MNIST experiments, comparing with several baselines. However, there are several gaps,\n\n1) what exactly is a non-robust feature, in synthetic examples, these are p_i and k, however, in experiments, it seems to colour. There is no definition and thus it fails to show what exact kinds of features the proposed EP actually can capture (p_i, k, colour, or something more general). In this sense, I have the impression that the presentation is not very clear (EP can learn something, but what it is?).\n\n2) The claim is that EP learns robust features for deep learning methods, but both the analyses and experiments are not enough to show that. First, several restricted assumptions are made as mentioned, learn the first layer only, Gaussian assumption, and the examples are too simple, and lack of calculation for other methods and comparisons. Second, just coloured version of MNIST is not convincing to show that EP captures many/most robust features, even though the proposed EP significantly outperforms the other baselines here because of there probably (must) have many different kinds of other features. More experiments are needed to support the efficacity of EP."}