{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a multigrid memory architecture by introducing multigrid CNN [1] into convolutional LSTM network [2]. The method extends the convolutional LSTM with bigger memory capacity in forms of multigrid CNNs. Some specific designs such as multiple threads and encoder-decoder are also proposed. The model is validated with synthetic tasks such as spatial mapping, associative recall and priority sort. \n\nPros: \n* The method is well-motivated. Utilizing multigrid hierarchy enables convolutional LSTM to operate across scale space and thus may achieve richer representation for the hidden state memory.\n* The experiments are well designed (especially RL tasks), demonstrating the advantage of the proposed model.\n\nCons:\n* The proposed model is not presented clearly. The paper does not show details on how [1] and [2] are integrated, which requires the reader to refer back to the old works and make inference on the integration. Besides graphic illustration, the authors should include a brief review on [1, 2] and introduce some basic formulas describing the combination between the two.\n* Section 3.2 is supposed to contain the most important design considerations, but details seem missing. For example, what does the Writer do to the memory?\n* The contribution is rather incremental. Without detailed description, it seems that the proposed model is a straightforward replacement of the vanilla CNN with another CNN (multigrid CNN) in the convolutional LSTM architecture.\n* The experiments are not very satisfactory for the \u201cgenerality\u201d claim that the paper makes. \n\nQuestions and concerns:\n* Could you explain the term \u201caddressable memory space\u201d? It seems that your network\u2019s memory comes from the internal states of LSTM. How is the memory addressable? \n* The analysis on information routing seems interesting. However, how does it relate to the memorization capacity? Is there any guarantee that the information from source grid is preserved in higher levels/layers? How does it differ from using vanilla multiple-layer neural networks?\n* As DNC is not originally designed for image inputs, how did you feed the images to DNC? Did you tune DNC carefully by adjusting the number of elements per memory slot? Also, DNC seems not a really strong baseline. Other solutions to increase memory capacity of MANNs exist [3, 4]\n* For algorithmic tasks, why don\u2019t you include ConvLSTM as a baseline? Also, NTM maybe a better baseline than DNC for these tasks. \n* What is the model size and computational complexity compared to other MANNs?\n* The model is naturally fit for visual inputs. Is there any advantage when applying it to other sequential data (NLP, time-series)? \n\nReference\n[1] Ke, Tsung-Wei, Michael Maire, and Stella X. Yu. \"Multigrid neural architectures.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6665-6673. 2017.\n[2] Xingjian, S. H. I., Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. \"Convolutional LSTM network: A machine learning approach for precipitation nowcasting.\" In Advances in neural information processing systems, pp. 802-810. 2015.\n[3] Rae, Jack, Jonathan J. Hunt, Ivo Danihelka, Timothy Harley, Andrew W. Senior, Gregory Wayne, Alex Graves, and Timothy Lillicrap. \"Scaling memory-augmented neural networks with sparse reads and writes.\" In Advances in Neural Information Processing Systems, pp. 3621-3629. 2016.\n[4] Hung Le, Truyen Tran, and Svetha Venkatesh. Learning to remember more with less memorization. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=r1xlvi0qYm.\n"}