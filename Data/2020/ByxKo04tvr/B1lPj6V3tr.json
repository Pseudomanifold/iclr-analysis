{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes the multigrid memory networks which combine multigrid convolutional layers with LSTMs and evaluates its performance on a reinforcement learning-based navigation task and two algorithmic tasks of priority sorting and associative recall. \n\nThe authors claim that by integrating LSTM within the layers of the network, it affords larger memory size while remaining parameter efficient compared to other memory-augmented networks like the DNC, which abstract its memory module as a separate unit from its computational units. \n\nThe authors show with their experiments that multigrid memory networks outperform DNC and other models that lack its multigrid inference property.\n\n\nWhile the experiments show the proposed networks\u2019 superior performance over baselines, my main concern about this paper is the lack of comparison with more recent memory-augmented models [1,2]. \n\nMoreover, in all the experiments, the DNC baseline has memory sizes that are much smaller than the multigrid memory networks. To my understanding, the memory module of DNC can be scaled up without increasing the number of computational parameters. Why is the DNC\u2019s memory not scaled to match that of the multigrid memory networks? It would seem unfair to compare against a baseline with much smaller memory in memory-intensive tasks.\n\n\nOther comments:\n\n\n1) How are the input tensors upsampled in the multigrid memory layers?\n\n2) How is the visualization on the right of Figure 3 generated? It\nwould be more convincing to compare it with that of DNC.\n\n3) Clarity of some parts, especially Section 3.1 & 3.2, could be\nimproved with more formal mathematical statements about the multigrid\nmemory networks.\n\n4) How would multigrid memory networks generalize to other tasks that\ndo not involve input images?\n\n\n\n[1] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured\nmemory for deep reinforcement learning. ICLR, 2018.\n\n\n[2] Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis,\nVijay Kumar, Daniel D. Lee. Memory augmented control networks. ICLR,\n2018."}