{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Recurrent neural networks that can grow their memory capacity independent of the number of training parameters are an interesting topic. DNC, memory networks and NTM (all cited in this work) are some examples.\n\nThis work proposes an architecture inspired by an approach used in the computer vision literature, a multi-scale CNN. However, each cell of the CNN here is a convolutional LSTM.\n\nThis approach allows the memory capacity of the architecture to be increased (by increasing the number of cells) while maintaining a fixed number of parameters. The multi-scale nature of it allows memory operations across multiple scales the 2D grid in an efficient manner.\n\nThey test this architecture on a mapping and localization task (a natural fit for the multi-scale architecture) and find it outperforms other architectures including a single scale version of the same architecture.\n\nThey also compare against the DNC on tasks similar to that used in the original paper (priority sort) and associative recall and again find it learns in fewer iterations and achieves good performance.\n\nOverall, this architecture, while not groundbreaking, is novel in this context and the results show empirical gains. The paper is fairly well written.\n\nThis work could be improved by providing more detail (e.g. in the appendix) on the losses and approach used in the navigation task (the only explicit discussion of the loss used in the navigation task is in figure 3). It would also be helpful to provide more detail on the other tasks in the appendix.\n\nFinally, there is little analysis (either theoretical or empirical) on the runtime and memory requirements of this model. For example, figure 6 would seem to imply this model is running slower than the DNC (already quite a slow model) since it has completed less iterations? At a minimum, some empirical numbers of run time speed and memory usage compared with the DNC would be helpful."}