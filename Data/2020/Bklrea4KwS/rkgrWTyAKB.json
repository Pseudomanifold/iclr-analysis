{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new approach to weighting in multiple-instance learning scenario. They multiply scores for instances by Gaussian normalized weights. The hyperparameters of the Gaussian RBF are either estimated locally (i.e., within a bag) or trained across bags. Additionally, the authors verify whether it is better to have two separate neural networks (one for instance scoring and one for instance weighting) or share weights between them. Eventually, a variant with globally trained hyperparameters of the Gaussian RBF and separate weights of neural networks performs the best.\n\nIn general, the paper is very well written and easy to follow. The proposed solution is reasonable and, importantly, performs better than other SOTA approaches. The organization of the paper is proper, all concepts are well explained. The experiments are meaningful and answer important questions (i.e., which of the considered variants is the best, how the proposed approach compares to SOTA methods). In my opinion, the paper could be accepted.\n\nREMARKS:\n- What is the temperature value used in the log-sum-exp?\n\n- The authors claim that the log-sum-exp pooling operator is theoretically more relevant for MIL. Could you comment more on that?\n\n- Why do the authors compare to ATT instead of Gated-ATT? In [8] it was shown that Gated-ATT performs better."}