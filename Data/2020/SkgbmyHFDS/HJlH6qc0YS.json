{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to study whether a learned reward function can serve as a locus of knowledge about the environment, that can be used to accelerate training of new agents. The authors create an algorithm that learns an intrinsic reward function, that when used to train a new agent over a \u201clifetime\u201d (which consists of multiple episodes), leads to the best cumulative reward over the lifetime. As a result, the learned intrinsic reward is incentivized to quickly \u201cteach\u201d the agent when and where to explore to find out as-yet unknown information, and then exploit that information once there is no more to be had. Experiments on gridworlds demonstrate that these learned intrinsic rewards: 1. switch between early exploration and later exploitation, 2. explore only for information that is relevant for optimal behavior, 3. capture invariant causal relationships, and 4. can anticipate and adapt to changes in the extrinsic reward within a lifetime.\n\nI very much appreciated the design of the environments to test for specific properties within the learning algorithm: I think these experiments provide a very useful conceptual analysis of what learned intrinsic rewards can do.\n\nMy main qualm with the paper is with its significance -- the authors claim that the goal is to find out whether reward functions can be loci of knowledge, but we already know the answer is yes: the whole point of reward shaping is to improve training dynamics by building in knowledge into the reward function. It is not a surprise that learned reward functions can be loci of knowledge if our hand-designed reward functions already do so.\n\nTo me, the more interesting aspect of this paper is how much benefit we can get by learning intrinsic reward functions, relative to other ways of improving training dynamics. The authors do show that by allowing the intrinsic reward to be recurrent (and so dependent on past episodes), it is able to first incentivize exploration and later exploitation, which standard reward shaping cannot do (since usually reward shaping still maintains the assumption that the reward is a function of the state). However, given this motivation, it would be important to see comparisons between the proposed method of learning intrinsic rewards, and other methods for fast adaptation in the literature, such as MAML, which as I understand also has many of the properties highlighted in this paper.\n\nIdeally there would also be experiments on more complex environments: the environments in the paper have 104, 25, and 49 states. If we in the ABC environments if you count \u201cwhether or not reward(object) is known\u201d as part of the state, that multiplies it by 2^3 = 8 giving 200 and 392 states, if you then further add the ordering of r(A), r(B), and r(C), that multiplies by a factor of 3! = 6 giving 1200 and 2352 states. These environments are excellent for demonstrating the properties of learned intrinsic rewards and I am glad the authors have done these experiments and analyzed the results. However, given that the paper aims to scale the optimal reward problem, it would have been useful to see examples where the state space cannot be fully enumerated to evaluate scalability.\n\nQuestions:\n\nIn Figure 5, in episode 1, why is the learned intrinsic reward heavily penalizing the path to C, but not penalizing the path to B? In the initial episode, the intrinsic reward should only know that B is to be avoided; it doesn\u2019t yet know whether A or C is the better object.  I would expect the learned intrinsic reward to put similar positive rewards on the path to C and the path to A, and negative reward on the path to B. (It is slightly more likely that C is the best object. This probably changes things slightly, but not significantly.)\n\nAlso in Figure 5, by episode 3, shouldn\u2019t the final states (A or C) have intrinsic rewards of larger magnitude? Otherwise the agent can go back and forth on the path to collect lots of intrinsic reward without terminating the episode, even though this wouldn\u2019t get extrinsic reward."}