{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "I think paper needs significant amount of rewriting and reorganization. It took me quite a lot of time to understand the claims and explanations.  Furthermore, paper is not well connected to existing art. It is very well possible that I am missing some points or I didn't understand some sections. Authors needs to work on their presentation.\n\nI will review this paper with respect to my understing and I expect from authors to clarify points during rebuttal.\n\nThe authors use untrained randomly initialized convolutional encoder to understand/analyse deep representations. They provide experimental results by changing several hyperparameters e.g. size of kernel. The most important claim is untrained randomly initialized convolutional encoder can contain more useful information for reconstruction purpose than the pretrained version. They also give theoretical justification to some of their findings. \n\nThe results are surprising and at the same time they are expected. It is surprising because if results carry out to generic tasks then the computational requirement for training of convolutional autoencoders will decrease. However, I believe it is also expected because random projections can keep significant amount of information e.g. Johnson\u2013Lindenstrauss lemma. Hence, a proper connection with random matric theory/random linear algebra is needed to access the novelty of the claims.\n\nAuthors support their claims with empirical evidence. However, they compare random initialized convolutional encoder with a pretrained auto encoder which is designed for classification tasks. Loosely speaking autoencoders can be considered as compression methods however they may require much more information than a classification task. Hence, I am not so sure whether comparing a randomly initialized autoencoder with a pretrained version is fair. I suggest authors to compare against some self-supervised method e.g. jigsaw methods. \n\nI have checked the theoretical justification. Although, everything seems correct, I didn't find the theorem 1 and theorem 2 interesting. I am quite sure some these theorems are known. Especially theorem 1 seems similar to the theorems which are connecting GPs with neural networks. I will be delighted to update my review if otherwise is shown.\n\nI have given a weak reject to the paper. if authors supply more information on theoretical justification, more experiments and rewrite the paper then I am open to revise my judgment."}