{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the problem of constructing a sentence embedding using a generative transformer model which encodes semantic aspects and language-specific aspect separately. They use transformers to encode and decode sentence embedding, and the objective reconstructs input with a latent variables (language variables for each language and semantic language).  These latent variables are sampled from multivariate Gaussian prior, and the learning uses evidence lower bound (ELBO) for variational approximation of the joint distribution of latent variables and input. \n\nThe method is evaluated on two tasks: sentence similarity task and machine translation evaluation metric tasks. Both tasks evaluates how similar are two sequences, and the metric is correlation score with score\u2019s from human judge. The model shows promising results on the first task, but weaker results on the second task, especially when compared against pretty naively built sentence embedding from BERT model. I\u2019m not expert in sentence embedding literature, so a bit tricky to evaluate, but baselines seem strong and experimental results on semantic textual similarity task.\n\nIn terms of evaluation, I appreciated how they defined harder subset of the evaluation dataset and showed a larger improvements on those portions of the dataset. The The paper also includes analysis on what is captured by their language-specific latent vector and semantic latent vector. While I\u2019m not totally convinced this distinction between language-specific characteristics and semantics of the sentence, it makes it easier to understand what\u2019s going on in the model. \n\nOne of my question is, why not test this method in more popular benchmark such as MNLI or other classification tasks? MNLI evaluates how each sentence pair relates to one another, thus would be a good benchmark for sentence embeddings as well. Having to encode all the information about a sentence into a single vector will make these sentence embedding model weaker than other models which can do cross sentence attentions and etc, but I think that\u2019s the genuine limitation of sentence embedding research and has to be clarified as such. I recommend discussing and clarifying these points. \n\nI\u2019m a bit unclear how these sentence embeddings are translated into a score that decides the degree to which sentences have the same meaning. Is it just cosine similarity of two sentence embedding vectors?\n\nWhile the purpose of these references is to generate sentences instead of building a sentence embedding, the method is related and comparison and discussion would be worthwhile. \n\nGenerating Sentences from a Continuous Space\nSamuel R. Bowman,\u00a0Luke Vilnis,\u00a0Oriol Vinyals,\u00a0Andrew M. Dai,\u00a0Rafal Jozefowicz,\u00a0Samy Bengio\nhttps://arxiv.org/abs/1511.06349\nToward Controlled Generation of Text\nZhiting Hu,\u00a0Zichao Yang,\u00a0Xiaodan Liang,\u00a0Ruslan Salakhutdinov,\u00a0Eric P. Xing\nhttps://arxiv.org/abs/1703.00955\n\n\nComments & Questions:\n- Methods using a large amount of unsupervised monolingual data shows very strong performance in a panoply of NLP tasks these days. If I understand correctly, this model is constrained by the amount of bitext \u2014 some analysis on this would be interesting. \n- Figure 1 mentions about \u201cSection 3, 4\u201d but I don\u2019t think they are correct references? \n- BERT baseline seemed not to allow fine-tuning of the LM parameters. I think this makes the baseline significantly weaker? \n- It seems odd that only English semantic encoder is used to downstream application.\n- Does table 3 covers all the data? What proportion of the data is covered by each row?\n- Given the similarity of English and French, I\u2019m not sure how \u201clanguage-specific\u201d such latent vectors are. It would be much more interesting analysis if it studies distant language pairs. "}