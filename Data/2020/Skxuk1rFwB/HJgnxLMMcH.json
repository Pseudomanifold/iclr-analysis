{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new variation on certified adversarial training method that builds on two prior works IBP and CROWN. They showed the method outperformed all previous linear relaxation and bound propagation based certified defenses. \n\nPros:\n1. The empirical results are strong. The method achieved SOTA.\n\nCons:\n1. Novelty seems small. It is a straightforward combination of prior works, by adding two bounds together.\n2. Adds a new hyperparameter for tuning.\n3. Lack of any theoretical insights/motivation for the proposed method. Why would we want to combine the two lower bounds? The reason given in the paper is not very convincing:\n\n\"IBP has better learning power at larger epsilon and can achieve much smaller verified error.\nHowever, it can be hard to tune due to its very imprecise bound at the beginning of training; on the\nother hand, linear relaxation based methods give tighter lower bounds which stabilize training, but it\nover-regularizes the network and forbids us to achieve good accuracy.\"\n\nMy questions with regards to this:\n(i) Why does loose bound result in unstable training? Tighter bound stabilize training?\n(ii) If we're concerned with using a tighter bound could result in over-regularization, then why not just combine the natural loss with the tight bound, as natural loss can be seen as the loosest bound. Is IBP crucial? and why?\n"}