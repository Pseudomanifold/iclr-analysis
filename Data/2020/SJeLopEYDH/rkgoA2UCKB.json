{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents 4D convolutional neural networks for video-level representations. To learn long-range evolution of spatio-temporal representation of videos, the authors proposed V4D convolution layer. Benchmark on several video classification dataset shows improvement.\n\n1. In section 3.1, the authors selected a snippet from each section, but this was not rigorously defined. Same for action units. It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.\n\n2. In section 3.2, the authors argued that 3D kernel suffers from trade-off between receptive field and cost of computation. At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting. 3D convolution is already expensive and not scalable, but 4D operation sounds even more expensive and more prohibitive.\n\n3. In the paper, the authors argued that clip-level feature learning is limited as it is hard to learn long-range spatio-temporal dependency. It makes sense, and I expect the proposed model may benefit from its design for long-range spatio-temporal feature learning. However, what I see in the experiments is on ~300 frames for Mini-Kinetics and 36-72 frames for Something-Something dataset. Assuming that a second is represented with 15-30 frames, this corresponds to 10-20 sec and 1-4 sec, respectively. I'd say these short videos are still clips.\n\nThe paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR."}