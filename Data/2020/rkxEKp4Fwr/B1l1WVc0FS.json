{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Review Summary\n--------------\nOverall, I'm not quite convinced this method would be worth the trouble to implement. On the more realistic benchmarks, they need to keep ~80% of the total dataset size and the claimed \"improvement\" is rather small (less than 0.6% absolute gain in accuracy, e.g. from 81.86% to 82.37% on CIFAR100 and from 72.33% to 72.78% on ImageNet). There is no runtime comparison, there are missing baselines, and most of the method development seems guided by trying out many options instead of taking a principled approach. Without these, the paper is just not ready for a top conference like ICLR.\n\nPaper Summary\n-------------\nThe paper considers a new take on active learning for image classification: given a large fully labeled dataset, identify a subset of the data that, when training on that subset alone, yields similar performance as training on the (much larger) full dataset. The paper focuses on \"ensembles\" of deep neural network classifiers as the prediction model, following Lakshminarayanan et al. (2017). \n\nThe presented method is summarized in Algorithm 1. Given a suitably initialized \"acquisition model\", the model makes predictions on each example in the full dataset, then ranks examples using an acquisition function to find the subset of size N_s (top N_s examples by rank) where there is most \"disagreement\" among the model ensemble. This subset is then used to train a \"subset\" model (again, an ensemble of DNNs).\n\nExperiments consider several possible initializations, acquisition functions, and ensemble sizes. Evaluation is done using the validation sets of three prominent image classification benchmarks: CIFAR10, CIFAR100, and ImageNet (1000 classes). \n\n\nSignificance\n------------\nI don't think a successful case has been made that the proposed solution would generate significant widespread interest, because the gains demonstrated here are too minimal. Looking at the primary results in Table 2, it's really only when using 80% of the total images of imagenet or cifar100 (the most realistic benchmarks) that there is a small (<1%) absolute gain in accuracy over the simpler approach of just using the full dataset. Thus, the approach is not going to significantly reduce computational burden but adds a lot of complexity.\n\nNovelty\n-----------\nThe method seems new to me.\n\n\nExperimental Concerns\n---------------------\n## E1: Need to consider runtime in evaluation\n\nNone of the figures/tables that I can see report elapsed runtimes for the different methods. To me this is the fundamental tradeoff: not how many fewer examples can I learn from, but how much faster is the method than the \"standard\" of using the full dataset? Showing curves of validation progress over wallclock time would be a better way to present results. \n\nThe important thing here is that even *full dataset* makes progress after each minibatch. You'd need to show progress at checkpoints for each epoch in  0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, .... \n\n## E2: Potential missing baseline: Random subset with balanced classes\n\nWhen you select a random subset, are you ensuring class balance? If not, that seems like a more refined baseline. Perhaps won't make too much difference for cifar10, but could be important for ensuring rarer classes in ImageNet are represented well. \n\n\nPresentation Concerns\n---------------------\n\n## P1: Initialization description confusing\n\nI didn't understand the \"build up\" method as described in Sec. 3. How large is the subset used at each phase? How do you know when to stop? This could use a rewrite to improve clarity.\n\n## P2: Missing some details for reproducibility\n\nHow was convergence assessed for all models? How were learning rates set? Many of these are crucial to understanding the runtime required for different models. (Sorry if these are in the appendix, but some short summary is needed in the main paper)\n\n## P3: Title Change Recommended\n\nI don't think the presented method is really doing a \"Distribution Search\"... I would suggest \"Training Data Subset Selection with Ensemble Active Learning\"\n\nMinor Method Concerns\n---------------------\n\n## M1: What about regression?\n\nAcquisition functions seem specialized to classification. What to do for regression or structure learning? Any general principles to recommend?\n"}