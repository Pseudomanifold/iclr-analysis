{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents the idea to use blurred images as regularizing examples to improve out-of-distribution (OOD) detection performance based on Random Network Distillation (RND). The paper proposes to generate sets of such blurred images via Singular Value Decomposition (SVD) on the training images by pruning the lowest K non-zero singular values. The proposed method, SVD-RND, then extends the standard RND objective, which is to train a predictor network f to minimize the L2 loss to the output of some randomly initialized network over the (original) training data, with an additional regularization term that minimizes the L2 loss to the outputs of further multiple randomly initialized networks over the sets of blurred images. In OOD experiments on CIFAR-10, SVHN, TinyImageNet, LSUN, and CelebA, the proposed SVD-RND consistently outperforms baselines and recent competitors which are demonstrated to be vulnerable to blurred images.\n\nI find it hard to make a definitive evaluation for this work at this point and would like to take the authors' responses into account for my final recommendation. The main idea of the paper to generate adversarial examples for training via blurring is rather simple and thus the novelty of this work is somewhat minor. I think the quality of the paper also suffers from many statements in the text that draw too general and too bold conclusions at this point in my mind. The presentation overall is rather unpolished (see comments below). However, I find the empirical results itself quite strong and convincing and think they would make a relevant and significant contribution to the community. I do have questions left open though that need clarification:\n\n(i) I don\u2019t see why different techniques for blurring (SVD, DCT, GB) should lead to such different results as the approach remains conceptually similar. Do you have a reason/intuition why SVD gives the best results? Might SVD just be the easiest to tune method?\n\n(ii) What do you think is the key reason that SVD-RND also appears to generalize too non-blurry OOD samples? Could you elaborate more on the two reasons you give in the paper? (1. RND performance on samples orthogonal to the data; 2. Discrimination between data and its low-rank projection)\n\n(iii) The generation and tuning of multiple sets of blurred images (how many samples per set?) may get quite extensive for large datasets. Could you be specific on the computational cost?\n\n(iv) Might the deep generative models (e.g. GPND) fail to detect blurred images due to insufficient model capacity of the decoder which results in blurry reconstructions? Have you varied the network capacity or latent space dimensionality of such models?\n\n(v) What is the idea behind choosing the log effective rank in such an equidistant manner as proposed?\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. SVD-RND shows strong OOD detection performance in OOD experiments on a variety of image dataset combinations (CIFAR-10, SVHN, TinyImageNet, LSUN, and CelebA).\n2. The related work includes major works from all the related lines of research (Deep anomaly detection; OOD detection using side information; Adversarial examples/training)\n3. Useful hyperparameter selection criterion based on effective rank if no OOD validation data is available.\n\n*Ideas for Improvement* \n4. I think the tone of the paper would greatly benefit from not drawing too general conclusions and too bold implications. Keep statements precise and evidence-based. Declare hypotheses as such.\n5. I would appreciate a plot showing samples before and after blurring in the appendix to see the most effective degree of blurring. Maybe also compare the different blurring baselines here to see differences.\n6. Report std. devs. with your performance results to infer statistical significance.\n7. Compare to the specific geometric transforms method as proposed in the paper [3] and not only using those transformations within your RND approach.\n8. Add missing deep anomaly detection related work [6, 2, 5, 1].\n9. Expand the sensitivity analysis in Figure 3 (left) over a greater range of K. Especially, I would like to also see K = 0 (unblurred, original images) as a sanity check which may only improve over RND due to ensembling over multiple randomly initialized networks.\n10. Consider the one-vs-rest anomaly detection evaluation benchmarks from Ruff et al. [4] or Golan and El-Yaniv [3] to further infer the generalization performance of the proposed method.\n11. I think the paper spends too much time on introducing previous work. Section 2 Related Work and Section 3 Background might be combined into one section.\n\n*Minor comments*\n12. In the abstract: \u201c... VAE or RND are known to assign lower uncertainty to the OOD data than the target distribution.\u201d is a bit strong. Rather \u201chave been observed\u201d etc. This is a working hypothesis in the community, but there is recent work (https://openreview.net/forum?id=Skg7VAEKDS) indicating (at least for VAEs) those are effects of poor model design.\n13. In the abstract: \u201c... efficient in test time ...\u201d \u00bb \u201c... efficient at test time ...\u201d\n14. In the abstract: \u201c... in CelebA domain.\u201d \u00bb \u201c... on the CelebA dataset.\u201d or just \u201c... on CelebA.\u201d\n15. Section 1: \u201cHowever, such models show underwhelming performance on detecting OOD, such as detecting SVHN from CIFAR-10. Specifically, generative models assign a higher likelihood to the OOD data than the training data.\u201d. I think those are way too general conclusions at the moment. Rather something like \u201cOOD detection performance of deep generative models has been called into question\u201d and \u201chave been observed to assign ...\u201d.\n16. Section 1: \u201cSuch results clearly support the degeneracy of deep OOD detection schemes\u201d. Again, I find this way too bold of a statement at this point in time.\n17. Section 2: \u201c... a recently proposed paper ...\u201d \u00bb \u201c... a recent paper ...\u201d\n18. Section 2: \u201c... outlier data independent of OOD data.\u201d. What would outlier data not being out-of- distribution be?\n19. Section 2: \u201cGolan et al. (2018) design geometrically transformed data and regularized the classifier ...\u201d. Not regularized. They trained a classifier on labels identified with these transformations.\n20. Section 2: \u201c..., resulting in OOD detection in each labeled data\u201d \u00bb \u201c..., resulting in OOD detection on labeled data\u201d\n21. A subsection title directly following a section title is bad style. A major section should be introduced with a few sentences on what this section is about.\n22. Figure 2, right plot: These loss curves are rather strange... Increasing, then sharply decreasing again. Is there a drop in learning rate at epoch 80?\n23. Many axis labels are too small and hard to read.\n24. In Section 3.3.: \u201c.. in Section 7.2.\u201d \u00bb \u201c.. in Section 4.2.\u201d?\n25. The definition of the log effective rank in Eq.~2 is weird. It's the entropy over the singular value distribution, i.e.~$H\\left(\\sigma_1 / \\sum_j \\sigma_j, \\ldots, \\sigma_N / \\sum_j \\sigma_j \\right)$. Also, the parameters/notation involved are not introduced.\n26. Make clear you apply SVD on single images. The paper alters formulations between data matrix and images...\n27. Several instances where citet is used instead of citep.\n28. In Table 1: Separate the target column from the three OOD columns more clearly. (e.g. vertical separator, center OOD, target in bold, etc.)\n29. In Section 5.1: \u201carea of the region under the ... curve\u201d \u00bb \u201carea under the ... curve\u201d.\n30. Figure 3: Better explain the plots. Are the three curves the respective target classes? What is the OOD set?\n31. Figure 4: Maybe use subfigures with individual titles/captions.\n32. Section 6: \u201cFor evidence, we fine-tune the classifier ...\u201d \u00bb \u201cFor evidence, we fine-tune a classifier ...\u201d\n33. Plots and Figures are somewhat scattered and not referenced chronologically.\n34. Introduce the effective rank at the point where it is used (Section 6.2). Somewhat unclear why to introduce this in Section 3 already.\n35. Visually speaking, I find the RND examples in Figure 4 actually more anomalous than the top-anomalous SVD-RND samples (flashy colors, weird angles, borders, high contrast, ...)\n\n\n####################\n*References*\n[1] R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\n[2] H. Choi, E. Jang, and A. A. Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n[3] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In NIPS, 2018.\n[4] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Mu\u0308ller, and M. Kloft. Deep one-class classification. In International Conference on Machine Learning, pages 4393\u20134402, 2018.\n[5] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, A. Binder, E. Mu\u0308ller, K.-R. Mu\u0308ller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019.\n[6] T. Schlegl, P. Seebo\u0308ck, S. M. Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In Proceedings International Conference on Information Processing in Medical Imaging, pages 146\u2013157. Springer, 2017.\n"}