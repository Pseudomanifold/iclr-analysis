{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary of the paper: \n\nThe paper proposes an extension of Normalizing flows to conditional distributions. The paper is well written overall and easy to follow. Basically the conditional prior z|x = z=f_{\\phi}(y,x), where x is the conditioning random variable, and we apply the change of variable formula to get the density of y|x . For example in super resolution y is the high res image and  x is the low res. image.  To sample from the models authors propose to use f^{-1}_{\\phi}(z;x). \n\nThe conditional modules are natural extensions of invertible blocks used in the literature (coupling layers, split priors, conditional coupling, 1x1 conv), where the conditioning is done on some hidden representations of the conditioning variable x (i.e one or multiple layers of NN).\n\nAuthors propose a dequantization for binary random variables (useful for segmentation applications), where they give an implicit model for the dequantizer (obtain a continuous variable from a discrete binary variable).\n\nAuthor apply the method in two applications super-resolution and vessel segmentation. the method is compared to supervised learning of the corresponds between x and y and to others competitive methods in the literature and shows some advantage. \n\nMinor comments : \n\n- Formatting the bibliography is messed up and needs some cleaning , Figure 5 is also making formatting issues of the paper. \n- Figure 1 for sampling it should be f^-1_{\\phi } and not f_{\\phi}\n\nReview: \n\n- Figure 2 is hard to get any idea of the sample quality would be good also to put the low resolution input to the algorithm . Also did you use a temperature sampling for the baseline ? otherwise the comparison is not fair.\n\n- The Drive database is too small 20 training samples and 20 testing only? can the model be just overfitting?\n\n- In the vessel implementation why do you drop the scaling modules? \n\n- The conditioning for the vessel implementation on x is on two layers , would be great to put all architectures of the models in details , and to show both sampling and training paths \n\n- It would be great to add the details of the skip connection used from the network processing x, and how ensure that the flow remains invertible.  \n\nOverall this is a well written paper and a good addition to normalizing flows methods , some discussion of related works on conditional normalizing flows and more baselines with other competitive methods based on GANs for example would be helpful but not necessary. \n\nIt would be great to add details of the architectures and on skip connections and how to ensure invertibility for this part in the model . \n"}