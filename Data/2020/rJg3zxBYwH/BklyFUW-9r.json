{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presented the conditional normalizing flows (CNFs) as a new kind of likelihood-based learning objective.  There are two keys in CNFs. One is the parametric mapping function f_{\\phi} and the other is the conditional prior. This paper assumed the conditional prior as Gaussian distribution of x. The mapping function is invertible with x as a parameter. The prior parameter and \\phi are updated by stochastic gradient descent. The latent variable z is then sampled from conditional prior. The output targe y is obtained with dependency on x and f_{\\phi}. \n\nStrength:\n1. This study adopted the flow-based model to estimate the conditional flow without using any generative model or adversarial method.\n2. This method obtained the advanced results on DRIU dataset without the requirement of pretraining.\n3. This paper proposed an useful solution to train continuous CNFs for binary problems.\n\nWeakness:\n1. It is required to address how to design the function f_{\\phi} which depends on x. In particular, the property invertibility should be clarified.\n2. Why the issues of mode collapse or training instability in flow are considerable in the experiments?\n3. It will be meaningful to evaluate this method by performing the tasks on text to image or label to image."}