{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary: This paper proposes to use the gradients of specific layers of convolutional networks as features in a linearized model for transfer learning and fast adaptation. The method is theoretically backed by an appeal to the recently proposed neural tangent kernel and seems like it could be practically useful.\n\nI tend to weakly reject this paper currently despite liking the simplicitly and timeliness of the approach. Specifically, I would like further discussion of the choice of the layers to use as gradient features and an ablation study on supervised trained networks.\n\nOriginality: I believe that this is one of the first papers to explicitly use the Taylor approximation (neural tangent kernel) in a transfer learning setting, making the approach timely and potentially practically useful. \n\nSignificance: The approach is a quite nice merging of theoretical insights with a neat practical implementation. Although the main methodological advance is a straightforward application of the thinking behind Jacobian vector products, the method is well described and ought to be practically useful. However, I have a bit of a concern as to its practical necessity in comparison to simple fine-tuning of the final softmax layer (referred to in Table 1 as \\theta_2) on a new dataset. \n\nClarity: While the approach described in Section 3 is quite generic \u2013 theoretically, the method should simply consist of training the final layer of the neural network (w_1) and weights from the Jacobian features around the Taylor expansion. However, the experimental approaches suggest that different layers were used in each experiment \u2013 see e.g. \u201c[we] \u2026 compute our gradient based features from one, two, or all of the top-3 conv layers\u201d (Section 4.2 for the BiGAN architectures) versus \u201cour gradient based features are from the last 2 conv layers\u201d (Table 2 for the AlexNet architectures). Why was the entire network (modulo the last feed forward layer) not simply used for the Jacobian features? \n\nSimilarly, why was the entirety of the model (again modulo the last feedforward layer) not used for forwards model in the AlexNet experiments?\n\nBased on the ablation study in Section 4.1, the authors find that \u201cit suffices to set the very top layer as \\theta_2 to enjoy a reasonably large performance gain.\u201d When this is the case, it is a bit tough to distinguish the approach from standard final layer transferring approaches (Sharif Razavian et al, 2014) and indeed Bayesian final layer approaches that simply retrain an output layer (e.g. Perrone et al, 2018). Indeed this seems to be the case in Table 1, if that is so, then why not just re-train \\theta_2 as well throughout the experiments as it will typically just be another stochastic convex optimization problem?\n\nCould the authors quickly describe an implementation of the scalable Jacobian inner products in Section 3.3? It seems like outputs will have to be cached during the forwards passes, thereby requiring a somewhat significant amount of software engineering (and memory overhead) to be have to do this process for each new layer. Is this the correct understanding of how one would implement the procedure? A quick paste of PyTorch pseudo-code would be sufficient here.\n\nQuality: The experiments seem to be relatively convincing \u2013 it seems exciting that a linear model + the network\u2019s features itself can typically perform as well as fine-tuning and occasionally even better than fine-tuning itself. \n\nHowever, I am a bit concerned by the fact that the ablation studies themselves only utilize models trained in an un-supervised fashion. \nI\u2019d instead like the authors to run an ablation study on supervised trained models (perhaps 8 layer conv nets or VGG16) in the same manner as in Table 1 and Section 4.1. Specifically, I\u2019d like to see this done to see whether the gradients in fact are as interesting as features when they have been trained in a supervised fashion. \n\nSimilarly, I\u2019d like to see the features themselves used as a linear classifier (no network forwards passed) in the same two ablation studies. That is, could the authors use w_1^T J w_2 as the features for their linear classifier. If they have already done so and I\u2019ve missed that in the tables somehow, I apologize. This experiment should help to test out how _useful_ the features defined by the Jacobian matrix are in comparison to the network\u2019s forward pass itself.\n\nMinor Comments: \n\nIntroduction: \u201cAfter learning, the the (sic) activation of the deep network are considered as generic features.\u201d Not only is there a small typo, but you should either include a citation here or be more specific as to what \u201cthe activation\u201d of the deep network is here.\n\nIntroduction: \u201cAnd the accuracy of the rthe (sic) \u2026\u201d typo + please do not start sentences with and if at all possible.\n\nSection 3.1 (beneath eq. 2): \u201cliner\u201d should be linear.\n\nTable 3: \u201cSelf-supervise\u201d should be \u201cSelf-supervised.\u201d\n\nReferences: \n\nPerrone et al, Scalable Hyperparameter Transfer Learning, NeurIPS, 2018. http://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf\n\nSharif Razavian et al, CNN Features off-the-Shelf: an Astounding Baseline for Image Recognition, CVPRW, 2014. https://arxiv.org/abs/1403.6382\n\n"}