{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work represents some new effort to stabilize & improve the training of Wasserstein-based generative networks. To this end, the author(s) have proposed to combine both the primal and dual formulation of Wasserstein distance. By \"clipping\" the dual estimate wrt the primal estimate, or simply an arbitrarily set upper bound,  the author(s) hope to overcome the volatile training of WGANs which might be the consequence of an unbounded dual. Despite the fact that the overall idea is new and the results supported the claim, I am not convinced of its significance. It's an okay paper, but I am expecting more from an ICLR paper. As such, I am willing to endorse this submission. A more detailed review can be found below to help the author(s) improve their work. \n\nStrength. \n+ I like the idea of \"bounding\" the dual W-distance estimate, although, in theory, the observed degeneracy should not happen if the network is always properly regularized. \n+ The overall writing is clear and well-motivated. \n\nWeakness. \n- The key observation that motivates this study is that W-distance often rises rapidly during the early training phase. This can be addressed by alternative tricks, say hot start the discriminator by pretraining it to be close to Lipschitz-1, or clipping the discriminator gradient to avoid drastic changes to the function. An ablation study is justified to pin down the real cause of this gain. \n- It is not clear from the paper whether the  []_+ op back-propagates gradient or not, which will make a big difference here. If it backpropagates gradients, then when the dual estimate is larger than the threshold, then the computation is wasted (zero gradients). If not so, then the proposed solution does not affect gradient at all. \n- It's worth noticing that vanilla Sinkhorn distance does not work well for generative purposes (see recent MMD-GAN and Sinkhorn-GAN papers, which all computes Sinkhorn distance in the feature space rather than data space). This questions the necessity of using the Sinkhorn estimate and why it works here. \n"}