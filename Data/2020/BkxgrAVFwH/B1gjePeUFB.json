{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper-bound the objective of WGAN's critic's loss during the training. GAN stabilization is a well-motivated problem and limiting the dramatic changes of discriminator loss clearly helps achieving this goal. Experiments show that in few settings the proposed method successfully addresses this issue. However, the theoretical insights have multiple flaws and incorrect proofs, while the choice of experiments raises few questions. Finally, the resolution limitations suggest the overall contribution is rather incremental.\n\nThe paper thus requires further work in terms of design, theory and experiments; for instance the choice of Sinkhorn distance as a boundary-heuristic might be too limiting. Although the main bounding strategy seems to be a promising idea, this work does not meet the quality requirements of ICLR. \n\nPros:\nA. Conceptual simplicity of the general method of bounding discriminator's objective.\nB. The method works well for a few variants of Wasserstein GAN on medium resolutions (64x64, 128x128).\n\nIssues:\n1. Resolution limitation. The upper-bound cannot be reasonably computed for higher resolutions in mini-batch training.\n2. It is unclear how costly the method is in terms of resources and training time.\n3. Experiments were done with CelebA/CelebA-hq with DCGAN and BigGAN architectures at 64x64 resolution and BigGAN architecture for 128x128 resolution. However, they lack comparison with vanilla BigGAN (trained with hinge loss) and the explanation of the architectual differences. Authors only mention the lack of self-attention, yet the used architecture has much less parameters than the original BigGAN; it is also unclear how BigGAN for 64x64 resolution should look like. In fact, the choice of BigGAN for 64x64 resolution is questionable, as this model has been designed for datasets of much higher complexity. \n4. In two out of three experiments, WBGAN-GP does not beat WGAN-GP. Given BigGAN's results at 64x64, it might be the case that bounding strategy just helps in the case of heavily-overparametrized model.\n5. Definiton (8) is unclear: firstly, the objective L_theta is parametrized by parameters of discriminator (theta), over which the maximum on the rhs is taken (this follows in Appendices). Secondly, it has non-deterministic term d^lambda(..) which depends on random empirical distributions \\hat(P_r), \\hat(P_g) (in Proposition 8. which follows, authors take expectation over a Wasserstein distance between these distributions).\n6. Remark 2's proof is incorrect. At the beginning of p.13 authors take D_theta(x) = sign(P_r(x) - P_g(x)), which is not a Lipschitz function. Also P_r and P_g may not be discrete so the entire expression does not make sense.\n7. Remark 2 is not true non-deterministic formulation of (8). It is possible that P_g != P_r and \\hat(P_g) = \\hat(P_r); in such case sinkhorn distance is 0 and the remaining terms of rhs of (8) cancell out (e.g. P_r is a Dirac delta at 0 and P_g = Bernoulli(0.1), then it is possible that \\hat(P_r) = \\hat(P_g) = delta(0)). Perhaps re-shaping definition (8) so that it is deterministic would help achieving correctness of Remark 2.\n8. Remark 3. is unclear: in what sense expression (8) can be optimized by gradient descent?\n9. The paper seems rougly written (see below) and the text needs some polishing.\n\nTypos/ requires clarification:\n[p2.] Backgrounds -> background\n[p.3] corresponds a \\lambda -> corresponds to \\lambda\n[p.3] 'The proposed bounded strategy' -> bounding\n[p.3] 'We name it general in two folds' (?)\n[p.7] 'High-resolution' - 128x128 is not high.\n[p.8] Discussions -> Discussion"}