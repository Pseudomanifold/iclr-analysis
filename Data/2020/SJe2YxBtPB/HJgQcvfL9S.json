{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a technique NFSIP to augment state-of-the-art Neural Fictitious Self Play (NFSP) with self-imitation learning, and show that this approach can achieve performance improvement compared to other Multi-agent RL baselines on 3 different problem domains in literature. \n\nThe key idea of this paper is applying the self-imitation loop at the end of each episode s.t. the agent will not forget learning from good experience. The idea is simple, but the experimental results show that for the tasks (e.g. V2) that require collaborations from multiple agents, the proposed approach outperforms other baselines including NFSP on both firefighting and search & rescue domains. \n\nQuestions:\n1. it seems that for simpler task V1, the result of NFSIP are very close to NFSP in Fig 3, 4. How to explain this phenomenon?\n2. why there are some baselines not having the same number of iterations in Fig 3, 4? e.g. COMA_SIL in Firefighting v2 seem only have results until 70k iterations. Also, for the task Box Pushing, NFSIP doesn't seem to have advantages?\n\n"}