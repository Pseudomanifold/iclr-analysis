{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method called Extractor-Attention Network (EAN) for Chinese text classification, which uses both word-level features and spelling-level features (pinyin to be more exact). The two types of input features are fed into multi-headed attention and/or CNN-based Extractor blocks for further feature extraction, before finally being passed to prediction layers for computing the output. The main motivation of the paper is to leverage both the pronunciation and the form of Chinese text, instead of most previous approaches which only pay attention to word-level or sub-word level features while being unable to handle common polyphony and homophones which occur quite often in certain domains. The paper also demonstrates the capabilities of the proposed EAN model on 5 different datasets, consistently surpassing all compared baseline models. Congratulations on that.\n\nHowever, upon closer inspection, there are possible details that require further clarifications and proofs:\n\n1. Most noticeably, the compared baseline models are not cited clearly, the citation used in the results table doesn't match the citation style of the rest of the paper, and I find it unintuitive to verify the included numbers. When I finally found the paper, I realised that all the compared models' numbers are from 2015, far from the current SoTA (e.g. Sun et al., WASSA'18; Meng et al., NIPS'19). The evaluated datasets also only contain 2-7 classes and lack domain diversity. Whether the EAN's demonstrated improvement over TAN is significant is also unclear.\n\n2. The exact role the proposed Extractor plays is unclear, as I am not sure whether the improvements come mainly from the Extractor/Multi-headed Attention(TAN) or the main motivation: the incorporation of both word-level features and spelling features. The complex construction of the Extractor although shown limited improvements over multi-headed attention blocks (TAN in the paper, which is hardly explained), is not justified. It would be much better if the authors could explain why they decided to design it like this instead of some other way.\n\nDue to these reasons, I strongly believe a more careful literature review, detailed explanation on the Extractor's design, and more comprehensive study on how spelling-features can actually help Chinese text classification, are absolutely instrumental for publication.\n\nMinor comments:\n\n1. Table 5 which compares the parameter size of different models doesn't make any sense, since the compared models' performances are not even mentioned in evaluation.\n\n2. There are numerous citation styling errors and grammar errors in the paper.\n\n3. Equation #1-6 are numbered incorrectly. Notation of H s in Equation 8-9 and A,X in Equation 11 are all incorrect.\n\n4. How were the hyperparameters chosen?\n\n5. Why so few dimensions of word embeddings but so much for spelling characters?\n\n6. Page 6 sec 5 '...only list the best results of their variations with different hyperparameters.' This doesn't look very nice."}