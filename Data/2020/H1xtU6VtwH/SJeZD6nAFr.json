{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed an attention networks named Extractor-attention network where both word and pinyin are considered for Chinese representation. Improved results are showed on several dataset for text classification task. Overall, the author trying to attack an important problem and the proposed model can potentially be extended for other east Asian language (Korean or Japanese). The experiment results are encouraging although the novelty is rather mild. Detail comments are as following.\n\n1. How was the model performance in other tasks, rather than classification. \n2. More model training detail need to be revealed? For example, the tricks used for training transformers. Does the proposed model need these similar tricks?\n3. a section about ablation studies will help the reader better understanding each component of the network structure.\n"}