{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution Matching\n\nSummary:\n\nThis paper proposes a modification to the VAE-GAN model where mode coverage is encouraged by passing samples G(Z) through the encoder and minimizing the forward KL between E(G(Z)) and the prior over Z. Results are presented on synthetic MoG datasets, MNIST variants, CIFAR and CelebA.\n\nMy Take:\n\nThis paper\u2019s only point of novelty over a vanilla VAE-GAN implementation is the inclusion of the KL(E(G(Z)) || p) term in the generator loss, which is very similar to the idea behind VEEGAN. The relative novelty over VEEGAN is also limited, the description of the method is exceptionally similar to the description used in the VEEGAN paper (going so far as to copy-paste a figure straight from VEEGAN without attribution), and the comparison to VEEGAN in the related work section is not sufficiently fleshed out. The difference in performance over a vanilla VAE-GAN on CIFAR and CelebA is negligible (6-9% relative reduction in FID on an already weak baseline), so there is no compelling empirical reason to adopt this method. I argue strongly in favor of rejection.\n\nNotes:\n\n-Mode collapse (when many points in z map to an unexpectedly small region in G(z)) is a different phenomenon from mode dropping (when many points in x are not represented in G(z), i.e. no point in z maps to a cluster of x\u2019s, as is the case if e.g. a celebA model generates frowning and neutral faces but no smiling faces). While these phenomena often co-occur (especially during complete training collapse), they are not the same thing, and this paper conflates them throughout. \n\n-For the synthetic dataset examples the comparison against VEEGAN appears to be unfair\u2014it\u2019s one thing to report robustness to hyperparameters, but this seems more like the authors have intentionally picked settings for which VEEGAN happens to fail (by halving the width). If the authors wish to use settings different from the standard ones used by most other papers which test on the MoG datasets they should justify it thoroughly and include this justification in their analysis\u2014do we have a compelling reason to believe that the LDM method is better suited to learning lower capacity models in general?\n\nMinor:\n\nTypos throughout, like \u201cregularized autoencoer.\u201d Please thoroughly proofread your paper for grammar and spelling mistakes.\n\nThere are formatting errors in the PDF, such as at the top of page 8. Please examine your paper for formatting mistakes.\n\n"}