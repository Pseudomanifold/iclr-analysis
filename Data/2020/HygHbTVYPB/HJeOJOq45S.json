{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The manuscript contains typing and grammatical errors. I also think that the presentation is not clear enough. The notation is not clearly defined also. I hope I am not missing it, but the function combination operation is not defined (e.g. f(x) o g(x), to denote f(g(x)), and used a lot. \n\nJust under equation 4, you are saying that comparing empirical distributions is difficult, and hence propose an alternative objective. But you could actually compare the two empirical distributions with a sample based divergence measure such maximum mean discrepancy, or Frechet Inception distance. Why don't you do this? \n\nIn the algorithms section, I don't understand why you don't compare with widely used WGAN-GP model. DCGAN, and VAEGAN are relatively old models. Your only results in Celeb-A are compared against these relatively older models. Also by looking at the provided samples, I think the quality of the results are far away from what is obtained with state of the Generative models currently. \n\nOverall, I think the idea is sound and sensible, but I don't think that results are convincing enough for a conference paper. Furthermore, the writing needs to be improved.  \n\nminor comments: \n\nIn Figure 2, I think you should provide the explanations also in the caption to make the concept easier to follow for the reader. \n\nIn Algorithm 1, I think it would be easier for the reader to follow if you don't use shorthand notations defined in lines 5, 6, 7, 8.\n"}