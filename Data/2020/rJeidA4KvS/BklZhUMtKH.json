{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper takes the idea of Population-Based Augmentation (PBA) and extends it to knowledge distillation (KD). The idea is that the ideal augmentation protocol for training-from-scratch (or in this context teacher training) may not be the best for student networks under a KD loss.\n\nI am borderline about this paper and had to pick one, so I landed on Weak Reject. On one hand, I think it\u2019s a really neat idea to apply PBA in this context, and package it as a strage-alpha/stage-beta training procedure. However, the experimental results seem very incremental and I\u2019m not convinced there is a genuine signal there.\n\nExperiments:\n\nTable 1 offers a great comparison of prior work, as well as the combination of prior work (II-KD). Table 2 tells me that PBA seems incremental both for the teacher and the student. Going from vanilla training to student with II-KD gets you most of the way there, and the primary contribution of this paper just gives you a slight benefit above this. It\u2019s great that a more traditional full-precision comparison was also added in Table 4, but this table also confuses me. First, it was unclear if \u201cvanilla training\u201d referred to the teacher or the student. The number 74.31 is not the same as in Table 2 (74.85), so I assume this means it\u2019s the student? If so, the student is already very close to the teacher, and this is not a great starting point for evaluating KD. The student after stage-beta also outperforms the teacher - something that was mentioned in the related work, but I would like more discussion around it specifically for Table 4. Another thing I was wondering was how important PBA is for the teacher\u2019s ability to be a good teacher. It gives a modest boost in Table 2; what if we skip PBA in the teacher but still do it for the student. This would be interesting to add.\n\nOverall, there aren\u2019t that many experiments. The dataset is never more challenging than CIFAR-100. There are also no error bars, which are particularly important when the improvements are small. As for Figure 3, I don\u2019t know if there is anything intuitive we can glean from this. I may just be variation between experiments, as far as I can tell.\n\nI think this paper can be made stronger by making the experimental evidence broader, as well as the analysis of why this works stronger. Without these improvements, the reader is left wondering if there really is any significant benefit. We have to remember that PBA is not cheap (perhaps much cheaper than AutoAugment, but more expensive than fixed augmentation). For most practitioners, the complication and compute costs of PBA would probably not be worth adding on top of KD, if the benefits are too modest.\n\nMinor:\n\nIn Table 3, it says \"Ours\" for AlexNet and \"II-KD\" for ResNet8. Should the both be the same?"}