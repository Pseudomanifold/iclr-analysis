{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors hypothesise that, in a Knowledge Distillation (KD) setting, the student network may benefit from learning from different training data than the teacher. The motivation comes from the fact that human teachers \"adapt\" the examples given their students, depending on their individual expertise on the subject, personal and cultural biases, and other factors.\n\nMore concretely, the paper proposes to use an evolutionary algorithm for data augmentation (PBA, already published) to train the teacher and student networks. The key aspect of the proposed method is that the teacher and the student use different augmentation schedules. The augmentation schedules improve results on both the teacher and student networks.\nOrthogonally to this, they also propose to combine different KD objective functions and show that this also improves the results over each loss in isolation. \n\nThe datasets used in the experiments are CIFAR-10 and CIFAR-100. In most of the experiments the student has a (much) lower bit-width than the teacher, but they also apply the method with a student network with less parameters. Two different network architectures are used in the experiments: AlexNet and Resnet18, their method shows consistent improvements over the baselines in all cases (although some improvements may be considered marginal).\n\nOverall the paper is well motivated, clearly written and, in the experiments section, they give enough details, and/or cite previous works that contain them, to reproduce the experiments.\n\nMost importantly, the experiments are well designed to test the initial hypothesis: \n1. The authors show that the student (and teacher) trained with the PBA data augmentation achieves a higher accuracy than the baseline method (Table 2). However, this is not enough to confirm/refute the hypothesis itself, since it is known that data augmentation generally helps.\n2. They show that the two augmentation strategies are different, by using the teacher\u2019s augmentation to train the student network (Table 3). The results show that it\u2019s better to use a specific data augmentation schedule for the student network.\n\nHowever, (the main criticism is that) the paper only partially confirms the hypothesis. For instance, it is not clear whether the hypothesis is true for other forms of KD, or it only applies when training a student with lower bit-width. In Section 5.6, they train a student with fewer parameters (less layers) than the teacher, instead of fewer bits/parameter. However, the improvements of their method over the baseline (II-KD) seem marginal there. And most importantly, as pointed out earlier, this alone is not enough to confirm the hypothesis. A similar table to Table 3 should be included in this section.\n\nSecondly, the proposed KD loss, which is just a combination of previously published works, is an orthogonal improvement to the main method (as the authors admit), and it is not related at all to the subject of the study. It is obviously good to introduce more than one contribution in a paper, but this second (and minor) contribution should be well motivated in its own, in order to avoid \"distracting\" the reader from the main contribution.\n\nIn addition, I would suggest to perform statistical tests to give additional robustness to the conclusions drawn from the experiments. They perform experiments on different architectures (AlexNet and Resnet18) and different KD losses, and the conclusions are always consistent with the hypothesis, but it\u2019s not clear whether the improvements are statistically significant or not. In this regard, it would also be appreciated to include results with other datasets, since only CIFAR-10 and CIFAR-100 (which are very similar datasets) were used. \n\nBeyond statistical significance, it\u2019s also not clear how important are these results for researchers working outside the scope of Knowledge Distillation. \n\nFinally, some minor comments:\n\n- L_original in Eq. (9) is not defined. I'm assuming it's the L_{KD}^{soft} loss.\n- Please, check consistency of pronouns: authors refer to the teacher network as \u201cit\u201d (e.g. \u201c[...] teacher distills knowledge to a narrow/shallow student to improve its performance\u201d, page 2), but to the student as \u201cshe\u201d (e.g. \u201c[...] to train the student better from her teacher\u201d, page 2).\n- \u201cour methods clearly outperforms [...]\u201d (page 6)  -> \u201cour method clearly outperforms [...]\u201d.\n\nScore: Borderline accept, but I will increase the score if the authors address my concerns and provide better evidence that the hypothesis is confirmed."}