{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new method to distill a teacher model into a student model. They demonstrate improvements over existing distillation variants. The results are impressive for low-precision networks. \n\nHowever, I found a few problems with the paper:\n\nIn the author\u2019s methods, there seem to be multiple steps:\n1. Train the teacher with PBA (stage-alpha).\n2. Train the student with PBA, using a subset of the data and then using the teacher to learn the augmentation policy (stage-beta). This is described in Section 4.2, but I found it incredibly confusing to get a clear picture since there are several moving parts here (augmentation, student solo training, distillation with teacher).\n3. Additionally in Section 5.2 (experiments), the authors propose the combined inter/intra distillation loss, which is named \u2018II-KD\u2019.\n\nIn Table 3, the accuracy of ResNet18 on CIFAR-100 with II-KD is the same as accuracy of Resnet18 Student after stage-beta in Table 2. Same for AlexNet on CIFAR-100. Which of the tables are wrong?\n\nAlso, in section 5.1, the authors mention that they use the pre-trained teacher as a starting point for the student network. This is not a fair comparison with the \u2018Soft Labels\u2019 approach of Hinton et al, where the student network is not initialized from the teacher.\n\nFurther, it is unclear why the augmentation plot in the student in Figure 3(c) differs wildly from the augmentation plot of the teacher. There is no augmentation for the first 50 epochs, and then the probability of all the augmentation operations moves in discrete steps together.\n\nThe novelty in the paper seems to be:\na) Applying PBA to a Distillation setting.\nb) Introducing the \u2018II-KD\u2019 loss.\n\nAs mentioned (a) is not explained clearly. (b) is explained in a generic way but the authors do not give an example of the inter/intra feature map comparisons.\n\nOverall, while I feel the results are impressive, the method is complex as it is presented. I would be reluctant to accept the paper without further clarification from the authors."}