{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors propose a three phase learning schedule to find embedding vectors for sequences. The goal is to have the euclidean distance of embedding vectors mimic the edit distance of input sequences. Given such embedding one can perform faster approximate nearest neighbor search in compare to calculating pairwise edit distances.\nThey use a RNN to output a real value per sequence step. At first they pretrain with the absolute difference of euclidean distance and edit distance. Then they fine tune with a triplet loss, such that the difference in euclidean distances be larger than the difference in edit distances. Finally they modify the embeddings with a stochastic, differentiable algorithm such that they can get guarantees for generalization.\n\nUnfortunately, the manuscript is not well written. There is a high chance of misunderstandings. What I gather from the experiment section is that their model is trained on the whole corpus. During training repeatedly trains with absolute loss on pairwise edit distances. During inference random 100 of those same sequences that has been trained on are selected to compare with the rest. If this is true, I fail to grasp the point of this paper. Since during training you have effectively calculated all the pairwise edit distances. There is no generalization happening. This paper has effectively memorized the edit distances of some sequences. \n\nIt seems that only phase 3 (cgk') is designed to have any accuracy on unseen sequences, and experiments show that it underperforms the original cgk.\n\nIf this is not true and indeed they are training for example on one half of the corpus and the 100 query + base are unseen during training I am willing to increase my score. Given the added clarification in the paper.\n\nAgain assuming that this is not just memorization:\n\nWhy eq 5 (regression loss) is the absolute value? It means that you will never get closer than lr/2 to the optimal point, where as with a least squares loss your gradients get smaller when you get closer.\n\nHow are the negative sample, positive samples selected for an anchor? Are they just two random points, is there any importance sampling happening?\n\nWhy during phase 2, the phase 1 loss is stopped? There is no intuition, justification in the paper. Why the loss is not eq 5 + eq 6 during the whole training?\n\nHow are you optimizing? SGD I assume? How are you selecting hyper-parameters, such as learning rate? Is there any validation set?\n\nIs there a typo in eq 7? The text says \"we calculate the absolute loss as in Phase 1 to optimize our embedding network f3\" but eq 7 is on f'(3). Are you backpropagating through algorithm 3 toward embeddings or just toward thres as in algorithm 4?\n\nCurrently, given the poor quality of the write up, the merit of the idea and the experiments is not clear.\n\nRelated work: LSDE: Levenshtein Space Deep Embedding for Query-by-string Word Spotting"}