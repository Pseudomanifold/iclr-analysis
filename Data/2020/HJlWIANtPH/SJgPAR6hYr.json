{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose an approach for learning embeddings for strings using a three-phase approach. Each phase uses a different neural network to learn embeddings for strings such that the distance between strings in embedding space approximates the edit distance between the strings. A modest set of empirical results suggests that the proposed approach outperforms hand-crafted embeddings.\n\nI found the presentation in this paper very disjointed and difficult to follow. While I believe (my interpretation of) the basic idea of this paper is interesting, I believe the current presentation significantly hinders readers from following the authors\u2019 intentions.\n\nComments\n\nThe description of how the network structure and weights are \u201cinitialized\u201d across the different phases is not clear. Different notation (f_1, f_2, f_3) is used for each network, but in reality, this is just the same network. However, the writing makes this very difficult to notice.\n\nThe authors introduce the CGK and CGK\u2019 embedding algorithms, and then proceed to prove various properties about them. However, it is not clear to me how these theoretical properties are used by the neural network. From what I can tell, CGK\u2019 is an alternative to CGK which reduces the output size relative to CGK (from 3n to at most 2n) while still ensuring exact reconstruction of the input. (I did not verify the proof in detail.) The authors then claim that this is helpful in the current context because it ensures the network parameters can be easily optimized. It is not clear to me what this means. (I guess that somehow using \u201cCGK\u2019 distance\u201d makes training the model easier than using \u201cCGK distance\u201d.) Additionally, the experiments do not verify this claim empirically. So it is unclear whether using \u201cCGK\u2019 distance\u201d helps in the context of learning embeddings.\n\nIt is really unclear to me whether the neural network outputs a continuous or a binary vector. In particular, Equations 5 - 8 all suggest that Hamming loss is defined on the outputs of the various neural networks (f_1, f_2, and f_3). The paper also refers to bits in the output of f_3. Later on, though, the paper mentions that the neural embedded strings are continuous vectors. While this could just be typos or inconsistent notation, considering that other parts of the paper do rely on binary representations, this makes the presentation very confusing.\n\nIt is unclear to me whether the can be (approximately) reconstructed from the embeddings. It seems that Theorem 1 suggests that the binary outputs of CGK\u2019 can be decoded, but I cannot tell whether that extends to the embeddings.\n\nIt is unclear to me how positives and negatives are sampled for training in Phase 2, and also whether that impacts training.\n\nThe experimental results should include some measure of variance based on different train and/or test splits.\n\nIt seems as though the three phases could be rolled into a single multi-task learning problem in which the network is trained during a single phase.\n\nTypos, etc.\n\nThe references are not consistently formatted.\n"}