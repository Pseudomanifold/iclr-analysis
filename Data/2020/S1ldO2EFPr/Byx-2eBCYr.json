{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes that the outputs for distinguish nodes exponentially approach signals that only carry topological information of the graph, when the weights in GCN satisfy conditions determined by spectra of the augmented normalized Laplacian, by generalizing the forward propagation of GCN to a specific dynamical system. With the guidance of this theory, it experimentally confirm weight scaling enhances the predictive performance of GCNs in both synthesized and real data.\n\nOverall, this paper could be a considerable theoretical contribution. I recommend a weak accept for this work. It explains the observation that GCNs do not improve (or sometimes worsen) the performance as we pile up more layers, which is indeed we met in our work. It also points out a useful technique, weights normalization, in training a GCN. As mentioned in the paper, scaling the weights is a trade-off between information loss and generalization error, which is an interesting topic worthing further exploration.\n\nHowever, there are one question needing more clarification. Although the output are being projected into the null space of the Laplacian, it does mean the signals only carry topological information. Distinguish nodes can still have different outputs that allows good classification results."}