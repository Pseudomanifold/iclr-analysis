{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose a novel architecture with sequential attention modules for tabular learning. An attention module is trained to select some elements from the (normalized) input feature, and a feature transformer takes the selected features for overall feature embedding. The model is evaluated on multiple tasks, and the proposed method outperforms prior approaches in the paper.\n\nI am in favor of this paper as it proposes an interpretable method for feature extraction in tabular learning. The learned overall feature embedding is shown to be effective in multiple tasks. It seems to me that the proposed method may have high generalization capability because the method is trained to select sparse feature attributes for decision. The authors may study the generalization performance further.\n\nThe experiment section may not be very reliable. The numbers indicated with * are copied from other papers, but these numbers may be compatible if there is any discrepancy in the experiment setup. It would be better if the authors can run the baselines on the datasets. \n\nI am a little disappointed that no study on the number of steps needed for the model. It seems non-trivial to me. In TabNet-L, the step size is 5 but in TabNet-M, the step size is 7 (even smaller?). How should we choose the right step size?"}