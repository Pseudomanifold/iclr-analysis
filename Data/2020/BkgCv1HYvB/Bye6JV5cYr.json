{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1786", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "<Strengths> \n+ This paper addresses the problem of summarizing interleaved posts, which involves two subtasks of identifying the clusters of threads and summarizing each thread into a sentence. \n+ This is an understudied area of research in summarization research, only few previous works have been done; for example, (Shang et al. 2018). The proposed approach is end-to-end  trainable, whereas (Shang et al. 2018)\u2019s method tackles the problem in two separate stages. \n\n<Weakness>\n1. The technical novelty may need to be further justified.\n- The proposed model (in Fig.2) consists of the encoder, the decode and hierarchical attention module. \n- The encoder and the decoders are standard ones based on (Nallapati et al. 2017) and (Li et al. 2015), respectively. It is hard to find any technical novelty here. \n- This paper argues that the proposed hierarchical attention is novel with no clear ground. It simply mentions the inspiration from (See et al. 2017), but does not explicitly what aspects are novel as Hierarchical attention has been popularly used in general seq2seq models for many NLP applications. \n- Table 1 is rather cursory and incomplete as it compares with only three different models.\n\n2. Experiments have much room for improvement. \n- The two datasets are simply mixture of text from existing datasets and rather artificial and far from actual interleaved posts, as shown in Table 2. \n- In page 6, three different corpora are introduced with varying interleave parameters. However, the parameters a,b,m, and n are rather small (<= 5), so it is hard to say the configurations are challenging.\n- The SOTA model (Shang et al. 2018) is only compared on the PubMed Easy Corpus in Table 3. Is there any special reason to exclude it from the comparison with more challenging corpora? \n- In most results of experiments, the paper focus on showing that the proposed hierarchical attention model is better than basic seq2seq, which may not be strong results to show the effectiveness of the proposed model.\n\n<Minor comments>\n- This draft does not seem to follow the ICLR format.\n- Fig.2-4 are hard to see in details. They should be enlarged and re-organized. \n- References should be added to Table 1. \n- This paper exceeds the limit of 8 pages. I cannot find any good strong ground to have more additional pages than the commended length. \n\n<Conclusion>\nMy rate is borderline with slightly leaning toward reject, mainly due to issues of technical novelty and evaluation. I will decide my rate finally, based on the authors\u2019 response to the concerns above. \n"}