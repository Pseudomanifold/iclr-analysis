{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work proposes an architecture to generate summaries for multi-participant postings as in chat conversations. Previous work tackled this problem as a pipeline of disentangling threads from mixed chat posts and then generating a summary for each post. The presented work proposes a hierarchical attention model to solve this task in an end-to-end fashion. \n\nThe proposed architecture is divided into :\n\n- a hierarchical encoder: encodes each sentence to a vector using bi-lstm and average pooling which is then fed to a post-to-post encoder which generates a set of representations corresponding to those posts.\n\n- a hierarchical decoder: number of disentangled posts are generated using a thread-to-thread decoder which given the max pooled representations from the post-to-post encoder generated a number of initial representations for each thread summary using hierarchical attention combining attention over paragraphs and over words at each decoding time step. \n\nExperiments: \nAuthors demonstrate the effectiveness of their approach by comparing against a set of baselines traditional seq2seq, as well as cluster-> seq2seq and the model from Shang et al. (2018). Evaluation datasets consisted of several datasets for chat summarization synthesized to mimic interleaved conversation with different difficulties. \n\nThrough an ablation study, authors also show the effectiveness of the hierarchical encoding and hierarchical decoding as well as the post level and high-level attention. \n\noverall:\nAlthough this paper is well written and well-motivated. The work is a bit incremental and builds upon previous ideas from hierarchical attention literature to apply for interleaved text generation. The provided baselines are quite weak compared to the SOTA summarization methods at the moment, although none of them is directly modelled for interleaved text summarization through multi summaries. \n\ntypos: \n* Nallapati et al. Nallapati et al. (2016)\n* See et al. See et al. (2017)\n\n\nQuestion: \nIs there an intuition behind penalizing a model that generates correct summaries for each thread with different order? Do references summaries follow the same order as in the threads in the conversation?\n\nIn the paper you dedicated several paragraphs to reimplement basic models such as seq2seq or (See et al.) pointer generator network and make sure they work correctly. There are existing implementations and pre-trained models available such as in OpenNMT. "}