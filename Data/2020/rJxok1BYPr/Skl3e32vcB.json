{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a novel approach to generating molecules using Black Box Recurrent Translation.\nThe authors uses existing machine-translation inspired schemes to generate new, similar, molecules with better properties according to some measure.\nThen, the recursion takes the top K best molecules and runs another iteration to generate even better molecules, ad infinitum.\nThe authors use the newly introduced SELFIES strings as vocabulary for generation.\nThe authors also analyze the decoding strategy, and how the process generates interpretable molecular traces.\nRelating to wetlab work, having a molecular trace available from the recursive translation scheme is valuable for drug-sythesis.\nThe authors also show that this technique can optimize multiple properties at once.\n\nI am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery.\n- Recursive black box translation seems to be widely applicable to new models.\n- The model seems to reach a significantly better state of the art on the metrics proposed.\n- None of the baselines seem to use SELFIES as the string of choice.\n  This means it's difficult to tell how much the \"Blackbox recursive\" part of the algorithm adds to the model.\n  An ablation experiment without BBRT might inform us of how much of the benefit is due to the molecule representation (Fig 4A reports the mean, but it would be good to have the same metric as Table 1).\n- The authors provide an in depth discussion about how having molecular traces would hhelp in drug design.\n  This makes the tool seem more widely appealing and useful.\n\nA few questions would clear up the strengths of the paper:\n- Is there a connection to the backtranslation work in Lample 2018? (Phrase-Based & Neural Unsupervised Machine Translation)\n  It seems like a similar idea - except in this domain, the target language and source language are the same.\n- How can there be multiple scoring functions? \n  Were they combined in one run, or were these separately optimized runs? Are these only used in Figure 4?\n- Why would beam search do less well than stochastic? \n  Is it because during recursive translation, the beam search variants have low diversity?\n  Then, training with stochastic decoding and generation with a beam search should do even better, right?\n  This would highlight that the advantage of stochastic decoding is really online in the context of recursive translation, not generally.\n- What is the point of Fig 4A right? Why do we expect that maximizing non-logP properties will increase mean logP?\n"}