{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper tackles the problem of off-policy successor feature learning, with the aim of using these successor features to quickly learn new tasks.  The paper proposes a method and evaluates it on a simple gridworld domain.\n\nAs is, the method proposed by the paper is unclear and the experiments insufficient to merit publication.\n\nI found Section 3 especially hard to follow:\n-- How do you take an expectation of policies from \\mathcal{M}?  Where are you getting these policies from?\n-- The method is stated as off-policy, but Eq 13 seems to be on-policy w.r.t. pi. \n-- Equations 13/14/15 don't make much sense. For example, Eq 15 does not describe a normalized probability distribution.\n-- A pseudocode would be helpful.\n\nIn addition, the experiments should be extended to more interesting domains. On the gridworld that is evaluated, many simple techniques can work very well."}