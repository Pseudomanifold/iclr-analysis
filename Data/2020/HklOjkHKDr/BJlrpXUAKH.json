{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Successor Representation/Features, decomposing the value function into a policy dependent state density function and a weighting over states is an old and powerful idea that has attracted renewed interest recently.\n\nHowever, a weakness of successor features has been that that the policy dependency means that this decomposition only transfers well to tasks which are similar to the training tasks.\n\nHere the author's propose a ``state-independent'' approach to learning features by sampling trajectories under a random policy and learning a state embedding (in a similar way to node2vec) such that discounted state density is predicted by the embeddings (eq 15). These state embeddings are then used as features to linearly predict the value in subsequent tasks.\n\nThe primary weakness of this work is lack of a significant contribution. The features are not really ``policy independent'', rather they are features learned under a random policy (which can be a good way of recovering a representation which maps the state topology well). SFs under random policies are not a new idea (e.g. [1]) and the approach used here is very similar to Madjiheurem & Toni (2019) which is cited and, as noted in the paper, node2vec. The addition of discounting the states is not a significant addition.\n\nThis lack of novelty along with the relatively small scale environments that is tested on lead me to conclude that this work may be of only limited interest.\n\nThere is a number of errors and notational issues in page 4/5 which make it hard to follow the method they are using. The notation $w$ is both used to denote a vector, and also to index over previous policies which is confusing.\n\nEquation 8 should say $w'$ for the final $w$ and the $max_w$, indexing over prior policies, is confusing notation. This notational confusion follows to eq 11 and eq 12 and makes this section difficult to understand. Why is there a maximisation over $\\theta_{w'}$, without constraints on $\\theta_{w'}$ wouldn't this be unbounded? Later, it appears (and makes more sense) that $\\theta_{w'}$ is being fitted to the state-action value (eq 16). Why is $\\psi(s,a)$ defined in terms of an expectation over tasks (what does this even mean), later $\\psi$ is trained in a task independent way. Essentially, section 3.1 seems much more understandable, and I struggled to understand the preceding section 3.\n\nMinor issues:\n\nThe paper would be improved by careful copy-editing (including node2vec being misspelled in the abstract).\n\n\nPage 2 the claim that ``this is of obvious interest as this formalism potentially model[s] real life application'' does not seem well-justified. How ``real world'' the successor feature assumption is non-obvious.\n\n[1] Stachenfeld, Kimberly L., Matthew M. Botvinick, and Samuel J. Gershman. \"The hippocampus as a predictive map.\" Nature neuroscience 20.11 (2017): 1643."}