{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new weight initialization method for sparse neural networks and develop a weight topology that satisfies desirable properties. Their derivation is data-free, and thus the analysis should generalize to arbitrary datasets. They demonstrate that their new topology outperforms existing approaches on a matrix reconstruction task.\n\nOverall I think this work is an interesting direction for designing static sparse neural network weight topologies, but it\u2019s lacking in empirical evidence of their claims and could do better to tie themselves to existing literature in training sparse neural networks. \n\nIf the authors could strengthen their results by a) experimenting with their newfound topology and initialization on standard sparsification benchmarks like CIFAR, ImageNet, and WMT EnDe b) comparing their approach to other static-sparse [1, 2, 3] and dynamic-sparse [4, 5] training algorithms this could be a good paper, but without more experimentation it\u2019s unclear what can be taken away from this work. If the authors added results in this direction I would be willing to increase my score.\n\nComments on Claims of the Paper:\n\n1. \u201cCascades\u201d are a known trick in both dense & sparse neural networks [2, 6].\n2. The authors describe their motivation for developing a new sparse initialization method in the first paragraph of section 4. It would be nice to see some of this experimental data, such that we could understand the magnitude of the vanishing gradient problem in these tests and see that the newly derived initialization alleviates it. The reason for my skepticism is that Liu et al [7] used a similar scheme where they re-scale the standard deviation of the gaussian based on the fraction of nonzero weights, but later found that it made no difference in their results for unstructured pruning (which I learned through discussion with the authors).\n4. The data-free derivation approach makes sense and I understand that this makes the approach theoretically applicable to arbitrary datasets, but the authors do not apply it to other datasets to show that it generalizes in practice.\n5. The authors show that their topology outperforms on the matrix reconstruction task, but they don\u2019t compare with other sparsification approaches used in deep learning like sparse evolutionary training [4], or SNIP [1] (note that these techniques also maintain a sparse network during training, as opposed to pruning approaches like magnitude pruning [8] that are dense during training but sparse for inference).\n\nComments on the Results of the Paper:\n\nThe authors appear to contextualize their work in deep neural networks, but all of their experimental results are on matrix reconstruction or linear models on MNIST. This is sufficient for analysis and motivation, but taking the developed approaches and applying them to a deep neural network and showing improvements would go a long way towards improving this paper.\n\nAssorted Notes:\n\nIn the first paragraph of the introduction:\n\n\u201cIn other words, doubling the size of layer inputs and outputs quadruples the size of the layer. This causes majority of the networks to be memory-bound, making DNN training impractical without batching, a method where training is performed on multiple inputs at a time and updates are aggregated per batch.\u201d\n\nIt is correct that a matrix-vector product (i.e., neural network training with batch size 1) is typically memory bound on accelerators like GPUs, but it\u2019s not clear why the quadratic growth in computational cost with input/output size has anything to do with this. The cause of memory bound-ness is lack of reuse of operands, which can be alleviated by increasing the batch size s.t. the computation becomes a matrix-matrix product. Batch size 1 training is also not desirable. Recent work has shown that large batch sizes do not degrade model quality with proper hyperparameter tuning [9], and larger batch sizes are desirable from a hardware perspective to achieve higher throughput.\n\nReferences:\n1. https://arxiv.org/abs/1810.02340\n2. https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf\n3. https://arxiv.org/abs/1903.05895\n4. https://www.nature.com/articles/s41467-018-04316-3\n5. https://openreview.net/forum?id=ryg7vA4tPB\n6. https://arxiv.org/abs/1811.10495v3\n7. https://arxiv.org/pdf/1810.05270v2.pdf\n8. https://arxiv.org/abs/1710.01878\n9. https://arxiv.org/abs/1811.03600"}