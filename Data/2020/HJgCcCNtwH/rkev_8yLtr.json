{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper tackles the problem of finding a sparse network architecture before training, so as to facilitate training on resource-constrained platforms. To this end, the authors propose to replace dense layers with series of sparsely-connected linear layers. They then study how to initialize such sparse layers to avoid gradient vanishing. Furthermore, they propose an approach to finding the best topology by measuring how well the sparse layers can approximate random weights of their dense counterparts.\n\nMethodology/clarity:\nWhile I found the beginning of the paper, up to Section 4 (included), easy to follow, I must admit that I have been struggling with the remainder of the paper. In particular:\n- In Section 5, the authors propose to use the L2 difference between random weights of a dense layer and the reconstruction of these weights with sparse layers. It seems that the author take this as a proxy to network accuracy (as indicated by a statement just below Fig. 3). It is not clear to me, however, that a small L2 norm will indeed correspond to a similar accuracy in practice, and this is never demonstrated in the paper.\n- This L2 loss is then replaced by an L0 one, which again I fail to find any justification for. In fact, I don't see why the authors did not directly use the L0 norm in the first place.\n- If I understood correctly, the goal of Sections 5-6 is to find the best topology for the sparse layers. However, it seems the number and width of such layers still need to be pre-defined. Correct?\n- Below Eq. 6, the authors mention that the networks are still trained using SGD with L1 loss. I do not understand this statement. On what is the L1 loss computed?\n\nExperiments:\nUltimately, the authors argue that their approach allowed them to find a parallel butterfly architecture that outperforms the other ones. However:\n- There is no evidence that the resulting architecture is indeed better than the others when it comes to solving an actual problem (e.g., in terms of classification accuracy);\n- There still seem to be a fair bit of manual design in this parallel butterfly architecture, and it is therefore not clear to me that it is truly the best possible one.\n\nRelated work:\nA number of architecture designs have been proposed to obtain a compact network prior to training. MobileNets, used here as baseline CNN, is one of them, but so are MobileNetv2, ShuffleNet and ShuffleNetv2, SqueezeNet, and ERFNet. I believe that it would be worth discussing these architectures and showing the benefits of the proposed approach over them.\n\nSummary:\nI have been struggling to follow the second half of this paper, and I am not convinced by the hypothesis that the L2 (or L0) norm between dense and sparse parameters is a good proxy for network accuracy, which is not demonstrated. I therefore feel that this paper is not ready for publication.\n"}