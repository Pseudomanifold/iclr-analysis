{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to replace dense layers with multiple sparse linear layers.  The idea is that the product ABC (for A, B, C all sparse matrices)  can accurately approximate a dense matrix D, but A(B(Cx))) requires much less work than Dx.  The paper then continues with the assumption that topology of the sparse matrices should be fixed before training, and that given this assumption we would like to find the \"best\" fixed topology to pick.  The paper introduces a new task to determine the \"best\" topology - that of reconstructing a random dense matrix.  On somewhat of a tangent the paper also introduces a minor modification to the Xavier initialization scheme that works better for deep stacks of sparse layers.\n\nMy current decision is one of Weak Reject.  I think the paper tackles an interesting line of research with some interesting ideas, but I'm concerned that the implications of the major assumptions that are made are never examined.\n\nIt is briefly implied that if the topology were fixed, then we could build hardware for such a topology (motivating the approach).  But we could also build hardware to accelerate general dynamic sparsity.  Given that it seems important to try and understand the tradeoffs involved in using fixed sparse topologies like the ones proposed and dynamic sparsity techniques such as:\n\n1) Deep Rewiring (https://arxiv.org/abs/1711.05136)\n2) Sparse Evolutionary Training (https://www.nature.com/articles/s41467-018-04316-3)\n3) Dynamic Sparse Reparameterization (https://arxiv.org/abs/1902.05967) \n4) Sparse Networks from Scratch (https://arxiv.org/abs/1907.04840)\n5) Rigging the Lottery (https://openreview.net/forum?id=ryg7vA4tPB)\n\nThe tradeoffs both in terms of the hardware that could be built for these different regimes _and_ the effect of static topologies vs. dynamic ones on accuracy (for a given parameter / flop / energy / etc. budget).  These dynamic techniques could also be used to decompose a dense layer into a product of multiple learned sparse matrices.\n\nThe effect of assuming that we want each connection to have equal controllability seems non-obvious. For example, if we imagine that we're replacing a convolutional (or at least locally connected) layer, then we _want_ to be able to take advantage of the structure/particularities of the input and doing so will lead to a more efficient model than one which is forced to ignore them (as the proposed topology necessarily does.)   How much less efficient will the proposed architecture be in this case?\n\nMany of these questions could be answered by trying the dynamic sparse techniques and the proposed static topologies on real problems (MB on ImageNet for example), maybe some kind of language modeling task, etc.  I find the use of one artificial task (of matrix reconstruction) which serves mainly to confirm the assumptions that are made rather than test them on real data and real problems a big weakness of the paper.\n\nSome general notes:\n\nThe enforcing sparsity before training section should mention SNIP: Single-shot Network Pruning based on Connection Sensitivity https://arxiv.org/abs/1810.02340\n\nThe enforcing sparsity during training section, should mention both the dynamic techniques that are mentioned above, but also techniques that are dense -> sparse but which significantly outperform the L1 techniques mentioned.\n\nFor example:\n\n1) Iterative Pruning as used in Learning both Weights and Connections for Efficient Neural Networks (https://arxiv.org/abs/1506.02626) and popularized by The Lottery Ticket Hypothesis (https://arxiv.org/abs/1803.03635)\n2) To prune or not to Prune (https://arxiv.org/abs/1710.01878)\n3) Variational Dropout (https://arxiv.org/abs/1701.05369)\n4) Dynamic Network Surgery (https://arxiv.org/abs/1608.04493)\n\nThe mobilenet reference seems a bit out of place as mobilenets are never otherwise used in the rest of the paper.  It seems like something very similar has already been done in (https://dawn.cs.stanford.edu/2019/06/13/butterfly/).\n\nvraiance -> variance"}