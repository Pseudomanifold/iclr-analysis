{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents an unsupervised learning approach to solve forward and inverse problems represented by partial differential equations. A framework is proposed based on the minimisation of loss functions that enforce the boundary conditions of the PDE solution and promote its smoothness. The method is then applied to solve forward and inverse problems in electrical impedance tomography.\n\nFlexible machine learning approaches to solving partial differential equations is a subject of ongoing research. While the paper presents an elegant solution folding forward and inverse problems into a single framework, the presentation is missing a few important details which difficult the assessment of the contribution and favour a rejection of the paper. The main issues are insufficient experimental comparisons and a lack of theoretical support for the method.\n\nMajor issues:\n\n1. When compared to DGM (Sirignano & Spiliopoulos, 2017), for the forward problem, the method only differs in the form of the loss function, which is almost identical, with the exception of the additional L-infinity term and the optional user-defined regularisers. The argument for the inclusion of the L-infinity loss is its high sensitivity to outliers, enforcing a smooth solution. (a) Why PDE solutions learned using the original loss from DGM, which also yields continuous functions, should present outliers in the first place? Moreover, the possibility of adding a user-defined regularizer seems to be a relatively simple extension. (b) What should the theoretical or practical implications for the extra user-defined regularisation term be?\n\n2. The loss function for the inverse problem, which seems to be one of the paper\u2019s contributions, misses a dedicated discussion. An important detail in this loss is the third term, which enforces boundary conditions for the coefficients at the boundary of the domain. In Equation 2, however, the coefficients only affect the PDE through the Lu term over the domain, not its boundary. So what does \\theta_0 mean in Equation 4 then?\n\n3. The paper proposes a general framework, but experimental results are presented for only one specific problem, the electrical impedance tomography. The generalisability of the method to more complex problems, such as PDEs with time components and a high-dimensional spatial domain, cannot be inferred. Adding experimental comparisons on higher-dimensional domains would strengthen the paper.\n\n4. Experiments only present comparisons to relevant state-of-the-art methods (DGM) in the forward problem. There are no comparisons against other methods for the inverse and the free-shape geometry problems. For example, have the authors considered the method in [A]?\n[A] Xu, Kailai, and Eric Darve. \"The neural network approach to inverse problems in differential equations.\" arXiv preprint arXiv:1901.07758 (2019).\n\nMinor issues:\n\n1. The background on PDEs is relatively short for a machine learning conference. (a) There lacks an explanation on what the operator \\mathcal{L} means. (b) Equation 1 lacks an explicit use of \u201cu(x)\u201d, instead of simply \u201cu\u201d, causing confusion with the dependence of the coefficients on \u201cx\u201d. (c) The meaning of the index subscripts on the partial derivatives is also not made clear, especially if \u201cu\u201d could be interpreted as a vector-valued function for someone unfamiliar with PDEs. Replacing \u201csome u\u201d by \u201csome u:R^d\\to\\R\u201d would already help.\n\n2. What does \u201cn\u201d mean in the electrical current equations in Sec. 3?\n\n3. The derivative of a scalar \u201cu(x)\u201d with respect to a vector \u201cx\u201d should be a vector. So what are the plots in figures 4, 5 and 8 showing when referring to du/dx? Is that the magnitude of the vector or the partial derivative with respect to a single spatial component?\n\n4. What does \u201cPSNR\u201d stand for?\n\n5. Indirect citations in the text should be enclosed by brackets using something like the \u201c\\citep\u201d command from the package \u201cnatbib\u201d.\n\n6. In Table 1, there is a typo: \u201cGDM\u201d->\u201dDGM\u201d.\n\n7. The context contains a few minor grammatical issues that can be distracting at times, but should be solvable by revision."}