{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work, the authors propose an approach to solve forward and inverse problems in partial differential equations (PDEs) using neural networks. In particular, they consider a specific type of second-order differential operator combined with Dirichlet boundary conditions, and suggest how neural networks with suitable training objectives can be used to solve both types of problems. For a specific elliptic system relevant for Electrical Impedance Tomography (EIT), they then present numerical results obtained with their method and compare those to high-resolution finite element solutions and previous work.\n\nThe paper addresses a very general and important problem with wide ranging applications. People have solved PDEs using spectral, finite difference, finite volume or finite element methods for decades and there is a huge body of literate on this subject. The neural network based approach proposed in this paper seems general and simple with encouraging experimental results. However, there are several important points missing in the current paper based on which I would recommend rejecting it. However, I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor comments:\n\n1) Choice of problems\n\nThere are many important problems in the literature that belong to the general type of PDE considered in this work (e.g. in electrostatics). To appreciate and understand the merits and limitations of the proposed approach better, it would be highly necessary to apply it to a wider range of problems, including ones with known analytical solution. \n\n2) Convergence tests\n\nConvergence tests are an important part that is missing in the current paper. In particular, how does the error in the neural-network solution decay with respect to the number of random points used to train the network? Plots comparing a suitable error norm versus the training dataset size would be very informative. Furthermore, what are the conditions, if any, to guarantee convergence to the exact solution? The authors mention in the last paragraph of the Discussion that a more rigorous analysis will be published elsewhere. However, as error analysis is such an integral part of the study, I think it needs to be addressed, at least to some extent, already in the current paper.\n\n3) Related work\n\nI don\u2019t think the current work is put in sufficient contrast with existing work. Several related papers are mentioned in the introduction and the experimental section includes one other method (GDM method of Sirignano & Spiliopoulos) for comparison. However, a more thorough discussion on how the current approach complements existing literature on neural network based PDE solvers would be in order. \n\nMinor comments:\n\ni) There are quite a few typos and grammar mistakes in the paper that need to be fixed. To give a few examples:\n\u2018the natural structure presents\u2019 -> \u2018the natural structure present\u2019\n\u2018Shr\u00f6dinger\u2019 -> \u2018Schr\u00f6dinger\u2019\n\u2018approximated by neural network\u2019 -> \u2018approximated by a neural network\u2019\n\u2018on circular and\u2019  -> \u2018on a circular and\u2019\n\u2018forces the equation\u2019 -> \u2018enforces the equation\u2019\netc...\n\nii) Eq. (7): There is a parenthesis missing on the LHS.\n\niii) The authors show many plots in Figs 3-9 but don\u2019t comment much on the results in the main text. A lot of the results presented could therefore go into an appendix. I would find it better to present fewer results/plots and discuss those in more detail.\n\niv) It would be good to define the Nabla operator in Eq. 5 for completeness.\n\nv) Please put references to equations and papers in parentheses or, at least, separate them otherwise from the sentence, to avoid confusion. For example:\n- the reference to \u2018Evans (2010)\u2019 below Eq. 5, or\n- the list of dangling references before the last paragraph on page 3,\n- the reference to Sirignano & Spiliopoulos in the caption of Table 1,\netc...\n\nvi) Please expand the captions in Fig. 3 to make the figure meaningful as a stand-alone, without having to read the entire text to understand what 'phantom' means. The same applies to all other captions.\n\nvii) Perhaps it would be clearer to use \u2018row\u2019 instead of \u2018line\u2019 when referring to the results in the table.\n\nviii) In Sec. 7 \u2018s\u2019 in \u2018Ns\u2019 should be a subscript.\n\nix) Explain what the acronym PSNR means.\n\nx) Please elaborate on why you take the top k values in Eq. (7). What happens if you take more/less?\n\nxi) First paragraph in Discussion: I am not sure the robustness w.r.t. the hyperparameters is really so surprising given that you apply the solver to very similar problems. If you could show that the same hyperparameters worked on a completely different problem, that would be much more interesting.\n\nxii) Please add references for the first paragraph of the introduction.\n"}