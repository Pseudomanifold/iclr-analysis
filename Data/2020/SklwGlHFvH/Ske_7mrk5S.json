{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis theoretical paper exploits a recent rigorous correspondence between very wide DNNs (trained in a certain way) and Neural Tangent Kernel (a case of Gaussian Process-based noiseless Bayesian Inference).  \nA field-theory formalism was developed for Gaussian Processes (2001). Here it is thus extended to the NTK case. There are 3 important theoretical results which are both proven and backed by numerical confirmation.  These results, in particular the first, provide a very accurate prediction for the learning curve of some models.  The paper is well situated within this literature.  I am not very knowledgeable about NTK or even Gps, however I understand the challenges of understanding DNNs and I am familiar with field theory and renormalization group.\nGiven the importance and quality of the results, and the overall quality and clarity of this (dense) paper, I recommend acceptation without hestiation.\n\nThere are a couple of points however that could be improved, that would make the paper more useful for the community.\nGiven the density of results in the paper, I would relax the length constraint, allowing up to 9 or 10 pages if possible, to add more explanations (not computations).\n\n\nI would like the paper to present more explicitly how the regression target labels g(x) are generated. Maybe it is said but I couldn\u2019t easily understand, for sure, how they are generated.\n\nAlso, please explain early enough what is meant by uniform dataset (I understood it simply means the data x is drawn uniformly at random over a manifold, here this manifold is often the d-dimensional hypersphere).\n\nClaim II states that \u2018...lead to clear relations between deep fully-connected networks and polynomial\nregression\u2019\u2019. This is, I believe, supported by theoretical proof and numerical double-check, however it is not discussed enough for the abstract\u2019s promise to be fulflled \u2018a coherent picture emerges wherein fully-connected DNNs ...\u2019.\nI think this point deserves a more ample discussion in section 7.\n\nMore generally, the claims in the introduction or at the end of section 3 are stated rather explicitly, but very densely, and the careful reader can get the hypothesis of each result from the text. \nHowever for the sake of ease of read of less patient readers, I think it would be appropriate to have, somewhere, a more self-contained description of the results\u2019 list. This paper is technical and some readers will be interested of simply knowing the hypothesis made and type of results obtained.\nFor instance, the sentence \u2018\u2018They [results] hold without any limitations on the dataset or the kernel and yield a variant of the EK result along with its sub-leading correction.\u2019\u2019 is misleading: as stated in the previous sentence in the text, this is for the fixed-teacher learning curve, etc.\n\nPlease try to explain a bit more the intuition behind renormalization / trimming terms q>r (r integer fixed, the higher the less approximated).  More specifically, it is not very clear to me how it can be interpreted in terms of how we look at the data. You mention (x.x\u2019)^r being negligible or not depending on r,d etc, but I wounder if there is some kind of simple \u2018geometrical\u2019 interpretation (is it a coarse graining of the data in angular space, the \u2018high energy\u2019 eigenvalues corresponding to the high frequency, high resolution distinction between very close angles ?).  On that point I am a bit lost and it\u2019s a pity because your results are strong and rely on few, rather simple/elegant assumptions (which call for some intuitive understanding).\n\nCould you explain intuitively, to the inexperienced reader, why the noiseless case is harder to deal with than the finite-noise one ?\n\nIn appendix D, it is mentioned that you need averaging over about 10 samples to have a decent average. For a single realization of a N-sized training set, there is additional variation (Adding or subtracting to the error epsilon).  Given actual experiments are typically performed for a single realization of the data, I think this point should be mentioned in the main text more explicitly. Ideally, you could add error bars to the data, accounting for the dispersion inherent to a single-realization case.\n\nIn early section 3, a short definition of a GP should be provided (there, or before).\n\nAround Eq. 2, you should specify the interpretation of P_0[f]\n\nAfter Eq. 3, \u2018where F is some functional of f.\u2019 I would add: \u2018[where F is some functional of f,] for instance Eq. 2.\u2019\nThe derivation of eq 6 is not obvious. You do detail it in the appendix, but forgot to cite the appendix !\n\n\u2018Notably none of these cited results apply in any straightforward manner in the NTK-regime.\u2019 : could you quickly explain why (no matched priors ? Noise ?) \n\n\u2018\u2018The d^-l scaling of eigenvalues\u2019\u2019 : at this point, the variable \u2018l\u2019 had not been defined.\n\n\u2018\u2018notably cross-talk between features has been eliminated\u2019\u2019 : has it been eliminated or does it simply become constant ? \n\n\u20183% accuracy\u2019 [relating to figure 1]\nI understand the idea but accuracy seems misleading. I would replace everywhere with something like \u2018relative mismatch\u2019.  OR explain better why you call this accuracy: usually a high accuracy is preferred, and here you are proud with this very low imprecision of 3%\n\n\u2018\u2019Taking the leading order term one obtains the aforementioned EK result with N\u2019\u2019 : maybe (just a suggestion here) you could recall it here, given it was in page 1 (and in-line).\n\n\nAppendix B:  could you explain why this difference increases with N ? I would have expected this kind of quantity to decrease with N.\n\nAppendix F: there are typos in the r.h.s. in the first line.\n\\sum_j f_j \\phi_j  (I think).\n\nAppendix G: \u2018noisy targets\u2019 : you mean fully random or Kernel + some degree of noise with variance sigma^2 ?  I think it\u2019s the first time you use this phrasing.\n\nAppendix G: you denote \\partial / \\partial \\alpha for the functional derivative. I would replace with \\delta to stress out it is a functional and not regular derivative.\n\nBeyond\t appendix G.1 : I confess I didn\u2019t have time to read it.\n\nDespite the overall quality of the text, there are a number of wrong singular/plural matchings, which can easily be corrected. Here are some of them, with other typos as well: \n\u2018Furthermore since our aim was to predict what the DNNs would predict rather [THAN?] reach SOTA predictions\u2019\n\n\u2018a factor of a factor\nof about 3.\u2019\n\nas do for \u2013 > as we do for\n\nuniformally - > uniformly"}