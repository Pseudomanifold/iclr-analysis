{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper demonstrates that for regularization, data augmentation usually works better than explicit regularization methods such as weight decay and dropout. The experiments are detailed and also includes theory explanation of why data augmentation works.\n\nData augmentation (or increasing the size of training data) and explicit regularization are standard methods to overcome overfitting. From my understanding, they are two different methods to tune the model quality, and they can combined to further improve the model performance, as discussed in [1]. So this paper is not well motivated. \n\n[1] DeVries, Terrance, and Graham W. Taylor. \"Improved regularization of convolutional neural networks with cutout.\" arXiv preprint arXiv:1708.04552 (2017).\n"}