{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies exposure bias in neural language models and proposes a method to quantify it. The method evaluates the discrepancy between the conditional distributions of the next token--given a history---of the data distribution and the model distribution. The authors conclude that exposure bias seems to play a fairly minor role especially for \"short generations\" (e.g., short sentences). \n\nThis paper studies exposure bias a seemingly important issue which has remained elusive. Exposure bias has fueled research on text GANs (and other non-MLE-based frameworks), as such better understanding it and/or how it affect MLE-trained models is important and could be impactful. I find that this paper makes a step in that direction. If the results are to be believed, the exposure bias problem may in fact not be as important as once thought. This results also goes in the same direction as recent research where the role of exposure bias never seemed to have materialized. Overall, while I think this kind of analysis if worthwhile, I am not sure that the contribution is significant enough to warrant a publication at this venue. \n\nDetailed comments/suggestions:\n- In Section 5.2, the synthetic data is generated using an LSTM LM. Another LSTM LM is then used to fit this data to quantify exposure bias. I am wondering if using this model could change the results since in this case the trained model could indeed recover the generating distribution (i.e., the model is perfect)? If anything, it seems like it could lessen the impact of exposure bias.\n\n- There are methods such as professor forcing (Lamb et al., NIPS'16) aimed at combating the exposure bias and seem to outperform LMs trained using teacher forcing. Could you briefly discuss these and hypothesize as to where their gains come from? This is in contrast with text GANs that---as you point out---may not be better than MLE-trained models. \n\n- I am a little puzzled by the second paragraph of the Related Works Section (6). If I understand correctly, your claim is that previous work implied that exposure bias may not be a serious problem in MLE training.  As far as I understand, these works do not make such a claim but rather come to the same conclusion as you do (i.e., it's also possible that text GANs simply do not solve the problem well enough). \n\n- In Table 1, why show the results using transformers since in the sequel you use an LSTM LM (and transformer results are in the appendix).\n\n\n\nMinor comments:\n\n- STOA -> SOTA\n\n- In Eq. 5 the position of the parameters of MGD are not the same as in Example 1."}