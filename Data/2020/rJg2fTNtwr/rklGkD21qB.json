{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies the seriousness of the exposure bias problem in language modeling. Many NLP researchers believe that the fact that an auto-regressive language model (LM) is conditioned on its own prediction at test time whereas it was conditioned only on training data at training time induces exposure biases. In this work, based on the observation that conditioning on even random inputs or model's past history will still generate valid continuations, questions whether exposure bias is a serious problem. \n\nThe first experiment in this paper is based on measuring the distance between the marginal distribution of the model's predicted word at a certain position to the ground truth marginals, where the model is either fed with ground truth prefix or its own predictions. While this metric shows around 10% difference between ground truth prefix and prediction prefix as the length of the generation goes to around 20, the authors argued that there might be multiple causes since the model's prediction prefix might have a different distribution to the ground truth prefix, leading to different marginals.\n\nTo separate out the effect, this work considers measuring the distance between conditionals instead of marginals at a certain position, where conditionals are either conditioned on ground truth prefix or language model prefix. However, since in real data we do not get access to the true conditionals (especially when prefix is generated from the model), this work did a synthetic experiment, using an LSTM language model to generate data as the true data distribution, and use other LSTM models to fit. On this synthetic task, 1) the difference in distances either conditioned on ground truth prefix or predicted prefix is within 3%, and 2) scheduled sampling does not help, but seqGAN helps a bit.\n\nPros:\n1. This paper raises a very interesting question of quantifying exposure bias and proposes a metric to quantify the phenomenon of exposure bias.\n2. The experiment on the synthetic task is convincing, showing that exposure bias is not a huge issue there.\n\nCons:\n1. In terms of motivation, the observation here is that auto-regressive LMs are able to self-correct, and authors found similar results for transformers. However, as shown by Holtzmann et al 2019, when directly beam search from the LM, it will produce repetitions that would be further reinforced by the model itself. Those two observations seem contradictory.\n2. In the synthetic experiment, the data generator is also of an LSTM architecture, so this setting is similar to knowledge distillation where the student and teacher networks have similar structures. It is well established that the data generated from an LSTM LM is biased (smoother and simpler) compared to the data distribution it's trained on. It is thus not very surprising that the distribution of student's samples is similar to the \"true\" data distribution (teacher's samples), whereas this is not the case in the real world. One way to evaluate this is to repeat the first (\"wrong\") experiment on this synthetic dataset, and I would expect the measured differences to be also smaller here. Another way would be using a much more powerful generator and a less powerful LM (what if we use transformer LM to generate but use RNN w/o attention to learn?)\n3. What does 3% difference in TV/JS mean? It is a token level metric anyway. Would it mean a less than 3% difference in sequence-level metrics such as BLEU? Does it mean 3% wouldn't compound over time steps? I don't think we can draw any conclusions by looking at this number. \n\nOverall, I think this paper asks a really interesting question, but this paper doesn't provide a convincing answer, and I am still not sure if exposure bias is a serious problem or not. I am inclined to reject this work unless I can get a convincing assessment of the exposure bias problem.\n\nReferences:\n1. Holtzmann et al 2019. The Curious Case of Neural Text Degeneration. https://arxiv.org/abs/1904.09751"}