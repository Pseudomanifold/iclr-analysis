{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies whether exposure bias (the fact that a test time, when  a model is making a sequence of predictions, its history is its own predictions, as opposed to during training, in MLE setting, where its history is ground truth predictions) is a real problem for generation systems. The goal is to find a way to really quantify whether there is an issue empirically. The paper presents one result which argues that it is not (by comparing p_model(x | ground truth history) vs p_model(x | predicted history) for a number of datasets and show it is not a problem. It then argues that this is a bad measure (which I somewhat agree with, although it is a deeply intuitive one), and then tries to compare p_gtdistribution(x | predicted history) vs p_model( x | predicted history). This is where the paper loses me. p_gtdistibution makes no sense from the point of view of real data because we just have samples, and assigning probability is the introduction of a model. The paper tries to continue by using a synthesized distribution here (an LSTM) and then somewhat off-handly declares that in this setting exposure bias is small so its not really a problem. \n\nI'm very torn about this paper. I found it very interesting, but I also don't think using a synthetic distribution makes sense but I do not have any proposals for how operationalize the paper's proposed definition for measuring exposure bias with real data. The issue is that I don't care if its a problem or not of a synthetic distribution, only real world data. As such, the results toward the end of the paper do not seem valid to me but aren't really overstated (because of the writing, its hard to take the result very seriously, and the authors seem to know). \n\nI am not sure if the ICLR audience would be as generally interested as I was in the paper, so I am tentatively marking weak reject, although if there is other interest among reviewers I would upgrade.\n"}