{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "---- Paper summary ----\nThis paper proposes a curriculum learning approach for classification. The proposed curriculum consists of two phases:\n(1) a \u201clabel introduction phase\u201d, in which the model is able to see and learn to classify only subset of labels (the model still trains on the samples belonging to the \u201cunseen\u201d classes, but their label is now set to a default class). The subset of seen labels is expanded incrementally, until the entire label set is observed. \n(2) an \u201cadaptive compensation phase\u201d, where the model trains on all labels, but the targets for each class are replaced from 1-hot vectors to smooth version. This only applies to the classes on which the model has made mistakes in a previous training round.\nThis method is tested on 3 image classification datasets, and a single neural network architecture is tested per dataset (either ResNet18 or ResNet34).\n\n---- Overall opinion ----\nWhile the ideas introduced in this paper may have merit, I believe the experimental evidence is quite limited. Based on the results shown in the paper, I am not convinced that this approach is better than the baselines it compares to. Moreover, since the claim is that this approach is a general curriculum learning method, I find the setting it was tested on very limited (3 datasets, 1 model per dataset), especially since there are no theoretical results to complement the empirical evidence. Finally, the method introduces several parameters (b, m, E, T, eta) that are treated is a somewhat hand-wavy manner, without a proper analysis on the effect of such parameters and how one should set them. Details on this, and other major issues, can be found below. For these reasons, I believe the paper in its current form is not yet ready for publication.\n\n---- Major issues ----\n1. The paper simply mentions that the unseen labels are set to a default label, which Figure 1 implies (and is not otherwise clarified) is one of the labels in the dataset. I am not sure intuitively why it makes sense to force the model in the beginning to map a sample\u2019s inputs to another label from the dataset, which in the end it has to learn is wrong. Doesn\u2019t it make more sense to map the unseen labels to a new, fictional label? If not, then how do you decide which of the M labels to choose as the fake label?\n\n2. The method introduces several hyperparameters, such as b (number of visible labels in the beginning), m (number of labels to reveal at each step), E (number of epochs in each incremental phase), T, epsilon. The specific numbers used in the experiments are reported, but it is not clear how these were chosen, and how one would choose them for a new dataset or model.\n\n3. In Table 1, the LILAC results are bolded with a caption saying that bold they means \u201cthe best case scenario\u201d, and the main text also claims that \u201c, LILAC is the only one to consistently increase classification accuracy and decrease the standard deviation across all datasets compared to batch learning\u201d. However, from Table 1 it seems LILAC has neither the highest accuracy (label smoothing has overall higher accuracy), neither the lowest std. It may seem that the authors arbitrarily decided what is the best accuracy/std tradeoff which makes their method seem the best. Please define clearly the criteria for establishing the best method, and explain in what setting this criteria is a valid choice.\n\n4. Aside from the issue mentioned above, the differences in accuracy or std in Table 1 seem minor (e.g., within 0.10% on Cifar10, and within 1% in the others). Please provide more evidence that these differences are significant.\n\n5. How was the consistency in table 1 decided? Please provide the specific metric.\n\n6. Regarding the choice of models and datasets, the method was only tested on image datasets. This is could be enough as a contribution, but in this case the introduction and abstract should not claim a generality that has not been tested. Similarly, the paper only considers 1 model per dataset. Does this work for other models too? \n\n7. From Table 3, it looks like the LILAC w/o AC is actually worse than the baseline. In this case, what is the benefit of having the label introduction phase? Why not just have the AC component alone? If there is a reason, please include the results for AC alone in Table 3.\n\n8. I find the comparison in Figure 2 potentially misleading, because I believe 1 epoch of batch training is not equal in terms of amount of training as 1 full span of the incremental phase. I believe these embeddings should be shown at convergence time.\n\n9. In Table 4 and accompanying text, the authors conclude that \u201cLILAC is relatively invariant to the order of the label introduction\u201d. However, to me both random and ascending order seem actually random with respect to how this order is used. I advise the authors to try other orderings too, such as sorting them by the error of an initial training of a classifier, or other more difficulty-based orders.\n\n---- Minor issues ----\n1. In Algorithm 1, there is a missing equation (\u201cEqn. ??\u201d), also I\u2019m not sure why the first line says \u201cWrite here the result\u201d.\n\n2. \u201csmall batch sizes help achieve more generalizable solutions, but do not scale as well to vast computational resources as large mini-batches\u201d \u2192 this is a bit confusing. How do large mini-batches scale better, and what is the difference between \u201csmall batch sizes\u201d and \u201clarge mini-batches\u201d? \n\n---- Questions ----\nPlease see the major issues questions above."}