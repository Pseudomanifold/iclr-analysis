{"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper proposes two approaches to quantify neuron importance in CNNs. The first method based on Shapley value computes the marginal contribution of each filter; and the second approach uses probabilistic variational inference. The algorithmic contribution is demonstrated with a suite of experiments on MNIST and Fashion-MNIST, using LeNet and VGG architectures. \n\nThis paper should be rejected primarily because (1) the algorithms are not well justified either by theory or practice; (2) the two approaches described are loosely connected and therefore making the paper lack of focus; and (3) the experiments lack comparison with many other existing state-of-the-art neural compression methods. \n\nSpecifically, this paper lacks theoretical and/or empirical justification on why individual neuron\u2019s contribution during the learning process can be characterized equivalently as a coalitional game. How is the characteristic function chosen in practice? Does this depend on the neural network architecture in use? Further experiments should be provided to show ablation on this. The authors should also consider reporting the measurement in time for computing the Shapley value, in order to justify the computation feasibility. How well does the approximated solution compared to the optimal solution? What\u2019s the time and performance tradeoff? \n\nIn the experimental section, the author should consider providing details on how to choose the number of channels (for different layers) to prune based on the ranking. How does the pruning strategy affect the model performance?\n\nThis paper can be strengthened by comparing with existing state-of-the-art compression methods such as knowledge distillation (Hinton et al), SqueezeNet, ShuffleNet and other quantization based methods. \n\n\n\n\n"}