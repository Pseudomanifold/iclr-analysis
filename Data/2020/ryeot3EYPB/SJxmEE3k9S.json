{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers the problem of cross-lingual document classification with domain gap in source and target languages. In this setting, labeled data is only available in the source language, while the target language has in-domain unlabeled data. The aim of the paper is to tackle the domain and language mismatch problem simultaneously.  \n\nThe paper then proposes two simple methods to overcome this gap. The first method uses MLM training (a la BERT) while the second uses the unsupervised data augmentation technique to generate new examples through translation. \n\nThe technical contribution of the proposed approach seems modest. Also, the experimental analysis is limited to sentiment and document classification, relatively easier tasks than say, XNLI. With such limited experiments, its difficult to assess the utility of the method. The experimental comparison could also be extended to include other approaches for domain adaptation across languages, discussed in the related work section. \n \nQuestions:\n1. Are the reduction in error rates statistically significant?\n\n2. The discussion argues that less impact of self-training for MLDoc dataset owes to it not having enough unlabeled examples. How to determine what is \"enough unlabeled examples\" for self training?  \n\n\n"}