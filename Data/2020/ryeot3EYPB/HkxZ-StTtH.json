{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper aims to tackle the domain shift problem for cross-lingual document classification by combining the masked language modeling (MLM) pre-training and unsupervised data augmentation (UDA) methods. Experimental results on multi-lingual Amazon and MLDoc datasets demonstrate the effectiveness of the proposed methods. The performances are superior. However, there exist some problems:\n1. The novelty and contributions are somewhat limited and weak. The techniques, i.e., MLM and UDA for addressing the domain gaps are widely used in transfer learning/domain adaptation problems. The combination of them to resolve the same domain shift problem in a new cross-lingual setting is somewhat straight-forward. So, compared with the previous domain adaptation problems, what\u2019s the new things or differences for the domain mismatch in the XLDC task? \n2. For the experiments, the baselines are very simple and not competitive.\n3. Also, there exist some typos in the paper, e.g., in section 3.3 MLM\u201cA standard practice is to to further pre-train\u201d, etc. "}