{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper aims to solve the multi-domain multi-modal image to image translation problem by categorizing domains into hierarchies. The paper models each domain as a gaussian distribution that is a subset of its parent domain, which is also a gaussian distribution. Borrowing ideas from Athiwaratkun & Wilson (2018), it measures the difference in probability densities and optimizes a thresholded version of the KL divergence of two densities, so that if child *f* is nested in parent *g*, then KL(f||g) is small but KL(g||f) is big.  By comparing with StarGAN (multi-domain) and MUNIT(multi-modal) baselines, the paper shows comparable performance while taking the best of both worlds.\n\nAlthough it is attractive that the paper has one generator and one encoder for multiple domains and supports multi-modal outputs, there are some unanswered key parts that made me hesitant to accept the paper. \n1. How widely applicable is the hierarchical approach? e.g. Gender and hair color may not fit the assumption that one is another's subset. By putting gender as the top-most node, you treat dark haired men and dark haired women as two separate groups where we maximize their thresholded KL divergence. The resulting distribution may not be a faithful representation of the real world. \n2. From fig. 12 it seems that the choice of alpha can vary widely between 50 and 2000 and still have similar IS metrics. On the other hand the choice of m seems to have a clearer effect. Does that suggest KL(child||parent) does not matter as much compared to KL(parent||child)? \n3. It is unclear what hyperparameters was used to train the baseline models. Did you use the default hyperparameters in the paper or did you tune them? The MUNIT on CelebA (fig. 3) looks a bit unnatural. \n4. Were metrics other than KL considered? e.g. you mentioned it is hard to compute the KL between two mixed gaussian distributions. Perhaps sliced wasserstein distance (e.g. Deshpande et al. 2018) or some other measurement would be a good alternative? \n\nMinor points that did not affect the rating:\n1. In TLDR \"controled\" -> controlled\n2. In section 3.1 the paper states \"We further assume G and E are deterministic and mutually inverse\". This is inaccurate since neither Karras et al. (2018) nor Huang et al. (2018) gives G and E that are exact inverses of each other. It mislead me to think you used Glow from Kingma & Prafulla 2018."}