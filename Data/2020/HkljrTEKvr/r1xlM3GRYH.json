{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method, Hierarchical image-to-image translation (HIT), enabling both multi-domain and multimodal image translation on multiple category domains. To this end, the proposed method utilizes the nested distributions based on a hierarchical structure that exists among image categories. (e.g., Dogs include Husky). The method attempts to model all category domains in a given hierarchy structure as nested Gaussian distributions in a common space. They perform qualitative and quantitative experiments on several categories and attributes for validation of the proposed method.\n\nThe idea of using the hierarchical structure among image categories is interesting. However, the technical novelty is marginal, considering that both the nested distribution loss and the hierarchical classification loss are based on previous studies [1,2].\n\nAdditionally, in Figure 4, the hierarchy on ImageNet dataset is reasonable, but in Figure 6, the hierarchy on CelebA dataset seems to be arbitrary and subjective. Also, the model performance may not be reliable nor robust against the hierarchy changes. In addition, the quality of generated images is somewhat low.  \n\nA few more questions include: \n1)\tFrom Figures 3 and 6, I think that this model mainly focuses on the skin and the hair color. I am curious about the results on other attributes. \n\n2)\tI think that the paper needs additional baselines to compare the proposed model against, e.g., [3] on ImageNet and CelebA datasets. \n\n3)\tIn order to check whether the inclusion relation has been properly learned, showing 2D embedding visualization of data items by t-SNE would make the paper convincing.\n\n\n[1] Choi et al., \u201cStarGAN : Unified Generative Adversarial Networks for Muli-Domain Image-to-Image Translation.\u201d CVPR\u201918\n[2] Ben et al., \u201cHierarchical Density Order Embeddings.\u201d ICLR\u201918\n[3] Cho et al., \u201cImage-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation.\u201d CVPR\u201919\n"}