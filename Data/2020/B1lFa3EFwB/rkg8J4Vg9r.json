{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "** Summary\nThe paper studies the problem of representation learning under invariance constraints (i.e., the representation should be invariant wrt some attributes). The authors first review the adversarial invariance induction (AII) approach and they point out its limitations and then they propose a novel variant, which introduces an explicit regularization to minimize pairwise divergence (i.e., different attributes should lead to the same conditional distribution over the learned representation). The authors support the modified objective function both from a formal point of view and with an extensive empirical validation\n\n** Evaluation\nThe paper lies a bit outside my area of expertise. Although the paper tries to capture intrinsic limitations of the AII approach and build a more stable algorithm, my impression is that too many elements in the discussion and derivation remain too vague at the current stage and they would require better and clearer explanation.\n\nDetailed comments:\n1- In many parts of the paper the notation is not very rigorous and sometimes it may create confusion. In general, the writing needs to be improved in many parts:\n- In eq. 1, it is not explained what the expectation is wrt. It should be x drawn from p(x). But it would be better to make it explicit.\n- The setting defined in sect.2 should be more rigorous: it is not clear what y is and what are the attributes we would like to be invariant for. This notation is not fully consistent with eq.1.\n- In the first paragraph of Sect.3, it would be very helpful to have a more complete sketch of the algorithm. In general, you mention in the Sect. 2 that you focus on the supervised case, but then it is not clear whether this is the case across the paper.\n2- While the intuition behind using the divergence is sound, as it is mentioned, it seems to suffer from the same issue as the original AII: minimizing a lower bound does not guarantee that we are minimizing the actual objective. Having the support of 0, does not seem to make it much more sensible.\n3- As the description of the algorithm advances in Sect.4.2, it is clear that many additional choices need to be made in order to have a full workable algorithm (e.g., how to estimate q_\\phi^i). In the paper, the actual algorithm is never reported in detail and this makes the experiments very hard to reproduce in my opinion.\n4- At the best of my understanding, the algorithm may become more and more intractable as the number of attribute values grows. In fact, you need to check the divergence for each pair of values and estimated distributions.\n5- The empirical analysis seems well executed and a good level of detail is reported on how the datasets are managed and the experiments are set up.\n\nMinor comments:\n- The caption of Fig.1 is not very clear. Many elements at this stage of the paper are not defined yet (e.g., \"our proposal minimize the proxy of divergence ...\").\n- p2: \"as the assumption of ... is rarely holds\", remove \"is\"\n- p3: \"(encoder) that parameterized\", remove \"that\"\n- last paragraph of Sect.2 is very confusing.\n- p4: \"updation\" is not a word\n- p4: \"In general, minimizing the upper bound...\" does not seem correct.\n- p4: \"even such a simple case\" -> \"even in such a simple case\"\n- Sect 4.1 \"the above theoretical\", I would not really say there is much theory behind the analysis in the previous section.\n- p5 \"an ttribute\" -> \"an attribute\"\n- p6 \"the average discriminator's perception\" what is the perception?"}