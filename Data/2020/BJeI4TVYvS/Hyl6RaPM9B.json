{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a Neural Networks based algorithm for the two-sample test problem, which is one of the most fundamental problem in statistics and machine learning. Given independent samples from two distributions p and q, the goal is to tell whether p = q or p = q. The test is usually based on some test statistic T and a threshold T_0 where Pr(T > T_0 | p = q) < a (a is the tolerance for the false alarm rate and usually set to be 0.05 for statistical significance). The decision rule is simply testing whether T > T_0 holds. \n\nIn this paper, the authors propose a test statistic based on the difference of the sums of output logit s(the difference between the output neurons of the last layer) of a trained neural network classifier on both sample sets. The training of the network is done using half of the samples and the threshold is set using a permutation test on the training set. The final decision is based on the outputs of the network on the other half of the data (test set). The paper shows that the test is consistent asymptotically if the network provides a good approximation for the JL divergence between the two distributions, which connects recent advances in network approximation theory.\n\nIn the experiment part, the paper considers synthetic experiments using 1-d and 2-d Gaussian distribution and real-data experiments using the MNIST dataset to compare the proposed method with other methods including net-acc (using the difference in the output probabilities) and variance kinds of Gaussian kernel MMD methods. \n\nThe paper presents some nice ideas, but overall I find the theory part not deep and fail to provide enough insights. In the training phase of the network, not only the number of the parameters in the network matters, but the number of training samples is also crucial for the network to approximate the optimal function. It would be nice to have some discussions on this. \n\nI also find the experiment part not convincing. In the experiment part, I think the fairest comparison to Gaussian kernel MMD is to use gmmd+ and gmmd++. For the 1-d Gaussian case (figure 2), the experiments show that the proposed test is not always better than these two in some distribution cases. In the 2-d case, the experiment is only done for one distribution. For real dataset (MNIST, figure 5),  results for gmmd+ and gmmd++ are not presented. I would expect more thorough experiments on different distributions to make a more convincing statement. \n\nSo overall I would not recommend this paper for admission.\n\nMinor comments:\nSection 2.2, line 3: y_i's should have label 1."}