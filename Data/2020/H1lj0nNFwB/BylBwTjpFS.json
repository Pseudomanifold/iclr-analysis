{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies the role of depth on incremental learning in several toy models for neural networks. In particular, they show that in these models, deep models require polynomially small initializations to exhibit incremental learning than shallow models. The paper is well written, and I think there are several interesting contributions.\n\nThe authors contribute analysis for non-asymptotically small initializations, and study an interesting role of depth in how small this initialization must be. Furthermore, they extend their results to several other models including matrix sensing, linear convnets, and classification.\n\nI think nonetheless the paper suffers from a few issues. Some very important ones.\n\n1) The authors study the role of depth on incremental learning, and exhibit how several models theoretically have this property. However, they do not study how incremental learning drives generalization. In the entire paper, the gradient flow is with respect to the *expected* loss, rather than the empirical loss. The research program of incremental learning for deep neural networks would show something like \"Incremental learning exists when minimizing empirical loss\", \"Incremental learning and early stopping imply certain properties (like low capacity) on the resulting neural network\" and \"These properties imply low generalization error\". However, the fact that the authors' models minimize the expected loss altogether a priori rules out the direct applicability of this result to explaining generalization. That is OK, in the sense that one could aim for these results to be modified and applied with empirical losses, and then a separate line of research could study how incremental learning bounds generalization error.\nIn this sense, I think the authors should take out the \"how incremental learning drives generalization\" since there is no study on generalization whatsoever, just how depth plays a role in incremental learning. An alternative title could be \"How depth drives incremental learning.\" or something like that.\n\n2) Another point is that all these models are very toy and mostly linear. That is OK again, but the introduction overclaims in this respect. The sentences \"we characterize the effect of the model's depth [...] showing how deeper models allow for incremental learning in larger (realistic) initialization scales.\" and \"Once incremental learning has been defined and characterized for the toy model, we generalize our results theoretically and empirically for larger models\". This makes it seem that results apply to realistic settings, which is really far from true. I'm not expecting realistic results, this is a nascent theory, but I am expected the claims made to be validated and not misleading.\n\n3) In section 2.2, sigma(t) for N-> \\infty is undefined, and the proof for this result is missing (only for finite N appears). In particular, it is not clear if sigma(t) for N -> \\infty is obtained by a) taking limit N -> \\infty in the ODE of equation (8), and then finding the solution of this limiting ODE, or b) finding \\sigma(t) for equation (8) on finite N and then taking limit N -> \\infty of the solution. I.e. there are two potentially different ways to define \\sigma(t) for N -> \\infty which are solving the ODE and then taking limit or taking limit and then solving the limiting ODE. The definition of \\sigma(t) for N -> \\infty is completely missing so I have no way to assess the validity of this result.\n\nA small pet peeve: when writing math, try to avoid using symbols like \\forall and \\exists unless you're writing a logic paper. Instead, try to write equation 2 like $$\\sigma_i = w_i^N \\quad \\text{for all i = 1, \\dots, d} $$, which reads a lot nicer. Also, avoid assigning equation numbers to equations you never reference."}