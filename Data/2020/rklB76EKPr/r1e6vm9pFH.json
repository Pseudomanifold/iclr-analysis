{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper studies the relationship between gradient clipping in stochastic gradient descent and robustness to label noise. Theoretical results show that gradient clipping in general is not robust to symmetric label noise. The paper then proposes a variant of gradient clipping (cl-clipping) that induces label noise robustness. Experiments support these claims on synthetic datasets and typical classification benchmarks.\n\n[Decision]\nThe first contribution, that gradient clipping does not induce robustness to label noise, is an important negative result given the prominence of gradient clipping and datasets with noisy labels. The second contribution, cl-clipping, amounts to minimizing a non-convex loss with saturating regions but, as far as I know, these properties are necessary for robustness to label noise. Theoretical results are limited to SGD with mini-batch size 1 but the insights carry over to larger mini-batches in the experiments. Overall, I recommend acceptance.\n\n[Comments]\nThe parameter tau controls robustness, and a higher noise level requires a higher tau. There is little discussion on how this parameter is chosen in the experiments. On the synthetic dataset, the Huberized loss uses tau=1 and the partially Huberized loss uses tau=2. How are these values chosen? Did the authors observe a U-shaped curve when sweeping over tau? On the real-world datasets, tau is fixed for each method across different noise levels. Does this mean that a single value of tau worked best regardless of the noise level, or was it tuned for a particular noise level?\n\nProposition 4 shows that symmetric noise breaks down the clipping method in Eq (7) which can be seen as a special case of gradient clipping. I might be missing something here, but it is not obvious to me that, when the norm of x is constant across the samples, Eq (7) is equal to gradient clipping.\n"}