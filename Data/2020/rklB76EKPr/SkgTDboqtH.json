{"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studied a fundamental problem in robust machine learning, that is, can gradient clipping mitigate label noise? The paper is well written, clearly motivated, highly novel and significant not only in a theoretical sense but also in a practical sense.\n\nAs argued in the abstract, gradient clipping is a widely-used technique which is generally motivated from the OPTIMIZATION point of view. In this paper, the authors proposed an entirely new motivation of gradient clipping from the ROBUSTNESS point of view, since intuitively it should be able to mitigate label noise. Surprisingly, the authors proved that for some simple binary classification with label noise, standard gradient clipping does not provide robustness; on the other hand, a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function.\n\nThe major contributions were well summarized in the bottom of page 1, the related work was discussed in sec 4.3, and the caveats of this new methodology were also given in sec 4.4. Note that the proposed composite loss-based gradient clipping is applicable even on top of existing noise-robust losses, for example, the generalized cross-entropy loss, and this serves as a convincing demonstration of the great significance of the paper. Actually, the paper is full of insights, and I really enjoyed reviewing it.\n\nI have a few questions on Tables 1 and 2. Table 1 said the standard gradient clipping is not robust according to Proposition 4. However, Proposition 4 is for the loss-based gradient clipping in (7) rather than the standard gradient clipping in (6). Perhaps I have missed something, but why the non-robustness of (7) can imply the non-robustness of (6)?\n\nIn Table 2, Linear loss on MNIST, the test accuracy was 9.6 when rho=0.6. Is this a typo? The accuracy was still 78.8 when rho=0.4. Moreover, the authors explained why the theoretically grounded linear loss performed so badly, that is, the optimization was so difficult. How about CE+clipping loss on CIFAR-100? The standard gradient clipping was also harmful here, even when there was no label noise at all. Was this due to bad optimization or robustness (or both)? \n\nSince distributionally robust supervised learning is a future direction to go, I think the authors may be interested in a thought-provoking paper:\nW. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers? ICML 2018."}