{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# overview\n\nThe paper proposes a hierarchical memory network for abstractive summarization. The claimed innovation is to add a memory component to the regular sequence-to-sequence model. With all the additional pieces, such as having the ability to use words in the document as in pointer networks and training with ROUGE score as rewards, the paper achieves strong performance on CNN/Daily Mail data set.\n\nI am giving the paper a score 3. I appreciate the engineering effort and the strong performance reported in the paper. However, I am afraid that the presentation of the paper is not clear enough that others can reproduce the results without trouble. In addition, the design decisions are based on hypotheses that are either not proven or false, and I believe the paper should make it clear that those hypotheses are speculations, and we are responsible to stop misinformation from spreading.\n\nThe feedforward process in section 3 has many errors, which makes it impossible to reproduce the model. (A detailed list of the errors is given below.) The representation of the input and output and the loss for training the network are missing. Besides, there is no mentioning of the network's size and number of layers. The largest gain (by looking at table 1) comes from using the lemmas and pos tags. However, the details have been omitted in the paper.\n\nThe motivation/hypotheses for many of the design decisions, as stated throughout the paper, are too strong. The fact that the model works well might have nothing to do with those motivations. (A detailed list is given below.) An even more severe problem is that the biggest gain in the paper actually comes from using lemmas and pos tags, which has nothing to do with all the motivations and other design decisions. It is likely that other models can get similar performance gain by incorporating lemmas and pos tags.\n\n\n# a list of unproven speculations\n\n... either hierarchical attention mechanism are too sparse using hard attention or noisy using soft attentions.\n--> evidence?\n\nLSTMs ... often fail to capture long-term dependencies while modeling sequences.\n--> evidence? this is not a proven fact.\n\nTo address these issues, we have adapted Neural Semantic Encoders (NES) ...\n--> how so? i don't see how adapting NES fixes the problem.\n\nEncoder-decoder models have proved effective for short sequence tasks such as machine translation where the length of a sequence is less than 120 tokens. However, in text summarization, the length of the sequences vary from 400 to 800 tokens, and modeling long-term dependencies becomes increasingly difficult.\n--> evidence that LSTMs can remember 120 tokens but not 800 tokens?\n\nThe use of two LSTMs separately for words and sentences improves the ability of the model to retain its memory for longer sequences.\n--> evidence? we should not correlate how much LSTMs remember with task performance.\n\n..., just a dot-product attention mechanism is too simplistic for text summarization.\n--> how so? what does it mean to be too simplistic?\n\nHumans first form an abstractive representation of what they want to say and then try to put it into words while communicating.\n--> cite?\n\nWhen humans read a document, we organize it in terms of word semantics followed by sentence semantics and then document semantics.\n--> cite?\n\nReplacing Multi-Layered Perceptron (MLP) in the NSE with an LSTM further improved the performance because it remembers what was previously composed and facilitates the composition of novel words.\n--> evidence that the LSTM remembers and facilitates composition?\n\n\n# problems in the equations\n\nequation (4)\n--> r does not appear on the right hand side\n\nequation (7)\n--> k does not appear on the left hand side\n\nequation (11)\n--> might be useful to say that w now belongs to a larger set that is the union of the vocabulary set and the words appeared in the documents\n\nequation (15), (16)\n--> r does not appear on the right hand side\n\nequation (17)\n--> r does not appear on the left hand side\n\n\n# minor errors\n\nFigure 1\n--> the figure is not being referred anymore in subsequent sections. besides, we don't even know if the figure is what is actually happening inside the model.\n\nExtractive approaches select sentences ... and groups them ...\n--> group\n\n3.1 neural semantic encoder:\n--> remove colon at the end of the subsection\n\n..., which is a weighted combination ... forms a highlight\n--> ungrammatical sentence\n\nhttps://stackoverflow.com/questions ...\n--> misplaced link?\n\n... dropout, and L_2 penalty ... with a drastically increased training time.\n--> slow in terms of convergence rate? dropout and L_2 are not computationally expensive. if it is indeed a convergence issue, then more tuning is required?\n"}