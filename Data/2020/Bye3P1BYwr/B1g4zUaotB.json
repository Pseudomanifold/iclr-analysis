{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper the authors propose a framework for anomaly detection. The method is based on autoencoders and reconstruction error, but instead of training the autoencoder using all the data-points, the method iteratively uses some form of clustering to determine the points which presumably belong to the normal set, and uses them for training the autoencoder. This helps make the method robust when the portion of anomalous data-points is high.\n\nThe paper is well-written and clear and the proposed architecture is novel to my knowledge. Nevertheless, I have the following concerns about the paper. Given clarifications in the author response, I would be willing to increase the score. \n\n- In terms of novelty, separating the dataset into normal + outliers/noise is not novel (Zhou 2017 cited in the paper). The novel part here is perhaps using variance and clustering for making the separation. However, using variance is not well motivated and it is referred to Figure 4, in which the argument is not clear. Similarly, why the reconstruction error is included in the latent representation (eq 5) is not clear. \n\n- Given the similarity of the idea to Zhou 2017, the comparison seems important (if the code is available), and also proper discussion of such similar works is required, which currently is not presented.\n\n- How is the performance affected by very low ratio of anomalies? This can be shown by including %2,%3 of anomalies in Table 2. \n\n- The sensitivity of the results to the choice of hyper-parameters: p0, p, and r is not clear, and how these parameters are chosen is not discussed. It would be interesting to see how the performance is affected by different choices of the hyper-parameters."}