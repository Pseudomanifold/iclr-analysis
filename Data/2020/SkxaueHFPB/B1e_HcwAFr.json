{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a novel training methodology for GANs to improve stability. The resulting regularization, termed implicit competitive regularization, updates the parameters of both the generator and the discriminator to be robust to one another. A framework for practical application of this approach is described -- this is done by a local Taylor approximation of the loss and updating each model\u2019s parameters to this approximate model\u2019s nash equilibrium.  The method is shown to prevent overfitting and produce high-performing models with consistent training.\n\nThe approach and insights are reasonable and the problem is worthwhile to approach. The method is clear and the associated code is appreciated. The results are interesting in terms of describing the ICR property and demonstrating its performance. \n\nThe paper gives background and intuition to solidifying the CGD update. Are there additional algorithmic approaches that are possible and potentially more efficient with this understanding in mind? This I believe would help solidify the paper and build beyond CGD.\n\nSome additional results that could clarify the benefits:\n- A primary contribution of the training approach is training consistency. The distribution over many training runs should be provided in figures.\n- Clearly due to the additional gradient calls the approach is computationally slower, as shown in Figure 4. If each approach is trained for the same amount of time, how does the performance compare?\n- One may expect that the update may result in more conservative updates and thus potentially lower-performing policies in the limit. If there iterations were instead log-scale to show performance in high training iterations, is there any loss of performance in top-performing runs? \n- Could a similar approach be used to allow safe gradient updates according to a risk over the opponent\u2019s possible updates, e.g., via CVar? This may also be a stable training procedure as well with less conservatism.  \n\nThe paper should be proofread, there are several minor typos throughout, e.g.:\n- \u201cgenerators producing that produce good\u201d -> generators that produce good\n- \u201cThis game is very similar similar\u201d -> repeated word\n- \u201cGAN trainin.\u201d\n"}