{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an implicit update scheme for the back propagation a anlgorithm. \nThe idea is quite simple and is based on proximal mappings that lead to implicit update.\nSpecifically, every update in the back propagation algorithm is being replaced by an implicit update except for the intermediate parameters that receive a \"semi-implicit\" update.\n\nThe idea is reasonable and seems to lead to good performance. This is more-or-less expected thanks to the superior performance of implicit updates in general. So, it's good that the authors could make this work in the context of deep nets as well. Here are some more critical thoughts about the paper:\n\n\n1) There is not much theoretical justification about the idea in the paper. Proposition 1 is a simple argument about the fixed point of the procedure. The argument could be made more rigorous, right now it is a bit of a sketch. \nApart from Proposition 1, there is no more theory offered. The authors could appeal in the theory of implicit SGD for that, e.g., [1,2,3,4]. This theory suggests a lot of stability properties for the implicit SGD update of Equation (27).\n\n2) Somewhat related to (1), the authors could make a more clear connection to prior work. \nFor example, there is not mention of a very similar idea of \"implicit back propagation\" [5]. \nAlso the literature in implicit SGD procedures is highly relevant.\n\n3) Can we explain the results in Table 1 theoretically?\n\n\n\n[1] Bertsekas, \"Incremental proximal methods for large scale convex optimization. Mathematical\nprogramming\", 2011\n[2] Kulis and Bartlett, \"Implicit online learning\", 2010\n[3] Toulis and Airoldi, \"Asymptotic and finite-sample properties of estimators\nbased on stochastic gradients\", 2017\n[4] Toulis, Airoldi, Rennie, \"Statistical analysis of stochastic gradient methods for generalized linear models\", 2014\n[5] Fagan and Iyengar, \"Robust Implicit Backpropagation\", 2018"}