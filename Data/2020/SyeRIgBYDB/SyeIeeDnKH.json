{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper introduces a novel algorithm for computing update directions for neural network's weights.\nThe algorithm consists of the modified backpropagation procedure where a layer's error is computed using implicitly-updated weights.\n\nThe proposed idea is interesting, but its presentation and evaluation could be significantly improved.\nFirst, it is not very clear what motivates the exact form of Semi-implicit BP.\nSecond, I find the notation a bit cumbersome, especially intermediate ^{k+1/2} updates. \nI also suspect that eq. 9 contains an error, probably the l.h.s. of the first part should be \\delta_i^{k+1}?\nAlgorithm 1 is not very helpful because only the forward pass is explained in detail and a reader must refer to the main text to understand the backward pass.\n\nThe experimental evaluation also raises a number of questions.\n1) Why did authors chose only one value of the learning rate and the lambda hyperparameter? A more appropriate comparison would require slightly more extensive hyperparameter search as it may well be that ProxBP would work better with different values.\n2) It is also unclear if 5 CG iterations is enough to solve the intermediate problem. Also all convergence guarantees are only provided for the exact implicit update, so at least one should ensure it is computed well enough.\n3) Isn't it suspucious that ProxBP performed so bad compared to other methods on MNIST? \n4) Should not the set of baselines include more advanced optimizers such as RMSProp, Adam etc? They don't seem to add more computational burden than Semi-implicit BP.\n\nI also think it is worth discussing/investigating if the obtained update directions can be used in other gradient-based optimizers instead of pure gradients and if it can have any advantages.\n\nOverall, it does not feel to me that the paper is ready for publication.\n"}