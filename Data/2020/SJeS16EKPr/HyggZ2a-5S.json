{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an approach to find a map between two feature spaces to maximize correlation between them and to use the resulting map for inference. A theoretical exposition is given and some empirical results are provided showing that the approach speeds up convergence on supervised MNIST and can be used for image completion (again on MNIST).\n\nThe paper should be rejected for the following reasons. First, the approach looks very similar to deep CCA, but the connection is never mentioned. This connection needs to be clarified. The objective function needs to be clearly stated and related to the loss function in eq. (7). In particular, I would suggest to give a clear definition of the problem before delving into the theory in section 2. In its current version, it is difficult to assess how the parts of section 2 relate to the overall objective. The paper severely lacks in relation to relevant related work. Half(!) of the 14 referenced papers are by the author himself. This can be verified since the double blind review process is compromised as the paper links to code in the author\u2019s public github account. Finally, the empirical results are quite incomplete. It is not clear how the results compare to generative methods like VAEs, which are referenced as a motivation for this work in this work.\n\nThe improvement in convergence from RFA for supervised learning is interesting and this aspect deserves more analysis. It would be useful to look at the total amount of computation required to reach a given loss. I also wonder how this differs from simply mapping the output of the first network to a low-rank space via PCA. Is the dual-view really necessary in this case since the information content in the label space must be very limited, beyond simple class balance statistics?\n"}