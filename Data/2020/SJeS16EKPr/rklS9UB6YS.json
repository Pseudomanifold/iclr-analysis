{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper considers how to learn correlations between two spaces, e.g., input/output, in order to generate data in one space conditioned on values from the other. This is performed by modeling features with neural networks and optimizing an objective function that maximizes a measure of correlation between the features versus learning a generative model such as a CVAE. Some illustrative examples using MNIST are provided.\n\nMy decision is to reject. I think there is value in the approach, but it is hard to see clearly at the moment given that the exposition is difficult to follow and the experiments aren't very compelling. If these issues could be addressed (concrete suggestions below), and some of the follow-on work in the last section could be performed, I think there could be a pretty interesting contribution here.\n\n***\n\nDecision-related suggestions/questions:\n\n* Include more datasets in the experimental section. The second sentence of the introduction lists possibilities such as time series and multi-modalities that I would have been very interesting.\n\n* The first claim that there is no tunable variable in the objective function is a little hard to parse. Clearly, the rank of the low-rank approximation must be set, and the features of the two spaces need to be learned. Some clarification here would be helpful. \n\n* There are a number of unfortunate typos/grammar issues/presentation choices that really impact clarity. Some examples:\n\t* In the third paragraph under theory, \"...linear spaces spanned by the probability distributions...\" should probably be \"...linear spaces spanned by all probability distributions...\" (?)\n\t* The following sentence is a run-on. \n\t* In (8), occurrences of g_*(x_n) should be replaced with g_*(y_n).\n\t* The replacement of the (low) rank symbol, k_0, with the sample size symbol, n, in the second paragraph of section 4.1.\n\t* Introducing a \"Bayesian estimator for an l^2 distance\" w/o explanation. What does this mean?\n\n* How should the low-rank parameter k_0 be selected generally given that the singular value distribution may not always be useful in selecting it?\n\n* Can anything be said quantitatively or qualitatively about the sample complexity required to estimate the matrices of (8) well enough to estimate the features?\n\n* Is there an interpretation for why both spaces require the same feature dimension, k_0?\n\n***\n\nComments not related to decision:\n\n* It is generally good to avoid sweeping statements such as the first sentence of the introduction. Perhaps consider replacing with a simple statement on the intended goal of the paper: \"...produce a useful model of correlations... for the task of data generation...\"\n\n* Consider placing a concrete, motivating example prior to the theory section as it is hard to digest (from an ML perspective) without a clear context. The analytical example with the Gaussian from the supplementary material is one option.\n\n* The last statement of the paragraph under (3) needs a reference.\n\n* It seems strange to have the supervised learning experiment of 4.1 as the first experimental result of the paper since it is an unintended and unexplained side-effect of the approach. Also, the claim of \"faster convergence\" should be demonstrated in wall-clock time."}