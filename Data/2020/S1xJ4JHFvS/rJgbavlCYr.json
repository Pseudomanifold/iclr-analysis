{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper attempts to remove the use of the second moment in Adam in order to improve the generalization ability of Adam. \n- Apriori, it is not clear why removing the second moment is important. Does it improve the generalization or decrease the runtime substantially?\n- Please clarify how our method compares against Yogi (Adaptive Methods for Nonconvex Optimization, Zaheer' 2018) and the more recent RADAM (ON THE VARIANCE OF THE ADAPTIVE LEARNING\nRATE AND BEYOND, Liu 2019) , both theoretically and empirically. \n- The paper is meandering and confusing. State your update/algorithm and then explain how it compares against the other methods. Besides, there might be technical problems with it. \n\nSee the detailed review below:\n- Section 1: \"the generalization results (of adaptive methods) cannot be as good as SGD\". Please cite the relevant papers, (for example, Wilson. 2017)  that show this empirically. \n-  Section 1: \"the proposed algorithm outperforms Adam in convergence speed\" - Please clarify what \"convergence speed\" refers to. Is it the number of gradient evaluations, the rate of convergence or the wall-clock time. Please state how did you conclude this. \n- Section 2: The update rule of Adagrad is incorrect. The step-size is constant \\alpha and it is decreased over time because of the v_t term. \n- Section 3: There is no guarantee on the approximation in Equation 5. Young's inequality and the resulting upper bound can be quite loose. \n- Section 3: In general, it is not possible to have an update that decreases the loss on the current batch, but does not increase the previous batches loss. It is always possible to construct a counter-example to this. \n- \"In practice, the computational cost of computing\" the gradient for all i is expensive. Indeed, this is batch gradient descent. I am not sure how this is relevant to the discussion in the paper. \n- Cite and compare against the variance reduced methods (Stochastic Average Gradient, Schmidt, 2013; SVRG, Johnson, 2013) as these try to \"approximate\" the full gradient in order to decrease the variance. \n- The derivation/formulation of Equation 8 is not clear to me. Why is the \\hat{m} normalized? \n- In algorithm 1, it seems you need to choose the sequence of step-sizes \\alpha_t and \\beta_t. How is this an adaptive method then? How are these sequences chosen theoretically and practically? Please clarify this. \n- Section 4: Please compare the resulting regret bound to that of Adam, Adagrad and AMSgrad. Why does \\alpha_t = O(1/t)? If we have to decrease the step-size according to a sequence, why should I not use standard SGD?\n- Section 5: \"we decay the learning rate by 0.1 every 50 epochs\" This is not aligned with either the theory or the algorithm you proposed. \n "}