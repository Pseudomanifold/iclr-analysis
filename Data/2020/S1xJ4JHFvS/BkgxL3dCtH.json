{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Balancing the generalization and convergence speed is an important direction in deep learning. This paper propose a new method to balance them. That is very interesting to me. However, I have several concerns as follows.\n1. I cannot agree that \"fewer hyper-parameters\" in Algorithm 1. The authors should provide more materials to support this claim, such as a comparison table of different mathods. \n2. In Theorem 42, how to set the parameter \\epsilon to obtain the conclusion.\n3. The authors should provide more comparison of the theoretical results of different SGD algorithms (such Adam, RMSProp ...)\nSome minor issues:\n1. The conditions of formulation of Eq. (6) should be placed together with Eq. (6).\n2. What is the definition to c_2,0, c_2,1 .... It is not clear to readers.\n3. The presentation and structure of the paper should be improved further, such as adding several subsection in Sec. 3, so that readers could easily follow the main idea of this paper."}