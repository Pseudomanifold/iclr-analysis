{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes new stochastic optimization methods to achieve a fast convergence of adaptive SGD and preserve the generalization ability of SGD. The idea is to let the search direction opposite to the gradient at the current batch of examples and a bit orthogonal to previous batch of examples. The algorithm is easy to implement and backed up with regret bounds. Several experimental results are also reported to verify the effectiveness of the algorithm.\n\nThe regret bound in Theorem 4.2. is not quite satisfactory. For example, Adagrad in Duchi et al (2011) can achieve the regret bound $O(\\sum_{i=1}^{d}\\|g_{1:T,i}\\|_2)$. However, there is an additional factor of $\\sqrt{d}$ in the regret boun in Theorem 4.2, which is not appealing in high dimensional problems. The regret analysis follows largely from standard arguments.\n\nI can not follow the last identity of (10). It should not hold since there is a missing $\\epsilon$ there.\n\nIn both eq (14) and (15), it is not clear to me why the authors divided the summation into two parts. Also, $\\sqrt{\\sum_{t=1}^{T}\\frac{1}{\\sqrt{t}}}$ should be $\\sqrt{\\sum_{t=1}^{T}\\frac{1}{t}}$ there."}