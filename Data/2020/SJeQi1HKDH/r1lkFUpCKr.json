{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a new way to incentivize diverse policy learning in RL agents: the key idea is that each agent receives an implicit negative reward (in the form of an early episode termination signal) from previous agents when an episode begins to resemble prior agents too much (as measured by the total variational distance measure between the two policy outputs). \n\nResults on three Mujoco tasks are mixed: when PPO is combined with the proposed objective for training diverse policies, it results in very strong performance boosts on Hopper and HalfCheetah, but falls significantly short of standard PPO on Walker 2D. I would have liked to see a deeper analysis of what makes the approach work in some environments and not in others. \n\nExperimental comparisons in the paper are only against alternative approaches to optimize the same diversity objective as the proposed approach (with weighted sum of rewards (WSR) or task novel bisection(TNB)). Given that this notion of diversity is itself being claimed as a contribution, I would expect to see comparisons against prior methods, such as in DIAYN. There are other methods that have been proposed before in similar spirit to induce diversity in the policies learned. Aside from the evolutionary approaches covered in related work, within RL too, there have been methods such as the max-entropy method proposed in Eysenbach et al, \"Diversity is All You Need...\". These methods, evolutionary and RL, could be compared against to make a more convincing experimental case for the proposed approach.\n\nThe experimental setting is also not fully clear to me: throughout experiments, are the diversity methods being evaluated for the average performance over all the policies learned in sequence to be different from prior policies? Or only the performance of the last policy? Related, I would be curious to know, if K policies are trained, the reward vs the training order k of the K policies. This is close to, but not identical to the study in Fig 4, to my understanding.\n\nAside from the above points being unclear, the paper in general could overall be better presented. While I am not an expert in this area, I would still expect to be able to understand and evaluate the paper better than I did. \n- Sec 3.1 makes a big deal of metric distance, but never quite explains how this is key to the method.\n- The exact baselines used in experiments are unhelpfully labeled \"TNB\" (with no nearby expansion) and \"weighted sum of rewards (WSR)\", with further description moved to appendix. In general, there are a few too many references to appendices.\n- The results in Fig 2 are difficult to assess for diversity, and this is also true for the video in the authors' comment.\n- There is an odd leap in the paper above Eq 7, where it claims that \"social uniqueness motivates people in passive ways\", which therefore suggests that \"it plays more like a constraint than an additional target\". \n- Sec 5.1 at one point points to Table 1 for \"detailed comparison on task related rewards\" but says nothing about any important conclusions from the table.\n- There are grammar errors throughout. "}