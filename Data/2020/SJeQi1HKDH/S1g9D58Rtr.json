{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method for learning diverse policies in RL environments, with the ultimate goal of increasing reward. The paper develops a novel method, called interior policy differentiation (IPD), that constrains trained policy to be sufficiently different from one another. They test on 3 Mujoco domains, showing improved diversity in all of them and improved performance in 2 of them.\n\nOverall, this paper is very well executed. The explanation of the method is thorough, and the paper is well-written and polished. I like the idea of enforcing a constraint on the policy diversity via manipulating the transitions that the agents can learn from.  The experiments section compares to two other methods of increasing policy diversity and IPD outperforms both of them. I think this is a solid contribution to the literature on improving policy diversity.\n\nThat being said, I have some concerns about the paper:\n1) The motivation for explicitly encouraging diverse policies is a bit confusing, and isn\u2019t very convincing. The paper draws inspiration from social influence in animal society, and say they formulate social influence in RL. First, the term social influence already has an established meaning in RL (see e.g. Jaques et al. (2018)) and refers to agents explicitly influencing others in a causal way in a multi-agent environment. Second, I think calling policy diversity a form of social influence is a bit of a stretch (and anthropomorphizes the agents unnecessarily). I think the paper should scrap the \u2018social influence\u2019 angle and instead frame it as \u2018increasing policy diversity\u2019. \n\nThe paper also motivates itself in comparison to Heess et al. (2017), which uses a set of environments to get diverse policies. However, the goal of these works are different: in Heess et al., the goal is to train agents that can exhibit complex behaviours in relatively simple environments (the focus is more on complexity of behaviours vs. the fact that agents in the same environment learn diverse policies). In this work, the goal is not to develop any more complex policies, but to have different agents on the same task learn diverse policies (and since the experiments are in Mujoco, the degree of diversity is limited). Thus, while the works are related, I don\u2019t think the Heess et al. paper is good motivation for this work. \n\nI think the primary motivation that makes sense for explicitly encouraging diversity is to improve final performance on the task. Thus, I think it would be best for the paper to clarify the introduction by focusing on this. The paper could also give some reasons why having diverse policies is inherently a good thing (maybe for some applications with humans-in-the-loop it could be helpful?), but currently this is absent. \n\n2) Given that improving the final reward of an RL agent is the main goal, it\u2019s not clear that the experiments (in 3 simple Mujoco settings) are enough to show this reliably. Specifically, it\u2019s unclear whether encouraging diversity in this way will generalize to more complex tasks or domains (e.g. tasks in Mujoco with sparser reward, or environments with larger state spaces). It is possible that the success of the technique is most prevalent when there is only a small observation space. \n\n3) I\u2019d like to see more discussion / analysis of *why* we\u2019d expect diverse policies to lead to better rewards. In work on intrinsic motivation / curiosity for better exploration, it\u2019s clear that encouraging agents to visit unseen states will lead to a better exploration of the state space, and thus will make them more likely to stumble upon rare rewards. But is this also true for policy diversity? Currently, the paper speculates that encouraging diversity could help agents not all fall into the same failure mode. But I could also imagine that it could lead agents to avoid a successful strategy that another agent learned. For example, if a certain sequence of moves is necessary at the beginning to avoid termination, the first agent could find this sequence of moves, but the other agents might avoid this sequence for the sake of diversity (depending on the threshold). Does something like this happen in practice? In my opinion, the environments considered aren\u2019t rich enough to know. \n\n4) There are also some inconsistencies in results section. Specifically:\n- There seems to be a disagreement between the results in Table 1 (which uses 10 peers) and the ablation over number of peers in Figure 4, which shows that the performance with 10 agents is roughly the same as it is with 1 agent (and overall shows little positive trend between the number of peers and performance). \n- If \u2018success rate\u2019 means \u2018percentage of time beating average PPO policy\u2019, why does PPO sometimes get 100% in Table 1? \n\nGiven the concerns above, I\u2019d assess the paper as being borderline for accept. I\u2019m currently erring on the side of rejection, but I\u2019d consider changing my score if some of the above points are addressed. \n\n\nSmaller concerns and questions:\n- There are a couple of instances where I found the claims of the paper with respect to related work to be over-stated. For example:\n\u2018Yet designing a complex environment requires a huge amount of manual efforts\u2019 -> not necessarily. There is an initial engineering overhead, but it\u2019s possible to generate environments programmatically with different properties, resulting in different agent behaviours. \nAlso:\nOn the Task-Novelty Bisector method of (Zhang et al., 2019): \u2018the foundation of such joint optimization is not solid\u2019. This is given without any explanation --- how is it not solid?\n\n- In the TNB and WSR implementation, what metric is being used? Is it the same as is defined in Section 3?\n\n- It would be nice to have some videos of the agents behavior to be able to more easily assess the learned policy diversity. \n\n\nSmall fixes:\n\u2018and similar results can be get\u2019 -> and get similar results. \n"}