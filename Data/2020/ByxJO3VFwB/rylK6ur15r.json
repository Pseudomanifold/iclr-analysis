{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors show that parameters of a DNN do not satisfy the i.i.d. prior assumption and that neural layer activations considered as i.i.d. are not valid assumptions for all hidden layers of the network. One can therefore not rightfully use GPs to describe the network\u2019s hidden layers. The authors suggest formulating the neurons per layer as energy functions thereby rendering a hidden layer as a Gibbs distribution and the connection between adjacent hidden layers as a PoE model. \n\nThe paper is well written and well postulated. \n\n> Fig 4: What is the information presented by each neuron? How would this have looked with the i.i.d. prior in place.\n\n> There are places in the paper where one must refer to the supplementary, for example sections H and J with the simulations. Do consider moving these crucial sections to the main paper.\n\n> One recurring thought I had when the authors bring up Bayesian Hierarchical model, is that most of the BHMs rely on i.i.d assumptions both in the prior space and with the observations. How would you stand by your claim of explaining a DNN's layers to be modelled as a BHM? \n"}