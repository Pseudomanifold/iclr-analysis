{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #27", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nMain contribution of the paper\n- The paper argues that the base assumption, the i.i.d. of the activated elements (activations) in the hidden layers, the existing methods (lee.et.al 2018) hold is not convincible.\n- Instead, the author proposes a new way to probabilistically model the hidden layers, activations, and layer/layer connections.\n- Based on the probabilistic model, the paper proposes a new regularizer.\n\nMethods\n- The author argues that the activation is not iid by empirically showing that the trained MLP (in most cases) does not un-correlated.\n- The author proposes a new probabilistic model for MLP, and CNN assuming the Gibbs distribution to each activation and also assuming the product of expert (poE) model to explain the layer/layer relationship.\n- And according to their model, CNN will be explained by the MRF model.\n- The author proposes a regularization term regarding layer/layer connection.\n- They argue that the SGD training can be seen as a first-order approximation of the inference of the hidden activations in MLP.\n\nQuestions\n- See the Concerns\n\nStrongpoints\n- The probabilistic explanation of the MLP and the CNN seems novel and was interesting to the reviewer\n- The proposed explanation assumes a weaker condition compared to the existing methods.\n\nConcerns\n- The main concern is that the reviewer cannot fully convince that i.i.d. assumption is wrong. \nEven though the trained MLP does not support the i.i.d. condition, one can suppose that the reason would be the typical training method (SGD), just finding the local minima in a deterministic way.\nMaybe the proof in Appendix.G. supports the argument of the author, but the reviewer failed to clearly agree with the argument.\nA clear explanation regarding the issue would be required.\n- As far as the author understands, the paper proposes a probabilistic (Bayesian) model for explaining MLP, but it seems that they just used SGD for training the model. \nIn that case, the reviewer is little suspicious of the role of the proposed regularization in that the regularization comes from Bayesian formulation, but the model was trained in a deterministic way.\nThe reviewer wants to ask the author that \n(1) is it possible to infer the model in a Bayesian manner such as sampling?\n(2) Is there any justification for using SGD when conducting the experiments regarding the regularization? If it is related to Appendix.G, clearer explanation would be appreciated.\n- As far as the reviewer understands, the regularization deals with the practical part of the paper. It would be better to see the effect of the regularization of widely used networks such as small-layered ResNet or others.\nIf the proposed formulation has other practical strongpoints, it would be nice to clarify them.\n- The explanation using Gibbs distribution and PoE looks similar to RBM. The reviewer strongly wants a clear explanation of the difference and the strongpoints compared to RBM.\n\nConclusion\n- The author proposed a new probabilistic explanation of the neural network, which seems novel and worth reporting.\n- However, the reviewer failed to fully agree on some steps in the process of the paper.\nTherefore, the reviewer temporary rates the paper as weak-reject, but this can be adjusted after seeing the answers of the author.\n \nInquiries\n- See the concerns parts."}