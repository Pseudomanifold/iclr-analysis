{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a piecewise linear close form expression for the Stein\u2019s unbiased risk estimator and use this formulation to construct a new Encoder-decoder convolutional neural network. The author claimed that this closely related to bagging. Improved experimental results on two inverse problems are presented. Overall, the experiment results are encouraging but the paper need clarification on a few points.\n\n1. In the model description part, the intuition behind the attention modules is never mentioned. It will be nice to explain the intuition and possibly attached the derivation of the loss function the attention modules. \n\n2. the author seems misunderstand the difference between boosting and bagging. The way described in the paper is bagging and in order to do boosting, a sequential type of network structure probably need to be proposed.\n\n3. How will be model performance compared with a simple bagging for the baseline compared in the experiment part?\n"}