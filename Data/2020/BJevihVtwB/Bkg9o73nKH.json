{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "1. Summary\nThe authors address the problem of efficiently employing the SURE estimator as a network training regularizer. They show that for CNN autoencoders this can be efficiently computed. Their other contribution is a bagging/boosting technique which is proved to avoid trivial solutions. The proposed architecture, motivated by the theoretical statements, is shown to outperform classic and 2019 state of the art image reconstruction algorithms in MRI and EDX.\n2. Decision and arguments\nUnfortunately this paper is outside my expertise so I can\u2019t evaluate the novelty of the theoretical accomplishments. However taking that as a given, they well-motive the proposed architecture and achieve impressive experimental results. The experiments are well described.\n3. Questions \na) Why do Table 1 and Figure 3 provide different PSNR and SSIM values?\nb) Is there any way to measure accuracy to ground-truth with the EDX data? Or are the results just qualitative? \nc) With respect to Figure 2, and in general for autoencoders, the input and output have the same dimension. So how do you reconcile this with undersampled MRI and EDX data? I understand you train on fully sampled data\u2014then how do you input undersampled data? Are the unknown samples set to zero?\n"}