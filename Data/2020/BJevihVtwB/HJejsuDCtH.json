{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The authors consider an encoder decoder setup for linear deblurring problem and propose efficient boosting estimators. Specifically, they use the Stein's unbiased risk estimator for the problem when the noise is gaussian. In the case when the encoder and decoder is represented by a convolutional neural network with RELU activations, they show how they can exploit the recent theoretical results that show the kernel type results to make their procedure efficient. They then propose using a set of models (boosting) and prove that the boosted loss function lower bounds the \"nonboosted\" loss function.\n\n1. I think Proposition 1 has minor errors, there is no need to apply Jensen's inequality since there's nothing random, but I think the claim is correct -- it is trivial. In experiments, they use attention network which is not a CNN, so I'm not sure how any of the theory applies to this case, can you please clarify?\n\n2. Experimental focus of the paper is to analyze biomedical datasets -- HCP, EDX and the authors compared their method to *only* one baseline. I suggest that they perform some more comparisons on natural images like http://vllab.ucmerced.edu/wlai24/cvpr16_deblur_study/"}