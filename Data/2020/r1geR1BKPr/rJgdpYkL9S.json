{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a multi-stage influence function for transfer learning to identify the impact of source samples to the performance of the learned target model on the target domain. It considers two cases: fixed pretrained parameters and fine-tuned parameters.\n\nWhy not to directly add a scaled identity matrix to problem (15) to avoid the non-PSD issue?\n\nHow to use the proposed method to identify source samples that cause negative transfer as discussed in the introduction?\n\nEven using the conjugate gradient method to reduce the complexity, the total complexity is still high as the number of parameters in a deep neural network is large. It is better to report the running time to see the efficiency of the proposed method.\n\nIn transfer learning, there is a setting that source data are not accessible due to, for example, the purpose of the privacy protection. In this case, can influence function be used?\n\nFor presentation, I think it is not correct to use \u2018pretrain\u2019 or \u2018finetune\u2019 before a noun. They should be replaced with \u2018pretrained\u2019 and \u2018finetuned\u2019."}