{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper approaches the CMDP problem, in which one wishes to learn a max return policy subject to trajectory-based constraints.  The paper proposes a technique based on the introduced concept of \"backward value functions\".  These functions satisfy a sort of Bellman equation.  The paper proposes a safe policy improvement step based on these value functions, with theoretical guarantees on the safety of the resulting policy.  The method is evaluated on gridworlds and mujoco tasks, showing good performance.\n\nThe paper provides nice results some intriguing ideas, although after reading I am left with several questions as to the details of the method and its place relative to previous works.\n\n-- How is the backwards value function learned exactly?  Since it relies on a backwards Bellman operator, it seems to necessitate on-policy samples.  Is the function learned in an off-policy fashion somehow?\n-- The problem solved by SPI (constraint for each state) seems much more conservative than the constraint at the bottom of page 4 (single constraint based on average over all states). In it current form, the algorithm appears to be overly conservative, to the point that it may converge to a policy very far from optimal.\n-- I am confused by the assumption on policies updating \"sufficiently slowly\".  Why is this needed?  Also, how does this relate to CPO (Achiam, et al.)?  It would appear that the methodology of CPO is more desirable, since it only requires the \"sufficiently slow\" assumption without the additional backward-value constraints. "}