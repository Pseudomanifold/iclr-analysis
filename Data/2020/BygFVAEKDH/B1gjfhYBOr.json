{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "--------------------------------------------\nSummary:\n--------------------------------------------\nThis paper studies knowledge distillation in the context of non-autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN=>ES/FR/DE, the authors argue that this necessity arises from the overly-multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT. \n\nBased on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model.  The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.\n\n--------------------------------------------\nStrengths:\n--------------------------------------------\n- Careful initial experiments to motivate the study.\n\n- The metrics under consideration (entropy/faithfulness) are carefully derived, and while some of the assumptions may be overly simplifying (e.g. conditional independence assumptions), they are reasonable enough such that the directionality is likely to be correct.\n\n- Thorough comparison of various NAT models. This will benefit the community greatly since to my knowledge, there has been no empirical head-to-head comparison of the different NAT methods that exist today.\n\n- Extensive experiments across various settings, e.g. varying the teacher model size, varying the decoding strategies, investigating BAN/MoE/Sequence-level interpolation, etc. \n\n--------------------------------------------\nWeaknesses:\n--------------------------------------------\n- It would have been interesting to consider a synthetic data setting such that one has access to the true underlying data distribution, such that approximations are not necessary.\n\n- While translation is an important application of non-autoregressive generation, it would have also been interesting to study this in other seq2seq regimes such as summarization where the conditional entropy would presumably be even higher. (However this would complicate things like calculation of alignment probabilities, etc.)\n\n--------------------------------------------\nOther Questions/Comments:\n--------------------------------------------\n- p(y_t | l_i) coming from a token frequency distribution seems a bit too simple. Do the plots change if you model p(y | l_i) with a full language model that conditions on l_i?\n\n- It's interesting to note that in Figures 1b,1c,1d, there is much more overlap between the romance languages es/fr, which are more closely related to each other. \n\n- I am not sure I agree with \"C(d) reflects the level of multi-modality of a parallel corpus\". One can certainly imagine a distribution which is unimodal but has high entropy...\n\n- It seems like that we want a teacher model with low complexity and high faithfulness. Have the authors tried training a teacher model to directly target this? The usual MLE objective obviously targets faithfulness, but perhaps one could use RL techniques to optimize for low complexity as well.\n\n"}