{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper studies the representation power of single layer neural networks with continuous non-polynomial activation, and specifically, provided a refinement for the universal approximation theorem:\n1.  Established an exact upper bound on the width needed to uniformly approximate polynomials of a finite degree (to any accuracy of which the upper bound is independent), and\n2.  using this error-free bound to deduce a rate (of width) for approximating continuous functions.\n\nThe writing of the paper is concrete and solid.  The techniques used in establishing the results are interesting, in that:\n1.  The proof for polynomial approximation (Thm 3.1) is direct, via a close examination of the Wronskian of the target polynomial function, and\n2.  the analysis provided that the abilty to universally approximate is also preserved after placing certain restriction on the magnitude of the weights in the approximating neural network.  Consequently, this property is inherited by continuous function approximation to which the result is extended (Thm 3.2).\n3.  This analysis and some of the results derived in the proof may be used for other analyses, e.g. representation power of multilayer networks.\n\nSome further discussion of the results may be of interest to the readers.  \n-  (Optimality of Thm 3.2).  When the result in Thm 3.1 is extended to general continous functions via Jackson's theorem, to what extend does the rate deteriorate?  What does the rate look like when using certain common activations (such as ReLU, sigmoid).\n-  (Reference to random features).  Thm 3.3 appears to be related to random feature representation, whose approximating ability has been studied in prior works.  Some comment on those results may be beneficial (e.g. https://arxiv.org/abs/1810.04374).\n-  Although already a straightforward proof, it seems natural, and as a result may promote the presentation and clarity, to organize the proof to Thm 3.1 using smaller parts, which currently spans over 2 pages."}