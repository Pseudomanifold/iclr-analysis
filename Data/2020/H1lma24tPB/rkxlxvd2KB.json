{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents an extension of Glorot/He weight variance initiazation formula for the hypernetworks. Hypernetworks are the class of neural networks where one (hyper) model generates the weights for the another (main) network, introduced by Ha et.al in 2016.\nAuthors argue and show via experiments that standard weight init formulas do not work for hypernetworks, resulting in vanishing or exploding activations and re-derive the formula for convolutional/fully-connected networks + ReLU. \nThey show that proposed method allows  hypernet training when the standard ways don`t. \n\nThe technical contribution seems as logical and straightforward yet necessary step for hypernetwork-related research.\n\nQuestions:\n \n - In standard NNs, initialization issues are mostly solved after introduction of BatchNorm. Wouldn`t it be the case for hypernetwork as well: to just add BN layers between main net layers?\n \n - Figure 2. What are is the y axis of the figure? Norm? Variance? Mean? The same question for the most of Figures in appendix \n \n- It would be nice to see how proposed method stands vs mentioned heuristics like M3 and M4. \n \nOverall I like the paper. \n\nMinor comments:\n\n \n > \"This fundamental difference suggests that conventional knowledge about neural networks may not\napply directly to hypernetworks and radically new ways of thinking about weight initialization,\noptimization dynamics and architecture design for hypernetworks are sorely needed.\"\n\nI don`t see anything \"radically new\" in re-derivation of Xavier formula to the new type of network.\n\n\n"}