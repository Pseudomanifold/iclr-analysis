{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a novel probabilistic continual learning approach which automatically learn an optimal adaptation for arriving tasks while maintaining the performance of the past tasks. CLAW learns element-wise weight masking per task task with respect to the several learnable parameters. \nCLAW fundamentally based on Variational Continual Learning, but it outperforms benchmarks on diverse dataset even without additional coreset. However, the ablation study and analysis on the model is weak and authors only show experimental observations. Also, the experiments are performed on old architectures. Then, it needs to show the model consistently outperform on recent deep network architectures, such as ResNet.\n\n\nI have several questions,\n\n- How about the training time / convergence rate of the CLAW compared to other methods? \n\n- If the model need to divide the sample into two halves, isn't the model vulnerable when there are only a few number of samples with high variance? This situation is quite natural on realistic problem, like Imagenet.\n\n- Why are the VCL variants with CNN not compared?\n\n- There might be used a wrong plots in Figure 1 (e). It doesn't make sense that all methods show equal accuracy on task 46.\n\n"}