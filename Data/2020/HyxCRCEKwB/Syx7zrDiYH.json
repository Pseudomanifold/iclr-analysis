{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Developing stable GAN training method has gained much attention these years.  This paper propose to tackle this issue via involving distributionally robust optimization into GAN training. Its main contribution is to combine Sinha et al with GAN, proposing a new GAN training method on the basis of vanilla GAN. Relative theory results are proved and detailed experiments are conducted. \n\nSome comments:\n1 The proposition 0.1 is not quite clear.  In fact it is correct only when the distribution discrepancy is Wasserstein. This paper reads \u201chere we use the wasserstein metric\u201d in \u201crobust training over generator\u201d subsection, the reviewer is not sure if the authors are aware of this point.\n\n2. There seems to be a lack of novelty except combining Sinha et al\u2019s theoretical result with GAN training objective. And there seems not much explanation about the reasons behind this combination. \n\n3. The proof in this paper shares similar analysis with that in vanilla GAN paper so theoretically there is also not much novelty. There seems not much insight can one get from the theory results.\n\nOverall, the proposed method is evaluated under elaborate and detailed experiments and enjoys promising results, but lacks novelty and theoretical contribution.  Therefore, the reviewer tends to reject this paper.\n\n### reference\nAman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.\n"}