{"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a technique based on genetic programming to generate a suitable training corpus of programs and I/O examples using a trained discriminator network. Given a set of human useful programs and the corresponding I/O examples, the main idea of the approach is to use genetic programming to iteratively construct new corpus leveraged by a discriminator that aims to make the I/O examples of the corpus closer to the I/O examples of the human-useful set. This approach is evaluated on 40 human useful corpus of array programs and it is able to synthesize more programs than random corpus or genetic programming based baselines.\n\nOverall, this paper presents an interesting idea of automatically generating corpus for training neural program synthesis architectures, where most previous techniques sample synthetic programs uniformly from the space of DSL programs. (Shin et al. 2019) also point out a similar issue in neural program synthesis approaches, but this paper presents an automated technique to construct a better training corpus of synthetic programs and the corresponding I/O examples. I like the idea and the overall direction, but the current paper looks a bit preliminary both in evaluation as well as presentation.\n\nFirst, the description of the overall method is too high-level. It would be better to formalize exactly the network architectures for both synthesis as well as the discriminator networks. Having precise inputs to the networks as well as equations would help with the description. The description of the genetic programming framework to generate child corpus from parent corpus also seems a bit high-level.\n\nIt wasn\u2019t clear what are the inputs to the discriminator network (in Section 3.5). Does it only take a single I/O example as input or a pair of I/O examples? Does it also take the current corpus program as input? How is the discriminator network used to select programs for the child corpus?\n\nI was trying to better understand the context in which such a dataset generation might be useful. It seems for such a technique, one has to come up with a dataset of human useful corpus of tasks. Doesn\u2019t it mean that one has to possibly identify all possible programs that a user might want to synthesize upfront? It would be interesting to evaluate how well the technique works when synthesizing for programs that are also interesting to users but not provided as a part of the human useful corpus.\n\nIt was surprising to see genetic programming baseline solving more unique programs in the test set. Is it the case that with the discriminator based corpora, the approach is overfitting to one class of problems and not on others? Evaluating the technique on unseen programs not in the human useful set might help better evaluate this point as well.\n\nFor the comparison in 4.2 with uniformly generated data, only 5000 programs were used for training, unlike typical approaches that train on millions of synthetic programs. Why not train the network on larger amount of programs?\n\nIt was also interesting to see that the identity program was difficult to synthesize. Wouldn\u2019t enumeration based approaches first start with that identity program? From the description it was also not clear how big the total search space for programs was in the language. Why wouldn\u2019t enumeration based approaches work here? Also, it would be good to better understand why Sketch (Solar-Lezama 2008) based symbolic approaches won\u2019t be able to synthesize these programs.\n\nIt would also help to provide some examples of synthesized programs and their I/O examples, and maybe also the child corpus that lead to successfully synthesizing them.\n\nThe paper mentions that DeepCoder based baseline isn\u2019t applicable in this setting because of the generality of the language. Is the search space too large? Alternatively, can the presented technique be applied to DeepCoder to train on a generated corpus to improve its performance? Karel (Shin et al. 2019) might be another domain used in previous literature to show the usefulness of such an approach.\n\nOn page 4, the paper mentions that the first 3 examples were provided with a specific form.  Is there any intuition why such examples are useful and whether such constraints are used for all newly generated corpus as well? What happens when only random I/O examples are used?\n\nMinor:\n\nReferences are not formatted correctly in the paper.\npage 4: value between 8 and 8\n"}