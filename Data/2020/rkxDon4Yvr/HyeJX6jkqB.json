{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "# Summary\n\nThis paper describes an approach for generating synthetic training corpora for neural program synthesis systems. It\nassumes a test set of tasks to be solved, and designates a special \"discriminator\" network to force a distribution of\nI/O examples derived from randomly sampled programs to be as close as possible to the I/O examples supplied for the\ndesired programs. The system first randomly samples a set of programs, then retains only those that are \"sufficiently\nclose\" to the desired I/O examples, and mutates them to further refine this training corpus. The whole approach is\nevaluated on a DSL of array-manipulating programs, assuming 40 reasonable test programs and randomly sampled I/O pairs.\n\n# Strengths\n\nI like the core idea of the paper: assuming some desired distribution of I/O examples, construct the training corpus of\nrandom programs in such a way that its associated I/O examples approximate the desired distribution. I can see how it\ncould force the sampling process to discover useful program snippets for training. If evaluated properly, this work\nmight get accepted at a future conference.\n\n# Weaknesses\n\nI have some issues with the (a) evaluation, and (b) presentation of the work in its current state.\n\nFirst, the evaluation process fixes a set of 40 \"useful\" programs along with their associated I/O pairs, uses these I/O\npairs to build the training corpus as described, and then tests the finally trained synthesizer on the **same** set.\nWhile the test programs themselves are not are not used in corpus building, only their I/O pairs are, this is still\nsufficient to significantly bias the training set toward the test set. To properly test the core idea above, I'd expect\n_at least_ something like:\n- Write down or collect a set of \"useful\" test programs. Ideally make sure they all use different compositions of the\n  language operators but still cover the space of combinatorial operator combinations well.\n- Split this set into \"dev\" and \"test\" randomly. Use the I/O pairs from the \"dev\" set to build the training corpus.\n  Evaluate the final synthesizer on the test set.\n- Ideally also evaluate the sensitivity of this process to the chosen dev/test split.\n\nSecond, the evaluation baselines for corpus generation are straightforward: random sampling and genetic programming. The\nformer is self-evidently strawman. The latter is interesting, but is effectively a different implementation of the same\ncore idea of the paper, with two changes: (a) hardcoded fitness function instead of a trained discriminator, (b)\ntournament selection for population evolution instead of roulette.\nIn any case, neither of the baselines are truly independent strong alternatives. The authors remarked that they were\n\"unable re-use baselines from ... literature, as for example ... DeepCoder framework caused by the relative generality\nof our programming language for synthesis\", which is a confusing statement for two reasons:\n1. The programming language considered is a simple DSL of array manipulations, not that different in spirit from the\n   DeepCoder DSL, AlgoLisp [Polosukhin & Skidanov, 2018], or the list manipulations DSL of Ellis et al. [2018].\n2. Regardless of the DSL, what is being compared are **corpus generation techniques, not synthesis algorithms**.\n   DeepCoder is not a baseline for that, its corpus generation is not a contribution.\nThe most proper baseline would be the work of Shin et al. [2019], which the authors mention in Section 2. It was\nevaluated on Karel, which leaves the authors the choice of either (a) additionally evaluate their technique on the\nKarel DSL, or (b) adapt the method of Shin et al. by designing some salient variables for the DSL of this paper.\nI would prefer (a), which would significantly strengthen the paper, but (b) is also acceptable assuming honest effort\nin designing the salient variables to make the baseline strong.\n\nFinally, the presentation of the technical sections is informal and often confusing. Section 3 required several passes\nto understand the corpus generation process. The authors should introduce proper formalism for all the involved concepts\nsuch as corpora, train/test programs, their associated I/O pair, both neural networks with their input-output signatures\nand architecture, and so on. Most of these concepts have established mathematical notation in the literature which would\nmake Section 3 much easier to follow. In addition, Sections 3.3-3.5 need to be formalized in algorithmic pseudocode.\n\n# Minor remarks\n\n* Please use \\citet and \\citep appropriately: citations should be enclosed in parentheticals if they don't participate\n  in a sentence.\n* In your synthesizer network architecture, do you know the number of program output lines ahead of time?\n* In this DSL, how many possibilities are there in total for an output line (including all its arguments)?\n  Given the beam size of 1M (which is a lot!), I want to compare it with the total space (#possibilities \u00d7 #lines).\n\n\n# References\n\nEllis, K., Morales, L., Sabl\u00e9-Meyer, M., Solar-Lezama, A., & Tenenbaum, J. (2018). Learning libraries of subroutines for\nneurally\u2013guided bayesian program induction. In Advances in Neural Information Processing Systems (pp. 7805-7815).\n\nPolosukhin, I., & Skidanov, A. (2018). Neural program search: Solving programming tasks from description and examples.\narXiv preprint arXiv:1802.04335.\n\nShin, R., Kant, N., Gupta, K., Bender, C., Trabucco, B., Singh, R., & Song, D. (2019). Synthetic Datasets for Neural\nProgram Synthesis. In ICLR.\n"}