{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThe paper shows that the learnt representations of robustly trained models align more closely with features that the human perceive as meaningful. They propose that robust optimization can be viewed as inducing a human prior over learnt features. Extensive experiments demonstrate that robust representations are approximately invertible, can be visualized yielding more human-interpretable features, and enable direct modes of input manipulations.\n\nThe paper indicate adversarial robustness as a promising avenue for improving learned representations in from several aspects. It is well written and contains extension experimental results. I'd suggest accepting the paper.\n\nQuestions and Comments:\n- Is there a particular reason that $L_{2}$ norm is used throughout the paper? How is the performance if using other ones?\n- Compared to the other methods introducing priors or additional components into the inversion process, how is the quantitative inversion quality and computational complexity of the proposed method?\n- The paper claims that the representations are more perceptually meaningful than the others, which may need to be evaluated with broader human subjective.\n- I think it should be argmin in Equation (1).\n- Also Equation (4) is exactly the same as (1).\n- Some symbols seem to be used somewhat interchangeably. E.g., x' in Equation (1) represents x+\\delta, while x' in Equation (5) is \\delta itself."}