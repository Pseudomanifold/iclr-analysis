{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies active learning in graph representations. To decide which nodes in a graph to label during the active labeling, the paper proposes two approaches. First it argues that one should consider region-based measures rather than single nodes. Second, it proposes to adapt the page rank algorithm (APR) to determine which nodes are far away from labeled nodes.\n\nOverall it remains unclear *how* to select the right strategy (before seeing the results for a dataset) i.e. which of the proposed approaches or variants should one select for a new dataset.\n\nStrength:\n-\tOne of the ideas of the paper, using region entropy over single node entropy makes sense to me.\n-\tThe paper evaluates on 6 datasets and compares different variants as well to related work on 2 datasets.\n\nWeaknesses:\n1.\tThe paper contains several confusing and contradicting statements or claims which are not supported by the experimental results:\nFor example:\n1.1.\t\u201cAPR outperforms all other methods at low sampling fractions\u201d.  This is supported neither in Table 1 nor Table 2, where APR is frequently not highest performing\n1.2.\t \u201cWe have here shown that the accuracy of AL when uncertainty is computed regionally is much higher than when either local uncertainty or representative nodes are used\u201d, this is not the case on CiteSeer in Table 1\n1.2.1.\tAlso e.g. \u201cRegion Margin\u201d is worse than random on 5% Email-EU; or \u201cRegion Margin AE\u201d on 3% SubeljCora (Table 2) [It is unclear how to select with or without AE]\n1.3.\t\u201cWe outperform all existing methods in the Cora dataset, and get very similar results to the best accuracy obtained by Chang et al methods:\u201d\n1.3.1.\tThe difference to Cai et al. on Cora is very small (improvement by only 0.002), while on Citeseer the performance is comparatively bigger (Cai et al. is by 0.016 better)\n1.3.2.\tIt should be \u201cCai et al\u201d\n2.\tClarity: I found the paper rather difficult to understand and follow:\nSome specifics:\n2.1.\tThe introduction could be more concisely discussing the motivation, the main idea of the paper, as well as contributions.\n2.2.\tFigure 1: according to the caption, APR should point to node 15, but in the figure it points to node 14. From the example it makes much more sense to label node 14 to me.\n2.3.\tPage 6 mentions twice the \u201cratio between APR and PR\u201d, is this is this used/evaluated in the results?\n2.4.\tThe decision what is bold and what is not is not consistent throughout the table 2.\n2.5.\t\u201cThus, hybrid techniques, combining several approaches, outperform using only one approach have been proposed.\u201d It is not clear what this refers to and where the hybrid techniques have been evaluated.\n\nMinor:\nThe paper contains many minor writing issues, e.g.\n-\tmissing spaces, e.g. \u201cdistribution,and\u201d (page 2)\n-\tTable 1: incomplete sentence: \u201c\u2217\u2217 scores for smaller budget, since it was the\u201d\n-\tTable 2: unclear: \u201caccuracy without content\u201d\n\nThe paper\u2019s incorrect claims (weakness 1) are highly concerning and strongly suggest rejecting the paper. Furthermore, the clarity of the paper should be improved to follow the author arguments and make the paper easier to read.\n\n"}