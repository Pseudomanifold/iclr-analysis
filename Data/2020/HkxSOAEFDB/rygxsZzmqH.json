{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use octave convolution to learn a representation of a graph.\nTypically, a learning on a graph is done either in a spatial domain or in a spectral domain.\nA spectral domain based approach uses a eigenvalue decomposition form of a graph Laplacian (a symmetric matrix) and learning a filter that acts on the eigenvalue of a graph Laplacian while preserving eigenvectors of a graph Laplacian.\nThis architecture is called the graph convolutional network on a spectral domain.\n\nThis paper's main contribution is to adapt octave convolutional network's architecture to the usual graph convolutional network. While I believe that this is the first work on applying the idea behind octave convolutional network architecture, separating low and high frequency component in the learning stage, to graph convolutional network architecture, I cannot see a good motivation on why this architecture is good for learning on a graph.\nA comprehensive study in the paper shows a better performance gain compared to the existing method, but it would be better if the gains were substantial or the authors presented a good motivation on why this architecture is good in some cases.\n\nOverall, I think the paper is well-written, but I would suggest to present more meaningful justification why and when the octave GCN is better than the GCN."}