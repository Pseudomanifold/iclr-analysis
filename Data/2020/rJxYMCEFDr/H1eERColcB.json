{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents a method to represent input points by their relative positions to decisions boundaries using an adversarial attack instead of the more usual output of a deep model (before the last linear classification layer). The motivation of the method is to obtain representations that are more robust to input distortions such as blur, noise, over-exposure, etc. More specifically, the representation obtained by a deep model before the last linear classification layer (f_L) is replaced by a concatenation of the perturbation needed for that representation to be misclassified by f_L. Using this new representation, they are able to obtain more robust classifiers against input perturbations. \n\nThis paper should be rejected for several reasons. First, the motivation of the paper is unclear; while being more robust to input perturbations is a desirable property of ML models, the reason to use these second-order representations is not justified. Second, the paper contains several imprecisions about the methodology, which make it complicated to follow. Finally, the presented results are not very convincing; it seems that most of the improvement obtained comes from deriving a larger representation of each sample and using a MLP to classify it instead of a simple linear layer.\n\nDetailed arguments:\n\nIt is not clear how r_i is obtained in section 3. This becomes even more confusing in section 5.3, when the features are instead generated as the gradient of the loss with respect to f_{L-1}. To the best of my understanding, r_i is supposed to be the distance between a point (before the final layer) to the decision boundary for class i. If we have x_k, the input which yields the closest point to the decision boundary of class i (as depicted in Fig 3.(b)), then we should have $r_i = f_{L-1}(x) - f_{L-1}(x_k)$. However, r_i is defined as $\\sum_{j=0}^{k-1} \\abs(\\nabla_{W_L^i} J(W, x_j, i)$. In this formula, the absolute value clearly changes the direction of the gradient. It would be appreciated to have a clear explanation for the reason of the absolute value.\n\nThe paper contains incorrect statements about standard adversarial attacks. FGSM is a one-step attack, and it does not make sense to do several iterations of it. A more powerful multi-step attack that can be considered is PGD.\n\nA lot of emphasis is put on the fact that the obtained representations might be too large to scale well to datasets where there are more classes than the ones considered in the article. However, there are no experiments to support this - the experiments in the paper do not go beyond 10 classes.\n\nAdditional comments:\n\nSome citations need to be fixed, removed or changed:\nSecond paragraph of the introduction: \u201c[...] can discriminate between classes (Goodfellow et al., 2016)(Krizhevsky et al., 2012)(He et al., 2016).\u201d Deep neural nets were not initially proposed in these papers, so it is somehow inappropriate to cite these papers here (except, maybe, Krizhevsky'12).\n\nThird paragraph of the introduction: \u201c[...] the representations (fL\u22121) are linearly projected (Wang et al., 2019).\u201d Same here.\n\nThe main figures to explain the idea are not really helping the argument: Figure 1. (b) is simply Fig 1. (a) rotated 180 degrees and re-scaled, but the information is essentially the same; maybe not the best example.\n\nFigure 2: the provided t-SNE projections do not show a clear advantage of the proposed method. A quantitative analysis would better support the argument.\n\nThe time measurement given in section 3 is not indicative: what are the 320s? An indication in terms of epochs or training iterations would be better.\n\nIt would be appreciated to have measures of standard deviations for the results especially in tables 1 and 2 since some results are very close and might depend mostly on random perturbations. Moreover, it is not mentioned for which datasets these tables are for.\n"}