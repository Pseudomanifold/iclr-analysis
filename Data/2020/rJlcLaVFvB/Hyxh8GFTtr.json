{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a study that compares two techniques for Hierarchical Sparse Coding. This is the problem of learning sparse representations in multi-layer (but not necessarily deep) models. The first method applies Lasso at each layer in a bottom up fashion, while the second method \u2014 introduced in the submitted work \u2014 adds an additional term which propagates information in a top-down manner. It is found that the top-down term is beneficial in terms of reducing predictive error and can learn faster. In terms of novelty the new term, the paper does not make a breakthrough contribution, but I consider this to be sufficient. Moreover, the paper is well written and presents some interesting results. However I am not entirely sure that the impact of these results will attract the interest of a broad audience since the experiments presented are on \u201csmall datasets\u201d and with some rather shallow neural nets. "}