{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary:\n\nAuthors propose ideas to perform safe RL in continuous actions domain with modifications to Policy Gradient (PG)  algorithms via either constraining the policy parameters or constraining the actions selected by PG with a surrogate/augmented state dependent objective. The paper is well motivated and the experiments (although I have some reservations about the setup) demonstrate efficacy of the proposed method.  \n\nReview:\n--> Introduction\nI do not agree with the statement that value function based algorithms are restricted to discrete action domains, especially when you rely on  \u201cignoring function approximation errors\u201d for some of your claims. \n\nAgain in switching from value function to PG is true for traditional RL/Control theory but this is not valid here. ( your methods rely on Q(s, a) which is action-value function, or the constraint in equation 3 is integral of Q over all actions which would be value-function in traditional definition )\nNote: this is explained very well towards the end in the Appendix B, but this is a review of the paper and not Appendix B or C. \n\n\n--> Section 2\nsection 2.3 I would strongly advise the authors to rewrite this, this section reads like it was copied as is from the reference [Chow et al 2018].  especially the way Lyapunov function is defined. And the language and arguments are almost same. Some sentences cite the reference but conclusions drawn on these are not cited, are you claiming that these conclusions are original from this paper ?\n\nIt is not clear to me how the feasibility of initial pi_0 is ensured ? Did I miss this somewhere ?\n\n\n\u2192 Section 3\n\nSection 3 is pleasant to read and very easy to understand, however, same cannot be said of the\nsection 3.1. I had to spend significant time reading 3.1 and I am still not sure I have understood it very well. \n\nExperiments:\n\nI don\u2019t think halfCheetah-Safe is actually actually an useful experiment, Limiting the joint torques is perfectly understandable, just limiting speed and getting smooth motion could just be an artifact of the simulation environment. Are both constraints applied simultaneously (torque and speed) ? It is unclear from the text. \n\nI am not sure CPO without linesearch is actually a fair comparison. Line search may actually deem of the actions unsafe and therefore I would presume original CPO do be less prone to constraint violation than the proposed modification in your experiments. Again PPO is more heuristic than TRPO which makes it hard to compare like for like. PPO might give higher rewards but constraint violations may increase as well. An important point for Safe-RL I feel.\n\nFigure 6 \nCan you be more specific as what the figure 6 is showing ? Constraint ? is this constraint violation count? or cumulative sum of constraint slack over the whole trajectory ?\n \n\nNot part of assessment :\n\n\nUnclear Statements:\n\nPage 7, DDPG Vs PPO: explain clearly what you mean by  \u201ccovariate shift\u201d or remove the statement altogether. \n\nPage 7, section 5 second paragraph, \u201cThe actions are the linear \u2026. center of mass\u201d I couldn\u2019t understand this ? What do you mean by actions are velocity ?\n\n\n\nMinor points (Language, Typos):\n\npage 3, last paragraph,  Chow et al. is repeated, I can see why this happens there but suggest editing to avoid this. [This is also in intro paragraph, there it is just a typo and should be rectified]\n\nFigure 6: Captions labels are incorrect.\n\n\n\n\n\n"}