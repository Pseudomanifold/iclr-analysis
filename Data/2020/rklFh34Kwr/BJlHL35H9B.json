{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors propose the adaptive thermostat Monte Carlo sampler for feedforward neural networks. The proposed approach dynamically adjust the amount of momentum and noisy applied to each model parameter during updates. ResNet++ (ResNet without batchnorm/dropout but adding SELU, fixup and weight normalization) is introduced. Further, the authors claim that the need for hyperparameter setup is reduced provided that early stopping, stochastic regularization and carefully tuned learning rate schedules are not required.\n\nThe authors highlight some practical issues with the Nose-hoover thermostat, however, recognize its mathematical soundness. When ATMC is described (temperature stages), a motivation is provided but not justified theoretically.\n\nIn (10) and (11), \\gamma_1(), \\gamma_2(), \\eta_t and a are introduced without definition or further explanation.\n\nThe authors claim that the need for hyperparameter setup is reduced, however, in the experiments they use a cyclic step size with length n=50 (20 for ImageNet), a Laplace prior with parameter b=5, momentum noise with parameter 0.9, pre-conditioner parameter 0.0003 and c parameter 0.001. The impact of these choices on performance is not described. Further, the number of filters is doubled relative to ResNet-56 without explanation.\n\nThe calibration curves in Figure 3 are underwhelming. ATMC is better than SGD but not necessarily well calibrated. Also, note that x and y scales are heavily biased toward 1.\n\nIn summary, the proposed approach needs to be described in more detail and the experiments are not very satisfying given the claims made by the authors in the Introduction.\n\nMinor:\n- In (1) W is not defined.\n- In (1) the dimensionality of D, Q and \\Gamma is not defined but their elements are used.\n- In (2) m is only defined after (3), in fact, only called by its name, pre-conditioner, in Algorithm 1.\n- In Section 2.3 there is a reference to the step size, though not introduced until discretization later in Section 3.\n- In (7) \\beta() is a function of p, but not in other instances, e.g., (4), (10) and (11).\n- Move Algorithm 1 closer to definition.\n- In (13), d is not defined."}