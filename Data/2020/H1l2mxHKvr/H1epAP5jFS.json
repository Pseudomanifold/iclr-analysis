{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a problem \u201cfew-shot few-shot learning\u201d that aims to firstly transfer prior knowledge from one domain to the domain where the base training tasks reside, and then train a few-shot learning model on training tasks and apply it to novel test tasks. The two \u201cfew-shot\u201d in the name refers to base training tasks and novel test tasks. In their algorithm, they use a model pre-trained on another dataset as the prior knowledge and fine-tune it on training tasks. During the test, they use the weighted average of samples\u2019 representations per class as the prototype of each class, where the weight is large for samples with more discriminative prediction over pre-trained domain\u2019s classes. Afterward, classification is reduced to finding the nearest neighbor among the class prototypes. Some experiments show that the pre-trained model can improve few-shot classification accuracy.\n\nMy major concerns:\n\n1) They try to propose a new problem, but their description shows that the problem is exactly the same as what most \u201cfew-shot learning\u201d works aim to solve: use a pre-trained model, train a meta-learner on few-shot training tasks, and apply it to novel test tasks. \n\n2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre-trained model and apply the nearest neighbor classification. The so-called \u201cprototypical classifier\u201d is actually the nearest neighbor classifier since no prototypical network structure is learned in the proposed method.\n\n3) I would not call the weighted average as \u201cattention\u201d because it is not: the weight in attention is computed by a module with learnable parameters, while the weight in this paper is computed by the entropy of a pre-defined model\u2019s output prediction. \n\n4) The \u201cspatial attention\u201d only makes sense when the pre-trained domain\u2019s classes can describe the main concepts appearing in the images of novel classes. This assumption is too strong since it requires class-level (rather than lower-level) relationships.\n\n5) The base training is not necessary in the algorithm: it is used to only fine-tuning theta and W. As the author said in the beginning of Section 4.1, they can directly solve novel tasks based on the pre-trained model.\n\n6) The experiments show that the pre-trained model is helpful in few-shot learning, which is a known fact.\n\n7) The writing of this paper is very poor: a lot of typos and grammar errors, inconsistency between narratives, abuse of notations, wrong equation reference, even missing punctuations. They make the paper hard to understand."}