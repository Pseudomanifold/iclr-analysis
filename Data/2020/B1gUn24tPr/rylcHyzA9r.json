{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Comments by sections : \n\nSummary : \n\nThe use of entity is not clear. Are you referring to named-entities ?\n\n1 INTRODUCTION\n\n\"Due to the differences in language structure\" : this paragraph is not clear. It should say explicitly that in Chinese word can be composed of one or a couple of characters and that word boundaries can not detected graphically. \n\n \"A common mitigation is to add external lexicon as a reference\"   references are needed on how to includes words in embedding (except just training word embeddings)\n \n \" an entity classification-assisted model\" : not very clear : the classification is assisted ?\n \n \" The first step is to form word embeddings of entities appearing in the trainning sentences through character embeddings and the second step is to aggregate entity embeddings by category and generate classification embeddings\" : is it a multi-task training ?\n \n \"After that, we designed a novel Attention mechanism to integrate entity \" : is it the same model or two different propositions of the paper ? \"After that\" is not very clear as a transition.\n \n Section 2 :\n \n \"more works Yang et al. (2016) Ruder12 et al. (2017) \" : strange formulation\n \n \"What\u2019s more, the attention mechanism...\" : odd expression.\n \n \"In this paper, we revise the Scaled Dot-Product Attention to Classification Attention which would give a weighted representation of the input sentences through a series of entity classes.\" : maybe it should be moved to the introduction as a novelty proposed by the paper.\n \n \n 3.2  EMBEDDING EXTRACTION FOR ENTITY CLASS\n \n \" the smooth inverse frequency is abandoned\" : why ? please explain this choice.\n \"the weighted projection of the word embeddings on their first singular vector is removed.\" : explain. If it is a common practice, give a citation otherwise justify this choice.\n \n 3.3 CLASSIFICATION ATTENTION \n \n Here again, the proposed attention system is described but not justified : why would a class specific attention system be better ? What are the expected advantages ?\n \n 4.1.3 EXPERIMENTAL RESULTS\n Experiments are conducted on 4 dataset and the proposed model is compared to a \"standard\" BERT-based model and several results form the litterature. The proposed model outperforms sometimes the other models, often by a small margin as it is usually the case in NER experiments. \n But more insight on the strengths of the models should be given by conducting an ablation study. \n  \n  \n In conclusion ,this paper present an incremental improvement over BERT-based NER for Chinese. The proposed approach is not sufficiently justified and experiments, even if showing improvements over state-of-the-art models or published results, does not sufficiently explore the benefits of the proposed model (with ablation study for example).  "}