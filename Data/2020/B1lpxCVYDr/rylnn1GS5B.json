{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper studies inlier-based outlier detection via density-ratio estimation in a deep learning context. Specifically, they consider the use of KLIEP in conjunction with a deep neural network. One finding is that batch normalisation is harmful for performance of this method. Experiments show consistent gains over a self-supervised baseline.\n\nThe paper is generally well-written, and provides a nice overview of the density-ratio approach to anomaly detection. Its aims are clearly spelled out, and the careful derivation in Sec 3 of the Bregman perspective of inlier-based anomaly detection serves as a clean introduction to the topic.\n\nAt the same time, the technical contribution of the paper is modest. As best I could tell, Sec 3 is entirely devoted to a (nice) review of the existing framework of (Sugiyama et al., 2012b), and contains no new material. Sec 4 seems to be the only novel technical contribution of the paper, namely, the observation that batch normalisation affects the KLIEP method adversely. This is certainly an interesting finding, but it is accompanied by no further critical analysis. In particular, there is no deeper explanation for why batch normalisation might affect KLIEP in this way; whether this finding is consistent for different choices of architecture; whether this finding is consistent for different choices of method (e.g., uLSIF); and so on. By itself, I feel there merely observing that batch norm interacts badly with KLIEP is not sufficiently deep as to carry the paper.\n\nThe experiments compare KLIEP against an unsupervised baseline based on geometric transforms (GTs). This comparison is somehow unfair, since (as I understand) GTs assume one operates in a different problem setting altogether (where there is only a single sample of inliers, and no background contrast sample). Given this, it is surprising that GTs are able to be competitive with the proposed method in several cases; I feel this deserves more comment. It also raises the question of whether one may combine the proposed method with GTs. More such experiments might have made the paper stronger, but in their current form I don't find them too illuminating or surprising.\n\nAnother confusion that arose was that the experiments, as best I can tell, focus entirely on KLIEP. This makes the motivation for Sec 3 unclear -- the section introduces an abstract Bregman divergence view of anomaly detection, but then settles on one specific case of this which can be derived rather directly without this (as was done in the original KLIEP paper, to my knowledge). Had there been some study of different instantiations of the power divergence empirically, e.g., that might better justify the discussion in Sec 3. (I would imagine that for example KLIEP is more susceptible to outliers or label noise, which could motivate using intermediate values of \u03b1.)\n\nOverall, while I appreciated the paper as an overview of inlier-based anomaly detection, its increment over the current literature appears modest.\n\nMinor comments:\n- \"and it is used as an outlier score\" -> \"and is used as an outlier score\"\n- \"neural network ARCHITECTURES for complex image datasets\"\n- \"Deep SVDD approach ARE the distance of a data point\"\n- why do you use p^* in Sec 3 rather than just p?\n- remove italics from Fig 1 labels.\n- \"let us consider using A CNN to ESTIMATE the density\"\n- it seems fairer to use the GT authors' implementation of their own method?\n- \"cleverly impose a non-negativity constraint\" -> omit the word \"cleverly\"."}