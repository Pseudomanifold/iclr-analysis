{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new suite of automatic evaluation metrics to measure the coherency, fluency and diversity of open-domain dialogue generation systems.  The authors leverage a strong language model (GPT-2) to assess fluency and coherency and use n-gram entropy of responses to rephrasals of the query input to measure diversity.  While there has been prior work in using language models to measure fluency, the authors demonstrate higher correlations than previous work, likely owing to the low perplexity of the language model used.  The proposed method to measure diversity is novel and addresses an outstanding issue with prior work: it allows one to measure diversity conditioned on the input query/prompt. The authors show that this metric correlates strongly with human judgments of diversity.\n  \n== Decision ==  \n\nWhile I like the ideas and execution in this paper, the paper is missing some key experiments needed to substantiate its claims and to validate the utility of the proposed metrics. I am leaning towards rejecting this paper.\n  \nMy biggest concern is that the method is only evaluated on a single system: a good evaluation metric (automatic or not) must have high correlation with human judgments for a variety of systems. Prior work, e.g. Chaganty et al. (2018) shows large variation in instance-level correlation depending on the target system. Do you have a sense for how these metrics behave on other models?\n  \nWhile the context coherence metric has relatively high correlation with human judgment, the plots in Figure 1 suggest that much of this correlation really comes from the fact that low scoring examples are indeed badly rated by humans, while higher scoring output has human scores across the spectrum. Novikova et al. (2017) and Chaganty et al.  (2018) observe this behavior as well and conclude that automatic metrics are great at identifying _bad_ generations, but are poor at distinguishing between good ones. How well does the context coherence metric correlate for examples that score greater than 0.5?\n  \nWhat instructions were annotators provided for each of these tasks?  Were they primed / provided instructions on what to rate as a 1 vs 5?  How much context is shown to annotators when evaluating context coherence: the previous utterance or the whole conversation history?\n  \nWhy is the human variance the highest for the baseline \"dataset\" in Table 6? If the claim is that most sets of responses are diverse if you do not control for the query set, then shouldn't most human responses agree on high diversity scores (reducing variance)?\n  \nCan you provide some examples of the diversity sets generated by the different methods (CTG in particular)? This is one of the papers core contributions, but it is hard to evaluate how good a technique it is without understanding how similar the context sets really are.\n  \nIt appears that the CTG method to generated controlled query sets consistently performs better than WS on Spearman correlation -- do you have an understanding of why this is the case?\n  \n== Additional comments ==  \n\nI highly recommend using violin plots to provide a more accurate picture of the distribution of examples at each human score (see Chaganty et al. (2018))\nSome additional references for using language models to evaluate fluency of generation include GLEU (Mutton et al., 2007) and ROUGE-LM (Kann et al., 2018).\n  \nThe use of \"dataset\" to describe the different controlled query sets was confusing to me -- I would choose a different word to describe them.\n  \n"}