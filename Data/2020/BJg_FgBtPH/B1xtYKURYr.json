{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "In this paper, the authors tackle the problem of automatic dialogue evaluation. They construct a method that relies on the recent GPT-2 language model, and divide their evaluation into three components: context coherence, fluency, and diversity. \n\nI really like that the authors are investigating automatic dialogue evaluation; I think it\u2019s an important and under-studied problem. The paper is fairly well-written, and the three components of evaluation make sense. I like the proposed idea of measuring diversity via augmenting queries from the source dataset (although it requires access to the dialogue model parameters, this is usually feasible). \n\nUltimately, though, I\u2019m not convinced by the paper\u2019s empirical evaluation. Specifically, the paper only uses a single dialogue model (a simple seq-to-seq model with attention) for calculating their correlations, with different top-k sampling levels. This means that there is no indication of whether the proposed methods will apply to other dialogue models (an important criteria of an evaluation metric) or whether there is any model-level correlation between humans and these metrics. Further, the number of human ratings in the test set is quite small (200, 200, 500 respectively), and there are no error bars shown in the paper, making statistical significance a concern. \n\nI also have more foundational concerns with using purely unreferenced evaluation metrics in dialogue (i.e. metrics that aren\u2019t trained on a dataset of (context, response, score) triples). We want to develop automated metrics that will be useful in driving progress in dialogue systems. However, unreferenced metrics have the same information available to them as the dialogue systems which they are evaluating. Thus, the only way that these unreferenced metrics will produce useful data is if the model it uses has been trained on significantly more data (or has much more capacity). My guess is that, in this paper, the correlation results are strong because the language model used for calculating the metrics (GPT-2 + fine-tuning) is much larger / has been trained on much more data than the dialogue response model (seq-to-seq + attention). I suspect the results would be different if comparisons were done on several GPT-2 based dialogue models. \n\nAnother way to think about it: if this metric were to be widely adopted by the dialogue community, one could simply create a dialogue generation model that uses GPT-2 + fine-tuning (and indeed, some researchers have already done so), and this would give maximum scores on these metrics (which are mostly the log-likelihood under GPT-2). \n\nGiven the above concerns, I cannot recommend acceptance in its current form, however I encourage the authors to continue working on this problem. \n\nOther questions:\n- How did the authors choose the 5th percentile for truncating the conditional log likelihood? \n- Given that context coherence is calculated via the log-likelihood under GPT-2, it should also capture things like response fluency. How much information is the response fluency (the unconditional log likelihood) really adding? \n- While the performance of each metric is measured independently, it would be interesting to see how well they worked in combination. \n\n\nSmall fixes:\nImpracticable -> impractical\nBLUE -> BLEU\nTable 1: this table is taken from Liu et al. (2016) without citation, please add the source\n"}