{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors offer a suite of three automated metrics for evaluating generated responses from open-domain dialogue models. The metrics are claimed to be measures of coherence, fluency, and diversity, and are shown to correlate positively with human evaluations along the same axes. The authors plan to open-source the code for making evaluations, in a step towards standardizing the way dialogue models are measured and compared. \n\nI recommend the authors revise and resubmit to a conference like COLING, where such evaluation work will be a bit more welcome than a conference on learning representations.\n\nThe authors provide extensive analysis correlating their proposed metrics with human evaluations. The tables and graphs are informative and concise, and relay the information effectively. I found the comparison of inter-evaluation  between their metrics and human evaluators to be particularly helpful, as it did provide some better evidence. If spearman is that much higher than pearson though, the authors may have some issues with ties? It would be something to double check. The extensive graphs of correlations were also nice, but some of these graphs could be relegated to the appendix to assist with length issues (The authors barely extend to 9, which means they are supposed to have a harsher review). \n\nThe authors have a fundamental flaw that they don't seem to address: that their metrics can be pretty easily gamed by anyone that does some sort of gpt-2 fine tuning as their proposed model. In this way, their evaluation heavily depends on the ability to have a language model that is more powerful and stronger than the model it is evaluating. In fact, even gpt-2 big is far from producing extremely high quality generations, and so the authors would find themselves hitting that ceiling very easily. (The fact that they use the 12-layer GPT2 makes this even more problematic).\n\nThe authors also have a bit of an issue in that their coherence metric is not really fundamentally different than ppl of neural language models. The wordnet-substitute weighted ngram model is different, but really just harkens back to pre-deep learning language modeling.\n\nThe work would be made significantly more robust by having a *large* number of models being compared, rather than just one model evaluated on a number of utterances. Really the authors have only shown that they can evaluate *THIS ONE SEQ2SEQ MODEL*, and NOT general generations. I would not trust papers which would use this as their evaluation metric.\n\nMissing citation: probably CIDEr. They could also dig up an earlier reference for language modeling than Bengio 2003.\n\n\nNits\n- \u201cBLEU\u201d is mispelled as \u201cBLUE\u201d throughout the paper; \u201cSoftmax\u201d is both capitalized and not.\n- The overall grammar of the paper is lacking, and impacts the legibility of the paper\n- It would be good to have tables on the same page as they are mentioned in the text.\n"}