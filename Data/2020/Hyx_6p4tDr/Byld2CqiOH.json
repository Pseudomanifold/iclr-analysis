{"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, a bilinear form based on a 3D tensor with low rank frontal slices is proposed in order to reduce the number of operations and memory requirements. Additionally, it is proved that, if factor matrices are randomly chosen and following predefined distributions, then the obtained fusion map is an unbiased estimator of some specific RKHS operators. I found interesting the fact that, instead of learning the projection matrices, this method proposes using randomly generated matrices. However, it is not clear the advantage of using randomly generated matrices in terms of computation cost and performance compared with matrices learned from data. Below, I list the issues of the paper:\n-\tIn this paper, it is suggested that using random matrices is better than learning them from data, but it is not clear what is the advantage. From the computational point of view, it is intuitive that learning matrices will increase the computation cost but there is no comparison or theoretical evaluation for this. Also, it is not clear if the performance using learned matrices will be better or worse than the proposed in the randomized algorithm. A comparison against learned projection matrices would clarify this point.\n-\tThe authors prove that using random matrices can be seen as an approximation of using RKHS (Theorem 1 and 2). However, what is the computational cost of using the RKHS operators instead? This should be clarified to highlight the advantage of the method. Additionally, comparisons with experiments using RKHS operators could give a better evaluation of the method.\n-\tIn the Introduction, it is suggested that standard deep convolutional neural networks extract only first-order information. I cannot agree with this. In fact, the use of nonlinear mappings such as sigmoid or ReLu functions has the effect of introducing higher order information. I think this should be clarified in the paper.\n-\tIn page 3, it is mentioned that \u201cVia sampling tensor entries from different distributions\u2026\u201d, but I see that what is actually controlled is the distribution of projection matrices entries instead, which, of course affects the distribution of tensor entries. I think this should be clarified in the paper.\n-\tQuality of plots in Fig. 1 are rather low. I would suggest to use bigger fonts, to set the same y-axis limits in each row, and to use thicker lines, etc.\n"}