{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper tackles dimensionality problems arising in bilinear pooling algorithms. These algorithms typically involve bilinear multiplication of third-order tensors (neural network weights) against second-order data features. When the features are high-dimensional, the tensor-product becomes computationally very expensive. The paper proposes to impose low-rank on the third-mode of the tensor; i.e., write the tensor mode as a sum of rank-one outer-products. In this case, the tensor-product reduces to an inner-product between data features projected into these rank-one components, which can then be evaluated cheaply. However, the question remains how close these projections are to the original tensor. The paper explores three possible distributions from which the projection matrices can be sampled from and their kernel linearization quality: i) Radamacher distribution, ii) Gaussian distribution, and iii) random orthogonal frames. The approximation quality of the tensors with these choices are theoretically investigated. Experiments are presented on two datasets, and show some promise.\n\nPros:\n1. The specific approach of low-rank tensor approximation and investigations into kernel approximation qualities, as is done in this paper, is perhaps marginally novel in the context of bilinear pooling.\n2. Theorem 2 is perhaps interesting, and it sort of extends some of the observations in Yu et al., 2016 into the specific setting of tensor-products.\n3. Experiments show some marginal improvements against baselines.\n\nCons:\n1. The key contribution in this paper do not appear significantly novel. The low-rank approximations are perhaps straightforward, as all it does is a direct re-writing of the standard tensor product as a matrix inner product along a mode.  The analysis of approximation qualities are mostly taken from prior works such as Yu et al., 2016 and Kar and Karnick, 2012. The proofs are almost direct extensions of technicalities from these papers. \n\n2. While, the motivations for the paper are to use bilinear pooling in a deep learning context, the analysis and technical details are assuming kernel linearization, and thus for kernel-based methods. This looks to me like a disconnect in what the paper wants to achieve. How would you justify the theoretical results when the low-rank factors are learned via back-propagation? How would you impose a specific distribution to these projection matrices? The experiments presented in the paper only consider using bilinear neural networks. Thus, it is unclear what is the purpose of the theoretical results, and what is it attempting to justify. In this light, I think the theoretical results are misplaced with regard to the goals of the paper.\n\n3. The experimental results are not entirely convincing; the computational complexity arguments the paper used in its favor do not appear that compelling in the experiments, (O(D\\sqrt(d)+d) against O(D+dlog d) of Gao et al, 2016 in Table 1), the accuracy numbers are  not consistently better, and are often inferior due to the approximation. \n\nOverall, the contributions appear marginal in comparison to prior results, and the results are not sufficiently convincing."}