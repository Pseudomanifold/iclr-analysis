{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary of the paper:\n  \n        The paper proposes a Bayesian approach to make inference about latent variables such as un-corrupted images. The prior distribution plays a key role in this task. The authors use a GAN to estimate this prior distribution. Then, standard Bayesian techniques such a Hamilton Monte Carlo are used to make inference about the latent variables.\n\nDetailed comments:\n\nEq. (8) is expected to give very bad results. The reason is that it is very unlikely to sample from the prior configurations for z that are compatible with y.\n\nThe paper does not address learning any model parameters. e.g. the amount of noise.\n\nA more principled approach would be to estimate the prior parameters using maximum likelihood estimation. That has already been done in the case of the\nvariational autoencoder.\n\nThe variational autoencoder is an already known method that can be used to solve the problem formulated by the authors. It also automatically proposes\nan inference network that can be used for recognition. If the likelihood is Gaussian and p(x|z) is also Gaussian, one can directly marginalize x and work\nwith p(y|z) and p(z). The authors should at leas discuss the potential use of this method alongside with the BIGAN model which also provides a recognition model.\n\nIt is not clear how the HMC parameters are fixed.\n\nThe experiments do not have error bars (Figure 4.) This questions the significance of the results.\n\nMy overall impression is that there is little novelty in the proposed approach. Namely, using a GAN to learn the prior distribution, and then very well known\ntechniques to infer the original input image.\n\nI have missed some references to related work on inverse problems. An example is:\n\nhttps://arxiv.org/pdf/1712.03353.pdf\n\n\nIs the original figure contained in the training set used to infer the GAN. If so that can lead to biased results.\n\nI have missed a simple baseline in which one simply finds the training image that is closest to the corrupted observed or partially observed image.\n\nMy overall impression is that there is not much novelty in the paper as it is simply a combination of well known techniques. E.g. GANs and Bayesian inference with Monte Carlo methods.\n\n"}