{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper propose an approach for self-supervised learning on time series.\nThree datasets are considered (simulation and 2 healthcare datasets).\nThe gist of the contribution is to both optimize prediction loss\nof the true task and at the same time do a good job for a family\nof auxiliary tasks. 4 auxiliary tasks are considered. While\nthe first 3 auxiliary tasks are quite common, the 4th tasks\ncalled piecewise-linear autoencoding appears novel. The idea\nis that the hidden representation of the LSTM should be a good predictor\nof the past using a piecewise-linear approximation.\nThe author coin the term \"limited self-supervision\" for their approach\nalthough it's not clear why it is fundamentally not just self-supervised\nlearning as it has been proposed in the past.\n\nThe paper is overall well written and addresses the relevant issue\nof learning from limited annotated data.\n\nMajor concerns\n\n- It is yet another way to do self-supervised learning on time series\nand no clear benchmark with alternatives is provided (time contrastive\nlearning (TCL) or Contrastive Predictive Coding (CPC) https://arxiv.org/pdf/1807.03748.pdf\netc.)\n\n- On any of the applied problem it is not clear if the proposed\napproach brings an improvement on the state-of-the-art or if it's\njust an illustration of the method disconnected from the literature\nof the application.\n"}