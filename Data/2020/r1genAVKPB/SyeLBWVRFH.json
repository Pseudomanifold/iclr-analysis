{"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\nThis paper presents theoretical lower bounds on sample complexities to learn good policies in reinforcement learning. The derived theorems show that there exists MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning. These results constitute the first lower bounds for RL with linear function approximation.\n\nRepresentation learning is an important area of research and this paper advances our theoretical understanding in a notable way, helping to elucidate the limits of representation learning alone. The lower bounds derived in the paper would be of particular interest to the community as they can apply to a wide range of function approximators, including neural networks. Although this is not my area, the contributions are well-explained in the context of previous work and the theory was fairly easy to follow. The discussion also contained interesting points and summarized possible implications of the theoretical results. Overall, I think this paper presents a solid contribution and recommend acceptance.\n\nAlthough the paper was clear in general, I would like to have certain points clarified:\n1) Just to check, is it still possible that there exists certain representations that are not perfect but do lead to sample efficient learning? If I'm interpreting the results correctly, the theorems only posit the existence of a representation that is good in the sense of approximation error, but bad in terms of sample complexity, which does not necessarily preclude the possibility of other efficient representations.\n2) More generally, why is it that there are few results for lower bounds when it seems like an obvious direction? Are there technical barriers to proving such results?\n3) For value-based learning, the good representation has approximation error \\Omega(\\sqrt(H / d)). Could the authors explain why this assumption on the error is reasonable? \n3) In assumption 4.4, the features are assumed to have an l2-norm of 1. This seems like a fairly restrictive assumption. How important is this assumption and can it be relaxed?\n4) Also, in theorem 4.2, it is assumed that the dimension of the features, d = H. This would seem to allow the possibility of policy-based learning being sample efficient when the number of features is much smaller. \n\nThe paper is well-polished with few noticeable typos.\nMinor comments:\n- p.5 sec3.3: \"knows the whole transition\" -> \"knows the whole transition function\"\n- p.8 sec5.1: \"our lower bound on policy-based learning thus demonstrates\" It may be worth reminding the reader that the bound applies to perfect representations -> \"our lower bound on policy-based learning---which applies to perfect representations---thus demonstrates\""}