{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors study the diagonal circulant neural network (DCNN), where the weight matrix is a product of alternating diagonal matrices and circulant matrices. They provide a theoretical analysis of the expressive power of DCNNs. Furthermore, they propose an initialization scheme for deep DCNNs. The performance of the DCNNs is evaluated on synthetic and real datasets. \n\nThe theoretical analysis seems interesting. However, the experimental results do not seem to support the effectiveness of the DCNN. The performance of DCNN, for example on CIFAR-10, is much worse than the state-of-the-art models, which makes the model barely usable by practitioners.\n\nThe baseline compression-based models are not the latest. More recent compression or pruning methods might be considered as well, for example, SqueezeNet, The Lottery Ticket Hypothesis. \n\nFurthermore, there is no empirical analysis of the new initialization scheme.\n\nIt would be interesting to see any experiments corroborating the theoretical results, for example, Lemma 1. That is to compare the performance of a deep ReLU network of width n and depth L and a DCNN N 0 of width n and of depth (2n-1)L.\n"}