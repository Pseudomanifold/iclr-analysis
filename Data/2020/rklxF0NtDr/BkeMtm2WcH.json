{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes Policy Message Passing, a Bayesian GNN which models edge message passing as a mixture distribution with corresponding coefficient generated by a learnable prior defined on graph state. I think the motivation is reasonable and the experiment on Jigsaw Puzzle is impressive. But the writing and other experiments can be further improved, as detailed below.\n\nSec 3.1: What does ```````+1 mean in Eq. (6)?\n\nSec 3.2: The choice for the variational distribution q is not mentioned. Is q a parametric model (e.g., NN) or non-parametric? It is hard to connect prior, posterior and variational posterior in Sec 3.2 to the reasoning agent/message formulation in Sec 3.1. I suggest reformulating Sec 3.1 & 3.2 to give a thorough presentation of the training objective, model structure and optimization details of PMP.\n\nModeling graph using graph neural network with Bayesian framework has been investigated in several papers, e.g., [1-2]. They also take a probabilistic perspective. I think this paper should discuss the connections to previous Bayesian GNNs and compare their performance with PMP at least in Sec 4.2. Besides, GNN with edge information (e.g., [3]) has also been investigated in several papers. According to my understanding, PMP can also be regarded as a GNN with latent edge information (modeled with a learnable prior and inferred with a variational posterior). So I think adding discussion/experiment to them can better support your claim.\n\nAs for the experiment part, my major concern is whether the comparison with other models (e.g., GCN and GAT) is fair. It seems that the number of parameters of PMP is much larger than GCN and GAT. Could you provide details for the model size and the training/inference time cost?\n\nSec 4.2. What's the performance of PMP when no noisy edges is added? \n\n[1] Bayesian graph convolutional neural networks for semi-supervised classification, AAAI 2019\n[2] Bayesian Semi-supervised Learning with Graph Gaussian Processes, NIPS 2018\n[3] Exploiting Edge Features for Graph Neural Networks, CVPR 2019\n"}