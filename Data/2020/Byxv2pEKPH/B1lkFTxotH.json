{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper introduces a new type of layer, Farkas layers, that are designed to ameliorate the \"dying ReLU problem\". The idea of repurposing Farkas' lemma from linear programming to a component in NNs is very nice. However, the paper doesn't have convincing baselines and doesn't dig deep enough into what the Farkas layer is actually doing.\n\nComments:\n\n1. Start of section 2.2: \u201cPrevious work on the dying ReLU problem, or vanishing gradient problem, in the case of using sigmoid-like activation functions, has heavily revolved around novel weight initializations\u201d. Dying ReLUs and vanishing gradients are different problems. In particular, it doesn\u2019t make sense to talk about the dying ReLU problem for sigmoid-like activation functions.\n\n\n2. Batchnorm (BN) consists in two operations: shifting and scaling. Both are relevant to vanishing gradients. However, only shifting (that is, subtracting the mean and resetting to a learned value) is relevant to dying ReLUs because rescaling the input to a ReLU by a positive number doesn\u2019t affect whether the ReLU is subsequently ON or OFF.\n\n\n3. Given point #2, a natural baseline to compare the Farkas layer against is a \u201cpared-down BN\u201d, which shifts but does not rescale.\n\n\n4. Similarly, when combining the Farkas operation with BN, it might be worth considering keeping BN\u2019s rescaling but dropping the shift -- since Farkas is analogous to the shift operation in BN.\n\n\n5. I claimed above that Farkas is analogous to the shift in BN, but haven\u2019t thought about it deeply. Do you agree? Any comments on how they differ and why?\n\n\n6. Our contributions, p2: \u201cWe empirically show an approximate 20% improvement on the first epoch over only using batch normalization.\u201d I\u2019m not sure what to make of this; improvements on the first epoch are only useful if they lead to overall improvements. \n\n\n7. Figure 4 of \u201cShattered gradients\u201d, https://arxiv.org/abs/1702.08591 looks at ReLU activations by layer, both with and without BN. It\u2019s worth doing a similar analysis for Farkas layers. Concretely: how do Farkas layers change the pattern and frequency of activation, both at activation and during training?\n\n\n8. Guaranteeing the activity of a single neuron per layer seems very weak. What is the empirical effect of Farkas on the number of live neurons? Is it really just making sure one ReLU is on, or does it do better? Is it possible to ensure more neurons are ON in each layer? The \u201cshattered\u201d paper above suggests BN sets close to half ReLUs as ON at initialization, and approximately controls how often ReLUs are ON or OFF during training via the shift. \n\n\n9. As a broader point, the paper proposes an algorithm based on a hypothesis: that having (at least one) ReLU on helps training. It\u2019s worth digging into the hypothesis a bit rather than just plotting training curves. \n\n\n10. I would expect ResNets have much *less* of a problem with dying ReLUs than standard NNs because of the skip-connections. One would therefore expect Farkas layers to help more with standard NNs than ResNets. However, the reported results are for ResNets. What happens when there are no skip-connections? \n\n\n\n"}