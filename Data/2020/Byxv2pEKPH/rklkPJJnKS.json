{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose a new `normalization' approach called Farkas layer for improving the training of neural networks. The main idea is to augment each layer with an extra hidden unit so that at least one hidden unit in each layer will be active. This is achieved by making the extra hidden unit dependent on the rest of the units in the layer, so that it will become active if the rest are inactive, and they name it after Farkas' lemma in linear programming. This avoids the gradient becoming zero when all the units in a layer are dead. \n\nThe empirical results show that this normalization method is effective, and improves the training of deep ResNets when no batch normalization is used. The accuracies on CIFAR10 and CIFAR100 are improved with the use of Farkas layers. Unfortunately it still cannot beat or replace batch normalization. When batch normalization is used, the benefit of using this Farkas layer becomes marginal (Tables 1 and 2). \n\nI am also not completely satisfied with the authors' explanation on why Farkas' layers work. The authors motivate the design of the layer with dead hidden units, but in the experiments they do not show if any layer actually becomes completely `dead' (or gradient becomes very small) when Farkas' layer is not used. There could be other reasons why the layer helps, other than keeping some units in a layer active. \n\nOverall I think the idea is novel and interesting, but the improvement is not big enough to replace existing normalization methods that makes this paper slightly below the acceptance threshold in my opinion.  \n"}