{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper provides a new method to deal with the zero gradient problem of ReLU (all input values are smaller than zero) when BN is removed, called Farkas layer. This Farkas layer concatenates one positive value to guarantee that, at least one neuron is active in every layer to reduce the challenge for optimization. Compared with the method without BN, Farkas layer shows better results on CIFAR-10 and CIFAR-100.\n\nThough, I still have several concerns:\n1.\tThe proposed Farkas layer is too simple and seems not work well. With BN, the FarkasNet does not show significant improvements than the traditional ResNet with BN on CIFAR-10, CIFAR-100 and ImageNet.\nWithout BN, though FarkasNet shows significant improvements than ResNet. But FarkasNet without BN cannot achieve comparable performance with FarkasNet with BN. With deeper networks, the performance further goes down, which really downgrade the rating of this paper.\nIn Fixup, ResNet w/o BN with mixup can achieve comparable performance with ResNet with BN on ImageNet. Could FarkasNet further improve the performance in the setting of ResNet w/o BN with mixup and fixup init? This would much more improve the application value of the proposed FarkasNet.\n\n2.\tFor the results of CIFAR-10 and CIFAR-100, the error bar should be added to make the results more convincing. \n\n3.\tFigure 7, the training curves seem weird. Why the training error goes up in some stages? \n"}