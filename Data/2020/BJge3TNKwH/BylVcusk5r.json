{"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "I think the paper is written quite well, and the approach makes a lot of sense. I think the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. The authors put in some effort in explaining why they propose this alternative distance between distributions. \n\nI think overall this is a great paper, very informative. If I need to nitpick, I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method. \n\nI think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community. "}