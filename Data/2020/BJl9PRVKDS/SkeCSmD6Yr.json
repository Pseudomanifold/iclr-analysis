{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\nThis paper proposes a functional characterization to understand the empirical success of deep neural networks. In particular, this paper focuses on the case of deep fully connected univariate ReLU networks, and show that the parameters will result in a Continuous Piecewise Linear (CPWL) approximation to the target function. Moreover, the authors derive the induced distributions of the function space parameters and show that increasing width can reduce the roughness of the initial function.\n\nBesides, this paper analyzes the loss surface in the function space and reveals some relationship between the critical points in the function space and original NN parameter space. Furthermore, a type of gradient descent dynamic in the function space has also been derived.\n\nMany experiments have been conducted to reveal how the expressiveness and optimization performance varies with the neural network width and depth.\n\nOverall, the functional characterization is interesting can potentially help explain the generalization/expressiveness of deep neural networks. However, this paper is not well written and organized, and there are some \u201c??'s\u201d appearing on page 4. The authors should pay more attention to improving the writing and organization of this paper. \n\nHere are the detailed comments:\n\nThe statements of theorems are not clear. For example, Theorems 3 and 4 convey too much information, I believe the authors should simplify the statements to make them more concise.  Moreover, the purpose of these two lemmas is not clear. Do they imply something related to expressiveness?\nThere should be some discussion in the surrounding text of Theorem 5. Some notations are also missing. For example, what\u2019s $\\hat \\epsilon(t)$?, what\u2019s $a_i(t)$? Besides, what\u2019s the purpose of Theorem 5. It seems that this Theorem is used in the main part of this paper.\nOne major drawback of this paper is that it only provides the theoretical analysis for two-layer networks, but the title and claimed contributions are related to deep ReLU networks. I wonder whether the theory can be extended to deep cases?\nIn Theorem 2, I think He initialization will give you $\\sigma_w = \\sqrt{2/(H+1)}$ and $\\sigma_v = \\sqrt{2/k}$, if the output dimension is k. In this way, I am curious whether Theorem 2 can still hold. Besides, what\u2019s the meaning of the so-called \u201croughness\u201c?\nI don\u2019t get the point in Implications of Corollary 1. For example, what do you mean \u201c$f$ has significant curvature in the boundaries?\u201d, why an initialization that allocates more breakpoints to the area where the curvature of $f$ lies can be faster to train? The authors should elaborate more on this.\n"}