{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper wants to answer the question what is the value of the neural network\u2019s depth? The author targets at Deep ReLU Network and uses Continuous Piecewise Linear (CWPL) to analyze the network\u2019s parameter distribution. The main contributions of this paper are as follows, (1) For common initializations, this paper proves a deeper model will lead to flatter approximations and better approximation over a broader range of inputs. (2) A deeper model performs better when one optimizes with (Gradient Descent) GD methods. (3) Flat Initialization in the overparameterized regime could explain generalization. They found that the value of depth in deep nets seems less about expressivity, but enable GD to find better solutions.\n\nMy decision is Weak Accept, considering the following aspects.\nPositive points: (1) The theory seems solid, authors prove breakpoint and delta-slope distribution will influence by the depth of the network. (2) The conclusions of the paper are inspiring, e.g., depth makes it easier for GD to the optimizer.\n\nNegative points: (1) For the experiment, the authors find breakpoints can\u2019t migrate very far from their initial location. I hope the author could explain this phenomenon since it is very crucial to proving the importance of the breakpoint\u2019s initial distribution. (2) Some formulas in the appendix parts are beyond the scope of the page. \n\nSuggestions: I think that a figure that shows breakpoint and input data distribution together will be very interesting. I want to see the breakpoint\u2019s distribution change as training and its relationship with input data distribution.\n"}