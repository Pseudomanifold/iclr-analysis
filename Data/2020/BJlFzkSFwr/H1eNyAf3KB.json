{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the effects of joint learning of multi-level tasks: sentence classification and sequence labeling using a hybrid architecture of bidirectional LSTM and multi-head attention. They introduce a modification of the multi-head architecture to create a sentence representation and auxiliary objectives to enforce some relationship between the two tasks. The experiments comparing different settings of the models demonstrate that the models jointly learned using the two tasks are somewhat better than individual task-specific models. In addition, the zero-shot model also performs better than the baseline in the sequence labeling task.\n\nThis paper requires some clarification and more experiment results. I am leaning toward rejecting this paper for the following reasons:\n\nIt is unclear on the novelty of this paper\u2019s contribution. It has been shown that multi-task learning objective is helpful. While the majority of recent work has focused on transfer learning (from LM objective) and weakly related tasks, the direction of the multi-level learning task has been explored to some extent, such as Hashimoto et al., 2017 (building from bottom-up) and Williams et al., 2017 (weakly supervised lower-level task). I think there should be more effort into distinguishing their findings from the previous. Additionally, the paper should build on top of a well-known architecture for a greater impact. For example,  using the multi-head attention architecture to perform classification is discussed in Devlin et al. 2018. A comparison with existing models, both theoretically and empirically, will be very helpful to readers.\n\nThe proposed method does not seem well justified by the experiment results. As mentioned previously, the paper lacks a comparison with existing models to justified architectural and training choices. Furthermore, the internal comparisons show that the proposed model has a small improvement from the individual models on both sentence-level and token-level tasks. As for the other auxiliary losses, we see all of their effects combined, rather than individual ones. The ablation analysis would be helpful. Finally, I think the zero-shot experiment is interesting. Unfortunately, we cannot draw a solid conclusion here. The MHAL-zero has an additional signal from the weak supervision from the proposed attention loss which is somewhat related to how the sentence labels are created (a big improvement on CoNLL-2003 and FCE). \n\nClarification Questions:\n1. How does this model work in the case of H != S and S > 2?\n2. How is the LM loss computed? I.e. Which representations are used to predict word distribution?\n3. In the semi-supervision experiment, I am not sure how you get \u201c38% on SST, \u2026\u201d for MHAL-zero. Additionally, can you explain \u201cUsing only 30% of data, MHAL already approaches its fully-supervised performance\u201d?\n\nHashimoto, Kazuma, et al. \"A joint many-task model: Growing a neural network for multiple NLP tasks.\" arXiv preprint arXiv:1611.01587 (2016).\n\nWilliams, Adina, Andrew Drozdov, and Samuel R. Bowman. \"Learning to parse from a semantic objective: It works. Is it syntax?\" arXiv preprint arXiv:1709.01121 4 (2017).\n\nDevlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n"}