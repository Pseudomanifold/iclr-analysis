{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Short summary:\nThis paper builds a two-level hierarchical text classification model; with token classification at the bottom and sentences classification on top of token classification results. This work is closely related to a previous art (https://arxiv.org/pdf/1811.05949.pdf).  The model setup shares ideas with multi-task learning, in which the two separate tasks could be co-trained.\n\nNovelties:\nIt extends the previous work by:\n1) using separate head for each distinct tag label so that there is an explicit correspondence between attention heads and token labels.\n2) due to 1), the classification extends to multi-labels naturally (instead of binary labels). \n3) a unique regularization term is introduced to enforce distinct representation sub-space for each of the query vector (head-specific). This is formulated in Eqn (13). \n\nWeakness:\n* The novelties introduced above seem minor in my opinion. The importance of using the same number of heads for each distinct label is not tested or studied with ablation. In other words, with the normal setting of multi-head attention (with the head size not correlate with the size of tag label, for example, using fixed number of heads - 4 heads or 8 heads) thus not collapsing the queries into head-specific, would you get similar results? Is it really helpful to obtain a per-head representation to improve model performance on sentence-level classification? I know it probably helps human interpretation when you have per head representation. But beyond that, do you really get quality benefit?\n\n* The experiment results would be more strong, if there are more reference base lines from the recent work. Currently all the models are different variations from this paper, except one trivial \"random\" model (Table 1 and Table 2).\n\n* Another novelty ( bullet 3 above) introduced is the regularization term in Eqn 13. However, this does not have its own experiment. The current experiment (that involves this term) is always combined with two other changes (i.e. MHAL-joint). I will be interested to see experiment results (probably with another model with MHAL-sent + Rq or MHAL-sent+tok + Rq) to see if this additional regularization really helps.\n\n* limitations of this model: \n  - since the number of heads is bound to be equal to the number of tags, the model would be difficult to scale when the tag vocabulary is large. In all the experimental dataset the author has worked with, the tag vocabulary size are all pretty small (from 3 to 6). \n  - It seems we always need a way to  map between token labels and sentence labels. This does not seem practical in a lot of text classification scenarios.\n  - To get the max benefit from this model, we need good quality of token-level labels and sentence-level labels, which also seems not very practical in real NLP tasks. \n"}