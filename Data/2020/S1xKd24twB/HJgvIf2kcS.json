{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an imitation learning approach via reinforcement learning. The imitation learning problem is transformed into an RL problem with a reward of +1 for matching an expert's action in a state and a reward of 0 for failing to do so. This encourages the agent to return to \"known\" states from out-of-distribution states and alleviates the problem of compounding errors. The authors derive an interpretation of their approach as regularized behavior cloning. Furthermore, they empirically evaluate their approach on a set of imitation learning problems, showing strong performance. The authors also stress the easy implementability of their approach within standard RL methods.\n\nI think this is a good paper which shows strong empirical results based on simple but effective idea. I would welcome an extended discussion of certain aspects of the experiments though -- for instance the trends in Figure 3. Furthermore, I think the huge gap in performance between SQIL and RBC warrants a more detailed discussion/analysis. Also the papers by Sasaki et al. (which was accepted at last year's ICLR) [and (maybe) the paper by Wang et al.] deserve to be discussed earlier in the paper (introduction?) and make it to the experimental results. \n\nThe paper could be improved considering the following minor suggestions:\n* Introducing the important bits of soft-Q learning formally.\n* Defining demonstrations formally (not only in the algorithm).\n* Showing the necessity to balance demonstrations and new experiences in the algorithm.\n* Removing implementation details from Section 2 and adding them to the experiments.\n* There is a bunch of repetitive paragraphs, that could be cleaned up.\n* Equation (2) and (5) are the same. Use $\\pi^*$ and $Q^*$ in (2)?\n* I disagree with paragraph following equation (6). If we phrase BC as an optimization problem over Q then Q should satisfy consistency. Otherwise, we are not optimizing over Q but some other object. \n* Please specify the initial state distribution."}