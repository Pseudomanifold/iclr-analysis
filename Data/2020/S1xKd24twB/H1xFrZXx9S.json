{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a simple method for imitation learning that is competitive with GAIL.  The approach, Soft Q Imitation Learning (SQIL), utilizes Soft Q Learning, and defines high rewards as faithfully following the demonstrations and low rewards as deviating from them.  Because SQIL is off-policy, it can utilize replay buffers to accelerate sample efficiency.  One can also interpret SQIL as a regularized version of behavioral cloning.\n\nThe authors really play up the \"surprising\" connection of SQIL to a regularized behavioral cloning.  But this isn't really that surprising in the general sense (although I applaud the authors for rigorously defining the connection).  SQIL is basically adding \"pseudo-data\" and the idea of using pseudo-counts as regularization has been around for a long time.   I think it's a stretch to hype up the \"surprise\" factor so much, and that it does a disservice to the paper overall.\n\nThe experiments are sound, showing competitive performance to GAIL, and also having a nice ablation study.  \n\nI wonder if there was a quantitative way to demonstrate that SQIL is \"simpler\" than GAIL.  It's certainly conceptually easier.  Is the implementation in any concrete sense easier as well?  Or sensitivity to hyperparameters or initial conditions?"}