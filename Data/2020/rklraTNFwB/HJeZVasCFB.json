{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors present a method of transferring template-based instruction following agents to natural language instructions by using language encoders trained on large text corpora. They explore different ways of combining text-based language encoders with visual representations and compare them. They find that contextual phrase-based representations learned by BERT significantly improve the performance on natural language instructions. \n\nStrengths:\n- The paper is written well, it is easy to understand and follow.\n- The task setup is good, authors collect natural language instruction data from humans.\n- The paper presents several language encoding methods for the task and systematically evaluates them in a scientific manner.  \n- The experimental results indicate that it is possible to transfer an agent trained on template-based instructions to natural language instructions using language models trained on large text corpora. It is not necessary to train the agent on natural language instructions. I find this result important and useful.\n\nWeaknesses:\n- The paper lacks significant technical novelty. It essentially combines known reinforcement learning based instruction following agents with known language models. The different ways of combining language encoding with visual representations are either trivial or adapted from prior work.\n- A major concern is that the natural language instructions considered in the paper do not have much diversity with respect to language. The paper only considers lifting and putting tasks and trains a separate model for both the tasks.  \n-- The lifting task always uses the verb 'lift' and replaces the object word with synonyms or referring expressions. There are 80 objects in the lifting task and I suspect there are very few referring expressions for these objects and they mostly involve a synonym. Furthermore, at test time, the agent only needs to distinguish between 2 objects. The performance with random embedding is around 50% for this task and the best model is around 76% which means the agent is not recognizing the correct object around 50% of the time. \n-- For the putting task, authors consider synonyms for object words and natural instructions which involve changing the verb \u2018put\u2019. It seems like humans mostly use only 4 verb words for this task, \u2018put\u2019, \u2018keep\u2019, \u2018move\u2019, \u2018place\u2019. This might be an artifact of the examples given to the human annotators. In any case, this word is inconsequential as the agent always lifts one of 3 available objects on one of 2 fixed objects. \n- It seems like the most diversity is coming from synonyms which can probably be handled with a dictionary or wordnet rather than requiring a language model. There is also some prior work on handling synonyms (https://arxiv.org/pdf/1902.04546.pdf). I would have liked to see many more tasks and a multi-task learning model which is also able to distinguish between the task based on natural language instructions in addition to understanding object word synonyms and referring expressions. More objects would also help.\n- The authors claim to tackle \u201cmore behavioural and environmental challenges than previous work\u201d. I do not agree with this claim. It is true that this paper handles object interaction and natural language instructions in a partially observable setting, however, previous work has tackled other challenges which this work does not tackle. For example, Oh et al. 2017 generalize to new sequence of instructions, Hermann et al. 2017 and Chaplot et al. 2018 also handle compositionality and generalize to unseen instructions referring to new objects, Hermann et al. 2017 handle negation, Chaplot et al. 2018 handle instructions involving \u2018largest\u2019 or \u2018smallest\u2019 objects, Misra et al. 2018 handle more diverse natural language and so on.\n- I wouldn\u2019t call moving objects using high-level symbolic actions as \u2018manipulation\u2019. This is a whole research area in robotics involving taking low-level actions to move an object. Also, the environment used in the paper is not visually realistic in my opinion. It looks game-like and visual encoders trained in this environment are unlikely to generalize to the real world. This is fine as it is mostly irrelevant to handling natural language instructions, but authors should not claim visual realism and object manipulation in my opinion. \n\n\nComments/Questions\n- I do not understand the meaning and purpose of some actions. Why are there GRAB + actions? Doesn\u2019t the object move with the agent once it is grabbed? What is SPIN_OBJECT? Why is it needed? It seems like there is no \u2018place\u2019 action, how does the agent place the object? I am guessing when the agent stops taking GRAB+ actions. If that is the case, then wouldn\u2019t it be easier to just have Grab and Place actions rather than 16 GRAB+ actions?\n- The meaning of \u2018(sub)-\u2019 in (sub)-word is not described.\n- What is the probability of typo noise in the experiments?\n- Many experimental details are missing. How long was the model trained for both the tasks? How many training samples/episodes? What were the hyperparameters used for reinforcement learning? Learning rate, optimizer, discount value and so on."}