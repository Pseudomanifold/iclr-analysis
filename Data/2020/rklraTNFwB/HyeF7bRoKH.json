{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes applying natural language encoders pre-trained on a large text corpora (e.g. BERT) to training agents to follow natural language instructions in a simulated environment. Overall, I greatly enjoyed reading this paper: clear exposition of the idea, sensible model architecture, reasonable baselines and good experimental performance. I only have minor questions & feedback, see below.\n\nPros\n- Well designed experiments with sensible baselines.\n- Strong transfer learning results.\n- Illuminating analysis where using BERT indeed performs better on capturing phrasal and sentence-level equivalence in natural language instructions.\n\nQuestions\n- In Table 5, BERT doesn't give performance improvement on natural instruction (over Word embedding+Transformers) until BERT+CMSA+TN. Why do you think this is the case? To phrase this in a different way, why do you think BERT+MP doesn't perform well on this?\n- Are the results in Figure 2 computed from a BERT+MP model or a BERT+CMSA model?\n- In the lifting results in Table 4, why doesn't BERT+CMSA+MP outperform BERT+MP?"}