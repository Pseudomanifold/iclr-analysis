{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This is one of those papers where the number of experiments conducted to produce the results is beyond the capabilities of \"almost all\" research groups. From the paper: \"we use 32 Cloud TPUs. The model is trained for 2 million iterations, which takes around 2 days.\" However, with that being said, it's a good paper of general interest to the community.\n\nThe paper focuses on self-supervised learning in video, and combines two contributions. The first is using a noise contrastive estimation loss (2016) which can be used for any visual dataset. The second is a cross-modal (BERT) model that requires language and vision. A few modifications over other BERT flavours are introduced. The cross-modal BERT is not tested alone, however when added to the NCE loss function, seems to suit a range of downstream tasks from classification to anticipation and captioning. NCE alone seems to clearly produce better results over published results, however these are not compared like-to-like, as published results are used for this comparison.\n\nThe paper is full of technical details to reproduce the results. This makes the main novelty is actually in showing that this approach works. However, the approach is technically sound and up to my knowledge has not been attempted before."}