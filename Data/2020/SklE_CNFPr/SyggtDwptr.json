{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is motivated to improve the performance of zeroth order stochastic search by incorporating a gaussian mixture as the candidate solution sampler. It is (almost) not possible to improve the state of the art zeroth order optimizer in all aspects, hence I expect a description in which situation the authors try to improve the performance by the proposed technique. However, it is not mentioned in the paper. \n\nThe experiments are far away from convincing. The authors pick only two very simple functions, convex one and non-convex but very simple one. The experimental results are compared only with the two baselines of the proposed algorithm. I have to say that the experimental design of this paper must be reconsidered to conclude and derive the goodness of the proposed algorithm.\n\nFirst of all, the effect of introducing non-identity covariance matrix is usually to tackle the ill-conditioning of the objective function, as is well discussed in papers addressing the covariance matrix adaptation in evolution strategies (CMA-ES) by Hansen and his co-authors. However, the objective function doesn't seem to be so. Therefore, this effect can not be tested from this experiments. It is not at all clear what the authors want to claim from these experiments. \n\nIf the non-isotropic part is not important but the scaling factor of the covariance matrix matters (for the step-size adaptation effect), one must compare algorithms using isotropic covariance matrix but with step-size adaptation mechanisms. For example, random pursuit (Stich, SIOPT 2013) is a hill-climbing algorithm with randomly sampled direction with a line search. It also has a convergence results on convex functions. (1+1)-evolution strategy is another random direction hill-climbing algorithm with a step-size control mechanism. (Morinaga and Akimoto, FOGA 2019) has shown its linear convergence of this algorithm on strongly convex functions and beyond. These algorithms works reasonably well without tuning the hyper-parameters. In the proposed approach, there are several hyper-parameters that needs to be tuned to guarantee the convergence and perform reasonably. What is then the goodness of the proposed algorithm compared with above mentioned algorithms?\n\nTheorems provided in the paper doesn't show the goodness of the proposed algorithm, but it tells that the idea of having a mixture model is not a bad idea. I do not see from these arguments the reason why a mixture model leads to a better performance than the baseline. Nonetheless, the theoretical analysis of this work is not rigorous, and if I understand correctly, the convergence (and its rate) of the algorithm is not formally proved. "}