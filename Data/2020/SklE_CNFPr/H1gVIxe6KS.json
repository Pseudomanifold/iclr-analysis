{"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to change the sampling scheme of evolutionary strategies to sample from a mixture of distributions (instead of the standard way of sampling from one Gaussian).\nAlthough this seems to provide slightly better performance, the contribution is incremental and more importantly the paper lacks clarity, especially regarding the theory part. The experiments are also not on par with what\u2019s typically expected in the literature. Overall, this paper is not ready for acceptance at a venue such as ICLR.\n\nEXP3.P algorithm\nThis is one of the key components of the algorithm presented by the authors but it is not properly explained in the paper. In fact, the algorithm implemented by the authors is very unclear to me. Below I quote the description given by the authors:\n\u201cOne can view that EXP3.P runs synchronously with Algorithm 5 in the way that step 4 of EXP3.P implements step 5 of Algorithm 5 and that step 5 and the subsequent steps of EXP3.P are conducted after step 8 of Algorithm 5 is finished\u201c\nThis is a very poor description of the algorithm. The authors should give a clear pseudo-code to explain the algorithm.\n\nTheoretical guarantees\n1) Again, the paper is very unclear on what algorithm is being implemented. The authors claim they can directly use the guarantees provided by Bubeck et al but this is unclear to me given that the algorithm is never fully stated. This needs to be clarified.\n2) The authors claim asymptotic convergence, specifically they claim \u201cTherefore, one might show that the proposed algorithm converges with a high probability.\u201c. This proof should be provided in the paper, not left as an exercise to the reader...\n3) Does Theorem 2 (Theorem 3.3 from Bubeck) require convexity of the function? Can you comment on guarantees for non-convex functions?\n\nPrior work\nThe authors do not give a clear description of prior work. There is a large literature on proving convergence guarantees under weaker assumptions than discussed in the paper, see e.g.\nY. Diouane, S. Gratton, and L. N. Vicente. Globally convergent evolution strategies. Math.\nProgram., 152:467\u2013490, 2015.\nY. Diouane, S. Gratton, and L. N. Vicente. Globally convergent evolution strategies for\nconstrained optimization. Comput. Optim. Appl., 62:323\u2013346, 2015.\nVincente. Worst case complexity of direct search\n\nExperiments\nThis is also a weak part of the paper.\n1) The authors only run experiments on very small datasets. Regarding the non-convex task, it would be much more interesting to consider a reinforcement learning task where evolutionary methods typically perform well.\n2) You should include stronger baselines too, e.g. https://arxiv.org/abs/1806.10230.\n3) An interesting question to look at would be to analyze the behavior of the algorithm as a function of the number of mixture components. Also, what is the computational increase in terms of the number of components? Consider giving plots in terms of computation time.\n"}