{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies the theory of sample efficiency in reinforcement learning, which is of great importance and has a potentially large audience. \n\nThe strong points of the paper:\n1. This paper proposed a new algorithm stochastic variance reduced policy gradient algorithms. This paper establishes better sample complexity compared with existing work. The key part of the proposed algorithm for variance reduction is to have step-wise importance weights to deal with the inconsistency caused by varying trajectory distribution.  \n2. This paper provides experimental results verifies the efficiency and effectiveness of the proposed algorithm. \n3. In addition, parameter-based exploration extension is discussed in the appendix, which enjoys the same order of sample complexity under mild assumptions and gives better empirical performance. \n4. This paper is easy to follow. In particular, there are a lot of discussions comparing this work with existing work. \n\nThe weak points of the paper:\n1. In section 3, it is not quite clear how the reference policy is defined, and the \\theta^s is not clearly defined when s >= 1.\n2. In the main part of the paper, the discussion in Remark 4.6 and the following Corollary 4.7 is not quite clear.\n\n\nSome minor comments of the paper:\n1. In introduce page 3, We note that a recent work by .... by a fator of H. --> Here H should be defined as the Horizon. \n2. There is one additional parenthesis in Theorem 4.5.  \n3. In  Corollary 4.7, T is not defined. \n"}