{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes a method that draws inspiration from neuroscience and aims to handle delayed feedback in continual learning (ie. When labels are provided for images after a phase of unsupervised learning on the same classes). It is an interesting idea, and worth exploring.\n\nI found the paper quite hard to follow at times, and I suggest the authors go through the paper in detail to address some of the issues with grammar and clarity of explanation.\n\nIn addition, I think the paper is lacking some grounding and context in terms of what problem is being solved and what previous work exists.\nFor example, based on the experimental setup section it seems like the problem being addressed is that of unsupervised learning on CIFAR images followed by supervised learning on images with labels (IE. Delayed feedback) - is this the case? If so, this is much more a semi-supervised learning or fine-tuning problem than a continual learning problem (which typically looks at a single class at a time or some other non-stationary sequence of tasks). Either way, the recent literature in semi-supervised learning and continual learning should be referenced - see the citations below as a few examples, and consider referencing and more closely perusing some of the examples in the cited review paper by Parisi et al (2019).\n\nLastly, the experiments show the performance as a function of queue length for different features and for CNNs versus decision trees, but there is no comparison to existing methods and very simple models are used - this means that again, it's difficult to gauge the efficacy of the approach and place this in the context of prior art.\n\nUnfortunately, I think the paper in its current state does not meet the bar for ICLR - I suggest the authors consult the vast literature in semi-supervised and continual learning, and try to place their work in this context, along with external comparisons.\n\n\nNguyen, Cuong V., et al. \"Variational continual learning.\"\u00a0arXiv preprint arXiv:1710.10628\u00a0(2017).\n\nLopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\"\u00a0Advances in Neural Information Processing Systems. 2017.\n\nMiyato, Takeru, et al. \"Virtual adversarial training: a regularization method for supervised and semi-supervised learning.\"\u00a0IEEE transactions on pattern analysis and machine intelligence\u00a041.8 (2018): 1979-1993."}