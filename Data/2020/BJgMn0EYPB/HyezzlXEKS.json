{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary\n========\nThis paper proposes a detection mechanism for adversarial examples based on capsule networks. The defense uses three distinct detection criteria to distinguish between benign and adversarial examples.\nExperiments on SVHN and CIFAR10 show that evading detection requires large perturbations that may change the class semantics of the attacked image.\nThe evaluation could be improved though, and does not sufficiently convince me of the validity of the proposed defense. For this reason, I am leaning towards rejection.\n\nComments\n==========\nThe detection mechanisms proposed in this paper are fairly natural and easy to understand. The authors also seem to have done an effort to consider adaptive attacks on their detection mechanism. Still, I think that the current experiments are not sufficiently convincing that this defense is indeed hard to attack. Addressing the comments below may help the authors in presenting a more compelling argument in favor of their defense.\n\n- The notion of \"deflected\" adversarial example proposed in this paper is simply the standard definition of a robust classifier. If a classifier is robust to attacks, then fooling that classifier should require making perturbations that change the input's underlying class. I think it's unnecessary to introduce some new terminology for this.\n\n- I have a hard time imagining that the perturbations in Figure 1 are indeed limited to eps=16/255 in the infinity norm. The changes are much too salient for such a small per-pixel perturbation. Some pixels have clearly changed from black to white or vice-versa.\n\n- The newly proposed cycle-consistent loss is natural but never evaluated. How much better do your model and defense perform thanks to this new loss? The ablation study in Section 5.3 does show the effect of the cycle-consistency detector, but not whether the loss improved the performance of this detector.\n\n- The assumption that the white-box adversary does not know the threshold used by the GTF defense is not realistic. An adversary could always try different thresholds and see which one performs best.\n\n- The multi-objective loss in (5) appears to be the right way to create a defense-aware attack. However, it is never discussed how the weights \\alpha_i are chosen. Optimizing these multi-objective losses is often fairly challenging, so a lot of care should be taken in choosing the right weights through extensive hyper-parameter searches. According to Figure 4, it seems that a single detector is often much more powerful than the two others. So an attack that only targets that detector might already do very well.\n\n-  The dual-stage optimization described in Section 4 does not seem optimal. Why not incorporate the misclassification objective into the loss in (5) as well?\nWith the proposed dual-stage method, why is the success rate of CC-PGD in Table 1 so low? If you first optimize for an example that fools the model, you should then at least aim to reduce the loss in (5) while maintaining the misclassification.\n\n- Experiments with hard-label (\"gradient-free\") attacks would be worthwhile to guard against gradient masking effects. Similarly, since this defense is attack agnostic, it would be good to evaluate against a spatial attack (e.g., rotations and translations) as the search space of these attacks can be brute-forced. \n\n- I couldn't find any mention of the clean accuracy of the networks you trained. \n\n- In Table 1, the success rate of white-box PGD on CIFAR10 (49.3%) is extremely low. This attack should succeed with 100% success rate at eps=8/255 if implemented correctly."}