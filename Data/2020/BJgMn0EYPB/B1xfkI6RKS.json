{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to detect adversarial attacks by checking consistency conditions coming from a reconstruction model. It shows some success in detecting them (around 50% detection rate at a 5% false positive rate on SVHN). It shows that many of the attacked images that evade detection are also misclassified by humans, although given that the allowed perturbation norm was large it isn't completely clear what to conclude from this.\n\nOverall assessment: There are some interesting ideas in the paper, and the adversarial evaluation is more thorough than most papers in the literature (although there are still a couple evaluation issues). There are several minor issues and a couple moderate issues that together make me hesitant to accept the paper in its current form, but I am open to revising my recommendation if these issues can be addressed.\n\nImportant issues:\n1. It is misleading to say that you introduce the notion of deflected attacks. Deflected attacks is the same as using a detection method which has been extensively considered, so much so that Carlini et al. wrote a 2017 paper bypassing ten different detection methods: https://arxiv.org/abs/1705.07263. In fact, many detection methods have been broken including the Defense-Gan method (https://arxiv.org/abs/1805.06605), which is fairly similar to the method proposed in this paper. it is certainly important to discuss this literature, and also to check the various ways of fooling detection methods to see if any of them break your approach.\n\n2. It is not meaningful to evaluate against white-box attacks or attacks that are not defense-aware. Consequently, most of the numbers presented in the paper are not meaningful and are misleading to include. The only meaningful numbers are those against the defense-aware, white-box attack. These other graphs should be removed and instead more extensive experiments with the current defense-aware attack and other strong attacks should be conducted.\n\n3. Some important sanity checks are not included. While the number of PGD iterations (200) is large enough in standard settings, your optimization problem may be more difficult due to the combination of four loss functions, and consequently you should test what happens if you increase the number of iterations and verify that the effectiveness of the attacker actually plateaus. More broadly it is important to verify that your optimizer is working well. For instance checking that you consistently hit zero reconstruction error in settings where this is known to be possible, and that attack effectiveness is 100% when the allowed ell-infinity norm is large.\n\nMore minor comments on the attack:\n-I also could not assess whether the step size was appropriately chosen because I couldn't interpret the units--does 0.01 mean 0.01 / 255 or 0.01 / 1? The former seems too small since we can't even reach the boundary of the ell_infinity ball in that case while the latter seems somewhat large.\n-Why do alternating gradient steps on the two losses instead of a single gradient step on the sum of the losses?\n\nPoint 3 is particularly important. It would be hard for me to accept the paper until I was convinced that the proposed defense-aware attack optimizer actually worked well enough to provide meaningful estimates of robustness.\n\nMinor comments:\n-Cisse et al. (2017) is not a certifiably robust model."}