{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to evaluate bias in pretrained language models by using a fixed sentiment system. The core of the idea is that if you generate with a given prefix from a language model the distribution of scores assigned by a sentiment system should be the same if the prefix is drawn for the same equivalence class (i.e. if the prefix includes different country names, or different occupations or person names). Several different prefix templates are tested and an objective is proposed for reducing this notion of bias (essentially applying transformations on unsupervised data corresponding to the equivalence classes that will be tested and then trying to match hidden states between transformed and untransformed instances). \n\nOverall, I have concerns about this paper on (a) methodological and (b) experimental grounds. \n\nMethodological: I can't understand the motivation behind the mixing of a generation system and a sentiment analysis system. While I recognize one could generate with an lm and then analyze its output with a sentiment system, why would one do this? This just doesn't correspond to any real usage of pretrained lm in nlp, or how any sentiment system would work. Pretrained lm are used to provide representations on human authored text and then processed for downstream tasks.  Why not just follow the methodology of Kiritchenko & Mohammad ( https://www.saifmohammad.com/WebDocs/EEC/ethics-StarSem-final_with_appendix.pdf ) and just compute difference in accuracy among templates where sentiment is known?\n\nExperimental: It seems like you are using the same augmentations for the training routine that will be testing your system on. Isn't it obvious that if you train your model to have equivalence for the small wordlists you constructed at training and then test with them again later, you will get improvement? I could not confirm any practical train/test separation between how the test was constructed and your proposed solution.  Furthermore, a simple missing baseline is just applying the augmentations and training with the LM objective and not doing any hidden representation matching.\n\nOverall I really like the direction of constructing equivalence sets and thinking about model behavior counterfactually but I feel this particular work has significant mythological and experimental flaws. \n\n\n\n\n\n"}