{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents two methods to debias the neural language models trained from large datasets. Concretely, one method is based on semantic similarity, i.e. two sentences describing the same content but with different control variables should have similar representation. The other method is based on sentiment similarity, i.e. regularizing on the predicted sentiments for sentences constructed with different control variables. There is no doubt that debiasing LM is important. The proposed methods also seem to be straightforward. My major concern is whether the resulting sentiment distribution associated with the prompts are desired and the generalizability of the method to broader categories of bias.\n\nDetailed comments:\n- Since the fairness metrics are defined based on sentiment prediction, it's surprising to see from Table 2 that the semantic similarity-based method is more effective than the sentiment similarity-based one.\n\n- From Fig 1, the distribution seems to become narrower, i.e. the predictions after debiasing seem to be more neural and less extreme. I'm not sure if this is what's desired for a debiasing method. \n\n- Other types of bias, which are beyond sentiment bias, exist in the language models too, e.g. the implicit association between general/race/nationality and properties. How to generalize the proposed method to other types of bias? \n\n- grammar: \"we can measure the the distance via cosine similarity.\""}