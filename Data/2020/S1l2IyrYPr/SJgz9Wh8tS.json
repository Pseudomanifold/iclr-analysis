{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nSummary\n-------------\nThis paper first measures sentiment bias in language models as reflected by text generated by the models. The models are conditioned on prefix texts that have certain attributes (e.g., gender), and then generate continuations. Then a sentiment classifier is used to evaluate whether the generated text exhibits bias. The classifier are used for calculating certain fairness metrics. To reduce bias, the paper adds other objective terms to the usual language modeling objective, where the new term encourages similarity between two texts generated by conditioning on attributes from different groups. This similarity may be employed at either the language model hidden representations or after a projection in the sentiment classifier. The proposed new models show reduced bias according to the fairness measures, while obtaining similar perplexity to the baseline model and slightly reduced semantic similarity of the generated text to the conditioning context. \n\nThe paper does a very good job of motivating and situating the work in light of recent research. The problem is important and timely. The adaptation of metrics from fair ML may be useful for other scenarios handling bias in NLP models. The proposed regularization techniques are interesting and may also be useful in other contexts. \nThe experimental setup makes some assumptions that are not completely justified in my opinion. In particular: (1) using a sentiment classifier to assess bias may be problemantic; (2) the templates may lead to mostly neutral texts; (3) the semantic-similarity measure may not be very informative; Please see detailed remarks below.  \n\nAccording to the ICLR call for papers, papers exceeding 8 pages will be held to a higher standard. At present, I think the paper needs to be improved to meet this expectation. I am willing to reconsider my evaluation depending on the author response.  \n\n\nMain comments\n----------------------\n1. The sentiment in generated texts is assessed via a sentiment classifier. The authors acknowledge that the classifier \"is not perfect and might exhibit some biases\" and \"leave investigation of an unbiased evaluator to future work\". I think this is a reasonable approximation, but would like to propose at least a small-scale human evaluation of sentiment in the generated texts.\n2. Besides the question of whether the classifier is biased, one concern that I have with the methodology is that most generated texts may not have a very strong sentiment signal in either direction. The templates and examples given in the paper and appendix suggest that most generated text might just be neutral in its sentiment. The histograms in figures 2+3 also hint at that. The fact that the histogram in figure 2 shows bias which is less evident in figure 3 is a good evidence to the issues discussed in the paper. However, if indeed most sentiment is neutral, the magnitude of bias might be quite small. I'm not sure how to interpret the magnitude of the various fairness metrics, but the numbers in tables 2+3 appear quite small. \n3. It makes sense to evaluate perplexity to judge the quality of the generated texts and the tradeoff with fairness. It is also important to evaluate whether the generated texts is faithfull/consistent with the prompt, which seems to be what the semantic similarity gets at. However I'm not sure it makes much sense to measure cosine similarity of a sentence representation and one word embedding. I am also not sure whether the semantic similarities are reasonable. The comment on how empirically irrelevant sentences happen when there is a >20% drop in semantic similarity is also made in passing. It may be good to re-consider how to measure semantic similarity, as well as provide more examples/statistics on what makes a large difference. In this context, a human evaluation may be very helpful. \n\nOther comments\n-----------------------\n1. The related work is comprehensive and sets the background well for the paper problem setup and contributions. \n2. The point made about specifying the fairness measure based on the distributions distance, rather than individual predictions, seems quite important. It does make sense to me. However, the proposed bias reduction methods then operate at an instance level, rather than the distribution level. Would it make sense to consider alternatives that work at the global level, such as posterior regularization?  \n3. Does the methodology extend to multiple attribute groups? That is, a sentence may have more than one kind of attribute, such as both gender and race. \n4. What is P^*_S? Is this the sentiment score distribution over the union of all subgroups a \\in A? Or something else? \n5. What are \\alpha's in section 4, bottom of page 5? Also, the features end up being the average of the top two layers, based on the intuition that sentiment is a high-level property that should be represented at the top layers. This seems reasonable, but have you tried other layers as well? \n6. The concern of over-regularizing with the embedding similarity sounds plausible. However, giving a weight \\lambda to the fairness objective, as is done, should be able to control that. I'm not sure this is a good motivation for the sentiment similarity, as that too may lead to over-regularization. In this context, do you back-prop through the sentiment classifier into the language model? \n7. The details on the sentiment similarity regularization are a bit blurry. Is it the sentiment score that is used or the projection? What projection exactly is used? I assume this is something from the 3-layered MLP mentioned later.\n8. What are the baseline models w.r.t to the proposed regularized models? Given the curriculum training, it sounds like the regularized models are in fact the baseline models that are continuing to be trained with the regularization terms. This means that they were trained for longer than the baseline models, which may make the results less comparable (although perplexity values are similar). \n9. Have you evaluated the importance of curriculum training? What happens if models are trained from scratch with the regularization? For example, one could perform embedding similarity or even get the sentiment score/projection from an off-the-shelf sentiment classifier for the sentiment similarity regularization. \n10. I am not convinced that \"[t]he simple counting-based method .. is less prone to giving biased judgements\". Is there any evidence or further argument for that? \n11. I do not understand this sentence from page 9: \"When fairness scores are similar, sentiment-similarity regularization achieves better fairness scores.\" \n12. Section 5, last paragraph: looks like the words \"in Wikitext-103\" are missing, to explain that the bias in Wikitext-103 is smaller than WMT-19. \n13. Comparing tables 2 and 3 (end of section 5) is tricky, because both datasets and models change between them. Thus it isn't possible to disentangle the two.  \n14. Table 4 with the examples is mentioned but not discussed at all. Do we learn something from it? \n15. Kirichenko and Mohammad's paper appears twice in the references. \n16. Appendix B, last sentence isn't clear: \"We sample with template of 1.0.\" \n\n\n\n "}