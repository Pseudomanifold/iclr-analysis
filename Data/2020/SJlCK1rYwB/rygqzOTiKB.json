{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents techniques  to reduce dataset-specific biases learnt by a model during training.  The overall idea behind all the proposed techniques is to give low weights to \"biased samples\" in the loss function compared to unbiased samples so that the mistakes on unbiased samples contribute more to the loss and thus forces the base model to learn robust or true boundaries. To identify biased examples, authors propose to simultaneously train a biased- model using \"bias-specific\" samples or features. Heuristics to generate such samples or features have been proposed for specific types of problems including fact verification, textual entailment, and syntactic bias. In their experiments, authors used two large-scale NLI datasets and FEVER dataset, and evaluated the proposed techniques' performance on their challenging unbiased evaluation datasets proposed very recently.\n\nMy major concerns about this work are limited applicability of the proposed techniques to certain NLP problems and lack of technical novelty, and therefore I give a weak-reject rating.\n\nStrengths: \n1) The paper is well-written and easy to follow. \n2) Proposed methodologies outperform latest baselines from NLP domain on the unbiased versions of above datasets.\n\nComments: \n1) To my understanding ,the performance of the techniques proposed in this paper rely heavily on the quality of biased features or samples. Such samples are identified based on the performance of bias-only model. The notion of bias-only model  (e.g. claim-only trained model) follows naturally  for the problems and datasets covered in this paper. However, that might be quite challenging to do in a general ML problem setting, which restricts the applicability of this work to specific problems.\n\n2) Technical contribution and novelty of the paper is limited. The proposed techniques are simple variations of existing techniques in the literature. Further, the authors provided multiple techniques to remove dataset biases, but didn't provide any insights on why a particular technique perform better than the others. For example, one of the proposed techniques based on Ruby Variations (RV) consistently underperform the other proposed approach \"Product of Experts\" (POE). Unless authors can provide a technical insight on why POE is better than RV, RV does not seem to add any value to the paper.\n\n3)  How do the proposed techniques compare with the general techniques of improving generalization error? E.g. bagging, regularization, dropout, adding noise in the data, etc. Similarly, to improve performance on harder examples, boosting algorithms are known to be powerful. There is no discussion on why such techniques might not be suitable for the problems discussed in this paper. I am curious as to why such methods are not applicable in the discussed problem settings. \n"}