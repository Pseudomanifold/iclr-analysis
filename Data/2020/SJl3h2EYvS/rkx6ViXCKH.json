{"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper tackles zero-shot and generalised zero-shot learning by using the per-image semantic information. An instance-based loss is introduced to align images and their corresponding text in the same embedding space. To solve the extreme imbalanced issue of generalized zero-shot learning, the authors propose to scale the prediction scores of seen classes by a constant factor. They demonstrate technic contributions on CUB and Flowers datasets and the results achieve the state-of-the-art.\nThis paper should be rejected because\u00a0(1) motivation is not well justified. This paper fails to convince me that it is practical to use per-image semantic information. Zero-shot learning aims to reduce the cost of annotation, but the\u00a0per-image semantic information used in this paper conversely increases the annotation efforts. More specifically, this paper directly applies the per-image text descriptions proposed by Reed et al. '16, in which they annotated each image of CUB and Flowers by 10 different sentences. Due to the lack of such expensive annotations, this paper can be only evaluated on CUB and Flowers datasets,\u00a0 results on other popular zero-shot learning datasets e.g., AWA, SUN, ImageNet, are essentially missing.\n(2) novelty is limited. Using per-image semantic information is not new in zero-shot learning at all.\u00a0Reed et al. '16 studied this problem and showed that per-image text-description can surpass per-image attributes in zero-shot learning. The loss function of this paper is also similar to\u00a0 Reed et al. '16, which used the max-margin loss to align image and text pairs, v.s. the cross-entropy loss of this paper. Claiming that they are the first to use per-image semantic information in GZSL is not convincing because GZSL was not introduced at the time of\u00a0Reed et al. '16. Metric scaling is not novel either. It is, in fact, equivalent to the calibration technic proposed by Chao et al. '16. The theory of metric scaling is redundant in my point of view because it is obvious that rescaling decreases the seen class scores.\u00a0\n(3) experiments are insufficient to support the contribution. Their experiments are not comparing apple to apple. Specifically, the competing methods are using per-class semantic information while their approach uses per-image semantic information which is unfair because\u00a0per-image\u00a0semantic information includes much more supervision. The authors should have compared the results of those methods trained with\u00a0per-image semantic information. I would expect all the approach will benefit from\u00a0per-image side information.\u00a0"}