{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes to to use four different losses to train a joint text-image embedding space for zero-shot learning. The four losses consist of a classification loss given text descriptions, a classification loss given images, two contrastive losses given pairs of text and images. The paper also discusses how to balance seen and unseen classes, and it seems that, empirically, embeddings for seen classes are closer together while the embeddings for the unseen classes are further apart. A scaling factor that makes sure these distances are comparable for seen and unseen classes is introduced, and the paper gives an explanation for such a scaling. The final performance on the CUB and FLOWERS data set is impressive.\n\nI am giving a score of 3. The engineering effort in the paper is appreciated, but the novelty is lacking. The losses introduced in the paper, broadly classified as contrastive loss, has been around since (Frome et al., 2013). See also (Karpathy et al., 2014; Wang et al., 2016) and in particular (Xian et al., 2016). The subtle differences between all these losses are whether there is a margin, whether the loss is smooth (i.e., a softmax as opposed a max), and how the negative samples are selected. I am surprised that the loss function being used by many are considered a contribution in the paper. The property of metric scaling is certainly interesting, but it does not answer why metric scaling is needed in the first place. Overall, the novelty is limited.\n\nBelow are some minor points and questions for the paper.\n\n... anchor embeddings learned in one modality as prototypes ...\n--> the word \"prototype\" is used extensively in the rest of the paper. i know this is a common term in the zero-shot learning community, but it might be good to give a formal definition early or at least give an informal definition.\n\nAlgorithm 1\n--> what is the choice of d in the experiments?\n\n... the probability of image v_i and text t_j to belong to the same object distance\n--> isn't this the constrastive loss? what does it mean for the two to belong to the same object distance?\n\nequations (2)\n--> it seems that v_i and t_i are paired, but it is unclear from the text.\n\nequation (1) and (2)\n--> have you considered other (distributions of) negative samples?\n\n... P{\\hat{y} \\in Y^{tr} | y_v \\in Y^{ts}} is significantly greater than P{\\hat{y} \\in Y^{tr} | y_v \\in Y^{tr}}\n--> what's the definition of P{\\hat{y} | y_v}? the notation is confusing and hinders the understanding of the rest of the discussion. it's probably easier to just talk about distances and use the nearest neighbor argument.\n\nFigure 2\n--> what's different between the 10 different runs? random seeds?\n\nFigure 3\n--> why is the performance significantly lower when lambda=0? can this be related to how negative samples are selected?\n\nReferences:\n\nDeViSE: a deep visual-semantic embedding model\nA. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, T. Mikolov\nNeurips, 2013\n\nDeep fragment embeddings for bidirectional image sentence mapping\nA. Karpathy, A. Joulin, F. Li\nNeurips, 2014\n\nLearning deep structure-preserving image-text embeddings\nL. Wang, Y. Li, S. Lazebnik\nCVPR, 2016\n\nLatent embeddings for zero-shot classification\nY. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, B. Schiele\nCVPR, 2016\n"}