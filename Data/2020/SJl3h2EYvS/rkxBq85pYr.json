{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents two main contributions: first, a simple retrieval-based objective for learning joint text and image representations, and second, a metric-scaling term that improves performance on unseen classes in the generalized zero-shot (GZSL) setting. They evaluate on two datasets, CUB and FLOWERS, consisting of images paired with text descriptions, and show that their relatively simple technique outperforms complex GAN and VAE-based models.\n\nWeak accept, see considerations below. The results seem solid, and the architecture is quite simple compared to other models on this task. However, there are some gaps in the analysis, and the paper would benefit from a more careful comparison to the literature.\n\nThe results are strong, and it\u2019s encouraging to see a simple approach like this outperform more complex models. However, it\u2019s not clear how novel the retrieval model is; as implemented, it\u2019s similar to a sampled softmax or noise contrastive estimation - the paper would benefit from a more careful comparison of their approach to established alternatives, and a discussion of why this approach should learn better representations than a VAE or GAN. It\u2019s also not clear how novel the metric scaling technique is. It undeniably works well, but the use seems remarkably similar to (Das & Lee 2019, Section II.D).\n\nThe observation (Section 4.5) that the retrieval-only model performs well without any class labels is interesting. How does this compare to other unsupervised approaches? It also appears that the version with the metric scaling here still performs better - is this because the retrieval model is trained only on examples from the \u201cseen\u201d classes? Does this gap go away if (unlabeled) examples from the unseen classes are available at training time?\n\nA few other notes:\n* Section 4.5: \u201cOur interpretation is that the addition of the image/text classifier loss helps to reduce the intraclass variability in embeddings and provides for tighter clustering.\u201d - could you test this directly, or provide some visualization, such as t-SNE plots?\n* The image representations are initialized from an ImageNet model, but text is trained from scratch. Why not use a pretrained text encoder like BERT?\n* Table 3: it would help to have text labels for these rows, or a reminder in the caption of what \\lambda = 0 and \\kappa = 0 mean (otherwise, one needs to flip/scroll back to page 4)\n* Figure 2: what are the actual values of u and s that contribute to this plot?\n"}