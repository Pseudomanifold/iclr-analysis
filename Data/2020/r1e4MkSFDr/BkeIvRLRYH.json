{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a continuous CNN model to accommodate the nonuniform time series data. The model learns the interpolation and the convolution kernel functions in an end-to-end manner, so that it can capture the signal patterns and be flexible. A layer has three networks, which learn a kernel function to represent the combination of interpolation and convolution, a bias function to represent the error correction with convolution, and then produce the output based on them. The authors introduce two assumptions and a two-hot encoding scheme for the input to control the model complexity. The paper also introduces an application of the proposed CCNN by combing with temporal point process. Experiments on simulated data compares the proposed method with some degenerative baselines show the advantage of learning the interpolation and the two-hot encoding configuration. The authors compare the performance on time interval prediction task based on real world dataset to show the model produces a better history embedding for the task.\nOverall, the paper has some incremental improvements on the existing methods that dealing with the nonuniform time series data. Instead of using preset interpolation kernels, the proposed model can learn it with the convolution in a data-driven manner.\nThe paper includes clear explanation on module structure and detailed experiment settings.\nThe experiments of signal value prediction support the claims of the advantages of the proposed model.\nThe notation in the caption of figure 1 is a little confusing: is x(t_i\u2019) the same as hat x(t) in the algorithm?\nIt is good that the related works section mentioned the adapted RNNs that are used as baselines in the real-world dataset experiment, and the differences between the proposed model and the related SNNs are introduced.\nHowever, this section and the introduction can be better organized to distinguish the novelty and the contribution of the work.\nIn page 7, the purpose of the reference in the sentence \u201cThe time information is either two-hot encoded (Adams et al., 2010) (CCNN-th), or not encoded (CCNN).\u201d is not very clear.\nIt will be better if there are a little more analysis of the experiment on predicting time intervals to next event.\nThe upper plots in the figure may be not convincing enough to support the claims.\nThe advantage of two-hot encoding seems subtle in the figure 5. Is there any reason for the significantly higher deviation of CCNN-th in StackOverflow?\nSometimes the usage of \u201cCCNN\u201d is not clear, for example, the experiment on speech interpolation compares the \u201cCCNN-th\u201d method with baselines, but uses \u201cCCNN\u201d in the analysis. Also, it could be better to show the \u201cCCNN-th\u201d result in the upper plots of figure 5 instead of \u201cCCNN\u201d.\nMinor comment:\nThere are some typos in the paper, for example, missing the right parenthesis in page3 \u201c(refer to Appendix A.1\u201d, in page4 section 4.1 \u201cAccording to Eq.(4), the input is \u2026\u201d.\n\u201cThe left plot shows\u201d in the last line of the caption of figure 3 should be \u201cright\u201d."}