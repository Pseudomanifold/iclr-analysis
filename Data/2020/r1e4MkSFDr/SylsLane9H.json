{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "1.\tThe motivation of continuous convolution is not very clear, can the authors please motivate? To my understanding this is just to handle inputs with unequal time steps, but that can be handled multiple ways, why not just naively resample?\n2.\tThe proposed network was defined as continuous convolution followed by the standard convolution. Why not just stack multiple continuous convolutions?\n3.\tContinuous convolution should be a general case for standard convolution, can authors explicitly show it?\n4.\tAnother way to handle unequal timesteps is by using dilated convolution, can authors please comment how they differ, pros and cons etc.?\n5.\tTwo hot encoding seems another way to discretize, no?\n6.\tThe experiments section is rather weak, CCNN seems to have a lot of spikes in prediction, e.g., in Fig. 5.\n7.\tIt\u2019s very strange why two hot encoding does not perform that well, while reading the method section, it seems very obvious to take two ends of an interval, in that way two hot encoding seems logical.\n\nOverall it seems like an easy extension with a lot of parts not well-justified. Also I don't clearly have a well-grounded motivation for a continuous convolution.\n"}