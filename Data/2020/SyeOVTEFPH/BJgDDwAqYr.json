{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new adversarial training method, which uses an instance-specific perturbation magnitude for each training instance. This methods can alleviate the accuracy-robustness tradeoff observed in previous works. The adaptive instance-specific perturbation magnitude is updated in each training iteration. Extensive experiments on CIFAR and ImageNet prove that the trained models has better natural performance and similar adversarial robustness with previous works.\n\nThe paper is generally well-written. The motivation behind the proposed method is clear. The experiments can consistently prove the effectiveness of the proposed method on training robust models. However, I have some concerns about the proposed algorithm.\n\n1. In algorithm 1 step 9-11, the authors split the training batch into the correctly classified samples and incorrectly classified samples, and train the model based on robust loss or natural loss for each part. However, there is no explanation on why it is necessary to do so, since previous works either directly optimize the robust loss for every sample or optimize the natural loss and robust loss together. The authors could discuss more on this choice.\n2. In algorithm 2, the \\epsilon selection procedure needs to generate adversarial examples for \\epsilon_1 and \\epsilon_2, which requires at least 2 times of computation complexity compared with the previous works. The efficiency of the proposed method could be poor. Are there any accelerating algorithms?\n3. How do you set the hyper-parameters \\beta and \\gamma? Is the result greatly sensitive to the specific values of these parameters?\n\nOverall, the authors could elaborate more on the technical details to help to better understand the proposed algorithm. "}