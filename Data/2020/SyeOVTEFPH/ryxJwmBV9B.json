{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper addresses a limitation of adversarial training: in the most common formulation, the radius of L_p balls used to constrain the adversary during training is shared by all training points. Instead, this paper proposed to set multiple radii, where each point is given a radius appropriate given how far it lies from the decision boundary. The intuition behind this paper is correct and confirms recent findings: e.g., on excessive invariance in adversarially trained models. \n\nTo set the radius for each example, a heuristic is introduced in Section 3. Because the heuristic is introduced without justification, it is difficult to assess the main contribution of this paper. Simple additions to the paper could help revise the submission. For instance, were other heuristics considered? If yes, why was this one chosen in particular? \n\nIt may be problematic to set the value of the radius based on the approach that will be used to attack the model: perhaps other prototypicality metrics could be used for setting the value of this radius independently of any algorithms for finding adversarial examples? It would be interesting to use a different attack to set the radius during training and to attack the model at test time.\n\nIn other words, given that the value of the per-example radius is set based on the model being trained, it is difficult to appreciate how much Algorithm 2 over or under estimates radii. The inadequacy of certain models in setting radii appears to be the motivation for the warm up step. Expanding Section 3 a bit to help the reader build their intuition for this aspect of the approach would help.\n\nClaims of robustness are made empirically. Unfortunately, results presented make it difficult to assess the validity of the approach. What is the value of epsilon chosen to attack the model in Figure 3? Does the adversary adapt and compute a per-example epsilon instead of a fixed epsilon? This should help attack examples whose radius during training was larger.\n\nHave you considered how the approach interacts with algorithms that achieve certifiable robustness?\n\n\n1: repetition in \u201cand enforcing large margins around these samples produce poor decision boundaries that generalize poorly\u201d\n\n1: The formulation of adversarial training was introduced in earlier papers from the 2000s, prior to Madry et al. \n\n2: The result described in Figure 1 was presented in Jacobsen et al. [https://arxiv.org/abs/1903.10484]\n\n2: The relationship to Ding et al. is not entirely clear, and would benefit from additional clarification.\n\n4: Typo WideRenset \n\n"}