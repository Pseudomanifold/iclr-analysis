{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Adversarial training typically trades off generalization accuracy for robustness to adversarial attacks. This paper introduces a simple but seemingly effective heuristic to improve this trade-off. Instead of performing adversarial training for a fixed radius $\\epsilon$, the authors propose to use a datapoint specific radius which is adapted during training: the radius is increased or decreased based on whether adversarial attacks (generated by k-steps of projected gradient descent) succeed in correctly classifying the label at the updated radius value. At a high-level this captures whether an example is close to the decision boundary or not. Only \u201csafe\u201d adversarial examples, which do not cause an error in the prediction and are thus \u201cfar from the decision boundary\u201d, are then used for adversarial training. The method, while being a heuristic in nature is shown to be effective for various architectures in the ResNet family, on CIFAR-10, CIFAR-100 and ImageNet.\n\nWithout being an expert in the domain of adversarial attacks, the method and evaluation appears sound. While the proposed method scores low in terms of novelty and would benefit from added theoretical justification (e.g. an adaptation mechanism which exploits local geometry of the loss surface), there is something to be said about simple and effective method.\n\nThat being said, I do have two main reservations which I would like to see addressed or discussed in the rebuttal. With respect to the targeted ImageNet evaluation, which generates adversarial attacks using PGD-1000 for various values of $\\epsilon$, the proposed method only seems effective for small values of $\\epsilon$: larger values of $\\epsilon$ are much less robust than standard adversarial training. Without being familiar with standard evaluation protocols, why should we care more about robustness at $\\epsilon=4$ vs $\\epsilon=16$ as used in [Xie, 2019] ? It seems that for a method to be effective, we should not be comparing single point estimates but the whole accuracy-robustness (i.e. showing better natural accuracy for any value of \u201ctarget\u201d robustness). Second, the ablative analysis is unfortunately missing the two most importance ablations: namely the impact of the hyper-parameters controlling the update step and moving average coefficients of the instance adaptive radius parameters. Finally, it is also regrettable that the \u201csimple heuristic\u201d is in fact complicated by a warmup schedule which starts adversarial training with fixed radius, before starting the tuning process after a fixed number of updates / epochs.\n\n\nDetailed Comments / Questions:\n* Have you considered multiplicative updates to $\\epsilon$ as in the Levenberg\u2013Marquardt algorithm?\n* Background section should acknowledge the existence of black-box or non-gradient based attacks, as discussed in [R1]\n* Equation 1: should read $f(x_i + \\delta_i)$ and not $f(x_i) + \\delta_i$\n* Algorithm 1, line 9, 10: should this not read f(x_i^{adv}) ?\n* Algorithm 1, line 11: please change to reflect that a partial minimization is performed wrt theta (e.g. a single SGD step).\n* Tables: for readability, please highlight in bold \u201cbest\u201d methods for each column.\n* Figure 4: change y-axis label to \u201cAdversarial Accuracy\u201d.\n* Figure 5(b): plotting epsilon trajectories for samples having highest and lowest epsilon values (at convergence) could be more informative.\n* Figure 6: \u201cvisualizing training samples with their corresponding perturbation\u201d. Please change caption as you are not plotting the perturbation, but rather showing the instance adaptive radius obtained at convergence, for each training sample.\n\n[R1] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. Uesato et al."}