{"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This is a good multiview representation learning paper with new insights. The authors propose to learn variables z_1 and z_2, which are consistent, contain view-invariant information but discard as much view-specific information as possible.\nThe paper relies on mutual information estimation and is reconstruction-free. It is mentioned in some previous works (e.g. Aaron van den Oord et al. 2018), that reconstruction loss can introduce bias that has a negative effect on the learned representation.\nComparing to existing multiview representation learning approaches that try to maximize the mutual information between learned representation and the view(s), this paper clearly defines superfluous information that we should try to throw away and figure out how to obtain sufficiency learned representation for output. The authors also draw clear connections between a few existing (multiview) representation learning methods to their proposed approaches.\nThe experimental results on the right side of Figure 3, deliver a very interesting conclusion. In low-resource case, robust feature (obtained by using the larger beta, discarding more superfluous information) is crucial for achieving good performance. While when the amount of labeled data samples is enough, vice-versa.\n\nHere are my major concerns:\n1.\tIn the paper, the authors said the original formulation of IB is only applicable to supervised learning. That is true, but the variational information bottleneck paper [Alexander A. Alem et al. 2017] already showed the connection of unsupervised VIB to VAE in the appendix.\n2.\tI would not consider the data augmentation used to extend single-view data to \u201cpseudo-multiview\u201d as a contribution. This has been done before (e.g. in the multiview MNIST experiment part of the paper \"On Deep Multi-View Representation Learning\").\n3.\tWhich MV-InfoMax do you really compare to? You listed a few of them: (Ji et al., 2019; Henaff et al., \u00b4 2019; Tian et al., 2019; Bachman et al., 2019) in the related work section.\n4.\tI think the authors should also make a more careful claim on their results in MIR-Flickr. \nI\u2019d rather not saying MIB generally outperforms MV-InfoMax on MIR-Flickr, as MIB does not (clearly) outperform MV-InfoMax when enough labeled data is available for training downstream recognizers. But MIB does clearly outperform MV-InfoMax when scaling down the percentage of labeled samples used.\n5.\tRegarding baselines/experiments\na.\tIn Figure 4, it seems that VAE (with beta=4) outperforms MV-InfoMax. Why the ``\"pseudo-second view\" does not help Mv-Infomax in this scenario? Why VAE is clearly better than Infomax?\nb.\tIn Figure 3, you might also tune beta for VCCA and its variants, like what you did for VAE/VIB in a single view. \n6.\tDo you think your approach can be extended to more than two views easily? \nFor me, it seems the extension is not trivial, as it requires o(n^2) terms in your loss for n views.\nBut this is minor.\n\n"}