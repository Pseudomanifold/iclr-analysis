{"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. The proposed method consists of two ideas: (1) spectral regularization (i.e., Dirichlet energy) with a re-parameterizing basis and (2) multiresolution of spectral loss (i.e., zoomout loss). In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Empirical results show the best performance compared to other recent methods under small-scale datasets. Moreover, the proposed method outperforms when the geometric model is accurate (verified on the synthetic setting) and this can reflect that the proposed method is a good choice when the graph structures are given.\n\nThis work can be a significant contribution as it is a simple linear model but practically performs better than other deep nonlinear networks (e.g., RGCNN). Additionally, the proposed loss functions utilize only the spectral information of graph structure with novel approaches. However, there are some drawbacks to this work. First, it requires a good quality of geometric model which is hard to obtain in practical datasets. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper). This can be a problem for real and large-scale datasets.\n\nOverall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. But, I vote for weak acceptance due to its drawbacks as mentioned above.\n\nMain concerns:\n\n1. It is not clear why minimizing Dirichlet energy can improve the performance of matrix completion. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. And how much does the accuracy increase (or decrease) when using the Dirichlet regularization? \n\n2. Authors argue that the re-parameterizing of the basis (emerging P and Q) can find a better geometric model (section 2). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. However, the empirical results are reported poor improvements for inaccurate geometric models. Does this make sense?\n\nFor experiments:\n\n1. What is the number of trainable parameters for each method? Since the proposed method is overparameterized, it is not clear that the empirical improvements come from the overparameterizing or the proposed loss function. It would be great to report the number of parameters of all other methods by setting similar numbers.\n\n2. It is not clear how to generate the synthetic dataset, i.e., projecting a random matrix on te the first few eigenvectors of L_r and L_c. It would be better to give more details.\n\n3. What are the training times of the proposed method and other competitors?\n\n4. Why results of FM are not reported under other datasets?\n\nMinor comments:\n\n1. In page 4, please edit \u201cWe explore The\u201d -> \u201cWe explore the\u201d.\n\n2. In equation (15), writing \u201c\\odot S\u201d twice seems to be unnecessary.\n"}