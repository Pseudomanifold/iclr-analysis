{"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper discusses the detection of out-of-distribution (OOD) samples for variational autoencoders (VAE).\nThe idea is to train the encoder such that its output variational distribution q(z|\\bar{x}) is pushed away from the prior of latent z. \nI think the paper needs more clarification and investigation for being published in the conference. \nMy major concern is that more empirical investigation is necessary since the formulation provides a minor novelty. \nSpecific points are given below. \n\n1) Weak novelty in terms of model design. \nThe objective function consists of the standard (negative) ELBO term and additional KL term to modify the variational posterior of negative samples. \nThis modification can be regarded as a form of outlier exposure (Hendrycks et al. 2018) specialized for VAE. \nThe choice of \\bar{p} is not much investigated. \nAny discussion if we use a more sophisticated model such as VampPrior* for stronger modeling capacity. \n* J. Tomczak and M. Welling, VAE with a VampPrior, AISTATS 2018.\n\n2) The use generated samples as negative samples is interesting but mysterious. \nThe authors conjecture that this works because the generated samples come from near the data manifold, but in-distribution samples and negative samples can be indistinguishable when the generative model is very well trained. \nWhat happens if, for example, the negative samples are generated by data augmentation techniques (such as cropping, rotation, mirroring, though mirroring and much rotation may be unsuitable for text images)? \nThis can also produce near-manifold points. \nA deeper analysis why generated samples can improve the OOD detection performance is necessary. \nFurthermore, why does not this approach impact much for color images in Table 4?\n\n3) More details of experimental procedures. \n3-1) How was data points are generated from VAE as negative samples? \nPossible ways are:\n* sample z ~ p(z), then draw from the decoder x ~ p(x|z).\n* use negative prior z ~ \\bar{p}(z), then draw from the decoder x ~ p(x|z).\n* this seems weird: use variational posterior z ~ q(z|x), then x ~ p(x|z).\n\n3-2) Latent dimension of 10 for grayscale images seems small. \nDoes the size affect the OOD detection performance when the size is 50 or 100 to make the model richer. \n\n3-3) How was the variance obtained when the decoder uses the Gaussian likelihood?\n* fixed value?\n* learned for each pixel?\n* output from the decoder?\n\n4) If we have access to diverse negative datasets, can the ODD detection perform better? \nMixing multiple datasets or using both available dataset and generated samples can improve the performance while the test OOD samples are kept unseen. \nFor example, train VAE on MNIST while using KMNIST and EMNIST as the negative sets to detect Fashion-MNIST as ODD. "}