{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a method for training GANs on target domains with a small number of samples, by adapting a single or multiple high-quality GANs pretrained on larger source domains (e.g. faces or imagenet). The method is based on training a small network M to transform a mixture-of-Gaussian samples to a Gaussian, which is fed to the pretrained GAN generator G (which can be kept fixed or finetuned). The paper distinguishes two transfer cases: on-manifold, in which the target domain is a subset of the source domain (e.g., celebA and FFHQ women), and off-manifold, in which the target domain is different from the source (e.g., celebA and FFHQ children). Experiments show that the proposed method can perform better than training from scratch or simply finetuning on the target domain.\n\nComments:\n-  The usage of the term \"knowledge transfer\" in the paper is a little confusing. I don't think the knowledge distillation of Hinton et al 2014 and Romero et al 2015 is relevant here, yet the paper references them in the sense of adapting/transferring a pretrained network to small data regimes.\n-  In addition, the notion of transferring/adapting a network trained on a large set to a subset of it doesn't seem very useful to me. It is certainly different from transferring a generic representation in supervised learning (e.g. from imagenet) to small data regimes. That being said, the paper discusses off-manifold transfer, which could be more interesting from a practical point of view.\n-  The method is simple, which is not particularly a bad thing, but it's not really clear what is the advantage of it over simply finetuning the whole network. I am not convinced that finetuning will result with mode-collapse (as mentioned in Sec. 4) while this method won't. I believe the authors mean overfitting here, which is probably why we see much better results in for their method (MineGAN) compared to finetuning (TransferGAN) in Fig. 3.\n-  Regarding Fig. 3, training from scratch seems of very low resolution. I believe more effort should be done on creating stronger baselines.\n\nIn conclusion, I don't think there's enough evidence (neither theoretical nor empirical) that the proposed approach is significantly better than simple finetuning which undermines the novelty and contribution of this work.\n"}