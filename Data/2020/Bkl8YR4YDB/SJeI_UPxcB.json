{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\nThis paper investigates the effectiveness of a massively large parallel corpus in NMT training, which consists of more than 40 billion En-Zh parallel sentences.\nTo the best of my knowledge, the 40 billion parallel corpus for the NMT training is the largest reported in the paper published so far.\n \nFor preventing long training time, this paper proposes a practical data split and utilization method, which the authors call \u201cdynamic-data-split.\u201d\nThe key idea of their method is to dynamically assign training instances to different model components and update different components according to the assigned instances.\n \nThis paper reports the BLEU score of WMT17 Chinese-English dataset for 32.3, which significantly outperformed the best score, and improved the performance of existing state-of-the-art results.\nThey also provide several deeper analyses of the proposed method by changing the model training strategy (pretrain only, pretrain+finetune), data split strategy, data size, and tokenization (word, BPE, character).\n \n\n\n\nThe main concern of this paper is the reproducibility of the experiments.\n \nTheir main focus is to investigate the effectiveness of 40B massive parallel data.\nHowever, the origin and how the authors correct the data is fully unknown; in the paper, they only say, \u201cThe data comes from diverse sources such as web pages (\u223c2 billion), digitized books (\u223c1 billion) and private purchase from translation agencies (\u223c46 billion).\u201d\nWhat is the \u201cprivate purchase from translation agencies.\u201d\nNo one knows how they were collected except the authors.\nIt is impossible to reproduce the results of the experiments conducted in this paper in future validation.\n\n"}