{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes adding additional flow layers on the decoder of VAEs. The authors make two claims\n1. The proposed model achieves better image quality than a standalone Glow.\n2. The proposed model is faster to train than Glows.\nThe intuition is a VAE can learn a distribution close enough to be target distribution, and the Glow only needs to do much less work than standalone Glow, hence faster. Some positive results are reported in the experiments, including better image quality, faster training time, and the Glow indeed sharpens the output of VAEs. \n\nThe paper indeed has some good results, particularly they can achieve it only with single-scale Glows with additive coupling layers. However, I think the claims are not sufficiently supported. Taking point 1 as an example, it is not clear to me why VAE+Glow is better than a standalone Glow. Imagine two models\n\nM1: VAE+Glow (proposed in the paper)\nM2: Glow0+Glow (standalone Glow)\n\nsharing the last \"Glow\" part. M1 is better than M2 implies \"VAE\" is more powerful than \"Glow0\", which I doubt. Similarly, for point 2, it is not clear to me why \"VAE\" is faster than \"Glow0\". I think comparing the proposed model with IAF make more sense, because the proposed model just adds flows to the decoder and the prior. But the relationship with Glows needs to be considered more thoroughly.\n\nAnother confusing detail for me is the two-stage training in Sec. 3.4. The explanation \"likely because the Glow layer is unable to train efficiently with a changing base distribution\" doesn't make sense. Because IAF does successfully train their q-net without 2-stage training. There might be other reasons?\n\nThe baselines are not strong enough. Most importantly, Flow++ [1] reports a likelihood 3.08 on Cifar10 with standalone flows, which should also be a part of the baseline. I also wonders whether the proposed model benefits from deeper model, like standalone flows do. Will standalone flows surpasses the proposed model as the number of layers goes to infinity?\n\n[1] Ho, Jonathan, et al. \"Flow++: Improving flow-based generative models with variational dequantization and architecture design.\" arXiv preprint arXiv:1902.00275 (2019).\n\nFinally, the paper is somewhat incremental. Particularly comparing with VAE-IAF, where this paper just adds flow layers to not only q but also p."}