{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "(emergency review)\n\nThe authors propose function-specific embeddings that use a multidirectional representation learning method for word-level semantics. In specific, vocabularies can be divided into groups (S, V, O, +iO) with overlapping allowed, then the word vector representations are trained synchronously by SGNS-like manner. Empirical experiments show that the model outperforms task-specific comparison models with a reduced number of parameters by sharing.\n\nStrength:\n- A simple but effective model for learning (function-specific) word-level representations.\n- The paper is easy to follow with extensive literature reviews.\n- Experiments show performance improvements with less number of trainable parameters.\n\nWeakness:\n- Word-level representations are usually used as an input layer. It might be better to show more about the improvement of performance in terms of downstream tasks.\n- Word-level representations could be dynamically changed with contexts, so it seems that those function-specific word vectors could be also constructed through contextualization. I would like to see the results of \"contextualized\" word vector representations models such as ELMo or language model based embedding techniques.\n\n"}