{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper carries out several kinds of analysis on the GAT networks of Velickovic (2018), which augment GNN updates with multihead self attention. Three standard attention types are compared, on several different datasets, and differences between uniform attention and learned attention are reported. An experiment is carried out where low-attention edges are pruned.\n\nWhile understanding the value of attention is important, this paper leaves many questions open. First, since the graphs studied in this paper are, if not generally sparse to begin with at least they only include connections that are meaningful, the sparsification experiment is a bit hard to understand. One particular extension would improve things: adding random edges (can the model learn to prune them out?), but learning sparse attention (see e.g., Maruf et al., 2019) rather than thresholding seems to be a reasonable point of comparison.\n\nOverall this paper would be more valuable if a clear and concise recommendation could be given regarding how to use or understand attention; but the lack of a consistent pattern of results makes any obvious narrative hard to support. I would encourage the authors to continue this line of work so that it can be used to provided guidance to those who would like to make more effective use of GNNs."}