{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper analyzes attention in graph neural networks. It makes two major claims:\n(1) Datasets have a strong influence on the effects of attention. The attention in citation networks are more uniform, but they behave differently in protein or molecule graphs. \n(2) With attention-based graph sparsification, it is possible to remove a large portion of edges while maintaining the performance.\n\nI have some concerns about this paper: (1) the analysis lacks theoretical insights and does not seem to be very useful in practice; (2) the proposed method for graph sparsification lacks novelty and the experiments are not thorough to validate its usefulness; (3) the writing of this paper is messy, missing many details.\n\nIn the analysis part (section 5), the choices of probing metrics seem arbitrary and lack theoretical insights. The authors used the L1 norm, but it seems not appropriate for the tasks here, e.g. KL divergence is preferred to measure the distributional discrepancy, entropy for concentration etc. Many important details are missing or not clear, for example, in Table 2, which head/layer is used for computing the attention, and what does \u201cGCN vs learned\u201d mean? The maximum pairwise difference is not clearly defined. The meta graph classification (section 5.2) only considers a synthetic dataset. Overall, I feel the analysis didn\u2019t present too many interesting observations, and I cannot see too much potential value in applications (even for the graph sparsification task in this paper, its correlation with the analysis is quite weak).\n\nIn section 6, it explores whether it is possible to remove part of the edges from the graph while maintaining the classification performance. It is an interesting task, but the method proposed in this paper is not realistic and lacks novelty. In 6.1 and 6.2, it needs to train a GAT first to get the attention scores, then remove edges according to attention scores and train another GAT. In this way, it doesn\u2019t reduce the computational requirement, as it still trains a full model to get the attention. Only in 6.3 it presents a realistic setting, where the attention scores are derived from a small GAT, and train another GNN on the sparsified graph. But the paper didn\u2019t explain why it is possible to get reliable attention scores with a small GAT, and the experiment is only on one dataset. Does it apply to other datasets (citation network, molecules) and settings (transductive, inductive)? So far the experiments are not enough to be considered as a valid contribution."}