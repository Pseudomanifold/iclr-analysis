{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Deep Randomized Least Squares Value Iteration\n=========================================================\n\nThis paper proposes a method for exploration via randomized value functions in Deep RL.\nThe algorithm performs a standard DQN update, but then acts according to an exploration policy sampled from a posterior approximation based on a last layer linear rule.\nThe authors show that this algorithm can perform well on a toy domain designed to require efficient exploration, together with some results on Atari games.\n\n\nThere are several things to like about this paper:\n- The problem of efficient exploration in Deep RL is a pressing one, and there is no clearly effective method out there widely used.\n- The proposed algorithm is interesting, and appears to have some reasonable properties. One nice thing is that it requires only relatively minor changes to the DQN algorithm.\n- The general flow of the paper and structured progression is nice.\n- The algorithm generally appears to bring superior exploration and outperform epsilon-greedy baseline.\n\n\nHowever, there are some other places the work could be improved:\n- I think that the name \"Deep RLSVI\" is a little imprecise... actually RLSVI could already be a \"deep\" algorithm as defined by the JMLR paper: http://jmlr.org/papers/volume20/18-339/18-339.pdf (Algorithm 4). I see that you mean this as an extension to the linear case for RLSVI... but I do think it would be better to call it something more explicit like \"Last-layer RLSVI for DQN\".\n- Related to the above, the comparison to other similar methods for exploration via \"randomized value functions\" is not very comprehensive. I'm not sure what the pros/cons are of this method versus BootDQN or the very similar work from Azizzadenesheli?\n- It would be good to compare these methods more explicitly, particularly on the domains designed specifically for testing exploration. To this end, I might suggest bsuite https://github.com/deepmind/bsuite and particularly the \"deep sea\" domains?\n- Something feels a little off about the Atari results, particularly the curves for \"rainbow\"... these appear to be inconsistent with published results (look at Breakout).\n\nOverall I think there is interesting material here, and I'd like to see more.\nHowever, I do have some concerns about the treatment/comparison to related work and I think without this it's not ready for publication."}