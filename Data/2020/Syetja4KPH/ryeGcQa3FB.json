{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a deep learning-based adaptation for the RLVSI algorithm, where the agent uses the representation learned by the deep neural network-based RL agent (DQN). They use the last layer of DQN as a state representation for RLSVI.  In order to work with the changing representations of the deep agent, they propose a likelihood matching mechanism. The approach is applied to two tasks: a) A toy modified n-chain experiment and b) set of 5 Atari games. They show that their method outperforms the DQN with naive exploration. \n\n\nThis paper should be rejected because of the following reasons: \n\n1) Lacking comparisons to Azizzadenesheli et al. (2018)\nThe authors acknowledge that their work resembles a lot to Azizzadenesheli et al. (2018), who also provide a deep extension of RLVSI. However, they do not provide any baselines or comparisons with this approach.  To me, this work is one particular form of the work by Azizzadenesheli et al. (2018), where instead of using Bayesian linear regression in the last layer, a specific parameterization of the prior and posterior families is used, that allows an analytical solution for the update (Eq 1).  They claim that they have lesser hyper-parameters but at the same time they introduce additional ones: N_{BLR}, T^{Sample}, etc. and it is not clear to me that if the new set of hyper-parameters are easier to tune than compared to Azizzadenesheli et al. (2018). They claim that it is not possible to compare with Azizzadenesheli et al. (2018)  but their code is available publicly.\n\n2) Lacking comparisons in general \nThe results on Atari are compared with vanilla-DQN (with epsilon greedy). Instead of comparing the method on other works that also extend the RLVSI to deep nets (eg: [1], [2], [3], etc) they compare it with RAINBOW, a method that is not based RLVSI.  \n\n3) Unexplained design choices \nA lot of design choices of the final algorithm are not explained, which makes me skeptical about the work. The main ones in question being:\nThe unique nature of the replay buffer: It is not clear why the experience replay buffer has the specific form, where each action has fixed memory, and a round-robin scheme is used to update the buffer. \n) Non-standard experiments\nIt is not clear why the authors did not use the standard n-chain task but rather used the modified version.  Also, why did the authors only selected the set of only those 5 specific games is not addressed.\n\n\nSuggestions \nI will recommend the authors to address why their algorithm should be used instead of the others I have mentioned above. They can do it either by providing any theoretical or empirical arguments. They also should use a few of the standard experiments so that it gives the reader more insight into where their algorithms excel. \n\n\nThings to improve the paper that did not impact the score:\n\nFigure 3 is too small to read. \nThe section on Likelihood matching is not clear: in motivation and impact. \n\n\nReferences: \n\n[1] ] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped DQN. In Advances In Neural Information Processing Systems 29, pages 4026\u20134034, 2016.\n\n[2] Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, and Pascal Vincent. Randomized value functions via multiplicative normalizing flows. arXiv preprint arXiv:1806.02315, 2018.\n\n[3] Osband, Ian, John Aslanides, and Albin Cassirer. \"Randomized prior functions for deep reinforcement learning.\" Advances in Neural Information Processing Systems. 2018.\n"}