{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work the authors point out an issue related to graph neural networks. Specifically, if two nodes, that may be far apart in the graph, may be represented as (almost) the same vector. This is simply because when no features/labels are associated with nodes, and the local structure around those two nodes is very similar then the local aggregation of information will result in a similar representation.  Therefore the authors introduce an embedding first of the graph in the Euclidean space using DeepWalk and then use this embedding in combination with the design of a CNN. The authors propose a pooling method that outperforms several state-of-the-art pooling techniques on real data. Overall, the empirical results are supportive of the fact that the proposed method can help improve the performance of GNNs. \n\nOverall I found the results of this paper to be weak, but nonetheless the paper is well-written and contains some interesting ideas. Hence my rating. Some questions follow. \n\n- While the authors call this as an \"issue\" it is more like a feature of these methods.  For instance, in \"RolX: Structural Role Extraction & Mining in Large Graphs\" by Henderson et al. this \"issue\" could turn out to be an interesting feature of the GNNs in the sense that these nodes may have a similar (structural) role.  It would be nice to have a short discussion related to this line of research in social networks' analysis. \n-  Some components of the CNN (e.g., node sampling) could be done using  well-developed tools for sampling matrices from numerical linear algebra, or by introducing some randomness when sampling a node as in kmeans++. \n- Graph downsampling appears to be an expensive operation. Can you please comment on the running times? The issue of scalability is not discussed, and the reader cannot easily infer to what sizes this method can scale up to. \n- Using other graph tasks, that are classical but also more challenging (e.g., learning 2-connected components of a graph just to mention something that comes up) would be interesting to see in Section 5.2.\n-  It would have been interesting to see the effect of the embedding step on the accuracy on the real data. E.g., using node2vec or standard spectral embeddings. "}