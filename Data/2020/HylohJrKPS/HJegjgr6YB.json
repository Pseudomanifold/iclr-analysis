{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposed an algorithm for synthesizing 3D-zoom behavior when the camera is moving forward. The network structure, called deep 3D-zoom-net, incorporates disparity estimation in a GANs framework to synthesize novel views. Experimental results on KITTI and Cityscapes datasets seem to suggest the proposed approach makes sense.\n\nThe proposed task is a new computer vision task, and the formulation of the approach (network structures, training protocols) makes sense. Some visualizations in the paper suggest that the prediction is sensible, and can potentially be useful for other computer vision tasks. \n\nHowever, I have a few questions regarding the intuitions behind this paper:\n- Why the proposed task is useful? Since this is a new task, the paper didn't discuss one concrete use cases beyond the scientific interest. I understand that it may potentially be useful for special effects or AR applications, but it'll be helpful to discuss more on why certain applications may need the proposed system, especially that the camera is only moving along the z-axis.\n- I think some of the experimental results make sense, but as I zoom in the pictures, most of them have very strong halo artifacts, and it doesn't seem to meet a very high standard in terms of the qualities. Moreover, those images comes from outdoor scenes, when generalizing the algorithm to indoor environments where the disparities are bigger overall, and perspective distortion is more severe, how will the proposed system perform? Regarding the realism metric, I think performing a user study is more legitimate, rather than verbally say that the proposed result is of high quality in the paper. \n- I think a better experimental setting would be testing the whole system on a synthetically generated dataset, where we know exactly what the ground truth image will look like. KITTI or cityscape are nice, but it's no guarantee the camera moves exactly along the z-axis. Using synthetic datasets can also help benchmark the quantitative performance of the system. "}