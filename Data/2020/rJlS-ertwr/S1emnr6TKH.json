{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the RL problem of learning an agent's policy in a high dimensional and only partially observable environment that can be modelled as a partially observable Markov decision process (POMDP). The authors propose InfluenceNet, a model-free deep RL architecture. The main intuition is that in a POMDP the Markov property is not satisfied, but on the basis of influence-based abstraction the non-Markovian dependencies can be accounted for by conditioning on the history of a subset of the locally observable variables. InfluenceNet incorporates this idea by processing the current local observation separately with (1) a FNN and (2) a weighted linear combination of vectors representing particular regions in the input image with an RNN. The weighting of the linear combination in (2) is determined by an attention mechanism and the results of (1) and (2) are combined to determine value and action probabilities. Experimental results on a simple traffic control task and different Atari games support the effectiveness of the approach in determining the regions which contain relevant information. Further, the results indicate that (while improving the runtime performance) InfluenceNets performance is on par or even better in comparison to two simple baseline setups for problems in which the information can be reduced to a small subset of the input.\n\nOverall, the paper is marginally below the acceptance threshold, but I am willing to increase my score if some more clarification are provided and the results of the experiments are revisited and extended (see comments below). \n\nThe paper provides a methodological contribution by an extension of existing approaches from the field of POMDP methods and the application of an attention mechanism to address the problem of high dimensional input which I believe can be quite useful for the RL community. The submission is well written and easy to follow, the problem is well motivated and the concept of separating the Markovian and non-Markovian dependencies in the proposed architecture is explained in a reasonable way. Suitable experimental results support the architectual decisions and illustrate the benefits of the proposed method, but should be revisited with respect to the given remarks below. The paper includes reasonably complete information that will help to reproduce them and I think that the reproducibility of the results is good, even though I did not explicitly validate it. The authors provide the source code whose documentation could be improved (README, empty folder etc.).\n\n\nQuestions / clarifications / remarks:\n\nThe experiments should consider an architecture based on an LSTM in addition to the RNN as baseline for a recurrent network. Hausknecht & Stone (2015) and Sorokin et al. (2015), which are both cited in the paper, use LSTMs in their architectures and can be seen as related approaches and the current state of research for the problem at hand. The influence of the long-term memory component of an LSTM would be particularly interesting when considering the memory capability of an agent.\n\nThe results for Pong and flickering Pong with the RNN look suspiciously poor as if the agent would behave totally at random. The results should be double checked and if they are correct, please comment on them. Again, an approach based on a LSTM should be considered here, as it is known to work better than random on the problem of Pong. Check for example https://www.endtoend.ai/envs/gym/atari/pong/ and \"A3C LSTM\".\n\nFurthermore, it should be worked out where the increase in performance of InfluenceNet comes from compared to completely recurrent models. How much can be attributed to the separate FNN added to the architecture and how much to the attention mechanism?\n\nTable 1 shows the runtime performance of the individual methods. It is not clear how the d-patches were selected for InfluenceNet. Since it can be assumed that there are differences in runtime performance between a manual and automatic selection of d-patches, both variants should be listed.\n\n\nMinor comments:\n\nPage 2:     - typo: R(s_t, a_t) is defines the ... >> R(s_t, a_t) defines the ...\nPage 8:     - typo: InlfuenceNet >> InfluenceNet\nFigure 2:   - The shape and color of action a_C is not explained and does not correspond to the shape of action a_A; missing arrow from x_2 to the other state factors for t=3\nFigure 4:   - blank space in the second last sentence\nTable 1:    - One result for the Atari games is incorrectly highlighted bold \nTable 2:    - Multiple results are highlighted as being the best \n\n\nFinal remark:\n\nThe paper shows a very high agreement with the following (workshop) paper and should be considered as plagiarism if the authors do not match:\nMiguel Suau de Castro, Elena Congeduti, Rolf A.N. Starre, Aleksander Czechowski, and Frans A. Oliehoek. Influence-Based Abstraction in Deep Reinforcement Learning. In Proceedings of the AAMAS Workshop on Adaptive Learning Agents (ALA), May 2019."}