{"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes applying an attention mechanism to the input of a recurrent neural network such that the resulting architecture can learn to store only parts of the input which are actually relevant to predicting the future. This is motivated by the concept of a d-separating set, which serves as a sufficient set of observed variables to predict the influence of hidden variables. The idea is implemented as a soft spatial attention mechanism applied to the output of a CNN before it is input to an RNN. The architecture also includes a feedforward component to which the attention mechanism is not applied. The benefits of this architecture are explored in a simple but illustrative traffic domain where the light at an intersection is controlled using only local information. The architecture is also explored on several Atari games as well as a variant of pong which randomly hides certain observations to increase partial observability.\n\nThe motivation of learning d-separating sets so that the agent can focus on only the variables that can have a meaningful impact on the future is interesting and perhaps warrants further exploration. However, I find the method presented in this paper to be lacking in significant novelty or new insight and recommend rejection primarily for that reason. The method itself is only very loosely connected to the idea of d-separation sets as ultimately the whole thing is just trained by standard back-prop and there is no direct exploitation or learning of a graph structure. The method seems to me to be essentially a rebranding of spatial attention, the benefits of which are fairly well explored, but applied only to the recurrent part of the architecture which I don't consider enough of a contribution to publish at this venue.\n\nIn addition to this, the experiments provide limited insight or evidence of the effectiveness of the architecture. While the traffic example is reasonable, the Atari games tested (aside from perhaps flickering pong) have only fairly trivial partial observability (e.g. the direction an object is travelling cannot be determined from a single frame). This makes them a fairly poor domain for testing the proposed approach and does not seem in line with the proposed motivation of the paper.\n\nAlso, according to the appendix default hyper-parameters were used for everything except the additional parameters of InfluenceNet, which were hand-tuned, further limiting the significance of the experiments. In particular, I have trouble believing the RNN baseline couldn't be made to do well on flickering pong given a more favourable hyper-parameter setting. Also, was the recurrent architecture tested a regular RNN or a more sophisticated architecture like a GRU or LSTM? Given I would think one of that latter would be a more relevant baseline, as well as a more relevant setting to apply the proposed method, given ordinary RNNs are rarely used in practice.\n\nWhile I don't feel the paper presented here is ready for publication, the idea of studying of how we can isolate sufficient variables for representing the history is worthwhile and I hope the authors continue expanding on this direction."}