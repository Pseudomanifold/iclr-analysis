{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The motivation for this paper is straightforward. The author believes that the semantic information of actions should be explicitly considered in the multi-agent reinforcement learning problem, and the information needed to make different semantic actions should be different. To this end, the author divides the action semantics into two categories, one is actions that only affect the environment and itself, and the other is actions that affects the other agents. Making a decision corresponding to the former requires relying on all local observations of the agent, but making a decision corresponding to the latter requires only local observations related to the affected agent.\n\nTo this end, the author proposes a novel network structure ASN that can achieve the above objectives. Because it is a modification of the network structure, ASN can be combined with any type of multi-agent reinforcement learning algorithm to improve its convergence speed and final performance. The author conducted a rich experiment and verified the validity and superiority of the ASN structure from various angles.\n\nBut for this paper, I have doubts in the following aspects:\n1. In the experimental part, the author compares the ASN-based algorithm with QMIX and VDN in the StarCraft II environment, but the performance of the benchmark algorithm on the 8m and 2s3z maps is significantly lower than that of the original paper. Because the author did not use the RNN network when implementing these algorithms? If so, why not implement it with the RNN network? Can ASN also be combined with RNN?\n2. The basic idea of ASN is similar to the attention mechanism, and the experimental part also shows that the algorithm with attention module also achieved good results. I think the author should also compare with the MAAC algorithm (Actor-Attention-Critic for Multi-Agent Reinforcement Learning, ICML2019). There are two reasons for this: First, the MAAC algorithm is a SOTA MARL method based on the attention mechanism; secondly it belongs to the actorcritic algorithm. In the comparison part of the JAL algorithm, the author only considered the value-based methods VDN, QMIX, etc., without considering the policy gradient method or the actor-critic method.\n3. In the experimental part, the author also compares the proportion of ASN-based methods and benchmark algorithms that perform non-valid actions during training, such as to attack an agent outside the scope. However, in the network structure of ASN, if an agent is outside the field of view, it will not output the Q value or probability of the action corresponding to that agent. Does this mean that ASN is impossible to choose non-valid actions? So is the corresponding experiment meaningless?\n4. If an action can affect multiple other agents at the same time, can the ASN network handle it?"}