{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes to leverage prototypes to solve the mismatch problem in unsupervised domain adaptation. It further imposes intra-class compactness to help ambiguous classes. Experiments show it achieves new state-of-the-art results in several datasets.\n\npros:\n+ intra-class compactness to help ambiguous classes\n\nconcerns:\n-- Prototypes does not come from nowhere. They come from predictions. If you worry about the quality of target predictions (pseudo labels), then Eq. 8 and Eq. 9 are questionable. The intra-class compactness relies on p_t, too. The authors should explain why prototypes are superior than pseudo labels in [1].\n-- How does the authors select hyper-parameters? There are lots of magic numbers in Section 4.1 about hyper-parameters but no clues about how to tune them. Recently there is a paper [2] about model selection for UDA, maybe the authors should try it.\n\ndetails:\n- terminology: \"intra-class\" is better than \"within class\"\n- separate citations: e.g. entropy minimization, mean-teacher, and virtual adversarial training, have been successfully applied to UDA (Vu et al., 2019; French et al., 2018; Shu et al., 2018) -> entropy minimization (Vu et al., 2019), mean-teacher (French et al., 2018), and virtual adversarial training (Shu et al., 2018), have been successfully applied to UDA\n- confusion: At the last of Section 3.2, it says \\hat{f}=M^{T}p. But in Eq. 9, \\hat{f} and M^{T}p are concatenated, which is confusing: why do you concatenate two identical vectors?\n- Implementation Details: Section 4.1, paragraph 4: \\lambda^{f}_{adv} =5e-3, \\lambda^{f}_{adv} and \\lambda^{p}_{adv} increase from 0 to 1. It is confusing that \\lambda^{f}_{adv} both is a constant and changes continuously. \n\n[1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019"}