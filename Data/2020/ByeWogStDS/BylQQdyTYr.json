{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is under the topic of hierarchical reinforcement learning. The motivation of this paper is \"most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task.\" The paper proposes a method to learn higher-level skill selection and lower-level skill improvement jointly.\n\nWhat I like in this paper:\n    1. The paper, in general, is well-written so that I can understand it well.\n    2. Experiments are question-driven and provide interesting results.\n    3. Theories are closely related to the algorithm.\n\nKey reasons for my rejection:\n    1. My biggest concern is the motivation of this paper. \n    The joint learning of higher-level policy and lower-level skill discovery is not rare in modern literature. Some works are even cited in this paper, for example, option-critic, feudal network, etc. These methods fix their skills in the new task, not because they are inherently not able to do so, but because they want to demonstrate that the learned skills can be reused in new tasks, even if there is no further adaptation. I agree with the author that the agent needs to adapt its skills when faced with new tasks. But I don't think most works are limited in this aspect, as claimed by the paper in the abstract.\n\n    2. I think the author didn't justify his key design choices well. \n    This paper is under the research area of \"hierarchical reinforcement learning.\" However, just like temporal abstraction, the HRL is a general idea instead of an existing problem formulation or a particular algorithm. It seems that the author is not aware of this point as the paper claims a particular way of achieving HRL is the HRL itself (in section 4.1 \"In the context of HRL, a hierarchical policy with a manager \u03c0\u03b8h(zt|st) selects every p time-steps one of n sub-policies to execute.\"). I would like to see the paper takes the responsibility to justify the reason it follows this particular way. There are two more key decisions the paper proposed but not fully justified and analyzed.\n        1. Why is random length a valid choice? The paper doesn't tell readers the consequence of this design choice. For example, what about the optimality of the solution? Since bounding the random length needs prior knowledge, how difficult is it to come up with the prior knowledge. Is the algorithm sensitive to prior knowledge?\n        2. Why is it fine to assume \"for each action, there is just one sub-policy that gives it high probability\"? What would be the consequence of this assumption? Well, the extreme case is each action is only being chosen by one sub-policy. Therefore, executing the sub-policy becomes executing a repeated sequence of the same action. Obviously, this is a kind of temporal abstraction but is a very limited one. \n\nOther small issues:\nSection 2: \"... maximize the discounted expected reward ...\" should be \"... maximize the discounted expected return ...\".\nSection 2: the horizon T in the definition in \\eta should be H.\nSection 4: the advantage function is not defined."}