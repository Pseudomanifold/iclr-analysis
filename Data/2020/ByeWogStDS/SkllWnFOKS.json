{"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #564", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The main problem they try to tackle is to train agents for unseen tasks and environmental changes. They show that their method has a better performance and is more robust against sensor errors and physical parameter alterations.\n\nThe authors clearly position their work in the HRL paradigm and explain current limitations/challenges within that field. Alike other HRL agents, their method has two types of policies (manager and subpolicies), but different from other works they do not keep parameters fixed in post training for new tasks. In addition to the parameters, they do not fix the time length. \n\nThe paper is very well written, clearly stated the contributions. \n\nRemarks:\n- Fig 2 has no caption. How are the colors of balls obtained, since they only explain how the sensors (lidar) measure distances to balls (bombs/apples).\n- Fig 4/5a, some agents (blue) seems to have undesired behaviour (until half of the iterations). This behaviour is not described anywhere. \n- The URL of the website with code and videos does not have any code. \n\nQuestions to the authors:\n- Closest work is Frans et al. (2018). The experiments do not show Frans et al. as a benchmark method. Why?\n- HiPPO shows to have a higher robustness. Why are the results of different methods (p=10/random) in Table 1 for different environments (Snake/Ant) different?\n\n\n"}