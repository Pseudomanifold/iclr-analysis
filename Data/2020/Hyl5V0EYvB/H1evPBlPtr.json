{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper proposes to examine robustness against more unforeseen attacks (than Lp attacks) such as Elastic, Fog and Snow etc. The Unforeseen Attack Robustness (UAT) evaluation metric is further proposed to produce a consistent score over a range of test attacks and perturbations sizes. They also demonstrate the overfitting issue of joint adversarial training on 2 different norms of attacks.\n\n--------------\nMy concerns:\n1. It is hard to tell which type of robustness is evaluating in this paper: adversarial or common corruptions? Adversarial robustness often refer to robustness to small imperceptible adversarial perturbations, however, many of the perturbations illustrated in this paper are neither small nor imperceptible. I would expect none of existing models can survive any of those large perturbations. On the other hand, the new attacks proposed in this paper are basically common corruptions, which has been comprehensively studied in [1] on 15 types of corruptions including the proposed Elastic, fog and snow. I don't think adversarially perturb those common corruptions can contribute more for robustness evaluation. There are also other corruptions such as translations and rotations [2].\n\n2. The real contribution of this paper is not clear to me. What is the focus of this paper: the robustness of different DNNs, training strategies or defense techniques? Why only one type of defense (eg. adversarial training) was considered (why not a few latest works in adversarial training: [3][4])? My concern comes from this \"one-defense\" setting. Since all models are trained by one fixed defense, it is hard to draw any concrete conclusions on other defenses, or defense in general.  In the abstract, the authors claimed that \"These results underscore the need to evaluate and study robustness against unforeseen distortions\", which seems quite obvious. It is always beneficial to include more corruptions, but why the suggested Elastic, Fog, Snow in particular? \nSo far, defense techniques still suffer from low robustness to even small perturbations (the strongest defense, adversarial training, only has 43% - 65% robustness against PGD perturbation L_{\\infty} < 8/255). Then the question is, why evaluating against so many attacks is necessary and important, considering they even fail the easiest Lp corruptions?\nBy the way, in Table 1, why the ATA4 is 66.9, which seems too high, even higher than state-of-the-art robustness?\n\n3. The proposed \\epsilon_{min} and \\epsilon_{max} calibration is a bit confusing. \"The minimum distortion size \\epsilon_{min} is the largest \\epsilon for which the adversarial validation accuracy against an adversarially trained model is comparable to that of a model trained and evaluated on unattacked data.\" -- what this means, exactly? The proposed UAR score is also not clear: 1) what is the different between Acc and ATA? 2) \"ATA is the best adversarial accuracy on the validation set that can be achieved by adversarially training a specific architecture\" -- how this best accuracy was obtained?? 3) the validation set is also confusing, is it part of the training data held out for validation?\n\"To make UAR roughly comparable between distortions, we evaluate at \\epsilon increasing geometrically from \\epsilon_{min} to \\epsilon_{max} by factors of 2 and take the subset of \\epsilon whose ATA values have minimum `1-distance to the ATA values of the L1 attack at geometrically increasing \" -- I have no idea what the authors are referring to here.\n\n4. The presentation can be improved. For example, shorten the ticks used in those matrix plots (Figure 6, 7, etc). It can also benefit from a summary table somewhere about all those distortion types and sizes. It also helps if the authors can explain the \\epsilon_{min}, \\epsilon_{max} and UAR score with a more concrete example.\n\n[1] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations\n[2] The limitations of adversarial training and the blind-spot attack. ICLR 2019\n[3] Theoretically principled trade-off between robustness and accuracy. ICML, 2019.\n[4] On the Convergence and Robustness of Adversarial Training. ICML, 2019."}