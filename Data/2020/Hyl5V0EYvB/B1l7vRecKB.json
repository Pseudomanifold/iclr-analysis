{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper investigates empirically to what extent adversarial training with respect to a certain set of adversaries (e.g. Projected Gradient Descent (PGD) with a maximum l_inf perturbation of epsilon) also provides robustness against types of adversaries (e.g. PGD with different l_p norm constraints). While this problem has been studied before in terms of robustness transferability between different l_p constrained adversaries, this paper proposes a few novel attacks (\"JPEG\", \"Fog\", \"Garbor\", \"Snow\") to be included in such analysis. It shows that, while robustness from/against l_p constrained adversaries transfers relatively well, this is not the case for some of the newly proposed adversaries. It therefore proposes a framework, including a new canon of adversaries (including in particular the \"Fog\" attack),  for a more broad assessment of empirical robustness against a variety of adversaries.\n\nThe paper contains a lot of material that should be interesting to browse through for any researcher working the area of adversarial machine learning. The number of experiments carried out and reported on is impressive. The purpose of broadly and thoroughly assessing empirical robustness against a variety of adversarial attacks is important. While the paper makes a convincing case for the importance, some of the concrete steps that it's trying to promote still seem preliminary to me. For instance, how complete is the set of adversaries in the proposed UAR framework? In the sense: isn't it to be expected that the set of sources considered in Figure 6 (b) will have to be extended, e.g. once a new adversaries has been discovered that has low correlation with any of the known adversaries? In that sense, testing robustness against unforeseen/unknown adversaries still appears to be an open problem, unless the space of adversaries is constrained and it is established that any unknown adversary is necessarily correlated with the known ones.\n\nOn the methodological side,  the calibration of distortion sizes (p.4) was not clear to me. In 1.: what exactly means \"comparable\"? Does 2.a (\"images which confuse humans\") involve surveys / user studies? And what does it mean in 2.b: \"reduce accuracy of adversarially trained models to below 25\"?\n\n"}