{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper considers the robustness against adversarial attacks in learning of deep neural networks. Other than the usual Lp-distortions, the authors introduce novel adversarial distortions, JPEG, Fog, Gabor and Snow. Then, the authors propose the summary metric, unforeseen attack robustness (UAR), to measure the robustness against a given distortion. In experiments, the authors report UAR scores for existing and novel attacks and conclude that the existing defense against Lp attacks does not generalize to unforeseen attacks and the five attacks, L_\\infty, L_1, Elastic, Fog and Snow, offer greater diversity. The joint adversarial training over these five attacks is also investigated in experiments.\n\nThe four novel distortions defined in this paper are interesting and promising in practice. However, there are little theoretical discussions in this paper and most arguments look somewhat ad-hoc.\n\nAlthough the experiments are in fact extensive to some extent, since the results are only for specific datasets and models, it is difficult to tell how general the obtained observations are.\n\nSection 5 discusses the joint adversarial training by experiments. Are there any theoretical results on joint adversarial training related to the discussion here?\n"}