{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "* Summary *\nThe paper proposes defective convolutional layers as a measure of defense against adversarial attacks on deep neural networks. This layer sets the outputs of a randomly sampled but *fixed* set of neurons in the convolutional layers to zero during training and testing. The authors claim that defective convolutional layers encourage the model to pick up features other than local textures, e.g. shape information. The shape-vs-texture tradeoff is supported by experiments showing that defective CNNs perform worse than normal CNNs on images with permuted patches and that adversarial examples with larger epsilons exhibit more semantic shapes. The detailed experiment section evaluates the method on transfer-based, gray-box and black-box adversarial attacks,\tincluding Gaussian noise. Additionally, it provides ablation studies on the keep-probability and position of the defective layer.\n\t\nOverall I think this is a valuable contribution to a topic of high interest for ICLR and should be accepted. The method is simple to implement, has minor impact on the test accuracy and seems to increase robustness measures under the proposed settings across all of the tested architectures. However, the evaluation is lacking w.r.t. natural robustness, more detailed evaluation on the gray-box, black-box and white-box attacks. In the white-box setting (Table 8) a stronger attacker with a larger number of iterations and random restarts should be used in order to ensure that the difference in defense performance is real.\n\t\nThe experiment section should have a stronger focus on the gray-box attacks where the source network also has defective layers, since the method alters the network architecture and presumably learn a different set of features. However, the lack of transfer from \"normal models\",\tcan also be seen as supporting argument for the model picking up different, potentially robust, features, following the argument in [4].\n\t\nBecause the idea is motivated from the texture-vs-shape discussion [1], an evaluation on natural/empirical robustness under image corruptions [2], e.g. CIFAR10-C or ImageNet-C, and/or comparison\tto a network trained on Stylized-ImageNet, should be conducted.\n\t\n\t\nAdditional Feedback:\n- Since the method is closer to the original (neuron-wise) dropout than to SpatialDropout and DropBlock, including this in the evaluation would be appreciated.\n- To show the alignment of the adversarial gradients with the human-vision, the authors could visualize the loss gradients similar to [3] (Figure 2b)\n- For comparison in A.4, the median squared L2-distance of the adversarially trained Madry model under the decision-based attack would be helpful.\n- Why is the sampling of 4000 images in the patch experiment 3.2 done? What happens to the images that are correctly predicted but with <99% confidence?\n- What happens if this method is combined with adversarial training?\n\t\n[1] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness, https://arxiv.org/abs/1811.12231\n[2] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations, https://arxiv.org/abs/1807.01697\n[3] Robustness May Be at Odds with Accuracy, https://arxiv.org/abs/1805.12152\n[4] Adversarial Examples Are Not Bugs, They Are Features, https://arxiv.org/abs/1905.02175 "}