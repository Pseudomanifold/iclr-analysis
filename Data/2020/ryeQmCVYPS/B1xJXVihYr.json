{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper presents a technique for adversarial defense, employing what the authors refer to as \"defective  convolution layers\".  It attempts to use such layers to provide more adversarially robust models. The paper is easy to follow and well written, but the experimentation is lacking, and so the contribution is limited.\n\nDefective  Convolution  layers make use of a random fixed masking matrix, to fix some number of neurons to constant values, effectively removing the incoming weights of these neurons from the optimization problem. The outgoing weights of the masked neurons are effectively additional bias terms for the layer above. Applying this technique to a fully connected layer would be equivalent to training with a fully connected layer with a smaller size.\n\nThe paper presents the accuracy of such models on a variety of black box attacks. They focus on black box for two reasons, neither of which are particularly convincing. They claim that the white box attacks are semantically meaningful and so could fool a human, but no human evaluation is presented and the examples which are illustrated do not demonstrate this property.  They also claim to focus on black box attacks because is it more practical in a real world setting. However the model they present achieves significantly lower test accuracy on clean data than a standard network, so the practical deployment of such a model seems unlikely in its current implementation. \n\nThe paper presents thorough ablation studies on the architectural choices that go into this model. This is a positive quality of the work. However they do not compare standard networks with similar test accuracy to their defective models. As they note in the paper and in the appendix, there is a correlation between test accuracy and adversarial robustness. Based on the findings which they present, it is unclear if the effect of their \"Defective Convolution Layer\" is simply to reduce the test accuracy and thereby increase the adversarial robustness. This issue must be addressed for the work to contribute to the adversarial literature in a meaningful way. "}