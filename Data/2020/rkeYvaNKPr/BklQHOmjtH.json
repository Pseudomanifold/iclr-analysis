{"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "Summary of the Paper:\n  \n        This paper proposes a method to address Non Markovian Reward Decision Processes using RL. For this, NMRPDs are transformed into Markovian Decision Processes. The idea is that the reward function can only depend on the last state of the trajectory and the task. A task representation is introduced. This is learned recursively. The proposed approach is evaluated in several experiments.\n\nDetailed comments:\n\nA point of criticism is that it seems that the authors do not compare with similar or related methods in the experiments. In any case, I think that the paper is interesting and will receive the attention of the community.\n\nThe paper is well written in general with only some typos: E.g.\n\n\"that separate\"\n\n\"these proposition\"\n"}