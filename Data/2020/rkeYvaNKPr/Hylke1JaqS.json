{"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Short summary of paper:\nThe paper investigates representation learning  from trajectories in a framework which generalizes MDPs (NMRDPs), in which rewards are non-Markovian and follow a hidden process (this can be seen as a more structured, special case of a POMDP). The authors suggest learning a trajectory embedding by using a triplet loss, justified by a sufficient condition for learning an embedding which corresponds to the true task id.  \n\nOverall I felt the paper fell a bit short of the standards for ICLR, based on subpar writing, lack of comparison to convincing baselines, and unclear applicability to more complex environments.\n\nMain issues:\n- The methodological sections of the paper seems conceptually accessible while being written in an overly mathematical, meandering fashion, using heavy, sometimes unclear notation, or referring to notation from other papers.\n\nAs an example, \\mathcal{M}^* is first introduced by reference to another paper (the notation does not really need to be introduced until the mapping is made precise later in the paper); \\phi(\\theta)_i^j(-1) is an unnecessarily heavy, unpleasant notation for the concept used. \nIt was unclear if T is meant to represent the total trajectory length or a time index of a given trajectory (the notation would imply the former, the way it used, the latter).  Similarly equation (6) is a very mathematical way to denote a relatively simple condition (which could be written more simply by comparing inf and sup distances between two trajectory embeddings corresponding to different or identical latent h). Again, equation (7) seems like a contrived way to write a relatively simple concept, and the simple proof that (6) implies (7) is not included.\nI feel this section could be improved by using precise but as simple notation as possible, and a sequence of propositions explaining why the triplet loss allows for identifiability of the reward process, with a clear flow between statements.\n\n- Lack of baselines:\nNo alternative or baseline seems considered in the paper. The setting is perhaps a bit unusual, but since the data considered is essentially sequence of observations, any model of sequences could be used to produce embeddings (for instance, recurrent VAE, autoregressive models, time-contrastive methods) and compare the different trajectory representations.\n\n- Unclear generalization to more complex problems:\n(6) only implies (7) as far as I can tell, is sufficient but not necessary, and may be too strong of a condition to enforce; similary, theoretically requiring injectivity of phi seems practically too strong of a condition, as it essentially requires hashing each trajectory into a different embedding, in spite of the fact that many aspects of observations may be irrelevant for the task at hand). A key difficulty to scale this algorithm will be to find how to distance between trajectories in high dimension - perhaps the most important question for representation learning of trajectories, and a point the paper does not address.\n\n\nPositives:\nThe method is overall well principled and the theoretical justification of the triplet loss is interesting. The dataset used for experiments is also interesting.\n\nMinor/Questions:\n- IIUC, the equation (3) needs to hold for all times T, this needs to be clearly stated as often T is used to denote total episode length.\n- Bottom of page 4, 'The construction of the task ... \\phi_T=\\phi(\\mathcal T(o_T, \\phi_{T-1}); does this not require injectivity of \\phi, which is only stated afterwards?\n- Calling the algorithm 'EM' is a bit of a stretch (there are no latent variables), it is better to call it a form of coordinate ascent.\n- The way the EM-like algorithm is set up, it appears the gradient with respect to theta is only computed with respect to a single time step (similar to elman networks style truncated backprop) - why not use regular backpropagation through time to train though the entire sequence?\n- typo on section 5.1: 'Grid-world problem'\n- It is very hard to see on figure 3 what we are supposed to see (I don't see these as having the same destination, perhaps highlight more clearly?)\n"}