{"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents a reinforcement learning method that regularizes the objective using the mutual information term.\nThe idea is simple and the paper is easy to follow. \n\nHowever, the novelty is limited since the difference between the proposed method and Soft Actor Critic (SAC) is just adding the entropy term of \\pi(a) to the objective function if I understand the method correctly. In addition, the intuition of adding the entropy term of \\pi(a) to the objective is not clearly described.\n\nThe proposed method is evaluated on continuous control tasks.The results shown in the paper is mixed, and I cannot conclude that the proposed method outperforms SAC. Thus, the benefit of the proposed method is not clearly supported by the experimental results.\n\nFor the current form of the paper, I give \"weak reject\" due to the weak support of the experimental results and the unclear motivation of the method.\n\nOne of my concerns is that the way of estimating the \\pi(a) which is the marginal distribution of the action.\nFrom the current manuscript, I did not fully understand how it is estimated in the proposed method.\nI think that the accuracy of the estimation of \\pi(a) is crucial in the proposed method since it is the difference from SAC.\n\nA comment on the paper structure is that the connection to the \"capacity-limited\" objective should be described more explicitly in Section 2.1.\nAlthough the \"capacity-limited\" reminds me of the objective something like \\mathcal{L} + | C - I(X;Y)  | as in [Dupont, 2018],\nthe objective in the proposed method shown in page 2 is \\mathcal{L} + \\beta I(X;Y).\nI did not understand why the proposed method is \"capacity-limited\" until Section 5.\nI think authors should explicitly mention in Section 2.1 that \\beta is adjusted so as to limit the information capacity.\n\nTo improve the manuscript, I request authors the following things:\n\n- The proposed method can be interpreted as adding the penalizing the entropy of \\pi(a) to the entropy-regularized RL.\nI do not fully understand the intuition of penalizing the entropy of \\pi(a) in the context of RL. Please explain it.\n\n- I think some tasks should be performed with longer training. \nFor example, agent should be trained for 1-2 millions steps on Humanoid and Ant tasks.\nIn addition, evaluation with 10 trials are preferable.\n\n- Please cite papers that estimate the marginal distribution \\pi(a) in the same manner. \nIf there is no previous work, please explain the details of estimating \\pi(a) and how \\pi(a) is approximated from samples.\n"}