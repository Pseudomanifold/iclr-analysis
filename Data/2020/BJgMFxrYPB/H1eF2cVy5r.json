{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes an interesting, and to the best of my knowledge novel, pipeline for learning a semantic map of the environment with respect to navigability, and simultaneously uses it for further exploring the environment.\n\nThe pipeline can be summarized as follows: Navigate somewhere using some heuristic. When navigation \"works\", as well as when encountering something \"negative\", back-project that into past frames, and label the corresponding pixels as such: either positive or negative. This generates a collection of partially densely labelled images, on which a segmentation network can be learned that learns which part of the RGBD input are navigable and which should be avoided. For navigation, navigability of the current frame is predicted, and that prediction is down-projected into an \"affordance map\" that is used for navigation. One experiment confirms the usefulness of such an affordance map.\n\n\nI am marking weak reject currently because of the following concerns, which might be me just missing something. On the one hand, I am glad to see something that is not just blind \"end to end RL with exploration bonus\", sounds reasonable, and works well. On the other hand, I do have several major concerns about the method, outlined as follows:\n\n1. How can this approach work for moving obstacles? Let's say a monster walks from point A to point B, and collides with the agent at point B. Then, point B is marked as a hazard, but in the previous frames, the monster is not located at point B, and thus an image region that does not contain the monster is marked as hazard. Am I missing something here?\n2. The method does not seem practical for actual mobile robots, only for in-game or in-simulation agents. The reason being that in order to learn \"robot should not bump into baby\", the robot actually needs to bump into multiple babies in order to collect data about that hazard. To be fair, blind \"PPO+exploration bonus\" suffers from the same problem, but in this paper, the whole motivation is about mobile robots (at least that was my impression after reading it).\n\nFurthermore, I do not think I would be able to reproduce any of the experiments, as many details are missing. Will code be released?"}