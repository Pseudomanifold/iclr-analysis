{"title": "Review", "review": "This paper introduces a normalization technique, which normalizes the weights of convolutional layers. The method looks like the combination of batch normalization(BN) and weight normalization. Several assumptions are provided to proceed what the authors would like to show, and some experimental results follow. \n\npros)\n(+)  The idea of normalizing weights looks good.\n\ncons)\n(-) The major problem is the experimental section, where the results are not convincing and cannot support the effectiveness of the proposed method. Specifically, the proposed method cannot outperform batch normalization (BN) in respect of validation/test accuracy even the authors claim that training is faster. It looks similar to the training curve when using smaller learning rates. \n(-) The proposed method (+ mixup or manifold mixup) cannot show better results over batch normalization on ImageNet dataset.\n(-) The proposed method seems to have a dependency on the additional method such as mixup and manifold mixup. Furthermore, the parameter of them could determine the overall performance of it.\n(-) Section 5.1 cannot fully support the conjecture which is that the proposed normalization method can concern the covariate shift effect. \n\ncomments)\n- What is the benefit of the faster convergence in early epochs? We can observe faster convergence when using smaller learning rates, but finally, the larger learning rates can show better validation/test accuracy. As shown in Figure 2,  and some results in Figure 4, the accuracy does now outperform BN.  In this light, the faster convergence does not seem to have any advantages.\n- The authors should put the training curve about the first approach (shown on page 6) to clarify how the training goes differently.\n- For the second approach on page 6, it looks interesting the proposed method can outperform BN on CIFAR datasets when combining with mixup or manifold mixup. However, there is no analysis of why it works.  \n- The first equation would be changed without the cyclic padding assumption, and this would cause errors normalizing all weights (just following the derived equations). So, this affects not only on edge pixels but all the pixels in features.\n- Updating r with \"batchsize x heightout x widthout x stide^2\" makes a tremendously big value in the earlier layers (i.e., close to the input), so it must affect the normalization results.\n\n\nThe paper contains an interesting approach by normalizing weights directly, but the grounds for the conjecture are not clearly addressed. A few minor things for better readability should be addressed: all the equation should have equation number so that the reader can easily follow the paper, and the subsection titles (e.g., 4.1 Full case, 4.2 Assumptions (for what?)) are quite hard to understand what the authors will tell us. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}