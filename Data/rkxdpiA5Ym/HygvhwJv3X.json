{"title": "Simple idea, off-the-shelf comparison, discussion incomplete", "review": "The authors introduce the very simple idea of correlating graph modularity of cross-lingual embedding nearest neighbour graphs with downstream performance on cross-language document classification. Pros and cons: \n\na) Pro: The idea of checking whether nearest neighbors are overwhelmingly in-language, is very intuitive, and it is surprising that no one did this before.\nb) Pro: The empirical analysis of their method is relatively thorough. \nc) Pro: The authors run experiments on actual low-resource languages (bonus points!). \nd) Con: Cross-language document classification was introduced as a way to evaluate cross-lingual embeddings in Klementiev et al. (2012), but is a very crude way of evaluating cross-lingual embeddings; see Upadhyay et al. (2016) or Ruder et al. (2017) for discussion and alternatives. \ne) Con: In their comparison, the authors simply ran off-the-shelf methods. This is not very informative. For example, MSE+Orth uses unit length normalization, projecting both sets of embeddings onto the same sphere. This is likely to decrease modularity. It is, however, unclear whether that is why MSE+Orth works best in your experiments. Similarly, MUSE uses GANs and Procrustes refinement; it would be more apples-to-apples to compare again MUSE using the same dictionary seeds you used for CCA and MSE (rather than the GAN-induced seeds). \nf) Con: Several important details were omitted. E.g.: What dictionary seeds were used? What hyper-parameters did you use? In MUSE, for example, performance is very sensitive to hyper-parameters such as drop-out rate and the number of epochs. \ng) Con: The authors do not discuss the effect of random initialization; see Artetxe et al. (2018) or S\u00f8gaard et al. (2018). Specifically, MUSE is known to be very unstable, and the above authors report performance over 10 randomly initialised runs. \nh) Con: The authors do not investigate what causes modularity. In GANs, for example, this could be a result of mode collapse. In the case of MSE, why do the authors think modularity is lower than MSE+Orth (assuming this holds in the normalized case)?\ni) Con: The authors do not discuss the linearity assumption implicit in all these methods. Nakashole et al. (2018) argues that often (ideal) mappings are not linear. How would this effect these methods?\n\nIn sum, I think modularity is probably an important concept in diagnosis, but I think the discussion is incomplete, ignoring the inductive biases of the embedding algorithms and the impact of hyper-parameters. While the empirical analysis was thorough, the main experiments were on a somewhat uninformative task (topic classification). I was also a bit under-whelmed by the off-the-shelf comparison of existing methods. \n\nQuestions and minor comments: \n\nThe authors say: \u201cwe focus on mapping-based approaches because of their applicability to low-resource languages by not requiring large bilingual dictionaries or parallel corpora\u201d. An important observation in Ruder et al. (2017), however, is that mapping-based approaches can be equivalent to joint and pseudo-mixed corpus approaches.\n\nIn \u00a72.2., you mention QVEC as a method for measuring consistency, monolingually. I guess coherence can also be measured by analogy, relatedness, or computing the distances (in embedding space) of edges in a knowledge base, say WikiData? This would be possible across languages. \n\nThe problem with mismatched senses is simply a side-effect of one-embedding-per-word-form, right? This is not mentioned explicitly. \n\nThe idea of checking whether nearest neighbors are overwhelmingly in-language, is intuitive, and it is surprising that no one did this before. However, I find statements such as \u201cExisting approaches do not reliably detect this problem\u201d a bit misleading. I feel it would be more fair to talk about the contribution here being a \u201csimple observation\u201d rather than a new approach. On a similar note, modularity is a very simple concept, so, again, it\u2019s a little misleading to say the authors \u201c apply concepts from network science\u201d. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}