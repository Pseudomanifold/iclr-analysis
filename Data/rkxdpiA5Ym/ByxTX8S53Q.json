{"title": "Nice idea, clean exposition, partly thin experiment", "review": "The paper proposes to inspect cross-lingual word embeddings as graphs, and suggests that high modularity of these graphs represents groupings by languages, and subsequently yields poor performing NLP models if they are based on such embeddings.\n\nThe idea is simple, clean, and nicely exposed. The experiment clearly suggests that inspecting modularity of graphs into which cross-lingual embeddings are cast makes a lot of sense: As per Figure 3, for example, modularity and performance are strongly inversely correlated. Modularity is further shown to be superior to alternative metrics in section 5.\n\nThe paper does leave me wanting for more cross-lingual embeddings to be analyzed in the experiment, as the list of four approaches does not chart the space of related work that nicely, e.g., no methods that feature parallel corpora as learning signal are included. For an updated version, I would be happy to see a nicer sample of embeddings construction methods.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}