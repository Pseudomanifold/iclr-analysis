{"title": "Should be better supported both theoretically and empirically", "review": "This paper presents a new evaluation method for cross-lingual word embeddings. The paper argues that the quality of cross-lingual word embeddings is correlated with their modularity (i.e. whether words in one language tend to appear next to words of the same language alone), proposes a new evaluation metric based on this idea, and explores the relationship between this metric and downstream performance in two tasks.\n\nWhile I think that the core idea of the paper is original and has some interest, I would say that the paper is not strong enough to get accepted. The topic of the paper seems rather narrow to me and, in addition to that, I think that the proposed method should be better supported both theoretically and empirically.\n\nA few comments:\n\n- It is not clear to me what the use case of the proposed method is. In particular, the proposed evaluation seems valid when applied to different cross-lingual mapping methods over the exact same embeddings, but its validity beyond that is not obvious nor tested empirically in the experiments. Is the proposed measure comparable across languages? What about different embedding learning methods and/or hyperparameters (e.g. CBOW vs skip-gram)? In other words, can the proposed method be used to predict downstream performance for any given set of cross-lingual embeddings (possibly using entirely different learning algorithms), or is it only valid to rank different cross-lingual mapping methods as applied to the exact same embeddings?\n\n- I assume that language typology can play an important role here, but this is not discussed at all in the paper. In particular, I think that the core idea of the paper can be somewhat questionable for languages that diverge morphologically. For instance, Spanish distinguishes between \"lento\" (masculine singular), \"lenta\" (feminine singular), \"lentos\" (masculine plural) and \"lentas\" (feminine plural), whereas English has a single word (\"slow\") for all of them. Should \"lento\" be closer to \"slow\" than to \"lenta\", \"lentos\" and \"lentas\"? That's what your proposal would favor, but it is not obvious at all to me! And this is probably the simplest case, as this phenomenon would be accentuated for morphologically rich languages.\n\n- Your experiments only compare 4 cross-lingual embedding mapping methods. This seems too little to me and, to make things worse, the compared methods are strongly connected. In particular, Artetxe et al. (AAAI 2018) show that MSE, CCA and MSE+Orth can be seen as variants of the same general framework, and the unsupervised method of Conneau et al. (ICLR 2018) also uses MSE+Orth in its iterative refinement. I think that you could (and should) compare more cross-lingual mapping methods and, more importantly, you should include other cross-lingual embedding methods that are not based on mappings (e.g. bilbowa and bivec).\n\n- The proposed evaluation method is compared to QVEC-CCA and cosine similarity between translation pairs. This looks like a very unusual choice to me, as most papers in the topic evaluate on bilingual lexicon induction and, to a less extent, cross-lingual word similarity. I think that you should definitely compare your proposed method to word translation accuracy, and including cross-lingual word similarity would also be good.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}