{"title": "Title claim seems wrong", "review": "The paper \u2018Generative model based on minimizing exact empirical Wasserstein distance' proposes\na variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying\non the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.\nComparisons with other variants of Wasserstein GAN is proposed on MNIST.\n\nI see little novelty in the paper. The derivation of the primal version of the problem is already \ngiven in  \nCuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).\n\nUsing optimal transport computed on batches rather the on the whole dataset is already used in (among\nothers)\n Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS\n Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  \n\nAlso, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on \nbatches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very \nwell described in the paper (Section 3): \nhttps://openreview.net/pdf?id=S1m6h21Cb\n\nComputing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be\nproved and discussed.\n\nFinally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).\n\n\nTypos:\n Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}