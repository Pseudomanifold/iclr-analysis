{"title": "Neural implementation of approximate SFA with a whitening layer using power iterations.", "review": "Summary\nThe manuscript proposes to use power iterations in an approximate \"whitening layer\" to optimize the slowness objective of SFA in a very general setting. A set of experiments illustrates that this way of doing nonlinear SFA is meaningful.\n\nQuality\nAlthough the idea is pretty straight forward and the paper shows qualitative results on a number of datasets, the relative merit of the approach is empirically not well characterized.\n\nClarity\nThe manuscript is in general well written and the technical content is well accessible. However the description of the whitening layer implementation needs some more details.\n\nOriginality\nThe idea of using a whitening layer together with the slowness objective has not been explored before. There is a second ICLR 2019 submission (Pfau et al.) with a very similar idea, though.\n\nEmpirical Evaluation\nThe approximate whitening should lead to a trade-off between whitening and slowness optimization. I miss an experiment illustrating that trade-off. Also the comparison to nonlinear SFA using expansion or kernelization of hierarchical SFA is empirically not properly characterized. In the end, if one takes the slowness objective seriously, one would use the method yielding slower results.\n\nSignificance\nThe manuscript introduces a way of running nonlinear SFA with approximate constraints in a general deep learning setting with a differentiable implementation using a dedicated whitening layer based on power iterations.\n\nReproducibility\nThe data is either synthetic or publicly available. The Keras implementation of the PowerWhitening layer as well as the entire neural network along with its optimization schedule is not shared. Hence, there should be some effort involved to reproduce the experiments.\n\nPros and Cons\n1+) The idea of an approximate whitening layer is conceptually simple and clear.\n2-) The description of the practical implementation of the power iteration is slightly imprecise.\n3-) The algorithm scales badly in the number of output dimensions. This scaling is bad in a computational sense and also in a statistical sense.\n\nDetails\na) Section 6.1: Why do you need to add the noise term? What is the statistical meaning of this added noise?\nb) Section 6.1: the solutions if comparable -> the solutions is comparable\nc) References: Shaham -> ICLR 2018 paper\nd) References: nystr\u00f6m -> Nystr\u00f6m\ne) The name for the algorithm \"Power SFA\" is a little bit bold.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}