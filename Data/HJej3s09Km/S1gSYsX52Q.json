{"title": "In this paper, the authors studied how the activation function affects the behavior of randomized deep networks. ", "review": "* summary\nIn this paper, the authors studied how the activation function affects the behavior of randomized deep networks. \nWhen the activation function is permissible and the weights of DNN are generated from the Gaussian distribution,\nthe output of each layer was related to the so-called length process. When the permissibility is violated,\nthe convergence property may not hold. Some numerical experiments confirm the theoretical findings. \n\n\n* comments\nHowever, The randomized DNN is not clear whether theoretical results in this paper is related to the practical DNN. \nThe authors showed intensive proofs of theorems.\nI think that the relation between DNN in practice and the results in this paper should be pursued more. \n\n* The meaning of Theorem 10 is not clear. What does the theorem reveal about the ReLU function in the practical usage?\n\n* In this paper, a limit theorem in terms of the dimension N is considered.\n  However, the limit theorem in terms of the depth D is also important for the DNN.\n  Some comments on that would be helpful for readers. \n\n* Is there any relation between the analysis in this paper and batch normalization or weight normalization? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}