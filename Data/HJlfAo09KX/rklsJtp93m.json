{"title": "Incremental work, not strong enough", "review": "This paper studies the problem of learning the parameter of one hidden layer neural network with sigmoid activation function based on the negative log likelihood loss. The authors consider the teacher network setting with Gaussian input, and show that gradient descent can recover the teacher network\u2019s parameter up to certain statistical accuracy when the initialization is sufficiently close to the true parameter. The main contribution of this paper is that the authors consider the classification problem with negative log likelihood loss, and provide the local convergence result for gradient descent. However, based on the previous results in Mei et al., 2016 and Zhong et al., 2017, this work is incremental, and current results in this paper is not strong enough. To be more specific, the paper has the following weaknesses:\n\n1.\tThe authors show the uniformly strongly convex and smooth property of the objective loss function which can get rid of the sample splitting procedure used in Zhong et al., 2017. However, the method for proving this uniform result has been previously used in Mei et al., 2016. And the extension to the negative log likelihood objective function is straightforward since the derivate and Hessian of the log likelihood function can be easily bounded given the sigmoid activation function. \n2.\tThe authors employ a tensor initialization algorithm proposed by Zhong et al, 2017 to satisfy their initialization requirement. However, it seems like that the tensor initialization almost enables the recovery as it already lands on a point close to the ground truth, the role of GD is somehow not \nthat crucial. If the authors can prove the convergence of GD with random initialization, the results of this paper will be much stronger.\n3.\tThe presentation of the current paper needs to be improved. The authors should distinguish \\cite and \\citep. There are some incomplete sentences in the current paper, such as in page 3, \u201cMoreover, (Zhong et al., 2017b) shows\u2026the ground truth From a technical perspective, our\u2026\u201d.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}