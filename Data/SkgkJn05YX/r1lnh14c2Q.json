{"title": "Simple approach with experimental validations, however, seems ad hoc", "review": "I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter \"drop rate\" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. \n\n======\nThe authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.\n\nMajor concerns:\nAn extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.\n\nAnother major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.\n\nMinor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}