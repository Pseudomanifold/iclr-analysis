{"title": "Needs some improvement.", "review": "The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. I have three major concerns:\n\n(1) The optimization problem for complete orthogonal dictionary learning in this paper is very different from overcomplete dictionary learning in practice. It is actually more similar to tensor decomposition-type problems, especially after smoothing. From this point of view, it is not as interesting as the optimization problem.\n\nArora et al. Simple, Efficient, and Neural Algorithms for Sparse Coding, 2015\n\n(2) Some recent works focus on analyzing gradient descent for phase retrieval and matrix sensing. These obtained results are significantly improved and near-optimal. However, the convergence rate in this paper is very loose. Besides, the paper even does not look into the last phase of gradient descent, when there exists restricted strong convexity. Thus, only sublinear convergence rate is presented.\n\nChen et al. Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval, 2018\n\nThe quality of this paper could be improved, if the author could sharpen their analysis.\n\n(3) The analysis for the manifold gradient methods is something new, but not very significant. There have already been some works on manifold gradient methods. For example, the following paper has established convergence rates to second order optimal solutions for general nonconvex function over manifold.\n\nBoumal et al. Global rates of convergence for nonconvex optimization on manifolds. 2016.\n\nThe following paper has established the asymptotic convergence to second order optimal solutions for general nonconvex function over manifold.\n\nLee et al. First-order Methods Almost Always Avoid Saddle Points, 2017.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}