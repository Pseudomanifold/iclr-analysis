{"title": "Interesting extension of minibatch GD using a straight-forward application of bandits", "review": "This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. Its results suggest that RMGD faster than MGD with grid search, and generalizes better.\n\nThe paper is well written. The idea itself is a simple and relatively straightforward application of bandits. The paper has some merits as it proposes an efficient and theoretically sound method to replace grid search in MGD.\n\nOne result that stands out is that RMGD achieves better results than the best performing batch size. The authors may want to discuss this in more depth. This may be due to the fact that the problem is inherently contextual: each epoch is different from other epochs, and may require a better-suited bach-size. Maybe contextual bandits would be a good candidate to try.\n\nComments:\n- offer some analysis or explanation of the surprising results\n- add equation numbers for ease of reference\n- in 4.1, why did you use this particular probability update? motivate/explain this choice.\n- appendix A: Specify that <> is dot product.\n                        introduce Beta\n                        briefly explain mirror descent\n                        why is beta z >= -1? My sense is that it is >= 0. can it be negative?\n                        explain, motivate or cite the equation following beta z >= -1\n\nI am pretty familiar wit bandit literature. Less so with GD literature. The paper's hybrid approach, although simple, exposes interesting questions. I tend towards accepting the paper.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}