{"title": "Review", "review": "The paper presents a new take on attention in which a large attention dataset is collected (crowdsourced) and used to train a NN (with a new module) in a supervised manner to exploit self-reported human attention. The empirical results demonstrate the advantages of this approach.\n\n*Pro*:\n-\tWell-written and relatively easily accessible paper (even for a non-expert in attention like myself)\n-\tWell-designed crowdsourcing experiment leading to a novel dataset (which is linked to state-of-the-art benchmark)\n-\tAn empirical study demonstrates a clear advantage of using human (attention) supervision in a relevant comparison \n\n*Cons*\n-\tSome notational confusion/uncertainty in sec 3.1 and Fig 3 (perhaps also Sec 4.1): E.g. $\\mathbf{M} and {L_clickmaps} are undefined in Sec 3.1.\n\n*Significance:* I believe this work would be of general interest to the image community at ICLR as it provides a new high-quality dataset and an attention module for grounding investigations into attention mechanisms for DNNs (and beyond). \n\n*Further comments/questions:*\n-\tThe transition between sec 2 and sec 3 seems abrupt; consider providing a smoother transition. \n-\tFigure 3: reconsider the logical flow in the figure; it took me a while to figure out what going on (especially the feedback path to U\u2019).\n-\tIt would be beneficial to provide some more insight into the statistical tests casually reported (i.e., where did the p values come from)\n-\tThe dataset appears to be available online but will the code for the GALA module also be published?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}