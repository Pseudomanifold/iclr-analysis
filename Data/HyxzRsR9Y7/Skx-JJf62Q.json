{"title": "intuitive/elegant idea, well-written, convincing results", "review": "The paper describes a method to improve reinforcement learning for task with sparse rewards signals.\n\nThe basic idea is to select the best episodes from the system's experience, and learn to imitate them step by step as the system evolves, aiming at providing a less sparse learning signal.\n\nThe math works out to a gradient that is of similar form as a policy gradient, which makes it easy to interpolate both of them. The resulting training procedure is a policy gradient that gets additional reinforcement of the system's best runs.\n\nThe experiments show the validity especially for the most extreme case (episodic rewards), while, as expected, for the other extreme of dense rewards, the method's effect is not consistently positive.\n\nThe paper then critiques its own method and identifies a critical weakness: the reliance on good exploration. I like that a lot. The paper goes on to suggest an extension to address this by training an ensemble, and shows the effectiveness of this for a number of tasks. However, I feel that the description of this extension is less clear than that of the core idea, and introduces too many new ideas and concepts in a too condensed text.\n\nThe paper seems a significant in that it provides a notable improvement for sparse-rewards tasks, which are a common sub-class of real-world problems.\n\nMy background is not RL. While I am quite confident in my understanding of the paper's math, I am not 100% familiar with the typical benchmark sets. Hence, I cannot judge whether the results include good baselines, or whether the task selection is biased. I can also not judge the completeness of the related work, and how novel the work is. For these questions, I hope that the other reviewers can provide more information.\n\nPros:\n - intuitive idea for a common problem\n - solution elegantly has the form of a modified policy gradient\n - convincing experimental results\n - self-critique of core idea, and extension to address its main weakness\n - nicely written text, does not leave a lot of questions\n\nCons:\n - while the core idea is nicely motivated and described and good to follow, Section 2.3 feels very dense and too short.\n\nOverall, I find the core idea quite intuitive and elegant. The paper's background, motivation, and core method are well-written and, with some effort, quite readable for someone who is not an RL expert. I found that several questions I had during reading were preempted promptly and addressed. However, the description of the secondary method (Section 2.3) is too dense.\n\nTo me, the paper solidly meets the threshold of publication. Since I have no good comparison to other papers, I rate it a \"clear accept\" (8).\n\nMinor points:\n\nI noticed a few superfluous \"the\", please double-check.\n\nIn Table 1, please use the same exponent for directly comparable numbers, e.g. instead of \"1.8e5 4.4e4\", say \"18e4 4.4e4\". Or best just print the full numbers without exponent, I think you have the space.\n\nWhen reading Table 1, I could bnot immediately line up \"PPO\" and \"Self-imitation\" in the caption with the table columns. It took a while to infer that PPO refers to \\nu=0, and SI to \\nu=0.8. Can you add PPO and SI to the table headings?\n\nYou define p as \"the masking probability\", but it is not clear whether that is the probability for keeping a \"1\" in the mask,\nor for masking out the value. I can only guess from the results. I suggest to rephrase as \"the probability of retaining a reward\". Also, how about using plain words in Table 1's heading, such as \"Noisy rewards\\nSuppressing 10% of rewards\", so that one can understand the table without having to search for its description in the text?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}