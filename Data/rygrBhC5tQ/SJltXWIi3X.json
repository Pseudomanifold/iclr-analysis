{"title": "Potentially very useful idea", "review": "** Summary **\nThe authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.\n\n** Quality & Clarity **\nThe paper is well written and features an extensive set of experiments.\n\n** Originality **\nI am not aware of similar work and believe the idea is novel.\n\n** Significance **\nSeveral recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}