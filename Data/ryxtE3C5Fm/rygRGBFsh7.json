{"title": "An interesting approach to improve generalization ability of adversarial training by using GAN.", "review": "This paper touches on the limitation of adversarial training and propose a method that combines generator, discriminator, and adversarial attacker to improve the robustness and generalization ability of the network. The paper reads well (through there are a number of typos) and the source code is publicly available on the github. \n\nPros:\n- It provides insights on why adversarial training may not improve robustness on the test data set. \n- It demonstrates that one can improve robustness on the test data set by using GAN (to predict the real data distribution) and plugging it into adversarial training.\n- Potential improvements on robustness and convergence speed of GAN.\n\nCons:\n- lack of theoretical analysis.\n- needs comparison with a number of relevant work in the literature that use GAN for adversarial training, more specifically, the following works are related to this paper:\n     * Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples, 2018\n     * Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial\n        examples with adversarial networks, 2018\n\nOther comments:\n- There is a contradiction between Table 1 and the text on Page 1 (the contribution section). As per table 1, the performance of the proposed approach drops to 30.25%, whereas on page 1 it is mentioned it is dropped to 36.4%.\n- Figure 4 needs to be elaborated. The symbols are also not introduced.\n- It also helps if the authors provide a pseudocode specifying all the steps they took to generate the results. \n- Figure 6 is hard to read and interpret. It could help if the authors show fewer images and discuss some of the differences.\n- There are several typos in the paper. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}