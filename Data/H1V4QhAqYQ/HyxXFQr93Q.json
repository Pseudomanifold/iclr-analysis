{"title": "Interesting idea with insufficient support", "review": "This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques to generate training data from a small minibatch. The authors claim the proposed method has better generalization performance. \n\nI think it is an interesting idea, but the current draft does not provide sufficient support.\n\n1. The proposed method is very simple. In this case, I would expect the authors provide more intuitive explanations. It looks to me the better generalization comes from more complicated data augmentation, not from the proposed large batch training.  \n\n2. It is unclear to me what is the benefit of the proposed method. Even provided more computing resources, the proposed method is not faster than small batch training. The improvement on test errors does not look significant. If given more computing resources, and under same timing constraint, we have many other methods to improve performance. For example, a simple thing to do is t0 separately train networks with standard setting and then ensemble trained networks. Or apply distributed knowledge distillation like in (Anil 2018 \nLarge scale distributed neural network training through online distillation)\n\n3. The experiments are not strong. The largest batch considered is 64*32, which is relatively small. In figure 1 (b), the results of M=4,8,16,32 are very similar, and it looks unstable. It is unclear what is the default batchsize for Imagenet. In Table 1, the proposed method tuned M as a hyperparameter. The baselines are fairly weak, the authors did not compare with any other method. I would expect at least the following baselines:\ni)  use normal large batch training and complicated data augmentation, train the model for same number of epochs\nii) use normal large batch training and complicated data augmentation, train the model for same number of iterations\nii) use normal large batch training and complicated data augmentation, scale the learning rate up as in Goyal et al. 2017\n\n4. For theorem 1, it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD. I fail to understand the the authors\u2019 augmentation. Following the author\u2019s logic, normal large batch training decrease the variability of <H>_k and \\lambda_max, which converges to \u2018\u2019flat\u2019\u2019 minima. It contradicts with the authors\u2019 other explanation. \n\n5. In section 4.2, I fail to understand why the proposed method can affect the norm of gradient. \n\n\n6. Related works:\nSmith et al. 2018 Don't Decay the Learning Rate, Increase the Batch Size. \n\n\n=============== after rebuttal ====================\nI appreciate the authors' response, but I do not think the rebuttal addressed my concerns. I will keep my score and argue for the rejection of this paper. \n\nMy main concern is that the benefit of this method is unclear. The main baseline  that has been compared is the standard small-batch training. However, the proposed method use a N times larger batch and same number of iterations, and hence N times more computation resources. Moreover, the proposed method also use N times more augmented samples. Like the authors said, they did not propose new data augmentation method, and their contribution is how to combine data augmentation with large-batch training. However, I am not convinced by the experiments that the good performance is from the proposed method, not from the N times more augmented samples. I have suggested the authors to compare with stronger baselines to demonstrate the benefits. However, the authors quote a previous paper that use different data augmentation and (potentially) other experimental settings. \n\nThe proposed method looks unstable. Moreover, instead of showing the consistent benefits of large batch, the authors tune the batchsize as a hyperparameter for different experiments. \n\nRegarding the theoretical part, I still do not follow the authors' explanation. I think it could at least be improved for clarity. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}