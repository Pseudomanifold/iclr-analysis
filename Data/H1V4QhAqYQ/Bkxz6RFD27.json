{"title": "review", "review": "The paper shows that training with large batch size (e.g., with MxB samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch; the transform is executed by a data augmentation method such as Cutout or Dropout. The authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks.\n\nThe paper is well written and easy to follow. Also, some interesting results are experimentally obtained such as the figures presented in Figure4. Nevertheless, the experimental studies are not very satisfactory in its current form.\n \nMajor remarks:\n\n1.\tIn terms of regularization with transformed data in a given batch, the proposed method is related to MixUp (Zhang et al., mixup: Beyond empirical risk minimization), AdaMixUp (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), Manifold Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), and AgrLearn (Guo et al. Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks). It would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect.  For example, in MixUp, AdaMixup and Manifold Mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. In these sense, using them as baselines would make the contribution of the proposed method much significant. \n2.\tIn the experiments, it seems the authors use different data augmentation methods for different datasets  (except for Cifar10 and Cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using Cutout for the MobileNet and ResNet50 on the ImageNet data set. \n3.\tRegarding the experimental study, I wonder if it would be beneficial to include three variations of the proposed method. First, use baseline with the same batch size, namely BxM, but with sampling with replacement. That is, using the same batchsize as that in Batch Augmentation but with repeated samples. In this way, the contribution of the data augmentation in the proposed method would be much clearer. Second, as suggested from the results in the PTB data in Table1, using only Dropout obtains very minor improvement over the baseline method. In this sense, using other data augmentation methods instead of Cutout for the image tasks would make the contribution of the paper much clear. Third, training the networks with the batchsize of BxM, but excluding the original data samples in the given batch would be another interesting experiment. That is, all samples of the batch in the batch augmentation are synthetic samples. \n\nMinor remarks:\n\n1.\tIs the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup?\n2.\tWould it be beneficial to include various data augmentation methods for the same batch? That is, each transformed sample may come from a different data augmentation strategy.\n\n==========after rebuttal===========\n\nMy main concern is that the paper did not clearly show where the performance improvement comes from. It may simply come from the larger batch size instead of the added augmented samples as claimed by the paper. I think the current comparison baseline in the paper is insufficient. I did propose three comparison baselines in my initial review, but I am not satisfied with the authors' rebuttal on that.  \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}