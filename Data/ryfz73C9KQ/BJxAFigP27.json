{"title": "Interesting ideas, excellent experiments but a big question mark and significance hard to assess", "review": "This paper learns a deep model encoding a representation of the state in a POMDP using one-step frame prediction or a  contrastive predictive coding loss function. They evaluate the learned representation and shows it can be used to construct a belief of the state of the agent. \n\nUsing deep networks in POMDP is not new, as the authors pointed out in their related work section. Thus I believe the originality of the paper lies in the type of loss used and the evaluation of the learned representation trough the construction of a belief over current and previous states. I think this method to evaluate the hidden state has the potential to be useful should one wish to evaluate the quality of the hidden representation by itself. In addition, I found the experimental evaluation of this method to be rather extensive and well conducted, as the authors experimented on 3 different (toy) environments and use it to quantify and discuss the performance of the three model architecture they develop.\n\nOn the other hand, the authors mention that other works have already shown that learned representations can improve agent performance, can be learned by supervised learning (= predicting future observations) or can be useful for transfer learning. So in that context, I am not sure the contributions of this paper are highly significant as they are presented. To better highlight the strength the evaluation method, I think it could be interesting to check that the accuracy in belief prediction is correlated with an improvement in agent performance or in transfer learning tasks. To better highlight the interest of the CPC loss, I think it could be interesting to compare it to similar approaches, for example the one by Dosovitskiy & Koltun (2017).\n\nI found the paper reasonably clear. However, the following sentence in the appendix puzzled me. \"The input to the [evaluation] MLP is the concatenation of b_t and a one-hot of the agent\u2019s initial discretised position and orientation.\" I may have missed something, but I do not understand how the model can contain the uncertainty shown in the experimental results if the agent's initial position is provided to the current or past position predictor.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}