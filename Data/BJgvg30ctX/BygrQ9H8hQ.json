{"title": "Review (updated after readng other reviews and other responses)", "review": "In this paper, the authors propose to train a model from the point of view of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. They show that the objective can be minimized by looking to make the final layer vectors be as uncorrelated as possible to the final layer representations, and simplify the same by applying Holder\u2019s inequality to make the optimization tractable. They also apply an L1 penalty on the final layer. Experiments on CIFAR and MNIST show that using their regularizer to train DNN models yield  gains in performance. The presence of the L1 penalty also makes the results more interpretable (to the extend possible by looking at a subset of features in the last layer of a DNN). \n\nCOMMENTS:\n\n- Meta Point: To really see that the regularization framework you\u2019re proposing is good, why not just pick a simple, feedforward model or convNet, see the performance and then compare it with the regularizer you\u2019re proposing? That will help hit the point home. \n\n- Page 1: before jumping to equations (1) and (2), please formally define Mutual Information. The actual definition is much later in the text, but it\u2019s better to define first. \n\n- Beyond  referring the user to section 3 on Page 1, please also mention a couple of key references in the appropriate locations. \n\n- Page 3 paragraph 2: \u201cMutual information is bounded \u2026 correct them\u201d : Can you provide some formulas for this and make this concrete? Or perhaps provide some references? This line is vague. \n\n - prop 2.1 and 2.2: can you define what you mean by \u201cempirical version\u201d? Again, it\u2019s probably good to have these terms crisply defined before using them. \n\n- eqn (6) is interesting. Holder\u2019s inequality gives you the product terms. Then you can also apply the AM-GM inequality, and get a sum. So then at the end of it all, you\u2019re left with the standard elastic net penalty and not the product form. In that case, aren\u2019t we back to just the usual regularization strategy? And in which case, should I interpret the results you have in sec 4 as \u201cusing L1 penalties with L2 is good\u201d ?\n\n- To the point above, I guess one difference after the AM-GM step is that you will not have a squared L2 norm, but just L2. This is reminiscent of linear models where they use L2 loss instead of squared L2 loss. But on the penalty, squaring just adds smoothness. Can you comment on this? \n\n- sec 4.2.1: I don\u2019t see how fig (2) (L) is \u201croughly Gaussian\u201d. Can you explain? Maybe plot the histogram? Also for fig (2, R): the coefficients are approximately sparse. It\u2019s not sparse as you claim since there are almost no zeros in the coefficients. \n\n- I don\u2019t get the point of sec 4.3: How does this claim not apply to all deep learning models, regardless of the penalizations you propose?\n\n\nedit: \nI have read the responses and the other reviews. The authors have addressed the few major points I had. I still think there are a few gaps that need to be addressed (as pointed by the other reviewers)\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}