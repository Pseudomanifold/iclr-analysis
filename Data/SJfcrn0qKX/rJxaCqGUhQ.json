{"title": "Not really convinced...", "review": "This paper describes a method for generating '3D adversarial meshes'. Existing 3D meshes are deformed and their textures changed to 'fool' an object classifier or an object detector. This is done by maximizing an adversarial loss computed on rendering of the 3D mesh, together with regularization terms also applied on the rendering but also on the 3D perturbations, to make sure the modified mesh is still plausible. The rendering is performed with the Neural 3D Mesh Renderer, to make the loss function differentiable. The final mesh is rendered over real images to test the method. The generated images were also tested on humans, which were not 'fooled' by the adversarial meshes, showing that the perturbations are indeed small.\n\nI found the paper difficult to read, and it is difficult to accept this paper for ICLR because of this lack of clarity. (See for example the sentence from the abstract: \"So far...\")\n\nBecause of this lack of clarity, I am not sure I understand completely the paper. Maybe I am wrong, but it seems that the mesh is perturbed to fool the classifier and the detector from only one viewpoint. If this is true, is it really interesting?  What do we learn from it? We already know that existing networks can make mistakes on adversarial images. Isn't the proposed method a complex way to generate new images?  It would probably be more interesting if the generated mesh was fooling the network independently of the viewpoint, however it seems that the viewpoint is constant.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}