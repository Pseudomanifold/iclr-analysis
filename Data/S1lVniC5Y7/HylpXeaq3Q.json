{"title": "Interesting but not enough ", "review": "This paper explores evolutionary optimization for LSTM architecture search. To better explore the search space, authors used tree-based encoding and Genetic Programing (GP) with homologous crossover, tree distance metric, etc.  The search process is pretty simple and fast. However, there is a lack of experiments and analysis to show the effectiveness of the search algorithm and of the architecture founded by the approach. \n\nRemarks:\nThe contents provided in this paper is not enough to be convinced that this is a better approach for RNN architecture search and for sequence modeling tasks. \nThis paper requires more comparisons and analysis.\n\nExperiments on Penn Tree Bank\n - The dataset on both experiments are pretty small to know the effect of the new architecture they found. More experiments on larger datasets e.g., wikitext-2 will be needed. \n - In the paper \"On the state of the art of evaluation in neural language models\", Melis et al., 2018 reported improvement using classic LSTM over other variations of LSTM. They intensively compared the performance of classic LSTM, NAS, and RHN (Recurrent Highway Network) as authors did. Melis et al. reported LSTM (with depth 1) can already achieve a test perplexity of 59.6 with 10M parameters and 59.5 with 24M parameters.\n- Could you analyze a new finding of the LSTM architecture compared to the classic LSTM and NAS? Figure 5 and 6 are not very clear how are their final architectures different and the important/useful nodes changes for different tasks?\n- Recently, there are a number of architecture search algorithms introduced, but there is only one comparison in this direction (Zoph&Le16). It is important to compare this approach with other architecture search methods.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}