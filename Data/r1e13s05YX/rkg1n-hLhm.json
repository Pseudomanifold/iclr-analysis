{"title": "A good idea with proper validation", "review": "The paper proposes a method to solve end-to-end learning tasks using a combination of deep networks and domain specific black-box functions. In many machine learning tasks there may be a sub-part of the task can be easily solved with a black-box function (e.g a hard coded logic).  The paper proposes to use this knowledge in order to design a deep net that mimics the black-box function. This deep net being differentiable can be utilized while training in order to perform back-propagation for the deep nets that are employed to solve the remaining parts of the task. \n\nThe paper is well written and in my opinion the experiments are solid. They show significant gains over well-designed baselines. (It should be noted that I am not super familiar with prior work in this area and may not be aware of some related baselines that can be compared with.)\n\nIn Section 3.1.2 the authors discuss offline and online methods to train the mimicking deep network of a black-box function. The offline version suffers from wasting samples on unwanted regions while the online version will have a cold-start problem. However, I believe there can be better solution than the hybrid strategy. In fact there is a clear explore/exploit trade-off  here. Therefore, one may start with a prior over the input domain of the black-box function and then as the argument extractor learns well the posterior can be updated. Then we can Thompson sample the inputs from this posterior in order to train the mimicking network.  I think such a bandit inspired approach will be interesting to try out. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}