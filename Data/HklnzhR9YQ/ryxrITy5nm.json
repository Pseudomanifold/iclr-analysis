{"title": "Interesting approximation and estimation results, but considers somewhat unrealistic CNNs", "review": "The paper studies approximation and estimation properties of CNNs with residual blocks in the context\nof non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),\nand leveraging previous approximation results for such functions.\nExplicit risk bounds are obtained for regression functions in Barron and Holder classes.\n\nThe main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs\ncontains a class of \"block-sparse\" fully-connected networks, with appropriate constraints on various size quantities.\nThis result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk\n(Theorem 2, which mostly follows Schmidt-Hieber (2017)),\nas well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.\n\nThe construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,\neven with a filter size that is arbitrarily fixed.\nHowever, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,\nsince they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,\nwhich is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).\nIn particular, CNNs are typically used when there is some relevant inductive bias such as equivariance\nto translations (and invariance with pooling operations) to take advantage of,\nso removing this inductive bias by approximating fully-connected architectures seems a bit twisted.\nThe approach of reducing the function class to be approximated would seem more relevant here,\nas in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of\nthe present paper can be useful in such a scenario as well.\n\nSeparately, the presentation of the paper could be significantly improved,\nfor instance by introducing relevant notions more clearly in the introduction and related work sections,\nand by providing more insight and discussion of the obtained results in the main paper.\n\nMore specific comments:\n- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs\n- Section 2: Explain what is \"s\" in the Barron class, or at least point to the relevant definition in the paper\n- Section 3.1:\n  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat\n  * why is the estimator \"regularized\"?\n- Definition 2: shouldn't it be D_m^(0) = D instead of 1?\n- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.\n- Section 4.2: M_1 is the Lipschitz constant of what function?\n- Section 5.1: \"M = 1\" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.\n- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}