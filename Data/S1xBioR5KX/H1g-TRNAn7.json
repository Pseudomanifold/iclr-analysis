{"title": "The authors designed a dynamic reparameterization method to apply model compression in deep neural architectures. They compared their proposed framework with three baseline methods in terms of test accuracy and sparsity.  The comparison to the existing works is lacking. ", "review": "Weaknesses:\n\n1-The authors claim that: \"Compared to other dynamic reparameterization methods that reallocate non-zero parameters during training, our approach broke free from a few key limitations and achieved much better performance at lower computational cost.\" => However, there is no quantitative experiments related to other dynamic reparameterization methods. There should be at least sparsity-accuracy comparison to claim achieving better performance. I expect authors compare their work at-least with with DEEP R, and NeST even if it is clear for them that they produce better results. \n2-The second and fourth contributions are inconsistent: In the second one, authors claimed that they are the first who designed the dynamic reparameterization method. In the fourth contribution, they claimed they outperformed existing dynamic sparse reparameterization.  Moreover, it seems DEEP R also is a  dynamic reparameterization method because DEEP R authors claimed: \"DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded.\"\n3- The authors claimed their proposed method has much lower computational costs, however, there is no running time or scalability comparison. \n\n\nSuggestions:\n1-Authors need to motivate the applications of their work. For instance, are they able to run their proposed method on mobile devices?\n2-For Figure 2 (c,d) you need to specify what each color is.\n3-In general, if you claim that your method is more accurate or more scalable you need to provide quantitative experiments. Claiming is not enough.\n4-It is better to define all parameters definition before you jump into the proposed section. Otherwise, it makes paper hard to follow.  For instance, you didn't define the R_l directly (It is just in the Algorithm 1).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}