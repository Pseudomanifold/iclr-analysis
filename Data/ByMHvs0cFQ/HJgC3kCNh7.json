{"title": "Simple but nice development of the framework based on mostly well-known algebra but lacks experimental validation", "review": "After the discussion with authors, I am happy to recommend acceptance.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n1.\tIn \u201cConsequently, for each input vector of size N, output vector of size M, dimensions are split into four parts: the first one equals to r, the second is xi, the third one equals to yj, and the last one to zk to compose a quaternion Q = r1 + xi + yj + zk\u201d, are you splitting dimension M or M\\times N? And if you split M \\times N (I believe that\u2019s what you are doing), in which order you are splitting (row major right?) Please explain.\n2.\tI did not understand why authors didn\u2019t go in the negative direction of the gradient in Eq. (10-11)?\n3.\tIn section 3.4, authors mentioned \u201cMoreover, an hyper-complex parameter cannot be simply initialized randomly and component-wise, due to the interactions between components.\u201d which I strongly agree. But in Eq. (7) and (9) why the update rules and activation function are applied component wise?\n4.\tI really like the elegance in the parameter initialization. Couple of minor things here: (1) It\u2019s better to mention in Eq. (16) why E(|W|) is 0 because of symmetry. (2) Reference should be 6.1 instead of 5.1.\n5.\tAnother reasonable baseline will be using a complex network like (https://openreview.net/forum?id=H1T2hmZAb) and use the first two terms in Eq. (19) for representation. This will also possibly justify the usefulness of using higher order partials. \n6.\tThe authors mentioned multiple times about the achieved state-of-the-art results without giving any citation. As a reader not well versed in the acoustic domain, it will be nice to see some references to cross-validate the claim made.\n\n\n\nGeneral Comments:\n1.\tI understand the necessity of defining RNN/ LSTM model in the space of quaternions. But unit quaternions can be identified with other spaces where convolution is defined recently, e.g., with S^3 (https://arxiv.org/abs/1809.06211). I can see that this paper is contemporary, but at least can authors comment on the applicability of this general method in their case? Given that in NIPS\u201918 the following paper talked about RNN model on non-Euclidean spaces (https://arxiv.org/pdf/1805.11204.pdf), one can extend these ideas to develop an RNN model in the space of quaternions. Authors should look into it rigorously as future directions? But at least please comments on the applicability.\n2.\tThe experimental results section is somewhat weak, the overall claim of using fewer parameters and achieving comparable results is only validated on TIMIT data. More experimentation is necessary. \n3.\tIn terms of technical novelty, though quaternion algebra is well-known, I like the parameter initialization algorithm. I can see the merit of this in ML/ vision community.  \n\nPros: \n1. Nice well grounded methodological development on well-known algebra. (simple but elegant, so that's good).\n2. Nicely written and all the maths check out (that's good).\n3. Experimental result on TIMIT dataset shows usefulness in terms of using fewer parameters (but still can achieve SOA results).\n\nCons:\n1. See my comments above. I expect the authors to rebut/ address the aforementioned comments. Overall though simple but nice (and necessary) development of RNN/ LSTM framework in the space of quaternions. \n2. Lacks extensive experimental validation.\n\nMy reason for my rating is mainly because of (1) lack of experimental validation. (2) being aware of the recent development of general RNN model on non-Euclidean spaces, I want some comments in this direction (see detailed comment and reference above).\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}