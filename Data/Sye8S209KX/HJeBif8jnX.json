{"title": "Slight gains from concatenating token-based and sentence-based representations", "review": "This paper proposes a sentence encoder that is the concatenation of three separately trained component representations:\n1.  An encoder adversarially trained in a multi-task setting (Section 3.1., trained with AllNLI and Quora)\n2.  GenSen from Subramanian et al, ICLR 2018 (encoder trained with multi-task learning with 5 tasks/datasets, one of which is AllNLI)\n3.  ELMo (average pooling of the word representations to get a sentence representation)\n\nThis combined representation is the best in the transfer learning setting for 5 out of the 15 tasks investigated (Tables 1 and 2).  \n\nThe portion of the representation learned in this paper (Sent2vec) appears to be mostly dominated by the existing GenSen.  In Table 1, GenSen is more accurate than Sent2vec for 7 out of the 8 investigated tasks.  Adding Sent2vec to the combination of GenSen+ELMo (comparing lines 4.2 and 4.3) changes the results by less than 1% absolute in 7 out of the 8 tasks.\n\nPage 7 mentions that \"we observe a significant improvement on 4 out of 8 tasks...\" -- how was significance determined here?\n\nIn Table 2, the line for GenSen+ELMo (the equivalent of line 4.2 in Table 1) is missing.  This would be good to include for completeness.  \n\nThe idea of multitask learning for sentence representations is not new to this paper (see for example GenSen, which also uses multitask learning, with more tasks and includes two out of the three source datasets used here already).  ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}