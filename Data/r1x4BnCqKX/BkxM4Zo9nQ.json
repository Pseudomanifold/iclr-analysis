{"title": "A quite interesting contribution that also brings more clearer interpretations on what is learned", "review": "Summary:\nThe paper presents a novel method for predicting organic chemical reactions, in particular, for learning (Robinson-Ingold's) ''arrow pushing\" mechanisms in an end-to-end manner. Organic molecules consist of covalent bonds (that's why we can model them as molecular graphs), and organic reactions are recombinations of these bonds. As seen in organic chemistry textbooks, traditional chemists would qualitatively understand organic reactions as an alternating series of electron movements by bond breaking (bond cleavage) and bond forming (bond formation). Though now quantum chemistry calculations can give accurate quantitative predictions, these qualitative understanding of organic reactions still also gives strong foundations to consider and develop organic reactions. The proposed method tries to learn these series of bond changes directly through differentiable architectures consisting of three graph neural networks: 1) the one for determining the initial atom where electron movements start, 2) the one for representing state transitions from  the previous bond change to the next, and 3) the one for determining when the electron movements end. Experimental evaluations illustrate the quantitative improvement in final product prediction against existing methods, as well as give chemical intuitions that the proposed method can detect a class of LEFs (linear electron flows).\n\nComment:\n- This study is a quite interesting contribution because many existing efforts focus on improving differentiable architecture design for graph transformation and test it using chemical reaction data without considering what is learned after all. In contrast, this paper gives the clear meaning to predict \"arrow pushing\" mechanism from chemical reaction data and also makes sure the limitation to LEFs that are heterolytic. Traditional graph rewrite systems or some recent methods directly borrowing ideas from NLP do not always give such clear interpretations even though it can somehow predict some outputs.\n\n- The presentation of the paper is clear and in very details, and also provides intuitive illustrative examples, and appendix details on data, implementations, and related knowledge. \n\n- The architecture is based on graph neural networks, and seem natural enough. Basically, I liked overall ideas and quite enjoyed them but several points also remained unclear though I'm not sure at all about chemical points of view.\n\n1) the state transition by eq (2)-(4) seems to assume 1-st order Markovian, but the electron flow can have longer dependence intuitively. Any hidden states are not considered and shared between these networks, but is this OK with the original chemical motivations to somehow model electron movements? The proposed masking heuristics to prevent stalling would be enough practically...? (LEF limitations might come from this or not...?)\n\n2) One thing that confuses me is the difference from approaches a couple of work described at the beginning of section 'Mechanism prediction (p.3)', i.e. Fooshee et al 2018; Kayala and Baldi, 2011, 2012; Kayala et al, 2011. I don't know much about these studies, but the paper describes as \"they require expert-curated training sets, for which organic chemists have to hand-code every electron pushing step\". But for \"Training\" (p.6) of the proposed method, it also describes \"this is evaluated by using a known electron path and intermediate products extracted from training data\". Does this also mean that the proposed method also needs a correct arrow pushing annotations for supervised learning?? Sounds a bit contradicting statements?\n\n3) Is it just for computational efficiency why we need to separate reactants and reagents? The reagent info M_e is only used for the network for \"starting location\", but it can affect any intermediate step of elementary transitions intuitively (to break through the highest energy barrier at some point of elementary transitions?). Don't we need to also pass M_e to other networks, in particular, the one for \"electron movement\"?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}