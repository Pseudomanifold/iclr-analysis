{"title": "An interesting proposal of flexible model-based planning ", "review": "I think the ideas proposed in this paper are interesting. The paper is quite clearly written and the authors have provided a thorough reviews of related works and stated how the current work is different. I think this work has some significance for model-based reinforcement learning, as it provided us with a new adaptive way to rollout the simulation. I see the the work as a nice extension/improvement of the I2A (Weber et. al. 2017) and the ATreeC/TreeQN work (Fraquhar et. al. 2017). As the authors pointed out, the L2P agent can adaptive rollout different trajectories by choosing to move back to the root (start state) or move one step backward in the tree (regret last planned action). This is different from ATreeC/TreeQN where the whole tree is expanded in BFS way, and from I2A where rollouts are linear for each possible actions at current state.\nI have a bit doubt about the experimental results though. The levels used to evaluate seems quite simple, and I wonder the baseline model-free agents are not properly tuned or are not trained long enough to be fair. I have a list of questions detailed below:\n\n1. The IA is trained with utility that is a measure of \"value of information\" provided to the OA. I think this is a cool idea. Though I think the readers could understand better the intuition better if the authors can expand the explanation further. any reference on the idea? Why it has the form of Q^ * D_KL, for example why not Q^ + D_KL? Has the authors try to only set the utility as Q^ or as D_KL only as controls?\n\n2. One key part of this model is that during IA's unroll, the agent will choose z* from (z^p, z^c, z^r} (previous, current, root states), and then choose an action to unroll from z*. I wonder if this can be even further extended. For example, one possibility is that the agent can have z* set to any z in the tree that has already expanded. Or, another possibility is that the agent can have z* set to any z along the path from current node to the root (i.e. regret k-steps).  Also, would it be possible to have a dynamic planning steps? These suggestions may be practically hard to work properly, but may worth discuss.\n\n3. \"Push is similar to Sokoban used by Weber et. al. (2017) with comparably difficulty\". I cannot quite be convinced by this statement. Any quantification or evidence to support this sentence? To me, Sokoban seems to be much harder, as the agent need to solve the whole level to get score and can get stuck if making a single bad decision, while the Push seems much more tolerable (a lot of boxes, the obstacles is softly defined.) So stating that L2P learn Push in an order of magnitude less steps in Push compared to I2A learn Sokoban seems a chicken to egg comparison to me.\n\n4. Is it possible to run I2A as a baseline in the two environment you tried?\n\n5. I don't quite understand why DQN-{Deep, Wide} perform badly in the Gridworld environment. Checking the learning curves, one can see they actually converged to lower score than when the models started (from close to -1 down to -1.3). Can the authors comment more on why this is the case? The authors mentioned 'the agents learn only to navigate around the map for 25-50 steps before an episode ends'. I could not digest this sentence and would hope to understand better. To me, this gridworld level is quite trivial, the agent decide which goal is closest to the agent, and then move forward to that one and then onto next goal sequentially. I would like to understand better why this is a good level to test model-based RL and why model-free RL should have a hard time.\n\n6. a few possible typos:\n(1) formula 5, 3rd equation, should it be:\n      z*_{tau+1} = z_tau + z'' (double prime instead of single prime)?\n\n(2) The last sentence of the paragraph after equation (5)\n     z_{tau+1}^r = z_{tau=0}  (tau+1 instead of tau) \n\n(3) the color indication in Figure 5 caption is wrong. (while the description is fine in the main text)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}