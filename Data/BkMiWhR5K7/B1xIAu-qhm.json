{"title": "A decent paper", "review": "UPDATE:\n\nI've read the revised version of this paper, I think the concernings have been clarified.\n\n-------\n\nThis paper proposes to employ the bandit optimization based approach for the generation of adversarial examples under the loss accessible black-box situation. The authors examine the feasibility of using the step and spatial dependence of the image gradients as the prior information for the estimation of true gradients. The experimental results show that the proposed method out-performs the Natural evolution strategies method with a large margin.\n\nAlthough I think this paper is a decent paper that deserves an acceptance, there are several concernings:\n\n1. Since the bound given in Theorem 1 is related to the square root of k/d, I wonder if the right-hand side could become \"vanishingly small\", in the case such as k=10000 and d=268203. I wish the authors could explain more about the significance of this Theorem, or provide numerical results (which could be hard).\n\n2. Indeed I am not sure if Section 2.4 is closely related to the main topic of this paper, these theoretical results seem to be not helpful in convincing the readers about the idea of gradient priors. Also, the length of the paper is one of the reasons for the rating.\n\n3. In the experimental results, what is the difference between one \"query\" and one \"iteration\"? It looks like in one iteration, the Algorithm 2 queries twice?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}