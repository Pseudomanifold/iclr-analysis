{"title": "With a combination of several similar techniques, text-to-video/image performance is improved", "review": "Summary:\nThis paper proposed conditional GAN models for text-to-video synthesis:\n(1) Text-feature-conditioned CNN filters is developed; (2) A moving-shape dataset is constructed; (3) Experimental results on video/image generation are shown with improved performance.\n\n\nOriginality: \n\n(0) The overall model can be considered as MoCoGAN [0], but with the text-conditioned information.  Note that [0] also has a conditional version, but based on labels.\n(1) Generating CNN filters conditioned on other feature is not new for video generation, for example, Dynamic Filter Networks in [1], where the CNN filters to produce next frame are generated.\n(2) The idea of creating moving shape datasets is explored and used in the previous papers [2]. This paper creates similar but a bit more comprehensive ones.\n\nThe above three points to the existing work should be made CLEARLY, to reflect the contributions of this submission. \n\n\nClarity: \nA. The current paper only reports the overall performance for a method with several \"new\" modifications: (a) TF, (b) ResNet-style architecture (c) Regularization. Please do some ablation study, and show which component helps solve the problem text-to-video.\n\nB. Following MoCoGAN [0], the latent feature consists of two parts: one controls the global video (content in [0]) and one controls the frame-wise dynamics (motion in [1]).  It is okay to follow [0]. But, did the authors verify the two parts of latent codes can really control the claimed generation aspects? Or, is it really necessary to have to the two separated parts for the problem? if the goal is just to have text-controllable video?\n\n\n\nSignificance: \nText-to-video is a very challenging task, it is encouraging to see the attempt on this problem. However, I hope each contribution to this important problem are concrete and solid. Therefore, it will much more convincing to study each part of \"contributions\" the more comprehensively (one significant contribution would be enough), rather than put several minor/existing techniques together.\n\n\nQuestions: \nA. The generated frames (Figure 5) for one video are so similar, it raise a questions: does this model really generate video (which captures the dynamics)? or it is just a model for the static image generation, with small perturbations? If the answer is the ground-truth frames (the employed training dataset) are very similar, I would suggest to change the dataset at the first place, the employed dataset is not a proper dataset for text-to-VIDEO generation. \n\nB. [3] is also a text-to-video paper, the comparison to it is missing. Why?\n\n\nReferences:\n[0] MoCoGAN: Decomposing motion and content for video generation, 2018\n[1] Dynamic Filter Networks, 2016\n[2] Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks, 2016\n[3] To create what you tell: Generating videos from captions, 2017", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}