{"title": "potentially interesting idea, but very confusing in current form", "review": "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).\n\nThere is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.\n\n- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.\n\n- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}