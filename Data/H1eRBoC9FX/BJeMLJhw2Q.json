{"title": "An interresting but rushed paper", "review": "*Summary:* The present paper proposes to use  Model Agnostic Meta Learning to (meta)-learn in an unsupervised fashion the reward function of a Markov decision process in the context of Reinforcement Learning (RL). The distribution of tasks corresponds to a distribution of reward functions which are created thanks to random discriminators or diversity driven exploration.\n\n*Clarity:* The goal is well stated but the presentation of the method is confusing.\n\nThere is a  constant switch between caligraphic and roman D. Could you homogenize the notations?\n\nCould you keep the same notation for the MDP (eg in the introduction and 3.5, the discount factor disappeared)\n\nIn the introduction the learning algorithm takes a MDP in mathcal{T} and return a policy. In the remaining of the paper mathcal{D} is used. Could you clarify? I guess this is because only the reward of the MDP is meta-learned, which is itself based on D_phi?\n\nyou choose r = log(Discriminator). Could you explain this choice? Is there alternative choices?\n\nIn subsection 3.4, why the p in the reward equation?\n\nAlgorithm 1 is not clear at all and needs to be rewritten:\n   - Could you specify the stopping criterion for MAML you used?\n   - Could you number the steps of the algorithm?\n\nConcerning the experiments:\n\nIn my opinion the picture of the dataset ant and cheeta is irrelevant and could be removed for more explainations of the method.\n\nIt would be very nice to have color-blind of black and white friendly graphs.\n\nIn the abstract, I don't think the word demonstrate should be used about the experimental result. As pointed out in the experimental section the experiment are here rather to give additional insight on why and when the proposed method works well.\n\nYour method learns faster than RL from scratch on the proposed dataset in terms of iteration. What about monitoring the reward in terms of time, including the meta-learning step. Is there any important constant overhead in you the proposed method? How does the meta training time impact the training time? Do you have examples of datasets where the inductive bias is not useful or worst than RL from scratch? If yes could you explain why the method is not as good as RL from scratch?\n\nThe presentation of the result is weird.\nWhy figure 4 does not include the ant dataset? Why no handcrafted misspecified on 2D navigation?\nFigure 3 and 4 could be merged since many curves are in common.\n\nHow have you tuned your hyperparameters of each methods? Could you put in appendix the exact protocol you used, specifying the how hyperparameters of the whole procedured are chosen, what stopping criterion are used, for the sake of reproducibility. A an internet link to a code repository used to produce the graphs would be very welcome in the final version if accepted.\n\nIn the conclusion, could you provide some of the questions raised?\n\n*Originality and Significance:* As I'm not an expert in the field, it is difficult for me to evaluate the interest of the RL comunity in this work. Yet to the best of my knowledge the work presented is original, but the lack of clarity and ability to reproduce the results might hinder the impact of the paper. \n\nTypos:\nEq (2) missing a full stop\nMissing capital at \u00b4\u00b4 update\u00b4\u00b4  in algorithm 1\nEnd of page 5, why the triple dots?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}