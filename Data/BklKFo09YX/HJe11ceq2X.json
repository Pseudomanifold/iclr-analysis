{"title": "Not good enough", "review": "A strand of papers has been proposed for optimizing molecules for a given property using different forms of variational autoencoders as a tool to extract embedding for molecules. This paper is also in this direction by applying Cycle-GAN [Zhu et al., 2017] in the latent space learnt from junction-tree VAE [Jin et.al, 2018], aiming to learn a translation function (in the latent space), from the set of molecules without the interested property to the set of molecules with the property.\n\nThe paper is well written and the contribution is more on the application side, although combining the above mentioned two gradients are indeed novel. That being said, I wish the authors can bring more perspectives from chemists or biologists which can justify and support the application setting.\n\nSince the focus is molecular optimization, I paid special attention to the results in Section 4.2. While the improvement regarding to the interested property (logP) is a pleasure to see, the drop of the success rate is also significant comparing to [Jin et.al, 2018], which undermines the significance of the results, especially given the increased complexity. Another issue is the requirement of a large set of molecules with desired property. This further restricts the applicability of the method. How to solve the cold start issue will be critical in this setting and this is not mentioned in the paper. Thirdly, combining two existing components is ok but not enough novelty from my point of view. Considering the high acceptance bar of ICLR, I will not accept this paper.\n\nDetailed comments:\n1. In Section 4.2, how similar the generated molecules are to the ones already in the Y_train? \n2. In Section 4.2, G(X) map to Y, what does it mean to apply G(G(X))? Do you decode G(X) to a molecule first and then feed into the encoder to apply G (as in Section 4.3)? If not, G is not supposed to learn the transition from Y to X. Will you always get exactly the same X when apply F(G(X))? Can the sequence be G(X), G(F(G(X))), \u2026? \n3. In Section 4.3, is it always that later step gives better logP? It seems so from Figure 6 for 1-30 iterations, how about later 30-80 iterations? If so, for the generated molecules, the method seems have a tension between the similarity to the original molecules and the level of the desired property. Can you comment on how important, in practice, the similarity matters?\n4. Following 3, \\lambda_2 seems directly affect the balance of the tension and it should be studied in more detail.\n5. How about reproducibility, will the data and code published?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}