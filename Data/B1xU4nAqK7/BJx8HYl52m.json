{"title": "An incremental work and needs more justification/clarification", "review": "The authors built upon the PETS algorithm to develop a state uncertainty-driven exploration strategy, for which the main point is to construct a reward function. The proposed algorithm was then tested on a specific domain to show some improvement. \n\nThe contribution of this paper may be limited, as it needs a specific setting, as shown in Figure 1. Furthermore, this paper is a bit difficult to follow, e.g., it was not until the 5th page to describe their algorithm. I summarize the pros and cons as follows.\n\nPros:\n- The idea to include the exploration for PETS is somewhat interesting.\nCons:\n- The paper is a bit difficult to follow. Just to list a few places:\n  1. The term \"unsupervised exploration\" was mentioned a few times in this paper. I am not sure if this is an accurate term. Is there a corresponding \"supervised exploration\" used elsewhere? \n  2. When you introduced r_t in Section 3.3, how did you use it next? Was it used in Phase II?\n  3. For the PETS (oracle) in Figure 4, why are the settings different for forward and backward tasks?\n  4. What does \"random\" mean in Figure 4?\n- The novelty of this paper is somewhat limited, as it requires a specific setting and has been applied in only one domain.\n- There are a few grammar mistakes/typos in this paper. \n  1. What is \"k\" in the equation for r_t?\n  2.  \"...we three methods...\" in Page 6.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}