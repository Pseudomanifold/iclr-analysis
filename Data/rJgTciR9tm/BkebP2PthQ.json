{"title": "Preliminary, with 1 promising experiment, but unclear and vague", "review": "The paper proposes a method to learn the conditional distribution of a random variable in order to minimize and maximize certain mutual information terms.  Interestingly, the proposed method can be applied to sentiment prediction and outperforms a 2018 method based on SVM.\n\nOverall, the ideas seem intriguing, and the results seem promising, but I really cannot understand what the paper is saying, and I think the paper would be much stronger if it was written more clearly (to make individual sentences more clear, but also to make the broader picture more clear). Not only is the writing hard to understand (some sentences lack a verb!), but it is vague, and the notion of a \"complex system\" is never defined.  It seems that the technique can be applied to any (potentially non-stationary) Markov process?\n\nAdditionally, due to the lack of clarity in the writing and lack of mathematical rigor, Theorem 1 does not seem to be true as stated. I think this is an issue of stating the assumptions, and not due to a mistake in the derivation.  Right now, the actual conclusion of theorem 1 is not even clear to me.\n\nQuality: poor/unclear\nClarity: very poor\nOriginality: unclear, perhaps high? Not clear how related it is to the methods of Tishby et al.\nSignificance: unclear, as clarity was poor, and there was minimal discussion of alternative methods.\n\nSpecific points:\n\n- Eq (2), the first term is included because it is for the \"information compression task\", but I do not understand that. Where is the actual compression?  This is not traditional compression (turning a large vector into a smaller vector), but more like turning one PDF into a PDF with lower entropy?\n\n- This paper seems to fall into the subfield of system identification (at which I am not an expert), so I'd expect to see some related literature in the field. The only compared method was the IF method of Tishby et al. from 18 years ago (and the current work seems to be a generalization of that).\n\n- Equation (4): what exactly is the object being minimized? Is it a PDF/probability measure? Is it an *instance* of a random variable?  If it is a PDF, is it the PDF of B_k | X_{k-1} ?\n\n- The statement of Theorem 1 is either too vague or wrong. To say \"The solution... is given by\" makes it sound like you are giving equations that define a unique solution. Perhaps you mean, \"Any solution ... must necessarily satisfy...\" ? And that is not clearly true without more work. You are basically saying that any minimizer must be a stationary point of the objective (since you are not assuming convexity). It seems everything is differentiable?  How do you know solutions even exist -- what if it is unbounded? In that case, these are not necessary conditions.\n\n- Lemma 1: \"The iterative procedure... is convergent.\"  The iterative procedure was never defined, so I don't even know what to make of this.\n\n- Section 3.2: \"As proved by prior work, the optimum solution obtained by a stochastic transformation that is jointly Gaussian with bottleneck's input.\"  I do not know what you are trying to say here. There's no predicate.\n\n- Section 4 wasn't that interesting to me yet, since it was abstract and it seemed possible that you make a model to fit your framework well. But section 5 is much better, since you apply it to a real problem. However, what you are actually solving in section 5 is unclear. The entire setup is poorly described, so I am very confused.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}