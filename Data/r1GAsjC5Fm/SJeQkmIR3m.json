{"title": "Interesting Approach to Route Instruction Following with Thorough Evaluation", "review": "The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed (\"self-aware\") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a \"progress monitor\" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction.\n\n\nSTRENGTHS\n\n+ The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a \"progress monitor\" allows the method to reason over whether the navigational progress matches the instruction.\n\n+ The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture.\n\n+ The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results.\n\n\nWEAKNESSES\n\n- The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next.\n\n- The paper emphasizes the use of images, the visual grounding reasons over visual features.\n\n- The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal.\n\nC. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, \u201cLearning to parse natural language commands to a robot control system,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012.\n\nS. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, \u201cLearning models for following natural language directions in unknown environments,\u201d in Proc. IEEE Int\u2019l Conf. on Robotics and Automation (ICRA), 2015\n\nF. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, \u201cInferring maps and behaviors\nfrom natural language instructions,\u201d in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014.\n\n- While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)\u2060\n\nJ. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. \"Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots,\" In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017\n\n- The paper misses the large body of literature on grounded language acquisition for robotics.\n\nQUESTIONS\n\n* What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016?\n\n* Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)?\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}