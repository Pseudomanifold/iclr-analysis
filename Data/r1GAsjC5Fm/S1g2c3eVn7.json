{"title": "Review", "review": "This paper describes a model for vision-and-language navigation. The proposed\nmodel adds two components to the baseline model proposed by Fried et al. (2018):\n\n- a panoramic visual attention (referred to in this paper as \"visual--textual\n  co-grounding\"), in which the full scene around the agent's current position is\n  attended to prior to selecting a direction to follow\n\n- an auxiliary \"progress monitoring\" loss which encourages the agent to to\n  produce textual attentions from which the distance to the goal can be directly\n  inferred\n\nThe two components combine to give state-of-the-art results on the Room2Room\ndataset: small improvements over existing approaches on the \"-seen\" evaluation\nset and larger improvements on the \"-unseen\" evaluation sets. These improvements\nalso stack with the data-augmentation approach of Fried et al.\n\nI think this is a reasonable submission and should probably be accepted. However, I\nhave some concerns about presentation and a number of specific questions about\nmodel implementation and evaluation.\n\nPRESENTATION AND NAMING\n\nFirst off: I implore the authors to find some descriptor other than \"self-aware\"\nfor the proposed model. \"Self-aware\" is an imprecise description of the agent in\nthis paper---the agent is specifically \"aware\" of its visual surroundings and\nits distance from the goal, neither of which is meaningfully an aspect of\n\"self\". Moreover, self-awareness means something quite different in adjacent\nareas of cognitive science and philosophy; overloading the term in the specific\n(and comparatively mundane) way used here creates confusion. See section 3.4 of\nhttps://arxiv.org/abs/1807.03341 for broader discussion. Perhaps something\nlike \"visual / temporal context-sensitivity\" to describe what's new here? A bit\nclunky, but I think it makes the contributions of this work much clearer.\n\nAs suggested in the summary above, I also think \"visual--textual co-attention\"\nis also an unhelpfully vague description of this aspect of the contribution. The\ntextual attention mechanism used in this paper is the same as in all previous\nwork on the task. Representations of language don't even interact with the\nvisual attention mechanism except by way of the hidden state, and the salient\nnew feature of the visual attention is the fact that it considers the full\npanoramic context before choosing a direction.\n\nMODELING QUESTIONS\n\n- p4: $y_t^{pm}$ is defined as the \"normalized distance from the current\n  viewpoint to the goal\". Is this distance in units of length (as defined by the\n  simulator) or units of time (i.e. the number of discrete \"steps\" needed to\n  reach the goal)?\n\n  The authors have already clarified on OpenReview that the progress monitor\n  objective uses an MSE loss rather than a likelihood loss. Do I understand\n  correctly that ground-truth distances are in [0, 1] but model predictions are\n  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search\n  candidates as $p_t^{pm} \\times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?\n\n- The input to the progress monitor is formed by concatenating the attention\n  vector $\\alpha_t$ to a vector of state features, and then multiplying by a\n  fixed weight matrix. How is this possible? The size of $\\alpha_t$ varies\n  depending on the length of the instruction sequence. Are attentions padded out\n  to the length of the longest instruction in the training set? If so, how can\n  the model learn when it's reached the end of a short instruction sequence?\n  What would happen if the agent encountered a sequence that was too long?\n\nEVALUATION QUESTIONS\n\n- The progress monitor is used both as an auxiliary training objective and as a\n  beam search heuristic. Is it possible to disentangle these two contributions?\n  (E.g. by ignoring the scores during beam search, or by doing augmented beam\n  search in a model that was trained without the auxiliary objective.)\n\n- Not critical, but it would be nice to know if the contributions here stack\n  with the pragmatic inference procedure in Fried et al.\n\n- While, as pointed out on OpenReview, it is not required to include SPL\n  evaluations, I think it would be informative to do so---the preliminary\n  results with no beam search look good!\n\nMISCELLANEOUS\n\np1: \"without a map\" If you can do beam search, you effectively have a map.\n\np1: \"...smoothly\" What does \"smoothly\" mean in this context?\n\np2: \"the position of grounded instruction can follow past and future\n    instructions\". Is the claim here that if instructions are of the form \"ACB\"\n    and the agent is supposed to do \"ABC\", that the proposed model will execute\n    these instructions successfully and the baseline will not? This claim does\n    not appear to be evaluated anywhere in the body of the paper.\n\np4: \"intelligently prunes\" \"Intelligently\" is unnecessary.\n\np4: \"for empirical reasons\" What does this mean?\n\np5: \"Intuitively, an instruction-following agent is required...\" The existence\n    of non-attentive models that do reasonably well at these\n    instruction-following tasks suggest that this is not actually a requirement.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}