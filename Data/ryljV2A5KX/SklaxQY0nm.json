{"title": "Nice idea.  Experiments lacking.", "review": "Please have your submission proof-read for English style and grammar issues. \n\nThis paper introduces the IB-GAN and information bottleneck inspired GAN variant.  The ordinary GAN objective is modified to include a variational lower and upper bound on the generative mutual information.  This should allow one to control the amount of information in the representation of the GAN, in contrast to the InfoGAN which simply maximizes the mutual information.  While lower bounding the generative mutual information is straight forward and only requires a variational inverting network (some q(z|x)) upper bounding the generative mutual information is trickier.  Here the paper offers a very nice solution.  Formally they realize a modified Markov chain   Z -> R -> X where R is made explicitly stochastic.  By Data Processing Inequality I(Z;X) <= I(Z;R) and with a tractable e(r|z), only a variational marginal m(r) is needed to obtain a variational upper bound on the mutual information in the GAN.  This then gives a GAN objective that looks like the information bottleneck interpretation of the VAE.\n\nWhile the idea for obtaining a variational upper bound on the generative mutual information is novel and clever, the experiments in the paper are lacking.\n\nIt should be noted that the variational lower bound on the generative mutual information has already been introduced as the GILBO (generative information lower bound) (arxiv:1802.04874) \n\nI take issue with the discussion in the \"Reconstruction of input noise z\" section.  It is claimed that beta-VAE \"applies the MSE loss to x and uses beta > 1\".  VAEs do not have to utilize gaussian observation models and can use powerful autoregressive decoders (e.g. arxiv:1611.02731).  \n\nLater down the page it is claimed that when m(r) and p(z) have the same distributional form and dimensionality the R will become independent of Z.  I do not believe this.  What prevents e(r|z) from being a near identity in this situation, for which there could be a large generative mutual information?\n\nThe experiments used batch normalization, itself a stochastic procedure that would make their tractable densities incorrect.  There is no discussion of the effect batch norm would have on their bounds.\n\nMy principal complaint is the general lack of experimental evidence.  The paper suggests what appears to be a nice framework and simple procedure for controlling the information flow in a GAN.  To do so they introduce two Lagrange multipliers, beta and lambda in their notation (Equation 11) but there are no experiments showing the effect of these two hyperparameters.  They have what should be both an upper and lower bound on the same quantity, the generative mutual information, but these are not shown separately for any of their experiments.  There is no discussion of how tight the bounds are and if they approach each other.  There is no discussion of how the beta and lambda might influence them either individually or jointly.  There is no evidence to demonstrate the effect of constraining the mutual information between X and Z.\n\nIn short, the paper offers what appears to be a very clever idea, but does very little to experimentally explore its effects.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}