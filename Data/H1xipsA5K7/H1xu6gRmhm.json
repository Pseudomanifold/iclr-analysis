{"title": "A Strong Theory Paper", "review": "This is a strong theory paper and I recommend to accept.\n\nPaper Summary:\nThis paper studies the problem of learning a two-layer fully connected neural network where both the output layer and the first layer are unknown. In contrast to previous papers in this line which require the input distribution being standard Gaussian, this paper only requires the input distribution is symmetric. This paper proposes an algorithm which only uses polynomial samples and runs in polynomial time. \nThe algorithm proposed in this paper is based on the method-of-moments framework and several new techniques that are specially designed to exploit this two-layer architecture and the symmetric input assumption.\nThis paper also presents experiments to illustrate the effectiveness of the proposed approach (though in experiments, the algorithm is slightly modified).\n\nNovelty:\n1. This paper extends the key observation by Goel et al. 2018 to higher orders (Lemma 6). I believe this is an important generalization as it is very useful in studying multi-neuron neural networks.\n2. This paper proposes the notation, distinguishing matrix, which is a natural concept to study multi-neuron neural networks in the population level.\n3. The \u201cPure Neuron Detector\u201d procedure is very interesting, as it reduces the problem of learning a group of weights to a much easier problem, learning a single weight vector. \n\nClarity:\nThis paper is well written.\n\nMajor comments:\nMy major concern is on the requirement of the output dimension. In the main text, this paper assumes the output dimension is the same as the number of neurons and in the appendix, the authors show this condition can be relaxed to the output dimension being larger than the number of neurons. This is a strong assumption, as in practice, the output dimension is usually 1 for many regression problems or the number of classes for classification problems. \nFurthermore, this assumption is actually crucial for the algorithm proposed in this paper. If the output dimension is small, then the \u201cPure Neuron Detection\u201d step does work. Please clarify if I understand incorrectly. If this is indeed the case, I suggest discussing this strong assumption in the main text and listing the problem of relaxing it as an open problem. \n\n\nMinor comments:\n1. I suggest adding the following papers to the related work section in the final version:\nhttps://arxiv.org/abs/1805.06523\nhttps://arxiv.org/abs/1810.02054\nhttps://arxiv.org/abs/1810.04133\nhttps://arxiv.org/abs/1712.00779\nThese paper are relatively new but very relevant. \n\n2. There are many typos in the references. For example, \u201crelu\u201d should be ReLU.\n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}