{"title": "Paper lacks many important details.", "review": "The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. The idea is novel (to the best of my knowledge), interesting and the experiments seem promising. The two main flaws in the paper are the lack of details and missing important experimental comparisons.\n\nMajor remarks:\n\n- The author state they add experimental details and videos via a link to a website. I think doing so is very problematic, as the website can be changed after the deadline but there was no real information on the website so it wasn\u2019t a problem this time.\n\n- While the idea seems very interesting, it is only presented in very high-level. I am very skeptical someone will be able to reproduce these results based only on the given details. For example - in eq.1 what is the distribution over s? How is the distance approximated? How is the goal-conditional policy trained? How many clusters and what clustering algorithm?\n\n- Main missing details is about how the goal reaching policy is trained. The authors admit that having one is \u201ca significant assumption\u201d and state that they will discuss why it is reasonable assumption but I didn\u2019t find any such discussion  (only a sentence in 6.4).  \n\n- While the algorithm compare to a variety of representation learning alternatives, it seems like the more natural comparison are model-based Rl algorithms, e.g. \u201cNeural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning\u201d. This is because the representation tries to implicitly learn the dynamics so it should be compared to models who explicitly learn the dynamics. \n\n- As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account.\n\n- I found Fig.6 very interesting and useful, very nice visual help.\n\n- In fig.8 your algorithm seems to flatline while the state keeps rising. It is not clear if the end results is the same, meaning you just learn faster, or does the state reach a better final policy. Should run and show on a longer horizon.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}