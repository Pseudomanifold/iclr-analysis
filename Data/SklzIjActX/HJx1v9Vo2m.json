{"title": "More experimental results should be provided", "review": "This paper try to speedup CNN inference with 8-bit quantization. It is practically useful and may be a nice reference for other developers. But the ideas in this paper are trivial and the motivation is not so convincing. \n\n1) For depthwise convolution in Figure 1(b), the dynamic range differences can be eliminated by batch normalization folding. I doubt the necessity of channel-wise scaling. Besides, the author didn't provide any comparison results of single scaling factor and channel-wise scaling factors for depthwise convolution and traditional convolution layers.\n2) For Winograd convolution,  the authors proposed to use scale factors after transformation. They should explain more about this method.\n3)Pooling and concatenation support is quite easy. Batch normalization folding is a common practice.\n4) The author should explain in more detail on fusing convolution and element-wise operations. Sorry I can't get their point. \n5) Calibration results should be provided. The author should also tell readers what hardware is used to evaluate the throughput and latency.\n6) Detail results of winograd should be given.\n\nOverall, this paper is in-complete and the authors should add more experimental results and improve their description.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}