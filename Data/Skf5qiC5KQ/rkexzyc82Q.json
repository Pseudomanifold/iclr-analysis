{"title": "Strong experimental results but the theory is not that strong", "review": "The paper analyses the gradients some deep metric learning (DML) approaches. Most of them optimize over pair-wise or triplet-wise constraints.\nThe main claim of the paper is that, during training, most DML approaches ignore some informative pairs of examples or do not take into account the value of the distance gap between similar and dissimilar pairs.\nA specific weighting strategy is then proposed to address those problems.\n\nThe main strength of the paper are the strong experimental results on different transfer learning benchmarks. The proposed approach seems to significantly outperform existing methods thanks to the proposed strategy. \nHowever, the theoretical aspect is weak.\n\nI have some concerns about the paper.\n\n- My first concern is about terminology: I disagree with the claims of the paper that there is any theoretical demonstration in the paper. \nI do not see the novelty of the gradient equivalence theorem (Theorem 3.1). It was already explained in (Law et al., ICCV 2013) (that is cited in the paper) that pairwise and triplet-wise constraints are just specific formulations of quadruplet-wise constraints. Pair-wise and triplet-wise can then be written in their quadruplet-wise formulation: we then obtain the induced gradient formulation that depends on positive and negative pairs (as formulated in Eq. (4)). The derivation of the proof is very straightforward once we have a formulation that generalizes both triplet-wise and pairwise constraints.\n\nMoreover, the gradient equivalence definition (in Definition 3.1) is not applicable to most deep learning optimization frameworks that use momentum-based optimizers (e.g. Adam which is a momentum solver, and SGD which is often optimized with momentum). Indeed, definition 3.1 only considers the value of theta at some given iteration, but not at the previous iterations. However, momentum-based approaches keep a history of the gradients from previous iterations and will then return different gradients.\n\n- One of the main claims of the paper against the triplet loss is that the sampling strategy ignores some informative pairs. This is mainly due to the fact that the triplet loss is generally used in the context of very large datasets and large mini-batches (Schroff et al., CVPR 2015) where it is computationally expensive to generate all the possible triplet constraints. Triplet sampling strategies are formulated to ensure fast convergence while avoiding degenerate solutions induced by the chosen triplet sampling strategy.\n\nThe submitted approach does not deal with very large datasets and then does not need to consider such sampling strategies. Although some sampling strategy is proposed, it is unclear if it would be scalable (i.e. trainable in reasonable time on the same dataset as FaceNet).\nMoreover, as mentioned in Section 5.1, the proposed strategy is a straightforward extension of lifted structure.\n\n- Why did you only report the recall@K evaluation when many methods report the normalized mutual information (NMI)? (Hyun Oh Song et al., CVPR 2017)\nCould you please include NMI scores?\n\n- If the problem of the contrastive loss is the fact that the gradient considers all pairs equally, why can it not be adapted to depend on the hardness of the constraint (e.g. by taking the squared of the loss for each pair)?\n\nIn conclusion, my opinion is borderline but only leans towards acceptance because the experimental results are strong.\nNonetheless, the reported results of baselines are often for different network architectures and using different output dimensions. Can the authors try their method with the same architecture and same output dimensionality as baselines?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}