{"title": "A timely and useful resource", "review": "This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.\n\nSome comments below. \n\n- The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. \n\n- Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.\n\n- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. \n\n- I would consider pushing Table 5 (Appendix) into the main text. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}