{"title": "The method needs more insight or novelty, and the results have room to improve", "review": "This manuscript extends the direct feedback alignment (DFA) approach to convolutional neural networks (CNN) by (1) only applying DFA to FC layers with backpropagation (BP) in place for convolutional layers (2) using binary numbers for feedback matrix.\n\nOriginality wise, I think (1) is a very straightforward extension to the original DFA approach by just applying DFA to places where it works. It still does not solve the ineffectiveness of DFA on convolutional layers. And there is no much insight obtained. (2) is interesting in that a binary matrix is sufficient to get good performance empirically. This would indeed save memory bandwidth and storage. This falls into the category of quantization or binarization, which is not super novel in the area of model compression. \n\nThe experimental results show that the proposed approach is better than BP based on accuracy. However, these results might be called into question because the shown accuracies on CIFAR10 and CIFAR100 are not state-of-the-art results. For example, the top 1 accuracy of CIFAR10 in this paper 81.11%. But with proper tuning, a CNN should be able to get more than 90% accuracy. See this page for more details.\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\nTherefore, though the claimed accuracy of the proposed method is 89%, it is still not the state-of-the-art result and it seems to be lack of tuning for the BP approach to perform similar level of accuracy. The same conclusion applies to CIFAR100. In fact, from figure 4, the training accuracy gets 100% while the testing accuracy is around 40% for BP, which seems to be overfitting. With these results, it is hard to judge the significance of the manuscript.\n\nMinor typos:\nIn Equation 1, the letter i is overloaded.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}