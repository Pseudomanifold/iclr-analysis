{"title": "An interesting paper too condensed and difficult to understand", "review": "Review:\n\n\tThis paper proposes a method to learn a neural network to perform optimization. The idea is that the neural network will receive as an input several parameters, including the weights of the network to be trained, the gradient, and so on, and will output new updated weights. The neural network that is used to compute new weights can be trained through a complicated process called un-rolled optimization. The authors of the paper show two problems with this approach. Namely, the gradients tend to explode as the number of iterations increases. Truncating the gradient computation introduces some bias. To solve these problems the authors propose a variational objective that smooths the objective surface. The proposed method is evaluated on the image net dataset showing better results than first order methods optimally optimized.\n\nQuality: \n\n\tThe quality of the paper is high. It addresses an important problem of the community and it seems to give better results than first other methods.\n\nClarity: \n\n\tThe clarity of the paper is low. It is difficult  to follow and includes many abstract concepts that the reader is not familiar with. I have had problems understanding what the truncation means. Furthermore, it is not clear at all how the validation data is used as a target in the outer-objective.  It is also unclear how the bias problem is addressed by the method proposed by the authors. They have said nothing about that, yet in the abstract they say that the proposed method alleviates the two problems detected.\n\nOriginality: \n\n\tAs far as I know the idea proposed is original and very useful to alleviate, at least, one of the problems mentioned of exploding gradients.\n\nSignificance:\n\n\tIt is not clear at all that the method is evaluated on unseen data when using the validation data for outer-training. This may question the significance of the results.\n\nPros:\n\n\t- Interesting idea.\n\n\t- Nice illustrative figures.\n\n\t- Good results.\n\nCons:\n\n\t- Unclear points in the paper with respect to what truncation means.\n\n\t- The validation data is used for training and there is no left-out data, which may bias the results.\n\n\t- Unclear how the authors address the bias problem in the gradients.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}