{"title": "Need to clarify some procedures", "review": "This paper try to formulate many-class-few-shot classification problem from 2 perspectives: supervised learning and meta-learning. Although solving this problem with class hierarchy is trivial,  combing MLP and KNN in these two ways seems interesting to me. I still have several questions:\n\n1) How the class hierarchy is got , manually set or automatically generate?  Whether the ideas still work if some coarse classes share same fine class?\n2) The so-called attention module is just classic KNN operations, please don't naming it attention just because the concept \"attention\" is hot.\n3) Why different \"attention\" operations are used for supervised learning and meta-learning?\n4) How to get the pre-trained models for supervised learning?\n5) What will happen if alternatively apply supervised learning and meta-learning?\n6) In Table 4, why MahiNet(Mem-2) w/o Attention and Hierarchy performs better than the one w/o attention?\n7) The authors just compare storing the average features and all features, I think results of different prototype number should be given, since one of their claim to apply KNN is to maintain a small memory.\n\n ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}