{"title": "Interesting read but unclear contribution/implications", "review": "This paper addresses the challenges of prediction-based, progressive planning on discrete state and action spaces. Their proposed method applies existing DAG-LSTM/Tree-LSTM architectures to iteratively refine local sections in the existing plan that could be improved until convergence. These models are then evaluated on a simulated job scheduling dataset and Halide expression simplification.\n\nWhile this paper presents an interesting approach to the above two problems, its presentation and overall contribution was pretty unclear to me. A few points:\n\n1. Ambiguous model setup: It may have been more advantageous to cut a large portion of Section 3 (Problem Setup), where the authors provide an extensive definition of an optimization problem, in favor of providing more critical details about the model setup. For example, how exactly should we view the job scheduling problem from an RL perspective? How are the state transitions characterized, how is the network actually trained (REINFORCE? something else?), is it episodic (if so, what constitutes an episode?), what is the exploration strategy, etc. It was hard for me to contextualize what exactly was going on\n\n2. Weak experimental section: The authors mention that they compare their Neural Rewriter against DeepRM using a simplified problem setup from the original baseline. I wonder how their method would have fared against a task that was comparable in difficulty to the original method -- this doesn\u2019t feel like a fair comparison. And although their expression simplification results were nice, I would also like to know why the authors chose to evaluate their method on the Halide repository specifically. Since they do not compare their method against any other baselines, it\u2019s hard for me to gauge the significance of their results.\n\n3. Variance across initializations: It would have been nice to see an experiment on how various initializations of schedules/expressions affect the policies learned. I would imagine that poor initializations could lead to poor results, but it would be interesting if the Neural Rewriter was robust to the quality of the initial policy. Since this is not addressed in the paper, it is difficult to gauge whether the authors\u2019 model performed well due to an unfair advantage. Additionally, how much computational overhead is there to providing these (reasonable) initial policies as opposed to learning from scratch?\n\n4. Unclear notation: As previously addressed by other reviewers, key definitions such as the predicted score SP(.) are missing from the text.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}