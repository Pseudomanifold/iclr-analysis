{"title": "Interesting approach to improve exploration in RL with unfair advantage", "review": "Summary:\nThe authors look at the problem of exploration in deep RL. They propose a \u201ccuriosity grid\u201d which is a virtual grid laid out on top of the current level/area that an Atari agent is in. Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game. The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).\n\nThe authors argue that this method enables better exploration and they obtain an impressive score on Montezuma\u2019s Revenge (MR). \n\nReview:\nThe paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm. The algorithm itself seems to work well and some of the results are convincing. I am a bit worried about the fact that the agents have access to their history of locations (\u201cthe grid\u201d). The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.\n\nThe authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly. Only removing the grid access made results on MR very unstable. However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there. \n\nI was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.\n\nThe future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage. \n\nNits/writing feedback:\n- There is no need for such repetitive citing (esp paragraph 2 on page 2). Sometimes the same paper is cited 4 times within a few lines. While it\u2019s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.\n- I think the comparison between prior lifetimes and humans mastering a language doesn\u2019t hold up and is distracting\n\n####\nRevision:\n\nThe rebuttal does little to clarify open questions:\n1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.\n2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.\n3. The authors argue in their rebuttal that \"the grid\" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm. This seems contradictory.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}