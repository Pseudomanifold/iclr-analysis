{"title": "An interesting paper, but a few questions needed to be answered", "review": "This paper investigates the effects of mean of variational posterior and proposes variance layer, which only uses variance to store information.\n\nOverally, this paper analyzes an important but not well explored topic of variational dropout methods\u2014the mean propagation at test time, and discusses the effect of weight variance in building a variational posterior for Bayesian neural networks. This findings are interesting and I appreciate the analysis. \n\nHowever, I think the claim for benefits of variance layer is not well supported. Variance layer requires test-time averaging in test time to achieve competitive accuracy, while the additive case in Eq. (14) using mean propagation achieves similar performance (e.g., the results in Table 1). The results in Sec 6 lack comparison to other Bayesian methods (e.g., the additive case in Eq. (14)). \n\nBesides, there exists several problems which needs to be addressed.\n\nSec 5.\nSec 5 is a little hard to follow. Which prior is chosen to produce the results in Table 1? KL(q||p)=0 for the zero-mean case corresponds to the fact that the variational posterior equals the prior, which implies the ARD prior if I did not misunderstand. In this case, the ground truth posterior p(w|D) for different methods is different and corresponding ELBO for them are incomparable.\n\nSec 6. \nThe setting in Table 2 is also unclear. As ``Variance\u2019\u2019 stands for variational dropout, what does ``Dropout\u2019\u2019 means? The original Bernoulli dropout? Besides, I\u2019m wondering why directly variance layer (i.e., zero-mean case in Eq. (14)) is not implemented in this case.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}