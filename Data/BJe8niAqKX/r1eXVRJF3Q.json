{"title": "Nice idea but I am not sure about the empirical evaluation.", "review": "This paper presents a method for improving text-based sentence embeddings through a joint multimodal framework. \n\nDifferent from typical multimodal work, the proposed model DOES NOT project text and vision derived representations into a single space. Instead the proposed model uses a joint loss function that allows text-derived representations and the vision-derived representations to influence each other, while remaining in their separate representation spaces. In particular the loss functions ensure the following:\n\nSentences that describe the same image should have similar text-derived representations. The similarity of text-derived representations of a pair of sentences should correlate with similarity of the representations of their corresponding images.\n\nThe paper then describes multiple experiments that test whether the vision-based loss functions lead to improvements over text-only representations and if this method of incorporation vision information is better than projection based methods for multimodal representations. \n\nStrengths:\n\n1. The problem is well motivated and the approach is interesting in that it tries to avoid a difficult problem of projecting text and visual features into a single space. These modalities don\u2019t often have 1-1 correspondence in the kinds of information they contain. \n2. The main idea can be implemented in a simple fashion using loss functions, without requiring any changes to the original models that compute the representations.\n\nIssues:\n\nThe main issue I had is with respect to the empirical evaluations. In general I found the presentation hard to follow and the results don\u2019t look convincing for an end application. Here are some follow up questions:\n\n1. The experimental results in Table 4 seem to suggest that there is no single setting works consistently better than the projection method across all tasks. \n\n2. Skipthought\u2019s spearman rho numbers in [Kiros et al., 2015] is much higher (~77 whereas whats reported in Table 1 here is only 52). Is this due to simple testing of the cosine distance as opposed to training?\n\n3. Temporal grounding discussion was unclear for me. There appears to be an attention model that somehow picks out relevant frames using the word embeddings and the frame representation. There seem to be two corresponding vectors that are used but how is this turned into a softmax weight? Is there a dot product that is missing somewhere?\n\n4. Are the sentences associated with each frame treated separately (or) all sentences for a video simply treated as multilabels?\n\n5. Can you argue whether the gains arise due to vision related information or because this setting simply introduces some additional training data . It would be cleaner to control for this if possible (ensuring that the text only and grounded models have same amounts of training somehow). Maybe this is a non issue because the size of the text available from the vision side (i.e. the # of sentences associated with the videos) is orders of magnitude smaller to what you get for text alone. I suspect this is the case but it would be helpful to point this out or control for it. \n\n6. I did not understand the explanation for random frame choice . What do you mean by \u201ceven when anchors bear no perceptual semantics\u201d?\n\nTo summarize, I liked the main idea behind the paper. It is simple and avoids a tricky issue of grounding different modalities in one place. The empirical evaluation is a bit hard to follow because of missing details and the results with respect to end tasks does not look convincing.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}