{"title": "Clearly-written, well-demonstrated learning and application of unsupervised skills via information-theoretic / entropic methods", "review": "The authors propose a learning scheme for the unsupervised acquisition of skills. These skills are then applied to (1) accelerate reinforcement learning to maximize a reward, (2) perform hierarchical RL, and (3) imitate an expert trajectory.\n\nThe unsupervised learning of skills maximizes an information theoretic objective function. The authors condition their policy on latent variable Z, and the term skill refers to a policy conditioned on a fixed Z. The mutual information between states and skills is maximized to ensure that skills control the states, while the mutual information between actions and skills given the state is minimized, to ensure that states, not actions distinguish skills. The entropy of the mixture of policies is also maximized. Further manipulations on this objective function enable the scheme to be implemented using a soft actor-critic maximizing a pseudo reward involving a learned skill discriminator.\n\nThe authors clearly position their work in relation to others, and especially point out the differences to the most similar work, namely Gregor et al 2016. These differences while seemingly minor end up providing exceptional improvement in the number of skills learned and the domains tackled.\n\nThe question-answer style is somewhat unconventional. While the content comes across clearly, the flow / narrative is a bit broken.\n\nOverall, I believe that applicability of the work is very wide, touching inverse RL, hierarchical RL, imitation learning, and more. The simulational comparisons are also very useful.\n\nHowever, there is an issue that I'd like to see addressed:\nFig 8: In a high-dimensional task, namely 111D ant navigation, DIAYN performs slightly worse than others. Incorporating a prior on useful skills makes DIAYN perform much better. Here, apart from the comparision with other state of the art RL methods, the authors should also compare to VIC. Indeed one of the key differences to VIC was the uniform prior on skills, which the authors now break albeit in a slightly different way. Thus, it is essential to also show the performance of VIC, and comment on any similarities / differences. The relation of this prior to the VIC prior should also be made clear. Further, the performance of VIC on the half cheetah hurdle should be also be shown.\n\nIf the above issue is addressed, I strongly recommend that the work be presented at ICLR.\n\nMinor issues / typos:\npg 1: \"policy that alters that state of the environment\" to \"policy that alters the state of the environment\"\npg 3: \"mutual information between skills and states, I(A; Z)\" to \"mutual information between skills and states, I(S; Z)\"\npg 4: \"guaranteeing that is has maximum entropy\" to \"guaranteeing that it has maximum entropy\"\npg 4: \" soft actor critic\" to \" soft actor critic (SAC)\" since SAC is used later.\npg 5: full form of VIME not introduced\nFig 5: would be good to also show the variance as a shaded area around the mean.\npg 7: \"whereas DIAYN explicitly skills that effectively partition the state space\" ?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}