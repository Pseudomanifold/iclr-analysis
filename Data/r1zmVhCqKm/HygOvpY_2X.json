{"title": "questions about experiments", "review": "Summary\nThis paper proposes an approach to fill in gaps in sentences. While usually generation assumes a provided left context, this paper generalizes generation to any missing gap of any number of words.\nThe method is based on Transformer which does attention on all the available context and generates in a left to right manner.\nThe authors validate their approach on rather small scale datasets. I am unclear about several details of their empirical validation.\n\nRelevance: this work is on text generation, which is very relevant to this conference.\n\nClarity: the description of the method is very clear, less so the description of the empirical validation.\n\nNovelty: The method per se is not novel, but the combination of method and application is.\n\nEmpirical validation: The empirical validation is not very clear and potentially weak.\nFirst, I did not understand the baselines the authors considered: seq2seq and GAN. There are LOTS of variants of these methods and the references cited by the authors do not immediately apply to filling-in tasks. The authors should explain in detail what these baselines are and how they work. \nSecond, MaskGAN ICLR 2018 was proposed to solve the filling task and code is publicly available by the authors of that paper. A direct comparison to MaskGAN seems a must.\nThird, there are lots of details that are unclear, such as : how are segments defined in practice? the ranking metric is not clearly defined (pair-wise ranking or ranking of N possible completion?). Could the authors be a little bit more formal?\nSpeaking of metrics, why don't the authors considered precision at K for the evaluation of small gaps?\nFourth, the datasets considered, in particular the Grimm's dataset, is very small. There are only 16K training sentences and the vocabulary size is only 7K. How big is the model? How comes it does not overfit to such a small dataset? Did the authors measure overlap between training and test sets? \n\nMore general comments\nThe beauty of the proposed approach is its simplicity. However, the proposed model feels not satisfactory as generation proceeds left to right, while the rightmost and the leftmost missing word in the gap should be treated as equal citizens.\n\nMinor note: positional embeddings are useful also with convolutional models. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}