{"title": "addressing an interesting problem but current contribution borderline", "review": "The paper proposes a method for neural architecture search. It is observed that in standard layer-wise neural networks, there is mass redundancy in the model weights. The authors propose to go beyond the hierarchy of layers, prune low-sensitivity neurons and add new neurons to the network. The concept of layer is weakened, and the relation graph between individual neurons are highlighted. Overall, the proposed research problem is interesting.\n\nHowever, one concern I have is the authors are not going as far as they claimed. The backbone of the neural network in Figure 2 is still a layered network structure. In a nutshell, a neural network is a computation graph. If the layers are not essential as claimed, why not go beyond such structures.\n\nThe method proposed for location and direction search is not entirely satisfactory. Given the typically large network structure, there has to be more informed methods/heuristics for selecting where to adjust the structure. Relying on genetic algorithms to find such locations and changes do not seem to be promising.\n\nFinally the results are not so convincing. The authors claim to have obtained higher test accuracy than state of the art architectures. But their method's results were worse than several existing methods on the same datasets. The definition of \"score\" is rather ad hoc; no (sufficient) justifications are given as to why the particular choice/design. Even using \"score\", the proposed method is not really the best among competing methods.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}