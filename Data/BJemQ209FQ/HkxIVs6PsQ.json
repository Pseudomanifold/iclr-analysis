{"title": "A novel proposal addressing a complex problem with a large number of components but without a clear analysis of their relevance", "review": "The paper propose a framework to deal with large state and action\nspaces with sparse rewards in reinforcement learning. In particular,\nthey propose to use a meta-learner to generate experience to the agent\nand to decompose the learning task into simpler sub-tasks. The authors\ntrain a DQN with a novel architecture to navigate the Web.\nIn addition the authors propose to use several strategies: shallow\nencoding (SE), reward shaping (AR) and curriculum learning (CI/CG). \nIt is shown how the proposed method outperforms state-of-the-art\nsystems on several tasks.\n\nIn the first set of experiments it is clear the improved performance\nof QWeb over Shi17 and Liu18, however, it is not clear why QWeb is not\nable to learn in the social-media-all problem. The authors tested only\none of the possible variants (AR) of the proposed approach with good\nperformance. \n\nIt is not clear in the book-flight-form environment, why the\nQWeb+SE+AR obtained 100% success while the MetaQWeb, which includes\none of main components in this paper, has a lower performance.\n\nThe proposed method uses a large number of components/methods, but it\nis not clear the relevance of each of them. The papers reads like, \"I\nhave a very complex problem to solve so I try all the methods that I\nthink will be useful\". The paper will benefit from an individual\nassessment of the different components.\n\nThe authors should include a section of conclusions and future work.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}