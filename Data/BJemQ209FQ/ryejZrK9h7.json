{"title": "a good RL application paper for dealing with large action and state spaces", "review": "This paper developed a curriculum learning method for training an RL agent to navigate a web. It is based on the idea of decomposing an instruction in to multiple sub-instructions, which is equivalent to decompose the original task into multiple easy to solve sub-tasks. The paper is well motivated and easily accessible. The problem tackled in this work is an interesting application of RL dealing with large action and state spaces. It also demonstrates superior performance over the state of the art methods on the same domains\n\nHere are the comments for improving this manuscript:\n  \nThere are a few notations used without definition, for example DOM tree, Potential (in equation (4))\n\nSome justification regarding the the Q value function specified in (1) might be helpful, otherwise it looks very adhoc.\n\nAlthough using both shallow encoding and augmented reward lead to good empirical results, it might be useful to give more insights, for example, sample size limit cause overfitting for deep models?\n\nWhat are the sizes of action state and action spaces?\n\nThe conclusion part is missing.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}