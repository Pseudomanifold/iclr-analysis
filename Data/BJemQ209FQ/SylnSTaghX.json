{"title": "solid experiments, needs clarity improvement", "review": "UPDATE:\n\nThank you to the authors for a comprehensive response.  I have increased my score based on these changes.  I apologize for the misunderstanding about ArXiV papers and indeed the authors are correct on that point.  Thank you as well for reporting the learning speeds.  As you mentioned, they confirm our intuitions and complete the picture of the algorithm\u2019s behavior.  The addition of pseudo-code does make the paper and algorithm easier to follow.  Thank you for adding it.  The rewritten section 5 is indeed much easier to follow and makes the coordination between the agents clear.  Seeing that the instructor is a fixed policy resolves the game theoretic issue form the original review.\n\n\nSummary:\n\nThe paper proposes a deep reinforcement learning approach to filling out web forms, called QWeb.  In addition to both deep and shallow embeddings of the states, the authors evaluate various methods for improving the learning system, including reward shaping, introducing subgoals, and even a meta-learning algorithm that is used as an instructor.  These variations are tested in several environments and basic QWeb is shown to outperform the baselines and many of the adaptations perform even better than that in more complex domains.\n\nReview:\n\nOverall, the problem the paper considers is important and their results seem significant.  The authors have derived a novel architecture and are the first to tackle the problem of filling in web forms at this scale with an autonomous learning agent rather than one that is taught mostly by demonstration.  \n\nThe related work section is very well written with topical references to recent results and solid differentiations to the new algorithm.  However, I see many references in the paper are not from peer reviewed conferences or journals.  Unless absolutely necessary, such papers should not be cited because they have not been properly peer reviewed.  If the papers cited have actually been in a conference or journal, please add the correct attribution.\n\nThe experiments seem well conducted.  I liked that each new addition to the algorithm was tested incrementally in Figure 7 to give a realistic view of the gains introduced by each change.  I also thought the earlier comparisons to the baselines were well done and I liked that they were done against modern cutting-edge LfD demonstrations.  The only thing I would have liked to seen beyond these results are actual learning curves showing, after X iterations, what percentage of the tasks could be completed.  I suspect that in many domains the baseline LfD techniques are learning much faster since learning from teachers tends to be more targeted and sample efficient.  Learning curves would show us whether or not this is the case. \n\nThe weakest part of the paper was the description of the instructor network and the Meta-training in general.  This portion seemed ill-described and largely speculative, despite the promising results in Figure 7.  In particular, Section 5 is very unclear on how exactly the Meta-Learning works.  Pseudocode is definitely needed in this portion well beyond the quick descriptions in Figure 4 and 5, which I could not understand, despite multiple readings.  I suggest eliminating those figures and providing concrete pseudo\u2014code describing the meta learning and also addressing the following open questions in the text:\n\u2022\tWhy is a rule based randomized policy good to learn from?  How is this different from learning from demonstration in the baselines?\n\u2022\tHow is a \u201cfine grained signal\u201d generated?  What does that mean?  Is it a reward?\n\u2022\tIn Section 5.1, are there two RL agents, an instructor and a learner with different reward functions?  If so, isn\u2019t this becoming game theoretic and is this likely to converge in most scenarios?\n\u2022\tWhat does Q_D^I actually represent?  Why is maximizing these values a good thing?\n\nThere are a few grammatical mistakes in the paper including:\n\nAbstract \u2013 simpler environments -> simple environments\nAbstract- with gradually increasing -> with a gradually increasing\nPage 2 \u2013 generate unbounded -> generate an unbounded\nPage 7 \u2013 correct value -> correct values\nPage 9 \u2013 episode length -> episode lengths\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}