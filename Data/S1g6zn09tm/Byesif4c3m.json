{"title": "Interesting idea, but crucial parts seem under-explained, despite the paper's length", "review": "This paper proposes a network architecture for conditional view synthesis. The task here is: given an image, and a condition variable representing a target viewpoint, render the content of the image under the target viewpoint. The idea is to first run the input image through an encoder, and then use the condition variable to transform this encoding to the target location in the latent space, and then decode the transformed encoding into the target image. This formulation allows the usage of new unsupervised losses in the embedding space. The paper also proposes a new output space for image generation, which splits the normal RGB space into subtasks: value, scaling, and global color balance, which improves performance. This is learned with supervised input-output pairs, along with adversarial and smoothness losses.\n\nMy main problem with this paper is its presentation. The paper uses a half-page for pseudocode that belongs in the appendix, and another half-page (Figure 2) displaying architectures that the method does NOT use. The whole paper is about image generation, and yet the paper does not show a single generated image until the appendix. This needs rearrangement. The text also has problems, which are maybe more critical. \n- The paper says it \"uses\" conditioning information to \"select which collection of weights\" of the CTU get applied to the encoded input. How exactly does this happen? Is there some attention mechanism doing a soft selection of weights? Or are the weight sets defined before any training begins, such that channels 0-31 are for View1, 32-63 are for View2, and so on? That second interpretation seems implied by Figure 1, but I could not find verification for it, even in the appendix. If this second interpretation is correct, then why not simply say that there is an independent CTU per viewpoint? \n- What is the architecture of the CTU? The main text says it is \"a collection of convolutional layers\", but the appendix contradicts this saying \"This CTU is implemented as a convolutional layer\" (i.e., a single layer). The CDU is described as similar to the CTU, so I am unclear about its architecture as well. \n- The appendix introduces (!) a noise vector to the architecture. What is this noise vector doing? Pix2Pix and similar papers show that a noise vector usually goes ignored by the decoder. Furthermore, the usage seems antithetical to the main method, since the paper argues against simple concatenation of conditioning information.\n\nOverall, I do like the paper, and the conditional latent space transformation seems useful, but it is a frustrating read. Critical questions that arise from the method's initial introduction go unanswered all the way past the 30th page.\n\n\n--------\n\nPost rebuttal: I lowered my rating from 5 to 4. While the paper's presentation improved during the rebuttal, I lost confidence in the method's novelty, thanks to prior work brought up by AnonReviewer3.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}