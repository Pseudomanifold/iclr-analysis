{"title": "Interesting architectures with limited novelty, also lack of clear presentation", "review": "This work proposes to use a single feed-forward network with two types of multi-scale transformers (MST) for image style transfer. The first transformer cascades existing single-scale transforms (SSTs), and the second one applies SST to the stacked feature maps. Skip connection is used between the MSTs and the decoder.\n\nPros\n\n- Has quantitative evaluation\nTable 1 includes quantitative results for different approaches, which is essential for proper evaluation.\n\nCons\n\n- The problem is not very well motivated and the novelty is limited. \nWhy shall we care about multi-scale style transformation? Style transformation is about modifying an image to match certain style WITHOUT completely destroy its content. Allowing low level details to interfere high level content seems to be a bad idea.\nAs for novelty, it is claimed that this work is the first to use skp connection. However, Avatar-Net uses skip connection before.\n\n- Empirical results not significant\nIn Table 1, why is (d) missing for the small set and (b) missing for the large set? It seems that (a) and (b) are already very good. Then why do we need the proposed methods? It is said that (b) is not extendable to arbitrary style transfer. How so? How do you define \"arbitrary style transferring methods\"?\nI do not see how Fig.6 can tell us useful information about the skip connections. The corresponding paragraph in Sec.4.3 is not very convincing or informative.\n\n- Writing can be improved.\nIn terms of content, terminologies are used without clear definitions. For example, what is \"inter-scale dependency\" in the introduction and what does \"merges multi-scaled styles optimally\" mean in Sec.3.1.3 (what is the optimality here)? It is confusing what corresponds to \"intra-scale\" or \"inter-scale\" throughout the paper. For example, the direct connection between relu_3_3 to relu_2_2 of the decoder in Fig.1b can also be interpreted as \"inter-scale\". As another example, \"4 batches of random image pairs\" in the experiment, do you mean a batch of 4 random pairs?\nIn terms of presentation, the grammar needs more careful checking. For examples, \"in the remained of this paper\", \"each methods\", etc. The meaning of the transpose in Eq.(3) is not clear. How do we transpose 3-dimensional tensor F? Also, know the difference between \\citet and \\citep and when to use them.\n\nMinors\n- In Eq.(4), according to the definition of C_i, for i > 1, the index should be (\\sum_k C_k + 1):(\\sum_k C_k + C_i)\n- Above Eq.(5), \"inter-scale feature transform (Sec.3.1.1)\" should be Sec.3.1.2.\n- In Fig. 7, the last column should be (g) instead of (e).", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}