{"title": "Lacking novelty, but cool results", "review": "This paper presents a joint optimization approach for the continuous weights and categorical structures of neural networks. The idea is the standard stochastic relaxation of introducing a parametrised distribution over the categorical parameters and marginalising it. The method then follows by alternating gradient descent on the weights and the parameters of the categorical distribution.\n\nThis exact approach was proposed in https://arxiv.org/abs/1801.07650 by Shirakawa et al. The only innovation in this work is that it uses categorical distributions with more than two values. This is a minor innovation.\n\nThe experiments are however interesting as the paper compares to the latest hyper-parameters optimization strategies for neural nets on simple tasks (eg CIFAR10) and gets comparable results. However, given that this is the biggest contribution of the paper, it would have been nice to see results in more complex tasks, eg imagenet or translation.\n\nI very much enjoyed the simplicity of the approach, but the question of innovation is making wonder whether this paper makes the ICLR bar of acceptance. The paper is also hard to read because of many English typos.   ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}