{"title": "Proposes an architecture search technique, easy to read. Not confident about the baselines and how this is compared to the literature.", "review": "This paper proposes an architecture search technique in which the hyperparameters are modeled as categorical distribution and learned jointly with the NN. The paper is written well. I am not an expert of the literature in this domain so will not be able to judge the paper regarding where it is located in the related work field.\n\nPros:\n-This is a very important line of research direction that aims to make DNNs practical, easy to deploy and cost-effective for production pipelines.  \n-The categorical distribution for hyperparameters makes sense, and the derivation of the joint training seems original idea. I liked the fact that you need to train the NN just twice (the second one only to fine tune with optimized parameters)   \n-Two very different problems (inpainting/encoding-decoding + CNN/classification) have been demonstrated.\n-Existing experiments have been explained with enough detail except for minor points.\n\nCons:\n-I speculate that there is a trade-off between the number of different parameters and whether one training is good enough to learn the architecture distribution. i.e., When you have huge networks and many parameters, how well this method works? I think the authors could provide some experimental study suggesting their users what a good use case of this algorithm is compared to other techniques in the literature. In what type of network and complexity this search method works better than others?\n-E-CAE for in-painting seems to be working significantly better than the proposed technique. Regarding results, I was expecting more insights into why this is the case. As above, at what type of a problem one should pick which algorithm? If the 7hours vs. 3days GPU difference negligible for a client, should one pick E-CAE?  \n-In theory, there has been shown lambda samples (equation 2 and 3). However, the algorithm seems to be using just 2? If I didn't miss, this is not discussed thoroughly. I speculate that this parameter is essential as the categorical distribution gets a bigger search space. Also the reliability of the model and final performance, how does it change concerning this parameter?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}