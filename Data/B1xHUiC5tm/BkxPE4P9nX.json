{"title": "Good paper for industrial engineering conference", "review": "The authors considered the problem of subway traffic forecasting, i.e. the data has temporal and spatial dimensions.\nThey propose to embed the current data point into the latent space and the, using the decoder, perform the prediction.\nThey considered several variants of this scheme:\n- construct a separate RNN for each subway station\n- perform prediction for all stations simultaneously to take into account dependencies between traffic on different stations\n- take into account a special context variable, characterizing object (subway station in our case) when performing embedding. The authors claimed that these additional variables responsible for the embedding of the object, can be learned simultaneously with the model itself and significantly improves prediction accuracy\nThe authors performed computational experiments using subway traffic data.\n\nComments:\n- the authors concentrated on a very particular engineering problem, related to traffic prediction. Even the notations/indexes, used for the algorithm description, are interpreted in terms of the traffic prediction problem\n- the text is written in such a way, that it is clear, that the main aim of the paper is to solve a particular engineering problem. In case the authors claim that they develop a general algorithm, they should compare it with other analogous approaches on different datasets. But this is not the case for the considered paper\n- the idea to embed to a latent space and then perform prediction in the latent space is not new. E.g. in [3] they motivate such approach by a Koopman operator theory, originated from theoretical physics. In [2] they used exactly such idea to construct a general scheme for approximation of dynamical systems, and then the authors of [2] demonstrated applicability of their approach on different datasets and problems. In [1] to construct the latent space they used matrix factorization, and then simple linear AR model to prediction in the latent space\n- taking into account the previous comment, the idea to introduce a context looks not very fundamental, rather as some simple engineering trick\n- as the baseline the authors considered only prediction by an average data values given their context. However, for such problems (traffic prediction) it is widely accepted that XGboost + feature engineering is the must-to-use tool (see numerous Kaggle competitions). Moreover, for time-series usually it is very difficult to get better results than results, obtained when predicting by a previous time-series value. I think that baselines provided by the authors are too weak. They should add more methods for comparison including those from [1-3]\n- the authors did not explained sufficiently clear how they tuned embeddings for the context\n\nConclusion:\n- I think that the paper, being well and clearly written, is better suited for applied industrial conferences than for a general academic conference such as ICLR, which is mainly about developing general concepts and algorithms\n\n[1] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. \u201cTemporal Regularized Matrix Factorization for High-dimensional Time Series Prediction\u201d. In: Advances in Neural Information Processing Systems 29. Ed. by D. D. Lee et al. Curran Associates, Inc., 2016, pp. 847\u2013855. URL: http://papers.nips.cc/paper/6160-temporal- regularized-matrix-factorization-for-high- dimensional-time-series-prediction.pdf.\n\n[2] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. 2015. Embed to control: a locally Linear Latent dynamics model for control from raw images. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'15), C. Cortes, D. D. Lee, M. Sugiyama, and R. Garnett (Eds.), Vol. 2. MIT Press, Cambridge, MA, USA, 2746-2754.\n\n[3] Bethany Lusch, J. Nathan Kutz, Steven L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. https://arxiv.org/abs/1712.09707", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}