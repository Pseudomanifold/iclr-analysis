{"title": "review for \"Domain Adaptive Transfer Learning\"", "review": "This paper tackles the problem of transfer learning. The approach\nproposed is simple but effective. It identifies the source training\nexamples that are most relevant for the target task and then over-samples \nthese examples when pre-training the classification network. The\n over-sampling is based on importance weights measuring the ratio \nof the prior probability for each source label in the target and source datasets. As not all source\nlabels will necessarily be present in the target dataset, the target\nprior probability for a source label is estimated by learning a\ngeneric classifier on the source dataset, applying it to each example\nin the target dataset, computing the average output probability for\neach source label an then using this average probability as the source\nlabel's prior probability.  After pre-training, fine-tuning is applied\nwith the labelled target data.\n\nExtensive experiments are performed -  pre-training on a very large source\ndataset and using large classification networks (Inception-v3 and\nAmoebaNet-B) and transferring each pre-trained network to 6 standard\nand relatively large target datasets. \n\nThe results show that pre-training which focuses on the subsets of the\nsource data that are the most similar to the target data is more\neffective, in general, than pre-training on all the source data which\ntreats each example equally. This finding is increasingly relevant the\nmore dissimilar the target and source datasets are and/or the more\n\"irrelevant\" examples for the target task the source dataset contains. \n\n\n\nPros:\n\n+ The experimental results of this paper are its main\n  strength. Results are presented on pre-training on a very large and\n  diverse dataset called \"ANON\" and applying the important sample\n  pre-training approach to both the Inception-v3 and AmoebaNet-B\n  networks.\n\n+ It adds more solid evidence that learning generic classification\nnetworks from diverse datasets do not outperform more specialised\nnetworks learnt from more relevant training data for specific tasks.\n\n+ The importance sampling approach to pre-training is compared to\npre-training on different subsets of the source dataset corresponding\nto images with certain high-level labels. Each subset is (potentially)\nrelevant to at least one particular target task. The importance\nsampling approach does not always outperform pre-training\nexclusively with the most relevant subset approach has a consistently high\nperformance across the board.\n\n+/- A new very large image dataset (which would seem to be a\ncompliment to ImageNet) is introduced though it is unclear\nwhether this dataset will be made available to the research\ncommunity at a later date.\n\n\nCons:\n\n- Details are lacking about the \"ANON\" dataset introduced in this\n  paper (where do the photos come from and the labels, visualization of a few examples...)\n\n- There are not many technical issues discussed in the paper and that\n  is fine as the main idea is relatively simple and its\n  effectiveness is mainly demonstrated empirically, but I\n  feel the paper is missing a discussion about the importance of the initial\n  classifier trained to estimate the target prior probabilities for\n  the source labels and whether it is crucial that it has a certain\n  level of accuracy etc.\n\n- The approach in the paper implies a practitioner should have \n  access to a very large target dataset and the computational and time\n  resources to appropriately pre-train a complex network for each new\n  target task encountered. This is probably not feasible if many\n  target tasks are considered. Unfortunately the paper does not\n  give insights into how pre-training from scratch for each new target\n  could be avoided. \n\n\n- The references in the paper, especially the \"Exploring the limits of\n  weakly supervised pre-training\", demonstrate that it is already\n  known that you do not increase the accuracy for the target task by\n  pre-training with many source examples that are not very relevant to the\n  target task. So one could argue that the findings in the paper are\n  not particularly novel.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}