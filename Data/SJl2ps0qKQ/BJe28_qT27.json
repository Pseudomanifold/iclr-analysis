{"title": "Good paper, but need more related work discussions", "review": "Summary: the paper is interested in parsing compound questions for querying on knowledge graph, e.g. MetaQA by Zhang et al. (2017). The paper proposes to have two modules, one that segments the question into partitions (up to three) and the other that looks at each segment to get the relation. The relations are merged to obtain a single KG path, which is queried to obtain the answer. Since the segmentation is a non-differentiable process, the paper uses reinforcement learning to propagate gradient to the segmentation model. The segmentation is a process of classifying each word for which partition it should be tied to. Answering is a process of classifying the partition into one of the possible relation edges. The model shows expected results in a synthetic arithmetic dataset, and obtains the state of the art in MetaQA, improving nearly 5% over the baseline. The model especially does much better on 3-hop questions, with nearly 20% improvement.\n\nStrengths: the paper is well-written. The model is simple yet effective and is a novel contribution to compound question answering on KG. Especially, the improvement on 3-hop category is nearly 20%, which is substantial and quite impressive. \n\nWeaknesses: My biggest concern is the lack of discussions on its relevance to  (Iyyer et al., 2016), which also proposed to decompose question into simpler ones for WIkiTableQuestions. Also, I think it would be good to mention Semantic Role Labeling as related literature, which is about tagging each word with its role in the sentence. The partition index can be somewhat considered as a \u201crole\u201d in the sentence.\n\nQuestions:\n1. How do you obtain x^(k)? Is it the last state of the LSTM?\n2. Why did you have to augment \u201cNO_OP\u201d relation in the MetaQA dataset?\n3. Why +1 reward has lower variance than probabilistic reward? Explanation or citation would be needed.\n4. What if two partitions need to share a word? The current setup necessitates that a word participates in only one partition. Wouldn\u2019t this be problematic?\n5. I am a bit confused about how the simple question answering module is trained. Is it directly trained by the gold relation label?\n\nTypos and Suggestions:\n- Second paragraph of 2.1: in stead -> instead\n- Third paragraph of 2.1: research. -> research\n- c_t + h_t: would be good to explicitly mention that the circled plus sign is concatenation.\n- Last paragraph on page 4: \u201cleave to be\u201d?\n- Second last paragraph of 4.1: he -> The\n- Second paragraph of 4.2: \u201cif exists a proper meaning\u201d?\n- First paragraph of page 7: be either assume -> either assume\n- Last paragraph of Section 5: generalizing -> generalize\n- I think you should not put acknowledgment in a double-blind submission.\n\nM Iyyer, W Yih, MW Chang. Answering complicated question intents expressed in decomposed question sequences. 2016 (https://arxiv.org/abs/1611.01242)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}