{"title": "Thorough experiments which prove there exist \"support examples\" in neural network training.", "review": "This paper studies the forgetting behavior of the training examples during SGD. Empirically it shows there are forgettable and unforgettable examples, unforgettable examples are like \"support examples\", one can achieve similar performance by training only on these \"support examples\". The paper also shows this phenomenon is consistent across different network architectures.\n\nPros:\nThis paper is written in high quality, clearly presented. It is original in the sense that this is the first empirical study on the forgettability of examples in during neural network training.\n\nComments and Questions on the experiment details:\n1. Is the dataset randomly shuffled after every epoch? One concern is that if the order is fixed, some of the examples will be unforgettable simply because the previous batches have similar examples , and training the model on the previous batches makes it good on some examples in the current batch.\n2. It would be more interesting to also include datasets like cifar100, which has more labels. The current datasets all have only 10 categories.\n3. An addition figure can be provided which switches the order of training in figure 4b. Namely, start with training on b.2.\n\nCons:\nLack of insight. Subjectively, I usually expect empirical analysis papers to either come up with unexpected observations or provide guidance for practice. In my opinion, the findings of this work is within expectation, and there is a gap for practice.\n\nOverall this paper is worth publishing for the systematic experiments which empirically verifies that there are support examples in neural networks.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}