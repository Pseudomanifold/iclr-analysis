{"title": "Good work in general", "review": "\n-- Originality --\n\nThis paper studies how to use KL-regularization with information asymmetry to speed up and improve reinforcement learning (RL). Compared with existing work, the major novelty in the proposed algorithm is that it uses a default policy learned from data, rather than a fixed default policy. Moreover, the proposed algorithm also limits the amount of information the default policy receives, i.e., there is an \"information asymmetry\" between the agent policy and the default policy. In many applications, the default policy is purposely chosen to be \"goal agnostic\" and hence conducts the \"transfer learning\". To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n-- Clarify --\n\nThe paper is well written in general and is easy to follow.\n\n-- Significance --\n\nI think the idea of regularizing RL via an informationally asymmetric default policy is interesting. It might be an efficient way to do transfer learning (generalization) in some RL applications. This paper has also done extensive and rigorous experiments. Some experiment results are thought-provoking.\n\n-- Pros and Cons\n\nPros:\n\n1)  The idea of regularizing RL via an informationally asymmetric default policy is interesting. To the best of my knowledge, this \"informationally asymmetric\" KL-regularization approach is novel.\n\n2) The experiment results are extensive, rigorous, and thought-provoking.\n\nCons:\n\n1) My understanding is that this \"informationally asymmetric\" KL-regularization approach is a general approach and can be combined with many policy learning algorithms. It is not completely clear to me why the authors choose to combine it with an actor-critic approach (see Algorithm 1)? Why not combine it with other policy learning algorithms? Please explain.\n\n2) This paper does not have any theoretical results. I fully understand that it is highly non-trivial or even impossible to analyze the proposed algorithm in the general case. However, I recommend the authors to analyze (possibly a variant of) the proposed algorithm in a simplified setting (e.g. the network has only one layer, or even is linear) to further strengthen the results.\n\n3) The experiment results of this paper are interesting, but I think the authors can do a better job of intuitively explaining the experiment results. For instance, the experiment results show that when the reward is \"dense shaping\", the proposed method and the baseline perform similarly. Might the authors provide an intuitive explanation for this observation? I recommend the authors to try to provide intuitive explanation for all such interesting observations in the paper. \n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}