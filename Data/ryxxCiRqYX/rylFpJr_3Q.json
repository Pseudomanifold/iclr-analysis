{"title": "Interesting paper, should be accepted", "review": "This paper presents a very interesting interpretation of the neural network architecture.\n\nI think what is remarkable is that the author presents the general results (beyond the dense layer) including a convolutional layer by using the higher-order tensor operation.\nAlso, this research gives us new insight into the network architecture, and have the potential which leads to many interesting future directions. \nSo I think this work has significant value for the community.\n\nThe paper is clearly written and easy to follow in the meaning that the statement is clear and enough validation is shown. (I found some part of the proof are hard to follow.)\n\n\\questions\nIn the experiment when you mention about \"embed solvers as a replacement to their corresponding blocks of layers\", I wonder how they are implemented. About the feedforward propagation, I guess that for example, the prox operator is applied multiple times to the input, but I cannot consider what happens about the backpropagation of the loss.\n\nIn the experiment, the author mentioned that  \"what happens if the algorithm is applied for multiple iterations?\". From this, I guess the author iterate the corresponding algorithms several times, but actually how many times were the iterations or are there any criterion to stop the algorithm?\n\n\\minor comments\nThe definition of \\lambda_max below Eq(3) are not shown, thus should be added.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "1: The reviewer's evaluation is an educated guess"}