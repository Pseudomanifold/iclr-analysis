{"title": "Interesting research direction, novel contributions, and well-written paper", "review": "This paper presented a novel approach for modeling a sequence of characters as a sequence of latent segmentations. The challenge here was how to efficiently compute the marginal likelihood of a character sequence (exponential number different of segmentations). The author(s) overcame this by having a segment generation process independent from the previous segment (only depends on a sequence of characters). The inference is then required a forward algorithm. To generate a segment, a model can either select a lexical unit (pre-processed from a training corpus) or generate character by character. \n\nOn the experiments, the author(s) showed that the model recovered semantical segmentation on many word segmentation dataset (including phonemes). The lexical memory and the length regularization both contribute significantly as shown in the analysis. The language modeling result (BPC) was also competitive with LSTM-based LMs. \n\nI think the overall model is interesting and well motivated, though it is a bit disappointing that the author(s) needed to use an extra regularizer to constraint the segment length (from the lexical memory?). Perhaps, the way they build a lexical memory should be investigated further. The experiment should also show an evidence that SNLM(+memory, -length) was overfitted as claimed.\n\nThe validation and test dataset have been modified to remove \"samples\" containing OOV characters. How many have been removed? The author(s) could opt for an unknown character similar to many word-level datasets.\n\nThe use of word segmentation data was quite clever, but this also downplayed other work that is not aimed to recover human-semantic segmentations. For example, a segment \"doyou\" on page 10 might be considered as a valid segmentation since it appears a whole lot. HM-LSTM though did poorly on the segmentation task but performed rather well on PTB LM task, but the author(s) decided to omit this comparison.\n\nSome minor comments:\n- A typo in the introduction \"... semi-Markov model. The the characters inside ...\".\n- Eq 3 is a bit hard to follow. Perhaps, a short derivation should be presented.\n- Is it possible to efficiently generate a sequence?\n\n[Updated after reconsidering other reviews]\nAlthough this paper misses some related work and comparison models, I think it still has a valid contribution to language modeling: a character-level language model that produces plausible word segmentation.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}