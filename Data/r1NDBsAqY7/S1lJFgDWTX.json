{"title": "Some interesting ideas, but comparison to previous work might be misleading", "review": "[Note to the authors: I was assigned this paper after the reviewing deadline.]\n\nThe authors train language models on unsegmented text, simultaneously discovering word boundaries\nwithout direct supervision. Given the past history, but ignoring past segmentation decisions\nto keep computations tractable, the model predicts the next character segment (word-like unit)\nby combining a character-level LSTM with a lexical memory. To prevent overusing the\nlexical memory, which would lead to poor generalization, the authors propose a segment\nlength penalty.\n\nStrengths:\n\nThe model architecture is interesting, combining the benefits of a character-level\nmodel (open vocabulary) with those of a lexical model (effective for frequent character\nsequences).\n\nDespite the exponential number of possible segmentations, inference remains tractable\nusing dynamic programming (with some simplifying assumptions).\n\nThe ablation study clearly shows that both the lexical memory and the length penalty\ncontribute significantly.\n\nWeaknesses:\n\nThe writing quality is somewhat weak. Many errors should have been caught when\nproofreading the paper (e.g. \"The segmentation decisions and decisions\" and \n\"The the characters\" on page 1).\n\nI am confused by the key-value pairs of the lexical memory. Shouldn't character\nsequences be keys, and their trainable vector representations be values?\n\nIt is hard to evaluate how good the language models are, as the strength of the\nbaselines is unclear. How well-tuned is the LSTM?\n\nComparison to some other segmentation approaches (not necessarily with language modeling)\nis limited. In particular, adaptor grammars perform very well on the Brent corpus [1].\nHowever, [2] is mentioned briefly. As these other approaches work better for segmentation,\nthe authors should carefully justify why having a single model that does both language\nmodeling and word segmentation well matters. Many neural approaches have also been\nsuggested for Chinese word segmentation (among others [3]). In these papers, results on the\nPKU dataset are much better.  Are these directly comparable with yours?\n\nI would have liked a finer analysis of the impact of the length penalty.\nA plot showing how validation likelihood and segmentation performance vary as\n\\lambda is increased could potentially be interesting.\n\n[1] Johnson and Goldwater. \"Improving nonparameteric Bayesian inference: experiments on\nunsupervised word segmentation with adaptor grammars\", HLT, 2009\n\n[2] Berg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\", NAACL, 2010\n\n[3] Yang et al. Yang, Jie, Yue Zhang, and Fei Dong. \"Neural Word Segmentation with Rich Pretraining\", ACL, 2017", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}