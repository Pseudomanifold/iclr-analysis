{"title": "Nice comparison but lacking variance discussion and clear way to usefulness.", "review": "The authors study the impact of using different representation units \u2013 words, characters, subword units, and morphological segments on the representations learned in neural machine translation systems. The paper is very well-written and the study is convincing: there are 4 NMT data-sets used and 4 tasks used to asses the quality of representations and their robustness to noise. The results are a bit mixed: character-level representations are most robust to noise and perform best in the morphological tagging task, morphological segments perform best in syntax tasks, and so on. One problem with the study is that the architecture of the NMT network is fixed to be an LSTM. It is unclear how this affects the results: does the fact that LSTM can make many steps on character-level input help? Does it hurt? Architectural variance is not measured nor discussed at all. In the introduction, the authors say \"We make practical recommendations based on our results.\" -- this is hard to see, as the results are very mixed and using a fixed architecture makes it hard to draw any recommendations from them in a wider setting.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}