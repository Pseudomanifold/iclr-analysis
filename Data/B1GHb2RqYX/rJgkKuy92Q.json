{"title": "Reduced parameter convolutions", "review": "It is an interesting idea for reducing the number of parameters, but I don't think the experimental section is adequate to judge it.\n\nModern ConvNet architectures achieve pretty good accuracy with fairly low overheads in terms of FLOPs and parameters:\n- SqueezeNet is a compressed version of AlexNet\n- MobileNet and ShuffleNets are compressed versions of ResNeXt\n- ConDenseNets are a compressed version of DenseNets.\n\nRegarding Table 7: It seems you are losing 7-8% absolute accuracy while still having the same computation overhead in terms of FLOPs. You do have few parameters, but trained ResNet models can be compressed substantially with relatively little loss of accuracy.\n\nSection 4.2 You say \"Since the convolutional weights are fixed, we do not have to compute the gradients nor update the weights.\"\nI don't understand this. Aren't the other (m-1) filters functions of the first filter? Should you not backpropagate through them to get an accurate gradient?", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}