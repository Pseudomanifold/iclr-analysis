{"title": "Review of PolyCNN: Learning Seed Convolutional Filters", "review": "Review of PolyCNN: Learning Seed Convolutional Filters\n\nSummary:\nIn this paper, the authors propose the PolyCNN weight sharing architecture for cutting down the number of parameters in a given CNN architecture. The authors employ a bank of pointwise nonlinearities and a single seed filter to emulate a full convolutional filter bank. The authors demonstrate that their resulting model achieves performances comparable to modern CNN architectures on SVHN, MNIST, CIFAR-10 and ImageNet.\n\nMajor Comments:\n\nMy largest concern is the degree to which the PolyCNN weight sharing architecture is 'generic' and not specialized for a particular network architecture. If one has a convolutional filter bank inside a CNN layer, what happens if I just swap that architecture for the PolyCNN representation? This question is not directly address in Table 1, nor in Table 4, or 5 as far as I can tell (but happy to be corrected).\n\nThe 'baseline' network the authors compare against the PolyCNN appears to be a custom CNN architecture in which they swap in a PolyCNN representation. This is a reasonable result, but it is not clear the degree to which their results are tied to the specific 'baseline' architecture.\n\nTo argue that the method works generically, the authors must show a stronger result -- namely, for each or many of the baseline's considered (e.g. BC, BNN, ResNet, NIN, AlexNet), what happens if you swap in PolyCNN for the CNNs? This would generate a PolyCNN-variant of each baseline network for a side-by-side comparison. For each of these apples-to-apples comparisons, we would then observe the trade-offs by employing a PolyCNN weight scheme. (See Table 2 and 3 in [1] for such an analysis with another CNN module.)\n\nAlso, note that the CIFAR-10, ImageNet and MNIST results, the results are not near state-of-the-art (see [2] for many publications). Thus, the authors should consider comparing against more modern baselines. For example, fractional max-pooling (published in 2015; see [2]) achieves 96.5% accuracy but the top comparisons presented are 93.5%.\n\n\nMinor Comments:\n- In each table row corresponding to a network architecture, the authors should add a column indicating the number of parameters in the network architecture so the reader may clearly observe parameter savings.\n\n- Given that the PolyCNN architecture only has to perform 1 dot-product corresponding to the seed kernel and given that the dot-product is often the vast majority of the computational cost, do the authors likewise see a speed up in terms of inference time (or just measuring Mult-Add operations)?\n\n- The authors need to document what data augmentation techniques were used for each result. For instance, the state-of-the-art in CIFAR-10 is often broken down into \"best method with no augmentation\", or \"best method with XXX augmentations/permutations\"\n\n[1] Squeeze-and-Excitation Networks\nJie Hu, Li Shen, Gang Sun\nhttps://arxiv.org/abs/1709.01507\n\n[2] http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}