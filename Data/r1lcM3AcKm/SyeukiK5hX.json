{"title": "Review ", "review": "Summary: \nTraining RNNs on long-sequences is a challenging task as gradients tend to explode or vanish. One way to mitigate this problem to some extent is to use semi-supervised learning in which objective function consists of unsupervised and supervised loss functions. Even though the contribution of unsupervised loss function can be controlled by a coefficient in the objective function, the unsupervised loss term can cause important information about the supervised task to be degraded or erased. This paper proposed a method to mitigate the problem of training of RNNs on long sequences by coordinating supervised and unsupervised loss functions. More specifically, they use two RNNs in which RNNs have a shared feature space for both supervised and unsupervised tasks and allow one of RNN to have a private space dedicated for the supervised task. \n\nStrengths:\n+ The idea of dividing the hidden states into two parts is interesting as it helps the model to control the effect of unsupervised loss on main supervised task.\n+ Shared and private spaces visualization are very informative. \n+ This model shows better results on MNIST and CIFAR-10 in compared with previous methods.\nWeaknesses:\n- Paper writing is good until section 3.1. This section is very confusing. I read it multiple times until understood what is happening. Lots of details are missing in sections 3.2 about how this model forces to not mix up the gradients for shared and private hidden units.\n- There are quite similarities between Trinh et al., 2018 and this paper. The only main difference is dividing the hidden state into shared and private ones. \n- Is there any reason why StanfordDogs and DBpedia are not used in this paper? Given the close relationship between Trinh et al., 2018 and this paper, it would have been better to have some results for these sets.\n- The paper claims that their model trains and evaluates faster. Rather than an argument about fewer parameters for auxiliary tasks, I don't see any justification. Fewer parameters don't necessarily lead to faster training or test time.\n\nComments and Questions\n- Is vanilla RNN used for the experiments? GRU is mentioned but my understanding is that it is only used for auxiliary loss.\n- There should be some detail about model architectures and training e.g. hidden units size, learning rate, dropout if any, etc. \n- It mentions that the model uses different time-scale updating operations of distinct RNNs with different representational spaces, I don't see how. Can you elaborate? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}