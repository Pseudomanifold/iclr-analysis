{"title": "Incremental work from previous results", "review": "The main contribution of the paper is claimed as providing \u201capriori\u201d guarantees for generalization where the generalization bounds depend only on the norm of the \u201ctrue predictor\u201d. This in contrast to what is termed as \u201cposterior\u201d guarantee where the generalization bound is provided in terms of the \u201clearned predictor\u201d. \n\nI could not appreciate the main motivation/challenge in this distinction. In particular, if additional constraints are imposed on the learned predictor during training, then it seems straightforward that any \u201cposterior\u201d guarantee can be immediately converted to an \u201capriori\u201d guarantee. For example, in the context of the problem in this paper, since it is known that the true function f* is approximated by a 2 layer network of path norm < 4\\gamma(f*). Thus,  if during training the empirical loss is minimized over the networks with hard constraint of $\\theta_P < 4\\gamma(f*)$, then from Lemma 2, we should immediately get a generalization bound in terms of $\\gamma(f*)$ instead of $||\\theta||_P$.\n\nThe main result in the paper (Theorem 5) seems to essentially do a variant of the above approach, except instead of a hard constraint on the path norm of parameters, the authors analyze the estimator from soft regularization (eq. 6) and provide the guarantee in terms of the regularization parameter. This although is not immediate like the hard constrained version, I do not see major challenges in the extension to regularized version. Moreover, I also do not see the immediate motivation for analyzing the regularized objective over the constrained one (at least for the purpose of generalization bounds since both estimators are intractable computationally). Please provide a response if I missed something.\n\n\nWriting:\n1 (MAIN). Proper citations missing from many places. e.g., definition of path norm, spectral norm. \nMore importantly for the theorems in Section 2, appropriate citations within the theorems are missing. For example in Theorem 2 the authors do cite Klusowski and Barron in the beginning of section but not in the theorem header. Same for Theorem 3, and 4 which are known results too. Lemma 1 follows from corollary 7 in Neyshabur et al. 2015. \n\n\n2. Undefined notation ||.||_\\infty in Assumption 1. \n3. Definition 2: please provide proper reference for the definition and for multivariate Fourier transform. \n\nMore generally it is hard to follow the notations through out the paper and the motivations for certain notations are not completely clear. Some intuition here would help. For example, the reason we care about Assumption 1 is that this class of functions are well approximated by 2 layer networks (as shown in theorem 2). This could be stated ahead of the definitions. \n\n\n Also a broader outline of the structure should help with readablity. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}