{"title": "Promising idea but devil in the details?", "review": "The paper proposes a novel approach to address the issue of mode collapse in the GANs. I see two interesting ideas here: (a) Since the GAN formulation seeks to achieve distributional alignment by the generator PDF end the PDF corresponding to the real, target data, the paper proposes to represent the real data using the embedding from an AE encoder, and, (b) achieve distributional alignment via an importance sampling formulation. The paper claims that their implementation of the above ideas helps improves the issues of mode collapse and mode oscillation, thereby (?) improving sample diversity and speeds up convergence. \n\nThe ideas seem promising and I was hoping that the paper carefully implemented an approach based on the above, compared with the existing approaches and reported the results carefully. I find that the paper falls short of doing this and does not merit a publication at this point. A major revision of the work is needed.\n\nThe main shortcomings of the paper follow:\n\n1.\tClarity: The paper greatly suffers from a loose handling of technical concepts and terminology; statements are not made in a careful and measured manner; and, grammatical mistakes abound. There are too many to list them all. Following are exemplary but symptomatic of the way the entire paper is written.\n\na.\t\u201cDual importance weights iteratively maximize the representation of generated samples for both distributions\u201d. Maximize the representation?\n\nb.\t\u201cExperimental evaluation shows that the proposed network learns complete modes of target distribution more stable and faster than state of the art methods.\u201d What is meant by \u2018more stable\u2019? Please provide the metric used to measure the stability and indicate which experiment validates this claim.\n\nc.\t\u201cMain reason of mode collapsing problem is that the discriminator is incapable of delivering any information about samples\u2019 diversity. Once the generator finds optimal point of a fixed discriminator in each generator training step, the network produces same samples driving the expectation value in objective function becomes minimum regardless of input noise vectors.\u201d Loose and overly broad statements about the underlying problems and speculations about the mechanisms and underlying behavior of the algorithms \u2013 both the state of the art ones and the proposed one, shouldn\u2019t be made unless there is a strong justification provided via mathematical proofs or experiments. \n\nd.\t\u201cAuto-encoder not only decreases the dimensionality of input data but also finds optimal abstraction of input data for better discrimination between subjects.\u201d Optimal abstraction of input data for better discrimination between subjects? \n\ne.\tEquation (2) should use the 2-norm since s_r and s_g are not scalars. \n\n2.\tImplementation and technical details:\n\na.\tThe distance defined in Equation (2) and using it to modify the generator loss function in (3) seems to be based on a heuristic. The authors should formally derive it from importance sampling. This will help formally ground the proposed modifications to the objective function and help understand the expected benefits and the convergence properties better.\n\nb.\tSecondly, the proposed distance function in (2) is supposed to be obtained using a kernel density estimator. KDEs bring their own challenges, especially when used in high-dimension spaces. It is not clear what the bias/ variance properties of the proposed KDE estimator are and how it impacts the overall objective. Further, no details are shared about the dimensionality of the embedding space and the bandwidths used to compute the KDE.\n\nc.\tBeyond the KDE, there are other mathematical / implementation details missing making it practically impossible to replicate results. For example, what are the mathematical expressions for the two set of weight?\n\nd.\tHigh quality sample ratio (HQS) metric is neither defined nor cited from other works. It is not clear how the quality is measured and how the high-quality samples are obtained. \n\n3.\tInsufficient/ inadequate experimental validation:\n\na.\tWhile experiments on toy dataset (mixture of gaussian) shows slight improvements compared to baselines, the experiments on CIFAR and MNIST only report qualitative results in Figure 4,5 and 6. This is not sufficient to establish that the proposed approach works better than approaches compared with. \n\nb.\tQuantitative evaluation on real datasets using the IS or the FID score (or even the metrics in Table 1) to establish the claimed improvement in performance \u2013 sample diversity and the quality of generated images. Perceptual measures such as SSIM and the one proposed in Zhang et al (CVPR 2018, arXiv 1801.03924) should be used to measure image quality. Precision, Recall and F1 can be used to provide a measure of similarity between the real and generated data.\n\nc.\tThe claim about improved convergence is not adequately validated. \n\nd.\tAside: Since the real target data is represented by an AE embedding, the time taken to train the AE should be used when comparing baselines that do not have pre-training, in Figure 4. \n\nI\u2019m willing to revise my opinion if the authors address the above issues in the forthcoming discussion phase.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}