{"title": "This paper describes an interesting phenomenon, but some of the experimental evidence is a bit lacking. ", "review": "This paper describes an interesting phenomenon: that most of the learning happens in a small subspace. However, the experimental evidence presented in this paper is a bit lacking. The authors also cook up a toy example on which gradient descent exhibits similar behavior. Here are a few detailed comments:\n\n1. The Hessian overlap metric is suitable for showing the gradient lies in an invariant subspace of the Hessian, but does not show it lies in the dominant invariant subspace. \n2. There are well-established notions of distances between subspaces in linear algebra, and I suggest the authors comment on the connection between their notion of overlap between subspaces and these established notions.\n3. The authors make a few statements along the lines of ``the Hessian is small, so the objective is flat''. This is a bit misleading as it is possible for the gradient to be large but the Hessian to be small.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}