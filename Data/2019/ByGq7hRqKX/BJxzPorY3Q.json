{"title": "Promising results in cross-task transfer. Missing references to prior works", "review": "This work proposes to train an RL-based agent to simultaneously learn Embodied Question Answering and Semantic Goal Navigation on the ViZDoom dataset. The proposed model incorporates visual attention over the input frames, and also further supervises the attention mechanism by incorporating an auxiliary task for detecting objects and attributes.\n\nPros:\n-Paper was easy to follow and well motivated\n-Design choices were extensively tested via ablation\n-Results demonstrate successful transfer between SGN, EQA, and the auxiliary detection task\n\nCons:\n-With the exception of the 2nd round of feature gating in equation (3), I fail to see how the proposed gating -> spatial attention scheme is any different from the common inner-product based spatial attention used in a large number of prior works, including  [1], [2], and [3] and many more.\n-The use of attribute and object recognition as an auxiliary task for zero-shot transfer has been previously explored in [3]\n\n\nOverall, while I like the results demonstrating successful inductive transfer across tasks, I did not find the ideas presented in this work to be sufficiently novel or new.\n\n[1] Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering, Huijuan Xu, Kate Saenko\n[2] Drew A. Hudson, Christopher D. Manning, Compositional Attention Networks for Machine Reasoning\n[3] Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks, Tanmay Gupta, Kevin Shih, Saurabh Singh, Derek Hoiem", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}