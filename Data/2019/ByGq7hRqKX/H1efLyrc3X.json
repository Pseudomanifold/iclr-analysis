{"title": "Sensible model, but missing some important justification / visualization / error analysis", "review": "The authors propose a multitask model using a novel \u201cdual-attention\u201d unit for embodied question answering (EQA) and semantic goal navigation (SGN) in the virtual game environment ViZDoom. They outperform a number of baseline models originally developed for EQA and SGN (but trained and evaluated in this multitask paradigm).\n\nComments and questions on the model and evaluation follow.\n\n1. Zero-shot transfer claim:\n1a. This is not really zero-shot transfer, is it? You need to derive object detectors for the meanings of the novel words (\u201cred\u201d and \u201cpillar\u201d from the example in the paper). It seems like this behavior is supported directly in the structure of the model, which is great \u2014 but I don\u2019t think it can be called \u201czero-shot\u201d inference. Let me know if I\u2019ve misunderstood!\n1b. Why is this evaluated only for SGN and not for EQA?\n\n2. Dual attention module:\n2a. The gated attention model only makes sense for inputs in which objects or properties (the things picked out by convolutional filters) are cued by single words. Are there examples in the dataset where this constraint hold (e.g. negated properties like \u201cnot red\u201d)? How does the model do? (How do you expect to scale this model to more naturalistic datasets with this strong constraint?)\n2b. A critical claim of the paper is that the model learns to \u201calign the words in both the tasks and transfer knowledge across tasks.\u201d (Earlier in the paper, the claim is that \u201cThis forces the convolutional network to encode all the information required with respect to a certain word in the corresponding output channel.\u201d) I was expecting you would show some gated-attention visualizations (not spatial-attention visualizations, which are downstream) to back up this claim. Can you show me visualizations of the gated-attention weights (especially when trained on the No-Aux task) which demonstrate that words and objects/properties in the images have been properly aligned? Show that e.g. the filter at index i only picks out objects/properties cued by word i?\n\n3. Auxiliary objective: it seems like this objective solves most of the language understanding problem relevant in this task. Can you motivate why it is necessary? What is missing in the No-Aux condition, exactly? Is it just an issue with PPO optimization? Can you do error analysis on No-Aux to motivate the use of the Aux task?\n\n4. Minor notes:\n4a. In appendix A, the action is labeled \u201cTurn Left\u201d but the frames seem to suggest that the agent is turning right.\n4b. How are the shaded regions estimated in figs. 7, 8? They are barely visible \u2014 are your models indeed that consistent across training runs? (This isn\u2019t what I\u2019d expect from an RL model! This is true even for No-Aux..?)\n4c. Can you make it clear (via bolding or coloring, perhaps) which words are out-of-vocabulary in Table 3? (I assume \u201clargest\u201d and \u201csmallest\u201d aren\u2019t OOV, for example?)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}