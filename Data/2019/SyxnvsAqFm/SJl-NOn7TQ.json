{"title": "Limited novelty and does not provide significant improvement compared to existing approaches", "review": "This work addresses the issue of quantization for neural network, and in particular focus on Ternary weight networks. The proposed approach has two phases, the first phase performs quantization and de-quantization at certain iterations during training, where the schedule of these operations are hyperparameters specified a priori. The second phase focuses on training the scaling factor. The first phase is similar to the iterative quantization method proposed in \u201cRetraining-Based Iterative Weight Quantization for Deep Neural Networks\u201d, and differs in that this work performs the quantization and de-quantization operations more frequently.\nThis work also proposed a modified version of L2 regularization, but it\u2019s not clear how much benefit it provides compared to a regular L2 regularization. There is also a shuffling approach, but seems to provide limited improvement.\nThe experiment results in general does not provide convincing evidence that the proposed method outperforms existing approaches. For example, the ResNet-32 on CIFAR-10 result does not perform better than the one reported in \u201cTrained ternary quantization\u201d, and the ImageNet result is also worse than some existing works.\nThe work is lack of novelty and the results do not show significant improvement over existing approaches.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}