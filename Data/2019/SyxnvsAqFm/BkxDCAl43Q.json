{"title": "An effective training methods for network quantization  was proposed, however, the claimed contribution has been well justified.", "review": "This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1)  the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where  the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. \n\nThis paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.\n\nIn the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to  analyze the reason. \n\nIn addition, as claimed from the introduction, the contribution of this paper was  to reduce the overhead of expensive quantization. However, no experimental results on computation time  and parameter size have been shown. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}