{"title": "This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning, but no definite answers are given.", "review": "This paper contains various numerical experiments to see the effects of some heuristics in reinforcement learning. Those heuristics include reward clipping, discounting for effective learning, repeating actions, and different network structures. However, since the training algorithms also greatly affect the performance of RL agents, it seems hard to draw any quantitive conclusions from this paper.\n\nDetailed comments:\n\n1. It seems that actor-critic algorithms are defined for RL with function approximation. What is the tabular A2C algorithm? A reference in Section 3.1 would be better.\n\n2. This paper claims to study the \"inductive biases\", which is not clearly defined. How to quantify those biases and how to measure \"generality\"?\n\n3. Are there any quantitive conclusions that can be drawn from the experiments?\n\n4. Since the performance of RL agents also relies on initialization and the training algorithms. There are a lot of tricks of optimization for deep learning. How to measure the \"inductive biases\" by ruling out the effects of training algorithms?\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}