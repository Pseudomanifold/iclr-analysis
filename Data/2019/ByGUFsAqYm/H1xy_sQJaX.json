{"title": "Interesting idea but stronger supporting theory and more clarity are needed", "review": "The paper tries to provide an explanation for a memorization phenomenon observed in convolutional autoencoders. In the case of memorization, the autoencoder always outputs the same fixed image for any input image, even when the input image is random noise. The authors provide an empirical analysis that connects such a phenomenon to strides in convolutional layers of the autoencoder. Then, a possible theoretical explanation is given in the form of conjecture with some empirical evidence.\n\nThe paper presents very interesting idea, however presentation and theoretical foundation can be significantly improved.\n\n- Please elaborate on how different initializations influence memorization effect. Currently the paper only mentions initialization approaches for which memorization can or cannot occur without going into deeper analysis.\n- Having linear operator extraction described in the paper somehow breaks the flow, please consider moving it to Appendix.\n- The comment after the Proposition section is not very clear. What does it mean that the Proposition does not imply that A_X must obtain rank which is given in the Conjecture? Please explain how is Proposition providing any theoretical support for Conjecture then.\n\n- Minor comments\n1. \u201c2000 iteration\u201d -> \u201c2000 iterations\u201d\n2. The text says \u201cNetwork ND trained on frog image\u201d while the following next sentence says that \u201cthe network reconstructed the digit 3\u201d. Please clarify.\n3. \u201cNetwork ND reconstructed the digit 3 with a training loss of 10^-4 and Network ND with loss 10^-2\u201d. It seems that one of these should be \u201cNetwork D\u201d.\n4. \u201c(with downsamling)\u201d ->  \u201c(with downsampling)\u201d", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}