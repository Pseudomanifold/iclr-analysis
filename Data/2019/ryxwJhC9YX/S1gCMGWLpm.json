{"title": "well-written paper, nice method, somewhat limited results+evaluation", "review": "This paper does unpaired cross-domain translation of multi-instance images, proposing a method -- InstaGAN -- which builds on CycleGAN by taking into account instance information in the form of per-instance segmentation masks. \n\n=====================================\n\nPros:\n\nThe paper is well-written and easy to understand. The proposed method is novel, and does a good job of handling a type of information that previous methods couldn\u2019t.\n\nThe motivation for each piece of the model and training objective is clearly explained in the context of the problem. Intuitively seems like a nice and elegant way to take advantage of the extra segmentation information available.\n\nThe results look pretty good and clearly compare favorably with CycleGAN and other baselines. The tested baselines seem like a fair comparison -- for example, the model capacity of the baseline is increased to compensate for the larger proposed model.\n\n=====================================\n\nCons / suggestions:\n\nThe results are somewhat limited in terms of the number of domains tested -- three pairs of categories (giraffe/sheep, pants/skirts, cup/bottle).  In a sense, this is somewhat understandable -- one wouldn\u2019t necessarily expect the method to be able to translate between objects with different scale or that are never seen in the same contexts (e.g. cups and giraffes). However, it would still have been nice to see e.g. more pairs of animal classes to confirm that the category pairs aren\u2019t the only ones where the method worked.\n\nRelatedly, it would have been interesting to see if a single model could be trained on multiple category pairs and benefit from information sharing between them.\n\nThe evaluation is primarily qualitative, with quantitative results limited to Appendix D showing a classification score. I think there could have been a few more interesting quantitative results, such as segmentation accuracy of the proposed images for the proposed masks, or reconstruction error. Visualizing some reconstruction pairs (i.e., x vs. Gyx(Gxy(x))) would have been interesting as well.\n\nI would have liked to see a more thorough ablation of parts of the model. For example, the L_idt piece of the loss enforcing that an image in the target domain (Y) remain identical after passing through the generator mapping X->Y. This loss term could have been included in the original CycleGAN as well (i.e. there is nothing about it that\u2019s specific to having instance information) but it was not -- is it necessary?\n\n=====================================\n\nOverall, while the evaluation could have been more thorough and quantitative, this is a well-written paper that proposes an interesting, well-motivated, and novel method with good results.\n\n\n==========================================================================\n\nREVISION\n\nThe authors' additional results and responses have addressed most of my concerns, and I've raised my rating from 6 to 7.\n\n> We remark that the identity mapping loss L_idt is already used by the authors of the original CycleGAN (see Figure 9 of [2]). \n\nThanks, you're right, I didn't know this was part of the original CycleGAN. As a final suggestion, it would be good to mention in your method section that this loss component is used in the original CycleGAN for less knowledgeable readers (like me) as it's somewhat hard to find in the original paper (only used in some of their experiments and not mentioned as part of the \"main objective\").", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}