{"title": "Nice motivation and impressive empirical result. More experiments needed.", "review": "This paper studies training adversarially robust models without generating adversarial examples online during training. This way, the training time can be significantly reduced. Firstly, the authors draw inspiration from linear (approximation of) classifier and work out the epsilon_L formula. Based on this, the authors proposes three ways of making the model more robust. And specifically, the authors focus on label smoothing and logit squeezing, because both of them have an effect of reducing the logit gap and shrinking the logit. Coupled with Gaussian Noise trick, both methods work pretty well. On MNIST, it come, to some extent, close to PGD7 trained models. On CIAFR10, using aggressive smoothing or squeezing hyper-parameters, it surpasses PGD7 trained models.\n\n1. On Table 3, I would like to see PGD-100 steps and PGD-200 steps for xent loss and CW loss.\n\n2. From Table 2,3,4, it seems like the accuracy varies a lot when the hyper-parameter changes. Is there a good rule of thumb on how to set up a good hyper-parameters in case of a new dataset? \n\n3. Have you checked if the accuracy goes down as the perturbation budget epsilon goes up, say 2, 4, 6, 8, 10, 12, using PGD20 or stronger attacks?\n\n4. I would like to see results on CIFAR100, which is a harder dataset, containing 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for empirical justification nowadays (maybe it is enough one year ago). I don't know how the accuracy becomes under aggressive \\alpha and \\beta settings for 100-way classification. Also, evaluate them using PGD20, PGD100, PGD200 in order to make rigorous claims.\n\n5. It seems that Gaussian trick is the key to good performance. Have you tried other form of random noise? Uniform or Laplace?\n\n####### post-rebuttal\n\nI have read the authors' reply, and other relevant comments in this page.  I am satisfied with the rebuttal. The weak point of this paper is 1) building blocks are all existing techniques so the overall method looks like tricks; 2) lack of explanation and understanding of why it works.  But I think this is a good empirical paper. It is true that nowadays there are many papers on this topic coming up, sometimes making contradictory claims (by using different experiment setup), but I am convinced by the results in the paper and in the rebuttal. Examining and running the released code would be better but that would take me too much time and effort.  I think the results are interesting and impressive. It tells the community something new. An important future study would be understanding why it works, and if (or how) it may implicitly inherently relate to adversarial training or regularization or other training methods for defense. Also, extending to ImageNet1000 would be very important. Given the CIFAR100 results, I am worried that smoothing or squeezing alone may not be readily working.  \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}