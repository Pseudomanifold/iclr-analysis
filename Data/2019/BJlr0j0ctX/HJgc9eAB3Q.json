{"title": "Interesting results on CIFAR-10, but how conclusive are they?", "review": "The paper addresses a highly relevant problem in Adversarial Machine Learning: devising alternatives to adversarial training that scale to high-dimensional datasets.\n\nThe proposed approach is a combination of known techniques: label smoothing, logit squeezing and Gaussian data augmentation. While those techniques had previously been studied in isolation, the authors argue that the combination thereof \"saves the day\", i.e. yielding high accuracy on clean data samples while exhibiting robustness against adversarial inputs.\n\nThe authors evaluate their approach on the MNIST and CIFAR-10 datasets. They compare against the adversarially trained and publicly available models by Madry et al (ICLR 2018). The results are mixed: on MNIST, Madry's model clearly outperforms the proposed approach in terms of adversarial robustness. On CIFAR-10, for a certain combination of logit squeezing and Gaussian data augmentation, the authors report results suggesting that their approach outperforms Madry's model both in terms of accuracy on clean samples and robustness against adversarial samples (white- and black-box). Specifically, the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 achieves 90.5% accuracy on clean test samples (compared to Madry's 87.3%), 49.7% under white-box attacks (compared to 45.8%), and 67.0% under black-box attacks (compared to 64.2%).\n\nOverall, a lot of the material in this manuscript didn't strike me as particularly deep or original. For instance, I am not convinced by the analysis \"What does adversarial training do?\" (Section 1) which is based on a linearity assumption, \nthe importance of which isn't properly discussed.\n\nThe proposed alternative to adversarial training seems a bit unprincipled: the building blocks had been there before, but it appears the combination with specific parameter choices does the trick. It is a bit disconcerting that the approach doesn't perform that well for MNIST. A key selling point of the proposed approach is its computational efficiency; from that regard it would have been nice to see results on large-scale datasets like ImageNet. \n\nHaving said all this, if the findings on CIFAR-10 indeed hold true, then I think this is an important result that is worth publishing at ICLR. It would indicate important directions for future research besides the mainstream work on adversarial training.\n\nOne concern that I have though is whether the CIFAR-10 models trained by the authors can withstand a wider range of attacks. To put it into perspective: Madry et al - in their CIFAR-10 challenge (https://github.com/MadryLab/cifar10_challenge) - report 44.71% accuracy under white-box attacks as the worst-case result *under any epsilon-bounded attack* that researchers have tried since this model was published about a year ago. The 49.7% accuracy for the (alpha=0, beta=0, sigma=30, k=160k)-model in Table 3 is the worst-case result under only 4 different attacks. One has to question: what would the worst-case result for this model be in one year from now if the community had white-box access to it?\n\nIs there any way the authors could make their models publicly available during the review period?\n\nOne specific question: Madry's 46.8% accuracy under the CW attack was obtained for 30-step PGD. Table 3 states that only 20-step PGD was performed. Can the authors shed more light on what exact setup they used for the CW attack and what - if not reported in Table 3 - was the result under a 30-step attack?\n\nA minor comment:\n- Figure 2 (a) vs Footnote 1: was 20- or 40-step PGD being used?", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}