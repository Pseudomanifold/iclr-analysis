{"title": "Learning shallow Hawkes kernels using wavelets", "review": "This paper centers around efficient estimation of the kernel function for the Hawkes process and relaxation of the \u201clinearity\u201d assumption in the original Hawkes process. They rely on a classical sparse generalized linear model using the wavelet basis set and Hawkes loss function to estimate a shallow kernel function. This approach is opposite to the deep function estimation approach which does not rely on a predefined basis set [e.g. see [Du et al, 2016]]. However, it can have an advantage that the learned functions are interpretable, thought the authors never demonstrate it in the paper.\n\nGiven this view of WRNs (an unfortunate coincidence with WideResNets), we understand how LSTM2 outperforms LSTM1 in the results. However, the results tables do have peculiar numbers too. For example, why the Goodman-Kruskal gammas for H. Poisson are exactly -1? Why is it always pointing in the wrong direction? There are other observations in the results table that the authors have listed without much explanation. For example, in Section 5, what is the reason for \u201cThe WRN-PPL method excelled particularly in tasks with many target occurrences\u201d?\n\nAnother example is the arguments in the discussion section about the use-case of rate functions. For example, the authors state: \u201c For example, the rate prediction for the individual denoted in green in Figure 5 (right) suggests that individual may have skipped, missed, or rescheduled 5 to 6 appointments over the last decade.\u201d How did the authors conclude this claim? What is the clinical significance of missing or rescheduling 5-6 appointments in the context of A1c prediction?\n\nWriting can be seriously improved (basically the paper is not ready in the current state). For example, only in Section 6, the authors have introduced the full name of WRN-PPL after using it many times before. \n\nThe motivation for this paper is misleading. There have been several works on \u201cDeep Cox\u201d and \u201cDeep Hawkes\u201d models. I don\u2019t see the novelty in the authors\u2019 contribution in defining the clinical risk. Especially Fig. 3 (left) is already known and does not add much value.\n\nOverall, on the positive side, this paper shows that in some datasets going back to the classical shallow models we might achieve better performance than the alternative deep models. Unfortunately, the authors do not clearly state how many training data points they have. They have a vague statement: \u201c798,818 timestamped events in a study population of 4,732 individuals\u201d, but it does not say exactly how many training examples they have.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}