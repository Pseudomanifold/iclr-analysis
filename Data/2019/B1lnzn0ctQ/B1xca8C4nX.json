{"title": "ALISTA - Review", "review": "The paper describes ALISTA, a version of LISTA that uses the dictionary only for one of its roles (synthesis) in ISTA and learns a matrix to play the other role (analysis), as seen in equations (3) and (6). The number of matrices to learn is reduced by tying the different layers of LISTA together.\n\nThe motivation for this paper is a little confusing. ISTA, FISTA, etc. are algorithms for sparse recovery that do not require training. LISTA modified ISTA to allow for training of the \"dictionary matrix\" used in each iteration of ISTA, assuming that it is unknown, and offering a deep-learning-based alternative to dictionary learning. ALISTA shows that the dictionary does not need to change, and fewer parameters are used than in LISTA, but it still requires learning matrices of the same dimensionality as LISTA (i.e., the reduction is in the constant, not the order). If the argument that fewer parameters are needed is impactful, then the paper should discuss the computational complexity (and computing times) for training ALISTA vs. the competing approaches.\n\nThere are approaches to sparse modeling that assume separate analysis and synthesis dictionaries (e.g., Rubinstein and Elad, \"Dictionary Learning for Analysis-Synthesis Thresholding\"). A discussion of these would be relevant in this paper.\n\n* The intuition and feasibility of identifying \"good\" matrices (Defs. 1 and 2) should be detailed. For example, how do we know that an arbitrary starting W belongs in the set (12) so that (14) applies? \n* Can you comment on the difference between the maximum entry \"norm\" used in Def. 1 and the Frobenius norm used in (17)?\n* Definition 3: No dependence on theta(k) appears in (13), thus it is not clear how \"as long as theta(k) is large enough\" is obtained. \n* How is gamma learned (Section 2.3)?\n* The notation in Section 3 is a bit confusing - lowercase letters b, d, x refer to matrices instead of vectors. In (20), Dconv,m(.) is undefined; later Wconv is undefined.\n* For the convolutional formulation of Section 3, it is not clear why some transposes from (6) disappear in (21).\n* In Section 3.1, \"an efficient approximated way\" is an incomplete sentence - perhaps you mean \"an efficient approximation\"?. Before (25), Dconv should be Dcir? The dependence on d should be more explicitly stated.\n* Page 8 typo \"Figure 1 (a) (a)\".\n* Figure 2(a): the legend is better used as the label for the y axis.\n* I do not think Figure 2(b) verifies Theorem 1; rather, it verifies that your learning scheme gives parameter values that allow for Theorem 1 to apply (which is true by design).\n* Figure 3: isn't it easier to use metrics from support detection (false alarm/missed detection proportions given by the ALISTA output)?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}