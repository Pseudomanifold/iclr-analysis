{"title": "important theoretical contribution to unrolling literature", "review": "The paper raises many important questions about unrolled iterative optimization algorithms, and answers many questions for the case of iterative soft thresholding algorithm (ISTA, and learned variant LISTA). The authors demonstrate that a major simplification is available for the learned network: instead of learning a matrix for each layer, or even a single (potentially large) matrix, one may obtain the matrix analytically and learn only a series of scalars. These simplifications are not only practically useful but allow for theoretical analysis in the context of optimization theory. On top of this seminal contribution, the results are extended to the convolutional-LISTA setting. Finally, yet another fascinating result is presented, namely that the analytic weights can  be determined from a Gaussian-perturbed version of the dictionary. Experimental validation of all results is presented.\n\nMy only constructive criticism of this paper are a few grammatical typos, but specifically the 2nd to  last sentence before Sec 2.1 states the wrong thing \"In this way, the LISTA model could be further significantly simplified, without little performance loss\"\n...\nit should be \"with little\".\n", "rating": "10: Top 5% of accepted papers, seminal paper", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}