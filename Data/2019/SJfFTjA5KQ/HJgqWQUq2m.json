{"title": "Interesting ideas but unclear exposition ", "review": "The authors make connections between RNN dynamics and those of a class of ODEs similar to RNNs (ORNN) that has different orders of nonlinearity and order of gradients in time. They show that typical RNN architectures can be described as members of the ORNN family. They then make the connection that quantum mechanical systems can be described as following the Schrodinger equation which can be cast as a series of coupled 1st order ODEs of the evolution of wavefunctions under an influencing Hamiltonian. They then claim that these discretized equations can be represented by a RNN similar to a unitary RNN. They go on to outline a RNN structure inspired by this insight that has time-dependent activations to increase the scale of temporal dependence. \n\nThe main challenge of this paper is that it does not present or support its arguments in a clear fashion, making it difficult to judge the merit of the claims. Given the nuance required for their arguments, a more robust Background section in the front that contextualizes the current work in terms of machine learning nomenclature and prior work could dramatically improve reader comprehension. Also, while the parallels to quantum mechanics are intriguing, given that the paper is arguing for their relevance to machine learning, using standard linear algebra notation would improve over the unnecessary obfuscation of Dirac notation for this audience. While I'm not an expert in quantum mechanics, I am somewhat proficient with it and very familiar with RNNs, and despite this, I found the arguments in this paper very hard to decipher. I don't think this is a necessity of the material, as the URNN paper (http://proceedings.mlr.press/v48/arjovsky16.pdf) describes very similar concepts with a much clearer presentation and background. \n\nFurther, despite claims of practical benefits of their proposed RNN structure, (reduced parameter counts required to achieve a given temporal correlation), no investigations or analyses (even basic ones) are performed to try and support the claim. For example, the proposed scheme requires a time varying weight matrix, which naively implemented would dramatically grow the parameter count over a standard LSTM. I can understand if the authors prefer to keep the paper strictly a theory paper, but even the main proof in Theorem 4 is not developed in detail and is simply stated with reference to the URNN paper. \n\nThere are some minor mistakes as well including a reference to a missing Appendix A in Theorem 3, \"Update rule of Eq. (15)-(15)\", \"stble regime\". Finally, as a nit, the claim of \"Universal computing\" in the name, while technically true like other neural networks asymptotically, does not seem particularly unique to the proposed RNN over others, and doesn't provide much information about the actual proposed network structure, vs. say \"Quantum inspired Time-dependent RNN\".", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}