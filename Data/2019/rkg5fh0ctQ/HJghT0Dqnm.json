{"title": "The basic idea is not fully original, but the task is important and experiments are clear and complete.", "review": "This paper focuses on dealing with a scenario where there are \"unseen\" intents or slots, which is very important in terms of the application perspective.\n\nThe proposed approach, TSSM, tries to form the embeddings for such unseen intents or slots with little training data in order to detect a new intent or slot in the current input.\nThe basic idea in the model is to learn the representations of utterances and intents/slots such that utterances with the same intents/slots are close to each other in the learned semantic space.\nThe experiments demonstrate the effectiveness of TSSM in the few-shot learning scenarios.\nThe idea about intent embeddings for zero-shot learning is not fully original (Chen, et al., 2016), but this paper extends to both intent classification and slot filling. \n\nThe paper tests the performance in different experimental settings, but the baselines used in the experiments are concerned.\nThis paper only compares with simple baselines (MaxEntropy, CRF, and basic DNN), but there should be more prior work or similar work that can be used for comparison in order to better justify the contributions of the model.\nIn addition, this paper only shows the curves and numbers in the experiments, but it is better to discuss some cases in the qualitative analysis, which may highlight the contributions of the paper.\nAlso, in some figures of Fig. 2, the proposed TSSM is not better than DNN, so adding explanation and discussion may be better.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}