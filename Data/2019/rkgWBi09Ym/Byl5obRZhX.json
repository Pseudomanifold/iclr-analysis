{"title": "a natural idea (combine GAN and Mixture of Gaussian)", "review": "To improve the synthetic image quality of GAN, the paper presents a method by combining GAN and Mixture of Gaussian (MOG), termed GM-GAN. In addition, a new scoring measurement is proposed by considering the diversity vs. quality of the generated samples. \n\nProbs:\n1.\tIt is a promising topic to improve the synthetic image quality \n2.\tIt is also promising to combine MOG and GAN.\n\nCons.\n1.\tMaybe the most ``tough\u2019\u2019 question is that how about the advantages of the proposed method over the recent BigGAN in terms of the generated data. \n2.\tthe difference should be further highlighted with VAE-GAN and its variants, as well as , Probabilistic Generative Adversarial Networks; \n3.\tCould the proposed method be verified on a higher resolution image dataset? The current used images are too small considering recent developments. \n4.\tBesides generated mnist (fig.4), the illustration on some more challenging data sets should be given, as well as some failed examples. \n5.\tAs you use the label to train the GM-GAN (alg 1), is it fair to compare the method with other unsupervised clustering approaches? i.e. the label has been implicitly/explicitly used for clustering. Moreover, pls also clarify step 7-8 in alg 2.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}