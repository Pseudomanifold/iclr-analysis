{"title": "nice results but limited novelty", "review": "The paper presents a RL approach for sequential feature acquisition in a budgeted learning setting, where each feature comes at some cost and the goal is to find a good trade-off between accuracy and cost. Starting with zero feature, the model sequentially acquires new features to update its prediction and stops when the budget is exhausted. The feature selection policy is learned by deep Q-learning. The authors have shown improvements over several prior approaches in terms of accuracy-cost trade-off on three datasets, including a real-world health dataset with real feature costs.\n\nWhile the results are nice, the novelty of this paper is limited. As mentioned in the paper, the RL framework for sequential feature acquisition has been explored multiple times. Compared to prior work, the main novelty in this paper is a reward function based on better calibrated classifier confidence. However, ablations study on the reward function is needed to understand to what extent is this helpful.\n\nI find the model description confusing. \n1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text.\n2. What is the reward function? Only immediate reward is given.\n3. What is the state representation? How do you represent features not acquired yet?\n\nIt is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better. There are at least 3 improvements: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Right now not clear which one gives the most improvement.\n\nOverall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}