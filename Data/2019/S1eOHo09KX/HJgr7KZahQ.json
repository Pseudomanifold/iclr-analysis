{"title": "Interesting approach with a confused exposition", "review": "I like the approach, however: I consider the paper to be poorly written.  The presentation needs to be improved for me to find it acceptable.\n\nIt presents a stream-oriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm.  This is particularly critical in this area because feature acquisition costs during the \"warm-up\" phase are actual costs, and given the inherent sample complexity challenges of reinforcement learning, I would expect them to be significant in practice.  This would be fine if the setup is \"we have a fixed offline set of examples where all features have been acquired (full cost paid) from which we will learn a selector+predictor for test time\".\n\nThe algorithm 1 float greatly helped intelligibility, but I'm left confused.  \n  * Is this underlying predictor trained simultaneously to the selector?  \n        * Exposition suggests yes (\"At the same time, learning should take place by updating the model while maintaining the budgets.\"), but algorithm block doesn't make it obvious.\n        * Maybe line 21 reference to \"train data\" refers to the underlying predictor.\n  * Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but:\n        * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and \n        * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but\n        * this is not discussed at all.\n\nAlso, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the\nalgorithm is not really an online algorithm).  The experiments are all silent on the \"exploration\" feature acquisition cost.  Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0?\n\nI also found the following disturbing: \"It is also worth noting that, as the proposed method is\nincremental, we continued feature acquisition until all features were acquired and reported the average\naccuracy corresponding to each feature acquisition budget.\"  Does this mean the underlying predictor was trained on data \nthat it would not have if the budget constraint were strictly enforced?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}