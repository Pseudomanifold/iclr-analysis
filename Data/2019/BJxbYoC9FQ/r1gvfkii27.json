{"title": "Issues with novelty and improvements in relation to prior works", "review": "This paper focuses on the extraction of high-quality model-agnostic saliency maps. The authors argue that when an extracted saliency map is directly dependent on a model, then it might not be useful for a different classifier and thus not general enough. To overcome this problem, they consider all the possible classifiers weighted by their posterior probabilities. This problem cannot be solved explicitly, and the authors suggest a scheme to approximate the solution using two networks. That is, pretrain an initial classifier and then, following an adversarial training procedure, one network is trying to confuse the classifier and the other one to maximize its accuracy. Using this formulation, the authors report state-of-the-art results for salience map extraction.\n\nSUMMARY/OVERALL COMMENTS\nThe authors present a simple and effective way to produce classifier-agnostic saliency maps. The argument for the approach is well justified and the results seem convincing on a first read. However, the novelty of the method is a concern given the previous work of Fan et al. (2017), and the manuscript is not upfront about the differences between the two works. The experiments are another cause for concern: Fan et al. should have been tested as a baseline with similar implementation (controlling for architecture and \\lambda), and implementation differences in prior works of Table 1 make it difficult to draw conclusions. \n\n\nRELATED WORKS\n* In the introduction, the authors mention related works but fail to mention the work of Fan et al. (2017) which is clearly the most relevant. The first mention of Fan et al is on page 4 in a very specific discussion the regularization coefficient. The problem formulation in Section 2 and the approach is Section 3 is largely borrowed from Fan et al but not acknowledged until the last page. This introduces bias and confusion to the reader in regards to the novelty of the approach. Please, mention the work of Fan et al. (2017) in the introduction and clearly delineate the differences in the works earlier in the text. (--)\n\n* Du et al. (2018), \u201cTowards Explanation of DNN-based Prediction with Guided Feature Inversion\u201d, use the VGG models for saliency map extraction and achieve a LE of 38.2. Note that Du et al. (2018), suggest that this modification could lead to SOTA results. I would like to see a comparison with this method. (-)\n\n* The work of Kindermans, et al. (2017), \u201cLearning how to explain neural networks: PatternNet and Pattern Attribution\u201d, although they do not aim for weakly supervised localization and thus, they do not present the LE, they produce saliency maps. I would like to see a LE comparison with that method. (minor -)\n\n* In the introduction, p1 (last paragraph) other methods are briefly mentioned (Extracted saliency maps show all the evidence\u2026.superpixels), etc.) without references. Please add references when needed. (-)\n\n* The framework presented in this paper was first proposed by Fan et al. (2017). The authors claim four main differences in their approach. In my eyes, not all of them are major or novel - probably the most impactful is removing superpixels as it simplifies the problem and implementation. (+)\n\n\nAPPROACH\n* The authors aim for simplicity (strong +)\n\n* The authors justify their approach and present their arguments clearly (strong ++)\n\n* In the algorithm section, the authors first mention the sampling procedure and then their motivation. Please alter the ordering of these to be conceptually easier to understand your approach\n\n* In the first sentence after the equation 6 I guess that \u201c(cf. Alg. 1)\u201d is a typo and should be modified to (Alg. 1)\u201d\n\n* After Equation 6 it is argued that the method resembles the training procedure of GANs (Godfellow et al., 2014) but not the work of Fan et al., 2017. (--)\n\n\nEXPERIMENTS\n* The authors define as their baseline the F thinning strategy (i.e. use only the first classifier) which is a model dependent salience map. While this is a useful comparison against the classifier dependent methods, given the similarity to the work of Fan et al. (2017), experiments comparing the proposed model to Fan et al. are necessary. It is important to control for network architecture (ResNet-50) and choice of \\lambda to properly determine if the four changes outlined in Section 6 result in any real improvement over Fan et al. (strong --)\n\n* The authors use the Table 1 (borrowed from Fong and Vedaldi (2017)) to compare their results against other methods. This comparison is problematic as different approaches are using different models as classifiers which may lead to increase or decrease of the LE. (--)\n\n* In Table 2 the authors do not report how many times they run the same experiments to get these values. They also run less experiments with non-shared weights and they report only the LE. In my eyes it looks that the authors are trying to force their argument that the sharing weights helps (probably because it is one of their novelties). Please report the statistics of your experiments and fill the empty entries in the table. (--)\n\n* In Table 3, what does the last row represent?\n\n* Table 1 errors: (1) You write \u201cLocalization evaluation using OM, LE and F1 scores\u201d. Please remove the F1 score as you do not report it. Also, correct the first sentence of the \u201cLocalization\u201d subsection which states that you use three different metrics to \u201ctwo different metrics\u201d. (2) The LE from Fong and Vedaldi (2017) should be 43.2 and not 43.1.\n\n* Regarding the unseen classes (section 5): (1) Please report in the appendix the classes that you are using in each subset. Are there classes correlated? (-)  (2) I see that there is a strong correlation between the LE on subset A and E. It looks like you are training on E and you generalize on A.\n\n\nNOVELTY/IMPACT\n* Novelty is a strong concern, given the work of Fan et al. (2017) (strong --). Nevertheless, the authors propose some changes that can be seen as more general, but the effectiveness of the changes is clearly established.\n\n* This paper\u2019s strongest point is the simplicity (conceptually and implementation-wise) of the method, an advantage over previous works (+)\n\n\nOTHER COMMENTS\n* Fan et al. (2017), use an adaptive \u03bb that pushes the mask to 10% of the image whereas you are using a fixed one that pushes the mask to approximately 50% of the image. How can you make sure that this is not the reason that you are getting better results?\n\nIf the authors can clearly and fairly demonstrate that the changes they propose over Fan et al (2017) result in improved performance, and the manuscript is adjusted to be more upfront about this prior work, I would consider increasing my rating.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}