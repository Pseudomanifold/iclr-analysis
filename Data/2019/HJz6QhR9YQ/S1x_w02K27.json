{"title": "Tackles an interesting problem, but fails to convincingly solve it", "review": "This paper proposes an algorithm for competitive games, based on the idea of learning then combining multiple sub-policies (each trained to counter a specific opponent). More precisely, in a first phase these sub-policies are trained in a sequential manner, each specialized by learning to beat one specific policy drawn from the pool of opponent policies, then being added to this pool afterwards. Once enough sub-policies have been obtained, a \u201cmeta\u201d agent is trained by self-play, where at each time step the actions available to this agent are the trained sub-policies (i.e. the chosen sub-policy is queried to compute the actual move performed by the agent). Experiments on iterated (two-round) Rock-Paper-Scissor (RPS) show that the meta-agent ends up being less exploitable than agents trained directly with \u201cconventional\u201d self-play. Other experiments with the Generals.io strategy game confirm this finding, also showing the meta-agent\u2019s superiority over \u201cthe strongest open-sourced scripted bot on the Generals.io online leaderboard\u201d (named FloBot).\n\nAt high level, this paper tackles an interesting research direction, namely \u201cexploring game-playing strategies that intentionally avoid the equilibrium solution and instead \u2018learn to exploit\u2019\u201d, so as to reach \u201cmore capable, adaptable, and ultimately more human-like artificial agents\u201d. The specific two-phase algorithm proposed here (consisting in first training specialized sub-policies sequentially, then a meta-agent to combine them), is new to the best of my knowledge. However, there exists previous work on training a meta-agent based on a set of sub-policies, and none of it is mentioned here. For instance, in the case of strategy games, the paper \u201cRock, Paper, StarCraft: Strategy Selection in Real-Time Strategy Games\u201d and references within investigate several related approaches.\n\nUnfortunately, the paper is clearly not ready for publication in its current form, given the many presentation issues. Besides spelling and grammar errors, there are missing items (ex: citations, the t-SNE map mentioned in 4.2, what are \u201cachievement rewards\u201d and the associated \u201cexpand_X\u201d agents in 4.3, ...). Overall the paper is somewhat confusing even if the proposed technique seems rather simple. Algorithm 1 in particular is not clear (e.g. a single pi^i is sampled? What does \u201cUpdate theta to theta\u2019 of Player 1\u201d mean, especially considering that both players play with the same shared parameters theta according to the previous line? How exactly would you solve Phase 2 with Linear Programming in the general case?). There are also zero details on experiments (network architectures, hyper-parameters, etc.) \u2014 at least the authors promise to share the code, but it is not available yet.\n\nMore fundamentally, I feel like the proposed technique does not live up to the expectations raised by the introduction. This research is motivated by how humans are able to adapt to varied opponents so as to learn and exploit them. However, here, even if we train varied \u201cexploitative\u201d strategies, the end result is a meta-agent that does not try to adapt to its opponent, instead its goal is to combine the sub-strategies to (ideally) reach the Nash equilibrium. As a result, the benefits of learning sub-policies in the first place becomes somewhat unclear. If we consider the RPS experiments, I wonder why the proposed technique (dubbed HASP) is able to reach a strategy near the Nash equilibrium while \u201cconventional\u201d self-play (which by the way is not described here) is not. Indeed, since each sub-policy is specialized to beat a single opponent, it should be close to deterministic, in which case choosing a sub-policy (the action of the meta-agent) should essentially be the same as directly choosing a move. The authors mention they had trouble with PPO converging too fast to deterministic strategies, but why is it not happening with HASP? Also, note that it can be helpful to train an agent against old copies of itself to stabilize training in self-play (see e.g. \u201cEmergent Complexity via Multi-Agent Competition\u201d, or \u201cBeating the World's Best at Super Smash Bros. Melee with Deep Reinforcement Learning\u201d): this does not seem to be done here, and it might be one reason why \u201cconventional\u201d self-play is not working well. Finally (as far as RPS is concerned) iterating on two rounds does not seem very \u201cinteresting\u201d in the sense that there is little opportunity to adapt to your opponent over only two rounds (if that was the goal). In the Generals.io game I can better see how training sub-policies may be beneficial, since there are fewer policies than moves available (thus the action space is reduced compared to directly training an agent by self-play). However the paper does not explicitly mention this point, and in my opinion does not convincingly show that this is indeed helping \u2014 especially due to my doubts about how the \u201cconventional\u201d self-play agent has been trained (note also that the random ensemble works as well as HASP, and that the conventional self-play agent performs much better than HASP against FloBot, and that there are no direct head-to-head match statistics between these agents, all of which raises some flags).\n\nAnother high-level concern I have regarding the methodology is that the \u201csub-policy discovery\u201d mechanism (first phase of the algorithm) seems pretty basic. If for instance strategy B beats A, C beats B, and A beats C, the algorithm may cycle between these three strategies without ever identifying other ones that may exist: there is no explicit mechanism encouraging novelty (or detecting such cycles to know when to stop: as far as I can tell the number of sub-policies being trained is a hyper-parameter that must be set by hand). The \u201ccoverage\u201d of the space of strategies the sub-policies can beat may thus be lacking, depending on the specific application. This limitation is not discussed at all in the paper.\n\nRegarding more specific details, there are too many small presentation issues for me to list them all, but here are a few representative samples:\n- Please use \u201ctile\u201d or \u201ccell\u201d to denote individual elements in the grid (not \u201cgrid\u201d itself)\n- It is unclear how states are represented as inputs to the model for Generals.io\n- The 77% win rate against FloBot seems to be an important achievement of the paper since it is mentioned in the abstract and introduction... but not in experiments (!)\n- It is not clear what exactly are the numbers in Table 4. A win rate would also be good to add there.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}