{"title": "ideas look interesting but paper is not finished.", "review": "This paper proposes a two-level hierarchical RL algorithm for games. First, a set of diverse policies is learned. These are supposed to be different possible (and potentially suboptimal) strategies. Then, a higher-level policy is trained to select the lower-level policy to be played. The approach is evaluated on rock-paper-scissors and a partially observable game.\n\nOn the positive side, I think the idea of building a set of strategies and trying to adapt the strategies picked based on the observation is an interesting idea. I also think the chosen game looks like a good experimental setting that allows to experiment quickly.\n\nHowever, I feel that the paper is currently too far for publication to consider acceptance.\n\nI was not able to follow the description of the algorithm. Sections 3.2 and 3.3 describe the general idea of the algorithm but I could not find a sufficient definition of some key notions. For example and unless I missed something, section 3.2 introduces the term \u201cachievement reward\u201d but contains neither a description nor an example. Furthermore, I could not really follow the Counter Self-Play algorithm and I am not sure I could reimplement it. Having a pseudo-algorithm would greatly help. It could go in an appendix if space is a concern. I assume Algorithm 1 describes the algorithm of section 3.3 as they have the same title but as Algorithm 1 is not mentioned in the text before section 4.3 I am not sure. I would also suggest introducing the notation of the algorithm in the body of the text. I also checked the paper website whose link is provided in the paper but it seemed to be empty. In any case, I think the paper should be self-contained. I also had trouble following the experimental section 4.3 Generals.IO.\n\nThere are also a few missing references in the paper, for example for the PPO algorithm, or in page 6. There seem to be a missing figure, I could not find what the following sentence refers to \"We provide a t-SNE projection of the learned policies onto the 2D plane below\".\n\nFor the reasons listed above it is a bit hard to comment on the content of the paper. Nevertheless, I would have liked to see how the proposed approach adapts to the policy the opponent is playing, for example by showing a difference in the distribution of the sub-policies selected. \n\nIt would also be interesting to compare the strategies learned to other methods learning a diverse set of strategies.", "rating": "2: Strong rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}