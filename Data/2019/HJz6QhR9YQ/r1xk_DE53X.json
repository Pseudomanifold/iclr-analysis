{"title": "A weak evaluation on an otherwise interesting task", "review": "The authors propose a method to learn a hierarchical reinforcement learning algorithm whose objective is to exploit weaknesses in its adversary, though also having weaknesses, itself. In order to encourage exploitation and not simply focus on winning the game, the agent is given a negative reward for every step taken during the game.\n\nThe evaluation was very unconvincing since FloBot was used in both training and testing. Also, there were no tests to demonstrate HASP was indeed exploiting weaknesses in FloBot. The frequency at which HASP selects sub-policies is not given any meaning. What are the situations in which these sub-policies are selected? How is the algorithm using them to exploit weaknesses?\n\nPros:\n- Generals.io is an interesting environment on which to test RL algorithms\n- Exploiting weaknesses instead of creating a generally powerful agent can be useful in situations where a generally powerful agent is difficult to obtain.\nCons:\n- Trained and tested on the same agent. While there may be cause for doing this in particular scenarios, the authors did not give any motivating examples.\n- No evidence is given that any weaknesses are being exploited\n- Lacks clarity\n  - Algorithm 1 was unclear\n  - How many times the achievement reward was used is not clear\n  - Only the reward is shown for Generals.io. How does this translate to win percentage?\n  - The tables were unclear, especially table 3", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}