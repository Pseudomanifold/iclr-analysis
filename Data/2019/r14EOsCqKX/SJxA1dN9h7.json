{"title": "Significance of the findings?", "review": "In this paper, authors propose a set of  control experiments in order to get a better understanding of different deep learning heuristics: stochastic gradient with restart (SGDR),  warmup and distillation. Authors leverage the recently proposed mode connectivity (which fits a simple piecewise linear curve to obtain a low loss path that connect two points in parameter space) and CCA is a way to compute a meaningful correlation of the networks activations. All the experiments are done using a VGG-16 networks on CIFAR10.\n\nFor SGDR, authors observe that the solutions found by SGDR or SGD does not appears to be in different basins. While this contradict previous claim, it goes in the same direction than recent works which  have similar observations for the small batch/large batch case [1]. Authors also identify that warmup tends to avoid large change the top-layers at the beginning of training and that you can achieve similar effect than warmup by freezing the top-layer. Finally authors show that most of the benefit of distillation happen by impacting the last deep layers of a network.\n \nWhile I find all those findings valuable, it is not straightforward to see how they connect to a better understanding of training deep network and how significant they are. In particular,  it is still unclear to me why heuristics such as SGDR is successful in practice or why freezing the top layer of a network improve trainability in a large batch setting?\n\nDoing control experiments in order to better understand the current practice in deep learning is extremely important, however, I don\u2019t think that the paper in its current shape is ready for publication. \n\n[1] Empirical Analysis of the Hessian of Over-Parametrized Neural Networks (Sagun et al., 2017).\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}