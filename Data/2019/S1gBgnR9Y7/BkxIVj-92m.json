{"title": "An empirical study with little analysis", "review": "Edit: changed \"Clarity\"\n\n[Relevance] Is this paper relevant to the ICLR audience? yes\n\n[Significance] Are the results significant? no\n\n[Novelty] Are the problems or approaches novel? no\n\n[Soundness] Is the paper technically sound? okay\n\n[Evaluation] Are claims well-supported by theoretical analysis or experimental results? marginal\n\n[Clarity] Is the paper well-organized and clearly written? no\n\nConfidence: 3/5\n\nSeen submission posted elsewhere: No\n\nDetailed comments:\n\nIn this work, the authors compare several state-of-the-art approaches for high-resolution microscopy analysis to predicting coarse labels for the outcomes of pharmacological assays. They also propose a new convolutional architecture for the same problem. An empirical comparison on a large dataset suggests that end-to-end systems outperform those which first perform a cell segmentation step; the predictive performance (AUC) of almost all the end-to-end systems is statistically indistinguishable.\n\n=== Major comments\n\nThe paper is primarily written as though its main contribution is as an empirical evaluation of different microscopy analysis approaches. Recently, there have been a large number of proposed approaches, and I believe a neutral evaluation of these approaches on datasets other than those used by the respective authors would be a meaningful contribution. However, the current paper has two major shortcomings that prevent it from fulfilling such a place.\n\nFirst, the authors propose a novel approach and include it in the evaluation. This undercuts claims of neutrality. (Minor comments about the proposed approach are given below.) \n\nSecond, the discussion of the results of the empirical evaluation is restricted almost solely to repeating in text the what the tables already show. Further, the discussion focuses only on the \u201ctop line\u201d numbers, with the exception of a deep look at the Gametocytocidal compounds screen. It would be helpful to instead (or additionally) identify meaningful trends, supported by the data acquired during the experiments. For example: (1) Do the end-to-end systems perform well on the same assays? (2) Would a simple ensemble approach improve things? if they perform well on different assays, then that suggests it might. (3) What are the characteristics of the assays on which the CNN-based approaches perform well or poorly (i.e., how representative is Figure 5)? (4) What happens when the FNN-based approach outperforms the CNN-based ones? in particular, what happens in A13? (5) How sensitive are the approaches to the number of labeled examples of each assay type? (6) Are there particular compounds which seem particularly informative for different assays?\n\nA second major concern is whether the binarized version of this problem (i.e., assay result prediction) is of interest to practitioners. In many contexts, quantitative information is also important (\u201chow much of a response do we see?\u201d). While one could imagine the rough qualitative predictions (\u201cdo we see a response?\u201d) shown here as an initial filtering step, it is hard to believe that the approach proposed here would replace other more informative analysis approaches.  \n\n=== Minor comments\n\nAre individual images from the same sample image always in only the training, validation, or testing set? that is, are there cases where some of the individual images from a particular sample image are in the training set, while others from that sample image are in the testing set?\n\nI did not find the dataset construction description very clear. Does each row in the final, 10 574 x 209 matrix correspond to a single image? Does each image correspond to a single row? For example, it seems as though multiple rows may correspond to the same image (up to four? the three pChEMBL thresholds as well as the activity comment). What is the order in which the filtering and augmenting happens? It would be very helpful to provide a coherent, pipeline description of this (say, in an appendix).\n\nDo all the images in the dataset come from the same microscope (and cell line) at the same resolution, zoom, etc.? If so, it is unclear how well this approach may work for images which are more heterogeneous. There are not very many datasets of the size described (I believe, at least) available. This may significantly limit the practical impact of this work.\n\nHow many epochs are required for convergence of the different architectures? For example, MIL-net has significantly fewer parameters than the others; does it converge on the validation set faster?\n\n=== Typos, etc.\n\nThe references are not consistently formatted.\n\n\u201cnot loosing\u201d -> \u201cnot losing\u201d\n\u201cdoesn\u2019t\u201d -> \u201cdoes not\u201d\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}