{"title": "interesting work, but slightly incremental", "review": "This paper studies the cross-entropy loss for binary classification problems. The authors show that if the norms of samples in two linear separable classes are different, gradient descent based methods minimizing cross-entropy loss may give a linear classifier that gives small margin.\n\nPros\n\n1. The paper is clearly written and very easy to follow. \n\n2. The authors show that for two point classification problems, if the norms of the points are very different then gradient descent will give a very small margin.\n\n3. Further theoretical results are given explaining the relation between cross-entropy loss and SVM.\n\n4. A new loss function called differential training is proposed, which is guaranteed to give SVM solution.\n\nCons\n\n1. My biggest concern is that, the paper, especially the title, may be slightly misleading in my opinion. Although the authors keep claiming that cross-entropy loss can lead to poor margins in certain circumstances (which I agree), in fact Theorem 1 and Theorem 2 have already clearly shown the connection between the cross-entropy solution and the maximum margin direction. For example, Theorem 1 literally proves that when the two points have the same norm (normalized data?), cross-entropy loss leads to maximum margin. Theorem 2 also clearly states that cross-entropy loss and SVM are closely related. Based on these two theorems, perhaps \u2018cross-entropy loss is closely related to maximum margin\u2019 is a more convincing statement.\n\n2. The theoretical results given in this paper is slightly incremental. As the authors mentioned, Theorem 1 and Theorem 2 are essentially already proved in previous works. The other results are not very significant either.\n\n3. The authors do not clearly state the advantages of the differential training method compared to SVM. It seems that one can just use SVM if the goal is maximum margin classifier.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}