{"title": "Insufficient novelty and significance. Also, the phrasing of the results is somewhat misleading.", "review": "Due to the large variance in reviewer scores, I was asked to give this additional review.\n\nBackground: [Soudry et al. 2018] showed that the iterates of gradient descent, when optimizing logistic regression on separable data, converge to the L2 max-margin (SVM) solution for homogeneous linear separators (without bias). These results were later extended to other models and optimization methods.\n\nThis paper has two main results:\n1)\tIt clarifies that the results of [Soudry et al. 2018] do not apply to logistic regression when the linear separator has a bias term (\u201cb\u201d). This is because the homogenous max margin solution in the extended [x,1] space is not the same as non-homogeneous max margin solution in the original space: the first has a penalty on the size of the bias term, i.e.\nmin_{w,b} ||w||^2 + b^2 s.t. y_n (w\u2019x_n+b) >= 1\n, while the latter does not: \nmin_{w,b} ||w||^2  s.t. y_n(w\u2019x_n+b) >= 1\n2)\tIt suggests using differential training to correct this issue.\n\n\nHowever, I do not believe these contributions are enough for a publication in ICLR. First, (2) is simply a combination of two known results, as mentioned by Reviewer 2. Second, though I commend the authors for pointing out (1), I do not feel this by itself warrants a publication, for the following reasons:\na) It is very simple to explain (1) in only a few lines (as I did above). Therefore, it would be more informative just to write (1) as a comment on the original paper (the ICLR 2018 forum is still open), not as a completely new publication. For me, all the numerical demonstrations and examples of this simple issue did not add much.\nb)\tRegularizing the bias term usually does not make a significant difference to the sample complexity (see the end of section 15.1.1 in the textbook \u201cUnderstanding Machine Learning: From Theory to Algorithms\u201d by Shai Shalev Shwartz.). Furthermore, the main motivation behind [Soudry et al. 2018] was to explain implicit bias and generalization in deep networks, where there such max-margin results (which penalize all the parameters) could be used to derive generalization bounds (e.g., https://arxiv.org/abs/1810.05369).\nc)\tLastly, the authors here say that \u201cthe solution obtained by cross-entropy minimization is different from the SVM solution\u201d. This (as well as the title and abstract) may mislead the readers to think there is something wrong in the proofs of [Soudry et al. 2018] and later papers, and that logistic regression does not converge to the max-margin solution for homogeneous linear separators. However, the max-margin solution for homogeneous linear separators is also called the \u201cmax margin\u201d or SVM solution (just for a different family). For example, see the previous paper on the topic [\u201cMargin Maximizing Loss Functions\u201d, Rosset et al. 2004] or section 15.1.1 in the textbook \u201cUnderstanding Machine Learning: From Theory to Algorithms\u201d by Shai Shalev Shwartz.  As I see it, the only issue in [Soudry et al. 2018] is the sentence \u201cA bias term could be added in the usual way, extending x_n by an additional \u20191\u2019 component.\" which is confusing since it cannot be applied directly to the SVM solution. The authors should aim to pinpoint this issue, and clarify their phrasing to avoid such confusions.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}