{"title": "Interesting Algorithm with Solid Theories", "review": "Authors proposes a new algorithm for improving the stability of class importance weighting estimation procedure (Lipton et al., 2018) with a two-step procedure. The reparamaterization of w using the weight shift theta and lambda allows authors develop a generalization upperbound with terms rely on theta, sigma and lambda. \n\nThe problem of label shift is a known important issue in transfer learning but has been understudied.\n\nThe paper is very well written and the algorithm is well-motivated (introducing regularization to avoid the singularity) and post processing step looks sound (using lambda to de-biase). I only have a few minor questions: \n\n1. How realistic it is to assume we have prior knowledge on theta and sigma_min? \n\n2. If I understand correctly, the only experiment where lambda is varied is Sec 3.3? It would be interesting if authors also included BBSE in Sec 3.3 as a baseline. \n\n3. The authors mentioned in the discussion that the generalization guarantee is obtained with no prior knowledge q/p is needed. However, doesn't theta implicitly represent the knowledge in p/q?   \n\n------------------------------------------------\n\nI have read authors' comments. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}