{"title": "A well written paper with a relevant contribution for imitation learning", "review": "The paper describes a new learning framework, based on generative\nadversarial imitation learning (GAIL), that is able to learn sub-tasks\npolicies from unsegmented demonstrations. In particular, it follows\nthe ideas presented in InfoGAIL, that depends on a latent variable,\nand extend them to include a sequence of latent variables representing\nthe sequence of different subtasks. The proposed approach uses a\npre-training step, based on a variational auto-encoder (VAE), to\nestimate latent variable sequences. The paper is well written and\nrelates the approach with the Options framework. It also shows,\nexperimentally, its performance against current state-of-the-art\nalgorithms.  \n\nAlthough the authors claim in the appendix that the approach is\nrelatively independent on the dimensionality of the context variable,\nthis statement needs further evidence. The approach is similar to HMMs\nwhere the number f hidden states or latent variables can make a\ndifference in the performance of the system.\n\nAlso, it seems that the learned contexts do not necessarily correspond\nto meaningful sub-tasks, as shown in the circle-world. In this sense,\nit is not only relevant to determine the \"right\" size of the context\nvariable, but also how to ensure a meaningful sub-task segmentation. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}