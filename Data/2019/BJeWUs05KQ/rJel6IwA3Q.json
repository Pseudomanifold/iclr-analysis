{"title": "Review of Directed-Info GAIL", "review": "Summary:\n\nThis paper proposes an extension over the popular GAIL method for imitation learning  for the multi-modal data or tasks that have hierarchical structure in them. To achieve that the paper introduces an unsupervised variational objective by maximizing the directed mutual information between the latents c\u2019s and the trajectories. The advantage of using directed information instead of regular MI based criterion is two-folds: 1) Being able to express the causal and temporal dependencies among the c\u2019s changing across time. 2) Being able to learn a macro-policy without needing to condition on the future trajectories. Authors present results both on continuous and discrete environments.\n\n\nQuestions: \n1) Can you give more detailed information about the hyperparameters of your model? For example how many seeds have you used?\n2) Have you tried pre-training c_t\u2019s as continuous latent variables?\n3) Have you tried pre-training your model as Variational RNN instead of VAE?\n4) Have you tried training your model on the pixels on the continuous control tasks?\n\nPros:\n* Although the approach bears some similarity to Info-GAIL approach. The idea of using directed information for GAIL is novel and very interesting. This approach can be in particular useful for the tasks that have \n* The paper is very well-written the goal and motivation of the paper is quite clear.\n\nCons:\n* Experiments are quite weak. Both the discrete and the continuous environment experiments are conducted on very simplistic and toyish tasks. There are much more complicated and modern continuous control environments such as control suite [1] or manipulation suite [2].  In particular tasks where there is a more clear hierarchy would be interesting to investigate.\n* Experimental results are underwhelming. For example Table 1, the results of the proposed approach is only barely better than the baseline.\n\n[1] https://github.com/deepmind/dm_control\n[2] Learning by Playing-Solving Sparse Reward Tasks from Scratch, M Riedmiller, R Hafner, T Lampe, M Neunert et al - arXiv preprint arXiv:1802.10567, 2018\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}