{"title": "An interesting problem, but unclear description and methodological issues ", "review": "* Summary of the paper\n\nThis paper studies how hyponymy between words can be mapped to feature representations. To this end, it lists out properties of such mappings and studies two methods from the perspective of how they address these properties.\n\n* Review\nThe goal of this paper: namely, formalizing the hypernymy relation over vector spaces is not only an interesting one, but also an important one -- being able to do so can help us understand and improve vector representations of words and reason about their quality.\n\nIn its execution, however, the paper does not seem ready for publication at this point. Two major issues stand out.\n\nFirst, several things are unclear about the paper. Here's a partial list:\n1. Property 3 is presented as a given. But why is this property necessary or sufficient for defining hyponymy?\n2. It is not clear why the measure based definition is introduced. Furthermore, the expression above the statement of property 4 is stated as following from the definition. It may be worth stating why.\n3. Section 3.1 is entirely unclear. The plots in Fig 2 are empty. And in the definition of Q on page 4, the predicate that defines the set states P(w, f(x)). But if the range of P is [0, 1], what does it mean as a predicate? Does this mean we restrict it to cases where P(w, f(x)) = 1?\n\nSecond, there are methodological concerns about the experiments.\n1. In essence, section 3 proposes to create a word-specific linear classifier that decides whether a new vector is a hypernym or not. But this classifier faces huge class imbalance issues, which suggests that simply training a classifier as described can not work (as the authors discovered). So it is not clear what we learn from this section? Especially because the paper says at the just before section 3.1 that \"we are ultimately not interested in property 5\".\n2. Perhaps most importantly, the method in section 4 basically represents a pruned version of WordNet as a collection of intervals. It is not surprising that this gets high recall because the data is explicitly stored in the form of intervals. Unfortunately, however, this means that there is no generalization and the proposed representation for hyponymy essentially remembers WordNet. If we are allowed to do that, then why not just define f(w) to be an indicator for w and P(w, f(w')) to be an indicator for whether the word w is a hyponym of w'. This would give us perfect precision and recall, at the cost of no generalization.\n\n* Minor points   \n1. The properties 1 and 2 are essentially saying that the precision and recall respectively are alpha. Is this correct?\n2. Should we interpret P as a probability? The paper doesn't explicitly say so, but why not?\n3. The paper is written in a somewhat informal style. Some examples:\n   - Before introducing property 3, the paper says that it is a \"satisfying way\". Why/for whom?\n   - The part about not admitting defeat (just above section 3.1)\n   While these are not bad by themselves, the style tends to be distracting from the point of the paper.\n\n* Missing reference\nSee: Faruqui, Manaal, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. \"Retrofitting Word Vectors to Semantic Lexicons.\" In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1606-1615. 2015.\n\nThis paper and its followup work discusses relationships between word embeddings and relations defined by semantic lexicons, including hyponymy.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}