{"title": "Review of EMI", "review": "This is a very interesting paper about a novel approach to exploration in agents with state and action representations, making heavy use of recent progress in the use of deep learning for estimating and maximizing mutual information, as well as introducing an approach to model the latent space dynamics with a linear models with sparse errors.\n\nA closely related work which is not mentioned is the work of Thomas et al 2017 arXiv:1708.01289 where they also maximize mutual information between distributed representations of actions (policies, actually) and of distributed representations of changes in the state (as the result of applying the policy).\n\nThe phrase 'functionally similar states' is used several times and would require a bit of explanation.\n\nI would also like to see more motivations for the two different reward functions r_e and r_d, and why one should be computed before the update while the other should be computed after.\n\nRegarding the experiments, and this is probably the weakest part of this paper, I would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided). Only one (EX2) was compared. The comparison with TRPO is without exploration (if I understand well, but should be stated clearly).  It's also not clear how these results compare to the best reported results on these games (whether or not exploration is used).\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}