{"title": "Unclear why this should work", "review": "This paper describes a novel method for solving inverse problems in imaging.\n\nThe basic idea of this approach is use the following steps:\n1. initialize with nonnegative least squares solution to inverse problem (x0)\n2. compute m different projections of x0\n3. estimate x from the m different projections by solving \"reformuated\" inverse problem using TV regularization.\n\nThe learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a \"good\" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be.\n\nMore specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. \n\nEmpirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem.\n\nThe core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the \"inverse-ness\" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned.\n\nAt the heart of this paper is the idea that for an L-Lipschitz function f : R^k \u2192 R the sample complexity\nis O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided.\nThis isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we\nreally get the benefits.\n\nThe authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares.\n\nPractically I'm quite concerned about their method requiring training 130 separate convolutional neural\nnets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is\nit possible that any network at all would be okay? Can we just reconstruct the image from regression\non 130 randomly-initialized convolutional networks? \n\nThe proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}