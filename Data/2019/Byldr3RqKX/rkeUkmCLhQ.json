{"title": "The authors of this paper propose a method for assessing modularity in deep networks and more specifically on deep generative networks.", "review": "AFTER REBUTTAL:\nI think that in its current version the paper is not yet ready for publication. Several issues have been raised by fellow reviewers as well. I think that they are not trivial and they regard key aspects like paper structure, quality of exposition and experimental analysis. I have detailed my initial opinion in response to the author request for more details.  I hope this will serve as useful  guidelines for improving the paper in the future. \n\n------------\nThe method tackles the problem of interpretability that is a very important issue for usually black-box deep networks. Unfortunately it is not very clear how is the achieved. I have read several times the part explaining the influence maps and the clustering based on them and it still doesn't make a lot of sense to me. I think that part has to be better justified and exposed. Moreover, results do not support the claim which makes me doubt even more about how effective the method proposed actually is. In conclusion, I think that better exposition and more solid experimental analysis is needed.  \n\nAlso please check some writing problems: \n\n> Introduction: \n\"to acquire a generative function mapping a latent space (such as Rn)\" >  difficult to read, rephrase. \n\"making it difficult to add human input\" > confusing. What do you mean by human input? I assume you refer to having control to make decisions about design. \n\n> Section 3.1\n\"the internal variable may leave the manifold it is implicitly embedded in as a result of the model\u2019s training\" : not clear, rephrase. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}