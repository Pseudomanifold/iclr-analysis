{"title": "Sound method, but the scope is limited to hyperparameter tuning, and no comparisons with other methods are provided", "review": "In this paper, authors propose a probabilistic extension of classic Neural Link Prediction models, such as DistMult and ComplEx. The underlying assumption is that the entity embeddings and the relation embeddings are sampled from a prior Multivariate Normal distribution, whose (hyper-)parameters can be estimated via maximum likelihood. In this paper, authors use Variational Inference (VI) for approximating the posterior distribution over the embeddings, and use Stochastic VI for maximising the Evidence Lower BOund (ELBO) while scaling to large datasets. In Sect. 3, authors introduce the generative process, and show how MAP estimation of the embedding matrices can recover the original models. In Sect. 4, authors start from the intractable marginal likelihood over the data (Eq. 5) for deriving the corresponding ELBO (Eq. 6), which is defined over:\n- The \"hyperparameters\" gamma, which define the parameters of the prior Multivariate Normal distribution over the embeddings, and\n- The parameters gamma of the variational distributions.\n\nQuestion: why the ELBO (Eq. 6) is not used anywhere in Algorithm 1?\n\nThe model does not mention a number of significantly more accurate models proposed in the literature, such as [1].\n\nFurthermore, it seems to me that the point of the whole paper is finding efficient ways of estimating the hyperparameters efficiently. In that sense, there are other methods that were not considered, either simple (e.g. random sampling or black-box optimization techniques [2]) or more complex (e.g. hypergradient descent [3]).\n\n[1] https://arxiv.org/abs/1707.01476\n[2] https://github.com/hyperopt/hyperopt\n[3] https://arxiv.org/abs/1703.04782", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}