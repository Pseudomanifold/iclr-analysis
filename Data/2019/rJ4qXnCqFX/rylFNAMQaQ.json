{"title": "Probabilistic Knowledge Graph Embeddings - Review", "review": "Summary:\nThe paper presents a probabilistic treatment of knowledge graph embeddings, motivating it in parameter uncertainty estimation and easier hyperparameter optimisation. The authors present density-based DistMult and ComplEx variants, where the posterior parameter distributions for entity and relation embeddings are approximated by diagonal Gaussians q_\\gamma. Variational EM is used to infer the variational parameters \\gamma as well as the per-entity/per-relation precision (\\lambda) hyperparameters. The training process proposed by the authors consists of three phases: (1) pretraining a MAP estimate that\u2019s used as initial means of the posterior approximating Gaussians, (2) variational EM (see above) to find better hyperparameters and (3) another MAP training phase that uses the updated per-entity/per-relation hyperparameters. Finally, experimental results indicate a slight improvement in MRR and HITS@10 across FB and WN datasets.\n\nOriginality:\nTo the best of the reviewer\u2019s knowledge, the presented approach is novel for knowledge graph embeddings.\n\nDiscussion:\nWhile the task is relevant, it is unclear how significant the improvements are. While overall, the proposed method seems to indicate small improvement upon a very strong baseline, in some cases it\u2019s very close (96.2 vs 96.4 HITS@10 on WN18, 36.4 vs 36\n.5 MRR on FB15K237), or worse (85.8 vs 85.4 MRR on FB15K). \nIt is unclear how adequate some details in the experimental setup are for verifying the main hyperparameter optimization claim. In particular, what is \u201ca reasonable choice of hyperparameters\u201d in the first training phase? From figure 3b it seems the initial lambda\u2019s are set proportionately to the frequency, as in the baseline. Are the initial hyperparameter values in EM set the same as the hyperparameter values used for MAP in the reported results? If the claim is to optimize hyperparameters, shouldn\u2019t their initial values be set as uninformed as possible? How do different initial hyperparameter values affect final performance?\nThe authors claim that the improvement is most notable for entities with fewer training points, however, this is only investigated by using a balanced MRR, where the results are again very close, the same (WN18) or worse (FB15K) for ComplEx. Wouldn\u2019t it be clearer to perform a separate evaluation only considering low-frequency entities to verify this claim?\nParameter uncertainty is not further handled in the paper, the final approach is a point estimate, which discards the uncertainties obtained by VI. Authors mention (last paragraph of Sec. 4) that for a large embedding dimension, bayesian predictions are worse, while for small dimension, they are better. The author\u2019s hypothesis is that a more flexible posterior approximation could solve this issue. No concrete numbers or further analysis are provided.\n\n\nClarity and presentation:\nThe result tables should be merged and formatted better. \nFigures need some work (Fig. 2 looks poorly scaled, all figures should be in vector format for scalability, typos in Fig. 1)\n\nQuestions:\n- How much additional computation is needed to achieve the reported results?\n- Would it be possible to group the entities in bins by frequencies (say 6-10 bins) and assign each bin a hyperparameter, and run grid search over just 6-10 hyperparameters, and then interpolate between the bins to set hyperparameters per entity as a function of its frequency?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}