{"title": "Pre-training for QA helps", "review": "This paper shows that a sentence selection / evidence scoring model for QA trained on SQuAD helps for QA datasets where such explicit per-evidence annotation is not available.\n\nQuality:\nPros: The paper is mostly well-written, and suggested models are sensible. Comparisons to the state of the art are appropriate, as is the related work description. the authors perform a sensible error analysis and ablation study. They further show that their suggested model outperforms existing models on three datasets.\nCons: The introduction and abstract over-sell the contribution of the paper. They make it sound like the authors introduce a new task and dataset for evidence scoring, but instead, they merely train on SQuAD with existing annotations. References in the method section could be added to compare how the proposed model relates to existing QA models. The multi-task \"requirement\" is implemented as merely a sharing of QA datasets' vocabularies, where much more involved MTL methods exist. What the authors refer to as \"semi-supervised learning\" is in fact transfer learning, unless I misunderstood something.\n\nClarity:\nApart from the slightly confusing introduction (see above), the paper is written clearly.\n\nOriginality:\nPros: The suggested model outperforms others on three QA datasets.\nCons: The way achievements are achieved is largely by using more data, making the comparison somewhat unfair. None of the suggested models are novel in themselves. The evidence scoring model is a rather straight-forward improvement that others could have come up with as well, but merely haven't tested for this particular task.\n\nSignificance:\nOther researchers within the QA community might cite this paper and build on the results. The significance of this paper to a larger representation learning audience is rather small.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}