{"title": "Review", "review": "\nThis paper proposes a new model based on graph neural networks for relation extraction and evaluates it on multiple benchmarks and demonstrates its ability to do some level of \u201cmulti-hop relational reasoning\u201d.\n\nAlthough the empirical results look good and the paper might present some interesting and effective ideas, I find the paper very difficult to follow and many concepts are confusing (and even misleading).\n\nSection 3-4:\n\n- \u201cl\u201d is not defined -- I assume it denotes the number of tokens in the sentence but |s| is used in other places.\n- Are \u201centires\u201d and \u201centities\u201d the same?\n- a series of tokens => a sequence of tokens.\n- Equation (5): \u201cn\u201d is not defined on the right of equation. Does this mean that different layers have LSTMs/MLPs with separate sets of parameters? If it is the case, why do you need the word embeddings at all the layers to construct the transition matrices? Please clarify. \n- Does BiLSTM return one vector or a sequence of l vectors? Even MLP needs to be defined.\n\nIn general I find the concept of \u201cgenerated parameters\u201d even confusing. How would traditional GNNs work in this context? Isn\u2019t the novelty that parameterizing word/positional information in the transition matrix which enables a graph-based neural network working?\n\nIt would be very important to explain the intuition of this model and make the presentation clear and understandable. I don\u2019t recommend this paper to be accepted in the current format.\n\nThe empirical results also make me wonder whether this model outperforms other models because the other models work on a single pair of entities while this model attempts to work on all pairs of entities at the same time so that it enables some level of reasoning at the entity level (e.g., language + cast member -> language spoken in Figure 1). If this is the real contribution, the paper has to make it clear enough. \n\nAnother related paper that needs to be cited:\n\n- Zhang et al, 2018: Graph Convolution over Pruned Dependency Trees Improves Relation Extraction. EMNLP.\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}