{"title": "In this paper, the authors considered source domain selection problem in transfer learning.", "review": "\nsummary:\nIn this paper, the authors considered a source domain selection problem in transfer learning. Given a feature representation function f, the H-score is defined as the normalized correlation between the output f(X) and the label Y. The transferability is then measured by the ratio of H-score on the target domain and the optimal one. The authors introduced the information-theoretic and statistical meaning of the H-score. Validation of H-score was confirmed by numerical experimenters with image data. \n\n\ncomments:\nApplication of H-score to transfer learning interesting. Numerical experiments using relatively large dataset were convincing to show the validity of the proposed method. The following is some minor comments. \n\n* Some notations and terms in section 2.1 were a bit hard to understand. e.g. what does \"the error exponent corresponding to f(x)\" mean? In particular, \"corresponding to f(x)\" was not clear for me. \n\n* The authors showed some relationship between H-score and error of the statistical test. It would be nice to show a more direct relation between H-score and test accuracy in transfer learning. Typically, the risk in the target domain (T) is bounded above by the risk in the source domain (S) plus some dispersion between T and S as shown in the following paper: \nShen, et al., Wasserstein Distance Guided Representation Learning for Domain Adaptation, AAAI (2018). \n\n* In the higher order transfer of numerical experiments, the concatenated features are employed. Showing a theoretical justification of such a concatenation would be nice. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}