{"title": "Review for State-Denoised Recurrent Neural Networks", "review": "In this paper the authors develop the clever idea to use attractor networks, inspired by Hopfield nets, to \u201cdenoise\u201d a recurrent neural network.  The idea is that for every normal step of an RNN, one induces an additional \"dimension\" of recurrency in order to create attractor dynamics around that particular hidden state. The authors introduce their idea and run some basic experiments. This paper is well written and the idea is novel (to me) and worthy of exploration.  Unfortunately, the experiments are seriously lacking in my opinion, as I believe *the major focus* of those experiments should be comparisons to other denoising / regularization techniques.\n\nMAJOR\n\nThe point is taken that RNNs are susceptible to noise due to iterated application of the function. In my experience, countering noise (in the sense of gaussian noise added) isn\u2019t a huge problem in practice because there are many regularization methodologies to handle it. This leads me to the point that I think the experiments need to compare across a number of regularization techniques.  The paper is motivated by discussion of noise, \u201cnoise robustness is a highly desirable property in neural networks\u201d, and the experiments show improved performance on smaller datasets, all of which speak to regularization. So I believe comparisons with regularization techniques are pretty important here. \n\nMODERATE\n\nThere is some motivation at the beginning of this piece, in particular about language, and does not contain citations, but should.\n\n\u201cTraining is in complete batches to avoid the noise of mini-batch training.\u201d  Please explain, I guess this is not a type of noise that the method handles? \n\nWhat about problems that require graded responses, which is likely anything requiring integration? For example,  what happens in the majority task if the inputs were switched to a non-discrete version, where one must hold analog numbers?\n\n\nMINOR\n\nAny discussion about the (presumably dramatic) increase in training time due to the attractor dynamics unrolling + additional batching due to noise vectors (if I understood correctly)?\n\nWhat are your confidence intervals over?  Presumably, we\u2019d like to get confidence over multiple network instantiations.\n\nPg 1. Articulated neural network? \n\n\nQUESTIONS\n\nDoes using a the \u2018c\u2019 variable as a bias instead of an initial condition really matter? \n\nHow does supervised training via eqn (4) relate to the classic training of Hopfield nets? I assume not at all, but it would be useful to clarify?\n\nWhat RNN architecture did you use in the Figure 5 simulations (tanh vanilla RNN or GRU?)\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}