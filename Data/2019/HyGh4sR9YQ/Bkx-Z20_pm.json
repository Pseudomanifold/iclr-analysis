{"title": "GAs for Deep-RL; good performance numbers, but needs more intuition and understanding.", "review": "The authors show that using a simple genetic algorithm to optimize the weights of a large DNN parameterizing the action-value function can result in competitive policies. The GA uses selection truncation and elitism as heuristics; compact-encoding of the genome to improve system efficiency. Results on Atari are presented, along with a comparison with standard gradient-based RL algorithms and ES.\n\nPros:\n\nThe knowledge that on a moderately complex optimization problem such as RL on Atari with pixel inputs, learning can be achieved using parameter perturbation (combined with heuristics) is valuable to the community. The authors\u2019 motivation --- that GAs could prove to be another interesting tool for RL --- is well-founded. The efficient implementation using CPU-GPU hybridization and the compact-encoding is a good engineering contribution and could help accelerate further research.\n\nCons:\n\na.) One issue is that the results are presented in a somewhat hard-to-read manner. Table 1. shows numbers on 13 selected Atari games, of which 3 work best with GA (considering 1B frames). Comparison on all games helps to understand the general applicability of GAs. This is provided in Table 6., but the GA (1B) is missing. So are we comparing ES (1B) vs GA (6B) in Table 6 on all games? I can understand that DQN/A3C take days to run that many frames, but what does a ES (1B) vs GA (6B) comparison tell us? Am I reading it right?\n\nb.) Deep RL algorithms are approximations of theoretically sound ideas; so is ES in a way. What would help make this an even better paper is if the authors attempt to demystify the GA results. For example, I would have enjoyed some discussion on the potential reasons for the superiority of GAs on the 3 games in Table 1. Are these hard exploration environments and the parameter-space exploration in GA outperforms the action-space (e-greedy) exploration in A3C (DQN)? Random search is also better than DQN on those 3 environments --- Is the availably of good policies around the initial weight distribution contributing to the GA results in anyway? Furthermore, it would be interesting to pick out the worst games for GA from Table 5. (cases of catastrophic failure of GA), and analyze the shortcomings of GA. This would be helpful in further advancing research in GAs for deep RL. As a suggestion, ablation studies could go some distance in improving the interpretability, since the GA in the paper uses critical choices (parameter-noise variance, population-size etc.) and heuristics (truncation, elitism, etc.) that affect performance w.r.t baselines.\n\nOther comments:\n\nComparing GA+NS with A3C/DQN under environments with deceptive rewards is not completely fair. If the GA is armed with an exploration strategy like NS, so should the standard RL algorithms (e.g. by adding count-based, curiosity exploration). \n\nSection 4.3 (Humanoid discussion) could be move to Appendix since it currently doesn\u2019t work with GA, and if anything, increases doubts on the robustness of GA since the Humanoid DNN is fairly small.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}