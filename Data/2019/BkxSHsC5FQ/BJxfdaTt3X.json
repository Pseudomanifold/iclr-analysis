{"title": "Redundant idea", "review": "This paper presents a continual learning method that aims to overcome the catastrophic forgetting problem by holding out small number of samples for each task to be used in training for new tasks. Specifially, these representative samples for each task are selected as support vectors of a SVM trained on it. The proposed method, SupportNet, is validated on a continual learning task of a classifier against two existing continual learning approaches, which it outperforms.\n\nPros\n- Idea of using SVM to identify the most important samples for classification makes sense.\n\nCons\n- The idea of storing a small subset of a original dataset for each task has been already explored in [Nguyen et al. 18], and thus is not novel.\n- Thus the contribution of this work reduces to the use of SVM to identify the most important samples, but the effectiveness of this approach is not validated since it does not compare against [Nguyen et al. 18].\n- Also it leaves out many of the recent work on continual learning.\n- The idea of using SVM for identifying important samples is not very attractive since an SVM will have a very different decision boundary from the one from the trained DNN.\n- Also this method is only applicable to a classification task and not to other tasks such as regression or RL.\n\nThus considering the lack of novelty and experimental validation, I recommend rejecting this paper.\n\n[Nguyen et al. 18] Variational Continual Learning, ICLR 2018", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}