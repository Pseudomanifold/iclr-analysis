{"title": "neat idea but not convincing enough", "review": "The authors propose a modified ReLU, the GaLU, where the nonlinearity gating role is decoupled from the linear weights. Similar ideas have been previously proposed. For example Tsai et al: http://papers.nips.cc/paper/6516-tensor-switching-networks: \"The TS network decouples a hidden unit\u2019s decision to activate (as encoded by the activation weights) from the analysis performed on the input when the unit is active (as encoded by the analysis weights)\" and Veness et al: Online learning with gated linear networks, https://arxiv.org/abs/1712.01897. \n\nIn short, the paper proposes a tweak to the nonlinearity in neural nets. Since many tweaks have been previously investigated, for such a paper to be worthy of publication, in 2018, the experimental results need to be extremely impressive. The results in this paper, on MNIST and fashion-MNIST are nowhere near sufficient.\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}