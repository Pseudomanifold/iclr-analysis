{"title": "it is not right to do analysis on test set.", "review": "\nThis work does extensive experiments on three different text generation tasks and shows the relationship between wider beam degradation and more and larger early discrepancies. This is an interesting observation but the reason behind the scene are still unclear to me. A lot of the statements in the paper lack of theoretical analysis. \n\nThe proposed solutions addressing the beam discrepancies are effective, which further proves the relationship between beam size and early discrepancies. My questions/suggestions are as follows:\n* It\u2019s better to show the dataset statistics along with Fig1,3. So that readers know how much of test set have discrepancies in early steps.\n* It is not right to conduct your analysis on the test set. You have to be very clear about which results are from test set or dev set.\n* All the results with BLEU score must include the brevity penalty as well. It is very useful to analyze the length ratio changes between baseline, other methods, and your proposal.\n* The example in Sec. 4.6 is unclear to me, maybe you could illustrate it more clearly.\n* Your approaches eliminate the discrepancies along with the diversity with a wider beam. I am curious what if you only apply those constraints on early steps.\n* I suggest comparing your proposal to the word reward model in [1] since it is also about improving beam search quality. Your threshold-based method is also kind of word reward method.\n* In eq.2, what do you mean by sequence y \\in V? y is a sequence, V just a set of vocabulary.  What do you mean by P (y|x;{y_0..y_t}). Why the whole sequence y is conditioned on a prefix of y?\n\n[1] Huang et al, \"When to Finish? Optimal Beam Search for Neural Text Generation\" 2017", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}