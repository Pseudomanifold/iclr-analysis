{"title": "A strong paper with innovative ideas, but somewhat unclear methods and results", "review": "\nIn this paper, the authors address the problem of learning to achieve perceptually specified goals in a fully unsupervised way. For doing so, they simultaneously learn a goal-conditioned policy and a goal achievement reward function based on the mutual information between goals sampled from an a priori distribution and states achieved using the goal-conditioned policy. These two learning processes are coupled through the mutual information criterion, which seems to result in efficient state representation learning for the visual specified goal space. A key feature is that the resulting metrics in the visual goal space helps the agent focus on what it can control and ignore distractors, which is critical for open-ended learning.\n\nOverall, the idea looks very original and promissing, but the methods are quite difficult to understand under the current form, the messages from the results are not always clear, and the lack of ablative studies makes it difficult to determine which of the mechanisms are crucial in the system performance and which are not.\n\n* Clarification of the methods:\n\nGiven the key features outlined above, I believe the work described in this paper has a lot of potential, but the main issue is that the methods are not easy to get, and the authors could do a better job in that respect. Here is a list of remarks meant to help the authors write a clearer presentation of their method:\n\n- the \"problem formulation\" section contains various things. Part of it could be inserted as a subsection in Section 3, and the last paragraph may rather come into the related work section.\n\n- in Section 3, optimization paragraph, the details given after \"As will be discussed\"... might rather come in Section 4 were most of all other details are given.\n\n- in Section 4, I would refer to Algorithm 1 only in the end of the section after all the details have been explained: I went first to the algorithm and could not understand many details that are explained only afterwards.\n\n- in Algorithm 1, shouldn't the two procedures be called \"Imitator\" and \"Teacher\", rather than \"actor\" and \"learner\", to be consistent with the end of Section 3?\n\n- there must be a mathematical relationship between $\\xsi_\\phi$ and $\\hat{q}$, but I could not find this relationship anywhere in the text. What is $\\xsi_\\phi$ is never introduced clearly...\n\n- p4: we treat h as fixed ... => explain why.\n\n- I don't have a strong background about variational methods, and it is unclear to me why using an expanding set of goals corresponding to already seen states recorded in a buffer makes it that maximizing the log likelihood given in (4) is easier than something else.\n\nMore generally, the above are local remarks from a reader who did not succeed in getting a clear picture of what is done exactly and why. Anything you can do to give a more didactic account of the methods is welcome.\n\n* Related work:\n\nThe related work section is too poor for a strong paper like this one. Learning to reach goals and learning goal representations are two extremely active domains at the moment and the authors should position themselves with respect to more of these works. Here is a short list in which the authors may find many more relevant papers:\n\n (Machado and Bowling, 2016), (Machado et al., 2017), GoalGANs (Florensa et al., 2018), RIG (Nair et al., 2018), Many-Goals RL (Veeriah et al., 2018), DAYN (Eysenbach et al., 2018), FUN (Vezhnevets et al., 2017), HierQ, HAC (Levy et al., 2018), HIRO (Nachum et al., 2018), IMGEP (Pere et al., 2018), MUGL IMGEP (Laversanne-Finot et al., 2018).\n\nIt would also be useful to position yourself with respect to Sermanet et al. : \"Unsupervised Perceptual Rewards for Imitation Learning\".\n\nAbout state representation learning, if you consider the topic as relevant for your work, you might have a look at the recent survey from Lesort et al. (2018).\n\nExternal comments on ICLR web site also point to missing references. The authors should definitely consider doing a much more serious job in positioning their work with respect to the relevant literature.\n\n* Experimental study:\n\nThe algorithm comes with a lot of mechanisms and small tricks (at the end of Section 3 and in Section 4) whose importance is never assessed by specific experimental studies. This matters all the more than some of the details do not seem to be much principled. It would be nice to have elements to figure out how important they are with ablative studies putting them aside and comparing performance. Among other things, I would be glad to know how well the system performs without its HER component. Is it critical?\n\nThe same about the goal sampling strategy, as mentioned in the discussion: how critical is it in the performance of the algorithms?\n\n- Fig. 1b is not so easy to exploit: it is hard to figure out what the reader should actually extract from these figures\n\n- difficult tasks like cartpole: other papers mention cartpole as a rather easy task.\n\nIn the begining of Section 4, the authors mention that the mechanisms of DISCERN naturally induce a form of curriculum (which may be debated), but this aspect is not highlighted clearly enough in the experimental study.\n\nIn my opinion, studying fewer environments but giving a more detailed analysis of the performance of DISCERN and its variations in these environment would make the paper stronger.\n\n\n\n* typos:\n\np3: the problem (of) learning a goal achievement reward function\n\nIn (3), p_g should most probably be p_{goal}\n\np4: we treated h(.) ... and did not adapt => treat, do not\n\np9: needn't => need not\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}