{"title": "Review", "review": "The paper proposes an unsupervised learning algorithm to learn a goal conditioned policy and the corresponding reward function (for the goal conditioned policy) by maximizing the mutual information b/w the goal state and the state achieved by running  the goal conditioned policy for K time steps. The paper proposes a tractable way to maximize this mutual information objective, which basically amounts to learning a reward function for the goal conditioned policy. \n\nThe paper is very well written and easy to understand. \n\nMISSING CITATIONS: Original UVFA [1] paper should be cited while citing goal conditioned policies. \n\nIn the paragraph,  \"Goal distribution\" , the paper uses a non parametric approach to approximate the goal distribution. Previous works ([2], [3]) have used such an approach and relevant work should be cited. \n\n[1] http://proceedings.mlr.press/v37/schaul15.html\n[2] Many Goals Reinforcement Learning https://arxiv.org/abs/1806.09605\n[3] Recall Traces: Backtracking Models for efficient RL https://arxiv.org/abs/1804.00379\n\nI wonder if  learning the variational distribution would be tricky in scenarios where one need to extract a representation of the end state that can distinguish states based on actions required to reach them. Like consider a U-shaped maze \n|       |         |\n|       |         |\n|_A__|__B__|\nIn this maze, even though the states represented by points A and B close to each other, but functionally they are very far apart.  I'm curious as to what authors have to say in this regard. \n\nBaseline Comparison: I find the experiment results not really convincing. First, comparison to other \"unsupervised\" exploration methods like Variational information maximizing exploration (VIME),  Variational Intrinsic Control (VIC), Curiosity driven learning (using inverse models) is missing.  I understand that VIME and VIC are really not scalable as compared to the proposed method, and hence it should be easy to construct a toy task where it is possible to intuitively understand whats really going on, as well as one can compare with the other baselines (VIME, VIC).\n\nI would recommend authors to study a toyish environment in a proper way as compared to running (incomplete) experiments on 3 different set of envs. It would make the paper really strong.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}