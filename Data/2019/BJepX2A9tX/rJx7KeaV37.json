{"title": "A reasonable solution to achieve rotation invariant representation, but experimental and analytic parts should be enhanced. ", "review": "Authors provide a rotation invariant neural network via combining conic convolution and 2D-DFT.\nThe proposed conic convolution is a natural extension of quadrant convolution, which modulates directional convolution results with a direction-related mask. \nThe magnitude response of a 2D-DFT is applied to the output of the conic convolution, which further achieves a rotation invariant transformation to the rotation invariant features. \n\nComments:\n1. Both the conic convolution and 2D-DFT module are rotation invariant transformation, which one is more important for the model? I think the G-CNN + DFT is a good baseline to evaluate the importance of the conic convolution part, and authors should take the variant \u201cthe Conic Conv without DFT\u201d into consideration, such that the contribution of the DFT module can be shown quantitatively. \n2. Compared with H-Net, the proposed CFNet is slightly worse on the testing error shown in Table 1. Could authors analyze the potential reason for this phenomenon? Compared with H-net, what is the advantage of the proposed method?\n3. In Figure 3(c, d), the learning curves of the proposed CFNet are not as stable as those of its baselines, especially in the initial training phase. What is the reason? And how the configure the hyperparameters of the model?\n\nMinors:\n- Section 3.1 is a little redundant in my opinion. Many simple and well-known background/properties are shown, which can be compressed.\n- Line 4 of Figure 1\u2019s caption, 90 degrees -> 180 degrees?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}