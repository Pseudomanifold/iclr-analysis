{"title": "cute idea but need more analysis", "review": "This paper proposes a new approach to compress neural networks by training the student's intermediate representation to match the teacher's.\n\nThe paper is easy to follow. The idea is simple. The motivation and contribution are clear. The experiments are comprehensive.\n\nOne advantage of the proposed approach that the authors did not mention is that LIT without KD can be optimized in parallel, though I'm not sure how useful this is.\n\nOne major weakness of the paper is how the hyperparameters, such as the number of layers, the alpha, beta, tau, and so on, are tuned. It is not clear from the paper that there is a separate development set for tuning these values. If the hyperparameters are tuned on the test set, then it is not surprising LIT works better.\n\nHere are some minor questions:\n\np.5\n\nLIT outperforms KD and hint training on all settings.\n--> what are the training errors (cross entropy) for LIT, KD and hint training? what about the KD objectives (on the training set) of the model trained with LIT and the one trained with KD? this might tell us why LIT is better than the two.\n\nLIT outperforms the recently proposed Born Again procedure ...\n--> what are the training errors (cross entropy) before and after the born again procedure? this might help us understand why LIT is better.\n\nKD degrades the accuracy of student models when the teacher model is the same architecture\n--> again, the training errors (cross entropy) might be able to help us understand what is going on.\n\np.7\n\nAs shown in Table 3, none of the three variants are as effective as LIT or KD.\n--> is this claim statistically significant? some of the differences are very small.\n\nWe additionally pruned ResNets trained from scratch.\n--> what pruning method is being used?\n\nAs shown in Figure 6., LIT models are pareto optimal in accuracy vs model size.\n--> this is a very strong claim. it's better to say we fail to prune the network with the approach, but we don't know whether there exists another approach that can reduce the network size while maintaining accuracy.\n\nAs shown, L2 and L1 do not significantly differ, but smoothed L1 degrades accuracy.\n--> is this claim statistically significant?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}