{"title": "A novel approach for compressing deep learning models", "review": "This paper proposes to compress the model by depth. It uses hint training and knowledge distillation techniques to compress a \"deep\" network block-wisely. It shows a better compression ratio than knowledge distillation or hint training while achieving comparable accuracy performance.\n\nPros: \n1. This paper considers block-wise compression. For each block, it uses the output of the teacher's last layer as input during training, which improves the learnability of the student models. \n2. The experiments include a large range of tasks, e.g., image classification, sentiment analysis and GAN. \n\nCons:\n1. Validation accuracy is used as the performance metric, which might be over-tuned. How is the performance on testing datasets?\n2. The writing and organization of the paper need some improvement, especially the experiments section.\n3. The compression ratio (3-5) is not very impressive compared with other compression techniques with pruning and quantization techniques, such as Han et al. 2015, Hubara et al. 2016.\n\nIn summary, I think this is an interesting approach to compress deep learning models. But I think the comparisons should be done in terms of testing accuracy. Otherwise, it is hard to judge the performance of this approach. \n\n=== after rebuttal ===\nThanks for the authors' response. Some of my concerns have been clarified. I increased my rating from 5 to 6. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}