{"title": "interesting paper, unsure of experimental validation", "review": "The authors design a new taxonomy of attacks on deep RL agents - they developed three classes of attacks - attacks the modify the observation given to the agent, attacks that modify the action used by the agent and attacks that change the dynamics of the environment. In settings that have been studied previously, the authors show that they can find attacks more effectively than previous approaches can. They also study learning based and online attack generation approaches, that can be effectively used to quickly find adversarial attacks on the agent. The authors validate their approaches experimentally on Mujoco tasks and the TORCS driving simulator.\n\nQuality: I found the paper's contributions difficult to understand - the significance of the three classes of attacks is not properly explained (in particular, I found the action perturbation to be difficult to justify in a real world setting). Further, the difficulty of generating attacks in each of these classes and the need for new algorithms is not explained properly. The need for effective ways to quickly generate adversarial attacks in RL is clear, but the authors' experiments don't seem to clarify that their proposed aproaches achieve this goal.\n\nClarity: The organization of section 4 makes the paper difficult to read - I would separate the taxonomy from the contribution of novel ways of generating adversarial attacks (the latter, imo, is the more significant contribution). \n\nOriginality: To the best of my knowledge, the authors propose novel kinds of attacks as well as novel attack algorithms on RL agent.\n\nSignificance: The problem considered is certainly significant. Despite the successes achieved by DeepRL, their robustness (in terms of distribution shifts, adversarial noise, model errors etc.) is of great importance when considering deploying these models. However, the presentation and experiments leave me unconvinced that the presented approaches are  a significant step ahead in attack generation (particularly in ways to generate attacks that can efficiently be incorporated into adversarial training of RL agents).\n\nCons\n1. Unclear presentation of technical contributions, experimental results do not support the key contributions of faster attack generation\n2. I am also unconvinced of the relevance of blackbox attack algorithms given the nascent stage of deepRL - since these agents are just being developed and their abilities need to improve significantly before they become deployable (and blackbox adversarial attacks are a real concern), I feel this work is premature and will need to be redone once more capable/robust agents can be trained for practical RL settings\n\n###\nIn light of the revision, I have revised my score given the rewriting of section 3 that addresses the second con I raised above. However, due to the lack of clarity in presentation of the technical results in section 4 and the experiments in section 5, I feel that the paper still require improvement before it can be accepted.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}