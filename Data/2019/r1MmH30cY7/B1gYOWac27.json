{"title": "Confidence Regularized Self-Training", "review": "The paper proposes the regularized framework of self-training, for the unsupervised domain adaptation. Authors reformulate basic self training problem with some relaxation and regularization encouraging not to use unconfident pseudo-labels. They also different instantiations of such regularizers under the proposed framework based on L2, entropy and KL and evaluate them on semantic segmentation and image classification tasks.  Overall, while the idea is potentially interesting and worthwhile to further explore, the current versions does not seem to be ready to be published:  the structure of paper needs to be improved, details about the experiments are missing and the connection to the EM should be clarified.  \n\nDetailed question/comments:\n-The structure of the paper is very kind in providing details on their derivations. However, given the space constraint, it seems the balance between the model derivation and experiments should considered.  The experimental verification is quite weak as it is while there are lots of white space and redundancies in equations.\n- After Eq. (1), authors argue that the constraint of (1) has the problem of over-confidence on wrong pseudo labels and derive Eq (2) with simplex relaxation. But it turns out after Eq (4) that the optimal solution for y_hat is still one hot binary vector. Can we say (2) is better in terms of pseudo label overconfidence problem (not just ease of optimization)?\n- In section 3, it is not clear which part is the contribution and which is just review. It would be great to add some references and comparisons on existing literatures.\n- In section 3, it is not clear to me how one p rules all lambda_t; it seems we need additional parameters to adaptively increase (5% for instance as authors mentioned) for every iteration.\n- Algorithm 1 and 2 are not given in the main part. This should be explicitly mentioned.\n- The connection to EM seems wrong. In E-step, we compute the posterior distribution given the parameter computed in previous M step. It's not computing \"argmax\", but should be computing the \ndistribution\", which will not probably be one-hot in eq (e).\n\n\n ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}