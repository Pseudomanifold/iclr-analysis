{"title": "The idea presented in the paper is good but its elaboration in the current version of the paper seems preliminary.", "review": "pros:\n\n- Introducing soft-labels to self-training has a good common sense motivation.\n- The experiments (if valid) suggest the method works.\n\ncons:\n\n- No theoretical justification. \n- One of the two contributions (connection between EM and self-training) is known.\n- Messy experiments. \n\n\n- Overall assessment: \n\nThe idea of using soft labels instead of hard labels makes sense as it allows to impose prior on the label distribution of the un-labeled data. The authors impose the label prior by introducing a regularization term into the objective. Besides the latent labels the authors also proposed to regularize the posterior distribution of the learned model. Motivation for the latter is much less clear. The authors argue that regularizing the posterior prevents over-fitting, however, then it is unclear why the regularization is NOT used also for the labeled examples. \n\nThe proposed approach is common a sense heuristic, however no theoretical justification of the idea is presented in the paper. As a result it is unclear which regularization is suitable for given type of data and mainly if there is any guarantee that the approach will work without just trying. The authors provide an empirical study in which several different regularizes are tested. The problem is that the experiments are a bit unorganized and there is no systematic evaluation/interpretation of the results. \n\n- Section 5: The connection between the self-training and regularized MMLE\n\nThe connection is claimed to be one of the two main contributions of the paper. However, the connection between self-training and the \"classification\" (=using hard labels) EM algorithm is known, e.g. see:\n\nMassih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression.\nIn Proceedings of the 15th European Conference on Artificial Intelligence, 2002.\n\nIn addition, the presented derivation has several flaws:\n\na) The initial likelihood should be defined for p(y|x) instead of p(x,y) because when using the NN we model the former as modeling p(x) is usually too difficult.\n\nb) The last step is probably not correct because: the omitted term $\\sum_{x_s} log p(x_s) + \\sum_{x_t}p(x_t)$ can be negative in general (if p(x) < 0).\n\nc) At the end it is assumed that the auxiliary distribution $q(k)$ is one-hot vector. However, in order to derive the EM lower bound it is crucial to assume that all components of $q(k)$ are non-zero, because of $q(k)/q(k)$ term, i.e you cannot divide by zero.\n\n- Section 6: Experiments \n\nReading this section is though. The selection of the regularization constants is not discussed. The results (test accuracy in tab 3 and 4) are given without any confidence measure. In turn, it is unclear if the differences are statistically significant. It is not 100% clear how the classification setting dealt with in the paper applies to image segmentation. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}