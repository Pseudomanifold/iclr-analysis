{"title": "The discussion and conclusion drawn from the experimental part are quite arguable. In my opinion, the paper would gain much more impact if the view developed in this paper were illustrated by more convincing experiments.", "review": "The authors propose a new interpretation of the batch normalization step inside a neural network.\nThe main result shows that the backpropagation of the gradient of some loss function through a batch normalization can be seen as a scaled residual of a least square linear fit. This new interpretation is extended to other normalization technics used in the literature and thus give a \"unified\" view of such methods. \n\nThe idea is simple yet very interesting and well introduced. The theoretical results are good and the proofs are well written and easy to follow.\n\nHowever the arguments brought forward by this new vision of batch normalization in applications look light (see sections 3.3, 4.1, 4.2). A more detailed interpretation of this new vision on a single application and its impact would have been preferred than numerous applications as it is done in this paper. \nNot all the existing normalization methods have been extended with success yet, this makes this unified vision a bit less convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}