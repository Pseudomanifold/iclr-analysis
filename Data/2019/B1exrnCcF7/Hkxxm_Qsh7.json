{"title": "Review of Disjoint Mapping Network for Cross-modal Matching of Voices and Faces", "review": "# Summary\n\nThe article proposes a deep learning-based approach aimed at matching face images to voice recordings belonging to the same person. \n\nTo this end, the authors use independently parametrized neural networks to map face images and audio recordings -- represented as spectrograms -- to embeddings of fixed and equal dimensionality. Key to the proposed approach, unlike related prior work, these modules are not directly trained on some particular form of the cross-modal matching task. Instead, the resulting embeddings are fed to a modality-agnostic, multiclass logistic regression classifier that aims to predict simple covariates such as gender, nationality or identity. The whole system is trained jointly to maximise the performance of these classifiers. Given that (face image, voice recording) pairs belonging to the same person must share equal for these covariates, the neural networks embedding face images and audio recordings are thus indirectly encouraged to map face images and voice recordings belonging to the same person to similar embeddings.\n\nThe article concludes with an exhaustive set of experiments using the VGGFace and VoxCeleb datasets that demonstrates improvements over prior work on the same set of tasks.\n\n# Originality and significance\n\nThe article follows-up on recent work [1, 2], building on their original application, experimental setup and model architecture. The key innovation of the article, compared to the aforementioned papers, lies on the idea of learning face/voice embeddings to maximise their ability to predict covariates, rather than by explicitly trying to optimise an objective related to cross-modal matching. While the fact that these covariates are strongly associated to face images and audio recordings had already been discussed in [1, 2], the idea of actually using them to drive the learning process is novel in this particular task.\n\nWhile the article does not present substantial, general-purpose methodological innovations in machine learning, I believe it constitutes a solid application of existing techniques. Empirically, the proposed covariate-driven architecture is demonstrated to lead to better performance in the (VGGFace, VoxCeleb) dataset in a comprehensive set of experiments. As a result, I believe the article might be of interest to practitioners interested in solving related cross-modal matching tasks.\n\n# Clarity\n\nThe descriptions of the approach, related work and the different experiments carried out are written clearly and precisely. Overall, the paper is rather easy to read and is presented using a logical, easy-to-follow structure.\n\nIn my opinion, perhaps the only exception to that claim lies in Section 3.4. If possible, I believe the Seen-Heard and Unseen-Unheard scenarios should be introduced in order to make the article self-contained. \n\n# Quality\n\nThe experimental section is rather exhaustive. Despite essentially consisting of a single dataset, it builds on [1, 2] and presents a solid study that rigorously accounts for many factors, such as potential confounding due to gender and/or nationality driving prediction performance in the test set. \n\nMultiple variations of the cross-modal matching task are studied. While, in absolute terms, no approach seems to have satisfactory performance yet, the experimental results seem to indicate that the proposed approach outperforms prior work.\n\nGiven that the authors claimed to have run 5 repetitions of the experiment, I believe reporting some form of uncertainty estimates around the reported performance values would strengthen the results.\n\nHowever, I believe that the success of the experimental results, more precisely, of the variants trained to predict the \"covariate\" identity, call into question the very premise of the article. Unlike gender or nationality, I believe that identity is not a \"covariate\" per se. In fact, as argued in Section 3.1, the prediction task for this covariate is not well-defined, as the set of identities in the training, validation and test sets are disjoint. In my opinion, this calls into question the hypothesis that what drives the improved performance is the fact that these models are trained to predict the covariates. Rather, I wonder if the advantages are instead a \"fortunate\" byproduct of the more efficient usage of the data during the training process, thanks to not requiring (face image, audio recording) pairs as input.\n\n# Typos\n\nSection 2.4\n1) \"... image.mGiven ...\"\n2) Cosine similarity written using absolute value |f| rather than L2-norm ||f||_{2}\n3) \"Here we are give a probe input ...\"\n\n# References\n\n[1] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. \"Learnable PINs: Cross-Modal Embeddings for Person Identity.\" arXiv preprint arXiv:1805.00833 (2018).\n[2] Nagrani, Arsha, Samuel Albanie, and Andrew Zisserman. \"Seeing voices and hearing faces: Cross-modal biometric matching.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}