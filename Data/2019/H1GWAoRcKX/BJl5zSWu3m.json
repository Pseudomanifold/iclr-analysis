{"title": " Review for \"A Teacher Student Network For Faster Video Classification\"", "review": "The authors present a teacher-student network to solve video classification problem, aiming at reducing computational cost. Specifically, the authors proposed two training algorithms, serial and parallel training. They showed the proposed approach can reduce inference time up to 30% on YouTube-8M dataset, taking a step forward to on-device classification.\n\nRelated work:\nThe YouTube-8M dataset has been updated this year, and the team hosted the 2nd video classification challenge and workshop at ECCV'18. As they limited the model size up to 1GB this year, many participants actually applied similar idea (distillation). They have posted a summary paper and all submitted papers, so I recommend the authors to refer those papers and summarize in the related work section.\n\nJ. Lee, A. Natsev, W. Reade, R. Sukthankar, G. Toderici. The 2nd YouTube-8M Large-Scale Video Understanding Challenge, ECCV workshop 2018.\n\nSee the list of accepted papers in https://research.google.com/youtube8m/workshop2018/\n\nThe authors motivated the work from high computational cost with RNN/LSTM-based models, claiming that they are the state-of-the-art. However, there are other computationally cheaper but still powerful approaches, such as temporal convolution or learnable pooling. It will be better to introduce these other lines of work to solve video classification problem.\n\nExperiment:\nThe authors conducted experiment mostly on YouTube-8M dataset, but did not specify which version they used. Given the dataset stats, it seems like their very first version with 8.2M videos. According to their update note (https://research.google.com/youtube8m/download.html), this version is most noisy, so they recommend to use more recent version. Also, it will be beneficial to use 2017 or 2018 version to compare against Kaggle challenge and corresponding workshop participants (CVPR'17 and ECCV'18, respectively) directly. As the reported score is based on the first version, it is not comparable to the state-of-the-art models in both workshops. (The first-place team achieved ~0.85 and 0.87+ each year, while this paper reports 0.806 as the best score. It is not directly comparable, as the training and eval set are not the same.)\n\nOther than this, the experiment was well designed, and it is also good to report both GAP and MAP. In video classification problem, examples are usually highly skewed among classes, so it is useful to evaluate with MAP as well, in order to verify if it performs well for rare classes as well as common ones. In Table 2, MAP score shows bigger gap than GAP, so it will be interesting to explain this phenomenon in more details.\n\nThe teacher model tends to be weaker, compared against the best performers at ECCV'18 participants. To fully take advantage of knowledge distillation, it makes more sense to make the teacher with ensembles of multiple models to maximize its performance. At the workshop, participants had to make the final model size less than 1GB, so it will be interesting to compare the model size (~number of parameters) as well.\n\nOverall, the idea presented in this paper would have been more interesting if it was submitted before ECCV'18. Knowledge distillation for video classification is no longer a novel idea unfortunately, so I encourage the authors to catch up with the recent work published through YouTube-8M workshop at ECCV, and propose more distinguishable work compared against to them.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}