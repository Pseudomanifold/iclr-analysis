{"title": "Nice idea but leaves several questions not answered", "review": "In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems.\n\nThis is an interesting point of view but the manuscript lacks discussion on several important questions:\n\n1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. \n2) Would this method also complement from overfitting?\n3) In the numerical experiments, the comparison is carried out against a \"baseline\" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair  if the regularization option is turned on for the baseline methods.\n4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?\n5) How does alternating between two objectives change the training time? Do the authors use backpropagation?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}