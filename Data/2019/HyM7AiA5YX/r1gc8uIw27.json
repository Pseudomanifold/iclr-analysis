{"title": "Interesting new idea, good experimental results, some points to clarify.", "review": "\n========\nSummary\n========\n\nThe paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a \"complementary entropy\" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.\n\nThe procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.\n\nAdversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit\nthat the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.\n\n===========================\n Main comments and questions\n===========================\n\nEnd of page 1: \"the model behavior for classes other than the ground  truth stays unharnessed and not well-defined\". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?\n\nPage 3, sec 2.1: \"optimizing on the complement entropy drives \u0177_ij to 1/(K \u2212 1)\". I believe that it drives each term \u0177_ij /(1 \u2212 \u0177_ig ) to be equal to 1/(K-1). Therefore, it drives \u0177_ij to (1 \u2212 \u0177_ig)/(K-1) for j!=g.\n\nThis indeed flattens the \u0177_ij for j!=g, but the effect on \u0177_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, \u0177_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? \n\nFor example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:\nSuppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 \nAfter step 2:  0.1 0.3 0.3 0.3\nThen step 1: 0.5 0.3 0.1 0.1\nThen step 2: 0.1 0.3 0.3 0.3\nAnd so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?\n\nSec 3.1:\n\"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance\": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?\n\nSec 3.2:\nThe additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.\n\nSec 3.4:\nAs the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?\n\n===========================\nSecondary comments and typos\n===========================\n\nPage 3, sec 2.1: \"...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...\", using maximizes instead of optimizes would be clearer.\n\nIn the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, \u0177_ig appears. Shouldn't C take all \\hat_y as an argument in this case?\n\nAlgorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}\n(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?\n\nSec 3:\n\"We perform extensive experiments to evaluate COT on the tasks\" --> COT on tasks\n\n\"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain.\" --> domainS\n\n\"to evaluate the model\u2019s robustness trained by COT when attacked\" needs reformulation.\n\n\"we select a state- of-the-art model that has the open-source implementation\" --> an open-source implementation\n\nSec 3.2:\nFigure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?\n\nTable 3 and 4: why is it the validation error that is reported and not the test error?\n\nSec 3.3:\n\"Neural machine translation (NMT) has populated the use of neural sequence models\": populated has not the intended meaning.\n\n\"We apply the same pre-processing steps as shown in the model\" --> in the paper?\n\nSec 3.4:\n\"We believe that the models trained using COT are generalized better\" --> \"..using COT generalize better\"\n\n\"using both FGSM and I-FGSM method\" --> methodS\n\n\"The baseline models are the same as Section 3.2.\" --> as in Section 3.2.\n\n\"the number of iteration is set at 10.\" --> to 10\n\n\"using complement objective may help defend adversarial attacks.\" --> defend against\n\n\"Studying on COT and adversarial attacks..\" --> could be better formulated\n\nReferences: there are some inconsistencies (e.g.: initials versus first name)\n\n\nPros\n====\n- Paper is clear and well-written\n- It seems to me that it is a new original idea\n- Wide applicability\n- Extensive convincing experimental results\n\nCons\n====\n- No theoretical guarantee that the procedure should converge\n- The training time may be twice longer (to clarify)\n- The adversarial section, as it is,  does not seem relevant for me\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}