{"title": "Very nice approach for hierarchical RL", "review": "This paper presents a novel approach for doing hierarchical deep RL. Each level of the hierarchy is rewarded for reaching a goal state. The top level's goal state is the environment goal, lower level goals states are the actions of the higher levels. The lowest level's actions are primitive actions. Each level can act until it reaches it goal or a maximum of T steps. Then HER is used to still learn from missed subgoals. For example, if the lowest level is given a subgoal and fails to achieve it, it is trained with a new experience where the goal was the achieved state. In addition, the level above is trained with an experience where the action it chose (the subgoal that was not achieved) is replaced with the subgoal that was achieved. So HER is replacing goals on one level and replacing actions on the higher level. The paper shows nice empirical results across 6 domains.\n\nThe two main differences from prior work are:\n1. Explicit constraint on how long the policies at each level can be.\n2. Use of HER in a novel way (on goals and actions) to learn from failed attempts at reaching subgoals from lower levels.\n\nThe use of HER in this work is really powerful and everything fits together nicely to make it work.\n\nThe only un-satisfying part of the algorithm is the need for a subgoal testing phase. Some actions are randomly decided to be testing phases, where all exploration is turned off at lower levels and the agent at the level selecting that subgoal is given negative reward if the subgoal is not achieved. This feels a bit unnatural to me. Does it not work if you punish a level for selecting a failed subgoal even if exploration is on? Does this phase unnecessarily punish levels for selecting subgoals that aren't reached early in learning, where even with no exploration a lower level may not have learned to reach the subgoal yet?\n\nThe main drawback of the paper is that there is no empirical comparison to related work. Instead the approach is only compared to doing learning with no hierarchy. Still, in all 6 domains, there is a clear improvement to using the hierarchy vs a flat hierarchy. \n\nPros:\n- Nice approach for hierarchical deep RL\n- Great use of HER to improve subgoal learning\n- Good empirical results showing benefit of approach over flat learning\nCons:\n- No empirical comparison to related work\n- Subgoal testing phase seems a bit hacky.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}