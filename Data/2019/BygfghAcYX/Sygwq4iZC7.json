{"title": "Solid paper.", "review": "The authors aim to shed light on the role of over-parametrization in generalization error. They do so for the special case of 2 layer fully connected ReLU networks, a \"simple\" setting where one still sees empirically that the test error decreasing as over-parametrization increases.\n\nBased on empirical observations of norms (and norms relative to initialization) in trained overparametrized networks, the authors are led to the definition of a new norm-bounded class of neural networks. Write u_i for the vector of weights incoming to hidden node i. Write v_i for the weights outgoing from hidden node i. They study classes where the Euclidean norm of v_i is bounded by a constant alpha_i and where the Euclidean norm of u_i - u^0_i is bounded by beta_i, where u^0_i is the value of u_i after random initialization. Call this class F_{alpha,beta} where alpha,beta are specific vectors of bounds.\n\nThe main result is a bound on the empirical Rademacher complexity of F_{alpha,beta}. \nThe authors also given lower bounds on the empirical Rademacher complexity for carefully chosen data points, showing that the bounds are tight. These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin.\n\nThe authors compare the bounds to existing norm-based bounds in the literature. The basic argument is that the terms in other bounds tend to grow as networks get much larger, while their terms shrink. Note that at no point are the bounds in this paper \"nonvacuous\", ie they are always larger than one.\n\nIn summary, I think this is a strong paper. The explanatory power of the results are still oversold in my opinion, even if they use hedged language like \"could explain the role...\". But the work is definitely pointing the way towards an explanation and deserves publication. The technical results in the appendix will be of interest to the learning theory community.\n\nissues:\n\n\"could explain role of over-parametrization\". Perhaps this work might point the way to an explanation, but it does not yet provide an explanation.  It is a big improvement it seems.\n\n\"bound improves over the existing bounds\". From this statement and the discussion comparing the bounds, it is not clear whether this bound formally dominates existing bounds or merely does so empirically (or under empirical conditions). \n\ntypos: \n\nbigger than the Lipschitz CONSTANT of the network class\n\nH undefined\n\nRademacher defined for H but must be defined on loss class (or a generic function class, not H)\n\n\"we need to cover\" --> \"it suffices to\"\n\n\"the following two inequaliTIES hold by Lemma 8\"\n\nbibliography is a mess: half of the arxiv papers are published. typos everywhere, very sloppy.\n\n(This review was requested late in the process due to another reviewer dropping out of the process.)\n\n[UPDATE]. The authors addressed my concerns stated in my review above. I think the bibliography has improved and I recommend acceptance. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}