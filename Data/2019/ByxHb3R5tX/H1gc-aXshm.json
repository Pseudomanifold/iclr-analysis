{"title": "Interesting and clear, but contribution small and with many experimental omissions.", "review": "In this paper the authors propose an extension to successor features (SF). Akin to UVFAs, they condition on some goal state by concatenating to the current state after some shared preprocessing. The authors claim three contributions: 1) introducing the USF, 2) proposing an appropriate deep learning architecture for it, and 3) showing experimentally that USFs improve transfer both within a goal set and to novel goals.\n\nClaims 1) and 2) don't seem particularly noteworthy. Extending SF to be goal-conditioned is very straightforward, doesn't leverage anything unique to the SF formalism (e.g. the reward weights w already encode a goal in some sense), and doesn't attempt to extend its theoretical grounding. The architecture is likewise unsurprising, and the lack of ablations or alternatives make it seem rather unmotivated.\n\nThe usage of a Q-learning loss instead of a reward-prediction loss for updating phi is mentioned without citation. This seems quite novel, and could be a significant contribution if its advantage was demonstrated experimentally.\n\nThe experiments appear to show a significant advantage for USFs. For the training-goal-set advantage, it would be useful to know the architecture of multi-goal DQN. One hypothesis is that the extra weight-sharing is what is giving USFs an edge, and this should be ruled out. It is briefly mentioned that UVFAs weren't considered due to their stated instability, but its unclear how they differ from the multi-goal DQN.\n\nThe novel-goal results are impressive at first glance, but there is a glaring omission. Hindsight experience replay (HER) is mentioned but not evaluated, and would very likely trivialise the train/test goal-set distinction (unless the test goals were never previously visited). As these results are the primary contribution of this paper, this must be addressed prior to publication acceptance.\n\nEdit: The addition of HER experiments push this up a bit (5-->6). I'm still concerned about how significant the contribution is (as it is a straightforward extension to SFs), but the empirical results are now quite strong.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}