{"title": "A linear method for speeding up PDE solvers with good empirical performances", "review": "Summary:\nThe authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and\nachieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   \n\nStrengths:\nSolving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. \n\nWeaknesses:\nThe method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. \n\nQuestions:\n- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? \n- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? \n- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?\n- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?\n- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}