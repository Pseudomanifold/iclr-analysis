{"title": "Lack of comparison with previous state-of-the-art methods over more widely used benchmarks", "review": "This paper proposes a new approach for answering questions requiring multi-hop reasoning. The key idea is to introduce a sequence labeler to divide the question into at most 3 parts, each part corresponds to a relation-tuple. The labeler is trained with the whole KB-QA pipeline with REINFORCE in an end-to-end way.\n\nThe proposed approach was applied to a synthetic dataset and a new KB-QA dataset MetaQA, and achieves good results.\n\nI like the proposed idea, which sounds a straightforward solution to compound question answering. I also like the clarification between \"compound questions\" instead of \"multi-hop questions\". In my opinion, \"multi-hop questions\" can also refer to the cases where the questions (can be simple questions) require multi-hop over evidence to answer.\n\nMy only concern is about the evaluation on MetaQA, which seems a not widely used dataset in our community. Therefore I am wondering whether the authors could address the following related questions in the rebuttal or revision:\n\n(1) I was surprised that WebQuestions is not used in the experiments. Could you explain the reason? My guess is that WebQuestions contains compound questions that cannot be simply decomposed as sequence labeling, because that some parts of the question can participant in different relations. If this is not true, could you provide results on WebQuestions (or WebQSP).\n\n(2) There were several previous methods proposed for decomposition of compound questions, although they are not proposed for KB-QA. Examples include \"Search-based Neural Structured Learning for Sequential Question Answering\" and \"ComplexWebQuestions\". I think the authors should compare their approach with previous work. One choice is to reimplement their methods. An easier option might be applying the proposed methods to some previous datasets, because the proposed method is not specific to KB-QA, as long as the simple question answerer is replaced to other components like a reader in the ComplexWebQuestions work.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}