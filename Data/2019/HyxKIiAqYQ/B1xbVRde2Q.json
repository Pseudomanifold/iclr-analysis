{"title": "A New Context-adaptive Entropy Model for Image Deep Compression", "review": "Summary. The paper is an improvement over (Balle et al 2018) for end-to-end image compression using deep neural networks. It relies on a generalized entropy model and some modifications in the training algorithm. Experimentals results on the Kodak PhotoCD dataset show improvements over the BPG format in terms of the peak signal-to-noise ratio (PSNR). It is not said whether the code will be made available.\n\nPros. \n* Deep image compression is an active field of research of interest for ICLR. The paper is a step forward w.r.t. (Balle et al 2018). \n* The paper is well written. \n* Experimental results are promising.\n\nCons.\n* Differences with (Balle et al 2018) should be emphasized. It is not easy to see where the improvements come from: from the new entropy model or from modifications in the training phase (using discrete representations on the conditions).\n* I am surprised that there is no discussion on the choice of the hyperparameter \\lambda: what are the optimal values in the experiments? Are the results varying a lot depending on the choice? Is there a strategy for an a priori choice? \n* Also is one dataset enough to draw conclusions on the proposed method?\n\nEvaluation.\nAs a non expert in deep learning compression, I have a positive opinion on the paper but the paper seems more a fine tuning of the method of (Balle et al 2018). Therefore I am not convinced that the improvements are sufficiently innovative for publication at ICLR despite the promising experimental results.\n\nSome details.\nTypos: the p1, the their p2 and p10, while whereas p3, and and figure 2 \np8: lower configurations, higher configurations, R-D configurations\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}