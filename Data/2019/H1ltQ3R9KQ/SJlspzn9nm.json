{"title": "Promising but several shortcomings", "review": "\nThe paper presents a meta-learning RL framework to train agents that\ncan learn and do causal reasoning.  The paper sets up three tasks for\nagents to learn to do associational, interventional, and\ncounterfactual reasoning. The training/testing is done on all\n5-variable graphs. The authors demonstrate how the agent can maximize\ntheir rewards, which demonstrate that the agent might have learnt to\nlearn some causal structure and do reasoning in the data.\n\nReview:\n\nI think Causality is an important area, and seeing how RL can help in\nany aspect is something really worth looking into.\n\nHowever, I have a few qualms about the setting and the way the tasks\nare modeled.\n\n1. Why is the task to select the node with the highest \"value\"\n(value=expected value?  the sample? what is it?) under some random\nexternal intervention? It feels very indirect.\n\nWhy not explicitly model certain useful actions that directly query\nthe structure, such as:\n\n- selecting nodes that are parents/children of a node\n- evaluating p(x | y) or p(x | do(y))?\n\n2. The way you generate test data might introduce biases:\n\n- If you enumerate 3^(n(n-1)/2) DAGs, some of them will have loops.  Do you weed them out?\n  Does it matter?\n\n- How do you sample weights from {-1, 0, 1}? uniform?  What happens if\n  wij = 0?  This introduces bias in your training data.  This means\n  your distribution is over DAGs + weights, not just DAGs.\n\n- Your training/test split doesn't take into account certain\n  equivalence/symmetries that might be present in your training data,\n  making it hard to rule out whether your agents are in effect\n  memorizing training data, specially that the number of test graphs\n  is so tiny (300, while test could have been in the thousands too):\n\nExample, if you graph just has one causal connection with weight = 1:\n  X1 -> X2; X3; X4; X5, This is clearly equivalent to X2 -> X1; X3; X4; X5.\n  Or the structure X1 -> X2 might be present in a larger graph, example with these two components:\n  X1 -> X2; X3 -> X4 -> X5;\n\n3. Why such a low number of learning steps T (T=5 in paper) in each episode? no\nexperimentation over choice of T or discussion of this choice is\ngiven.  And it is mentioned in the findings, in several cases, that\nthe active agent is only merely comparable to the passive agent, while\none would think active would be better. If T were reasonably higher\n(not too low, not too high), one would expect to see a difference.\n\n4. Although I have concerns listed above, something about Figure 2(a)\n  seems to suggest that the agent is learning something.  I think if\n  you had tried to probe into what the agent is actually learning, it\n  would have clarified many doubts.\n\nHowever, in Figure 2(c), if the black node is -5, why is the node\nbelow left at -2.5?  The weight on the edge is 1 and other parent is\n0, so -2.5 seems extremely unlikely, given that the variance is 0.1\n(stdev ~ 0.3, so ~8 standard deviations away!).  (Similar issue in\nFigure 3c)\n\n5. Although the random choice would result in a score of -5/4, I think\n  it's quite easy and trivial to beat that by just ignoring the node\n  that's externally intervened on and assigned -5, given it's a small\n  value. This probably doesn't require the agent to be able to do\n  \"causal reasoning\" ...  That immediately gives you a lower bound of\n  0.  That might be more appropriate.\n\n  If you could give a distribution of the max(mean_i(Xi)) over all\n  graphs (including weights in your distribution), it could give an\n  idea of how unlikely it is for the agent to get a high score without\n  actually learning the causal structure.\n\nSuggestions for improving the work:\n\n- Provide results on wider range of experiments (eg more even\n  train-test split, choice of T), or at minimum justify choices\n  made. And address the issues above.\n\n- Focus on more intuitive notions that clearly require causal\n  knowledge, or motivate your objective very clearly to show its\n  sufficiency.\n\n- Perhaps discuss simpler examples (e.g., 3 node), where it's easy to\n  enumerate all causal structures and group them into appropriate\n  equivalence classes.\n\n- Please proof-read and make sure you've defined all terms (there are\n  a few, such as Xp/Xf in Expt 3, where p/f are not really defined).\n\n- You could show a few experiments with larger N by sampling from the space of all possible\n  DAGs, instead of enumerating everything.\n\nOf course, it would be great if you can probe the agent to see what it\nreally learnt. But I understand that could be a long-shot.\n\nAnother open problem  is whether this approach can scale to larger number of\nvariables, in particular the learning might be very data hungry.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}