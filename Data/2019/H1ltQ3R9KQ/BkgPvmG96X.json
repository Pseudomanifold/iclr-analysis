{"title": "Potentially interesting, but possibly not ready yet", "review": "This paper aims at training agents to perform causal reasoning with RL in three settings: observational (the agent can only obtain one observational sample at a time), interventional (the agent can obtain an interventional sample at a time for a given perfect intervention on a given variable) and a counterfactual setting (the agent can obtains interventional samples, but the prediction is about the case in which the same noise variables were sampled, but a different intervention was performed) . In each of these settings, after T-1 steps of information gathering, the algorithm is supposed to select the node with the highest value in the last step. Different types of agents are evaluated on a limited simulated dataset, with weak and not completely interpretable results.\n\nPros:\n-Using RL to learn causal reasoning is a very interesting and worthwhile task.\n-The paper tries to systematize the comparison of different settings with different available data.\n\n\nCons:\n-Task does not necessarily require causal knowledge (predict the node with the highest value in this restricted linear setting)\n-Very limited experimental setting (causal graphs with 5 nodes, one of which hidden, linear Gaussian with +/- 1 coefficients, with interventions in training set always +5, and in test set always -5) and lukewarm results, that don\u2019t seem enough for the strong claims. This is one of the easiest ways to improve the paper.\n-In the rare cases in which there are some causal baselines (e.g. MAP baseline), they seem to outperform the proposed algorithms (e.g. Experiment 2)\n-Somehow the \u201cactive\u201d setting in which the agent can decide the intervention targets seems to always perform worse than the \u201cpassive\u201d setting, in which the targets are already chosen. This is very puzzling for me, I thought that choosing the targets should improve the results...\n-Seem to be missing a lot of literature on causality and bandits, or reinforcement learning (for example: https://arxiv.org/abs/1606.03203, https://arxiv.org/abs/1701.02789, http://proceedings.mlr.press/v70/forney17a.html)\n-Many details were unclear to me and in general the clarity of the description could be improved\n\nIn general, I think the paper could be opening up an interesting research direction, but unfortunately I\u2019m not sure it is ready yet. \n\n\nDetails:\n-Abstract: \u201cThough powerful formalisms for causal reasoning have been developed, applying them in real-world domains is often difficult because the frameworks make idealized assumptions\u201d. Although possibly true, this sounds a bit strong, given the paper\u2019s results. What assumptions do your agents make? At the moment the agents you presented work on an incredibly small subset of causal graphs (not even all linear gaussian models with a hidden variable\u2026), and it\u2019s even not compared properly against the standard causal reasoning/causal discovery algorithms...\n-Footnote 1: \u201cthis formalism for causal reasoning assumes that the structure of the causal graph is known\u201d - (Spirtes et al. 2001) present several causal discovery (here \u201ccausal induction\u201d) methods that recover the graph from data.\n-Section 2.1 \u201cX_i is a potential cause of X_j\u201d - it\u2019s a cause, not potential, maybe potentially not direct.\n-Section 3: 3^(N-1)/2 is not the number of possible DAGs, that\u2019s described by this sequence: https://oeis.org/A003024. Rather that is the number of (possibly cyclic) graphs with either -1, 1 or 0 on the edges. \n-\u201cThe values of all but one node (the root node, which is always hidden)\u201d - so is it 4 or 5 nodes? Or it is that all possible DAGs on N=6 nodes one of which is hidden? I\u2019m asking because in the following it seems you can intervene on any of the 5 nodes\u2026 \n-The intervention action is to set a given node to +5 (not really clear why), while in the quiz phase (in which the agent tries to predict the node with the highest variable) there is an intervention on a known node that is set to -5 (again not clear why, but different from the interventions seen in the T-1 steps). \n-Active-Conditional is only marginally below Passive-Conditional, \u201cindicating that when the agent is allowed to choose its actions, it makes reasonable choices\u201d - not really, it should perform better, not \u201cmarginally below\u201d... Same for all the other settings\n-Why not use the MAP baseline for the observational case?\n-What data does the Passive Conditional algorithms in Experiment 2? Only observations  (so a subset of the data)?\n-What are the unobserved confounders you mention in the results of Experiment 2? I thought there is only one unobserved confounder (the root node)? Where do the others come from?\n-The counterfactual setting possibly lacks an optimal algorithm? \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}