{"title": "Good Paper, which achieves the competitive results over the state-of-the-art methods.", "review": "1. The abstract of this paper should be further refined. I could not find the technical contributions of the proposed method in it.\n\n2. The proposed method for training BNNs in Section 3 is designed by combining or modifying some existing techniques, such as regularized training and approximated gradient. Thus, the novelty of this paper is somewhat weak.\n\n3. Fcn.3 is a complex function for deep neural networks, which integrates three terms of x. I am worried about the convergence of the proposed method.\n\n4. Fortunately, the performance of the proposed method is very promising, especially the results on the Imagenet, which achieves the highest accuracy over the state-of-the-art methods. Considering that the difficulty for training BNNs, I vote it for acceptance.  \n\n---------------------------------\n\nAfter reading the responses from authors, I have clearer noticed some important contributions in the proposed methods:\n\n1) A novel regularization function with a scaling factor was introduced for improving the capability of binary neural networks; \n2) The proposed activation function can enhance the training procedure of BNNs effectively;\n3) Binary networks trained using the proposed method achieved the highest performance over the state-of-the-art methods.\n\nThus, I think this is a nice work for improving performance of  binary neural networks, and some of techniques in this paper can be elegantly applied into any other approaches such as binary dictionary learning and binary projections. Therefore, I have increased my score.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}