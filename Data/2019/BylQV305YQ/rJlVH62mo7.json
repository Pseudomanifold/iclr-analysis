{"title": "Empirical explanation of the impact of staleness ", "review": "This paper tries to analyze the impact of the staleness on machine learning models in different settings, including model complexity, optimization methods or the number of workers. In this work, they study the convergence behaviors of a wide array of ML models and algorithms under delayed updates, and propose a new convergence analysis of asynchronous SGD method for non-convex optimization.\n\nThe following are my concerns:\n1. \"For CNNs and DNNs, the staleness slows down deeper models much more than shallower counterparts.\" I think it is straightforward. I want to see the theoretical analysis of the relation between model complexity and staleness.  \n2. \"Different algorithms respond to staleness very differently\".  This finding is quite interesting. Is there any theoretical analysis of this phenomenon?  \n3. The \"gradient coherence\"  in the paper is not new. I am certain that \"gradient coherence\" is very similar to the \"sufficient direction\" in [1]. \n4. What is the architecture of the network? in the paper, each worker p can communicate with other workers p'. Does it mean that it is a grid network? or it is just a start network. \n5. in the top of page 3, why the average delay under the model is 1/2s +1, isn't it (s-1)/2? \n6.  on page 5, \"This is perhaps not surprising, given the fact that deeper models pose more optimization challenges even under the sequential settings.\" why it is obvious opposite to your experimental results in figure 1(a)? Could you explain why shallower CNN requires more iterations to get the same accuracy? it is a little counter-intuitive.\n7. I don't understand what does \"note that s = 0 execution treats each worker\u2019s update as separate updates instead of one large batch in other synchronous systems\" mean in the footnote of page 5.\n\n\nAbove all, this paper empirically analyzes the effect of the staleness on the model and optimization methods. It would be better if there is some theoretical analysis to support these findings.\n\n[1] Training Neural Networks Using Features Replay  https://arxiv.org/pdf/1807.04511.pdf\n\n\n===after rebuttal===\nAll my concerns are addressed. I will upgrade the score.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}