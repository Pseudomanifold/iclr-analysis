{"title": "Interesting approach, proposes representation augmentation as opposed to representation learning and the proposed distance not used for training.", "review": "The paper proposes a method to augment representation of an entity (such as a word) from standard \"point in a vector space\" to a histogram with bins located at some points in that vector space. In this model, the bins correspond the context objects, the location of which are the standard point embedding of those objects, and the histogram weights correspond to the strength of the contextual association. The distance between two representations is then measured with, Context Mover Distance, based on the theory of optimal transport, which is suitable for computing the discrepancy between distributions. \nThe representation of a sentence is proposed to be computed as the barycenter of the representation of words inside.\nEmpirical study evaluate the method in a number of semantic textual similarity and hypernymy detection tasks. \n\nThe topic is important. The paper is well written and well structured and clear. The method could be interesting for the community. However, there are a number of conceptual issues that make the design a little surprising. First, the method does not learn the representations. Instead, augments a given one and computes the context mover distance on top of that. But, if the proposed context mover distance is an effective distance, maybe representations are better to be \"learned\" based on the same distance rather than being received as inputs.\nAlso, whether an object is represented as a single point or as a distribution seems to be an orthogonal matter to whether the context predicts the entity or vice versa. This two topics are kind of mixed up in the discussions in this paper.\n\nOther issues:\n\n- One important technicality which seems to be missing is the exact value of p in Wp which is used. This becomes important for barycenters computations and the uniqueness of barycenters. \n- Competitors in Table 1 are limited. Standard embedding methods are missing from the list.\n- Authors raise a question in the title of the paper, but the content of the paper is not much in the direction of trying to answer the question. \n- It is not clear why the \"context\" of hyponym is expected to be a subset of the context of the hypernym. This should not always be true.\n- Table 4 gives the impression that parameter might not be set based on performance on validation set, but instead based on the performance on the test set.\n\n- Minor:\nof of\ndata ,\nby\nbyMuzellec\nCITE\n\nOverall, comparing strengths and shortcomings of the paper, I vote for the paper to be marginally accepted.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}