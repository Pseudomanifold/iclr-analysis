{"title": "Unconvincing", "review": "The authors examine the effect of logit regularization techniques on the robustness of neural network models to adversarial examples, under the threat model of an L-infinity attacker. Logit pairing is a recently proposed defense technique, which adds an L-2 loss penalizing the difference of the logits on the correct class between the clean and adversarial example. The authors of this paper argue that logit pairing effect tends to make the distribution of clean logits smaller and adversarial logits larger. Thus this can be thought of as regularizing the logits (similar to weight decay) and the authors argue that some other recently proposed measures of defense (like label smoothing, paired example data augmentation etc) can also be understood through the lens of logit regularization. From this insight, the authors propose a new defense: by separating the difference of logit terms from the original ALP loss using (a-b)^2 = a^2 + b^2 - 2ab, they can now vary the effect of logit regularization by having different multipliers on the logit norms, and claim to have more robust models on CIFAR-10.\n\nMy main concern with this paper is that it is basing it's arguments on defenses that are well known to cause gradient masking/obfuscation. See this paper https://arxiv.org/abs/1807.10272 for an illustration of how ALP just makes the optimization landscape harder, but it is not inherently more robust. Increasing the number of PGD steps with random restarts reduces model accuracy from the claimed 27.9% to 0.6% which is worse than a PGD trained model (which gets 1.5%) on Imagenet. Therefore, one can conclude that the ALP defense is broken and I suspect that the gain from the logit regularization method that the current authors report will not stand up to scrutiny - in particular the authors only report numbers for 20 step PGD and show better numbers compared to PGD, but it is likely to be broken on increasing the number of iterations. Given the large number of questionable defense papers being accepted to deep learning conferences which are then immediately broken following publication, I think it would serve the community to reject such papers, thereby reducing the noise in this research area.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}