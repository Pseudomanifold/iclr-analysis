{"title": "Interesting read; would be helpful to better explain difficulties in training", "review": "This well-written paper addresses the restrictions imposed by binary communication channels on the deployment of latent variable models in practice. In order to range code the (floating point) latent representations into bit-strings for practical data compression, both the sender and receiver of the binary channel must have identical instances of the prior despite non-deterministic floating point arithmetic across different platforms. The authors propose using neural networks that perform integer arithmetic (integer networks) to mitigate this issue.\n\nPros:\n- The problem statement is clear, as well as the approach taken to addressing the issue.\n- Section 5 did a nice job tying together the relevant literature on using latent variable models for compression with the proposed integer network framework.\n- The experimental results are good; particularly, Table 1 provides a convincing case for how using integer networks remedies the issue of decompression failure across heterogeneous platforms.\n\nCons:\n- In Section 3, it wasn\u2019t clear to me as to why the authors were using their chosen gradient approximations with respect to H\u2019, b\u2019 and c\u2019. Did they try other approximations but empirically find that these worked best? Where did the special rescaling function s come from? Some justifications for their design choices would be appreciated. \n- The authors state in Section 2 that the input scaling is best determined empirically -- is this just a scan over possible values during training? This feels like an added layer of complexity when trying to train these networks. It would be nice if the authors could provide some insight into exactly how much easier/difficult it is to train integer networks as opposed to the standard floating point architectures.\n- In Section 6, the authors state that the compromised representational capacity of integer networks can be remedied by increasing the number of filters. This goes back to my previous point, but how does this \u201clarger\u201d integer network compare to standard floating point networks in terms of training time?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}