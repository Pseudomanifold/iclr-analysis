{"title": "Original problem and well written paper, but that lacks comparisons to baselines", "review": "\n=== Post rebuttal update ===\n\nThanks to the addition of better baselines, I've increased my score for this paper. While I'm still not super convinced of its potential for application, I find the idea original and worth discussing at the conference.\n\n=== Pre-rebuttal review ===\nThis paper presents an approach to compress a dataset into a much smaller number of synthetic samples that are optimized to yield as good performance as possible when a given model is trained on that smaller dataset. This is done by unrolling the gradient descent procedure of training such a model to allow for gradient-based optimization of synthetic samples themselves as well as the used learning rates.\n\nIn summary, my evaluation is as follow:\n\n*Pros*\n- Pretty original problem formulation\n- Generally well written paper\n\n*Cons*\n- Lack of comparison with simple baselines in basic dataset distillation setting\n- Use in practical applications (domain adaptation, data poisoning) yet to be convincingly demonstrated\n- Possibly a mistake in the theoretical analysis of the linear case\n\nIndeed, I found the paper to be generally quite clear and enjoyed reading it. One minor thing I struggled a bit with is the distinction between \"SG steps\" and \"Epochs\" (I believe the former corresponds to when the synthetic samples are different between GD steps, whereas the later corresponds to the number of times the method repeatedly cycles over these samples) so I would perhaps encourage the authors to emphasize that difference. \n\nI also find the problem statement that proposed to be interesting and thought provoking, and the solution that's proposed seems quite appropriate and well thought out.\n\nThat said, I'm worried about the following:\n\n- Unless I misunderstood, in the basic dataset distillation setting a comparison is never provided with training on a randomly selected subset of the training set. Presumably the results are worse, but I think these results should be in the paper. I would also argue for having another baseline, which would try to (approximately) optimize the choice of which training examples are put in the subset. A very simple approach would be to take the 200 runs already performed for the random selection and select the subset providing the best accuracy on the full training set and only report the performance of that subset (instead of the mean and std of all 200 runs). In short, this would help determine to what extent there is value to synthesizing entirely new samples. Moreover, I think a simple alternative baseline for creating synthesized samples should be considered. Specifically, I'd personally would like to know the performance of using per-class k-means clustering and training on the cluster centroids as the distilled dataset.\n\n- While I appreciate that the authors identify potential applications and report some results on them, I think they currently fall short of convincing the reader of the potential of dataset distillation for these applications. For domain adaptation, no actual domain adaptation baseline is compared against (a good candidate would be method from Daume III 2007, at the very least). For data poisoning, I find that the assumptions for attacks are pretty strong, i.e. that a) you have access to the parameters of the pre-trained models to attack and b) that the model is doing additional updates *only* on the synthesized data. If there are reasons to think that such assumptions are reasonable, I'd at least expect the paper to motivate why that is. \n\n- In the analysis of the simple linear case, in Equation 7, there appears to be a mistake, specifically some missing parentheses:\n\nd^Td( (I-\\eta/M d^Td)\\theta_0 + \\eta/M \\tilde{d}^T\\tilde{t}) = d^T t\n\ni.e. there should be parentheses right after \"d^Td\" and right before \"=\". This is from replacing \\theta^* by the expression for \\theta_1 in Equation 6, which is what I think Equation 7 is supposed to be doing. This possibly doesn't affect some of the conclusions taken from this section, but I'd like to see this potential mistake discussed/addressed.\n\n\nThat said, if the authors can sufficiently address the 3 points above, I'd be willing to increase my rating for this paper.\n\nFinally, I have a few other more minor (nice-to-have) points:\n- Having in the related work a discussion on the relationship with coreset methods would be nice\n- Experiments showing how well the distilled datasets transfer to different network architectures than those used in training would be interesting? Even other ML algorithms would be quite interesting?\n- \"We often find that the number of distilled images required to achieve good performance is an informative indicator of the dataset diversity\" => I'm not sure what in the paper actually justifies / demonstrates this statement.\n- Figure 4 is presented as an \"Ablation study\", but an ablation study is where you remove certains parts of a model or algorithm and see what happens, which isn't the case here. I think it's better described as a hyper-parameter sensitivity study. \n- Some typos:\n   * the below objective => the objective below\n   * w.r.t. to => w.r.t\n   * the discrete part rather => the discrete parts rather\n   * necesary => necessary\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}