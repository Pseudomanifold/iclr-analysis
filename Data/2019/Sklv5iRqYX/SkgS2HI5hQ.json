{"title": "Interesting known/unknown label multidomain learning setting, but weak evaluation", "review": "Summary:\n\nThe manuscript proposes a multi-domain adversarial learning (MDL) method called MULANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting. The authors define a new discrimination task to discriminate, within each domain, labeled samples from unlabeled ones that most likely belong to extra classes (classes with no labeled or unlabeled samples in the domain). They also introduce a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence.\n\nStrengths:\n\n-\u00a0The idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong\u00a0to extra classes is interesting.\n\n-\u00a0A new generalization bound for MDL is introduced.\n\n-\u00a0The paper was clear, well written, well-motivated and nicely structured.\n\n-\u00a0The authors perform numerous empirical experiments on several types of problems on various datasets (Digit, OFFICE,CELL) successfully showing how the MULANN can reduce the nasty effects of the adversarial domain\u00a0discriminator and repulse (a fraction of) unlabeled examples from labeled ones in each domain.\n\nWeaknesses:\n\n-\u00a0all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context. More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.\u00a0Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.\n\n-\u00a0The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier. Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples (e.g. the classifier may output high entropy for the unlabeled samples of the classes with labeled samples) and they may harm the performance of adaptation.\u00a0 It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.\n\n-\u00a0Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL (MULANN handles only the class asymmetry when domains involve distinct sets of classes and it has nothing to do with MDL). hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.\n\n-\u00a0Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).\n\nThe reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.\n\n[1]\u00a0Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\"\u00a0Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017.\n\n[2]\u00a0Bousmalis, Konstantinos, et al. \"Domain separation networks.\"\u00a0Advances in Neural Information Processing Systems. 2016.\n\n[3]\u00a0Zhao, Han, et al. \"Multiple Source Domain Adaptation with Adversarial Learning.\" Advances in Neural Information Processing Systems. 2018.\n\n[4]\u00a0Hoffman, Judy, Mehryar Mohri, and Ningshan Zhang. \"Algorithms and Theory for Multiple-Source Adaptation.\"\u00a0\u00a0Advances in Neural Information Processing Systems. 2018.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}