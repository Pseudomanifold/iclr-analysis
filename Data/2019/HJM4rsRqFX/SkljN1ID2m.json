{"title": "Interesting variational framework, results not convincing enough", "review": "* The paper proposed a neural variational inference framework for knowledge graph embedding. The paper proposed two models (Latent Fact Model and Latent Information Model) where the neural variational inference is carried out, with competitive results on standard datasets (WN18 and FB15K).\n\n* I am not fully convinced of the advantage of this variational inference approach compared with the optimization approach used in TransE, TransG, DistMult and ComplEx. As can be seen in Table 1, the best performance on WN18-RR and FB15K-257 are obtained without variational inference. In Table 2, performance of the variational inference approach is not as good as other approaches under the MR or Raw Hits metrics. Moreover, performance on FB15K is not reported in Table 2, which makes the result not as complete as Table 1.\n\n* It seems that the main difference between Latent Fact Model and Latent Information Model is the way prior is imposed on h. The authors may want to explain in plain language the differences and the motivation behind that.\n\n* It is unclear how the (\\tau,y) labeled triples are generated, especially for the negative examples with y=0. Is it obtained by randomly corrupting the triples in the knowledge base, as done in other work? It would be better to make this point clear.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}