{"title": "Nice results but difficult to understand", "review": "This paper proposes a new architecture for learning dynamics models in 2D Atari-like game words. The architecture includes multiple layers of abstraction: a \u201cmotion detection\u201d level, which looks at which pixels change over time in order to guess at which parts of the image are in the foreground or not; a \u201cinstance segmentation\u201d level, which segments the foreground into regions and instances; and a \u201cdynamics learning\u201d level, which learns the dynamics of object instances using a interaction network-style approach.\n\nPros:\n- Impressive-looking dynamics predictions in Atari-like games.\n- An object-based prediction model, which could enable predictions about specific entities in the scene rather than holistic frame predictions.\n\nCons:\n- Very complicated and difficult-to-understand architecture.\n- No ablation studies to validate different components of the architecture.\n- No validation in a model-based RL or control setting.\n- Experiments are only done on one-step predictions, rather than long-term rollouts.\n\nQuality\n---------\n\nThe quality of the predictions seems quite high (based on Figure 6 and the results tables), though there are a number of opportunities to further strengthen the evaluation and analysis:\n\n- I wish that there were more than a single figure of qualitative results to go on. I highly recommend that a revision include a link to a video showing more predictions over time for each environment, ideally with comparisons to the other baselines as well.\n- The introduction of the paper motivates the learning of the model in terms of model-based RL, however, the model is not actually used in a model-based RL setting. It would be nice to see at least a simple validation that the model can be used with an off-the-shelf planner to solve one of the games which are evaluated in the paper. If it cannot, then that limits the significance of the model.\n- As far as I can tell, all the results reported in the tables are based on one-step predictions only. While it is great to show that even in this regime the other models struggle, it would be even better if results could be reported for longer rollouts (i.e., taking the model outputs and feeding it back in as input, and repeating this procedure say 50 steps into the future). Models are not particularly useful in a MBRL setting if they can only be used to predict a single timestep, so it is important to validate that longer-term predictions can be made as well.\n\nOverall the literature review is reasonably solid, but I am not sure the citations in the opening sentence are quite appropriate as model-based DRL has been around for longer than 2017 (see for example [1-3]). Moreover, Chiappa et al (2017) only learns a model and does not use it for planning, so I am not sure it is quite appropriate as a citation for MBRL. \n\n\nClarity\n--------\n\nUnfortunately, I had a very hard time understanding how exactly the architecture works and I felt like there were a lot of details missing. I am not confident that I would be able to reproduce the architecture from reading the paper alone. Below, I will list some of the specific points where I was confused, but I think overall the paper needs to be substantially reorganized in order to be clearer as to how the architecture actually works.\n\nMore broadly, I think some of my confusion stems from the fact that there are very similar computations occurring across the three levels of abstraction but the paper does not really make it clear how these computations relate to one another or how they are similar/different. For example, in the \u201cdynamics learning\u201d level there are modules for performing object detection and instance localization. But then in the \u201cinstance segmentation\u201d level, there are similarly modules for detecting and masking out instances. It is not clear to me why this needs to be done twice? \n\nIn general, I would *strongly* recommend including at least in the appendix an algorithm box that sketches out the computational graph for the whole architecture (not in as much detail as the existing algorithm boxes, but in more detail than what is given in Figure 1).\n\nSpecific places where I was confused:\n\n- Where do the region proposals (P) come from?\n- If I\u2019m understanding correctly, the variable M is used multiple times in multiple different ways. It seems to be produced from the \u201cinstance localization\u201d module in the \u201cdynamics learning\u201d level, but also from the \u201cdynamic instance segmentation network\u201d in the \u201cinstance segmentation\u201d level. Are these M different or the same?\n- Where does F_foreground^(t) come from?\n\n\nOriginality\n-------------\n\nOverall, the idea of learning object-based transition models is not really new (and there are a few citations missing regarding prior work in this regard, e.g. [4-6]). However, there is yet to be an accepted solution for actually learning object-based models robustly and the present work seems to result in the cleanest separation between dynamic objects and background that I have seen so far, and is therefore quite original in that regard.\n\nThis paper appears to be quite similar to Zhu & Zhang (2018), with the main difference being additional functionality to handle multiple dynamic objects in a scene rather than just a single dynamic object. This is a fairly significant difference and the improvement over Zhu & Zhang (2018) seems quite large, so even though the papers seem quite similar on the surface I think the difference is actually quite substantial.\n\nSignificance\n----------------\n\nIf it were clearer how to reproduce this paper, and if it could be shown to apply to a wider range of environments (e.g. the Atari suite, or even better the Sonic domains from the OpenAI Retro contest), then I believe this paper could be quite significant as it would open up new avenues for model-based learning in these domains. Unfortunately, however, it is not clear to me as the paper is currently written how well it would do on other 2D environments, thus limiting the significance. If the model only works on Monster Kong and Flappy Bird---neither of which are commonly used in the RL literature---then it has limited applicability to the rest of the model-based RL community. Similarly, as stated above, it is not clear how well the model will work with longer rollouts or in actual in MBRL settings, thus limiting its significance.\n\nReferences\n---------------\n\n[1] Heess, Wayne, Silver, Lillicrap, Tassa, & Erez (2015). Learning Continuous Control Policies by Stochastic Value Gradients. NIPS 2015.\n[2] Gu, Lillicrap, Sutskever, & Levine (2016). Continuous Deep Q-Learning with Model-based Acceleration. ICML 2016.\n[3] Schmidhuber (2015). On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models. arXiv 2015.\n[4] Wu, Yildirim, Lim, Freeman, & Tenenbaum (2015). Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning. NIPS 2015.\n[5] Fragkiadaki, Agrawal, Levine, & Malik (2016). Learning visual predictive models of physics for playing billiards. ICLR 2016.\n[6] Kansky, Silver, Mely, Eldawy, Lazaro-Gredilla, Lou, Dorfman, Sido, Phoenix, & George (2017). Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics. ICML 2017.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}