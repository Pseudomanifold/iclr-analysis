{"title": "Good results, but method unconvincing", "review": "Summary:\nThis paper presents an end-to-end learnable deep network for action recognition that estimates \"optical flow\" within the neural network layers. The \"flow\" is quoted as it may not be the optical flow in traditional sense over image frames, but could be the \"flow\" over the intermediate channels of a layer from the two frames.  The paper further extends this idea to compute the flow of flow. For the flow computation, the paper uses the standard TV-L1 scheme, but its steps are being done within the network and thus could be back propagated. Finally, the entire framework is trained on a loss defined over action classification. Experiments are presented on Kinetics and HMDB datasets, and show good performance.\n\nStrengths:  I think the main strength of this paper is its competitive performances in Table 9 against RGB only methods. There are also various analysis presented on several architectural choices for the fusion with the flow layer.\n\nWeaknesses: \n1) I think the main weakness of this paper is its lack of any significant novelty, or inadequate coverage of state of the art in deep learning based flow estimation. For example, there is prior work such as FlowNet or FlowNet2.0 that could compute optical flow in an end-to-end manner. This paper does not cite these prior works and in the context of which the novelty as claimed in the paper is not substantial.\n\n2) Another important weakness is the lack of any convincing argument on why it is a good idea to compute/apply an optical flow algorithm on the intermediate feature maps of a network? How is the fundamental assumption of flow -- the brightness constancy -- applicable to such intermediate layers? \n\n3) Further, while the idea of proposing the deep variant of flow was to avoid the computational expense of an otherwise out-of-the-box flow algorithm, the paper ultimately has to resort to several heuristic workarounds such as low-resolution of the inputs, or reducing the iterations of flow optimization, etc. to make the model practical. The flow estimation of the feature maps also require trimming down the number of channels in a layer. \n\n4) It is surprising to see that even after such heuristic workarounds to compute an approximate flow, the final performance of the model is compelling; which needs more analysis to understand where precisely is the proposed scheme gaining in performance even with these approximations in comparison to an otherwise accurate external optical flow scheme?\n\n5) Finally, given that the paper proposes to optimize the model against the action classification loss, it is unclear to me if the proposed flow layers are in fact learning anything related to flow, or why should they learn flow, or for that matter, if the proposed iterative scheme is even necessary? If learning flow is important, shouldn't there be some intermediate objective that ensures that the flow layers do learn flow, using a suitable loss? For example, as in the FlowNet2.0 paper. \n\nMinor comments: \n1) The section 3.1 on optical flow methods could be improved to be more intuitive. For example, it is unclear what \\rho_c is capturing, and Eq.(2) seems to be just |I_2-I_1|.  What precisely are the roles of \\lambda, \\theta etc? How is the second term \\lambda |I_1-I_2| useful in (7) ? Isn't it a constant?\n\n2) More details and discussions need to be added to the comparisons against state of the art in Table 9. Why are the HMDB results missing from the middle column? \n\nOverall, I think the paper has some interesting results, however the method is unconvincing/unclear why it should work nor its novelty commendable.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}