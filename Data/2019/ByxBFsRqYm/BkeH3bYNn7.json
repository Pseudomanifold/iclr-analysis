{"title": "A nice contribution to neural combinatorial optimisation", "review": "This paper is one of a sequence of works trying to learn heuristics for solving combinatorial optimisation problems. Compared to its predecessors, its contributions are three-fold. First, it introduces a tweak on the REINFORCE learning algorithm, outperforming more complicated methods. Second, it introduces a new model for combinatorial tasks which delivers interesting results on several tasks which are varied though related. Finally, it evaluates this model on many tasks.\n\n****Quality and clarity****\nThis is a very high-quality paper. \nThe writing is clear and sharp, and the reading experience is quite enjoyable (the witty first paragraph sets the tone for what is to follow), even if the text is at times a bit verbose. \nAnother point to commend is the honesty of the paper (see e.g. the comment on the performance of the model on TSP vs specialised solvers such as Concord).\nThe related work section is complete and well documented.\nFinally, the experimental results are clearly presented and well-illustrated.\n\n****Originality and significance****\nOn the theoretical side, the contributions of this paper are interesting but not ground-breaking. The REINFORCE tweak is close to other algorithms that have been tried in the last few years (such as indeed the one presented in Rennie et al, 2016). The model architecture, while successful, is not a large departure from the Transformer presented in Vaswani et al, 2017.\n\nMore significant is the complete set of experiments on a varied subset of combinatorial tasks, which showcases one of the promises of using machine learning for combinatorial optimisation: reusability of a single model for many tasks.\n\n****Conclusion****\nOverall, this is a nice, very well-written paper. Its contributions, though not ground-breaking, are significant to the field, and constitute another step in the right direction.\n\nPros\n- high-quality writing\n- very clear\n- complete experiments on a variety of tasks, some of which do not have optimal solvers\n- honest assessment of the model\n\nCons\n- the theoretical contributions are not ground-breaking (either the the tweak on REINFORCE or the model architecture)\n- the model is still far from obtaining meaningful results on TSP (although it's interesting to compare to previous learned models, only solving problems with 100 nodes also illustrates how far we have to go...)\n\nDetails\n- Dai et al has been published at NIPS and is no longer an arxiv preprint\n- the comparison to AlphaGo should either be expanded upon or scratched. Although it could be quite interesting, as it is it's not very well motivated.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}