{"title": "Scaling up of Neural ODE model for generative model", "review": "This paper further explores the work of Chen et al. (2018) applied to reversible generative modelling. While section 1 and 2 focuses on framing the context of this work. The ODE solver architecture for continuous normalizing flow learn a density mapping using an instantaneous change of variable formula.\nThe contribution of this work seems to be enabling the use of deeper neural network than in Chen et al. (2018)  as part of the ODE solver flow. While the single-layer architecture in Chen et al. (2018) enable efficient exact computation of the Jacobian Trace, using a deeper architecture compromises that property. As a result, the authors propose to use the unbiased Hutchinson trace estimator of the Jacobian Trace. Furthermore, the authors observe that using a bottleneck architecture reduces the rank of the Jacobian and can therefore help reducing the variance of the estimator. \nThe density estimation task in 2D is nice to see but lacks comparison with Chen et al. (2018), on which this paper improves. Moreover, is the Glow model used here only using additive coupling layers? If so, this might explain the difficulties of this Glow model. \nAlthough the model presented in this paper doesn't obtain state-of-the-art results on the larger problems, the work presented in this paper demonstrates the ability of ODE solvers as continuous normalizing flows to be competitive in the space of prescribed model.\nConcerning discussions and analysis:\n- given the lack of improvement using the bottleneck trick, is there an actual improvement in variance using this trick? or is this trick merely explaining why using a bottleneck architecture more suited for the Hutchinson trace estimator?\nIn algorithm 1, is \\epsilon only one random vector that keeps being reused at every step of the solver algorithm? I would be surprised that the use of a single random vector across different steps did not significantly increased the variance of the estimator.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}