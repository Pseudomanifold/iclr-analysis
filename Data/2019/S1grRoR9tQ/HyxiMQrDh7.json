{"title": "The proposed SGLD-SA algorithm with its convergence properties is interesting", "review": "* The proposed SGLD-SA algorithm, together with its convergence properties, is very interesting. The introduction of step size $w^{k}$ is very similar to the \"convex combination rule\" in (Zhang & Brand 2017) to guarantee convergence.\n  \n* It seems that this paper only introduced Bayesian inference in the output layers. It would be more interesting to have a complete Bayesian model for the full network including the inner and activation layers.\n\n* This paper imposed spike-and-slab prior on the weight vector which can yield sparse connectivity. Similar ideas have been explored to compress the model size of deep networks (Lobacheva, Chirkova and Vetrov 2017; Louizos, Ullrich and Welling 2017 ). It would make this paper stronger to compare the sparsification and compression properties with the above work.\n\n* In equation (11) there is a summation from $\\beta_{p+1}$ to $\\beta_{p+u}$. I wonder where this term comes from, as I thought $\\beta$ is a vector of dimension $p$.\n\nReference:\nZhang, Ziming, and Matthew Brand. \"Convergent block coordinate descent for training tikhonov regularized deep neural networks.\" Advances in Neural Information Processing Systems. 2017.\n\nLobacheva, Ekaterina, Nadezhda Chirkova, and Dmitry Vetrov. \"Bayesian Sparsification of Recurrent Neural Networks.\" arXiv preprint arXiv:1708.00077 (2017).\n\nLouizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}