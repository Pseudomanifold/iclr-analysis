{"title": "Interesting work, but in my view not substantial novelty and significance", "review": "TITLE\nBayesian deep learning via stochastic gradient mcmc with a stochastic approximation adaptation\n\nREVIEW SUMMARY\nFairly well written paper on SG-MCMC type inference in neural networks with slab and spike priors. In my view, the originality and significance is limited.\n\nPAPER SUMMARY\nThe paper develops a method for sampling/optimization of a Bayesian neural network with slab and spike priors on the weights.\n\nQUALITY\nI belive the contribution is technically sound (but I have not checked all equations or the proof of Theorem 1). The empirical evaluation is not unreasonable, but also not strongly convincing.\n\nCLARITY\nThe paper is fairly well written, but grammar and use of English could be slightly improved (not so important).    \n\nORIGINALITY\nThe paper builds on existing work on EM-type algorithms for slab and spike models and SG-MCMC for Bayesian inference in neural networks. The novelty of the contribution is limited: The main contribution is the combination of the two methods and some theoretical results. I am not able to judge if there is significant originality in the theoretical results (Theorem 1 + Corr 1+2) but if I am not mistaken it is more or less an application of a known result to this particular setting?\n\nSIGNIFICANCE\nWhile I think the proposed algorithm is reasonable and most likely useful in practice, I am not sure the contribution is substantial enough to gain large interest in the community.  \n\nFURTHER COMMENTS\nFigure 2 (d+e) are in my view not so useful for assessing the training/test performance, but I am not even completely sure what the figures shows, as there are no axis labels. I would prefer some results on the loss, perhaps averaged over multiple data sets.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}