{"title": "Interesting paper", "review": "Quality: This paper proposes learning to plan approach that can learn to search with an inner agent; conditioned on the output of the inner agent (IA) the outer agent starts to learn a reactive policy in the environments. The inner agent, different from other searching agent, learns to decide what searching pattern to choose. The presented method shows better computation efficiency than competitive baselines. The main applications are on \"box pushing\" game and grid-world navigation.   \n\nclarity: The paper is well-written.\noriginality: The paper is original. \nsignificance: This paper shows a promising method to combine traditional search method with machine learning techniques and therefore boost sample-efficiency of RL method. \n\ncons:\n1. The dynamics model used to plan is given and fully observable. That means a pure Monte-Carlo tree search can achieve very high accuracy.  In the figure 6, AtreeC can also have good performance after 4e7 steps, even better than the proposed method. I am wondering what would happen if 4e7 steps were applied to the proposed method.\n2.  One argument from the paper is that their method is computationally efficient. However, this should be presented in a more realistic test environment. In the push and gridworld environment, 84 steps of planning wouldn't be too bad. So a demonstration of the effectiveness from the proposed method on a visually complex game would be great.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}