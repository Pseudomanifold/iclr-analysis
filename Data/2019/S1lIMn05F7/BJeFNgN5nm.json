{"title": "An interesting method for robust deep learning", "review": "The paper \"A Direct Approach to Robust Deep Learning Using Adversarial Networks\" proposes a GAN solution for deep models of classification, faced to white and black box attacks. It defines an architecture where a generator network seeks to produce slight pertubations that succeed in fooling the discriminator. The discriminator is the targetted classification model. \n\nThe paper is globally well written and easy to follow. It well presents related works and the approach is well justified. Though the global idea is rather straightforward from my point of view, it looks to be a novel - effective - application of GANs. The implementation is well designed (it notably uses recent GAN stabilization techniques). The experiments are quite convincing, since it looks to produce rather robust models, without a loss of performance with clean (which appears crucial to me and  is not the case of its main competitors). \n\nMinor comments:\n    - eq1 : I do not understand the argmax (the support is missing). It corresponds to the class with higher probability I suppose but...\n    - Authors say that GANs are usually useful for the generator (this is not always the case by the way), while in their case both obtained discriminator and generator have value. I do not understand in what the generator could be useful here, since it is only fitted to attack its own model (so what is the interest, are its attacks transferable on other models?)\n    - Tables 1 and 2 are described as giving attack accuracies. But scores reported are classification accuracy right ? This is rather defense accuracies so...\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}