{"title": "Algorithm and presentation are flawed", "review": "The submission made a few modifications to the RDA (regularized dual averaging) optimization solver to form the proposed \"iterative RDA (iRDA)\" algorithm, and shows that empirically the proposed algorithm could  reduce the number of non-zero parameters by an order of magnitude on CIFAR10 for a number of benchmark network architectures (Resnet18, VGG16, VGG19).\n\nThe experimental result of the paper is strong but the algorithm and also a couple of statements seem flawed. In particular:\n\n* For Algorithm 1, consider the case when lamda=0 and t -> infinity,  the minimization eq (28) goes to negative infinity for any non-zero gradient, which corresponds to an update of infinitely large step size. It seems something is wrong.\n\n* Why in Step 2 the algorithm sets both g_t and w_t to 0 during each iterate? It looks so wrong.\n\n*The whole paper did not mention batch size even once. Does the algorithm apply only with batch size=1? \n\n*What is the \"MRDA\" method in the figure? Is it mentioned anywhere in the paper?\n\n*What are \"k\", \"c\"  in eq (25)? Are they defined anywhere in the paper?\n\n*Theorem states 1/sqrt(t) convergence but eq (28), (31) have updates of unbounded step size. How is this possible?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}