{"title": "The paper is not written well and needs major modifications. The contribution of the paper which is analyzing RDA with arbitrary init point is a small incremental contribution. ", "review": "iRDA Method for sparse convolutional neural networks \n\nThis paper considers the problem of training a sparse neural network. The main motivation is that usually all state of the art neural network\u2019s size or the number of weights is enormous and saving them in memory is costly. So it would be of great interest to train a sparse neural network. To do so, this paper proposed adding l1 regularizer to RDA method in order to encourage sparsity throughout training. Furthermore, they add an extra phase to  RAD algorithm where they set the stochastic gradient of zero weights to be zero. They show experimentally that the method could give up to 95% sparsity while keeping the accuracy at an acceptable level. \nMore detail comments: \n\n1- In your analysis for the convergence, you totally ignored the second step. How do you show that with the second step still the method converge? \n\n2- \\bar{w} which is used in the thm 1, is not introduced. \n\n3- In eq 5, you say g_t is subfunction. What is it? \n\n4- When does the algorithm switch from step 1 to step 2? \n\n5- In eq 35 what is \\sigma? \n\n6- What is the relation between eq 23 and 24? The paper says 23 is an approximation for 24 but the result of 23 is a point and 24 is a function. \n\n7- What is MRDA in the Fig 1? \n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}