{"title": "Review of sparse dictionary learning by dynamical neural networks", "review": "The seminal work of Olshausen and Field on sparse coding is widely accepted as one of the main sources of inspiration for dictionary learning. This contribution makes the connection from dictionary learning back to a neuronal approach. Building on the Local Competitive Algorithm (LCA) of Rozell et al. and the theoretical analysis of Tang et al., this submission revisits the dictionary learning under two constraints that the gradient is learned locally and that the neural assemblies maintain consistent weight in the network. These constraints are relevant for a better understanding of the underlying principles in neuroscience and for applicative development on neuromorphic chipsets.\n\nThe proposed theorems extend the previous work of sparse coding with spiking neurons and address the update of dictionary using only information available from local neurons. The submission cares as well for the possible implementation on parallel architectures. The numerical experiments are conducted on three datasets and show the influence of weight initialization and the convergence on each dataset. An example of image denoising is provided in appendix. ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}