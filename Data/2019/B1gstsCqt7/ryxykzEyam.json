{"title": "An interesting, fully local learning approach to sparse coding", "review": "This paper proposes a dynamical neural network for sparse coding where all the interactions terms are learned.  In previous approaches (Rozell et al.) some weights were tied to the others.  Here the network consists of feedforward, lateral, and feedback weights, all of which have their own learning rule.  The authors show that the learned weights converge to the desired solution for solving the sparse coding objective.  This seems like a nice piece of work, an original approach that solves a problem that was never really fully resolved in previous work, and it brings things one step closer to both neurobiological plausibility and hardware implementation.\n\nOther comments:\n\nWhat exactly is being shown in Figure 2 is still not clear to me.\n\n It would be nice to see some other evaluations, for example sparsity vs. MSE tradeoff (this is reflected in the objective function in part but it would be nice to see the tradeoff).  \n\nThere is recent work from Mitya Chklovskii's group on \"similarity matching\" that also addresses the problem of developing a fully local learning rule.  The authors should incorporate a discussion of this in their final paper.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}