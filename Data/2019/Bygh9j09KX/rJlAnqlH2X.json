{"title": "Review of ImageNet-trained CNNs are biased towards texture", "review": "Review of ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. \n\nIn this submission, the authors provide evidence through clever image manipulations and psychophysical experiments that CNNs image recognition is strongly influenced by texture identification as opposed the global object shape (as opposed to humans). The authors attempt to address this problem by using image stylization to augment the training data. The resulting networks appear much more aligned with human judgements and less biased towards image textures.\n\nIf the authors address my major concerns, I would increasing my rating 1-2 points.\n\nMajor Comments:\n\nThe results of this paper are quite compelling and address some underlying challenges in the literature on how CNN's function. I particularly appreciated Figure 5 demonstrating how the resulting stylized-augmented networks more closely align with human judgements. Additionally, it is surprising to me how poor BagNet performs on Stylized-ImageNet (SIN) implying that ResNet-50 trained on Stylized ImageNet may be better perceptually aligned with global object structure. Very cool.\n\n1. Please make sure to tone down the claims in your manuscript. Although I share enthusiasm for your results, please recognize that stating that your results are 'conclusive' is premature and not appropriate. (Conclusive requires more papers and much work by the larger scientific community for a hypothesis to become readily accepted). Some sentences of concern include:\n\n  --> \"These experiments provide conclusive behavioural evidence in favour of the texture hypothesis\"\n  --> \"we conclude the following: Textures, not object shapes, are the most important cues for CNN object recognition.\"\n\nI would prefer to see language such as \"We provide evidence that textures provide a more powerful statistical signal then global object shape for CNNs.\" or \"We provide evidence that CNNs are overly sensitive to textures in comparison to humans perceptual judgements\". This would be more measured and better reflect what has been accomplished in this study. Please do a thorough read of the rest of your manuscript and identify other text accordingly.\n\n2. Domain shifts and data augmentation. I agree with your comment that domain shifts present the largest confound to Figure 2. The results of Geirhos et al, 2018 (Figure 4) indicate that individual image augmentations/distortions do not generalize well. Given these results, I would like to understand what image distortions were used in training each and all of your networks. Did you try a baseline with no image distortions (and/or just Stylized-ImageNet)?\n\nAlthough the robustness in Figure 6 are great, how much of this can be attributed solely to Stylized-ImageNet versus the other types of image distortions/augmentations in each network. For instance, would contrast-insensitivity in Stylized-ImageNet diminish substantially if no contrast image distortion were used during training?\n\n3. Semantics of 'object shape'. I suspect that others in the field of computer vision may take issue with your definition of 'object shape'. Please provide a crisp definition of what you test for as 'object shape' in each of your experiments (i.e. \"the convex outline of object segmentation\", etc.).\n\nMinor Comments:\n\n- Writing style in introduction. Rather then quoting phrases from individual papers, I would rather see you summarize their ideas in your own language and cite accordingly. This would demonstrate how you regard for their ideas and how these ideas fit together.\n\n- Figure 2. Are people forced to select a choice or could they select 'I don't know'? Did you monitor response times to see if the manipulated images required longer times for individuals to pass decisions? I would expect that for some of the image manipulations that humans would have less confidence about their choices and that to be reflected in this study above and beyond an accuracy score.\n\n- In your human studies, please provide some discussion about how you monitored performance to guard against human fatigue or lack of interest.\n\n- Why did you use AdaIN instead of the original Gatys et al optimization method for image stylization? Was there some requirement/need for fast image stylization?\n\n- Do you have any comment on the large variations in the results across class labels in Figure 4? Are there any easy explanations for this variation across class labels?\n\n- Please use names of Shape-ResNet, etc. in Table 2.\n\n- Are Pascal-VOC mAP results with fixed image features or did you fine-tune (back-propagate the errors to update the image features) during training? The latter would be particularly interesting as this would indicate that the resulting network features are better generic features as opposed to having used better data augmentation techniques.\n\n- A.2. \"not not used in the experiment\" --> \"not used in the experiment\"\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}