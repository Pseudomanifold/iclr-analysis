{"title": "GP kernel inspired by CNNs outperforms previous non-parametric approaches", "review": "This paper\n\n1) extends an argument for the GP behaviour of deep, infinitely-wide fully-connected networks to convolutional and residual deep neural networks with infinitely many channels and\n2) provides a computationally tractable approach to compute the corresponding GP kernel. This kernel has few hyper-parameters, and achieves state-of-the-art results on the MNIST dataset. \n\nWhile point (1) is a relatively straightforward adaptation of Lee et. al (2017) and Matthews et al. (2018) to a different network structure, point (2) is original and non-trivial. All in all, I think this paper makes a significant contribution that I believe will spark interesting follow-up work (hinted at in the last section of the paper).\n\nQuestions:\n\n- In my understanding, the kernels of Section 3 do not require the weight matrices W to share the same values across rows. Accordingly, their performance cannot necessarily be explained by properties of convolutional filters (in particular translation invariance). Can the authors comment on that?\n- What would be the performance of a parametric CNN trained with SGD that matches the architecture (# layers) & the squared loss function of ResNet GP? The only point of comparison is Chen & al. (2018), which I suppose optimizes a log loss? Specifically, I would like to understand the impact of the loss function and of the number of layers on the relative performance of the two approaches.\n\nThe paper is clear and easy to follow. A few suggestions:\n\n- I recommend turning the argument in section 2.2 into a formal, self-contained theorem that states a result on A_L, defined in eq. 17 (which I would move to the main text). This would make the precise claim easier to understand.\n- I suggest including a more thorough discussion of the results. Table 1 is only introduced in the related work section.\n- If space is a concern, I would move part of Section 2.2 outside of the main text, since it mostly follows Lee et al. & Matthews et al\n\nSmall questions/comments:\n\n- Eqs 1 and 2: b_j should be multiplied by the all-ones vector, just like in (5) and (6).\n- Below eq. 5: \"while the *elements of the* feature maps themselves display...\"\n- Paragraph above eq. 7: \"in order to achieve an output suitable for *binary* classification or *univariate* regression\"\n- Paragraph above eq. 7: \"if we only need the covariance at *certain* locations in the outputs...\"\n- Algorithm 1: you might want to add a loop over g for clarity", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}