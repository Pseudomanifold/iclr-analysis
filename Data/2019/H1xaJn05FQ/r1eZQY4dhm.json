{"title": "An interesting work which adopts the sliced-Wasserstein distance to simplify the realization of Wasserstein autoencoder ", "review": "This paper presents an extension of Wasserstein autoencoder (WAE) by modifying the regularization term in learning objective of variational autoencoder. This term measures the divergence between the distribution of the encoded training samples and the samplable prior distribution. The modification is based on the sliced-Wasserstein distance where the distance between two distributions is measured through slicing or projecting the high-dimensional distributions into one-dimensional marginal distributions. As a result, a closed-form solution to the integral in Eq. (9) is obtained via a numerical method. The adversarial learning in WAE, designed to fulfill the calculation of high-dimensional distance, can be avoided. In general, this is an interesting work by introducing new idea of sliced-Wasserstein distance.\n\nRemarks:\n1. A theoretical paper which addresses how and why the sliced-Wasserstein distance between p_z and q_z is reasonable to build a new variant of variational auto-encoder.\n2.  Reformulating the Wasserstein distance into Monge primal formulation with the assumption based on the property of diffeomorphic mapping.\n3. As a result, the implementation based on the unstable adversarial training or the maximum mean discrepancy (MMD) training can be avoided. Computational attractiveness is assured. MMD needs the choice of kernel function which is basically a data-dependent design parameter.\n4. Provide an empirical numerical solution which is compatible with SGD optimization.\n5. The key idea of this paper is shown in Eq. (14). Learning objective is expressed in a deterministic way. However, the style of objective in Eq. (14) involves the stochastic learning.\n6. This paper is not actually doubly-blind reviewed. Authors have exposed their identities in arXiv.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}