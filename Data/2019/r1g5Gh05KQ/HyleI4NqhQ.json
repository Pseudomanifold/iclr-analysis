{"title": "An evolutionary algorithm that performs much worse than SGD", "review": "This paper describes an evolutionary algorithm, with the goal of providing an alternative to SGD. Unfortunately, the only comparison to SGD provided is an experiment where the proposed approach is orders of magnitude *worse* than SGD. And that is the only machine learning experiment in the paper. Thus, the results obviously do not warrant publication at ICLR; in fact, I am wondering why the paper was submitted to this conference.\n\nSome more details about the one machine learning experiment reported in the paper:\nHere the authors train a ConvNet on a subset of MNIST. While MNIST is by now a toy problem, it is OK to start with it. Only reaching a validation accuracy of 90% on it sets off an alarm in my head (since the state of the art is beyond 99.5%), but even the time to reach this performance is worse than that of standard SGD (with unspecified, and possibly untuned, learning rate, momentum, etc). And not just a little worse, but requiring almost 10x the number of \"generations\", with a generation of the authors' method performing 50 evaluations and a generation of SGD performing 1 evaluation. So in total the authors' method is about 500 times slower.\n\nI do not expect the authors to be able to modify this paper to be accepted at a machine learning venue, but the experiments for the blackbox functions appear more promising. I am wondering whether these positive results would hold up when comparing to good implementations of the other algorithms. Therefore, I encourage the authors to evaluate their algorithm on the benchmark functions in the BBOB challenge (https://bbcomp.ini.rub.de/); if they can beat the algorithms that participated in that challenge they should be able to publish their results at one of the evolutionary search conferences, such as GECCO. ", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}