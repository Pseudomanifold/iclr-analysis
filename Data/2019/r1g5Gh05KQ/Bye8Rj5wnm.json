{"title": "Interesting concept, not thoroughly backed up by experiments.", "review": "The paper proposes a new, evolutionary, gradient-free algorithm, aimed at training deep networks. The algorithm is compared against other evolutionary algorithms and against Stochastic Gradient Descent.\n\nHigh level comments:\n* Clarity: The paper is written very clearly and easy to follow.\n* Quality: The introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing.\n* Originality: I am not familiar with the literature in evolutionary methods, therefore I cannot evaluate this fairly. \n* Significance: Based on the presented experiments, I cannot see a use case for choosing this method instead of SGD or other competitors.\n\nPros:\n-\tThe introduced concept is interesting\n-\tThe language in the paper is simple, making it easy to follow.\n-\tSeveral of the decisions in the paper are explained intuitively. \n-\tI appreciate the honest comparison when specifying that the competitors\u2019 implementation is suboptimal, and in the comparison with SGD.\n\nCons:\n-\tThe experiments do not entirely support the utility of the method, even as proof of concept (e.g. not varying the pool size, no results on computation speed comparison, competitors are not optimized, MNIST experiments have not converged).\n-\tSome details are missing (e.g. what is the loss function? Are there any constraints on choosing it?)\n-\tSome choices of parameters in the paper that are not justified (e.g. numbers in equation 1, loose stopping criteria)\n-\tThere are some minor typos and editing issues that need to be fixed.\n\nDetailed comments:\n\u2022\tMajor issues:\n   1.\tPart of your motivation in the intro states that your algorithm is not replacing SGD, but \u201ccomplementing\u201d it. Can you please describe more specifically how this would be done? Give examples. When should one use your algorithm, and how do you use it \u201cin tandem\u201d with SGD. It is not clear from the paper when/how your algorithm should be used.\n   2.\tYou mention that: \u201cOur algorithm should be able to handle sparse-rewards, exploration, exploitation, high-dimensionality, computational efficiency, pool-efficiency and sample-efficiency\u201d. None of these are proved by experiments:\n      a.\thigh-dimensionality is not addressed\n      b.\tcomputational efficiency is mentioned in the conclusions, but there are no experiments comparing run times, and no discussion about algorithmic complexity.\n      c.\tpool-efficiency: indeed you use a small pool, but you didn\u2019t mention how this is for other competitors, and you didn\u2019t vary the size of this pool to see what happens.\n      d.\tsample-efficiency: you run MNIST on a subset of samples, but the accuracy you get is nowhere near state-of-the-art. The accuracy you get is the same as SGD, but in Figure 1,  none of the two losses has converged (see downward trend  of the curve), so maybe SGD would be better at convergence.\n   3.\tRL vs not RL: often times in discussion you mention \u201cagents\u201d and RL (e.g. entire section 1.1), but none of the experiments are about RL. While RL may be a natural application, since it is not evaluated here, I suggest reformulating the paper using supervised learning terms.\n   4.\tFrom the comparison with SGD in terms of number inference calls it sounds like your algorithm would be very expensive on large networks. Because of that, it would be good to see come computation time plots on larger networks. However, I appreciate that you did bring this comparison up!\n\n\u2022\tMinor issues:\n   1.\tThe entire section 2 relies on picking samples based on some performance measure. However, you never mention what this measure should be. Is it a loss function? If so, are there any constraints on this?\n   2.\t\u201cThe historical elite is the best performing sample across all generations\u201d. This is very vague. How is this quantified? It would help to introduce some mathematical notation for the loss, and describe how the elite is chosen in terms of this loss.\n   3.\tThe experimenter has to choose several constants in your model. Is there some intuition how to do that?\n   4.\tSection 1.1, paragraph 2: RL doesn\u2019t necessarily need labeled data. What are other potential issues with simulations and how is this related to the proposed method?\n   5.\t\u201cIt is typical that any optimization algorithm is sensitive to the choice of parameters. However, tuning those parameters is beyond the scope of this paper.\u201d  -- not true. If  you don\u2019t make a sensible choice of  step size in SGD, the comparison will not be fair.  The fairest thing to do is to compare these methods, each using its best hyperparameters.\n   6.\tSection 3.2: \u201cThe test is terminated once the algorithm achieves a loss of 0.15 or lower\u201d. You don\u2019t mention what loss is used to be able to tell if 0.15 is  small enough. From the plot it seems the models have not converged.\n   7.\tTable 1 caption does not specify the metric of the numbers in the table.\n   8.\tThe conclusion mentions new results on a RL experiment, which are not discussed in the paper.\n   9.\tPlease briefly define what are evolutionary optimization algorithms in the intro.\n\n\u2022\tTypos and writing style:\n   1.\tThe citation style is most often used incorrectly. Put the reference in brackets when is not part of the sentence (e.g.  in first paragraph: Object Detection (Liu et. Al, 2016), \u2026 ), and without brackets when you specifically refer to that paper title.\n   2.\tTypos: last paragraph of background (\u201calso use computer vision is a starter\u201d),  Section 3 second paragraph (\u201cThat is, the models for copied into each\u201d).\n\nFinal remarks and advice: \nIt seems that the authors pose this work more as a proof-of-concept, than a new algorithm ready to be used in practice. While the idea definitely has value, I believe the experiments are not sufficiently convincing or adequately chosen, to prove what you want to show. If sample complexity is your selling point, gear the experiments more in that direction and show what happens for various sample sizes and pool sizes, for you and competitors. If it is computationally efficient (as you say in the conclusion), show experiments comparing run times with other competitors. If complexity is the selling point, tell us exactly how much computation is required in every generation, compared to other methods. Also, if SGD is your competitor, find a scenario where SGD doesn\u2019t work but your method does. Moreover, for the presentation of the algorithm in section 2, an algorithm box summarizing the method would be of great help. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}