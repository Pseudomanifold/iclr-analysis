{"title": "Multiple GT labels", "review": "The authors propose to improve classification accuracy in a supervised learning framework, by providing richer ground truth in the form a distribution over labels, that is not a Dirac delta function of the label space. This idea is sound and should improve performance.\n\nUnfortunately this work lacks novelty and isn't clearly presented.\n(1) Throughout the paper, there are turns that used without definition prior to use, all table headers in table 1. \n(2) Results are hard to interpret in the tables, and there are limited details. Mixup for example, doesn't provide exact parameters, but only mentions that its a convex sum.\n(3) There is no theoretical justification for the approach.\n(4) This approach isn't scalable past small datasets, which the authors acknowledge. \n(6) This has been already done. In the discussion the authors bring up two potential directions of work:\n   (a) providing a distribution over classes by another model - > this is distillation (https://arxiv.org/abs/1503.02531)\n   (b) adding a source of relationships between classes into the objective function -> this is (https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42854.pdf)\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}