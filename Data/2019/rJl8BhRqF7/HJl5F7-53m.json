{"title": "Official Review: Not fully motivated and related to previous work on learning with class uncertainty and calibration", "review": "The paper presents a new version of CIFAR10 that is labelled by multiple people (the test part of the data). They use it to improve the calibration of several image classifiers through \u201cfine-tuning\u201d and other techniques\nThe title is too general, taking into account that this setting has appeared in classification in many domains, with different names (learning from class distributions, crowd labellers, learning from class scores, etc.). See for instance,\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3994863/\nhttp://www.cs.utexas.edu/~atn/nguyen-hcomp15.pdf \nAlso, at the end of section 2 we simply reach logloss, which is a traditional way of evaluating the calibration of a classifier, but other options exist, such as the Brier score. At times, the authors mention the trade-off between classification accuracy and cross-entropy. This sounds very much the trade-off between refinement and calibration, as one of the possible decompositions of the Brier score.\nThe authors highlight the limitations of this work, and they usually mention that the problem must be difficult (e.g., low resolution). Otherwise, humans are too good to be useful. I suggest the authors to compare with psychophysics and possible distortions of the images, or time limits for doing the classifications. \nNevertheless, the paper is not well motivated, and the key procedures, such as \u201cfine-tuning\u201d lack detail, and comparison with other options.\nIn section 2, which is generally good and straightforward, we find that p(x|c) being non-overlapping as a situation where uncertainty would be not justified. Overlap would simply say that it is a categorisation (multilabel classification) problem rather than a classification problem, but this is different from the situation where labels are soft or given by several users. \nIn the end, the paper is presented from the perspective of image recognition, but it should be compared with many other areas in classification evaluation where different metrics, presentation of the data, levels of uncertainty, etc., are used, including different calibration methods, as alternatives to the expensive method presented here based on crowd labelling.\nPros:\n-\tMore information about borderline cases may be useful for learning. This new dataset seems to capture this information.\nCons:\n-\tThe extra labelling is very costly, as the authors recognise.\n-\tThe task is known in the classification literature, and a proper comparison with other approaches is required.\n-\tNot compared with calibration approaches or other ways where boundaries can be softened with less information from human experts. For instance, a cost matrix about how critical a misclassification is considered by humans (cat <-> dog, versus cat <-> car) could also be very useful, and much easier to obtain.\n", "rating": "3: Clear rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}