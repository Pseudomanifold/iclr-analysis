{"title": "Interesting paper studying gradient descent in over-parameterized simple NNs", "review": "This paper studies one hidden layer neural networks with square loss, where they show that in over-parameterized setting, random initialization + gradient descent gets to zero loss. The results depend on the property of data matrix, but not the output values.\n\nThe high level idea of the proof is quite different from recent papers, and it would be quite interesting to see how powerful this is for deep neural nets, and whether any insights could help practitioners in the future. \n\nSome discussions regarding the results: \n\nI would suggest the authors to be specific about \u2018with high probability\u2019, whether it is 1-c, or 1-n^{-c}. The proof step using Markov\u2019s inequality gives 1-c probability, which is stated as \u2018with high probability\u2019. What about other \u2018high probability\u2019 statements?\n\nIn the statement of Theorem 3.1 and 4.1, please add \u2018i.i.d.\u2019 (independence) for generating w_r s.\n\nThe current statement of Lemma 3.2 is confusing. The authors state that given t, w.h.p. (let\u2019s say 0.9 for now) over initialization, the minimum eigenvalue is lower bounded. This does not imply, for example, that there exists an initialization, such that for 20 different t s, the minimum eigenvalue is lower bounded. The proof uses Markov\u2019s inequality for a single t. Therefore, I am slightly worried about its correctness. I hope the authors could address my concern. \n\nAlso, in the proof of Lemma 3.2, (just to improve the readability,) I would suggest the authors to make it clear that the expectation is taken over the initialization of the weights. \n\nSome typos: \n\n\u2018converges\u2019 -> \u2018converges to\u2019 in the abstract\n\u2018close\u2019 -> \u2018close to\u2019 on page 5\n\u2018a crucial\u2019 -> \u2018a crucial role\u2019 on page 5\nIn the proof of Lemma 3.2, x_0 should be x_i\nwhether using boldface for H_{ij} should be consistent\n'The next lemma shows we show' in page 6\n'Markov inequality' -> \u2018Markov\u2019s inequality\u2019\n\u2018a fixed a neural network architecture\u2019 in page 8\n\nIt is good to see other comments and discussions on this paper. I believe the authors will make a revision and I would be happy to see the new version of the paper and re-evaluate if some of my comments are not correct.  \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}