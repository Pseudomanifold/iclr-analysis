{"title": "The paper addresses fast/slow learning modules in deep networks. Good paper. Needs more clarity/work. ", "review": "The overall contribution makes sense. Consider solving a linear system i.e., learning an unknown matrix. Splitting it into two components (like in NMF or MMF) and learning each separately gives more control on the conditioning of the matrices. This is the basis of residual networks (at least the theory for linear resnets). Within this, the technical/theoretical results presented in the paper are sensible. Couple of issues: \n1) Where are we breaking the slow/fast learners in terms of the depth of the network? I.e., How many of the layers are slow? Does this break point influence the overall convergence? \n2) It is unclear what the aim of simulations is? The reported figures are not conveying useful information. It makes sense to do a repeatability experiment here with multiple sets of simulated datasets. \n3) Put confidence intervals on the results (table/figure). \n4) What is the nature and choice of g()? The evaluations uses LSTM but will the structure of g() influence the rate of learning? \n5) The authors should choose a better reference than miracle for the ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}