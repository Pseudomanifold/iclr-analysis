{"title": "An interesting treatment to meta-learning with fast/slow learning components. ", "review": "[Summary]\nThe paper presents a novel learning framework for meta-learning that is motivated by neural learning process of human over long periods. Specifically, the process of meta-learning is divided into a slow and a fast learning modules, where the slowly-learnt component accounts for low-level representation that is progressively optimized over all data seen so far to achieve generalization power, and the fastly-learnt component is supposed to pick up the target in a new task for quick adaptation. It is proposed that meta-learning should focus on capturing the meta-information for the fast learning module, and leave the slow module being updated steadily without task-specific adaptation. Theoretical analysis is presented on a linear MLP examples to shed some light on the properties of the proposed algorithm. Results on both synthetic dataset and benchmarks justify the theoretical observation and advantages.               \n\nPros\nNovel treatment and formulation of meta-learning from the perspective of fast and slow  learning process\nCons\nSome interesting cases not tested\nPresentation could be improved \n\n[Originality]\nThe paper approaches the recently popular meta-learning from a novel perspective by decomposing the learning process into slow and fast ones. \n\n[Quality]\nOverall,  the paper is well motivated and implemented with both theoretical study and empirical justification. There are a few questions / areas for further improvements, though:\n- It seems that to initialize the slow module, another set of data is needed to pretrain it before the actual meta-learning takes place to learn to optimize the fast learner (as opposed to other meta-learning methods where all parameters in a base model were meta-learnt over the meta training set). How does this affect the performance? E.g., what if the slow module is only updated over the meta-training set (still without reinitialization across different batches) without pre-training?\n- In the current formulation, the base model is decomposed into two distinct (slow and fast) modules. What is the rule to decide which layers should belong to slow or fast modules? How does different choice affect the performance? Can we decompose the base model into finer granularities for different learning behaviors? E.g., a third module module in-between the fast and slow ones that follows medium learning pace.          \n- The theoretical study can be better organized. The proofs can be left in appendix to make room for more discussion on conclusions, non-linear and / or non-Gaussian cases.     \n- The write-up can be improved too at some places: proper reference at line 4 of section 1 is missing; \\phi in (1) is not well defined, as well as \u201cSOA\u201d in section 2;\n\n[Clarity]\nThe paper is generally clearly written, with a few places to improve (see comments above).\n\n[Significance]\nThe paper brings in an interesting perspective to meta-learning. It can also inspire more follow-up work to better understand the problem.   \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}