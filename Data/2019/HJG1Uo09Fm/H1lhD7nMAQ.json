{"title": "Review of \"Learning to Reinforcement Learning by Imitation\"", "review": "This paper proposes a meta-learning algorithm for reinforcement learning that incorporates expert demonstrations. The goal is to reduce the sample complexity of meta-RL algorithms in the validation phase. The paper provides a good discussion of the background literature. Experimental results are provided on multi-goal planning problems for three prototypical simulated systems, namely, a 2D point-mass robot, a 7-DOF manipulator and a quadruped crawler.\n\nThe theoretical and practical contributions of this paper are minor. The authors propose a straight-forward combination of MAML, importance-weighted policy gradients across the inner-outer loop and off-policy supervised learning of the expert demonstrations, all standard techniques in reinforcement learning and meta-learning. The experimental section is unconvincing and lacking in details. I however find the approach well-motivated and pertinent. Demonstrations on a real robotic platform, where the improved sample complexity is essential, would make this paper much more impressive.\n\nDetailed comments:\n\n1. Can you compare the different algorithms using the number of rollouts as the X-axis? The narrative provides this information but it is difficult to judge the performance based on Figs. 2 and 3.\n2. It is unfair to compare MRI (the approach of this paper) with MAML which does not have access to expert demonstrations for validation tasks. The improved sample complexity is thus directly coming from demonstrations. It is difficult to compare MRI and MAML-RL/Imitation on an equal footing. Perhaps the validation tasks could be significantly harder, e.g., sub-goals in the planning problems, or one could consider a large number of validation tasks.\n3. It seems the improvement over MAML-RL/Imitation in Fig. 3 is minor. Why is this so?", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}