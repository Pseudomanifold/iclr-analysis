{"title": "Interesting work, strong results, good paper", "review": "Overall, this is a really good paper.\nThe authors propose an alternative to content based similarity for NL applications as compared to self-attention models by proposing the parameter and sequence length efficient Lightweight and Dynamic Convolutions.\nThe authors show, over various NL tasks like Translation, LM and Abstractive summarisation, the comparison of self attention models with Lightweight and Dynamic convolution layer.\nThe weight sharing was particularly interesting and can be seen as applying different heads for the same kernel. \n\nThe experimental results give strong evidence for these alternatives proposed by the authors.\nThe lightweight and dynamic convolution layers, both perform similar or better than the self-attention layer in all the tasks.\nThe WMT EnFr result is much better than all the other models, establishing a new state of the art.\n\nQuestion for the authors:\n1. Is the weight sharing within the kernel mostly for reducing computation?\nIf so, did you trying varying H size and measure how much that affects performance? What is surprising is that, in the ablation table the weight sharing increases the BLEU score by 0.1. \n2. Did you run any experiments where the kernel size covers the whole sentence?\n3. Since the number of parameters only change linearly wrt sequence length, did you try running this on datasets that have really long sequences to show the effectiveness of this approach further?\n4. How important was softmax normalization for training?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}