{"title": "simple idea that works along with some theory to support it", "review": "This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}