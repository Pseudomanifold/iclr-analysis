{"title": "An approach that can work well in practice, but not principled", "review": "I have read the feedback and discussed with the authors on my concerns for a few rounds. \n\nThe revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments.\n\nI have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. \n\nI have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score \"6\" reflects my opinion that the approach is not principled.\n\n=========================================================\n\nThank you for an interesting read.\n\nThe paper proposes training a Bayesian neural network (BNN) with adversarial training. To the best of my knowledge the idea is new (although from my perspective is quite straight-forward, but see some discussions below). The paper is well written and easy to understand. Experimental results are promising, but I don't understand how the last experiment relates to the main idea, see comments below.\n\nThere are a few issues to be addressed in revision:\n\n1. The paper seems to have ignored many papers in BNN literature on defending adversarial attacks. See e.g. [1][2][3][4] and papers citing them. In fact robustness to adversarial attacks is becoming a standard test case for developing approximate inference on Bayesian neural networks. This means Figure 2 is misleading as in the paper \"BNN\" actually refers to BNN with mean-field variational Gaussian approximations.\n\n2. Carlini and Wagner (2017a) has discussed a CW-based attack that can increase the success rate of attack on (dropout) BNNs, which can be easily transferred to a corresponding PGD version. Essentially the PGD attack tested in the paper does not assume the knowledge of BNN, let alone the adversarial training. This seems to contradict to the pledge in Athalye et al. that the defence method should be tested against an attack that is aware of the defence.\n\n3. I am not exactly sure if equation 7 is the most appropriate way to do adversarial training for BNNs. From a modelling perspective, if we can do Bayesian inference exactly, then after marginalisation of w, the model does NOT assume independence between datapoints. This means if we want to attack the model, then we need to do \n\\min_{||\\delta_x|| < \\gamma} log p(D_adv), \nD_adv = {(x + \\delta_x, y) | (x, y) \\sim \\sim D_tr},\nlog p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw.\nNow the model evidence log p(D_adv) is intractable and you resort to variational lower-bound. But from the above equation we can see the lower bound writes as\n\\min_{||\\delta_x|| < \\gamma} \\max_{q} E_{q} [\\sum_{(x, y) \\sim D_tr} \\log p(y|x + \\delta_x, w) ] - KL[q||p],\nwhich is different from your equation 7. In fact equation 7 is a lower-bound of the above, which means the adversaries are somehow \"weakened\".\n\n4. I am not exactly sure the purpose of section 3.3. True, that variational inference has been used for compressing neural networks, and the experiment in section 3.3 also support this. However, how does network pruning relate to adversarial robustness? I didn't see any discussion on this point. Therefore section 3.3 seems to be irrelevant to the paper.\n\nSome papers on BNN's adversarial robustness:\n[1] Li and Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. ICML 2017\n[2] Feinman et al. Detecting Adversarial Samples from Artifacts. arXiv:1703.00410\n[3] Louizos and Welling. Multiplicative Normalizing Flows for Variational Bayesian Neural Networks. ICML 2017 \n[4] Smith and Gal. Understanding Measures of Uncertainty for Adversarial Example Detection. UAI 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}