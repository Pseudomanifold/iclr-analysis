{"title": "contributions for ICLR community are unclear; after revisions and discussion, I am more positive", "review": "After reading through the authors' comments and rereading parts of the submission, I have become a little more positive about this paper. \n\nI am still unsure about the contributions to the ICLR community. The authors merely state \"We believe our contributions to ICLR community are clear and valuable\" without backing up this claim with an argument. \n\nBut in the rest of the authors' comment, they make some good points. I think those points should be made more prominently in the paper itself. I would suggest that the authors describe their approach as using different, complementary encoders of the input sentence and consensus maximization. If they wish to describe this as multi-view learning, that's fine, but I think using the term \"consensus maximization\" (or something more descriptive like that) in prominent places would be helpful. \n\nIf the approach is applicable beyond sentence embedding learning, then it would behoove the authors to describe the approach in a general way so that readers will see how they can apply it to their own tasks. As currently written, the paper is very much focused on sentence embedding learning, which causes me to think that the paper is more appropriate for an NLP venue. But it is true that ICLR publishes papers that are application-specific, so I can't consider this to be a deal-breaker for the paper.\n\nI raised my score to a 6.\n\n--------------------------------- original review follows: ----------------------------------\n\nThis paper describes experiments in learning sentence embeddings from unlabeled text. The paper compares a few different compositional architectures and training objectives. The story of the paper focuses on the training of multiple architectures jointly for a single sentence, then ensembling those architectures at test time to represent sentences. One architecture is an RNN and the other is a word averaging model, and the idea is that these two architectures capture different \"views\" of the sentence. \n\nPros:\n\nAs a general-purpose method to get sentence embeddings without using any resources other than unannotated text documents, this approach has strong results, including solid performance on the SentEval tasks and relatively-low training times. \n\nIt was also nice to see how the results depend on the domain of the training data. Review data definitely helps on the several sentiment-related tasks, which provides further evidence of a worrisome aspect of SentEval. \n\nCons:\n\nOverall, the paper feels incremental and is likely a better fit for an NLP conference. What are the generalizable contributions to the ICLR community? Given the known differences between RNNs and word averaging models for sentences (especially on the SentEval tasks, which, as the authors note, was discussed by Hill et al.), it's entirely unsurprising that combining the two would be a good idea. But even if this were not the case, the ubiquity of ensembles outperforming single models in deep learning also makes it unsurprising that combining these two kinds of model architectures would be beneficial. So I'm just not sure if there is a significant contribution beyond the NLP results. These results seem solid (though a bit incremental), but if the primary contribution is empirical, then the paper would be a better fit for an NLP venue. \n\nIn addition, I'm not sure if \"multi-view\" is an appropriate description of the approach. In Sec. 1, we find the sentence \"Compared to earlier work on multi-view learning (de Sa, 1993; Dhillon et al., 2011) that takes data from various sources or splits data into disjoint populations, our framework processes the exact same input data in two distinctive ways.\"  Therefore, maybe it's not quite accurate to describe this approach using the term \"multi-view learning\"? I think it would make more sense to use a different term rather than stretch the definition for a well-known one. \n\nI kept expecting the paper to present results when combining the generative and discriminative objectives, but as far as I can tell, this was never done. What would happen if one were to use multitask learning and just optimize the sum of the two losses? \n\nI'd suggest citing and comparing to the results from \"Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\" by Subramanian et al. (ICLR 2018) and the Byte mLSTM from \"Learning to generate reviews and discovering sentiment\" by Radford et al. \n\nI'm not sure how excited we should get about not using any annotations or structured resources for learning sentence embeddings. The authors do not motivate this goal. \n\n\nBelow are more specific comments/questions:\n\nSec. 2.2 contains the sentence \"Ideally, the inverse of h should be easy to compute so that during testing we can set g = h^-1.\" At this point in the paper, it is not clear what g is going to be applied to at test time, since presumably the following sentence is not going to be available at test time, right? I think it would be good to discuss how the model is going to be used at test time before discussing the inverse of h. \n\nSec. 2.3: \nIn Eq. (3), why does the denominator sum always start at 1 no matter what i and j are? That is, why would the denominator always sum over the first N sentences in the dataset?\n\nI think the pooling methods in Table 2 should be described in Section 2. \n\nIn Table 2, it is not clear what h_i^{M_i} is. If M_i is the number of words in sentence i, that should be mentioned somewhere. \n\nSec. 3.1:\n\"For a given sentence input s with M words, suggested by (Pennington et al., 2014; Levy et al., 2015), the representation is calculated as z = (\\hat{z}_f + \\hat{z}_g) / 2, where \\hat{z} refers to the post-processed and normalised vector, and is mentioned in Table 2.\"  I don't understand. Where is this mentioned in Table 2?\n\n\nMinor issues follow:\n\nSec. 1:\n\"Distributional hypothesis\" --> \"the distributional hypothesis\"\n\"in machine learning community\" --> \"in the machine learning community\"\n\"and distributional hypothesis\" --> \"and the distributional hypothesis\"\n\"the linear/log-linear models\" --> \"linear/log-linear models\"\n\"based on distributional hypothesis\" --> \"based on the distributional hypothesis\"\n\"contraint\" --> \"constraint\"\n\nSec. 2:\n\"marry RNN-based sentence encoder\" --> \"marry RNN-based sentence encoders\" or \"marry the RNN-based sentence encoder\"\n\nSec. 2.1:\n\"only hidden state\" --> \"only the hidden state\"\n\nSec. 2.2:\n\"prior work with generative objective\" --> \"prior work with generative objectives\"\n\nSec. 2.3:\n\"with discriminative objective learns\" --> \"with the discriminative objective learns\"\n\nSec. 3.1:\nIn Table 3, I don't see where superscript 5 is shown in the table.\n\nSec. 3.2:\nIn Table 5, the numbered superscripts at the top of the table do not show up next to the methods in the actual table rows. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}