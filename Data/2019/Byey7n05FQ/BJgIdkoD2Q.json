{"title": "Nice results but lean technical contribution", "review": "This paper proposes to combine fitted value iteration with model predictive control (MPC) to speed up the learning process. The value iteration is the \"Learn offline\" subsystem while MPC is the \"Plan online\" subsystem. In addition, this paper also proposes an exploration technique that increases exploration if the multiple value function estimators disagree. The evaluation is complete and shows nice results.\n\nHowever, I did not rank this paper high for two reasons. First, it is not clear to me how the model is acquired in MPC. Does the method learn the model? Does the method linearize the dynamics and assume a linear model? I am not sure. I suspect that the method just uses the simulator as the model. If it is the case, the method is not so useful because for complexity systems, such as humanoids, we do not know the model. And the comparisons with model-free learning algorithms are not fair because the paper assumes that the model is given. If this is not the case, I suggest that a more detailed description of MPC should be presented in Section 2.3.\n\nSecond, the technical contributions are lean. The three main components, 1) fitted value iteration, 2) MPC and 3) exploration based on multiple value function estimates, are not novel. The combination of them seems straight forward. For example, the H-step Bellman update (Section 2.3) is a blend between Monte-Carlo method and Q learning. It seems to be similar to the TD(\\lambda) method. Thus, it is not surprising that it can accelerate convergence of value function.\n\nFor the above reasons, I would not recommend accepting this paper at this time.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}