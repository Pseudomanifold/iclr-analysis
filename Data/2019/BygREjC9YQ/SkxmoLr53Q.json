{"title": "I do not find the results of the paper particularly convincing though I would not rule out Bayesian filtering as a framework for analyzing adaptive methods", "review": "Paper summary: The authors analyze stochastic gradient descent through the lens of Bayesian filtering. In doing so they (approximately) recover several common adaptive gradient optimization schemes. The paper focuses on a theoretical construction of this framework and offers a limited empirical study.\n\nDetailed comments:\n\nI thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified. While the Bayesian filtering framework is interesting in that it recovers slight variations of existing algorithms, and also caters for some recent practical tricks, I do not feel that it substantially improves our theoretical understanding of these methods.\n\n1) I found the notation difficult to follow in the introduction and parts of section 2. I have highlighted several places explicitly below. I found paragraphs 2 and 3 of the introduction particularly challenging.\n\n2) I found the introduction of Bayesian filtering challenging to follow. For example, which form of the likelihood is assumed for the Taylor expansion? How/why is $\\mu_{like}$ identified using the gradient? Linking to Kalman filtering made things easier to follow.\n\n3) I think that the related work, and possibly a chunk of section 2, should include a discussion of Noisy Natural Gradient [1]. While the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited.\n\n4) Start of 2.1: \"z will have on element representing a single parameter\", after which z is treated as a vector. I believe this sentence is present to distinguish RMSProp from Adam when momentum is added but I found it confusing at first.\n\n5) I found the comparisons between BRMSProp-vs-RMSProp and BAdam-vs-Adam fairly unconvincing. The assumptions are not clearly demonstrated to have little practical significance and Figure 2. does not seem to support the claim that these methods are strongly related. Is it possible to demonstrate empirically that these algorithms have equivalent behaviour under some limiting factors? And if not, is there a good reason for this that still justifies the comparison? I would appreciate some clarifications on these points.\n\n6) I am not sure what you mean by \"We now assume that the data is strong enough to reduce the uncertainty in the momentum below its levels under the prior\". I believe that I am following the mathematical arguments correctly but I find this phrasing misleading. Furthermore, this section uses e.g. ppth and wpth to refer to coordinates, I think it would be clearer to simply write Sigma_{pp}, etc.\n\n7) Section 4.2 is lacking justification in my opinion (am I missing something?). I think that this section needs to have the derivation clearly laid out (in the appendix would be fine). Furthermore, the NAWD algorithm is not explored empirically, or analyzed theoretically at all. I would argue that more evidence is needed that this is a reasonable thing to do before it is meaningful to include it in the final print of this paper. In general, sections 4.4 - 4.7 feel a little out-of-place and thrown together. I think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.\n\nMinor comments:\n\n- In introduction, how exactly does $w'_i$ differ from $w_i$?\n- In introduction, after para 2, the notation in the equation is confusing, e.g. overloading w_i(t) and w_i(mu_{-i}(t)).\n- In introduction,  para 3, \"must depend on other parameters\" - this seems like an obvious statement but it is presented as being crucial\n- Should \"Related Work\" start at 1 or 2?\n- (VERY MINOR) In section 2.2 and 2.3, \"christen\" seems like an add choice of word. Perhaps just \"call\"?\n- Equations 10 and 11 introduce an independence assumption on the dimensions of the parameter vector. I think this should be explicitly stated.\n- Section 7.2 heading typo: MOMEMTUM\n\nClarity: I found the paper challenging to follow in places due to choices of notation (and a weak background in Kalman filtering and related techniques).\n\nSignificance: I do not feel that this work offers a strong case for significance. The empirical evaluation is very limited. The theoretical framework introduces is interesting but is not justified particularly well in the paper and does not directly offer explanations for many of the observations noted in this paper and elsewhere.\n\nOriginality: To my knowledge, the ideas presented in the paper are original and hint at potentially interesting viewpoints of optimization.\n\nReferences:\n\n[1] Zhang et al. \"Noisy Natural Gradient as Variational Inference\" https://arxiv.org/pdf/1712.02390.pdf", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}