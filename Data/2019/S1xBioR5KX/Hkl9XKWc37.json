{"title": "Efficient dynamic reparameterization but the claims need to be revisited", "review": "The paper provides a dynamic sparse reparameterization method allowing small networks to be trained at a comparable accuracy as pruned network with (initially) large parameter spaces. Improper initialization along with a fewer number of parameters requires a large parameter model, to begin with (Frankle and Carbin, 2018).  The proposed method which is basically a global pooling followed by a tensorwise growth allocates free parameters using an efficient weight re-distribution scheme, uses an approximate thresholding method and provides automatic parameter re-allocation to achieve its goals efficiently. The authors empirically demonstrate their results on MNIST, CIFAR-10, and Imagenet and show that dynamic sparse provides higher accuracy than compressed sparse (and other) networks.\n\nThe paper is addressing an important problem where instead of training and pruning, directly training smaller networks is considered. In that respect, the paper does provide some useful tricks to reparameterize and pick the top filters to prune. I especially enjoyed reading the discussion section.\n\nHowever, the hyperbole in claims such as \"first dynamic reparameterization method for training convolutional network\" makes it hard to judge the paper favorably given previous methods that have already proposed dynamic reparameterization and explored. This language is consistent throughout the paper and the paper needs a revision that positions this paper appropriately before it is accepted.\n\nThe proposed technique provides limited but useful contributions over existing work as in SET and DeepR. However, an empirical comparison against them in your evaluation section can make the paper stronger especially if you claim your methods are superior.\n\nHow does your training times compare with the other methods? Re-training times are a big drawback of pruning methods and showing those numbers will be useful.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}