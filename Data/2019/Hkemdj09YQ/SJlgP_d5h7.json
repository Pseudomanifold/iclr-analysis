{"title": "Interesting work, but I believe a few questions need to be answered to make the paper strong enough for acceptance. ", "review": "Summary of the paper:\nThis paper proposed RectGrad, a gradient-based attribution method that tries to avoid the problem of noise in the attribution map. Further, authors hypothesize that noise is caused by the network carrying irrelevant features, as opposed to saturation, discontinuities, etc as hypothesized by related papers. \n\nThe paper is well written and easy to read through. \n\nStrengths:\n- Formally addresses a hitherto unanswered question of why saliency maps are noisy. This is an important contribution.\n- RectGrad is easy to implement.\n\nQuestions for authors:\n- Since the authors are saying that the validity of their hypothesis is \u201ctrivial\u201d, it would be nice to have this statement supported by more quantitative, dataset-wide analyses on the feature map and training dataset occlusion tests. For e.g., what percentage of the test dataset shows attributions on the 10x10 occluded patch? \n- How does RectGrad compare with simply applying a final threshold on other attribution maps? How do the results on training data and feature occlusion change after such a threshold is applied? How do results on adversarial attacks change?\n- Could this method generalize to non-ReLU networks?  \n- Premise that auxiliary objects in the image are part of the background is not necessarily true. For instance, the hand in \u201clighter\u201d is clearly important to know that the flame is from a lighter and not from a candle or some other form of fire. Similarly, the leaves in the \u201cfrog\u201d example. \n- (Optional) As shown in (https://openreview.net/forum?id=B1xeyhCctQ) gradients on ReLU networks overlook the bias term. In the light of this, what is the authors\u2019 take on whether a high bias-attribution is the cause for the noisy gradient-attribution? \n- (Optional) In some sense, RectGrad works because layers closer to the input may capture more focussed features than layers close to input which may activate features spread out all over the image. It would be interesting to see if RectGrad works for really small networks such as MobileNet (https://arxiv.org/abs/1801.04381) where such an explicit hierarchy of features may not be there. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}