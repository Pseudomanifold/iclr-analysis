{"title": "Good Paper", "review": "Update: the authors' response and changes to the paper properly addressed the concerns below. Therefore the score was improved from 6 to 8.\n\n----\n\n\nThe paper makes several contributions: 1. it extends GAN to text via Gumbel-softmax relaxation, which seems more effective than the other approaches using REINFORCE or maximum likelihood principle. 2. It shows that using relational memory for LSTM gives better results. 3. Ablation study on the necessity of the relational memory, the relaxation parameter and multi-embedding in the discriminator is performed.\n\nThe paper's ideas are novel and good in general, and would make a good contribution to ICLR 2019. However, there are a few things in need of improvement before it is suite for publication. I am willing to improve the scores if the following comments are properly addressed.\n\nFirst of all, the paper does not compare with recurrent networks trained using only the \"teacher-forcing\" algorithm without using GAN. This means that at a high level, the paper is insufficient to show that GAN is necessary for text generation at all. That said, since almost every other text GAN paper also failed to do this, and the paper's contribution on using Gumbel-softmax relaxation and the relational memory is novel, I did not get too harsh on the scoring because of this.\n\nSecondly, whether using BLEU on the entire testing dataset is a good idea for benchmarking is controversial. If the testing data is too large, it could be easily saturated. On the other hand, if the testing data is small, it may not be sufficient to capture the quality well. I did not hold the authors responsible on this either, because it was used in previously published results. However, the paper did propose to use an oracle, and it might be a good idea to use a \"teacher-forcing\" trained RNN anyways since it is necessary to show whether GAN is a good idea for text generation to begin with (see the previous comment).\n\nA third comment is that I had wished the paper did more exploration on the relaxation parameter \\beta. Ideally, if \\beta is too large, the output would be too skewed towards a one-hot vector such that instability in the gradients occurs. On the other hand, if \\beta is too small, the output might not be close enough to one-hot vectors to make the discriminator focus on textual differences rather than numerical differences (i.e., between a continuous and a one-hot vector). It would make sense for the paper to show both ends of these failing cases, which is not apparent with only 2 hyper-parameter choices.\n\nFinally, the first paragraph in section 2.2.2 suggests that the gap between discrete and continuous outputs is the reason for mode collapsing. This is false. For image generation, when all the outputs are continuous, there is still mode collapsing happening with GANs. The authors could say that the discrete-continuous gap contributes to mode-collapsing, but this is not too good either because it will require the paper to conduct experiments beyond text generation to show this. Authors should make changes here.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}