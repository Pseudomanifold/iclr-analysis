{"title": "Two somewhat disconnected small contributions", "review": "This paper uses a language model for scoring of question answer candidates in the Winograd schema dataset, as well as introduces a heuristic for scoring common-sense knowledge triples.\n\nQuality:\nPros: The paper shows improvements over previous papers for two tasks related to common-sense knowledge. They both mainly utilise simple language models, which is impressive. The second one uses an additional supervised collaborative filtering-style model. The authors further perform a detailed error analysis and ablation study.\nCons: The paper isn't very well-written. It contains quite a few spelling mistakes and is unclear in places. The Winograd Scheme Challenge isn't a very interesting dataset and isn't widely used. In fact, this is evidenced by the fact that most cited papers on that datasets are preprints and technical reports.\n\nClarity:\nThe paper is confusing in places. It should really be introduced in the abstract what is meant by \"common sense\". Details of the language model are missing. It is only clear towards the end of the introduction that the paper explores two loosely-related tasks using language models.\n\nOriginality:\nPros: The suggested model outperforms others on two datasets.\nCons: The suggested models are novel in themselves. As the authors also acknowledge, using language models for scoring candidates is a simple baseline in multiple-choice QA and merely hasn't been tested for the Winograd schema dataset.\n\nSignificance:\nOther researchers within the common-sense reasoning community might cite this paper. The significance of this paper to a larger representation learning audience is rather small.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}