{"title": "Studying whether LM encode common-sense information. Novelty, clarity and methodology concerns", "review": "This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.\n\nThis paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. \n\nThe paper also suffers from methodological problems:\n-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. \n-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.\n\nFinally, the paper suffers clarity issues. \n-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). \n-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).\n-- The authors do not address hyper-parameter tuning. \n-- What is the gold standard for the \"special word retrieved\" data? how is it computed?\n\n\nOther comments: \n-- Page 2: \"In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning.\": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.\n-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. \n-- Page 5: how is F1 defined?\n\nMinor comments: \n-- Page 2: \" ... despite the small training data size (100K instances).\": 100K is typically not considered a small training set (for most tasks at least)\n-- Page 5: \"... most of the constituent documents ...\": was this validated in any way? how?\n-- The word \"extremely\" is used throughout the paper without justification in most cases.\n\n\nTypos and such:\npage 1: \"... a relevant knowledge to the above Winograd Schema example, **does** not present ... \": should be \"is\"\npage 5: \"In the previous sections, we ***show*** ...\": showed\npage 7: \"For example, with the ***test*** ...\": \"test instance\"\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}