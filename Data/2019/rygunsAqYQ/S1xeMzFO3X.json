{"title": "Nice Theory, Questionable Practicality", "review": "Summary:\n\nThis paper proposes a nearest-neighbor-based algorithm for implicit maximum likelihood.  Samples are produced by the generator network and then a nearest neighbors algorithm is run to match the samples with their nearest data point.  The generator is then updated using the Euclidean distance between samples and neighbors as the optimization objective.  Six conditions are then provided, and if they are met, then the authors show that this method is performing maximum likelihood on the implied density.  Experiments report Parzen window density estimates, samples from the model, and latent-space interpolations for MNIST, Toronto Faces, and CIFAR-10.\n\nPros: \n\nThe primary contribution of this paper is an algorithm for implicit likelihood maximization with theoretic guarantees.  As far as I\u2019m aware, this is a novel and noteworthy contribution.  Moreover, as each sample must be paired with an observation, it does seem like the algorithm would be somewhat robust to the notorious mode collapse problem.\n\nCons:\n\nMy primary critique of the paper is that there is very little experimental investigation of the crucial details of the algorithm.  Firstly, running the nearest neighbors algorithm seems like it could be a computational bottleneck.  The authors acknowledge this, but then say \u201cthis is no longer the case due to recent advances in nearest neighbor search algorithms (Li & Malik 2016; 2017)\u201d (p 3-4).  No other justification is given, from what I can tell.  A simulation showing how the runtime scales with dimensionality or number of data points would be very useful for knowing the scalability and practicality of the algorithm.  In the same vein, showing that the algorithm works well even with a relaxation such as approximate neighbors or random projections would make the algorithm more attractive to adopt.  \n\nMoreover, I found it frustrating that the paper teases a fix to several well-known GAN issues: \u201cThe proposed method could sidestep the three issues mentioned above: mode collapse, vanishing gradients and training instability\u201d (p 3).  But the paper never experimentally investigates if the proposed approach indeed is better in these aspects.  I was disappointed since, intuitively, the algorithm does seem like it could be robust to mode collapse.  In addition to this lack of experimental focus, the only quantitative result is the Parzen window estimates in Table 1.  The proposed method does best the others but the other reported results are quite old---all from 2015 or earlier. \n\nMinor points: \n\nThe paper is at 10 pages, and while it is well-written, the writing is verbose and could be use tightening.  \n\n\nEvaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis.  Moreover, the theory seems not too divorced from practice (but I didn't check every detail).  However, the evaluation of this method is where the paper falters.  A big issue (that the authors note themselves) is the practicality of performing repeated nearest neighbor iterations.  No runtimes are report, nor are any approximations considered.  Rather, samples and interpolations are given the most discussion.  Furthermore, there is no demonstrations of training stability or quantitative analysis of mode collapse.  Due to these experimental deficiencies, I recommend rejection, weakly.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}