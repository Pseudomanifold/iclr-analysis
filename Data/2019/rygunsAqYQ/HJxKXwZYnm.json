{"title": "Interesting idea, but there is room for improving the presentation and the strength of the results", "review": "The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. The algorithm can be used to implicitly maximize the likelihood of models for which the former quantity is not intractable but for which sampling is easy which is typically the case for implicit models.  The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. However, The paper lacks clarity and the experiments are not really convincing. Here are some remarks:\nExperiments:\n- The estimated likelihood was reported on table 1 using parson window which is known to have bad scaling behavior with the dimension of data. In the end, the table compares methods that maximize different objectives and are evaluated with an unreliable metric. Here are two possible experiments that could be more informative:\n- Consider toy examples for which the likelihood can be evaluated and the MLE obtained easily and then compare with the proposed method. This would already give a good sense of how well the algorithm behaves in simple cases.\n- Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form. This would allow comparing the proposed algorithm to direct likelihood maximization on more complicated datasets as done in [1].\nIt seems like having experiments of this nature is far more convincing than a long justification for why the results are not necessarily state-of-the-art.\n- There are way too many samples on the figures so it is very hard to perform any visual assessment.\n\nTheory:\n- More discussions of the assumptions are needed, concrete examples for which these assumptions hold or not would be very useful. \n- Lemma 2 is a direct consequence of the following result: if p is continuous at x_0 then x_0 is a Lebesgue point.\n\nGeneral remarks on the paper:\n- What complexity is the nearest neighbor algorithm? Since it is crucial for the proposed method to be scalable it is worth presenting this algorithm at a high level in the main paper.\n- The discussion in section 3 could be much more concise if concrete examples and figures were provided. Most of the facts discussed in that section are generally well understood, so conciseness is very appreciated in this case.\n- \u00ab\u00a0\u00a0A secondary issue that is more easily solvable is that samples presented in papers are sometimes cherry-picked; as a result, they capture the maximum sample quality, but not necessarily the mean sample quality.\u00a0\u00bb Could you please provide an example of such paper? I would be very interested in having a closer look.\n- In the last paragraph of section 5, it is said that although the samples may not be state of the art in terms of precision, other methods which achieve better precision \u00ab\u00a0\u00a0may\u00a0\u00bb have less recall. It would be good to have empirical evidence to back this claim.\n\n\nRevision: Although this paper presents an interesting idea, there is a serious lack of evidence to support the claims in the paper:\n- Missing experimental evidence for the efficiency of the NN search algorithm.\n- Experiments are using Parzen window for estimating likelihood which  are known to be unreliable in high dimensions.  \n- None of the suggested experiments were considered. In my opinion these experiments could improve the quality of this work. \n- Moreover, as mentioned by reviewer 1, Grover et al., 2017 provides evidence contrary to what the authors claim but this was never addressed so far in the paper.\n- Theorem 1 makes rather strong assumptions: as pointed out by reviewer 1, assumption 3 is unlikely to hold for the distributions used in practice\n\nFor these reasons I recommend a clear reject. \n\n[1] I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of Maximum Likelihood and GAN-based training of Real NVPs.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}