{"title": "The paper lacks a solid motivation", "review": "Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.\n\nComments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.\n\nSecond, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?\n\nFinally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?\n\nMinor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?\n\nOverall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.\n\n[1] Venna et al. \"Information retrieval perspective to nonlinear dimensionality reduction for data visualization.\" Journal of Machine Learning Research 11, no. Feb (2010): 451-490.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}