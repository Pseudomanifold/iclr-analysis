{"title": "Combine KNN of each training data point into the neural network models, but the motivation does not make sense.", "review": "To exploit the near neighbor/manifold features, this paper proposes to combine k-nearest neighbors of each training data point into the neural network models.  Specifically, the authors propose two families of models built on the popular sequence to sequence neural network models and memory network models, which mimic the k-nearest neighbors model in model learning. Besides, the final label of the classification task will be learned, a sequence of nearest neighbor labels and a sequence of out-of-sample feature vectors (for oversampling) will be also learned in the same time, similar with the multi-task approaches. Since the proposed models are based k-nearest neighbor calculations, which is time-consuming, they also design an algorithm for the \u2018out-of-core\u2019 situation, say load a small portion of data each time to approximately calculate the neighbors. Experiments show that some proposed models work better than baselines in classification and oversampling.\nStrong points:\n(1) As similar with the multi-task setting, the proposed model can output some side useful results, such as oversampling vectors.\n(2) The proposed models work well on the \u2018out-of-core\u2019 situation, which shows that the models are robust.\nConcerns or suggestions:\n(1) The training data $x$ is just one data point, it is not a sequence of data. So the idea to model it in a sequence to sequence setting does not make sense.\n(2) K-nearest neighbors are a set but not a sequence. To model them as a sequence is also strange. The i-th nearest neighbor does not necessarily dependent on the i-1-th nearest neighbor. For example, we consider the one-dimensional case, the focus data may lie between its first and second nearest neighbors. In this case, there is no clear sequence dependence from the second neighbor to the first neighbor. \n(3) The experiments are not sufficient. They only compare with some weak baselines, such as KNN. As the classification task, there are many state-of-the-art models. Besides of these standard classification models, we strongly suggest comparing with the previous method, Wang et al. (2017), which also proposes to combine the k-nearest neighbors into memory network models. I am surprised that the authors did not compare with this very related work. In my opinion, the idea of utilizing nearest neighbors as external memory in Wang et al. (2017) makes more senses.\n(4) The experimental results of some proposed sub-models (key parts of final models) are even worse than the basic kNN model. I should say that the results are not good enough to support the proposed methods. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}