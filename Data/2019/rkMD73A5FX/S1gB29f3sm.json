{"title": "Clear methodological contribution but not written clearly enough", "review": "Summary\n=======\nThe authors extended the linear local attribution method LIME for interpreting black box models by non-linear functions to more accurately approximate black box models locally and identifying interactions between model input variables using the previously published neural interaction detection (NID) framework. They further propose a method to discern between context-dependent and context-free interactions. I found the paper hard to understand without being familiar with previously published literature on detecting interactions and could not understand their approach to detect context-free interactions as well as some aspects of their evaluation. I have also concerns about the practically of their method due to the high runtime and the notion of locality in the light of high-dimensional inputs. In the following, I will briefly summarizing my major criticism and give further details below.\n\nSummary of major criticism\n=====================\n1) The paper is hard to understand without being familiar with previously published literature in the field.\nThe authors do not describe how they define the interaction sets X_I in equation (3).\n\n2) I could not understand their approach for detecting context-free interactions (section 4.2).\n3) It is unclear how discrete variables are locally modified. Can the approach be used for a combination of differently distributed variables, e.g. categorical and continuous variables?\n4) Evaluation metrics such as MSE and R-precision are not described and I could not understand other important aspects of their evaluation, e.g. superpixels (figure 6), why and how they modified network architectures for evaluating context-free interactions (section 5.3), or how they asked Amazon Medical Turk users.\n5) The authors did not compare the runtime of Mathe with LIME, which is presumably high due to the need of fitting multiple non-linear models (equation 3) and sampling the local neighborhood.\n6) The authors  did not compare the total number of model parameters of Mathe with LIME, which might also account for the higher accuracy (lower MSE). \n7) The authors did not evaluate the accuracy and runtime of Mathe on high-dimensional inputs, e.g. large images.\n\n\nDetails\n=====  \n\nAbstract\n---------------------------\n1. The abstract is hard to understand since \u2018context-dependent\u2019 and \u2018context-free\u2019 are undefined. The authors should also not use \u2018dependencies\u2019 as synonym for \u2018interactions\u2019.\n\nIntroduction\n---------------------------\n2. The difference between interactions and context-free and context dependent interactions is unclear. Is a variable without interactions context-free, e.g. Buffalo does not interact with water, and a variable with interactions context-dependent? Also, \u2018classes of data\u2019 is unclear, which can be misinterpreted as class labels for classification problems. The authors should also clarify what they mean by \u2018performance and generality\u2019 in the last section of the introduction.\n\nSection 3.1\n---------------\n3. The description of the model function f(x) and attribution scores phi(x) is unclear. Since f(x) is undefined when it is first mentioned after equation (1), I suggest to first define f(x) and afterwards define phi(x). The purpose of the attribution score function phi(x) is also unclear without prior knowledge. The authors should more clearly describe that f(x) is the target function (model) of interest, e.g. a classifier, and phi(x) locally approximates f(x) and is interpretable in contrast to f(x).\n\nSection 4.1\n---------------\n4. The authors should justify why they are sampling x ~ N(x, sigma * I), which assumes that data instances are iid Normal. This is not the case, e.g., if x is categorical or contains a combination of categorical and continuous variables, and variables are correlated. How is sigma chosen? How many samples are used depending on the dimension of x?\n\n5. The authors should briefly describe the basic idea of NID.\n\n6. How are points sampled from the epsilon neighborhood of x? What is a link function?\n\n7. The subsection \u2018Hierarchical Interaction Attribution\u2019 is hard to understand without being familiar Tsang et al. The authors should give an example of a hierarchical explanation with different layers. \n\nSection 4.2\n---------------\n8. How is the \u2018local vicinity\u2019 defined? Which distance metric is used? This is in particular problematic if x is high-dimensional due the the curse of dimensionality. How are continuous and categorical variables locally modified? Did the authors meant to use a lowercase \u2018k\u2019 in equation (4), i.e. \u2018phi_k =\u2019 instead of \u2018phi_K\u2019? I find this section hard to understand without being familiar with the cited literature.\n\nSection 5.1\n---------------\n9. The number of local vicinity samples is unclear. Did the authors use 1k local vicinity samples for synthetic experiments (Table 1) and 5k samples for real-word synthetic experiments?\n\n10. What is the dimensionality (number of words, characters, or pixels) of real world datasets? This is important since it influences the number samples that are required to approximate the vicinity of a particular data point. It is in particular interesting to know how the model accuracy and runtime depends on the dimensionality and the number of local vicinity samples.\n\n11. The authors should define the evaluation metrics (MSE, R-precision) in addition to citing them.\n\n12. How did the authors choose the interaction sets X_I in equation (3) and (4)? How many MLPs (functions g(.)) did the authors fit to learn phi(x)? Is the number of MLPs the same for LIME and Mathe? Otherwise the performance gain of Mathe over LIME can also be attributed the increased number of models and model parameters (ensemble size). \n\n13. What is the average training time of Mathe and baseline models on the different datasets?\n\nSection 5.2.2\n-----------------\n14. How did the authors choose sigma (0.4, 6, 0.4) for the different datasets?\n\n15. How did Amazon medical turk users evaluate Mathe vs. LIME interactions? Were they given for each sentence the best Mathe and best LIME interaction and asked to decide which one is better? Figure 4 should be more clearly described in the caption. The sentence \u2018The result of this experiment is that the majority of preferred explanation \u2026\u2019 is unclear and unjustified by only showing one example in Figure 4.\n\nSection 5.2.3\n-----------------\n16. The authors should discuss figure 6. The results indicate that neither LIME nor Mathe is able to clearly identify the object of interest, e.g. the water buffalo, and interactions, e.g. between the buffalo and water.\n\nSection 5.3\n---------------\n17. The sentence \u2018... the presence of a French word for \u201cthis\u201d or \u201cthat\u201d, cet, which \u2026\u2019 is unclear. I suggest to give an example to illustrate which interactions are supposed to be detected. The modification of the Transformer model and the reason why this is necessary is unclear. Overall, I find this evaluation unclear and insufficient since it only applies to a particular interaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}