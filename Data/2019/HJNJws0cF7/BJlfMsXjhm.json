{"title": "Review TLDR. Good subject, paper can be presented a bit more clearer ", "review": "\nAuthors describe a method  in which based on the dynamical system description of how a neural networks work, one can construct an analogues process based on Runge Kutta method.\n\nThe paper is sound mathematically, touches upon ideas that have been circling around for a while (https://arxiv.org/pdf/1804.04272.pdf for example) and presents a nice test case for integrating tools from dynamical systems into deep learning.\n\nThe presentation though is a bit convoluted and unclear and I would strongly encourage the writers to break it down and make it more readable \n\nThe method is based on looking at the midsection rules of RK as a feed forward process and mapping that as information propagation in the network. RK methods can be pretty sensitive to Chaos and other non linear effects if one does not take a *very small* time step discretization step. Did you perform some sort of stability analysis on networks as a function of non linear activations inside of it ?\nAlso RK methods a pretty expensive compared to most standard Suzuki-Trotter, can you remark a bit on the computational aspects of you experiments ? \nI think that a big chunk of 3.1 can be either taken out to the appendix or just give a reference to a standard text book and elaborate on training time/resources etc \u2026.\n\nAs a generic question I wonder how does this approach handle things which are weak perception or non perception methods, did you try this on any NLP problems or generic tabular data ?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}