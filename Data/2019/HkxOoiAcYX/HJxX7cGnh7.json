{"title": "Clarification of Compression Phrase in Information Bottleneck theory of DNNs", "review": "This paper provides a principled way to examine the compression phrase, i.e, I(X;T) in deep neural networks. To achieve this, the authors provides an theoretical sounding entropy estimator to estimate mutual information.  Empirically, the paper did observe this compression phrase across both synthetic and real-world data and relates this compression behavior with geometric clustering. \n\nPros:\n- The paper is well-written and easy to understand.\n- The framework for analyzing the mutual information in DNNs is theoretically sounding and robust.\n- The finding of connecting clustering with compression is novel and inspiring. \n\nQuestions:\n- The main concern of the paper is its conclusion. While the experiments in the paper did show the mutual information goes down as the clustering effect enhanced, it only means `clustering` and `compression` are correlated; but the paper claims `clustering` is the source of `compression`, i.e., `clustering` leads to `compression`. This conclusion is problematic. For example, looking at Figure 5(a), as the mutual information goes down from epoch 28 to epoch 8796, not only the clustering gets enhanced, but also the loss is going down. Thus, alternatively, one can also argue the loss (i.e., `relevance`) is the cause of `compression` instead of `clustering`. From another aspect, the effect of `clustering` is also related to the loss, i.e., it is the loss function that pushes the points of the same class to be closer; then, even if the direct cause of `compression` is `clustering`, the root cause might still be the loss (i.e., `relevance`). \n- In Figure 5(a). Why the mutual information increases from epoch 80 - epoch 541? Also, it seems that the test loss increases as the I(X;T) decreases from epoch 541 to epoch 8796. This seems to be counter-intuitive to the claim that \"lower I(X;T) implies higher generalization ability\". Can you explain this phenomenon?\n\n[UPDATE] the authors address my concerns in a detailed way, and the updated revision is rather robust, therefore, I decide to change my score to accept.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}