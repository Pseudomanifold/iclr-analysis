{"title": "see review", "review": "The paper discusses connections between the properties of DNN loss surfaces and the step length SGD algorithms take, a timely topic.  On the whole, reasonably well done, with some interesting observations.\n\nIt makes several claims, most notably that there is an initial regime where SGD visits increasingly sharp regions of the loss surface, followed by a regime where the loss surface gets smoother.  Useful to know, and characterized moderately well.\n\nA weakness is that the generality of that claim is not made clear.  Like many papers in the area, it is an observation, the realm of which is not clarified.  E.g., what properties of the neural network or data does it depend on.  Also not clarified is how this depends on initialization, etc.\n\nThe evaluation should be more systematic, as it is hard to tell how general is the claims of the paper as well as how they depend on implementation details.\n \nThe discussion of Hessian directions ignores very relevant work by Yao et al (https://arxiv.org/abs/1802.08241 and follow up).\n\nThe first figure in Fig 1 is probably misleading, and probably not worth having, the latter two are what is measured and thus more interesting.\n\nThe obvious conclusion from the poor conditioning is that methods designed to addressed poor conditioning, i.e., second order methods, should be considered.  Those should have a complementary dynamics to what is discussed.  This is what is the elephant in the room when you talk about steering towards or away from regions whose curvature matches the SGD step. \n\nI don't know what it means to say \"Where applicable, the Hessian is estimated with regularization applied\"  Is this to speed up computation, why doesn't this change the loss surface, etc.  If you are not measuring Hessian information precisely, then all the claims of the paper fall apart.\n\nSeveral times claims like \"SGD reaches a region in which the SGD step matches ...\"  Of course, the energy surface changes with training time, so it is a little unclear what is being said.\n\nThe main method Nudged-SGD sounds like a poor-mans second order method.  Why not describe it as such (in more than a footnote and appendix), rather than introducing a new acronym.  I don't know that I believe the \"key design principle\" in the appendix for second order methods.  Second order methods rotate and stretch to take a locally-correct step length, and this method sounds like it is doing a poor mans version of that.  There is a good question as to whether the \"thresholding\" into large and small that NSGD is doing causes it to do something very different, but that isn't really evaluated.\n\nAveraging over two random seeds is not a lot.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}