{"title": "interesting idea, but not convincing exps", "review": "This paper proposed a new training algorithm, G-SGD, by exploring the positively scale-invariant space for relu neural networks. The basic idea is to identify the basis paths in the path graph, and convert the weight update in SGD to the weight rescaling in G-SGD. My major concerns are as follows:\n\n1. Empirical significance of G-SGD: While the idea of exploring the structures of relu neural networks for training based on group theory on graphs is interesting, I do not see significant improvement over SGD. The accuracy differences in Table 1 are marginal, training/testing behaviors in Fig. 3 are very similar, and more importantly there is no evidence to support the claims \"with little extra cost\" in the abstract/Sec. 4.3 in terms of computation. Therefore, I do not see the truly contribution of the proposed method.\n\nPS: After reading the revision, I am happy to see the results on computational time that support the authors' claim. However, I still have doubts on the significance of the improvement on CIFAR10 and CIFAR100, because the performance is heavily dependent on network architectures. In my experience, using resnet101 it can easily achieve >96% accuracy. So can you achieve better than this using G-SGD? The training and testing behaviors on both datasets somehow show improvement over SGD, which I take it more importantly than just those numbers. Therefore, I am glad to raise my score.\n\n2. In Alg. 3 I do not quite understand how to apply step 3 to step 4. The connection needs more explanation.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}