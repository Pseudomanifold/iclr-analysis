{"title": "An interesting paper whose novelty seems incremental to the reviewer", "review": "The authors proposed a feature smoothing method without adding any computational burden for defensing against adversarial examples. The idea is that both feature smoothing and Gaussian noise can help extend the range of data. Moreover, the authors combined these methods together to gain a better test and adversarial accuracy. They further proved 3 theorems to try to analyze the biases and variances of decision boundary based on the fisher information and delta method.  \n\nIn my opinion, the main contribution of this paper is to prove that the boundary variance will decrease due to adding one additional regularization term to the loss function. \n\nMain comments:\n1.\tThe proposed feature smoothing method seems less novel to me. In contrast to the mixup method, the proposed method appears to remove the label smoothing part, so it is better to explain or justify why this could be better theoretically.  Moreover, in the PGD and PGD-cw results, the performance is not as good as the Gaussian random noise method. Can the authors offer any discussion or comments on the possible reasons?\n2.\tSome details of the proof of Theorem 4.1 seemed to be omitted. I am a bit confused about this. \na.\t\u201cWithout loss of generality, we further assume b = 0 and w > 0.\u201d  With smaller magnitude, b=0 is reasonable, but why to assume w>0?\nb.\tCould you present the derivation details or the backing theory of the approximation of var(b), when one more regularization term are added?  \n3.\tIn addition, a method of modifying the network is proposed to adapt to the feature smoothing method. However, no experimental results are reported to support its effectiveness. I would believe some empirical evaluations may further strengthen the paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}