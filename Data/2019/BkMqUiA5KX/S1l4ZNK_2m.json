{"title": "Reviews", "review": "This paper aims to address the problem that the LSTM decoder of a language model might be too strong to ignore the information from the latent variable. This is a well-known problem and Bowman et al.(2016) proposed to use KL-annealing to help relieve this issue. The proposed solution in this work is to add a stochastic autoencoder to the original VAE model. The experimental results to some extent confirm the advantage of the model. \n\nMy major concerns are \n1. The scope of the model is relative small. The problem to be solved is not the key problem in language modeling. And the KL-annealing trick is a good and cheap solution. Although the performance is relative better, the model is way to complicated than KL-annealing. \n2. The proposed method is somewhat contrived and not well motivated. What is the motivation behind Equation 2? And the assumption of the model, i.e. Eq. 5 and 6, seems too strong. \n\nBased on the above concerns, I will suggest a rejection for the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}