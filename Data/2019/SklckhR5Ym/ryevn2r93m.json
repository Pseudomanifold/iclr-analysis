{"title": "Weak accept", "review": "This paper proposes an additional loss term to use when training an LSTM LM.  The authors argue that, intuitively, we want the output distribution to retain some information about the context, or \"past\".  Given this, they use the output distribution as input to a one layer network that must predict the current token.  The loss for this network is incorporated as an additional term used when training the LM.  The authors show that by adding this loss term they can achieve SOTA (for single softmax model) perplexity on a number of LM benchmarks.\n\nThe technical contribution is proposing a new loss term to use when training a language model.  The idea is clear, simple, and well explained, and it seems to be effective in practice.  One drawback is that it is highly specific to language models.  Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.  In addition, there is not much theoretical justification for it, it seems like a one-off trick.  The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?\n\nAlthough it is specific to language models, there are a few reasons it might be of broader significance:\n- It falls in the recent line of work in incorporating auxiliary losses for various tasks.  This idea has touched many problems and seen success in practice.\n- Perhaps it can be applied to other sequence models.  For example in encoder-decoder models, the decoder can be thought of as a conditional LM.\n\nExperiments are comprehensive and rigorous.  They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.\n\nPros:\n- New SOTA for single softmax model on LM benchmarks.\n- Simple, clearly explained idea.\n- Demonstrates effectiveness of auxiliary losses.\n- Rigorous experiments.\n\nCons\n- Trick is specific to LM.\n- No large corpus results.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}