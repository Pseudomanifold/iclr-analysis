{"title": "Solid Idea with Focused Experiments", "review": "Summary:\n\nThis paper proposes the \u2018deep weight prior\u2019: the idea is to elicit a prior on an auxiliary dataset and then use that prior over the CNN filters to jump start inference for a data set of interest.  Both explicit and implicit priors are considered, with the latter having the benefit of increased flexibility but having the drawback of a lack of a parametric form to plug in to the ELBO.  The authors address this last point by extending the ELBO appropriately.  Experiments are performed testing the prior\u2019s ability to capture trained filters (Figure 1), provide a good initialization (Figure 2), improve sample efficiency (Figure 3), improve training speed (Figure 4).  \n\nPros:\n\nI like this paper: it is a intuitive idea, and the experiments explore exactly what one would hope to gain from the prior (i.e. better initialization, improved sample efficiency).  I find the paper clearly written and to have a logical flow.  Furthermore, I think eliciting priors---while so crucial in more traditional Bayesian modeling---has been mostly overlooked by the Bayesian ML community, and this paper clearly shows that there are gains to be had from a fairly straightforward procedure.  \n\n\nCons:\n\nThe only potential issue with the paper is the use of the implicit prior, as it complicates variational inference, requiring the extension to the ELBO described in Section 3.2.  As far as I can tell, all experiments use the implicit priors.  I would have liked to have seen an experiment using a parametric prior (eg Gaussian) that shows what gains the implicit prior provides.  Or is it simply a matter of memory efficiency?  \n\n\nOther comments:\n\n-- Nice first sentence in the introduction!  I like how it\u2019s a general statement but immediately focuses the reader\u2019s attention to the paper\u2019s topic.\n\n-- While it doesn\u2019t say so explicitly, the paper seems to imply it is the first to use implicit priors.  Some previous work that uses some form of implicit prior includes:\n\nRuns a chain to refine the prior: Alex Lamb et al. \"GibbsNet: Iterative Adversarial Inference for Deep Graphical Models.\" Advances in Neural Information Processing Systems. 2017.\n\nOptimizes a NN implicit prior based on an invariance objective: Eric Nalisnick and Padhraic Smyth. \"Learning priors for invariance.\" International Conference on Artificial Intelligence and Statistics. 2018.\n\nDefines implicit priors over functions through samplers: Chao Ma, Yingzhen Li, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. \"Variational Implicit Processes.\" arXiv preprint arXiv:1806.02390 (2018).\n\n\nEvaluation:  I recommend this paper for acceptance.  It is a sensible idea with pointed experimental validation.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}