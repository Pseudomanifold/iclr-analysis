{"title": "This paper has encouraging experimental result and the formulation is plausible, but I'm confused about how the proposed model tends to overlook irrelevant information.", "review": "This paper proposed to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer. They aim to maximize mutual information I(Y, \\hat{Y}) while constraining irrelevant information, which further transfers to regularization on F and w. The authors also spend pages explaining how the hyper-parameters can be chosen.\n\nComments:\n1. The experimental results showed a noticeable improvement on CIFAR-100 and is fairly robust to alpha_2.\n2. The formulation seems plausible. \n3. For Figure 2 and discussion in Section 4.2.1, I'm less convinced that the entries with high feature mean is 'relevant' and the others are not by looking at just digit 9 samples. For example, an entry with small feature mean should still be given high w_10 value if for all other 9 digits the same entry has even smaller feature mean. \n\n--------UPDATE AFTER READING THE AUTHORS' COMMENTS-----------\n1. Appendix F lacks explanation. So I'm going to say what I meant in details. \n\nIn order to achieve high accuracy the model must assign high values on some entries of weights to separate the different classes. w_10 is a linear separator, not necessarily entry-wise (unless the features are independent). \nI would take 1k features of each class and compute their principal components. Check if these components are different from class to class and plot the dot product of components and weights. If the following happens I would be more convinced:\n1) principal components of digit 0 and digit 9 differs a lot AND \n2) w_0 weights components of digit 0 higher but weights those of digit 9 lower\n\n2. \"But for our regularized model, the number of weights with high values is smaller compared to that of normal model ...\"\nI'm not convinced. When perturbed by Gaussian noise, the variance on output does not necessarily depends on sparsity. In fact, it depends on the norm of the weights. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}