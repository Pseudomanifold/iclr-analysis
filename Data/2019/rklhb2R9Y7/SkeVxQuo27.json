{"title": "First review", "review": "This paper proposes some new angles to the problem of imitation learning from state only observations (not state-action pairs which are more expensive). \nSpecifically, the paper proposes \"self exploration\", in which it mixes the imitation reward with environment reward from the MDP itself in a gradual manner, guided by the rate of learning.\nIt also proposes a couple of variants of imitation rewards, RTGD and ATD inparticular, which formulate the imitation rewards for random or exhaustive pairs of states in the observation data, as opposed to the rewards proposed in existing works (CSD, SSD), which are based on either consecutive or single states, which constitute the baseline methods for comparison.\nThe authors then perform a systematic experiment using a particular navigation problem on a grid world, and inspect under what scenarios (e.g. when the action spaces of the expert and learner are the same, disjoint or in a containment relationship) which of the methods perform well relative to the baselines. \nSome moderately interesting observations are reported, which largely confirm one's intuition about when these methods may perform relatively well. \nThere is not very much theoretical support for the proposed methods per se, the paper is mostly an empirical study on these competing reward schemes for imitation learning.\nThe empirical evaluation is done in a single domain/problem, and in that sense it is questionable how far the observed trends on the relative performance of the competing methods generalizes to other problems and domains. \nAlso the proposed ideas are all reasonable but relatively simple and unsurprising, casting some doubt as to the extent to which the paper contributes to the state of understanding of this area of research. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}