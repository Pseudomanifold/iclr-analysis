{"title": "State only demonstrations but in deterministic environments ", "review": "The paper proposes to combine expert demonstration together with reinforcement learning to speed up learning of control policies. To do so, the authors modify the GAIL algorithm and create a composite reward function as a linear combination of the extrinsic reward and the imitation reward. They test their approach on several toy problems (small grid worlds). \n\nThe idea of combining GAIL reward and extrinsic reward is not really new and quite straight forward so I wouldn't consider this as a contribution. Also, using state only demonstration in the framework of GAIL is not new as the authors also acknowledge in the paper. Finally, I don't think the experiments are convincing since the chosen problems are rather simple. \n\nBut my main concern is that the major claim of the authors is that they don't use expert actions as input to their algorithm, but only sequences of states. Yet they test their algorithm on deterministic environments. In such a case, two consecutive states kind of encode the action and all the information is there. Even if the action sets are different in some of the experiments, they are still very close to each other and the encoding of the expert actions in the state sequence is probably helping a lot. So I would like to see how this method works in stochastic environments. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}