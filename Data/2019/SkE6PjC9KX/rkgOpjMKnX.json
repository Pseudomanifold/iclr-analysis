{"title": "Official review", "review": "Summary:\nThe authors extend neural processes by incorporating two types of attention processes: self-attention for enriching the features of the context points and cross-attention for producing a query-specific representation. By replacing MLPs and mean pooling with these attention processes, the authors resolve the underfitting problem of NPs. The experimental results show that ANPs converge better and faster than NPs.\n\nOverall, I had fun to read the paper and have not much to complain. Below are some comments and questions.\n\n1. It is intuitive and reasonable that the cross-attention process makes ANPs fit with smaller predictive uncertainty for those regions with many context points. This is well illustrated in the qualitative results in the experiment section.\n\n2. I would like to see an ablation study with the two separate techniques (self- and cross-attention processes) on NPs since the two techniques aim to improve different aspects of NPs. More specifically, I wonder the results of just adding cross-attention with the vanilla MLPs for feature encoding and just replacing the MLPs with self-attention modules while keeping using mean pooling.\n\n3. While the dot product improves the performance significantly, the gain of Laplace is much lower. Also, qualitatively it fails to overcome the underfitting problem. Do you have any intuition about why performs worse than other models?\n\n4. Do you have any specific application in mind? I just wonder some example tasks where contexts are given as inputs.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}