{"title": "Important problem, interesting solutions but less convincing evaluation", "review": "Summary\n========\nThe paper focuses on memory management problem of memory-augmented neural networks when the length of the streaming data is much larger than the number of memory entries. The paper proposes Long-term Episodic Memory Networks (LEMN) which learn a RNN-based agent to erase less important memory entries for storing incoming data by computing a retention score for each memory entry based on:\n* The importance relative to other memory entries: a RNN through all memory entries. \n* An entry\u2019s historical importance: a RNN on an entry\u2019s hidden values over time. \n\nComment\n========\nThe target problem of memory management in MANN is of importance, and the solutions are interesting, especially the Spatio-Temporal LEMN, where both spatial dependencies between memory slots and temporal evolution of each slot itself are modeled.\n\nHowever, the experiments give only proof of concepts without comparison against state-of-the-art for each task. For example, the paper lacks comparison with differentiable neural computer (DNC) [1], the well-known memory-augmented neural networks. Since the DNC also has the ability to keep track on the usage information of memory entries and decide whether to free them or not, there should be a comparison between the proposed LEMN and the DNC. \n\nThe model can be considered as an extension of the DNTM [2], referred to as IM-LEMN in the paper, with the introduction of recurrent connection over space and time. Although comparisons between the LEMN and IM-LEMN are available in section 4.2 and 4.3, there should be a similar comparison in section 4.1 to see whether the addition of recurrent connections brings benefits or not. \n\nAbbreviations should be made clear. E.g., MQN should be written in the full form before using it. The MQN should be cited with Oh et al (2016). \n\nReferences:\n \n[1] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adria Puigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): 471\u2013476, 2016. doi: 10.1038/nature20101. \n\n[2] Caglar Gulc\u00b8ehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural Turing machine with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}