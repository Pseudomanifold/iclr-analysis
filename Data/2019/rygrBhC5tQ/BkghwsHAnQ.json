{"title": " Useful  learning scheme for transitioning between options in continuous domains.", "review": "The paper proposes a scheme for transitioning to favorable starting states for executing given options in continuous domains. Two learning processes are carried out simultaneously: one learns a proximity function to favorable states from previous trajectories and executions of the option,  and the other learns the transition policies based on dense reward provided by the proximity function.\n\t\nBoth parts of the learning algorithms are pretty straightforward, but their combination turns out to be quite elegant. The experiments suggest that the scheme works,  and in particular does not get stuck in local minima. \n\nThe experiments involve fairly realistic robotic applications with complex options,  which renders credibility to the results.    \n\nOverall this is a nice contribution to the options literature. The scheme itself is quite simple and straightforward, but still useful. \n\nOne point that I would like to see elaborated is the choice of exponential (\"discounted\") proximity function. Wouldn't a linear function of \"step\" be \n more natural here? The exponent loses sensitivity as the number of steps away increases, which may lead to sparser rewards.\n  \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}