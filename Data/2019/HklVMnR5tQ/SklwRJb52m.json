{"title": "This paper explores the interpretability of LSTM with multivariable data while providing accurate forecasts in the context of time series. The paper is interesting and addresses a relevant topic. But it has several drawbacks that need to be addressed.", "review": "The contributions of this paper are in the field of LSTM, where the authors explore the interpretability of LSTM with multivariate data obtained from various and disparate applications. To this end, the authors endow their approach with tensorized hidden states and an update process in order to learn the hidden states. Furthermore, the authors develop a mixture attention mechanism and a summarization methods to quantify the temporal and variable importance in the data. They validate the forecasting  and interpretability performance of their approach with experiments. \n\nThe parer is interesting, well structured and and clearly written. Also, the addressed topic of interpretability is pertinent. However, I have several concerns.\n\n1. In the related work the authors state that \u201cIn time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model \u201d . This is not always right - it is not imperative to add the auto-regressive terms, this is optional and depends on the way we want to formulate our time series forecasting approach and the known constraints.  \n2. In section 3 \u2014 Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. And I have several concerns with this for time series forecasting. Because, the past is not always a predictor of the future even - particularly in time series context and in industrial settings. And in the occasions where the past allows to predict the future we do not necessarily need to use LSTM to forecast (the notion of persistency in forecasting is enough).  Therefore, the power of LSTM in forecasting would have been convincing if you omit the target series in your multi-variable input.\n3. In Network Architecture section the authors develop tensorized hidden state and an update scheme. This idea is interesting, I think it would also be good to know what is the algorithmic complexity of this approach? \n4.  In section 3.3 the authors state that \"In the present paper, we choose the simple normalized summation function eq.(9). \" Could the authors justify the reason behind this choice? I am not convinced of the reason behind this, especially the authors mention, right after,  that \"It is flexible to choose alternative functions for f_{agg}\"\n\n5. In the experiment section, concerning the prediction performance the authors present a table showing their results, I believe it would have been more compelling to present the prediction results with graphs showing the normalized cumulative errors, as an example.\n\n6. With regard to the interpretation of the results, the authors show the variable importance as a function of the epoch number, it would be equally important to correlate the same figure with the associated prediction results/normalized cumulative errors as a function of the epoch number - this will allow to assess the importance of the interpretability.\n\nI think it would be important to further justify the pertinence of this work in terms of interpretability (the statement in the introduction \"the interpretability of prediction models is essential for deployment and\nknowledge extraction\" seems to be limited) for example what does it bring knowing the variance importance  as a function of the epoch number. As an example, the Pearson correlation coefficient can help select relevant features to a model, and restrict the number of inputs to the relevant ones - can we draw inspiration from this and explain what the authors are proposing in terms of interpretability... Here the idea is to have a motivation presenting the merits of this work, which I think is missing - particularly with the experiments presented here.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}