{"title": "Overall interesting, but concerns about the key idea and the applicability of the method", "review": "The paper considers the problem of transfer in continuous-action deep RL. In particular, the authors consider the setting where the dynamics of the task change slightly, but the effect on the policy is significant. They suggest that values are better suited for transfer and suggest learning a model to obtain these values.\n\nOverall, there are interesting ideas here, but I am concerned about whether the proposed approach actually solves the problem the authors consider and its general applicability.\n\nThe point about value functions being better suited for transfer than policies is indeed true for greedy policies: it is well-known that they are discontinuous, and small differences in value can result in large differences in policy. This point is hence relevant in continuous control, where deterministic policies are considered.\n\nBut I am a bit confused as to why the proposed approach is better though. Eq. (4) still takes a max w.r.t. the estimated dynamics, etc. So even if the value function is continuous, by taking the max, we get a deterministic policy which has the same problem! That is probably why the performance is quite similar to DDPG. Considering a softer policy parameterization (a continuous softmax analogue) would be more in line with the authors\u2019 motivation.\n\nThe proposed method itself doesn\u2019t seem generally practical unfortunately, as it is suggested to learn the *model* of the environment for with a high-dimensional state space and a continuous action space, and do value iteration. In other words, if Property 2 was easy to satisfy, we wouldn\u2019t be struggling with model-based methods as much as we are! However, I do appreciate that the authors illustrate the model loss curves in their considered domains. This raises a question of when are dynamics \u201ceasy\u201d.\n\nThe theoretical justification is quite weak, since the bound in Proposition 2 is too loose to be meaningful (as the authors themselves acknowledge). One way to mitigate this would be to support it empirically, by considering a range of disturbances of the specified form, and showing the shape of the bound on a small domain. The same thing can be done for the parametric modifications considered in the experiments -- instead of considering a set of instances, consider the performance as a function of the range of disturbances to the same dynamics parameter.\n\nMinor comments:\n* The italicization of certain keywords in the intro is confusing, in particular precise, imprecise -- these aren\u2019t well-defined terms, and don\u2019t make sense to me in the mentioned context. The policy function isn\u2019t more \u201cprecise\u201d than the value.\n* I suggest including the statements of the propositions in the main text", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}