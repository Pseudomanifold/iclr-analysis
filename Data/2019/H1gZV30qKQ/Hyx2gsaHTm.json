{"title": "Limited novelty and inconclusive experiments", "review": "The paper proposes a model-based value-centric (MVC) deep RL algorithm for transfer learning. The algorithm optimizes neural networks to estimate the deterministic transitions and rewards, and uses the these models to learn a value function by minimizing the Bellman residual. Policy is represented implicitly as the action that greedily maximizes the return, expressed in terms of the learned models. The experiments show some improvement on transferability over DDPG and TRPO policies.\n\nThe paper has two relatively independent stories: The title and the introduction motivates the work by discussing the transferability of policies and value functions. However, instead of rigorously evaluating transferability, the paper proposes a model-based algorithm (MVC) for learning policies for continuous actions. Novelty of the new algorithm is quite limited, as it simply uses a learned dynamics model and reward function to learn a value function. Regarding transferability, introducing MVC seem quite orthogonal, and instead, it would be better to have a clear comparison of transferability using existing methods (e.g., DDPG). If having an explicit policy network hurts transferability, then existing algorithms can be modified by replacing the actor with greedy maximization, or alternatively other value based methods that do not involve actor network (NAF, SQL, QT-Opt) could be used.\n\nRegarding the intuition why values transfer better, the examples given in the introduction and Section 3 are good and intuitive. However, from my experience, the limited information content of a policy is only a partial reason for poor transferability, and in practice I have seen policies to transfer, in fact, better than values. The chosen viewpoint based on information content is nice as it can be proven mathematically, but might not be the most insightful and important in practice. The experimental evaluation is not rigorous enough to allow drawing further conclusions. For example, one could compare the two approaches using a wider set of RL algorithms, include more realistic environments (ideally transfer to real-world), or have a heat map illustrating transferability w.r.t. selected parameters. Also, no comparison to the  state-of-the-art methods is provided (PPO, TD3, SAC).\n\nMinor points:\n- Please include the theorems in Section 5 (and proofs in the appendix). The intuition provided in the body is not very clear.\n- Why is it necessary to assume a deterministic dynamics model? Why only the dynamics model can vary between the domains and not also the reward (second paragraph in Section 1)?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}