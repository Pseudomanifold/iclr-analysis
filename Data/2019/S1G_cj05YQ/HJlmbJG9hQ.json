{"title": "Simple approach, but limited novelty, and needs some improvement in exposition and benchmarking of related work", "review": "This paper proposes an approach to mitigate catastrophic forgetting in supervised learning by regularizing activations. The paper views previous techniques (EWC, SI, and GEM) under a multi-task learning lens, and then proposes an additional loss term to minimise the KL between activations from previous and current models, on previous tasks - this is based on a memory which stores some previous samples and their corresponding activations.\n\nI think it is a simple and intuitive approach and a well-written paper. Unfortunately I have a number of concerns that I think preclude publication in the current state.\n\nFirst, in terms of related work, I believe this is very similar to Learning without forgetting (LwF), with the difference that the KL-divergence is computed on samples kept from the previous tasks. This is briefly mentioned in the paper, but I think it needs to be made more explicit, and LwF should be a baseline in the experiments to clearly indicate the benefit of keeping this data. There is also a relationship to EWC: given the connection between the Fisher information and KL, it can be viewed as minimising the KL divergence in parameter space, rather than in activation space (which is the case here). Also note that EWC uses the true Fisher rather than the empirical, contrary to the derivation in equation (2).\nThere are also a number of papers that haven\u2019t been cited in the related work [1][2][3][4].\n\nSecond, I think the motivation in Section 3.1 could be more convincing. Most importantly, it\u2019s not clear to me that the decision boundary *shouldn\u2019t* change for previously misclassified examples, as this could be an opportunity for backwards transfer.\nFurther, I don\u2019t think the point in the last paragraph about having a small data portion is relevant, since they are from the same data distribution, and we would expect misclassified samples to be in the same (low) frequency in Fisher estimation as overall. I think the point of this paragraph is just that it is important to consider the entire predictive distribution of previous tasks rather than the probability of the correct class, so this should be stated more clearly and then justified. \n\nFinally, I think the experimental justification could be improved as well. Beyond permuted MNIST (which it has been argued is not as useful as other baselines [4]), only the final performance on split notMNIST / CIFAR-100 is reported. Some comments and questions:\n- The accuracies of EWC (and possibly SI) in the table are worse than reported in previous work (eg. [1]), so I think this needs to be examined.\n- What is the fine-tuning baseline (I don't believe it is actually clearly defined)? How can it be so low in figure 2a but better in 2b?\n- I think plots over time (performance on all tasks) would be much more useful than the final performance in Table 2 and Fig 2.\n- Errors and error bars would be beneficial for all results.\n- Table 1 should also include the references provided.\n\nSome other comments and questions:\n- Compared to eqn (2), eqn (6) is missing the \u00bd constant.\n- Typos in section 5.3: \"SI performs better than SI\", and VAR instead of SAR.\n- Section 2, unclear of meaning of \"coined with the likelihood\" (should this be \u201ccoincide\u201d?)\n- The first line should be \u201cHumans have the ability to learn...\u201d In general, I think the introduction could use another proofread for grammar and readability as I saw a few minor things.\n\n[1] Nguyen, Cuong V., et al. \"Variational Continual Learning.\" ICLR, 2018.\n[2] Schwarz, Jonathan, et al. \"Progress & Compress: A scalable framework for continual learning.\" ICML, 2018.\n[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" NIPS, 2017.\n[4] Farquhar, Sebastian, and Yarin Gal. \"Towards Robust Evaluations of Continual Learning.\" arXiv, 2018.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}