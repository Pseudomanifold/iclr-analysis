{"title": "The idea of jointly training a word/phrase/sentence mask with the classifier at training time (with some added complexity) to get faster predictions at test time is interesting; however, current results are largely preliminary and requires refinement of performance and understanding", "review": "The authors consider the setting of training a RNN-based text classification where there is a resource restriction on test-time prediction (e.g., time). Conceptually, the approach is essentially a masking mechanism to reduce the number of words/phrases/sentences used in the prediction at test time followed by a classifier trained to handle these \u2018missing\u2019 components. This robustness is achieved by a data aggregation approach that trains on several masked variants determined by the budget level to generalize when encountered new masked instances. This work is most similar to finding \u2018rationales\u2019 in text (e.g., [Lei, Barzilay & Jakkola; EMNLP16]) except the selector is much simpler (specifically, using \u2018shallow\u2019 learners) and is compensated for with the robust classifier component. Experiments are conducted on four datasets, demonstrating varying levels of performance improvements over baseline and competing methods in terms of accuracy and test-time speedup \u2014 followed by more in-depth experiments on multi-aspect sentiment and IMDB datasets, resulting in several more detailed observations.\n\nFrom a high-level perspective, reducing resource requirements specifically at test-time is ostensibly useful in practice as many applications would likely be willing to trade-off some accuracy for {faster performance, lower power requirements, reduced communication bandwidth}, etc. Furthermore, the approach described is sensible and results in some promising empirical findings. My primary concerns with this specific paper are two-fold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what I would expect to work on a new dataset (2) the tradeoffs aren\u2019t well calibrated in the sense that I don\u2019t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesn\u2019t seem trivial to set tradeoffs).  Some of this was considered with additional empirical results, but these also didn\u2019t have sufficient clarity nor discussion for me to really understand the recommendation. Accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.\n\nEvaluating the paper along the requested dimensions:\n\n= Quality: The number of datasets are sufficient and the results are convincing for some configurations. Thus, the authors have established that the method is practical and is promising. However, there isn\u2019t an in-depth study of when the proposed method works, under what configurations, and discussion on how to push this forward. One way to make this more transparent would be to include some curves wrt Table 1 such that I can observe the performance/speedup tradeoffs (is it monotonic?). A good start, but seems incomplete to be really convincing. (4/10)\n\n= Clarity: The writing is fairly clear and well-contextualized wrt existing work. I do think that the discussion could be improved overall in terns of better telling the reader what they should be focusing on in the empirical results. My one specific issue was I still don\u2019t understand what is in the parentheses of {LSTM-jump, skim-RNN} in Table 1. Maybe it is in the text, but I really couldn\u2019t decode (and I can understand the semantics of parentheses in other parts of the table). (4/10)\n\n= Originality: I haven\u2019t seen anything specifically like this, even if it does draw on very closely related methods. However, I think putting them together required good knowledge from ML and NLP communities, and thus wasn\u2019t trivial and the basic idea is clever. (6/10)\n\n= Significance: The underlying idea is interesting and could be significant if further developed, both in terms of better understanding the dynamics and providing better guidance regarding how to do this well in practice. However, just based on the content of the paper, I might play with this, but don\u2019t see it as a solution to problems in low-resource settings, mixed-computation networks (like the cloud example in 4.2). Interesting, but needs more development to be widely adopted. (5/10)\n\n=== Pros ===\n+ an interesting idea that pragmatically builds on existing work to provide a practical solution\n+ experiments conducted on several datasets with some interesting results\n\n=== Cons ===\n- empirical results mixed and little clarity provide on why\n- other than speedup, doesn\u2019t consider other settings in experiments (e.g., power, compression)\n- little theoretical development\n- nothing said about training time difference (even if not the point)\n\nOverall, I like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur). However, I think the idea needs to be further developed before being published at a top-tier venue.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}