{"title": "A good start", "review": "Standard dense 2D convolution (dense in space and channels) may waste parameters. This paper points out the many ways that sparser convolutional operators (\u201ckernels\u201d) may be combined into a single combined operator that may be used in place of dense convolution.\n\nThe paper waxes grandiose about the exponentially many ways that operations may be combined but then defines and tries only four. While trying four alternatives may be quite interesting, the paper could have avoided grandiose language by just stating: \u201cWe tried four things. If you restrict yourself to kernels with 3x3 receptive field and no repeated operations <and probably other assumptions>, there are only four unique combinations to be tried.\u201d Perhaps a page of text could have been saved.\n\nThe paper also defines \u201cinformation field\u201d as the product of the operator\u2019s (spatial) receptive field and the number of channels that each unit can see. Authors proceed to make broad claims about how information field is an important concept that predicts performance. While this may indeed turn out to be an important concept, it is not shown as such by the paper.\n\nClaims:\n\n\u201c\u2026we identify a easily measurable quantity named information field behind various sparse kernel designs, which is closely related to the model accuracy.\u201d\n\n\u201cDuring the process to reduce the design space, we find an unified property named information field behind various designs, which could directly indicate the final accuracy.\u201d\n\nBut the paper does not substantiate these claims.\n\nSince information field is defined as the product of the receptive field and the number of channels seen, it would seem necessary to show, say, at least some experiments with varying receptive field sizes and number of channels. Then it might be shown, for example, that across a wide array of network sizes, widths, depths, holding all but information field constant, information field is predictive of performance. But these experiments are not done.\n\nReceptive fields: the paper *only ever tries 3x3 receptive fields* (Table 2, 3, 4). So absolutely no support is given for the relevance of two out of the three components (i size, j size) comprising information field!\n\nNumber of channels: as far as I can tell, Table 2 and 3 contain the only results in this direction. Reading off of Table 2: for networks of the same depth (98), info size 256 works a bit better than 128*, and 512 works a bit better than 256. \n\n* (see also Table 3 lines 4 vs 5 show the same 256 vs 128 effect.)\n\nCool. But *two comparisons* are not even close to enough to support the statement \u201cwe find an unified property named information field behind various designs\u201d. It is enough to support the statement \u201cfor this single network we tried and using 3x3 receptive fields, we found that letting units see more channels seemed to help.\u201d Unfortunately, this conclusion on its own is not a publishable result.\n\n\n\nTo make this paper great, you will have to close the gap between what you believe and what you have shown.\n\n(1) You believe that information field is predictive of accuracy. So show it is predictive of accuracy across sufficiently many well-controlled experiments.\n\n(2) It may also be that the PWGConv+DW+PWGConv combination is a winning one; in this case, show that swapping it in for standard convolution helps in a variety of networks (not just ResNet) and tasks (not just ImageNet).\n\n\n\nOther minor notes:\n\n - Equations are critical in some parts of some papers, but e.g. triple nested sums probably aren\u2019t the easiest way of describing group convolution.\n\n - The part about regexes seemed unnecessary. If 1000 different designs were tried in a large automated study where architectures were generated and pruned automatically, this detail might be important (but put it in SI). But if only four are tried this detail isn\u2019t needed: we can see all four are different.\n\n - Figure 1 is a great diagram!\n\n - How efficient are these kernels to compute on the GPU? Include computation time.\n\n - \u201cEfficiency given the total amount of parameters.\u201d These equations and scaling properties seemed to miss the point. For example, \u201cIt can be easily verified that given the total number of parameters the greatest width is reached when the best efficiency is achieved.\u201d This is just saying that standard convolution scales poorly as F -> infinity. This doesn\u2019t seem like the most useful definition of efficiency. A better one might be \u201cHow many params do you need to get to x% accuracy on ImageNet?\u201d Then show curves (# of params vs accuracy) for variants of a few popular model architectures (like ResNet or Xception with varying width and depth).\n\n - 3.3.2: define M and N\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}