{"title": "New technique for positive-unlabeled learning focussing on addressing selection bias.", "review": "In this paper, the authors present a new technique to learn from positive and unlabeled data. Specifically they are addressing the issues that arise when the positive and unlabeled data do not come from the same distribution. The way to achieve this is to learn a scoring function which preserves -the order- of the label posteriors. In other words, the authors are not making assumptions and then learning the exact posterior of p(y|...) but rather just a function r(x) with the property that if p(y_i) < p(y_j) then r(x_i) < r(x_j).\n\nI am not super familiar in the area but I didn't see any fundamental flaws. The approach makes sense and although I cannot judge the novelty of this paper, it is a useful tool in the PU learning toolbox addressing an arguably important problem (selection bias). Except for section 5.3, the experiments are not that interesting as they are made up artificially by the authors.\n\nThoughts:\n- In example 1, be specific about what p(y|...) and p(o|...) are.\n- In example 2, I wasn't sure what p(o|...) exactly would be.\n- Assumption 1, the first sentence I understand. The \"if and only if\" part I don't see. Can you clarify?\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}