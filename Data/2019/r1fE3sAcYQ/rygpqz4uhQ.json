{"title": "a good work on one shot model training", "review": "This paper discusses the phenomena of \u201cneural brainwashing\u201d, which refers to that the performance of one model is affected via another model sharing model parameters. To solve the issue, the authors derived a new loss out from maximizing the posterior of the parameters. With the new loss, the neural brainwashing is largely diminished. \nThe derived new loss looks meaningful to me and I think this is a valuable work for handling the weights coadaptation between two neural models, which with no doubt will bring great interests within the community of neural architecture search.\n\nHere are some comments on the aspects that this paper can be improved: \n\n1)\tA very important related work [1] is missed in this paper. [1] discussed the properties of \u201cone-shot model\u201d, which means that several different architectures are unified into the same model by sharing model weights. Furthermore, [1] discussed \u201cneural brainwashing\u201d (although not with the same name) and how to handle it in a very simple way (by randomly dropping path). This definitely should be a baseline to compare with. In addition, a very recent work [2] also leverages model sharing to conduct neural architecture search.\n\n2)\tAlthough I understand that to improve accuracy of NAS is not the main goal of this paper, the baseline number to be improved over is too weak. For example, 4.87 of CIFAR10 in ENAS. Per my own hands on experience, it does not need too many hyperparameter tuning of ENAS to obtain < 4% error rate. Please provide more convincing baseline numbers and supporting evidences of the better performance of WPL in NAS.\n\n\n[1] Bender, Gabriel, et al. \"Understanding and simplifying one-shot architecture search.\" International Conference on Machine Learning. 2018.\n[2] Luo, Renqian, et al. \"Neural architecture optimization.\" NIPS (2018).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}