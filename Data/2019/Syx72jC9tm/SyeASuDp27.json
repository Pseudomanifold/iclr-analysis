{"title": "Nice combinatorics, but this is not what graph neural networks actually do", "review": "Given a graph G of n vertices, the activations at each level of a graph neural network (G-NN) for G \ncan be arranged in an n^k tensor T for some k. A fundamental criterion is that this tensor must be equivariant \nto permutations of the vertices of G in the sense of each index of of T being permuted simultaneously. \n\nThis paper enumerates the set of all linear maps that satisfy this criterion, i.e., all linear maps \nwhich (the authors claim) can serve as the analog of convolution in equivariant G-NNs. \nThe authors find that for invariant neural networks such maps span a space of dimension just b(k), whereas \nfor equivariant neural networks they span a space of dimension b(2k).\n\nThe proof of this result is simple, but elegant. It hinges on the fact that the set of tensor elements of \nthe same equality type is both closed and transitive under the permutation action. Therefore, the \ndimensionality of the subspace in question is just the number of different identity types, i.e., \npartitions of either {1,...,k} or {1,...,2k}, depending on whether we are talking about invariance or \nequivariance.\n\nMy problem with the paper is that the authors' model of G-NNs doesn't actually map to what is used \nin practice or what is interesting and useful. Let me list my reservations in increasing order of significance.\n\n1. The authors claim that they give a ``full characterization'' of equivariant layers. This is not true. \nEquivariance means that there is *some* action of the symmetric group S_n on each layer, and wrt these actions \nthe network is equivariant. Collecting all the activations of a given layer together into a single object L, \nthis means that L is transformed according to some representation of S_n. Such a representation can always be \nreduced into a direct sum of the irreducible representations of S_n. The authors only consider the case then \nthe representation is the k'th power of the permutation representation (technically called the defining \nrepresentation of the S_n). This corresponds to a specific choice of irreducibles and is not the most general case. \nIn fact, this is not an unnatural choice, and all G-NNs that I know follow this route. \nNonetheless, technically, saying that they consider all possible equivariant networks is not correct.\n\n2. The paper does not discuss what happens when the input tensor is symmetric. On the surface this might seem \nlike a strength, since it just means that they can consider the more general case of undirected graphs (although \nthey should really say so). In reality, when considering higher order activations it is very misleading because \nit leads to a massive overcounting of the dimensionality of the space of convolutions. In the case of k=2, for \nexample, the dimensionality for undirected graphs is probably closer to 5 than 15 for example (I didn't count).\n\n3. Finally, and critically, in actual G-NNs, the aggregation operation in each layer is *not* \nlinear, in the sense that it involves a product of the activations of the previous layer with the adjacency \nmatrix (messages might be linear but they are only propagated along the edges of the graph). \nIn most cases this is motivated by making some reference to the geometric meaning of convolution,  \nthe Weisfeiler-Lehman algorithm or message passing in graphical models. In any case, it is critical that the \ngraph topology be reintroduced into the network at each layer. The algebraic way to see it is that each layer \nmust mix the information from the vertices, edges, hyperedges, etc.. The model in this paper could only aggregated \nedge information at the vertices. Vertex information could not be broadcast to neighboring vertices again. \nThe elemenary step of ``collecting vertex information from the neighbors but only the neighbors'' cannot be \nrealized in this model.\n\nTherefore, I feel that the model used in this paper is rather uninteresting and irrelevant for practical \npurposes. If the authors disagree, I would encourage them to explicitly write down how they think the model \ncan replicate one of the standard message passing networks. It is apparent from the 15 operations listed on \npage 11 that they have nothing to do with the graph topology at all.\n\nMinor gripes:\n\n- I wouldn't call (3) and (4) fixed point equations, that's usually used in dynamical systems. Here there is \nan entire subspace fixed by *all* permutations.\n\n- Below (1), they probably mean that ``up to permutation vec(L)=vec(L^T)''. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}