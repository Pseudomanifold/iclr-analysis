{"title": "Simple but useful method, substantial experiments", "review": "This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The authors argue that binarising RNNs is due to a covariate shift, and address it with stochastic quantised weights and batch normalisation.\nThe proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs, with almost no loss in test performance.\nBased on the more efficient RNN cell, the authors furthermore describe a more efficient hardware implementation, compared to an implementation of the full-precision RNN.\n\nThe core message I took away from this work is: \u201cOne can get away with stochastic binarised weights in a forward pass by compensating for it with batch normalisation\u201d.\n\nStrengths:\n- substantial number of experiments (6 datasets), different domains\n- surprisingly simple methodological fix \n- substantial literature review\n- it has been argued that char-level / pixel-level RNNs present somewhat artificial tasks \u2014 even better that the authors test for a more realistic RNN application (Reading Comprehension) with an actually previously published model.\n\nWeaknesses:\n- little understanding is provided into _why_ covariance shift occurs/ why batch normalisation is so useful. The method works, but the authors could elaborate more on this, given that this is the core argument motivating the chosen method.\n- some statements are too bold/vague , e.g. page 3: \u201ca binary/ternary model that can perform all temporal tasks\u201d\n- unclear: by adapting a probabilistic formulation / sampling quantised weights, some variance is introduced. Does it matter for predictions (which should now also be stochastic)? How large is this variance? Even if negligible, it is not obvious and should be addressed.\n\n\nOther Questions / Comments\n-  How dependent is the method on the batch size chosen? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var. What happens at batch size 1? Are predictions of poorer for smaller batches?\n- Section 2, second line \u2014 detail: case w_{i,j}=0 is not covered\n- equation (5): total probability mass does not add up to 1\n- a direct comparison with models from previous work would have been interesting, where these previous methods also rely on batch normalisation\n- as I understand, the main contribution is in the inference (forward pass), not in training. It is somewhat misleading when the authors speak about \u201cthe proposed training algorithm\u201d or \u201cwe introduced a training algorithm\u201d\n- unclear: last sentence before section 6.\n\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}