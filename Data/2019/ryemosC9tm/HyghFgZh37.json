{"title": "Learning representation-constrained autoencoders", "review": "The paper propose to learn autoencoders which incorporate pairwise constraints while learning the representation. Such constraints are motivated from the available side information in a given application. Inducing application-specific structure while training autoencoders allows to learn embeddings with better neighborhood preserving properties. In wireless positioning application, the paper proposes fixed absolute/relative distance and maximum absolute/relative distance constraints. Experiments on synthetic and real-word datasets show improved performance with the proposed approach.\n\nSome comments/questions:\n\n1. Table 1 shows different constraints along with the corresponding regularizers which are employed while training autoencoders. How is the regularization parameter set for (so many) regularizers?\n\n2. Employing constraints (e.g. manifolds) while learning representation has recently attracted attention (see the references below). The proposed approach may benefit from learning the constraints directly on the manifolds (than via regularizers). Some of the constraints discussed in the paper can be modeled on manifolds.\n\nArjovsky et al (2016). Unitary evolution recurrent neural networks\nHuang et al (2017). Orthogonal weight normalization: Solution to optimization over multiple dependent Stiefel manifolds in deep neural networks.\nHuang et al (2018). Building deep networks on Grassmann manifolds\nOzay and Okatani (2018). Training CNNs with normalized kernels.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}