{"title": "simple and reasonable idea, somewhat unconvincing theoretical analysis, weak experiments", "review": "Summary of the paper:\nThis paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. \"structured\" means that instead of just minimizing the L2 norm of the gradients, a \"mahalanobis norm\" is minimized. The covariance matrix is updated continuously to track the \"structure\" of gradients/perturbations. Whitebox attack and blackbox attack \n\nThe paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting.\n\nHowever, I believe the paper has major flaws in several aspects.\n\nThe whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. https://github.com/MadryLab/mnist_challenge\nI noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST.\n\nIn my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because \"intrinsic\" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated.\nSo if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.\n\nThere're also a few problems in the motivation / analysis. \n\"\"\"A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations.\"\"\"\nThe adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss.\n\n\"\"\"Thus, under the assumption that \\phi \\approx \\phi^* and of small perturbations (such that we can ignore higher order terms.\"\"\"\nThe Bayes optimal assumption seems to be arbitrary to me. If \\phi is nearly Bayes-optimal, why would we worry about adversarial examples?\n\n\n\nOther relatively minor problems\n\nIn the caption of Figure 1, \"\"\"Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure.\"\"\"\nPGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse?\n\nIn Section 3.1, the paper talks about both centered and uncentered adversarial examples.\nI assumed that the authors mean that the distribution of perturbations are centered?\nFirst, I think this the authors should make this more explicit.\nSecond, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered?\n\nFigure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper.\n\nI don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved?\n\nFigure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}