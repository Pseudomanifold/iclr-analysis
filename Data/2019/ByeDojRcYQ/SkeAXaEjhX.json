{"title": "An interesting idea, with some good theoretical justification and interesting evaluations. There could be more precise details (particularly on experiments) and the approach used to combine gradients could be better related to prior work. ", "review": "## Summary\nThe authors present an approach to training collaborative swarms of agents based around giving all agents identical (or near identical) policies. The training regime involves individual agents rolling out trajectories based on slight perturbations of an agent of focus keeping the policy of other agents fixed. This is repeated for each agent, then these trajectories are used to batch update the joint policy with an average gradient.\n\nOn the whole, I think the paper is well written and the idea novel. There are places where the explanations could be clearer and details more explicit (see below for examples). There are some interesting evaluations but I am not sure these are as rigourous as they could be, in particular (but not limited to) the survival game. I am, however, recommending this for acceptance as on balance the positives outweigh the negatives.\n\n## More detailed comments\nThe authors could make it a bit clearer what existing work on averaging policy gradients exist, and whether their approach is a natural extention of these existing approaches to their swarm domain, or whether there is additional novelty there. It is unclear to me which is the case. They talk about meta-learning in the related work but it is unclear precisely how they relate this to their own work.\n\nThe authors could describe their experiments a little more explicitly. For instance, they say that agents are penalised for getting too close in the navigation task, but do not say how this penalty is constructed. Is it a step function based on distance or something else? Also, they should state what parameters they use for each of the environmental factors, e..g minimum distance etc.\n\nThe survival game is poorly described, as are choices for the evaluation of it. I realise these games are designed elsewhere, but if the exact same parameters are used as in the original papers then this should be stated. Finally, I am a little unclear why the survival game cannot be compared with other algorithms, even if those algorithms fail to learn anything. I realise that the algorithms with decentralised actors won't scale here, but something like the mean field approaches described by the authors in the related work, or even less sophisticated algorithms using some (but not all) features of their own approach would show something interesting. And the choices of 225 and 630 agents needs better justification.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}