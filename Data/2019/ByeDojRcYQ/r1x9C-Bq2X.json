{"title": "This paper addresses MARL in the case of having many homogeneous agents with shared policy. Some concerns about the the validity of projection.", "review": "The introduction of the paper is well-written and the authors quite clearly explain the purpose; however, I believe that the notation should be revisited to further simplify them. The algorithm pretty much is similar to the A2C algorithm (very minor differences) and overall, I don't see the contribution of the paper to be significant enough. Also, there are a few other concerns that I summarize next:\n\n1) In example 1, just knowing the relative distance with all other agents is not equivalent to knowing the full state of the environment. This is because the angles with the other agents are important; i.e. you need to know (r,\\theta) Polar Coordinates.\n\n2) I personally don't like using the word \"constrained'' used in this paper. Going back to constrained RL literature, the purpose of constrained RL is, for example, not entering the hazardous states. At the first time reading the paper, I thought that the constraints are referring to such cases, e.g. make sure that the agents never hit each other. But, the concept of constraint used in this work is totally different and it simply means copying the network weights.\n\n3) In section 3, using the neural networks and averaging of the weight does not make any sense. What does it mean to average weights of several NN? NN is a nonlinear function approximator, and you cannot average weights. Based on your algorithm, I see that you aggregate the gradients which is a correct approach. In fact, the projection step defined in page 5 is never used in your implementation I guess, because otherwise, this algorithm will not work.\n\n4) The distributed model pretty much resembles A2C algorithm where each agent can be considered as a thread. At every time, you only do a gradient step in one of the threads and for the rest, you use the central policy. This way, you stabilize the non-stationarity caused by concurrently learning policy. I do not see any major difference.\n\n5) What is the reason that you do not use the Critic?\n\n6) Having $\\theta_n=\\theta$ implies that $\\pi_n = \\pi$, but the other way around does not hold. Constraints of (8) are not equivalent to (5).\n\n7) Are you using different policies for different agents when using MADDPG or TRPO_Kitchensink? I think for a fair comparison, the agents should also share the policies in these algorithms too. It is very hard to believe that TRPO_Kitchensink and MADDPG almost learn nothing, or even learn in reverse direction (Fig 3).\n\n8) I think that the baseline comparisons for the case of having a small number of agents are necessary.\nMinor:\n* In section 3.1, the notations are over-populated. I would suggest simplifying the notations. \nIn (4), (5),(6), argmax_\\pi\n* (16) is simply the sum of gradients of two consecutive policy gradient steps which can be derived by (sum of grad = grad of sum). You might add this as an intuition beyond this formula.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}