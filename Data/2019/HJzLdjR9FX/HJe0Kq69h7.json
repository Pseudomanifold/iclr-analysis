{"title": "A simple repeated compress and fine-tune method.", "review": "The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). \n\nPros:\n\n- The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization.\n\n\nCons:\n\n- The idea is a simple extension of existing work.\n- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.\n  ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}