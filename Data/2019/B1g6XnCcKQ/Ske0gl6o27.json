{"title": "An interesting idea in an unnatural setup", "review": "This paper explored self-supervised learning of object representations. The main idea is to encourage objects with similar features to get further \u2018attracted\u2019 to each other. The authors demonstrated that the system works on real objects with simple geometry. \n\nThe problem of self-supervised learning from video data is an important one. This paper is making progress along this direction. The approach is new. The paper is in general well written and clear. \n\nMy main concern is the problem setup is unnatural. I can imagine two types of approaches with video input: object-based or pixel-based. An object-based approach detects and tracks objects over time, and then learns object representations upon correspondence. Tracking objects is hard, but gives object correspondence as the basis for learning. A pixel-based approach does not detect objects, but learns dense feature representations for each pixel via signals such as optical flow. Here, training signals become noisier, but the algorithm no longer suffers from the errors that may arise in object detection. It can also generalize to objects that are hard to be detected, such as soft bodies and liquids.\n\nThe proposed system however lies in an uncanny valley: it performs object detection per frame (which is hard and noisy), but it then discards the temporal signals in the video and therefore loses object correspondence. To compensate that, the authors proposed a sometimes incorrect heuristic (based on nearest neighbors) as supervision. The motivation behind such design is unclear, and I\u2019d love to hear the authors\u2019 feedback on it. \n\nThe authors should also cite and discuss those pixel-based methods (see below).\n\nThe experimental results are neat, but not too impressive. The objects used are all rigid with simple geometry. As said before, such an approach would have a hard time generalize to deformable objects and liquids. The results on the real robot is not very convincing, as the system doesn\u2019t really learn object representations that can be used for manipulation. For example, for the results in Fig 7, I assume the way of grasping is handcrafted, instead of learned by the network. Please let me know if I\u2019m wrong.\n\nIn general, I feel this paper interesting but not exciting, and it\u2019s unclear what we can really learn from it. I\u2019m therefore on the border, leaning slightly toward rejection. I\u2019m happy to adjust my rating based on the discussion and revision.\n\nRelated work\n\nSelf-supervised Visual Descriptor Learning for Dense Correspondence. ICRA 17.\nDense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. CORL 18 (concurrent work, though on arxiv since June).\nUnsupervised learning of object frames by dense equivariant image labelling. NIPS 17.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}