{"title": "Simplistic experimental setup, no technical novelty, missing baselines and experimental details", "review": "Summary:\nThis paper aim for learning a feature representation from video sequences captured from a scene from different view points. The proposed approach is tested on a table top scenario for synthetic and real scenes. Pairs of frames from captured video is selected, then a pre-trained object detector finds the object proposal bounding boxes. The positive pairs are found using nearest neighbor between cropped bounding boxed from two random frames and finally a network is trained using an n-pair contrastive loss function and hence called object-contrastive network.\n\nPros: Unsupervised feature learning is an interesting area in computer vision and ML and this paper tries to tackle this problem for objects seen from different viewpoints. \n \nCons:\n-Not enough technical novelty compared to current unsupervised feature learning approaches. The proposed approach uses two random frame from a sequence and use nearest neighbor match based on some pre-trained network and compute an n-pair contrastive loss of Sohn 2016 on top. \n\n-Experimental set up for the real experiment is very simplistic and objects with similar appearance and colors are appearing in both train and test sets which is far from random selection of object instances and categories into test and train (plates, bowls and cups with similar colors and similar shapes).\nWhy the proposed method is not trained and tested on tasks similar to [a]? There can be similar setup in training videos of [a] and tested on object detection task on videos of natural scenes (rather than a particular indoor table top scenario). [a] is a relevant baseline which is missed.\n[a] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, 2015. \n\nMissing Baselines:\n-Comparing against learned embedding feature with feature trained on (a) ResNet50 pre-trained ImageNet or (b) ResNet50 pre-trained COCO for both NN and linear setup is missed. There is only ResNet50 embedding pre-trained on ImageNet shown in table 1.\n-Comparing against previous self-supervised methods that use tracking is missed.\n-Comparing against previous methods that learn embedding on delta time and/or camera location is missed.\n\nIssues in experimental setups:\n\n-Section 5.2 with title \u201cInstance Detection and Tracking\u201d only shows three qualitative example if instance retrieval and ranking for a pair of views. There is no standard quantitative result for instance tracking in this section such accuracy of trajectory over time. Also the detail of experimental setup for table 2 is missing. Number of instances, pairs, real or synthetic, etc.\n\n-The object appearance is not similar from different view. In the current experimental setup (which is less than 90 degrees different viewpoint) the appearance can be similar. It is not clear if the proposed approach can work with more variation of camera viewpoint.\n\n-There are many hand designed assumptions in the experimental setup which makes it unnatural in real scenario. For instance, the number of objects in all frames are approximately equal and all objects are visible in all frames. In real scenario the objects can appear and disappear from the camera viewpoint based on camera field of view and can can cause drastic changes in the nearest neighbor set up in the method formulation. What happen if in extreme case there is no object in one of the frames when wants to find the pairs? It can match with some random patches then? \n\n-In Page 5, section 4.1, it is mentioned \u201cWe randomly define the number of objects (up to 20) in a scene and select half of the objects from two randomly selected categories. The other half is selected from the remaining object categories.\u201d. What is the logistic behind this choice? The reason for this setup is not explained in the paper.\n\n-Throughout the paper the words \u201cattribute\u201d, \u201cclass\u201d, \u201csemantic\u201d, \u201clabel\u201d are used in a confusing manner based on the current literature. For example, \u201c\u2026naturally encoding attributes like class, color, texture and function\u2026\u201d in Introduction section. Class is not an object attribute.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}