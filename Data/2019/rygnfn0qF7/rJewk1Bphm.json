{"title": "Reasonable method, but not too much novelty", "review": "Reasonable method, but not too much novelty\n\n[Summary]\n\nThe paper proposed techniques to pretrain two-layer hierarchical bi-directional or single-directional LSTM networks for language processing tasks. In particular, the paper uses the word prediction, either for the next work or randomly missing words, as the self-supervised pretraining tasks. The main idea is to not only train text embedding using context from the same sentence but also take the embedding of the surrounding sentences into account, where the sentence embedding is also context-aware. Experiments are done for document segmentation, answer passage retrieval, extractive document summary.\n\n[Pros]\n\n1.\tThe idea of considering across-sentence/paragraph context for text embedding learning is very reasonable. \n2.\tThe random missing-word completion is also a reasonable self-supervised learning task. \n3.\tThe results are consistently encouraging across all three task. And the performance for \u201canswer passage retrieval\u201d is especially good. \n\n[Cons]\n\n1.\tThe ideas of predicting the next word (L+R-LM) or missing words (mask-LM) have been around and widely used for a long time. Apply this idea to an two-layer hierarchical LSTM is a straightforward extension of this existing idea.\n2.\tFor document segmentation, no comparison with other methods is provided. For extractive document summary, the performance difference between the proposed method and the previous methods are very minor.\n3.\tImportantly, the experiments can be stronger if the learned embedding can be successfully applied to more fundamental tasks, such as document classification and retrieval. \n\nOverall, the paper proposed a reasonable method, but the significance of the paper can be better justified by more solid experiments.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}