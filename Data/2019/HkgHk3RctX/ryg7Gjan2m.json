{"title": "Interesting application of pointer networks for re-ranking", "review": "If the stated revisions are incorporated into the paper, it will be a substantially stronger version. I'm leaning towards accepting the revised version -- all my concerns are addressed by the authors' comments.\n---\nThe paper uses a Seq2Seq network to re-rank candidate items in an information retrieval task so as to account for inter-item dependencies in a weakly supervised manner. The gain from using such a re-ranker is demonstrated using synthetic experiments as well as a real-world experiment on live traffic to a recommender system.\n\nParagraph below eqn2: for *any* fixed permutation. Figure1 and notation indicates that, at each step of decoding, the selected input x_j is fed to the decoder. The text suggests the embedding of the input e_j is fed to the decoder (which is consistent with \"go\" being a d-dimensional vector, rather than the dimensionality of x).\n\nSingle step decoder with linear cost: Is there a missing footnote? If not, why call it p^1? Simpler notation to just call it p.\nEqn7: Role of baseline. In REINFORCE, b(x) is typically action-independent (e.g. approximating the state value function induced by the current policy). L_pi(theta) is action dependent (depends on the permutation sampled from P_theta). So, I'm unclear about the correctness of Eqn7 (does it yield an unbiased policy gradient?)\n\nEqn5: Expected some discussion about the mismatch between training loss (per-step cross entropy) vs. testing loss (e.g. NDCG@k). Does a suitable choice of w_j allow us to recover standard listwise metrics (that capture interactions, e.g. ERR)?\n\nExpt implementation: Why was REINFORCE optimizing NDCG@10 not regularized?\nExpt cascade click model: Expected an even simpler experiment to begin with; [Is the Seq2Slate model expressive enough to capture listwise metrics?] Since the relevances are available, we can check if Seq2Slate trained to the relevance labels yields NDCG performance comparable to LambdaMART, and whether it can optimize metrics like ERR.\n\nTable1: On the test set, is NDCG&MAP being computed w.r.t the ground truth relevances? So, is the claim that Seq2Slate is more robust when clicks are noisy in a one-sided way (i.e. relevant items may not receive clicks)? Not clear how much of this benefit comes from a more expressive model to predict relevances (see suggested expt above) vs. Seq2Slate from clicks. NDCG & MAP definitely don't account for inter-item dependencies, so unclear what is being tested in this experiment.\n\nFor diverse-clicks, eta=0 while for similar-clicks, eta>0 (in a dataset dependent way). Why? Can expt numbers for the 3 choices of eta be added to the appendix? [Seems like cherry-picking otherwise]\n\nCan Ai et al, 2018 be benchmarked on the current expt setup? Is it identical to the single-step decoder proposed in the paper?\n\nComment: Would be more accurate to call seq2slate a re-ranker throughout the text (in the abstract and intro, the claim is seq2slate is a ranker).\n\nExpected to see training time and inference time numbers. Since Seq2Slate does extra computation on top of, e.g. LambdaMART, it is useful to know how scalable it can be during training, and when the extra perf is worth the O(n^2) or O(n) [for single-step decoding] during inference.\n\nGeneral comments:\nClarity: The paper is well written and easy to follow. There are a few notation choices that can be improved/clarified.\nOriginality: This work seems closely related to Ai et al, SIGIR 2018. Going from a single-shot decoding to sequential decoding is an incremental step; the real-world experiment seemed the most novel and compelling contribution (however, it is unclear how one can reproduce it).\nSignificance: The paper addresses a significant real-world problem. Many high-impact applications of ranking rely on being able to model inter-dependencies well.\nQuality: The paper has interesting contributions, but can be substantially stronger (see some of the specific comments above). For instance, A careful study of the computational vs. performance trade-off, fine-grained comparison of different decoding architectures, better understanding of which architectural choices allow us to model any arbitrary ranking metric more effectively vs. which ones are more robust to click noise vs. which ones capture inter-item dependencies.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}