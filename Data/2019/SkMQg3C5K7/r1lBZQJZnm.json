{"title": "A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks", "review": "Summary: \n \nThe paper provides the convergence analysis at linear rate of gradient descent to global minima for deep linear neural networks \u2013 the fully-connected neural networks with linear activation with l2 loss. The convergence only works under two necessary assumptions on initialization: \u201cweight matrices at initialization are approximately balanced\u201d and \u201cthe initial loss is smaller than the loss of any rank-deficient solution\u201d. The result of this work is similar to that of Barlett et al. 2018, but the difference is that, in Barlett et al. 2018, they consider a subclass of linear neural networks (linear residual networks \u2013 a subclass of linear neural networks which the input, output and all hidden layers are the same dimensions). \n \nComments: \n \nThis paper focuses on theoretical aspect of Deep Learning. Yes, theoretical study of gradient-based optimization in deep learning is still open and needs to spread more. I have the following comments and questions to the author(s) and hope to discuss further during the rebuttal period: \n \n1) Most of the deep learning applications are well-known used the neural networks with non-linear activation (specifically ReLU). Could you please provide any successful applications that linear neural networks could achieve better performance over the \u201cnon-linear\u201d one? Yes, more layers may lead to better performance since we have more parameters. However, it is still not clear that which one is better between \u201clinear\u201d and \u201cnon-linear\u201d with the same size of networks. I am not sure if this linear neural networks could generalize well. \n \n2) For N=1, the problem should become linear regression with strongly convex loss, which means that there exists a unique W: y = W*x in order to minimize the loss. Hence, if W = W_N*....*W_1, the problem becomes non-convex w.r.t parameters W_N, ...., W_1 but all the minima could be global. Can you please provide some intuitions why the loss function could have saddle points? Also, is not easier to just solve the minimization problem on W?\n\n3) Similar with l2 loss, it seems that the problem needs to be restricted on l2 loss. In understand that it could have in some applications. Do you try to think of different loss for example in binary classification problems? \n \n4) I wonder about the constant \u201cc > 0\u201d in the definition 2 and it would use it to determine the learning rate. Do you think that in order to satisfy the definition 2 for the most cases, constant c would be (arbitrarily) small or may be very close to 0? If so, the convergence rate may be affected in this case. \n \n5) The result of Theorem 2 is nice and seems new in term of probabilistic bound.  I did not see the similar result in the existing literature for neural networks. \n \n6)  It would be nice if the author(s) could provide some experiments to verify the theory. I am also curious to know what performance it could achieve for this kind of networks. \n \nI would love to discuss with the author(s) during the rebuttal period. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}