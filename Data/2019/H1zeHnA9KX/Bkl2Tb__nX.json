{"title": "Well written paper -- One major concern", "review": "Paper Summary -\nThe authors trained RNNs to recognize formal languages defined by random regular expressions, then measured the accuracy of decoders that predict states of the minimal deterministic finite automata (MDFA) from the RNN hidden states. They then perform a greedy search over partitions of the set of MDFA states to find the groups of states which, when merged into a single decoder target, maximize prediction accuracy. For both the MDFA and the merged classes prediction problems, linear decoders perform as well as non-linear decoders.\nClarity - The paper is very clear, both in its prose and maths.\nOriginality - I don't know of any prior work that approaches the relationship between RNNs and automata in quite this way.\nQuality/Significance - I have one major concern about the interpretation of the experiments in this paper.\n\nThe paper seems to express the following logic:\n1 - linear (and non-linear) decoders aren't so good at predicting MDFA states from RNN hidden states\n2 - if we make an \"abstract\" finite automata (FA) by merging states of the MDFA to optimize decoder performance, the linear (and non-linear) decoders are much better at predicting this new, smaller FA's states.\n3 - thus, trained RNNs implement something like an abstract FA to recognize formal languages.\n\nHowever, a more appropriate interpretation of these experiments seems to be:\n1 - (same)\n2 - if we find the output classes the decoder is most often confused between, then merge them into one class, the decoder's performance increases -- trivially. in other words, you just removed the hardest parts of the classification problem, so performance increased. note: performance also increases because there are fewer classes in the merged-state FA prediction problem (e.g., chance accuracy is higher).\n3 - thus, from these experiments it's hard to say much about the relationship between trained RNNs and finite automata.\n\nI see that the \"accuracy\" measurement for the merged-state FA prediction problem, \\rho, is somewhat more complicated than I would have expected; e.g., it takes into account \\delta and f(h_t) as well as f(h_{t+1}). Ultimately, this formulation still asks whether any state in the merged state-set that contains f(h) transitions under the MDFA to the any state in the merged state-set that contains f(h_{t+1}). As a result, as far as I can tell the basic logic of the interpretation I laid out still applies.\n\nPerhaps I've missed something -- I'll look forward to the author response which may alleviate my concern.\n\nPros - very clearly written, understanding trained RNNs is an important topic\nCons - the basic logic of the conclusion may be flawed (will await author response)\n\nMinor -\nThe regular expression in Figure 6 (Top) is for phone numbers instead of emails.\n\"Average linear decoding accuracy as a function of M in the MDFA\" -- I don't think \"M\" was ever defined. From contexts it looks like it's the number of nodes in the MDFA.\n\"Average ratio of coarseness\" -- It would be nice to be explicit about what the \"ratio of coarseness\" is. I'm guessing it's (number of nodes in MDFA)/(number of nodes in abstracted DFA).\nWhat are the integers and percentages inside the circles in Figure 6?\nFigures 4 and 5 are difficult to interpret because the same (or at least very similar) colors are used multiple times.\nI don't see \"a\" (as in a_t in the equations on page 3) defined anywhere. I think it's meant to indicate a symbol in the alphabet \\Sigma. Maybe I missed it.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}