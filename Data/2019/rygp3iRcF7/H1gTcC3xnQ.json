{"title": "A few concerns", "review": "I prefer the idea of using some statistics (such as variances) of multiple items for attention. \nThis direction may lead to better attention units for future works. \n\nI do not fully understand the argument, \"Attention mechanisms are designed to focus on a single item in the entire memory\". \nIn my understanding, the attention formulation has no mathematical bias to focus on a single item. \nI have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. \nDo you have any comments? \n\nCould you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? \nFor example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT.  \nFor the image case, the adjacent area looks like a \"super pixel\". \n\nIt is unfortunate to observe that the gains on BLEU and perplexity are limited. \nSince the authors do not provide any statistical tests, or a confidence interval of the scores, \nI cannot be sure these gains are truly significant. \nFrom my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). \n\nSummary\n+ A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting\n- Claims are not so much convincing for the need of attending multiple adjacent items. \n- Gains in experiments are limited. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}