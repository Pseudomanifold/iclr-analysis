{"title": "Paper is confusing", "review": "The paper proposes an acceleration method that slightly changes the AMSGrad algorithm when successive stochastic gradients point in different directions.  I found the paper confusing to read because the critical points of Algorithm 1 are very unclear. For instance the \\phi function defined by Reddi et al. takes as argument all the past gradients g1...gt (see paper at the bottom of page 3) but is used inside Algorithm 1 with only the current gradient --\\phi_t(g_t)-- or an enigmatic \"max\" of two vectors --\\phi_t(max(g_t,pg_t))--  I have no idea what the actual calculation is supposed to be. The proof of the theorem (equation 6 in the appendix) suggests that this is a componentwise maximum and that the other gradients are still in.  But a componentwise maximum is a surprisingly assymetric construction. What if we reparametrize by changing the sign of one particular weight?  We get a different maximum?\n\nI finally looked into the empirical evaluation. I am not sure that the purported effect cannot be ascribed to other factors such as the choice of stepsize --they do not seem to have been looking for the best stepsize for each algorithm. The MNIST experiments are performed with a bizarre variant of CNN that seems to perform substantially worse than comparable system. They show the test loss but not the test accuracy though.\n\nIn conclusion I remain confused and unconvinced.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}