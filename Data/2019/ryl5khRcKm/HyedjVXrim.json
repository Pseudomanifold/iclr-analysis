{"title": "A CNN that boosts the state of the art on an important image classification task in biology", "review": "This manuscript describes a deep convolutional neural network for\nassigning proteins to subcellular compartments on the basis of\nmicroscopy images.\n\nPositive points:\n\n- This is an important, well-studied problem.\n\n- The results appear to improve significantly on the state of the art.\n\n- The experimental comparison is quite extensive, including\n  reimplementations of four, competing state-of-the-art methods, and\n  lots of details about how the comparisons were carried out.\n\n- The manuscript also includes a human-computer competition, which the\n  computer soundly wins.\n\n- The manuscript is written very clearly.\n\nConcerns:\n\nThere is not much here in the way of new machine learning methods.\nThe authors describe a particular neural network architecture\n(\"GapNet-PL\") and show empirical evidence that it performs well on a\nparticular dataset.  No claims are made about the generalizability of\nthe particular model architecture used here to other datasets or other\ntasks.\n\nA significant concern is one that is common to much of the deep\nlearning literature these days, namely, that the manuscript fails to\nseparate model development from model validation. We are told only\nabout the final model that the authors propose here, with no\ndiscussion of how the model was arrived at.  The concern here is that,\nin all likelihood, the authors had to try various model topologies,\ntraining strategies, etc., before settling on this particular setup.\nIf all of this was done on the same train/validation/test split, then\nthere is a risk of overfitting.\n\nThe dataset used here is not new; it was the basis for a competition\ncarried out previously.  It is therefore somewhat strange that the\nauthors chose to report only the results from their reimplementations\nof competing methods.  There is a risk that the authors'\nreimplementations involve some suboptimal choices, relative to the\nmethods used by the originators of those methods.\n\nAnother concern is the potential circularity of the labels.  At one\npoint, we are told that \"Most importantly, these labels have not been\nderived from the given microscopy images, but from other\nbiotechnologies such as microarrays or from literature.\"  However,\nearlier we are told that the labels come from \"a large battery of\nbiotechnologies and approaches, such as microarrays, confocal\nmicroscopy, knowledge from literature, bioinformatics predictions and\nadditional experimental evidence, such as western blots, or small\ninterfering RNA knockdowns.\"  The concern is that, to the extent that\nthe labels are due to bioinformatics predictions, then we may simply\nbe learning to re-create some other image processing tool.\n\nThe manuscript contains a fair amount of biology jargon (western\nblots, small interfering RNA knockdowns, antibodies, Hoechst staining,\netc.) that will not be understandable to a typical ICLR reader.\n\nAt the end, I think it would be instructive to show some examples\nwhere the human expert and the network disagreed.\n\nMinor:\n\np. 2: \"automatic detection of malaria\" -- from images of what?\n\np. 2: Put a semicolon before \"however\" and a comma after.\n\np. 2: Change \"Linear Discriminant\" to \"linear discriminant.\" Also, remove\nthe abbreviations (SVM and LDA), since they are never used again in\nthis manuscript.\n\np. 5: Delete comma in \"assumption, that.\"\n\np. 8: \"nearly perfect\" -> \"nearly perfectly\"\n\nThe confusion matrices in Figure 5 should not be row normalized --\njust report raw counts.  Also, it would be better to order the classes\nso that confusable ones are nearby in the list.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}