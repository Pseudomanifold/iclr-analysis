{"title": "high-performance method with minor methodological contributions", "review": "The paper proposes a CNN variant tailored for high-resolution\nimmunofluorescence confocal microscopy data.  The authors show\nthat the method outperforms a human expert.\n\nThe proposed method is evaluated on benchmark instances\ndistributed by Cyto Challenge '17, which is presumably the best\ndata source for the target application.  Indeed, the method\nperforms better than several competitors plus a single human\nexpert.\n\nThe paper is well written and easy to follow.  I could not spot any\nmajor technical issues.\n\nThis is an applicative paper targeting a problem that is very\nrelevant in bioinformatics, but it sports little methodological\ninnovation.  On the biological side, the contribution looks\nsignificant.  Why not targeting a bioinformatics venue?\n\n\nDetailed comments:\n\nPapers that stretch multiple fields are always hard to review.  On\none hand, having contributions that cross different fields is a\nhigh-risk (but potentially highly rewarding) route, and I applaud\nthe authors for taking the risk.  On the other hand, there's the risk\nof having unbalanced contributions.\n\nI think that the contribution here is mostly on the bioinformatics\nside, not on the deep learning side.  Indeed, the method boils\ndown to a variant of CNNs.  I am skeptical that this is enough to\nspark useful discussion with practitioners of deep learning\n(although I could be wrong?).\n\nFinally, I am always skeptical of \"human-level\" performance claims.\nThese are strong claims that are also hard to substantiate.  I don't\nthink that comparing to a *single* expert is quite enough.  The fact\nthat \"the human expert stated that he would be capable to localize\nproteins with the provided data\" doesn't sound quite enough.  I\nagree that the user study could be biased (and that \"It would be\na tremendous effort to find a completely fair experimental\nsetting\"), but, if this is the case, the argument that the method\nreaches human-level performance is brittle.\n\n\nOther remarks and questions:\n\n- Why wasn't the dataset of Liimatainen et al. used for the\ncomparison?\n\n- The authors say that \"due to memory restrictions, the smallest\nvariant of DenseNet was used\".  How much of an impact could have\nthis had on performance?\n\n- \"One random crop per training sample is extracted in every epoch\".\nDoesn't this potentially introduce labeling errors?  Did you observe\nthis to occur in practice?\n\n- The authors claim that the method is close to perfect in terms\nof AUC.  In decision-making applications, the AUC is a very\nindirect measure of performance, because it is independent of\nany decision threshold.  In other words, the AUC does not measure\nthe yes/no decisions suggested by the method.  Why is the AUC\nimportant in the biological application at hand?  Why is it important\nto the users (biologists, I suppose) of the system?\n\nIn particular, \"our method performs nearly perfect, achieving an\naverage AUC of 98% and an F1 score of 78%\" seems inconsistent\nto me---the F1 is indeed \"only\" 78%.\n\n- I would appreciate if there was a thorough discussion of the\nfailure mode of the expert.  What kind of errors did he/she\nmake?  How are these cases handled by the model?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}