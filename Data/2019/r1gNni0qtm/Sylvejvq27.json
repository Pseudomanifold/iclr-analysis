{"title": "Good paper that would benefit from a meaningful and concrete toy example.", "review": "This paper would benefit from a meaningful and concrete toy example. \n\nThe toy example from section 3.1 eq(3) amounts to stating that the Kronecker product  of a set of eigenparts is equal to the PCA eigenpixels for  a set of complete images, or more generally that the kronecker product of a set of  image-part feature tensors is equal to the complete image-feature tensor  (assuming no pixel overlap).  Sure.  What does that buy?   Hierarchical Tucker (including Tensor Train) does indeed compute the standard Tucker mode representation of a tensor in an efficient manner using a set of sequential SVDs rather than using a single SVD per mode.   Is there anything else?  Depending on how the data is organized into a data tensor, the object representation and its properties can differ dramatically.  Section 3.1 needs further clarification.\n\nQuestions:\n1. What is data tensor organization and what tensor decomposition model are you using? Tucker but implemented as a TT? \nWhat is the resulting object representation?\n2. In the context of the toy example, please give a concrete mapping between your tensor decomposition (object representation)  and RNN.\n\nThe rest of the paper is a lot of mathematical manipulation which looks correct.\n\n\nPlease reference the first papers to employ tensor decompositions for imaging.\n\nM. A. O. Vasilescu, D. Terzopoulos, \"Multilinear Analysis of Image Ensembles: TensorFaces,\"  Proc. 7th European Conference on Computer Vision (ECCV'02), Copenhagen, Denmark, May, 2002, in Computer Vision -- ECCV 2002, Lecture Notes in Computer Science, Vol. 2350, A. Heyden et al. (Eds.), Springer-Verlag, Berlin, 2002, 447-460. \n\nM.A.O. Vasilescu, \"Multilinear Projection for Face Recognition via Canonical Decomposition \",  In Proc. Face and Gesture Conf. (FG'11), 476-483.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}