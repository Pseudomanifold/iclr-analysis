{"title": "The theoretical and experimental aspects of the paper are weak", "review": "The paper proposes to learn representations by using Krein inner products which are generalizations of standard inner products in Hilbert spaces. \nA Krein inner product can be formulated as the difference between two inner products, each in a Hilbert space.\nAlthough the paper mentions three main contributions, the contributions of the papers are mainly:\n- 1. using Krein distances instead of Hilbert distances to compare examples\n- 2. better empirical performance on some datasets\n\nThe paper is clear in general, and maybe provides too many details on standard linear algebra (about 3 pages to explain the eigendecomposition of symmetric matrices). \nMy main concern is about novelty. The proposed method has already been published in the literature but never been called \"representation in a Krein space\" (see Novelty point).\n\n1. Motivation:\n\nIt is already known in the literature that Hilbert spaces are limited for certain tasks, non-Euclidean distances have then been used to compare examples.\nFor instance, based on the work of [A], some works have proposed to learn deep representations exploiting hyperbolic distances to represent data with hierarchical structure [B].\nThe motivation of the superiority of Krein representations is not clear in the paper. In what specific contexts is it better to use Krein spaces?\n\n2. Novelty:\n\nThe authors take the example of Mahalanobis-like distance metric learning where a distance function parameterized by a symmetric matrix usually constrained to be positive semi-definite (PSD) is learned. The paper argues that constraining the matrix to be PSD limits the expressiveness of the model as the negative spectrum is also meaningful. \nIf the PSD constraint is removed, the problem simply corresponds to learning a symmetric matrix. Indeed, by definition, symmetric matrices have real eigenvalues which can be either non-negative or negative. The PSD constraint enforces the eigenvalues to be non-negative. If the PSD constraint is removed, then the symmetric matrix can be indefinite, which corresponds to the proposed representation in Krein space.\n\nSome papers have already proposed to remove the PSD constraint (e.g. [C], Section 2.3) when learning a distance parameterized by a symmetric matrix, which corresponds to the proposed model. However, they did not sell the relaxation of the PSD constraint as a contribution.\n\n3. Experiments:\n\nThe experimental section is weak as it provides a simple experiment on a toy dataset, and quantitative results on 4 datasets only for one optimization problem and one specific network architecture. \nSince the theoretical aspect of the paper is weak, I would expect more comparisons by replacing standard deep metric learning formulations (that use the squared Euclidean distance) with the proposed Krein distance. For instance, the submission cites (Oh Song et al.), and (Schroff et al.) as deep metric learning approaches. How would the proposed Krein distance perform in the contexts considered in those references?\n\nDemonstrating that the Krein distance also outperforms the squared Euclidean distance in that case would strengthen the experimental aspect of the paper.\n\n\nIn conclusion, the theoretical and experimental aspects of the paper are weak.\n\n\n[A] Gromov, Hyperbolic groups, 1987\n[B] Ganea et al, Hyperbolic Neural Networks, NIPS 2018\n[C] Guillaumin et al., Is that you? Metric learning approaches for face identification, ICCV 2009", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}