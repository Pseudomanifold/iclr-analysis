{"title": "Embeddings in Krein space - review", "review": "LEARNING DEEP EMBEDDINGS IN KREIN SPACES\n\nQuality: average\nOriginality: original\nSignificance: relevant for ICLR\nPros: - interesting idea -- see detailed comments\nCons: limited experiments -- see detailed comments\n\nThe underlying idea is interesting and probably novel. Large parts\nof the paper are a bit straight forward although sometimes a complicated\ndescribed and not always with the reference to existing work (but I am\nwilling to believe that the authors were not aware of it). Although\nthe paper is conceptually a nice thing the experiments are very limited\nand not yet convincing. In particular it remains rather unclear if the\neffort in learning and using an indefinite metric for represent the data\nis valuable.\n\n- your approach makes essential use of label information - I would like to see this reflected in the title - e.g. by adding 'supervised'\n- I may suggest to add a recent review about indefinite learning, Neural Computation by P. Tino and F.-M. Schleif\n  some parts of your intro (and references) could be a bit summarized in this way as well\n- 'The negative spectral of an inner product is usually attributed to observation noise, though such a claim has never been fully studied, nor proved.'\n  - well and as indicated in the review above and by discussed work from Pekalska / Duin and others it is actually multiple times true that the negative part of the inner\n  spectrum contains relevant information. There are e.g. datasets (protein data - see Tino) get very bad classification models if the negativ contributions are\n  removed - accordingly they are not just noise but contain valuable information to the problem\n- it would be good to add a few sentences at the beginning of the paper to (super brief) review the core idea/concept of siamese networks\n- the english is sometimes a bit bulky and it would be good to check it by a native speaker\n  e.g ' proposal do not require' -- proposal does not require\n  ' unlike the our'\n  - follow by a spell-checker 'embeddng', 'interms', 'Simalry'\n- 'followed by removing it by flipping' - with flipping the contribution is not 'removed' but mapped to the positive part of the spectrum\n- how does your approach compare to a classical embedding of the indefinite kernel matrix into a pseudo euclidean space (see e.g. Tino or Pekalska)?\n  -- there you end up also with a vectorial representation of the similarities - with some issues like complexity of the embedding or valid generalization to new test points ...\n- I am a bit surprised that you completely skipped the work by Pekalska and Duin -> I suggest to have a look into their work and link a bit to it \n  (there is also a book by Pekalska around this topic)\n- in Def 1 - I would call it a squared Mahalanobis distance because it is missing the square root (cmp squared Euclidean distance, where M is an eye matrix)\n- 'feature vectors from a non-linear deep model' - obtained in which way?\n- just a hint: you may find the work of Michael Biehl on Matrix Learning in prototype networks interesting - your matrix L is called omega or lambda there \n- if I am not misleading there was a paper at AAAI a few year ago about negative locality sensitive hashing where such an embedding is discussed (- not in the\n  same way as you do - but related)\n- in some way Eq 4 prunes down to a eigenvalue decomposition of your matrix M - in metric learning M could be e.g. the covariance matrix of the data and decomposing\n  it into E * V * E' - will give you exactly this\n- you allow for quite some flexibility in learning your projection matrix in Eq 5 - how do you make sure that this is not oversimplifying the problem? - E.g. if \n  I place this into a supervised learning context it could easily happen that the mapping fits in perfect alignment to the training data but may be much less\n  effective (or even useless) if it is applied on test data\n- w.r.t. Remark 1: 'Furthermore, we consider that a negative distance always indi- cates better similarity than a positive or a \u201c0\u201d distance.' -- if this is reflected\n  in your model/optimization I would expect that the learned matrix W/Lambda will always aim on negative distances? - shouldnt it?\n- 'Here, we show that our distance ...' - this is basically equivalent what is shown already in the book by Pekalska ( I am not sure if it is original proposed/shown by\n  them but it least it is there - maybe from 2001 or so)\n- 'In other words, the numbers p + and p \u2212 are entirely decided by the data' - well yes, but I would slightly reformulate this. The amount of negative spectrum elements (p-)\n  and positive spectrum elements (p-) originates within the metric learning optimization and is not pre-specified by the user.\n- around Eq 8 a multivariate Gauss (exp) is not shown there ...\n- \\Sigma_{s,d} - is simply the covariance matrix of the pairwise difference vectors z from all point of the positive class (assuming that the data in (z_i - z_j) are centered to the mean \n  (otherwise this is not a valid covariance matrix formulation in the statistical sense) - so you may expect that your original data X are centered \n\t-- I think you could try to make this a bit more clear \n- using this approach you encoded already in the metric initalization the discrimination problem in an explicit way\n- I feel a bit uncomfortable with a related work section in sec 5 it really doesnt fit in the flow of your article there\n  - I think it should come earlier (after the intro) - it also repeats currently parts of the intro\n  and the way to sec 5 - just reorganize\n- '(Fine & Scheinberg, 2001)' - why this reference to kernel approaches - why not a text book e.g. by N Cristianini\n- typos ref 'Laub & M\u00c3\u017eller' \n- please provide a full crossvalidation e.g. 10 repeated runs and report mean/std-dev. a simple split in 1 train/test set could give just a lucky shot\n- the improvements shown in Tab 1 are very minor ! - because you had to do some processing in the metric learning/ parameter decisions it is hard to judge\n  whether the tiny benefits are really due to the non-metric behavious. Could you please also provide - data - (not only plots) how far the metric contains now negative spectrum contributions?\n  (a measure is give in Tino et al.) - I would give a link to Figure 2 here. Have you done some normalization in Fig 2 - I am very much surprised about the\n  scaling?\n- in your position I would look for a few more challenging datasets such that the claimed benefit of your method is more clear to observe\n- 6.1.1 MDS is a very old and in my view outdated method which should not be used any longer - there are many new more effective visualization methods which also work\n  for dissimilarities as input (maybe with the same shift preprocessing) - see e.g. t-SNE. But, there would also be an approach by Laurens v. d. Maaten about \n  non-metric t-SNE or so which you could use straight away. (for some comparison on embedding methods see work by J.A. Lee & Verleysen or A. Gisbrecht, Wiley  \n  Fig 1 does not say much and is not discussed a lot\n- Some references are incomplete (Hal Daum\u00e9 III.); Gregory Koch.\n- there is a broken reference in 8.1.1 -- Table ?? ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}