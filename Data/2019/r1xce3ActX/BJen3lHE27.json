{"title": "We do not think this paper appropriate for publication in ICLR for the following reasons.", "review": "After reading the paper, we are not sure about the main contributions of this paper. This paper is not on deep embeddings, nor on Krein spaces (as explained below). It seems that the contribution is in Section 2.1, on the explicit rewriting the distance in terms of the eigenvalues. This is from simple linear algebra, with very simple diagonalization of the matrix in the conventional Mahalanobis distance.\n\nIn the abstract, introduction and conclusion, the authors emphasize that this work is on the modification to the Siamese network architecture. However, it turns out that this paper is not on these types of networks, and the attempts to provide connections to these networks are too artificial. For example, nothing is said about these networks in the experiments.\n\nIt is not clear what is \u201cdeep\u201d in this work. The authors did put the word \u201cdeep\u201d in expressions like \u201cdeep Krein embeddings\u201d, \u201cdeep embeddings\u201d, \u201cdeep model\u201d, \u201cfull deep\u201d, \u2026 However, nothing is deep in the used model or embedding. The definition of the distance is not based on a deep formulation.\n\nSection 2 is based on the concept that \u201ca negative distance with a high absolute value will reflect a high similarity between the compared instances\u201d, while Section 4 relies on \u201cthe prior assumption that the negative distances are equally important as positive distances\u201d. However, it is not clear how these two concepts are investigated/applied/implemented in the corresponding sections. \n\nSection 4 ends up with what could be very important in general, and of great importance to understand the use of these distances: \u201cHere,the inputs xi,xj \\in R^n are output features from the deep feature extractor\u201d.\n\nOn connections with the Krein space, the authors rewrite in (7) the distance as a difference of of two inner products. Based on this observation, they argue that this is a Krein decomposition. However, the Krein decomposition is on decomposition the inner product as a difference of two inner products. Not the distance, nor the squared distance. Otherwise, any distance can be written as a difference of two inner product.\n\nAn important part of this paper is on the initialization, which is also the case in the experiments (\u201cwe pre-train\u2026\u201d, \u201cFor pre-training\u2026\u201d). However, the most important part needs to be on the learning algorithm, and the so-called \u201cdata-driven technique\u201d; however, almost nothing is given on this.\n\nThe initialization is too simple, relying on some parametric assumptions. Moreover, the initialization uses a simple PCA projection. This may deeply affect the spectral distribution. The authors need to examine in detail the influence of this initialization on the negative embedding space.\n\nIn Section 6.1 \u201cVisualization\u201d, we are a bit disappointed how the authors operated the visualization, by subtracting the least distance. By the way, one should not say \u201clargest negative value\u201d, but the absolute one. In general, we do not understand why the proposed method gives \u201cthe best discrimination\u201d, since we are not using the labels when computing the distance.\n\nThere are many typos:\n- In most of the paper (and mostly in the introduction), the authors need to use \\citep not \\cite.\n- \u201cour discussion often requireS\u201d\n-\u201cMahalanobis distaNce\u201d\n- L should be bold everywhere because it is a matrix\n- \u201cembeddng\u201d\n- \u201cbetween between two vectors\u201d\n- \u201cinterms\u201d\n- \u201cSimalry\u201d\n- \u201cScalling\u201d -> scaling\n- correct name \u201cM\u00fcller\u201d", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}