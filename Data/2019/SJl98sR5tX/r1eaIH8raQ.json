{"title": "Review", "review": "1) Summary\nThis paper proposes a method for learning an agent by interacting and probing an expert agents behavior. This method is composed of a policy that learns to imitate an expert\u2019s action, and a policy that challenges the expert in order to get it to take multiple possible routes to solve a task. The two policies share a \u201cbehavior tracker\u201d that models the expert\u2019s behavior, and communicates it to both policies being learned. The probing policy is optimized using a curiosity-driven reward in order to get the expert take trajectories the probing policy has not seen before. In experiments, the authors perform experiments to show how the learned agent can generalize to unseen configurations in the corresponding environments in which the agents were trained, and also use the proposed technique in a sorting task in which the method generalizes to longer arrays to be sorted.\n\n\n2) Pros:\n+ Neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).\n+ Cool experiments for applicability.\n+ Well written paper and easy to understand.\n\n3 Comments:\n- Equation 1 typo?:\nTo my understanding, in curiosity driven exploration, the exploration is driven based on how well the next state can be predicted by the agent. In equation 1, different time steps are being compared, m^t and m^{t-1}, but the comparison should be between the predicted time step t and real time step t. Can the authors clarify why different time steps are compared in the equation?\n\n- Baseline missing: Random actions from expert\nA simple baseline to compare against could be to simply force the expert to take a few random actions during its trajectory and let the imitator learn from these. Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.\n\n- Baseline missing: Simple RNN policies that communicate hidden states.\nAnother baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states. While optimizing the curiosity reward the hidden states could be used as well. If successful, this baseline can show that we actually need to model the \u201cbehavior\u201d with a separate network.\n\n- Ablation study for the importance of fusion:\nThe authors have a \u201cfusion\u201d layer within the imitator and probing policies. An ablation study showing that this layer is actually necessary is missing from the paper.\n\n- Generalizability argument\nThe authors claim that they show a single starting configuration for the agents during training, and different starting configurations during testing. While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment. It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert. A more drastic change of the environment could make for a stronger argument. \n\n\n4) Conclusion:\nOverall, I like the idea of having a policy that tries to figure out the general behavior of a demonstrator by probing it. Having said that, I feel this paper needs to improve in the aspects mentioned above. If the authors present more convincing evidence that successfully address the comments above, I am willing to increase my score.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}