{"title": "an abstract analysis that does not aim to derive any conclusions", "review": "This paper performs an analysis of the length scale of activations for deep fully-connected neural networks with respect to the activation function in neural networks. The authors show that for a very large class of activation functions, the length process converges in probability.\n\nI am listing my main concerns about this manuscript below.\n\n1. The paper is poorly motivated and does not make an attempt to relate its results to observations in practice or the design of new techniques. It is an abstract analysis of the probability distribution of the activations.\n\n2. Theorem 2, which is the main theoretical contribution of the paper, hinges on fixing the inputs of the neural network with weights sampled randomly from a Gaussian distribution. It is difficult to connect this with practice. This is not unreasonable and indeed common in mean-field analyses. However such analyses go further in their implications, e.g., https://arxiv.org/abs/1606.05340, https://arxiv.org/abs/1806.05393 etc. This is my main concern about the paper, its lack of concrete implications despite the simplifying assumptions.\n\n3. It would be very interesting if the analysis in this manuscript informs new activation functions or new initialization methods for training deep networks.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}