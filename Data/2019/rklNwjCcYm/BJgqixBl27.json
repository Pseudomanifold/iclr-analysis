{"title": "This paper presents a sequence labeling model that incorporates Bi-LSTM-CNN with multi-head self-attention module. The proposed architecture models cross-interaction between past and future information. Detailed analysis on how the attentions discover the patterns of chunking and entity typing is conducted that provides more understanding of what the model learns.", "review": "In general, the paper proposes a deep architecture combining bi-LSTM, CNN and self-attentions to better model the interactions among the contexts for sequence labeling. The model is then evaluated on a sequence-labeling problem (NER) with one dataset. Although effective, the method seems to be a bit trivial through a combination. Evaluating on only one NER dataset is also not too convincing for me. Detailed comments are the following:\n\nPros:\n1. The paper is clearly written with a good organization to fully explain their model. The motivation is clear and comparison with existing work is clearly illustrated.\n2. It is interesting to decouple the problem to entity chunking and entity typing for detailed evaluation, although I do not see any decoupling methodology in terms of the model itself. It would be better to also make the decoupling idea integrated into sequence-labeling models, in this way the model could be more different and novel compared to existing works.\n3. The experimental section provides detailed analysis of the proposed model on the performance of each component as well as some qualitative analysis on what different attention heads learn.\n\nCons:\n1. The methodology itself is a bit trivial to me, since the bi-LSTM-CNN and self-attention mechanism were already proposed previously. It seems to be an improved combination, as also stated in Related Work (\"The subtle difference is that they...\"). It also seems over-stated in the abstract about the functionality on providing insightful understanding of attentions and decoupling NER into two subproblems. To my understanding, the understanding is only qualitative by observing a few examples, and the decoupling does not refer to the method, but the evaluation.\n2. More existing works should be added in Related Work including sequence-labeling models as well as attention models.\n3. It's still unclear if one wants to re-implement the model. Section 5.2 still lacks some important parameters, e.g., learning rate, batch-size, training method, how to parallel etc. More explanations on baseline models should be added to better understand the comparisons. \n4. Section 6.2 is confusing. Does the cross construction used in your experiments? Why do you put this part at the end of the paper. If section 6.2 provides better construction, why not making it integrated in the main part of the paper? Moreover, the authors claim the model's ability to exploit cross-interactions among input sequence, but I do not really get this point. The paper should illustrate this characteristics into more details for the readers to understand. \n\nQuestions:\n1. Since your model is quite similar to Chiu & Nichols (2016), can you illustrate what's the architecture of their model and what's the difference between their work and Bi-LSTM-CNN in Table 2?\n2. Can you explain how do you parallel the computation for self-attentions?\n3. The statistics in Table 3 is quite unexpected to me. Although you mentioned C^{all} is much better than H for classifying \"I\", it did much worse for all the other classes (except \"O\"). In this case, I suppose the attention is actually worse than simply computing LSTM hidden representations (H). Can you explain why? The last column for NativeH actually shows little degradation compared to the full model, may I know how's that happen, and what's the difference between NativeH and H? Section 5.4.1 mentions that Table 3 shows \"per-token recalls\", can you explain more what does this mean? How's the f1 scores?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}