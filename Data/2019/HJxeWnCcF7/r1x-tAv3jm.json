{"title": "Interesting ideas to explore towards understanding the geometry of data sets", "review": "The paper proposes a dimensionality reduction method that embeds data into a product manifold of spherical, Euclidean, and hyperbolic manifolds. The proposed algorithm is based on matching the geodesic distances on the product manifold to graph distances. I find the proposed method quite interesting and think that it might be promising in data analysis problems. Here are a few issues that would be good to clarify:\n\n- Could you please formally define K in page 3?\n\n- I find the estimation of the signature very interesting. However, I am confused about how the curvature calculation process is (or can be) integrated into the embedding method proposed in Algorithm 1. How exactly does the sectional curvature estimation find use in the current results? Is the \u201cBest model\u201d reported in Table 2 determined via the sectional curvature estimation method? If yes, it would be good to see also the Davg and mAP figures of the best model in Table 2 for comparison.\n\n- I think it would also be good to compare the results in Table 2 to some standard dimensionality reduction algorithms like ISOMAP, for instance in terms of Davg. Does the proposed approach bring advantage over such algorithms that try to match the distances in the learnt domain with the geodesic distances in the original graph?\n\n- As a general comment, my feeling about this paper is that the link between the different contributions does not stand out so clearly. In particular, how are the embedding algorithm in Section 3.1, the signature estimation algorithm in Section 3.2, and the Karcher mean discussed in Section 3.3 related? Can all these ideas find use in an overall representation learning framework? \n\n- In the experimental results in page 7, it is argued that the product space does not perform worse than the optimal single constant curvature spaces. The figures in the experimental results seem to support this. However, I am wondering whether the complexity of learning the product space should also play a role in deciding in what kind of space the data should be embedded in. In particular, in a setting with limited availability of data samples, I guess the sample error might get too high if one tries to learn a very high dimensional product space.  \n\n\nTypos: \n\nPage 3: Note the \u201canalogy\u201d to Euclidean products\nPage 7 and Table 1: I guess \u201cring of cycles\u201d should have been \u201cring of trees\u201d instead\nPage 13: Ganea et al formulates \u201cbasic basic\u201d machine learning tools \u2026", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}