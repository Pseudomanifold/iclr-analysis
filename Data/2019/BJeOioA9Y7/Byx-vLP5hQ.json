{"title": "interesting approach in combining multiple trained models for transfer", "review": "This paper presents a method for distilling multiple teacher networks into a student, by linearly combining feature representations from all networks at multiple intermediate layers, and gradually forcing the student to \"take over\" the learned combination.  Networks to be used as teachers are first pretrained on various initial tasks.  A student network is then trained on a target task (possibly different from any teacher task), by combining corresponding hidden layers from each teacher using learned linear remappings and weighted combinations.  Learning this combination allows the system to find appropriate teachers for the target task; eventually, a penalty on the combination weights forces all weight onto the student network, resulting in the distillation.\n\nApplications to both reinforcement learning (atari game) and supervised image classification (cifar, svhn) are evaluated.  The reinforcement learning application is particularly fitting, since combining tasks together is less straightforward in this domain.\n\nI wonder whether any experiments were performed where the layers correspondence between teacher models was less clear --- say, using teachers with different architectures.  Figure 1(a) (different teacher archs) as well as the text (\"candidate set\" on p.4) indicate this is possible, but experiment details describe combinations of same-architecture teachers only.\n\nIn addition, I would have liked to see some further exploration of the KL term and use of \"theta_old\".  This seems potentially important, and also has ties to self-ensembling through teachers with exponential weight averaging.  Could an average network also be used here?  And how important is this term in linking student to teachers as the weights change?\n\nOverall I find this a very interesting approach.  Rather than training a large joint model on multiple tasks simultaneously as a transfer initialization, this approach uses models already fully trained for different tasks.  This results in a potentially advantageous trade-off:  One no longer needs to carefully calibrate the different tasks and common task components in a joint model, but at the cost of requiring inference through multiple teachers when training the student.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}