{"title": "clear simple idea and good results", "review": "The paper is well written and easy to read. Exploration is one of the fundamental problems in RL, and the idea of using two agents for better exploration is interesting and novel. However, an explanation of the intuition behind the method would be useful. The experimental results show that the method works well in complex tasks. Since states are compared to each other in L2 distance, the method might not generalize to other domains where L2 distance is not a good distance metric.\n\nPros:\n- well written\n- a simple and novel idea tackling a hard problem\n- good results on hard tasks\n\nCons: \n- an explanation of why the method should work is missing\n- plot text is too small (what is the unit of X-axis?)\n\nQuestions:\n- what is the intuition behind the method?\n- during training, randomly sampled two states are compared. why it is a good idea? how the replay buffer size will affect it?\n- since it is a two-player game, is there anything you can say about its Nash equilibrium? \n- why A is better than B at the task?\n- when comparing states, are whole raw observations (including velocity etc.) used?\n- section 4.2 doesn't seem to be that relevant or helpful. is it really necessary? \n- fig 4 is missing CER alone results? why is that? it doesn't work by itself on those tasks? ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}