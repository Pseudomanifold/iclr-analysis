{"title": "review", "review": "This paper gives a new algorithm for learning a two layer neural network which involves a single convolutional filter and a weight vector for different locations. The algorithm works on any symmetric input data. The techniques in this paper combines two previous approaches: 1. the algorithm Convotron for learning a single convolutional filter (while the second layer has fixed weight) on any symmetric input distributions; 2. non-convex optimization for low rank matrix factorization.\n\nThe main observation in the paper is that if the overlap in the convolutions is not large (in the sense that each location of the convolution has at least one input coordinate that is not used in any other locations), then the weight that corresponds to the non-overlapping part and the weights in the second layer can be computed by a matrix factorization step (the paper gives a way to estimate a gradient that is similar to the gradient for a linear neural network, and then the problem is very similar to a rank-1 matrix factorization). After this step, we know the second layer and the algorithm can generalize the previous Convotron algorithm to learn the full convolutional filter.\n\nThis is an interesting observation that allows the algorithm to learn a two-layer neural network. On the other hand this two layer neural network is still a bit limited as there is still only one convolutional filter, and in particular there is only one local and global optimum (up to scaling the two layers). The observation also limited how much the patches can overlap which was not a problem in the original convotron algorithm. \n\nOverall I feel the paper is interesting but a bit incremental.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}