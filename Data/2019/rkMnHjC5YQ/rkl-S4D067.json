{"title": "Interesting theoretical study of One-hidden-layer Conv Nets", "review": "This paper studies the theoretical learning of one-hidden-layer convolutional neural nets. The main result is a learning algorithm and provable guarantees using the algorithm.  This result extends previous analysis to handle the learning of the output layer weights, and holds for symmetric input distributions with identity covariance matrix.\n\nAt a high level, the proof works by using the non-overlapping part of the filter to reduce the problem to matrix factorization. \nThe reduced problem corresponds to learning a rank-one matrix, from which one can learn the output layer weight vector approximately. Given the output weight vector, then the hidden layer weight is learnt using the Convotron algorithm from previous analysis. I think that the technical contribution is interesting.\n\nWeakness: Given the existing work (Goel et al. 2018), I am concerned that the current work is a bit incremental. Secondly, it is unclear if the technical insight has any applications or not. How does the proposed algorithm work on real world data? Even some simple comparisons to other algorithms on a few datasets would provide insight.\n\nQuestion: Where does Assumption 3.2 arise in the proof? Is it necessary (for the proof)?\n\nOther issues: A few typos you may need to fix (e.g. the S notation in Thm 3.1, first sentence in Sec 4.3).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}