{"title": "Very confusingly written with unclear experiments.", "review": "Summary: \n\nThis paper proposes a method to get feedback from humans in an autonomous vehicle (AV). Labels are collected such that the human actually moves a steering wheel and depending on the steering wheel angle disagreement with the direction the vehicle is actually moving a feedback value is collected which is used to weight the scalar loss function used to learn from these demonstrations. \n\nExperiments on a simple driving simulator is presented. \n\nComments: \n\nI think this paper is attempting to address an important problem in imitation learning that is encountered quite often in DAgger, AggreVate and variants where the expert feedback is provided on the state distribution induced by the learnt policy via a mixture policy. In DAgger (where the corrections are one-step as opposed to AggreVate where the expert takes over and shows the full demonstration to get Q(s,a)) it is difficult to actually provide good feedback especially when the expert demonstrations are not getting executed on the vehicle and hence hard for humans to ascertain what would be the actual effect of the actions they are recommending. In fact there is always a tendency to overcorrect which leads to instability in DAgger iterations. \n\nThe paper proposes using a modified feedback algorithm on page 6 whose magnitude and sign is based on how much the correction signal is in agreement or disagreement with the current policy being executed on the vehicle. \n\nUnfortunately this paper is very confusingly written at the moment. I had to take multiple passes and still can't figure out many claims and discussions: \n\n- \"To the best of our knowledge, no research so far has focused on using any kind of human feedback in the context of AV control with learning from demonstration\" - This is not true. See: \n\n\"Learning Monocular Reactive UAV Control in Cluttered Natural Environments, Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert\" who used DAgger for autonomous driving of a drone with human pilot feedback. \n\n- Lots of terms are introduced without definition or forward references. Example: \\theta and \\hat{\\theta} are provided early-on are refered to on page 3 in the middle of the page but only defined at the end of the page in 3.1. \n\n- Lots of confusing statements have been made without clear discussion like \"...we could also view our problem as a contextual bandit, since the feedback for every action falls in the same range...\" This was a baffling statement since contextual bandit is a one-step RL problem where there is no credit assignment problem unlike sequential decision-making settings as being dealt with in this paper. Perhaps something deeper was meant but it was not clear at all from text. \n\n- The paper is strewn with typos, is really verbose and seems to be written in a rush. For example, \"Since we are off-policy the neural network cannot not influence the probability of seeing an example again, and this leads can lead to problems.\"\n\n- The experiments are very simple and it is not clear whether the images in figure 2 are the actual camera images used (which would be weird since they are from an overhead view which is not what human safety drivers would actually see) or hand-drawn illustrations.\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}