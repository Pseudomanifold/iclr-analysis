{"title": "Review", "review": "# Summary\nThis paper proposes a generative model of visual observations in RL that is capable of generating observations of interests. The idea is to first train a VAE which reconstructs a given observation with a regularizer that encourages the generated images from which the policy can reconstruct its output. This regularizer makes the VAE focus on reconstructing parts of images that are important for the given policy. After training the VAE, this paper proposes to optimize the latent variable of the VAE with respect to a target function (e.g., high/low values, specific actions) to generate states of interest. The experimental result shows that the proposed model can generate realistic images in Atari games that are more interpretable to the agent compared to a vanilla VAE. It is also shown that it is possible to generate states of interest such as high/ow-value states by plugging in different target functions, which allows users to analyze RL agents easily. \n\n[Pros]\n- An interesting attempt to learn a controllable generative model of states to analyze deep RL agents. \n\n[Cons]\n- (Major) The experimental results are not comprehensive.\n- (Minor) The usefulness of the proposed generative model is not clear.\n- (Minor) Inconsistent equations/notations\n\n# Novelty and Significance\n- The proposed regularizer for VAE and gradient descent approach for generating particular types of states are novel and interesting.\n- Although this type of generative model can be useful for analyzing learned RL agents more efficiently, it is unclear how this could be useful for improving AI safety as claimed by this paper. In Section 4.6, for example, this paper shows \"distracted pedestrians\" example and claims that their generative model is useful to verify whether the policy can handle such an adversarial example or not. However, users still need to come up with such a scenario, manually construct such an adversarial example, and test it using the policy. I do not see any role of the proposed generative model here. A more convincing example or experiment would be necessary to support the main contribution of this work. \n\n# Quality and Experiments\n- This paper shows a few generated samples for qualitative evaluation. However, it is unclear whether such samples are cherry-picked or randomly-picked. It would be important to present many \"random\" samples from the generative model to evaluate the quality of samples. \n- Related to the above comment, it would be also important to show interpolation / extrapolation in the latent space to show that the model has learned a reasonable manifold instead of overfitting to the training data.\n- Instead of presenting Table 2 to show generalization performance, it would be more informative to show examples of generative samples + nearest neighbors from the training data, which is a common practice to verify overfitting in generative models.\n- Figure 5 (+ discussion on why activation maximization does not work) looks relatively less important, which could be condensed or moved to the appendix.\n- It would be interesting to see how the generated samples change as the number of gradient-descent steps for the target function increases.\n\n# Clarity and Presentation\n- This paper does not fully describe how the energy function is optimized (initial latent variable, the number of gradient-descent steps).\n- There are some inconsistent and undefined notations, which need to be fixed.\n1) KL divergence should be defined over two probability distributions. However, KL-term in this paper is defined over the output of the network (Equation 1).\n2) What is the form of the generator g? It is introduced as \"g(\\mu, \\sigma, z)\" but described as g(x, z) in Equation 4.\n3) In VAE, we normally define the latent variable z as random variable from N(\\mu, \\sigma^2). But, it seems like this paper uses z as unit Gaussian, which is a bit confusing.\n4) I_n in Equation 1 is not defined. \n5) In page 3, A(s)_a is not a well-defined notation?\n6) Is \\pi(s) a vector or scalar?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}