{"title": "Good experimental results but lacking rigour", "review": "This works propose a new approach to learn to sample (or generate) the parameters of a deep neural networks to solve a task. They propose a new architecture inspired by hyper networks and adversarial auto-encoders, where the parameters of the networks are generated from a low dimensional latent space. By using an ensemble of networks sampled with their approach they're able to get state of the art results on uncertainty estimation.\n\nThe notations are confusing and the paper contains several mistakes. In particular:\n- P_z is used to represent different distributions. It sometimes refers to the distribution of the latent variables and sometimes to the prior over the weight embeddings. Different notation should be used to represent different quantity.\n- D_z sometimes refers to the regularization term or to the discriminator.\n- Eq 2. I believe there is a bug in the equation, the expectation is over Q(z) but it should be P_z (distribution of the latent variable z), otherwise it doesn't make much sense.\n- The equation for the cross entropy is wrong. If y_i are the true labels and F(x_i, theta) is the prediction then it should be y_i*log(F(x_i, theta)).\n- It's not clear if the loss of the discriminator should be maximized for the parameters of the discriminator and minimized with respect to the parameters of the encoder. Furthermore it would be interesting to study what is the impact of this particular choice of loss for the discriminator. In particular I invite the author to compare the loss proposed to the loss in [1].\nFixing these, would make the paper much easier to understand.\n\nThe authors motivates their approach by drawing a link with wasserstein (WAE) and adversarial auto-encoders. While this could be interesting I think this link should be made more formal. \nIndeed, the WAE is derived from the wasserstein distance between the true data distribution and the distribution of the model. However it's not clear if the approach proposed can still be derived from such a principle. I would invite the author to make the link between wasserstein distance minimization and their approach more explicit.\n\nTo my knowledge the method proposed is novel, however using implicit posterior to learn the weights is not novel and several other works have looked at it. In particular I think [1,2] should be discussed in the related work. \nThe difference with traditional bayesian approach such as variational inference should also be discussed, since the approach is really close to approximating the posterior with an implicit distribution and computing the KL term using a GAN (like in [3,4]).\n\nI think one interesting novelty that needs to be emphasized is that the model has both: parameters that are point estimates (the parameters of the generators) and parameters that are sampled from a posterior distribution (the weight embeddings). \n\nPros:\n- Good and promising experimental results.\n\nCons:\n- The paper combines several tricks and ideas but it's not really clear what is important and why such an approach works. For example how important is the latent space and the encoder ? Could we just sample directly the weight embeddings from a gaussian and remove the regularization ?\n- The other points mentioned above about the clarity of the paper.\n\nOthers:\n- The title is misleading, the manifold is not really explored... If the author really want to explore the manifold some interesting questions are:  what happens if we try to interpolate between two latent variables ? What do the latent variables represent ? what's the influence of the dimension of the latent space ?\n- In the experiments: what is the number of networks used for the other methods ?\n- It would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5.\n\nConclusion:\nThe experimental results seem promising however the motivation for the approach is not clear. I think fixing some of the points mentioned above could greatly improve the clarity of the paper and make it a stronger submission. In the current state I don't believe the paper is rigorous enough to be accepted.\n\nReferences:\n[1] Pawlowski, N., Rajchl, M., & Glocker, B. (2017). Implicit weight uncertainty in neural networks.\u00a0arXiv:1711.01297.\n[2] Wang, K. C., Vicol, P., Lucas, J., Gu, L., Grosse, R., & Zemel, R. (2018, July). Adversarial Distillation of Bayesian Neural Network Posteriors. ICML\n[3] Mescheder, L., Nowozin, S., & Geiger, A. (2017, July). Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML\n[4] Husz\u00e1r, F. (2017). Variational inference using implicit distributions.\u00a0arXiv:1702.08235.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}