{"title": "Simple and intuitive idea but needs more clarification", "review": "I raised my rating. After the rebuttal.\n\n- the authors address most of my concerns.\n- it's better to show time v.s. testing accuracy as well. the per-epoch time for each method is different.\n- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.\n\n-------------------------------------------------------------\nThis paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters. The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.\n\n1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters. \n- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters. After all, newton method and natural gradients method are not used in experiments.\n- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed. \n\n2. No need to write so much decorated bounds in section 3. The convergence analysis is on Z, not on parameters x and hyper-parameters theta. So, bounds here can not be used to explain empirical observations in Section 5. \n\n3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?\n\n4. Authors have done a good comparison in the context of deep nets.  However,\n- could the authors compare with changing step-size? In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates. Is it better to decay learning rates for toy data sets? It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems. \n- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001\". What are reasons for these?\n- In Section 5.2, it is said lambda is tuned by grid-search. Tuning a good lambda v.s. tuning a good step-size, which one costs more?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}