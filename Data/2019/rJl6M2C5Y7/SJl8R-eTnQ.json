{"title": "Update baselines", "review": "The paper proposes an approach to adapt hyperparameters online. \nWhen learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures. \nA. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning. Thus, it is hard to say whether the results are applicable in practice. \nB. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate. \nC. Your method involves a hyperparameter to be tuned which affects the shape of the schedule. This hyperparameter itself benefits from (requires?) some scheduling. \n\nIt would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes. Online tuning of  hyperparameters is an important functionality and I hope your paper will make it more straightforward to use it in practice. \n\n\n* Minor notes:\n\nYou mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it. I guess you mean w.r.t. the original RMSprop. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}