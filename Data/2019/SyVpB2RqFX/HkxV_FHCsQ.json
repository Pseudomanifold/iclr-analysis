{"title": "Not Compelling.", "review": "This paper proposes an objective function for auto-encoding they\ncall information maximizing auto encoding (IMAE).  To set the stage\nfor my review I will start with the following \"classical\" formulation\nof auto-encoding as the minimization of the following where we are\ntraining models for P(z|x) and P(x|z).\n\nbeta H(z) + E_{x,z sim P(z|x)} -log P(x|z) (1)\n\nHere H(z) is defined by drawing x from the population and then drawing\nz from P(z|x).  This is equivalent to classical rate-distortion coding\nwhen P(x|z) is an isotropic Gaussian in which case -log P(x|z) is just\nthe L2 distortion between x and its reconstruction.  The parameter\nbeta controls the trade-off between the compression rate and the L2\ndistortion.\n\nThis paper replaces minimizing (1) with maximizing\n\nbeta I(x,z) + E_{x,z sim P(z|x)} log P(x|z) (2)\n\nThis is equivalent to replacing H(z) in (1) by -I(x,z).  But (2)\nadmits a trivial solution of z=x.  To prevent the trivial solution this\npaper proposes to regularize P(z) toward a\ndesired distribution Q(z) and replacing I(x,z) with KL(P(z),Q(z))\nby minimizing\n\nbeta KL(P(z),Q(z)) + E_{x,z sim P(z|x)} - log P(x|z) (3)\n\nThe paper contains an argument that this replacement is reasonable\nwhen Q(z) and P(z|x) are both Gaussian with diagonal covariances.  I\ndid not verify that argument but in any case it seems (3) is better than (2). \nFor beta large (3) forces P(z) = Q(z) which fixes H(z) and the a-priori value\nH(Q).  The regularization probably has other benefits.\n\nBut these suggestions are fairly simple and any real assessment of their\nvalue must be done empirically.  The papers experiments with MNIST\nseem insufficient for this.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}