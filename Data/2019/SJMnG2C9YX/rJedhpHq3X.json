{"title": "Well written paper about an interesting problem. The major problem is that the core part of the contribution is a special case of previously published more general framework not cited in the paper. ", "review": "pros:\n\n- Clearly written and sound paper.\n- Addresses interesting problem. \n- Improves existing methods used for this learning scenario.  \n\ncons:\n\n- The core contribution is a special case of previously published more general framework which is not cited in the paper.\n\nIt is clearly written paper with a good motivation. The major problem is that the core contribution, namely, the risk reformulation in Theorem 1 and the derived loss (6), are special cases of more general framework published in \n   Jesus Cid-Sueiro et al. Consistency of Losses for Learning from Weak Labels. ECML 2014.\n\nThe work of [Cid-Sueiro2014] proposes a general way how to construct losses for learning from weak labels. They require that the distribution of weak labels is a linear transformation of the true label distribution, i.e. the assumption (3) of the paper under review. According to [Cid-Sueiro2014], the loss on weak labels is constructed by $weak_loss = L*original_loss$, where $L$ is the left inversion of the \"mixing matrix\" $T$ in (3). [Cid-Sueiro2014] also shows that such weak loss is classification calibrated which implies statistical consistency of the method. \n\nLearning from complementary labels is a special case when the mixing matrix is $T=(E-I)/(K-1)$ (E is unitary matrix, I is matrix of ones, K is number of labels). In this case, the left inversion of $T$ is simply $L=- E*(K-1) + I$ and so the weak loss is $weak_loss=L*loss$ which corresponds to the loss (5) proposed in the paper under review (in fact, the loss (5) also adds a constant term (Y-2)/(Y-1) which however has no effect on the minimizer). \n\nThe novel part of the paper is the non-negative risk estimator proposed in sec 3.3 and the online optimization methods addressed in sec 3.4. These extensions, although relatively straightforward, are empirically shown to significantly improve the results.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}