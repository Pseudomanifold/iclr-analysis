{"title": "Promising, but more work needed ", "review": "This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). They propose an EM algorithm for joint training of parameter initializations and assignment of tasks to initializations. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet.\n\n[Strengths]\n\n+ Modeling the initialization space is an open research question and the authors make a sound proposal to tackle this.\n+ The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. inevitably saturate model parameters. By dynamically increasing the meta-learner's capacity, this approach can in principle bypass catastrophic forgetting.\n\n[Weaknesses]\n\n- There is nothing in the algorithm that prevents mode collapse, and the only thing breaking symmetry is random initialization. In fact, figure 5 and 6 suggest mode collapse occurs even in the non-parametric case. A closely related paper that may be of interest ( Kim et al., 2018, https://arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD.\n- Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance.\n- Experiments on evolving tasks suggest the method is not able to capture task diversity. In the synthetic experiment (figure 5), the model suffers mode collapse when a sufficiently difficult task is introduced. Ultimately, it performs on par with MAML, despite having three times the capacity. Similarly, on the evolving miniImagenet dataset, figure 6 indicates there is no cluster differentiation across tasks.\n- The paper needs major polishing.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}