{"title": "Gradient-base few-shot learning. Extends MAML to a mixture distribution, to allow for internal task clustering. Falls short of recent state-of-art results, while being even a lot slower than MAML", "review": "Summary:\n\nThis work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic.\n\nComparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.\n\n- Quality: Paper is technically complex, but based on simple ideas. In the case of\n   infinite mixtures, it is not clear what is done in the end in the experiments.\n   Experimental results are rather poor, given state-of-the-art.\n- Clarity: The paper is not hard to understand. What is done, is done cleanly.\n- Originality: The idea of putting a mixture model on the global parameters is not\n   surprising. Important questions, such as how to make this faster, are not\n   addressed.\n- Significance: The only comparative results on miniImageNet are worse than the\n   state-of-the-art by quite a margin (admittedly, the field moves fast here, but it\n   is also likely these benchmarks are not all that hard). This is even though better\n   performing methods, like Versa, are much cheaper to run\n\nWhile the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea.\n\nState of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53:\n- Versa: https://arxiv.org/abs/1805.09921.\n   Importantly, this method uses a simpler model (logistic regression head models)\n   and is quite a bit faster than MAML, so much faster than what is proposed here\n- BMAML: https://arxiv.org/abs/1806.03836.\n   This is also quite complex and expensive, compared to Versa, but provides good\n   results.\n\nOther points:\n- You use a set of size N+M per task update. In your 5-way, 1-shot experiments,\n   what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5,\n   then I wonder why results are branded as 5-way, 1-shot, which to mean means\n   that each update can use exactly 5 labeled points.\n   Please just be exact in the main paper about what you do, and what main\n   competitors do, in particular about the number of points to use in each task\n   update.\n- Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and\n   uses further approximations (ICM, instead of Gibbs sampling).\n   Can be seen as a heuristic to evolve the number of components.\n   What is given in Algorithm 2, is not compatible with Section 4. How do you merge\n   your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid\n   that there is always one more (L -> L+1) components? Some threshold must be\n   applied somewhere.\n   An alternative would be to use split&merge heuristics for EM.\n- Results reported in Section 5 are potentially interesting, but entirely lack a\n   reference point. The first is artificial, and surely does not need an algorithm of this\n   complexity. The setup in Section 5.2 is potentially interesting, but needs more\n   work, in particular a proper comparison to related work.\n   This type of effort is needed to motivate an extension of MAML which makes\n   everything quite a bit more expensive, and lacks behind the state-of-art, which\n   uses amortized inference networks (Versa, neural processes) rather than\n   gradient-based.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}