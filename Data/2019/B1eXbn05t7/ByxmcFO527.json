{"title": "Well-written, but incomplete comparisons to prior work", "review": "SUMMARY\nThe paper considers several methods for building generative models that disentangle image content (category label) and style (within-category variation). Experiments on MNIST, Omniglot, and VGG-Faces demonstrate that the proposed methods can learn to generate images combining the style of one image and the content of another. The proposed method is also used as a form of learned data augmentation, where it improves one-shot and low-shot learning on Omniglot.\n\nPros:\n- The paper is well-written and easy to follow\n- The proposed methods CC, CE, PM, and LF are all simple and intuitive\n- Improving low-shot learning via generative models is an interesting and important direction\n\nCons:\n- No comparison to prior work on generation results\n- Limited discussion comparing the proposed methods to other published alternatives\n- No ablations on Omniglot or VGG-Faces generation\n- Low-shot results are not very convincing\n\nCOMPARISON WITH PRIOR WORK\nThere have been many methods that propose various forms of conditional image generation in generative models, such as conditional VAEs in [Sohn et al, 2015]; there have also been previous methods such as [Siddharth et al, 2017] which disentangle style and content using the same sort of supervision as in this paper. Given the extensive prior work on generative models I was a bit surprised to see no comparisons of images generated with the proposed method against those generated by previously proposed methods. Without such comparisons it is difficult to judge the significance of the qualitative results in Figures 3, 5, and 6. In Figure 3 I also find it difficult to tell whether there are any substantial differences between the four proposed methods.\n\nThe proposed predictiability minimization is very related to some recent approaches for domain transfer such as [Tzeng et al, 2017]; I would have liked to see a more detailed discussion of how the proposed methods relate to others.\n\nOMNIGLOT / VGG-FACES ABLATIONS\nThe final model includes several components - the KL divergence term from the VAE, two terms from LF, and a WGAN-GP adversarial loss. How much do each of these terms contribute the quality of the generated results?\n\nLOW-SHOT RESULTS\nI appreciate low-shot learning as a testbed for this sort of disentangled image generation, but unfortunately the experimental results are not very convincing. For one-shot performance on Omniglot, the baseline Histogram Embedding methods achieves 0.974 accuracy which improves to 0.975 using STOC. Is such a small improvement significant, or can it be explained due to variance in other factors (random initializations, hyperparameters, etc)?\n\nFor low-shot learning on Omniglot, the proposed method is outperformed by [Antoniou et al, 2017] at all values of k. More importantly, I\u2019m concerned that the comparison between the two methods is unfair due to the use of different dataset splits, as demonstrated by the drastically different baseline accuracies. Although it\u2019s true that the proposed method achieves a proportionally larger improvement over the baseline compared with [Antoniou et al, 2017], the differences in experimental setup may be too large to draw a conclusion one way or the other about which method is better.\n\nOVERALL\nAlthough the paper is well-written and presents several intuitive methods for content/style decomposition with generative models, it\u2019s hard to tell whether the results are significant due to incomplete comparison with prior work. On the generation side I would like to see a comparison especially with [Siddharth et al, 2017]. For low shot learning I think that the proposed method shows some promise, but it is difficult to draw hard conclusions from the experiments. For these reasons I lean slightly toward rejection.\n\nMISSING REFERENCES\nSiddharth et al, \u201cLearning Disentangled Representations with Semi-Supervised Deep Generative Models\u201d, NIPS 2017\n\nSohn, Lee, and Yan, \u201cLearning structured output representation using deep conditional generative models\u201d, NIPS 2015\n\nTzeng, Hoffman, Darrell, and Saenko, \u201cAdversarial Discriminative Domain Adaptation\u201d, CVPR 2017\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}