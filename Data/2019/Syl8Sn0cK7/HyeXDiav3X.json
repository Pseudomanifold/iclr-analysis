{"title": "Generating (syntactic and functional) specification-satisfying programs via Reinforcement Learning", "review": "The authors design a program synthesizer that tries to satisfy per-instance specific syntactic and functional constraints,\nbased on sampling trajectories from an RL agent that at each time-step expands a partial-program.\n\nThe agent is trained with policy gradients with a reward shaped as the ratio of input/output examples that the synthesized program satisfies.\n\nWith the 'out-of-box' evaluation, the authors show that their agent can explore more efficiently the harder problems than their non-learning alternatives even from scratch.\n(My intuition is that the agent learns to generate the most promising programs)\nIt would be good to have a Monte Carlo Tree Search baseline on the'out-of-box' evaluation, to detect exploration exploitation trade-offs.\n\nThe authors show with the 'meta-solver' approach that the agent can generalize to and also speed up unseen (albeit easy-ish in the authors words) instances.\n\nClarity: Paper is clear and nicely written.\n\nSignificance: Imagine a single program synthesizer that could generate C++/Java/Python/DSLs  programs and learn from all its successes and failures! This is a step towards that.\n\nPros:\n+ Generating spec-following programs for different grammars.\n+ partial tree expansion takes care of syntactic constraints.\nNeutral\n\u00b7 The grammar and specification diversity may be too low to feel impressive.\n\u00b7 It would have been nicer by computing likelihood for unseen instances with unique and known solutions (that is, without finetuning).\nCons:\n- No Tree Search baseline.\n- No results on programs with control flow/internal state.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}