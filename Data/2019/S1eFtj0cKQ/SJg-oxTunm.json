{"title": "Potentially nice empirical study, but needs more work on experimental analysis and discussion", "review": "This paper presents an empirical evaluation of continual learning approaches for generative modelling. Noting that much of previous work focuses on supervised tasks, the paper evaluates various combinations of continual learning strategies (EWC, rehearsal/replay-based, or generative replay) and generative models (GANs or likelihood-based).\nThe experiments evaluate all combinations on MNIST and Fashion MNIST, and the resulting best-performing combination on CIFAR.\nThe paper is well-written and structured, and although there are no new proposed algorithms or measures, I think this has the potential to be a useful empirical study on the relatively unstudied topic of continual learning with generative models.\n\nHowever, my main concern is in the detail of analysis and discussion: for an empirical study, it would be much more beneficial to empirically investigate *why* certain combinations are more effective than others. For example:\n- Is the reason GANs are better than likelihood models with generative replay purely because of sample quality? Or is it sufficient for the generator to learn some key characteristics for a class that lead to sufficient discriminability?\n- Why is rehearsal better for likelihood models? (And how does this relate to the hypothesis of overfitting to a few real examples?)\n\nThe CIFAR-10 results also require more work - it is unclear why the existing approaches could not be made to work, and whether this is a fundamental deficiency in the existing approaches or other factors (hyperparameters, architecture choices, lack of time, etc). Presuming the sample quality is as good as in the WGAN-GP work (given the original implementation is used for experiments), why is this insufficient for generative replay? More detailed analysis / discussion, or another combinatorial study, would help for CIFAR too.\n\nSome comments:\n- The poor performance of EWC across the board is concerning. Was this implemented by computing the Fisher of the ELBO with respect to parameters? Was the empirical or true Fisher used? Why does the performance appear so poor compared to Seff et al (2017)? This suggests that either more thought is required on how to best protect parameters of generative models, or the baseline has not been properly implemented/tuned.\n- Given this is an entirely empirical study, I would strongly encourage the authors to release code sooner than the acceptance deadline - this can be achieved using an anonymous repository.\n- Figure 2 and 3 plots are a little difficult to parse without axis labels.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}