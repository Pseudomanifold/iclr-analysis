{"title": "Review of \"In search of theoretically grounded pruning\"", "review": "# Summary\n\nThe article proposes a variation of magnitude-based pruning for neural networks. In a nutshell, the authors argue that standard MLE asymptotic theory is applicable to deep learning, and propose to use the Wald test statistic as a salience criterion for pruning. Limited experimental results are provided using the LeNet-300-100 and LeNet-5 architectures and the MNIST dataset.\n\n# Assessment\n\nCompression and pruning of neural networks is a topic with a vast wealth of prior work spanning several decades. Nonetheless, the interest in deploying deep learning-based models to devices with limited compute capabilities has kept the topic particularly active in the last years.\n\nDespite the relevance of this paper's topic, unfortunately I do not believe that the methodological contribution is sufficiently novel and significant nor that the experimental results are sufficiently thorough and compelling to recommend the paper for publication.\n\n# Major points\n\n1) Novelty and Significance of the methodological contribution\n\nA large part of the paper is devoted to justify the applicability of the Wald test to deep learning models. Nonetheless, I do not believe the arguments put forward are sufficiently rigorous or original to consider those as a substantial contribution. Perhaps most importantly, I have strong concerns about several of those arguments.\n\nFor example, using the Wald test requires identifiability, which is clearly severely violated by most commonly used architectures. While the paper argues that the parameter space can be restricted to contain a single minimum, that hardly resembles the way models are trained in practice. \n\nThe Wald test also requires the training to converge to a local minimum, i.e., the Hessian must be positive definite. Again, I do not believe this assumption holds in many real-world models. Convergence to saddle points often occurs, with the Hessian thus containing negative eigenvalues [1].\n\nAs accepted in the Appendix, the differentiability condition is violated by some of the most commonly used activation functions, including ReLUs.\n\nNormality of the score distribution depends on asymptotic arguments. However, many deep learning models have a much larger number of parameters than samples they are trained on. Consequently, it is unclear whether asymptotic results are applicable in the regime in which deep learning-based approaches are commonly used.\n\nFinally, perhaps less importantly, the assumption if i.i.d. samples might also be too strong for many real-world datasets.\n\nThe proposed criterion is otherwise a straightforward application of the Wald test.\n\n2) Experimental results\n\nThe experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of state-of-the-art competitors among the very many related methods published recently (e.g. [2-8], just to mention a small subset). \n\nPerhaps most importantly, the method does not appear to clearly outperform the state of the art. However, the insufficient experimental results reported in the manuscript render a precise assessment difficult.\n\n# References\n\n[1] Martens, James. \"New insights and perspectives on the natural gradient method.\" arXiv preprint arXiv:1412.1193 (2014).\n[2] Guo, Yiwen, Anbang Yao, and Yurong Chen. \"Dynamic network surgery for efficient dnns.\" Advances In Neural Information Processing Systems. 2016.\n[3] Dong, Xin, Shangyu Chen, and Sinno Pan. \"Learning to prune deep neural networks via layer-wise optimal brain surgeon.\" Advances in Neural Information Processing Systems. 2017.\n[4] Ullrich, Karen, Edward Meeds, and Max Welling. \"Soft weight-sharing for neural network compression.\" International Conference on Learning Representations. 2017.\n[5] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational dropout sparsifies deep neural networks.\" International Conference on Machine Learning. 2017.\n[6] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian compression for deep learning.\" Advances in Neural Information Processing Systems. 2017.\n[7] Louizos, Christos, Max Welling, and Diederik P. Kingma. \"Learning Sparse Neural Networks through $ L_0 $ Regularization.\" International Conference on Learning Representations. 2018.\n[8] Yang, Yibo, Nicholas Ruozzi, and Vibhav Gogate. \"Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization.\" arXiv preprint arXiv:1806.05355 (2018).", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}