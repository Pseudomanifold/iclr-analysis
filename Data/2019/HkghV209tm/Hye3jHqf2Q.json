{"title": "Main idea is not sufficiently novel and technical and empirical results are not convincing enough", "review": "This paper combines recent results in online learning and convex optimization, specifically adaptivity, momentum, and optimism. The authors add an optimistic gradient prediction step into the AMSGrad algorithm proposed by Reddi et al, 2018. Moreover, they propose using the RMPE algorithm of Scieur et al, 2016 to come up with the gradient prediction step. The new method that they introduce is called Optimistic AMSGrad, and the authors present both theoretical guarantees as well as numerical experiments justifying this new method.\n\nThe paper is relatively well-written, and the authors do a good job of explaining recent work on adaptivity, momentum, and optimism in online learning and convex optimization to motivate their algorithm. The algorithm is also presented clearly, and the fact that the method is accompanied by both a regret bound as well as numerical experiments is appreciated.\n\nAt the same time, I found the presentation of this work to be a little misleading. The idea of applying optimism to Adam was already presented in Daskalakis et al, 2018. The algorithm in that paper is, in fact, called \"Optimistic Adam\". I found it very strange that the authors chose to rename that algorithm in this paper. There are two main differences between Optimistic Adam in Daskalakis et al, 2018 and Optimistic AMSGrad. The first is the extension from Adam to AMSGrad, which involves an extra maximization step (line 7 in Algorithm 2) that is immediate. The second is the choice of gradient prediction method. Since Daskalakis et al, 2018 were concerned with equilibrium convergence, they opted to use the most recent gradient as the prediction. On the other hand, the authors in this work are concerned with general online optimization, so they use a linear combination of past gradients as the prediction, based on a method introduced by Scieur et al, 2016. On its own, I do not find this extensions to be sufficiently novel or significant to merit publication. \n\nThe fact that this paper includes theoretical guarantees for Optimistic AMSGrad that were missing in Daskalakis et al, 2018 for Optimistic Adam does make it a little more compelling. However, I found the bound in Theorem 1 to be a little strange in that\n(1) it doesn't reduce to the AMSGrad bound when the gradient predictions are 0 and (2) it doesn't seem better than the AMSGrad or optimistic FTRL bounds. The authors claim to justify (2) by saying that the extra g_t - h_t term is O(\\sqrt{T}), but the whole appeal of adaptive algorithms is that the \\sqrt{T} terms are data-dependent. The empirical results also do not include error bars, which makes it hard to judge their significance. \n\nThere were also many grammatical errors and typos in the paper. \n\nOther comments and questions:\n1) Page 1: \"Their theoretical analysis are the regret analysis in online learning.\" Grammatical error.\n2) Page 2: \"The concern is that how to get good m_t\". Grammatical error.\n3) Page 3: \"as the future work\". Grammatical error.\n4) Page 3: \"Nestrov's method\". Typo. \n5) Page 4: \"with input consists of\". Grammatical error\n6) Page 4: \"Call Algorithm 3 with...\" What is the computational cost of this step? One of the main benefits of algorithms like AMSGrad is that they run in O(d) time with very mild constants. \n7) Page 4: \"For this extrapolation method to well well..., the gradient vectors at a specific time span is assumed to be captured by (5). If the gradient does not change significantly, this will be a mild condition.\" If the gradient doesn't change significantly, then choosing m_t = g_t would also work well, wouldn't it? Can you come up with examples of objectives for which this method makes sense? Even toy ones would strengthen this paper.\n8) Page 5: Equation (8). As discussed above, this bound doesn't appear to reduce to the AMSGrad bound for m_t = 0, which makes it a little unsatisfying. The fact that there is an extra expression that isn't in terms of the \"gradient prediction error\" that one has for optimistic FTRL also makes the bound a little strange.\n9) Page 7: \"The conduct Optimistic-AMSGrad with different values of r and observe similar performance\". You should mention that you show the performance for some of these different values in the appendix.\n10) Page 7: \"multi-classification problems\". Typo.\n11) Page 7: Figure 1. Without error bars, it's impossible to tell whether these results are meaningful. Moreover, it's strange to evaluate algorithms with online convex optimization guarantees on off-line non-convex problems.\n12) Page 7: \"widely studied and playing\". Grammatical error.\n13) Page 8: \"A potential directions\". Grammatical error.\n\n\n\n\n\n\n\n   \n\n \n\n\n\n\n\n\n\n\n\n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}