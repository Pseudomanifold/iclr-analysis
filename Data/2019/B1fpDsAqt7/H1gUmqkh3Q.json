{"title": "very interesting work, but a lot of the details are not clear. ", "review": "[Summary]\nThis paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner. The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention. The final prediction uses all the states and question to infer the final answers. The authors verify the effectiveness of the proposed method on the performance of different tasks and modules. Experiment on VQA shows the proposed model benefits from utilizing different modules. The authors also qualitatively show the model's reasoning process and human study on judging answering quality.\n\n[Strength]\n1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.  This is different from most existing work. \n\n2: By examing different modules, the proposed method is more interpretable compare to canonical methods. \n\n3: The experiment results are good, especially for the counting problem. \n\n[Weakness] \n1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).  \n\n2. Annotation is not clear in this paper. For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k). \" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand. On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation? From the supplementary, it seems Epsilon means the environment? \n\n3. On the object counting task, the query transmitter needs to produce a query for a relationship module. The authors mentioned that this is softly calculated by softmax on the importance score. Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case? \n\n4. The cider score of image captioning is 109 compared to the baseline 108. The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data. Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input. My assumption is the visual feature already contains the label information for image captioning. \n\n5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results. \n\n6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods? \n\n7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others. Is the number right? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}