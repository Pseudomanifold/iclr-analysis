{"title": "Good demonstration of adversarial training but evaluation doesn't seem very promising", "review": "This paper proposes a framework which preserves the private information in the image and at the same time doesn\u2019t compromise the usability of the image. This framework is composed of three models - the obfuscator, the classifier, and the reconstructor. While the obfuscator\u2019s objective is to remove the sensitive information from the image and extract feature maps that still have it\u2019s discriminative features, the classifier uses these feature maps to do the classification learning task. On the other hand, the reconstructor is like an attacker which tries to expose the private information by restoring the image. The framework thus uses adversarial training since the obfuscator and the reconstructor have opposite goals. The paper shows that the reconstructed images don\u2019t seem to have crucial sensitive information of the raw image yet it still manages to achieve high classification accuracy. \n\nPros:\n- While we focus on achieving better and better results with Machine Learning, we tend to forget about the private nature of the data we often have to work with. I believe it is a very important issue to address and a great idea to work on.\n- The idea of using the reconstructor as an adversary in the framework seems so intuitive. \nThe visualizations at the end comparing the reconstructed images with the raw images reveal how this training procedure doesn\u2019t allow sensitive information to leak.\n\nCons:\n- I am not very convinced with choosing the image reconstruction quality as a measure for the privacy preservation. I am unable to see how those two can be so strongly related. There can be scenarios I believe, where the image doesn\u2019t reconstruct well but can still retain the sensitive features of the image. \n- I am just curious about a few aspects of the design of the network. Firstly, I would have expected to a deeper network required to remove the private information of the image. I think a visualization on how the feature maps look after 3rd layer when they lose their private traits, with possibly a comparison with an early, say 1st layer would help understand it better. \n- When I try to think of a data with private information, it seems to be pretty different than the MNIST or CIFAR data. I can\u2019t seem to imagine any sort of privacy linked to these datasets (with a digit for example). It would be more helpful to look at the performance of the framework with images having some private content as we have from social networking sites or medical images which contains sensitive details. \n\nQuestions:\n- Are you using any sort of pretrained weights from the VGG network and fine-tuning on top of it or are you just borrowing the architecture and training it from scratch in your experiments?\n- Table 1 shows a sigmoid layer in the last layer of the reconstruction networks. It was a little confusing since we are expecting an image as an output of the network.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}