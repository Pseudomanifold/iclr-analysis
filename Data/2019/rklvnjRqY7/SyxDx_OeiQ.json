{"title": "Important topic to study but deeper investigation is suggested", "review": "Privacy issues are a burning issue in ML. Elevated awareness to privacy issues, as expressed, for example, by GDPR regulations, might make it hard to collect large datasets for training machine learning models. This current work suggests using adversarial networks to obfuscate images and thus allow collecting them without privacy concerns. \nThe problemed studied in this work is of great interest and there many researchers are studying it today. The popularity of this topic makes it hard to present a good background on the state of current research. However, I fined the method proposed here na\u00efve compared to the methods proposed by previous researchers, even the ones that are cited in the related work section: while methods that use differential privacy, homomorphic encryptions or secure-multi-party-computation provide theoretical guaranties on the privacy provided, the work presented here shows only that the obfuscated data does not allow reconstructing the image using several networks with which such reconstruction was attempted.  \n\nThe problem discussed above breaks into two questions that are left un-answered. First, what is the trust model? That is, under what assumptions can we prove that the private data is protected? The authors here propose using the reconstruction quality is a measure of privacy. The advantage of this measure is that it is easy to evaluate, and it is generic. However, this leads to a second question, why is reconstruction error a good measure of privacy? For example, even a well distorted image may allow identifying the identity of a person in an image.\n\nIn the following there are some detailed comments on specific sections in this work:\n\u2022\tIntroduction \no\t \u201cGDPR stipulates that personal data cannot be stored for long periods of time \u2026 In other words, this regulation prevents long-term storage of video/image data\u201d \u2013 this is not accurate. GDPR allows for pseudo-anonymization of data. Therefore, for example, encrypted data can be stored for long periods of time.\no\tDoes the obfuscated image qualify as anonymization/pseudo-anonymization according to GDPR?\no\t\u201cThus the obfuscated intermediate representation can be stored indefinitely\u201d \u2013 is this representation not subject to GDPR deletion requests?\no\t\u201cwe choose image reconstruction quality as a general measure for privacy preservation\u201d \u2013 why is it a good measure of privacy preservation? For example, could it be that the reconstruction quality is low, but it is still possible to identify the person in the image?\no\t\u201cTo the best of our knowledge, this is the first study of using the adversarial training methodology for privacy-preserving image classification\u201d \u2013 there have been studies in the past of using adversarial networks for privacy tasks. For example, Abadi & Andersen, 2016 studied using adversarial networks as an encryption mechanism without specifying whether the data is an image or not. The work of Yu, Zhang, Kuang, Lin & Fan, 2017 is also very close in nature to the current one. One of the contributions Yu et al. propose is image privacy protection by blurring private sensitive objects from the image.\n\u2022\tRelated Work\no\tThe main limitation of CryptoNets in the context of this work is that it only handles inference and does not handle training of neural networks. There are some attempts to perform training using homomorphic encryptions, for example see Kim, Song, Wang, Xia, and Jiang, 2018.\no\tIt is not accurate to say about DeepSecure that it is limited to cases in which each client sends less than 2600 examples. Instead, what is true about DeepSecure and related methods such as SecureML is that there is a big computation cost.\no\tThere are methods that use Intel\u2019s SGX for training neural networks which provide privacy but have better computation performance, for example see Ohrimenko, Schuste, Fournet, Mehta, Nowozin, Vaswani and Costa, 2016\no\tWhen discussing privacy methods, you should also mention the trust model \u2013 that is, what is an adversary allowed to do to try to capture private information. For example, DeepSecure, assumes the non-collusion & semi-honest model which means that the data is protected by splitting it on several servers, each server will try to use the information it can see to infer the private information but multiple parties do not collaborate in trying to get to private information and moreover, each party follows the designed protocol. Other methods use different trust models.\no\tOn the work of Osia et al you say that it can\u2019t be used for training because of the communication costs. The large communication costs are common to techniques that use multi-parties to secure computation, this includes DeepSecure and SecureML. Note, however, that the communication costs do not make it impossible to train, instead, it makes it slow and costly.\n\u2022\tSecurity Analysis \no\tis it necessary the case that there is a single data provider? If there are more than a single data provider, then you may have more that 3 entities in the system. In this case you may say that there are 3 entity types.\no\tWhat are the restrictions on the power of the attacker? According to the description the attacker can be an internal staff which may allow the attacker to obtain access to the data before obfuscation\no\tBesides the nice acronym, why is \u201cChosen Image Attack\u201d a proper name for an attack that modifies the representation of images?\no\tWhy is the Chosen Image Attack the \u201cmost natural and convenient way of launching an attack\u201d? In many cases, privacy leaks because of joining data from multiple sources, see for example the attacks on the Netflix challenge data, see for example Ohm, 2009.\no\t\n\u2022\t Obfuscator-Adversary Framework\no\tA similar model to the one proposed here has been suggested by Moore, Pfeiffer, Wei, Iyer, Charles, Gilad-Bachrach, Boyles and Manavoglu, 2018 in the context of removing biases from datasets.\n\u2022\tExperiments\no\tThe images used in this experiment are very different from the images on which VGG and ResNet were designed to work. The images here are very small around 30x30 pixels while VGG and ResNet were designed to work on large images, for example 200x300 pixels. \no\tIt may be interesting to conduct an experiment on large images (ImageNet, Caltech dataset) and see how the reconstruction error differs between the proposed representation compared to standard representations such as the ones generated by AlexNet, VGG, Inception and others.\n\n\nTherefore, a lot of important and interesting work is reported in this report, however, some additional thinking is required. I advise the authors to keep working in this direction.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}