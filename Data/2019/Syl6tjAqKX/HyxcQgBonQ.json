{"title": "Review", "review": "# Summary\nThis paper proposes to learn behaviors independently from the main task. The main idea is to train a behavior classifier and use domain-adversarial training idea to make the features invariant to sources of behaviors for transfer learning to new behaviors/tasks. The results on Atari games show that the proposed idea learns new behavior more quickly than the baseline approaches. \n\n[Cons]\n- Some descriptions are ambiguous, which makes it hard to understand the core idea and goal of this paper. \n- The experimental setup is not well-designed to show the benefit of the idea. \n\n# Comments\n- This overall idea is a straightforward extension from domain-adversarial learning except that this paper considers transfer learning in RL.\n- The goal/motivation of this paper is not very clearly described. It seems like there is a \"main task\" (e.g., maximizing scores in Atari games) and \"behavior modules\" (e.g., specific action sequences). It is unclear whether the goal of this paper is to learn 1) the main task, 2) learning new behavior modules quickly, or 3) learning new (main) tasks quickly. In the abstract/introduction, the paper seems to address 3), whereas the actual experimental result aims to solve 2). The term \"task\" in this paper often refers to \"main task\" or \"behavior\" interchangeably, which makes it hard to understand what the paper is trying to do. \n- The experiment is not well-designed. If the main focus of the paper is \"transfer to new tasks\", Atari is a not a good domain because the main task is fixed. Also, behavior modules are just \"hand-crafted\" sequences of actions. Transfer learning across different behaviors are not interesting unless they are \"discovered\" in an unsupervised fashion. \n- The paper claims that \"zero-shot\" transfer is one of the main contributions. Zero-shot learning by definition does not require any additional learning. However, they \"trained\" the network on the new behavior modules (only the main network is fixed), which is no longer \"zero-shot\" learning. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}