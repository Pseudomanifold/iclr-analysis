{"title": "Review for \"Dynamic Channel Pruning: Feature Boosting and Suppression\"", "review": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss.  The technique prunes channels in an input-dependent way through the addition of auxiliary channel saliency prediction+pruning connections.\n\nPros:\n- The paper is well-written and clearly explains the technique, and Figure 1 nicely summarizes the weakness of static channel pruning\n- The technique itself is simple and memory-efficient\n- The performance decrease is small\n\nCons:\n- There is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x)\n- In contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, real-time)\n- The experiments are limited to classification and fairly dated architectures (VGG16, ResNet-18)\n\nOverall, the method is nicely explained but the motivation is not clear.  Provided that speeding up inference without reducing the size of the model is desirable, this paper gives a good technique for preserving accuracy.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}