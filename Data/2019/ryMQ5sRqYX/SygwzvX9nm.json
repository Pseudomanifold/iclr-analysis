{"title": "An interesting mixed strategy perspective to train GANs", "review": "This paper uses a mixed strategy perspective for GANs. With this formulation the non-convex game formulation of GANs can be transformed into a infinite dimensional problem analog to a finite dimensional bilinear problem.  \n\nI really like this approach, that tries to find methods that converge globally to (mixed) Nash equilibriums. However I have some concerns. \n\n- I'm concerned about the definition of a $O(T^{-1})-NE$. Actually, this merit function is not standard for game. It can be 0 even if $x_t,y_t$ is far from the equilibrium (for instance for the problem $\\min_{x \\in \\Delta_d}\\max_{y \\in \\Delta_d} x^\\top y$ with $x_t = (1,0,\\ldots,0)$ and $y_t= (F(x_{NE},y_{NE}),1-F(x_{NE},y_{NE}),0,\\ldots,0)$ we have $F(x_t,y_t) = F(x_{NE},y_{NE})$ but $x_{NE} = y_{NE} =(1/d,\\ldots,1/d)$). One merit function that could be considered is $\\max_{y} F(x,y_t) - \\min_{y} F(x_t,y)$. \n\n- There is a gap between the theory and the practical method that could be bridged. Actually Theorem 2 assume that the stochastic derivatives are unbiased but since your Langevin dynamics gives you an *approximate* of the next distribution an analysis taking into account this bias would provide much stronger results. More precisely, it would be interesting to have a result similar as Theorem 2 with conditions on $\\epsilon_t$ and $K_t$. For instance, if the theoretical $K_t$ is too large it would reduce the interest of your algorithm. I think this analysis is key since it allows to claim that you can properly approximate the distributions of interest.\n\nIf you are able to ease these concerns I'm eager to increase my grade.\n\n\n- \"(5) is exactly the infinite-dimensional analogue of (1):\" Actually it is not exactly the analogue since $<.,.>$ is not a scalar product anymore (particularly, $<g,\\mu>$ is not defined) but the canonical pairing between a space and its dual (we are loosing something going to infinite dimension).\nI think it should be clarified somewhere. \n\nMinor comments: \n- on the updates rules of $\\theta$ and $\\omega$ (Page 6) the Gaussian noises are missing. \n- On algorithm 3,4,5 and 6 the Gaussian noise is too wide and causes an Overfull.\n\n=== After Authors response ===\nThe authors fixed some major issues. That is why I improved my grade. \nHowever I'm still concerned about the scalability of this algorithm\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}