{"title": "An interesting solution to a challenging problem, but lacking in quantitative results", "review": "Summary: This paper aims to imitate, via Imitation Learning, the actions of a humanoid agent given only video demonstrations of the desired task, including walking, running, back-flipping, and front-flipping. Since the algorithm does not have direct access to the underlying actions or rewards, the agent aims to learn an embedding space over instances with the hope that distance in this embedding space corresponds to reward. Deep RL is used to optimize a policy to maximize cumulative reward in an effort to reproduce the behavior of the expert.\n\nHigh-level comments:\n- My biggest concern with this paper is the lack of a baseline example that we can use to evaluate performance. The walking task is interesting, but the lack of a means by which we can evaluate a comparison between different approaches makes it very difficult to optimize. This makes evaluation of quality and significance rather difficult. A number of other questions I have stem from this concern:\n    = The paper is missing a comparison between the recurrent Siamese network and the non-recurrent Siamese network. The difficulty in comparing these approaches without a quantitative performance metric.\n    = The authors also mention that they tried using GAIL to solve this problem, but do not show these results. Again, a success metric would be very helpful here.\n    = Finally, a simpler task for which the reward is more easily specified may be a better test case for the quantitative results. Right now, the provided example of walking agents seems to only provide quantitative results.\n- The authors need to be more clear about the structure of the training data and the procedure. As written, the structure of the triplet loss is particular ambiguous: the condition for positive/negative examples is not clearly specified.\n- There are a number of decisions made in the paper that feel rather arbitrary or lack justification. In particular, the \"normalization\" scaling factor fits into this category. Some intuition or explanation for why this is necessary (or why this functional form should be preferred) would be helpful.\n- A description of what the error bars represent in all of the plots is necessary.\n\nMore minor comments and questions:\n- The choice of RL algorithm is not the purpose of this paper. Much of this section, and perhaps many of the training curves, are probably better suited to appear in the Appendix. Relatedly, why are training curves only shown for the 2D environment? If space was a concern, the appendix should probably contain these results.\n- An additional agent that may be a useful comparison is one that is directly provided the actions. It might then be more clear how well. (Again, this would require a way to compare performance between different approaches.)\n- How many demonstrations are there? At training vs testing?\n- Where are the other demonstrations? The TSNE embedding plot mentions other tasks which do not appear in the rest of the paper. Did these demonstrations not work very well?\n\nA Comment on Quality: Right now, the paper needs a fair bit of cleaning up. For instance, the word \"Rienforcement\" is misspelled in the abstract. There is also at least one hanging reference. Finally, a number of references need to be added. For example, when the authors introduce GAIL, they mention GANs and cite Goodfellow et al. 2014, but do not cite GAIL. There is also a lot of good research on Behavioral Cloning, and where it can go wrong, that the authors mention, but do not cite.\n\nConclusion: At this point it is difficult to recommend this paper for acceptance, because it is very hard to evaluate performance of the technique. With a more concrete way of evaluating performance on a different task with a clearer reward function for comparison, the paper could be much stronger, because this would allow the authors to compare the techniques they propose to one another and to other algorithms (like GAIL).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}