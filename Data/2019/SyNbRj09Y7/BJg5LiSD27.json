{"title": "Interesting idea, writing needs significant work and comparisons", "review": "\nBrief summary:\u2028This work proposes a way to perform imitation learning from raw videos of behaviors, without the need for any special time-alignment or actions present. They are able to do this by using a recurrent siamese network architecture to learn a distance function, which can be used to provide rewards for learning behaviors, without the need for any explicit pose estimation. They demonstrate effectiveness on 2 different locomotion domains. \n\nOverall impression:\nOverall, my impression from this paper is that the idea is to use a recurrent siamese network to learn distances which make sense in latent space and provide rewards for RL. This is able to learn interesting behaviors for 2 tasks. But I think the writing needs significant work for clarity and completeness, and there needs to be many more baseline comparisons. \n\nAbstract comments:\ntrail and error -> trial and error\n\nIntroduction comments:\n\nAlternative reasons why pose estimation won\u2019t work is because for any manipulation tasks, you can\u2019t just detect pose of the agent, you also have to detect pose of the objects which may be novel/different\n\nFew use image based inputs and none consider the importance of learning a distance function in time as well as space -> missed a few citations (eg imitation from observation (Liu, Gupta, et al))\n\nTherefore we learned an RNN-based distance function that can give reward for out of sync but similar behaviour -> could be good to emphasize difference from imitation from observation (Liu, Gupta, et al) and TCN (Semanet et al), since they both assume some sort of time alignment\n\nMissing related work section. There is a lot of related work at this point and it is crucial to add this in. Some things that come to mind beyond those already covered are:\n1. Model-based Imitation Learning from State Trajectories\n2. Reward Estimation via State Prediction\n3. infoGAIL\n4. Imitation from observation \n5. SFV: Reinforcement Learning of Physical Skills from Videos\n6. Universal planning networks\n7. https://arxiv.org/abs/1808.00928\n8. This might also be related to VICE (Fu, Singh et al), in that they also hope to learn distances but for goal images only.\nIt seems like there is some discussion of this in Section 3.1, but it should be it\u2019s own separate section.\n\nSection 3 comments:\na new model can be learned to match this trajectory using some distance metric between the expert trajectories and trajectories produced by the policy \u03c0 -> what does this mean. Can this be clarified?\n\u2028The first part of Section 3 belongs in preliminaries. It is not a part of the approach. \n\nSection 3.2\nEquations 9 and 10 are a bit unnecessary, take away from the main point\n\nWhat does distance from desired behaviour mean? This is not common terminology and should be clarified explicitly.\n\nEquation 11 is very confusing. The loss function is double defined.  what exactly Is the margin \\rho (is it learned?) The exact rationale behind this objective, the relationship to standard siamese networks/triplet losses like TCN should be discussed carefully. This is potentially the most important part of the paper, it should be discussed in detail.Also is there a typo, should it be || f(si) - f(sn)|| if we want it to be distances? Also the role of trajectories is completely not discussed in equation 11.\n\nSection 3.3 \nThe recurrent siamese architecture makes sense, but what the positive and negative examples are, what exactly the loss function is, needs to be defined clearly. Also if there are multiple demonstrations of a task, which distance do we use then?\n\nThe RL simulation environment is it made in-house, based on bullet or something else?\n\nData augmentation - how necessary is this for method success? Can an ablation be done to show the necessity of this?\n\nAlgorithm 1 has some typos \n- > is missing in line 3\n- Describe where reward r is coming from in line 10\n\nSection 4.1\nWalking gate -> walking gait\n\nThere are no comparisons with any of the prior methods for performing this kind of thing. For example, using the pose estimation baseline etc. Using the non-recurrent version. Using TCN type of things. It\u2019s not hard to run these and might help a lot, because right now there are no baseline comparisons\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}