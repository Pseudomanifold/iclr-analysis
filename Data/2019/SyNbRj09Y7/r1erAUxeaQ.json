{"title": "Issues with significance of results", "review": "This paper proposes an imitation learning method solely from video demonstrations by learning recurrent image-based distance model and in conjunction using RL to track that distance.\n\nClarity: The paper writing is mostly clear. The motivation for using videos as a demonstration source could be more clearly stated. One reason is because it would pave the way to learn from real-world video demonstrations. Another reason is that robot's state space is an ill-suited space to be comparing distances over and image space is more suitable. Choosing one would help the readers identify the paper's motivation and contribution.\n\nOriginality: The individual parts of this work (siamese networks, inverse RL, learning distance functions for IRL, tracking from video) have all been previously studied (which would be good to discuss in a relate work section), so nothing stands out as original, however the combination of existing ideas is well-chosen and sensible.\n\nSignificance: There are a number of factors that limit the significance of this work. \n\nFirst, the demonstration videos come from synthetic rendered systems very similar the characters that imitate them, making it hard to evaluate whether this approach can be applied to imitation of real-world videos (and if this is not the goal, please state this explicitly in the paper). Some evaluation of robustness due to variation in the demonstration videos (character width, color, etc) could have been helpful to assure the reader this approach could scale to real-world videos.\n\nSecond, only two demonstrations were showcased - 2D walking and 3D walking. It's hard to judge how this method (especially using RNNs to handle phase mismatch) would work for other motions.\n\nThird, the evaluation to baselines is not adequate. Authors mention that GAIL does not work well, but hypothesize it may be due to not having a recurrent architecture. This really needs to be evaluated. A possibility is to set up a 2x2 matrix of tests between [state space, image space] condition and [recurrent, not recurrent] model. Would state space + not recurrent reduce to GAIL?\n\nFourth and most major to me is that looking at the videos the method doesn't actually work very well qualitatively, unless I'm misunderstanding the supplementary video. The tracking of 2D human does not match the style of the demonstration motion, and matches even less in 3D case. Even if other issues were to be addressed, this would still be a serious issue to me and I would encourage authors to investigate the reasons for this when attempting to improve their work.\n\nOverall, I do not think the results as presented in the submission are up to the standards for an ICLR publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}