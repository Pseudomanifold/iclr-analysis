{"title": "Interesting idea for efficient NAS that gives state-of-the-art results (on limited datasets)", "review": "The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.\n\n- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?\n- (minor) I do not think that the size of the search space a very meaningful metric\n\nPros:\n- Good exposition\n- Interesting and fairly elegant idea\n- Good experimental results\n\nCons\n- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers\n- No source code available\n\nSome typos:\n\n- Fo example, when proxy strategy -> Fo*r* example\n- normal training in following ways. -> in *the* following ways\n- we can then derive optimized compact architecture.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}