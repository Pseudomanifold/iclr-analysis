{"title": "Neat contribution", "review": "Learning graphs from data fine tunes standard similarity graph constructions such as k-nearest neighbor graphs.  \nThere has been a line of research works that focuses on learning graphs and that shows that this results in superior\nresults in various machine learning tasks.  The current state-of-the-art method is the method proposed by Kalofolias, \nwhich however is slow.   The authors suggest a method to avoid searching for the parameters that achieve a desired\nlevel of sparsity by providing closed a formula. The parameter that determines the sparsity is theta, see proposition 1 on page 4. This was originally shown by Kalofolias. To achieve their goal, the authors first consider the degree of any given node by looking at equation (8), page 4. They prove theorem 1, that is intuitive and  provides the form of the optimal \nweights that connect this node to the rest of the nodes in the graph.  The proof is based on applying the KKT conditions on\nthe objective (8), with the single constraint that there are no negative weights.  Finally, since we care about the \nsparsity of the graph as a whole, the authors use the average of the parameter theta over all nodes. The authors perform \nexperiments on real-world graphs, and show basic properties of their method, as well as the main source of mistakes ,i.e., disconnected nodes, figure 5.\n\nEssentially, this paper starts from the work of Kalofolias  and improves it significantly. This by itself is \na neat contribution, but the authors could improve their paper by showing a more complete view  of graph \nlearning methods, with respect to the quality of the produced graphs and the scalability. I find this aspect of the paper narrowing its contribution, hence my evaluation. Some remarks follow.\n\n- A different family of graph learning methods is based on the objective ||LX||_F^2 or equivalently tr(X^TLLX). \nFor this objective, Daitch et al. proved certain neat properties, such as the existence of a sparse optimal graph. \nThis allows Daitch et al. to solve the primal dual significantly faster than O(n^2) since by their theorem, \nO(nd) edges are required where d is the dimension of the data points. When d is large, a random projection can be applied. \nThe paper should compare with this family of methods that are more scalable both with respect to the accuracy, \nand to the runtimes. \n\n- While the proposed method scales significantly better than Kalofolias, the datasets used are small. \n\n- Using LSH for k-nn graphs results in a  scalable, practical way to construct similarity graphs. The authors should cite\nthe following related work, and compare with such methods.\n\u201cEfficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures\u201c by Dong, Charikar, Li. \n\n- An interesting experiment would be to inject outliers in the dataset, or use some dataset with outliers. \nWould this affect the tightness of the interval in equation (17)? ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}