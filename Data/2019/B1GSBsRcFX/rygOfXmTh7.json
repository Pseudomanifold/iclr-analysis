{"title": "Stop memorizing: A data-dependent regularization framework for intrinsic pattern learning", "review": "Previous works have shown that DNNs are able to memorize random training data, even ignoring the enforcing of data-dependent geometric regularization constraints. In this work, authors show convincing results indicating that this is due to a lack of consistency between the main classification loss (typically soft-max cross entropy) and the selected geometric constraint. Consequently, they propose a simple approach where the softmax loss is replaced by a validation loss that is consistent with the enforced geometry. Specifically, for each training batch, instead of considering a join loss (soft-max cross entropy + geometric constraint), they apply a sequential process, where each training batch is split into two sub-batches: a first batch used to apply the geometric constraint, and a second batch (based on the proposed feature geometry) where a validation loss is used to generate a predicted label distribution. Authors test the proposed idea using an implementation that enforces that samples from each class belong to an independent low-rank sub-space (enforced geometric constraint). Results verify the main hypothesis. Specifically, the resulting model is able to fit real data but not data with random labels. The strength of this evaluation is enhanced including results from relevant baselines. In terms of generalization of real data, the proposed approach offers a small increase in accuracy.\n\nPaper is well written, main hypothesis is relevant, and results are convincing. While not a complete answer to the main questions related to the abilities of DNNs to fit and generalize on real data, this paper offers relevant insights related to the role of finding/using a suitable loss function to train DNNs. These results are relevant to the community and they can illuminate future work, so this reviewer recommend to accept this paper.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}