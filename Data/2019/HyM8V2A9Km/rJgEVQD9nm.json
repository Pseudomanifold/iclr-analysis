{"title": "Good paper, very interesting analysis and insights", "review": "This submission presents a method to improve the sample-efficiency of instruction-following models by leveraging the Hindsight Experience Replay framework with natural language goals. \n\nHere are  my comments/questions:\n- The paper is well written and easy to follow, it introduces a simple idea which achieves very good results.\n- In addition to improving the performance as compared to the baselines, the authors perform a wide variety of experiments such as analysis of language representations, visualization of embeddings, etc. which lead several insightful results such as ability of sentence embeddings to generalize to unseen lexicon, ability of the model to perform well with just 1% advice.\n- It is important to note that as compared to the baselines, the proposed method requires access to the set of goals and extra information about which goal was reached in each episode.\n- In Table 1, how many frames were DQN and ACTRCE trained for? I am wondering why the MT performance for DQN is so low. Did the DQN have Gated-Attention?\n- The composition task is very interesting, did the agent receive intermediate rewards for completing a part of the instruction in this task?\n- Some implementation details questions:\n\t- In Appendix D Training details, what do you mean by 'chosen from the range {1000, 10000, 10000}'?\n\t- In Appendix D Training details,  it is mentioned that you reproduce training using Asynchronous Advantage Actor Critic (A3C), where is A3C used in the experiments?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}