{"title": "Logical continuation of existing work", "review": "At the core of this paper is the insight from [1] that a neural network layer constructed from a combination of linear, piecewise affine and convex operators can be interpreted as a max-affine spline operator (MASO). MASOs are directly connected to vector quantization (VQ) and K-means clustering, which means that a deep network implicitly constructs a hierarchical clustering of the training data during learning. This paper now substitutes VQ with probabilistic clustering models (GMMs) and extends the MASO interpretation of a wider range of possible operations in deep neural networks (sigmoidal activation functions, etc.).\n\nGiven the detailed treatment of MASOs in [1], this paper is a logical continuation of this approach. As such, it may seem only incremental, but I would consider it as an important piece to ensure a solid foundation of the 'MASO-view' on deep neural networks.\n\nMy main criticism is with respect to the quality and clarity of the presentation. Without reading in detail [1] it is very difficult to understand the presented work here. Moreover, compared to [1], a lot of explanatory content is missing, e.g. [1] had nice visualisations of the resulting partitioning on toy data.\n\nClearly, this work and [1] belong together in a larger form (e.g. a journal article), I hope that this is considered by the authors.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}