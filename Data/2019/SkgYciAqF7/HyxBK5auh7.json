{"title": "An acceleration of PixelCNN", "review": "This paper describes a method to accelerate PixelCNN inference with limited loss by using a fast generator (lower quality) and its predicted quality to switch to the expensive PIXelCNN generation only when needed. The fast generator is trained to mimic the PixelCNN generator but, instead of generating the pixels sequentially, generates them in one shot. A confidence estimator is also trained to assign a score to pixels generated by the fast generator. If the score is low for a pixel, the system uses PixelCNN to generate the pixel instead. The fast generator is implemented with a UNET architecture  and uses a set of latent pixels that are independent of each other and can be used to generate the new pixel in one shot. \n\nExperiments are conducted using CelebA and CIFAR-10 datasets. Analysis is presented that 1) shows acceleration versus degradation by varying the confidence threshold. 2) show the confidence map versus the perceptual quality. The acceleration obtained ranges from 1x to 60x depending how aggressively the threshold is set. A perceptual score (FID - Frechet Inception Score) is used to measure the quality of generated images. The experiments show, surprisingly, that the FID follows a curve as the threshold is varied and the extrema is around 0.4. Thus the authors imply that their method can improve perceptual quality, which is nonsense. Indeed the FID score should use an Inception model trained on the same data to be meaningful. Also, a look at the generated images show substantial artifacts for e=0.r compared to e=1.0 (no skimming). Confidence maps also show a lot of raster line artifacts. These artifacts should be explained. Also, the authors hint at using anchor pixels (generated by PixelCNN) at the start of the generation loop to presumably improve the quality. Are the FID scores given in table 1 using anchors or not ? The acceleration bar graph in figure 4(a) is without anchors, so the FID scores should also be without anchors. Finally, the full training procedure is quite complicated and probably slow. There should be some discussion on that.\n\nThe article is technically sound. The citations are extensive. The math is well laid out and adequately referenced. The English is fine with some missing articles being the only issue. The figures are readable and well captioned.\n\nOverall, I find this paper not very significant. It proposes an acceleration for PixelCNN but the trade-off in quality is not well discussed. It would be better to compare classification accuracy of corrupted images fixed by the method, for example.  \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}