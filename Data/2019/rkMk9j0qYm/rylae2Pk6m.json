{"title": "Potentially interesting ideas but exposition and motivation are too imprecise", "review": "The authors suggest a novel approach, ExL to adversial training using multiplicate noise that is learned jointly with model parameters using SGD. They propose a likelihood framework to interpret why the approach is successful. PCA of intermediate layers is used to suggest that ExL trained NNs are more robust because they better cover space around the observed data manifold. Results on three canonical datasets using blackbox and whitebox adversarial attacks suggest that ExL can be helpful in defending against BB attacks, and is easily combined with other adversial training approaches such as PGD to further improve robustness. \n\nI think there are some interesting ideas in the paper but the motivation is too hand-wavey and the method exposition is insufficient. Firstly, refering to N as \"noise\" seems problematic. N is learned so is more akin to a latent variable than noise. In particular it is not random at any point apart from initialization (which is equally true for model parameters). Additionally it is very strange to me that these \"learned masks\" are fixed in position in the mini-batch, since an index in the minibatch has no external meaning. The authors don't say whether the data points are permuted at each epoch, which is important since it effects whether the same data point at subsequent epochs uses the same mask (I don't even know from the exposition whether this is intended or not). \n\nSecondly, the \"likelihood framework\" is very hand-wavey. When modeling P(Y|X) we think of Y and X as random variables, with the training set (X_train, Y_train) as samples. Nothing in this says that P(Y|X) should not hold for X other than X_train, e.g. X=A. Thus the introduction of P(Y|X,A) is superfluous. While Eq 1 is mathematically correct it is therefore pretty meaningless in terms of understanding what ExL is doing. The text does little to help. \n\nThirdly, \"Explainable\" in the title is dangerously close to \"interpretable\", which ExL is certainly not. The N give only the vaguest sense of the features being used in the data (see Fig 1b). \n\nThere might be Bayesian/likelihood-based interpretation of ExL, other than that proposed by the authors. I was surprised the similarity to dropout wasn't mentioned. Yarin Gal's work pointed out the similarity between dropout and variational Bayes with a specific variational posterior, and the ExL looks a look like opitmizing the resulting objective including per pixel dropout weights (although not stochastically which is strange). Alternatively one could think of X*N as latent variables and we are modeling P(Y|X*N). This could be interpreted as modeling noise in X for example. \n\nThe connection to dropout makes me suspicious that while ExL is proposed as an adversarial training method it is really just performing more effective regularization, which should in itself smooth the prediciton surface and improve robustness to adversarial examples. It would be feasible to test this, for example by comparing different formes of regularization with and without ExL. \n\nWhy threshold grad L < 0 when learning N? What stops the model from just setting N=0? \n\nMinor comments\n- Abstact: \"prove\" -> \"show\" (proofs require proofs!)\n- Don't re-use N as the number of training images. \n- page 5. X=X+alpha... ugh. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}