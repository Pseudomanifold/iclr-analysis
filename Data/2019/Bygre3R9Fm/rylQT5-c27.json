{"title": "Experiments and Writing Need Improvement", "review": "In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.\n\nStrength:\n\n1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. \n\n2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.\n\nWeakness:\n\n1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.\n\n2, Authors motive their work by saying in the abstract that \u201cother graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters\u201d. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.\n\n3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.\n\n4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?\n\n5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.\n\nTypos:\n\n1, There are a few references missing (question mark) in the first and second paragraphs of section 2.\n\n2, Methods in the experiment section are given without explicit reference, like GCPN.\n\n3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. \n\nOverall, I do not think this paper is ready for publishing and it could be improved significantly.\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nUpdate:\n\nThanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. \n\nHowever, I still found some claims made by authors problematic. \nFor example, it reads in the abstract that \"...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters...\". \nClearly, Li et al. 2018b has a differentiable formulation which falls under your description.\n\nBesides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. \nAlso, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.\nDirectly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.\n\nTherefore, I would like to keep my original rating. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}