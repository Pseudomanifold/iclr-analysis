{"title": "The paper could be more readable, the contribution is limited. The machine translation experiments should be compared with previous work on a benchmark setup. The generation experiments should consider obvious baseline such as sampling from a language model.", "review": "Bilingual GANs\n\n\nPaper Summary:\n\nThe paper proposes to learn a generative model of sentences leveraging machine translation.\nA first task learns a model which auto-encodes and translates sequences in a language agnostic continuous space (a first GAN objective penalizes including language information in the code). The second GAN task learns a generator that allows sampling could that cannot be distinguished with genuine sentence codes from the encoder of the first task. The first model is evaluated in a supervised and unsupervised translation setup. The second model is evaluated by generating text, measuring language model perplexity.\n\nReview:\n\nReadability:\n\nThis paper could be more readable. In particular, it is not easy to understand how the two models interact: do you share the encoder and decoder parameters between the two tasks? Does the generation task fine tune these parameters? The evaluation protocol is unclear: what is the difference between supervised and half supervised MT? I do not understand what BLEU means for unconditional generation. E.g. when you generate 40k sentence sentences from the Europarl model, you compute the precision of your prediction with respect to which reference?\n\n\nContribution:\n\nThe contribution in terms of (un)-supervised MT is limited at best compared to (Artetxe et al 2017,2018) or (Lample et al, 2017, 2018). In particular, I do not see any reason not to use the empirical setup used in these papers and compare your results with them. The experimental setup seems weak with only short sentences < 20 words, with a small vocabulary and no attempt to deal with unknown replacements or subword units.\n\nThe contribution for unconditional generation is more interesting as relying on a space defined by translation seems a good idea. But it is hard to assess. Again, the paper lacks comparison with baselines. In particular, sampling from a large language model to generate text seems an obvious baseline. It is also necessary to compare with a monolingual GAN (Zhao et al 2017), since it would quantify the benefit of the bilingual latent space.\n\nRelated Work:\n\nPrevious work is mischaracterized: Schwenk & Douze 2017, Lample et al 2018, Artexte et al 2017b are not relying on adversarial training. \n\nReview Summary:\n\nThe paper could be more readable, the contribution is limited. The machine translation experiments should be compared with previous work on a benchmark setup. The generation experiments should consider obvious baseline such as sampling from a language model.\n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}