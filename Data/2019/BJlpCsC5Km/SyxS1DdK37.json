{"title": "Review", "review": "This paper combines a number of ideas to train generative models with (deep) structured constraints. The general idea is similar to Flow-GAN, which learns a normalizing flow-based generator by optimizing the negative loglikelihood with an augmented GAN loss. However, It\u2019s difficult to impose prior structure information in the GAN framework. To address this problem, the authors proposed to minimize a so-called Gibbs-regularized variational bound of Jeffery divergence, which is the summation of KL and reverse KL divergence. The authors provide some justification that the Jeffery divergence works by yielding good mass-covering and mode-seeing properties. \n\nIt appears that the parameterization and adaptation of v throughout optimization is the key contribution of this work --- the technical details are not clear from the paper.\n\n1.    Typo in the training objective (Eq .1):  the second (or the first) \"sup\" should be removed? \n\n2.    Section 2.3 is very confusing. Particularly, how is the parameter \\phi introduced? What\u2019s the detailed update of \\phi? \n- \"We now observe that our methods can also be interpreted as a way of learning v as a Gibbs distribution approximating p.\" If v_\\phi(x) is a distribution, what\u2019s the parametric form of v?  \n- \"Generally, this is achieved by structuring the energy function V_\\phi:=\\log v_\\phi.\" It seems that V_\\phi(x) is a scalar-valued function that represents the negative energy of the distribution v_\\phi(x), however, why the distribution is self-normalized? Specifically, why \\int \\exp(V_\\phi) dx = 1? Otherwise, how the authors deal with the partition function \\int \\exp(V_\\phi(x)).  \n- It is unclear to me why the inner loop optimization is connected with Itakura-Saito divergence minimization? The authors may consider including the detailed proofs?\n\n3.    With the given description, the proposed algorithm is not easy to follow and implement by the reader. The paper would benefit from an Algorithm box with pseudocode.\n\nIf the authors can fully address the concerns above, I will consider changing the scores.  \n\nOther comments:\n1. The empirical results are fairly weak. Similar datasets are used, the authors may consider evaluating their approach on various different tasks. \n\n2. Duplicate citations \u2013 R2P2 [35] [36]\n\n3. Other related papers:\n - Belanger et al., End-to-End Learning for Structured Prediction Energy Networks, ICML 17\n- Tu et al., Learning Approximate Inference Networks for Structured Prediction, ICLR 18", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}