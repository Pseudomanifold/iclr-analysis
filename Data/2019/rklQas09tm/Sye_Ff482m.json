{"title": "Interpretability is important; but without human data we cannot evaluate it", "review": "This paper proposes an approach to introduce interpretability in NLP tasks involving text matching. However, the evaluation is not evaluated using human input, thus it is not clear whether the model is indeed meeting this important goal. Furthermore, there is no direct comparison against related work on the same topic, so it is not possible to assess the contributions over the state of the art on the topic. In more detail:\n\n- There are versions of attention mechanisms that are spare and differentiable. See here: \nFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\nAndr\u00e9 F. T. Martins, Ram\u00f3n Fernandez Astudillo \n\n- Why is \"rationalizing textual matching\" different than other approaches to explaining the predictions of a model? As far as I can tell, thresholding on existing attention would give the same output. I am not arguing that there is nothing different, but there should be a direct comparison, especially since eventually the method proposed is thresholded as well by limiting the number of highlights.\n\n- A key assumption in the paper is that the method identifies rationales that humans would find useful as explanations. However there is no evaluation of this assumption. For a recent example of how such human evaluation could be done see:\nD. Nguyen. Comparing automatic and human evaluation of local explanations for text classification. NAACL 2018\nhttp://www.dongnguyen.nl/publications/nguyen-naacl2018.pdf\n\n- I don't agree that explanations are sufficient if removing them doesn't degrade performance. While these two are related concepts, the quality of the explanation to a human is different to a system. In fact, more text can degrade performance when it is unrelated. See the experiments of this paper:\nAdversarial Examples for Evaluating Reading Comprehension Systems.\nRobin Jia and Percy Liang. EMNLP 2017: http://stanford.edu/~robinjia/pdf/emnlp2017-adversarial.pdf\n\n- Reducing the selection of rationales to sequence tagging eventually done as classification is suboptimal compared to work on submodular optimization (cited in the introduction) if being concise is important. A comparison is needed.\n\n- There is an argument that the training objective makes generated rationales corresponded and sufficient. This requires some evidence to support it.\n\n- What is the \"certificate of exclusion of unselected parts\" that the proposed method has?\n\n- An important argument is that the performance does not degrade. However there is no comparison against state of the art models to verfiy it.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}