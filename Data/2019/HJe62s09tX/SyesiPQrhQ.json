{"title": "Good work for multilingual embedding alignment", "review": "This is a work regarding the alignment of word embedding for multiple languages.Though there are existing works similar to this one, most of them are only considering a pair of two languages, resulting in the composition issue mentioned in this work. The authors proposed a way of using a regularization term to reduce such degraded accuracy and demonstrate the validity of the proposed algorithm via experiments. I find the work to be interesting and well written. Several points that I want to bring up:\n\n1. The language tree at the end of section 5 is very interesting. Does it change if the initialization/parameter is different?\n\n2. The matrix P in (1) is simply a standard permutation matrix. I think the definitions are redundant.\n\n3. The experiment results are expected since the algorithms are designed for better composition quality. An additional experiment, e.g. classification of instances in multiple languages, could further help demonstrate the strength of the proposed technic.\n\n4. How to choose the regularization parameter \\mu and what's the effect of \\mu?\n\n5. Some written issues like the notation of orthogonal matrix set, both \\mathcal{O} and \\mathbb{O} are used.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}