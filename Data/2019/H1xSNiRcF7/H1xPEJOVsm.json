{"title": "review", "review": "Post-rebuttal revision: All my concerns were adressed by the authors. This is a great paper and should be accepted.\n\n------\n\nThe paper presents smoothing probabilistic box embeddings with softplus functions, which make the optimization landscape continuous, while also presenting the theoretical background of the proposed method well. The paper presents the overall idea beautifully and is very easy to follow. The overall idea of smoothed sotfplus boxes is well-founded, elegant and practical. The results on standard WordNet do not improve upon state-of-the-art, however imbalanced WordNet with abundance of negative examples gain remarkable improvements. Similarly in Flickr and MovieLens the method performs well. This paper presents a novel, theoretically well-justified idea with excellent results, and is likely going to be a high-impact paper. \n\nAn illustrating figure would still be nice to include, also for the convolutions of eq 2. The paper does not comment on running times, some kind of scalability comparison should be included since the paper claims that the model is easier to train.\n\nThe paper should clarify that the \\prod in 3.3. meet and join definitions seems to refer to a set product, while the p(a) equation has a standard product (or does it?). What is the \u201ca\u201d in the p(a), should it be \"p(x)\u201d ? \n\nI have trouble understanding eq 1: the difference inside the function is always negative, while the hinge function seems to clip negative values away. The definition of the m(x) is too clever, please clarify the function in more conventional notation.  ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}