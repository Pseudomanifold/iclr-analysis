{"title": "clearly written, nice work", "review": "The authors focus on the selection problem of k statistically significant features discriminating 2 probability distributions accessible via samples. They propose a non-parametric approach under the PSI (post selection inference) umbrella using MMD (maximum mean discrepancy) as a discrepancy measure between probability distributions. The idea is to apply (asymptotically) normal MMD estimators, rephrase the top-k selection problem as a linear constraint, and reduce the problem to Lee et al., 2016. The efficiency of the approach is illustrated on toy examples and in GAN (generative adversarial network) context. The technique complements the PSI-based independence testing approach recently proposed by Yamada et al., 2018. \n\nThe submission is a well-organized, clearly written, nice contribution; it can be relevant to the machine learning community.\n\nBelow I enlist a few suggestions to improve the manuscript:\n-Section 1: The notion of characteristic kernel (kernel when MMD is metric) has not been defined, but it was referred to. 'Due to the mean embeddings in RKHS, all moment information is stored.': This sentence is somewhat vague.\n-Section 1: 'MMD can be computed in closed form'. This is rarely the case (except for e.g. Gaussian distributions with Gaussian or polynomial kernels). I assume that the authors wanted refer to the estimation of MMD.\n-Section 1: 'K nearest neighbor approaches (Poczos & Schneider, 2011)'. The citation to this specific estimator can go under alpha-divergences. The Wasserstein metric could also be mentioned.\n-Section 3.1: k is used to denote the number of selected features and also the kernel used in MMD. I suggest using different notations.\n-Theorem 1: '\\Phi is the CDF...'. There is no \\Phi in the theorem.\n-Section 3.2: The existence of MMD (mean embedding) requires certain assumptions: E_{x\\sim p}\\sqrt{k(x,x)} < \\infty, E_{x\\sim q}\\sqrt{k(x,x)} < \\infty.\n-Section 3.2.: block estimator: 'B_1 and B_2 are finite'. 'fixed'?\n-Section 3.2.: MMD_{inc}: \n   i) 'S_{n,k}': k looks superfluous.\n   ii) 'l': it has not been introduced (cardinality of D).\n-Section 3.3: typo: 'covraiance' (2x)\n-Section 3.3: Fan et al. 2013: The citation can go to \\citep{}.  \n-Theorem 2: \n   i)'c' is left undefined.\n   ii)Comma is missing before 'where'.\n   iii)\\xrightarrow{d} (Theorem 2, Corollary 3-4): Given that 'd' also denotes dimension in the submission, I suggest using a different notation for convergence in distribution.\n-At the introduction of block-MMD the block size (B) was fixed, while in the experiments (e.g. Figure 3) it is growing with the sample size (B=\\sqrt{n}). The assumption on B should be clearly stated.\n-Section 5.1: (b) mean shift: comma is missing before 'where'.\n-References: \n   i) Abbreviations and names in the titles should be capitalized (such as cramer, wasserstein, hilbert-schmidt, gan, nash). \n   ii) Scholkopf should be Sch\\{\"o}lkopf (in the ALT 2005 work).\n   iii) 'Exact post-selection inference, with application to the lasso': All the authors are listed; 'et al.' is not needed.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}