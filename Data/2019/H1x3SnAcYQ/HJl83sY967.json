{"title": "Interesting paper, could push it further", "review": "This paper extends the \"infinitely differentiable Monte Carlo gradient estimator\" (or DiCE) with a better control variate baseline for reducing the variance of the second order gradient estimates.\n\nThe paper is fairly clear and well written, and shows significant improvements on the tasks used in the DiCE paper.\n\nI think the paper would be a much stronger submission with the following improvements:\n\n- More explanation/intuition for how the authors came up with their new baseline (eq. (8)). As the paper currently reads, it feels as if it comes out of nowhere.\n- Some analysis of the variance of the two terms in the second derivative in eq. (11). In particular, it would be nice to show the variance of the two terms separately (for both DiCE and this paper), to show that the reduction in variance is isolated to the second term (I get that this must be the case, given the math, but would be nice to see some verification of this). Also I do not have good intuition for which of these two terms dominates the variance. \n- I appreciate that the authors tested their estimator on the same tasks as in the DiCE paper, which makes it easy to compare them. However, I think the paper would have much more impact if the authors could demonstrate that their estimator allows them to solve new, more difficult problems. Some of these potential applications are discussed in the introduction, it would be nice if the authors could demonstrate improvements in those domains.\n\nAs is, the paper is still a nice contribution.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}