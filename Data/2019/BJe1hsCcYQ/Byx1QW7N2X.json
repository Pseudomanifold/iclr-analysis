{"title": "An incremental work on hyperbolic embedding with Lorentzian Distance", "review": "This paper proposed an unsupervised embedding method for hierarchical or graph datasets. The embedding space is a hyperbolic space as in several recent works such as (Nickel & Kiela, 2017). The author(s) showed that using the proposed embedding, the optimization has better numerical stability and better performance.\n\nI am convinced of the correctness and the experiment results, and I appreciate that the paper is well written with interesting interpretations such as the demonstration of the centroid. However, the novelty of this contribution is limited and may not meet the publication standard of ICLR.  I suggest that the authors enhance the results and resubmit this work in a future venue.\n\nTheoretically, there are three theorems in section 3:\n\nTheorem 3.1 shows the coordinate transformation from the proposed parametrization to hyperboloid then to Poincare ball preserves the monotonicity of the Euclidean norm. This is straightforward by writing down the two transformations.\n\nLemma 3.2 and theorem 3.3 state the centroid in closed expression based on the Lorentzian distance, taking advantage that the  Lorentzian distance is in a bi-linear form (no need to take the arcosh()  therefore the analysis are much more simplified) These results are quite striaghtforward from the expression of the energy function.\n\nTheorem 3.4  (centroid computation with constraints) shows that minimizing the energy function\n$\\sum_{i} d_L^2 (x_i, a)$, when a is constrained to a discrete set, is equivalent to minimizing $d_L(c,a)$, where $c$ is given by Lemma 3.2.\nThis is interesting as compared to the previous two theorems, but it is not clear whether/how this equivalence is used in the proposed embedding.\n\nTechnically, there are three novel contributions,\n\n1. The proposed unconstrained reparametrization of the hyperboloid model does not require to project the embedding points onto the hyperbolic manifold in each update.\n\n2. The cost is based on the Lorentzian Distance, that is a monotonic transformation of the Riemannian distance (without taking the arccosh function). Therefore the similarity (a heat kernel applied on the modified distance function) is measured differently than the other works. Informally one can think it as t-SNE v.s. SNE which use different similarity measures in the target embedding.\n\n3. The authors discussed empirically the different choice of beta, which was typically chosen as beta=1 in previous works, showing that tunning the beta hyperparameter can give better embeddings.\n\nThese contributions are useful but incremental. Notably, (1) needs more experimental evidence (e.g. a toy example) to show the numerical instability of the other methods, and to show the learning curves of the proposed re-parametrization against the Riemannian stochastic gradient descent, which are not given in the paper.\n\nBy listing these theoretical and technical contributions, overall I find that most of these contributions are incremental and not significant enough for ICLR.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}