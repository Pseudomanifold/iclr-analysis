{"title": "Review - Lorentzian Distance Learning", "review": "The paper proposes a new approach to compute hyperbolic embeddings based on the squared Lorentzian distance. This choice of distance function is motivated by the observation that the ranking of these distances is equivalent to the ranking of the true hyperbolic distance (e.g., on the hyperboloid). For this reason, the paper proposes to use this distance function in combination with ranking losses as proposed by Nickel & Kiela (2017), as it might be easier to optimize. Moreover, the paper proposes to use Weierstrass coordinates as a representation for points on the hyperboloid.\n\nHyperbolic embeddings are a promising new research area that fits well into ICLR. Overall, the paper is written well and good to understand. It introduces interesting ideas that are promising to advance hyperbolic embeddings. However, in the current version of the paper, these ideas are not fully developed or their impact is unclear.\n\nFor instance, using Weierstrass coordinates as a representations seems to make sense, as it allows to use standard optimization methods without leaving the manifold. However, it is important to note that the optimization is still performed on a Riemannian manifold. For that reason, following the Riemannian gradient along geodesics would still require the exponential map. Moreover, optimization methods like Adam or SVRG are still not directly applicable. Therefore, it seems that the practical benefit of this representation is limited.\n\nSimilarly, being able to compute the centroid efficiently in closed form is indeed an interesting aspect of the proposed approach. Moreover, the paper explicitly connects the centroid to the least common ancestor of children in a\ntree, what could be very useful to derive new embedding methods. Unfortunately, this is advantage isn't really exploited in the paper. The approach taken in the paper simply uses the loss function of Nickel & Kiela (2017) and this loss doesn't make use of centroids, as the paper notes itself. The only use of the centroid seems then to justify the regularization method, i.e., that parents should have a smaller norm than their children. However, this insight alone seems not particularly novel, as the same insight can be derived for standard hyperbolic method and has, for instance, been discussed in Nickel & Kiela (2017, 2018), Ganea et al (2018), De Sa (2018). Using the centroid to derive new hyperbolic embeddings could be interesting, but, unfortunately, is currently not done in the paper.\n\nFurther comments\n- p.3: Projection back onto the Poincar\u00e9 ball/manifold is only necessary when\n  the exponential map isn't used. The methods of Nickel & Kiela (2018), Ganea et al (2018) therefore don't have this problem.\n- p.7: Since MR and MAP are ranking measures, and the ranking of distances between H^d and the L^2 distance should be identical, it is not clear to me why the experiments show significant differences for these methods when \\beta=1\n- p.7: Embeddings in the Poincar\u00e9 ball and the Hyperboloid are both compatible with the regularization method in eq.14 (using their respective norms). It would be interesting to also see results for these methods with regularization.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}