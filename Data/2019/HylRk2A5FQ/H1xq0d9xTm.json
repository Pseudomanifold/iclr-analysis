{"title": "A heuristically designed method for learning graph networks", "review": "This papers presents a supervised method to learn from network data. The method alternates two steps: a node embedding step (using convolutions) and an adjacency matrix update (using local convolutions or fully connected layers). These steps are stacked forming a NN that is used to represent the learning steps. The objective function is composed of a linear combination of typical losses such as cross-entropy, intersection over union and other regularization terms. The linear coefficients are treated as hyper-parameters. The methods are evaluated on graph generation and edge prediction tasks, showing results comparable to the state-of-the-art.\n\nOverall, the paper is clearly written and addresses an important problem. However, I found the proposed method rather heuristic and not very well theoretically principled. Why should one use the proposed architecture (stacking learning steps)? What is the latent structure that this method is trying to learn, a particular sequence of graphs? Which one? Where do the supposed benefits come from? In general, both the architecture and loss (or combination of losses) need to be better justified.\n\nRegarding experiments, on the positive side, the authors consider a representative set of methods. However, the tasks are too simple. I miss some sensitivity analysis, e.g., on the different loss functions or the number of layers. It is not clear how the method scales on the size of the networks and the depth of the layers.\n\nminor:\n\n- specify better how the ground truth is used in the objective\n- How was the noise added? uncorrelated noise over the features?\n- the loss function is referenced before being presented\n- \"set of node embedding\" -> \"set of node embedings\"", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}