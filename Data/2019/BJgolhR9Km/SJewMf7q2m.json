{"title": "Experiments are not convincing", "review": "This paper proposes an infinity norm variant of the RBF as the activation function of neural networks. The authors demonstrate that the proposed unit is less sensitive to the out-liar generated by adversarial attacks, and the experimental results on MNIST confirmed the robustness of the proposed method against several gradient-based attacks.\n\nIntuitively, the idea should work well against the features of adversarial examples which are far from the center of the cluster of \"normal\" features. However, the experiments are not convincing enough to show this point, and the entire method looks like a simple gradient mask technique. In my opinion, two types of experiments should be further considered:\n\n1. Pseudo-gradient-based attacks. Since the networks are trained using Pseudo gradients, all the attacks utilized in this paper should be pseudo-gradient-based as well.\n\n2. Black-Box attacks which do not rely on the information provided by gradients, such as transferable adversarial examples.\n\nFurthermore, the robustness revealed on the \"noise\" attack is interesting, I wish the authors could provide an analysis of the effects on feature distributions using different types of attacks.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}