{"title": "Interesting idea, but limited evaluation of effectiveness as a defense", "review": "Summary: The paper proposes a new architecture to defend against adversarial examples. The authors propose a network with new type of hidden units (RBFI units). They also provide a training algorithm to train such networks and evaluate the robustness of these models against different attacks in the literature. \n\nMain concern: I think the idea  proposed here of using RBFI units is very interesting and intuitive. As pointed out in the paper, the RBFI units make it difficult to train networks using standard gradient descent, because the gradients can be uninformative. They propose a new training algorithm based on \"pseudogradients\" to mitigate this problem. However, while evaluating the model against attacks, only gradient based attacks are used (like PGD attack of Madry et al., or Carlini and Wagner). It's natural to expect that since the gradients are uninformative, these attacks might fail. However, what if we considered similar \"pseudogradient\" based attacks? In particular, just use the same training procedure formulation to attack (where instead of minimizing loss like in training, we maximize loss)?\nI think this key experiment is missing in the paper and without this evaluation, it's hard to claim whether the models are more robust fundamentally, or it's just gradient masking. \n\n\nRevision: After the authors revision, I change my score since they addressed my main complaint about results using pseudogradient attacks \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}