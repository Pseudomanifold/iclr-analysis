{"title": "Improved graph representation for learning from programs", "review": "The submission presents an extension to the Allamanis et al ICLR'18 paper on learning from programs as graphs. The core contribution is the idea of introducing extra nodes and edges into the graph that correspond to (potentially rare) subwords used in the analyzed program code. Experiments show that this extended graph leads to better performance on two tasks, compared to a wide range of baseline methods.\n\nOverall, this is a nice paper with a small, incremental idea and substantial experiments that show its practical value. I only have minor comments / questions on the actual core content. However, the contribution is very incremental and of interest to a specialized subsegment of the ICLR audience, so it may be appropriate to reject the paper and redirect the authors to a more specialized venue.\n\nMinor comments:\n- There's a bunch of places where \\citep/\\citet are mixed up (e.g., second to last paragraph of page 2). It would make sense to go through the paper one more time to clean this up.\n- Sect. 4: I understand the need to introduce context, but it feels that more space should be spent on the actual contribution here (step 3). For example, it remains unclear why this extra nodes / edges are only introduced for subwords appearing in variables - why not also for field names / method names?\n- Sect. 5: It would be helpful if the authors would explicitly handle the code duplication problem (Lopes et al., OOPSLA'17), or discuss how they avoided these problems. Duplicated data files occurring in several folds are a significant risk to the validity of their experimental findings, and very common in code corpora.\n- Table 1: It is unclear to me what the \"Pointer Sentinel\" model can achieve. Without edges connecting the additional words to where they occur, it seems that this should not be performing different than \"Closed Vocab\", apart from noise introduced by additional nodes.\n- Table 1: Do Pointer Sentinel/GSC use a CharCNN to embed node labels of nodes that are not part of the \"cache\", or a closed vocabulary? [i.e., what's the embedding of a variable \"foo\"?] If not, what is the performance of the GSC model with CharCNN-embeddings everywhere? That would be architecturally simpler than the split variant, and so may be of interest.\n- Page 6: When truncating to 500 nodes per graph: How many graphs in your dataset are larger than that?\n- Page 7: Do you really use attention over all nodes, instead of only nodes corresponding to variables? How do you deal with results where the model picks a non-variable (e.g., a corresponding cache node)? Does this happen?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}