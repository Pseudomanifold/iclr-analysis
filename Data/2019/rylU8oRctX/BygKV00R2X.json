{"title": "An Interesting Comparative Study", "review": "This work compares different approaches to few-shots/semi-supervised learning. In particular, the work compares Siamese networks, virtual adversarial training (VAT) and generative adversarial networks (GAN). The authors use MNIST to perform various experiments that shed light on the performance of the compared approaches as the amount of labeled/unlabeled data varies as well as when there is a class distribution mismatch between labeled and unlabeled data. The results seem to suggest that with very little labeled and no additional unlabeled data Siamese networks perform the best, but when additional unlabeled data is available, VAT outperforms GAN in the regime with low amount of labeled. GAN overtakes VAT with moderate amount of labeled data. GANs also seem to suffer less with an increasing amount of class mismatch between labeled and unlabeled data.\n\nSignificance:\nI think that the paper provides a warranted empirical study about recent approaches to learning with little or weakly labeled data, which is of general relevance in the field\n\nPros:\nThe paper is well-written and easy to follow\nNice overview of recent approaches to few-shots/semi-supervised learning.\nDetailed empirical analysis covering a range of training data regimes\n\nCons:\nOnly one benchmark is used\nResults may also vary for different network architectures or application domains\n\nI in general found the paper interesting; however as mentioned, I find it hard to gauge the scope of the results presented here given that they are only provided for one benchmark and one network architecture.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}