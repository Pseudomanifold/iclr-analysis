{"title": "Very well-written paper that introduces to important innovations to the problem of interpreting black-box NNs", "review": "The paper is aimed at answering the following question: \"for model M, given an instance input and a predicted label, what parts of the input are most relevant for making the M choose the predicted label?\". \nThis is by far not the first paper aimed at answering this question, but it makes important innovations to the best of my knowledge. The most important one is proposing a stronger approach to the counterfactual question \"had this part of the input been different, what would have been the output?\". Because the input can be different in many ways, an important question is addressing in what specific way would it have been different. \n\nSpecifically in the domain of images, most models assume a blurring or simple local in-painting approach: \"if this patch were just a blurry average, what would have been the output?\". However, ss the current paper correctly points out, blurring or other simple in-painting methods leads to an image which is outside the manifold of natural images and outside the domain of the training set. This can lead to biased or inaccurate results. \n\nThe paper therefore propose two innovations on top of existing methods, most closely building on work by Fong & Vedaldi (2017): \n(1) Optimizing an inference network for discovering image regions which are most informative\n(2) Using a GAN to in-paint the proposed regions, leading to a much more natural image and a more meaningful counterfactual question.\n\nThe presentation is crisp, especially the pseudo-code in Figure 5. In addition, the paper includes several well-executed experiments assessing the contributions of different design choices on different metrics and making careful comparisons with several recent methods addressing the same problem. \n\nSpecific comments:\n\n1. In sec. 4.5, the comparison is not entirely fair because FIDO was already trained with CA-GAN, and therefore might be better adapted for it.\n2. Related to the point above: could one train BBMP with a CA-GAN in-painting model?\n3. I would have liked to see an ablation experiment where either one of the two innovations presented in this paper is missing.\n\n\nMinor:\n1. In eq. (2), wouldn't it be more accurate to denote it as \\phi(x,z,\\hat{x}) ? \n2. I would like to know the true labels for all the examples presented in the paper.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}