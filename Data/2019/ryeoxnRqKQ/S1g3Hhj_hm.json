{"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "review": "Summary: In this paper the authors discuss a black-box method to learn\nadversarial inputs to DNNs which are \"close\" to some nominal example\nbut nevertheless get misclassified. The algorithm essentially tries to\nlearn the mean of a joint Gaussian distribution over image\nperturbations so that the perturbed image has high likelihood of being\nmisclassified. The method takes the form of zero-th order gradient\nupdates on an objective measuring to what degree the perturbed example\nis misclassified. The authors test their method against 10 recent DNN\ndefense mechanisms, which showed higher attack-success rates than\nother methods. Additionally the authors looked at transferrability of\nthe learned adversarial examples.\n\nFeedback: As noted before, this paper shares many similarities with\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)\n\nand the authors have responded to those similarities in two follow-ups. I have reviewed these results and their \nmethod does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, \nmainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. \n\nI appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we \nalso see that many of the percentages in Figure 1 converge  to 1. On the one hand this suggests that all defense methods \nare in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \n\"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}