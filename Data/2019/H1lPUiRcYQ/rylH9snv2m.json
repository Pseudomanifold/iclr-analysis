{"title": "review", "review": "This paper presents a method to train NNs as black box estimators of the commitor function for a physical, statistical mechanical, distribution. This training is performed using samples from the distribution. As the committor function is used to understand transitions between modes of the distribution, it is important that the training samples include points between modes, which are often extremely low probability. To address this concern, this paper draws MCMC samples at a high temperature, and then uses importance weights when training the committor function using these samples. Overall -- this seemed like a good application paper. It applies largely off-the-shelf machine learning techniques to a problem in physics. I don't have enough background to judge the quality of the experimental results.\n\nI had one major concern: the approach in this paper is motivated as a solution to estimating commitor functions in high-d. The variance of importance sampling estimates typically increases exponentially in the dimensionality of the problem, so I suspect this technique as presented would fall apart quickly if pushed to higher dimensions. All experiments are on problems with either 9 or 10 (effective) degrees of freedom, which from the ML perspective at least is quite low dimensional, and which is consistent with this exponentially poor scaling. There are likely fixes to this problem -- e.g. the authors might want to look into annealed importance sampling*.\n\nmore specific comments:\n\n\"and dislocation dynamics\" -> \"dislocation dynamics\"\n\n\"One can easily check\" -> \"One can check\" :P\n\neq 5 -- this is very sudden deep water! Especially for an ML audience. You should either give more context for the Kolmogorov backward equation, or just drop it. (The Kolmogorov formulation of the problem is not used later, and for an ML audience describing the task in terms of it will confuse rather than clarify.)\nwhat is \\Delta q? Does that indicate the Laplacian? Not standard ML notation -- define.\n\nsimilarly, define what is intended by \\partial A and \\partial B (boundary of the respective regions?)\n\neq. 9 -- nit -- recommend using a symbol other than rho for regularization coefficient. visually resembles p, and is rarely used this way. lambda is very common.\n\neqs 10/11 -- include some text motivation for why the definition of chi explicitly excludes the regions inside A and B.\n\neq 14: cleverly formulated!\n\neq 14 / eq 20:\nfactor of 1000 is very fast! corresponds to an epsilon of O(1e-3). You need to make sure that training samples are generated in the epsilon width border around A and B, otherwise the effect of chi will be invisible when training q_theta. So it seems like epsilon should be chosen significantly larger than this. Might want to include some discussion of how to choose epsilon.\n\n* Totally incidental to the current context, but fascinatingly, annealed importance sampling turns out to be equivalent to the Jarzynski equality in nonequilibrium physics.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}