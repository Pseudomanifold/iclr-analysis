{"title": "Review", "review": "General:\nThe paper proposed an algorithm named Select Via Proxy(SVP), which can be used for data sampling. The idea is simple and straightforward: 1) use a proxy model to get decision boundary 2) train the large target model on the data points close to the decision boundary.\n\nStrength:\n1. Roughly this is a well-written paper. The main idea is quite clear to me.\n2. Empirical validation of the experiments looks good. The results show that SVP help reduce the training time with ResNet. The author(s) also showed the influence of different quantifying uncertainty methods.\n\nPossible Improvements:\n1. In Related Work, several previous works were mentioned. Although the author(s) claimed that SVP can be combined with them, it's better to show the performance of SVP compared with them. This would show the significance of the work.\n2. In the experiments, I was hoping to see how well SVP works on ImageNet. The problem is that: For ResNet152 and ResNet164, they are relatively too deep on such small data sets. Since the dimension of the data points(images) is not high, SVP can easily catch a reasonable decision boundary with a smaller model. I am almost sure ResNet20 is good enough to do this. I am more concerned about the situation where the capacity of the model is challenged by the size of the dataset. e.g. The data sets of autonomous driving are usually extremely large and even very deep models cannot be fully trained on that. \n3. The data points close to the decision boundary can be considered as tough data points, whose features might be hard to be caught by the model. If training model only on these data points, the trained model may just memorize tough data points and not learn the other data points from the data set. One solution is that, while training on tough data points, the model should also be trained on a small portion of well-learned data points. I don't think training only on the points close to the decision boundary is enough and was more expecting to see some discussion about this in the paper.\n\nConclusion:\nMy two biggest concerns are: 1) The algorithm is not tested on large data seta 2) The algorithm is not tested with the models of limited capacity. As a conclusion, I tend to vote for rejection.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}