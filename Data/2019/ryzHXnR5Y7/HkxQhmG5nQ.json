{"title": "Interesting approach to data selection, but needs comparative experiments", "review": "# Summary\nThe paper presents a method for identifying and selecting the most informative subset of the training dataset in order to reduce training time while maintaining test accuracy. The method consists of training a proxy model that is smaller and has been trained for fewer epochs, and which can optionally be ensembled. Experiments show promising results, indicating that some datasets can be reduced to half the size without impacting model performance.\n\n# Quality\nThe paper appears sound and of good quality. Background literature is cited and the proposed method is discussed in sufficient detail.\n\nI would, however, like to see some additional comparative experiments. All experiments are constructed to show that the method can indeed achieve accuracy comparable to the full model but with a smaller training set. I would like to see how it compares to existing strategies -- are there any reason to pick this method over existing ones?\nSince the last sentence in section 2 states that the proposed method is orthogonal to previous subsampling techniques, and therefore can be combined with any of them, it would be interesting to see how SVP compares to these and whether a combination of, say, SVP and importance sampling will in fact achieve better performance than the importance sampling on its own.\nAdditionally, given the model's high resemblance to active learning, it would be interesting to see it compared to some prominent active learning methods.\n\n# Clarity\nThe paper reads quite well. I particularly like the paragraph headlines, which makes it easy to get an overview of the paper.\n\nThe figures are generally nice and readable, except for figure 3, which I don't understand. Maybe I am missing it, but I can't find an explanation for what the rows and columns indicate, and the labels themselves should also be increased in size.\n\n# Originality\nI do not find the paper particularly novel. To me, the proposed method seems to be a variant of active learning, not orthogonal to this as it is claimed in section 2. The choice of surrogate model and uncertainty metric might be new, but the method itself boils down to uncertainty sampling, a well-known strategy in active learning.\nHowever, I am happy to change my mind if the authors can explain to me exactly how their method differs from active learning.\n\n# Significance\nWhile techniques for speeding up training without sacrificing performance are, of course, always interesting, I find the proposed method to be rather incremental and not significant enough for ICLR. It would be better suited as a workshop paper.\n\n# Other notes\nIn the last paragraph of section 1, you write that \"Our proposed framework is robust to the choice of proxy model architecture.\" I am not sure what you mean by this. Do you mean that one can choose any model as the proxy (which is clearly correct) or do you mean that the method is \"proxy agnostic\" in the sense that any proxy model will work better than no proxy? If the latter is the case, I would like some arguments for this. Also, if the method is indeed proxy agnostic, it should be possible to remove the proxy completely and select the data in some other way.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}