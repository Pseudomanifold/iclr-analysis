{"title": "Interesting paper and ideas, a bit low on results maybe", "review": "This paper generalizes scattering transform to graphs. It defines wavelets, scattering coefficients on graph signals. The experimental section describes their use in classification tasks with comparisons with recent methods. It seems scattering performs less well than SOTA methods, but has the advantages of not requiring any training so potentially good candidates for low data regimes application. Interesting and original paper and ideas being developed, but might be a tiny bit weak in term of results, both theoretical and experimental ?\n\nThere is not much theoretical results (mostly definition and hints that some of the results from euclidian case might generalize without formal investigation).\n\nRegarding the results, in particular table3, given that you use particular hyper parameters J and Q, for each dataset, this is arguably a bit of architectural overfitting ? Results would be more convincing IMO if obtained with a single set of hyper parameters. What was the procedure to come up with those parameters ?\n\nRegarding the methodology for training the classifier, I am not familiar with these datasets but using just a 1/10 of the data to train classifier seems a bit extreme ? \nHow about training each on 90% random subset of training set and averaging ? Or just the whole training subset ? That would still be fine in the sense that none of the classifier would have seen the test set ?\n\np2 '~it naturally extends to multiple signals by concatenating their scattering features~'\n\nP4 figure 1: Not very clear what those visualizations are. \\Psi_j is supposedly a n x n matrix so, is this \\Psi_j applied to a two different Dirac on the graph ? Would be good to clarify exactly what is being plotted in the legend.\n\nseems to be the biggest limitations of the proposed approach. By not early mixing of different features one might lose the high frequencies correlations between different signals defined on a single graph.\n\nP4. IMO capsule is not such a great name / already used in ML by Hinton's capsule etc... Why not simply 'moments' or 'statistics' ? \n\n'We can replace (3) with normalized moments of x ... how exactly do you normalize ? ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}