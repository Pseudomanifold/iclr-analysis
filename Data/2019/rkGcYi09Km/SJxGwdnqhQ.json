{"title": "Reasonable approach (but somewhat unsurprising) for a not entirely convincing problem", "review": "The authors introduce the problem of telegraphic summarization: given a sentence, we want to reduce its size while retaining its meaning, with no penalty for grammatical mistakes. The main application presented by the author is that of summarizing fictional stories and plays.\n\nThe setting proposed by the author prescribes that the summarized sentence can be obtained by the input sentence by dropping some words. So, for example, the simplest baseline for this problem would consist of simply dropping stop words.\n\nThe approach proposed is basically an auto-encoder, consisting of a 2-step encoder-decoder network: in the first step, the sentence is encoded into a vector which is in turn decoded to a (smooth) indicator vector to mask words in the sentence; in the second step, the masked sentence is encoded into a vector, which is in turn decoded into the output (summarized) sentence. \n\nThe optimization is a tradeoff between recoverability of the input sentence and norm of the indicator vector (how many words are dropped). In order for the network not to learn repetitive masking patterns (eg, drop first half of the sentence, or drop every other word), an additional loss is introduced, that penalizes keeping easily inferable words or dropping hard-to-infer words.\n\nConcerns:\n- the problem doesn't seem to be well-motivated. Also, the length of the obtained summarized sentences is ~70% that of the original sentences, which makes the summaries seem not very useful.\n- the proposed complex architecture seems not to justify the goal, especially considering that simply dropping stop words works already quite well. \n- In order for the presented architecture to beat the simple stop-words baseline, an additional loss (L4, linkage loss) with \"retention weights\" which need to be tuned manually (as hyper-parameters) is required. \n- there's not enough discussion about the related work by Malireddy et al, which is extremely similar to this paper. A good part of that work overlaps with this paper.\n- comparison with literature about abstractive summarization is completely missing.\n\nMinor comments:\n- Figure 1: Indicator Encoder should be Indicator Decoder.\n- Are negations part of your stop words? From your discussion, you should make sure that \"not\", \"don't\", \"doesn't\", ... do not belong to your stop word set.\n- How did you optimize the hyper-parameters r (desired compression), the regularization weights, and the retention weights?\n- Were pre-trained word embeddings used as initialization?\n- What's the average compression of golden sentences?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}