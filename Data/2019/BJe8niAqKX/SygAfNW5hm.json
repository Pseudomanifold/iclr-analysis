{"title": "Text-and-video based sentence representations", "review": "This submission proposes a model for sentence learning sentence representations that are grounded, based on associated video data. The model consists in a loss function composed of three terms. The first one is a text-only loss, for instance skip-thought. The second one pushes closer together representations of sentences that are describing the same video. The third one enforces a good correlation between similarity scored computed from text and video representations. The obtained representations are evaluated on standard transfer tasks for NLP and caption and image retrieval on COCO.\n\n- First of all, the proposed model has very weak performance, bellow a simple baseline such as average fasttext vector + logistic regression on tasks that are part of senteval. This raises doubts about the importance of grounding, as the baseline sentence representation does not even beat such a trivial baseline. Even for caption and image retrieval, better results could probably be obtained with a CCA between average word vectors and say VGG features.\n\n- Results in Table 4 are hard to parse. Please avoid reporting all possible variants of the model, and move these comparison to a separate ablation study. Moreover, the table lacks many simple baselines, which beat the proposed approach with no visual information whatsoever. \n\n- The related work section does not mention works on image or video captioning. Here is a loose list of at least a few models that cope with two modalities and would be worth mentioning, while the related work states that \"visual grounding of sentences is quite new\":\n- Barnard, Kobus, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. \"Matching words and pictures.\" Journal of machine learning research 3, no. Feb (2003): 1107-1135.\n- Hodosh, Micah, Peter Young, and Julia Hockenmaier. \"Framing image description as a ranking task: Data, models and evaluation metrics.\" Journal of Artificial Intelligence Research 47 (2013): 853-899.\n- Regneri, Michaela, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. \"Grounding action descriptions in videos.\" Transactions of the Association of Computational Linguistics 1 (2013): 25-36.\n- Socher, Richard, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. \"Grounded compositional semantics for finding and describing images with sentences.\" Transactions of the Association of Computational Linguistics 2, no. 1 (2014): 207-218.\n\nOverall, I think that the model proposed in this work is barely novel and the experimental results are very weak. The proposed approach can be outperformed by a very simple word-vector-based baseline. Moreover, the submission lacks a good discussion of work on joint modeling of visual and textual domains. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}