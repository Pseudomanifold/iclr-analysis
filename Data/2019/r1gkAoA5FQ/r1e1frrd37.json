{"title": "Interesting proposal to use discriminators to model coherence in NLG, but completely ignores prior work and presentation is confusing", "review": "The idea of training discriminators to determine coherence and cohesion, and training those discriminators as part of an NLG system using policy gradients, is an interesting one. However, there are two major problems with the papers as it stands:\n\n1) it completely ignores the decades of NLG literature on this topic before the \"neural revolution\" in NLP;\n2) the presentation of the paper is confusing, in a number of respects (some details below).\n\nTo claim that this is the first paper to capture cross-sentence linguistic properties for text generation is the sort of comment that is likely to make experienced NLG researchers very grumpy. A good place to start looking at the extensive literature on this topic is the following paper:\n\nModeling Local Coherence: An Entity-Based Approach, Barzilay and Lapata (2007)\n\nOne aspect in which the presentation is muddled is the order of the results tables. Table 2 is far too early in the paper. I had no idea at that point why the retrieval results were being presented (or what the numbers meant). You also have cohesion in the table before the cohesion section in 3.2. Likewise, Table 1, which is on p.2 and gives examples of system output, is far too early.\n\nPerhaps the biggest confusion for me was the difference between cohesion and coherence, and in particular how they are modeled. The intro does a good job of describing the two concepts, and making the contrast between local and global coherence, but when I was reading 3.1 I kept thinking this was describing cohesion (\"T that follows S in the data\" - sounds local, no?). And then 3.2 seems to suggest that coherence and cohesion essentially are being modeled in the same way, except shuffling happens on the word level? I suppose what I was expecting was some attempt at a global model for coherence which goes beyond just looking at consecutive sentence pairs.\n\nI wonder why you didn't try a sequence model of sentences (eg bidirectional LSTM). These are so standard now it seems odd not to have them.\n\nDo you describe the decoding procedure (greedy? beam?) at test time anywhere?\n\nI liked Table 4 and found the example pairs with the scores to be useful qualitative analysis.\n\n\"Based on automated NLP metrics, we showed a significant improvement\" - which metrics? not clear to me that the improvements in Table 3 are significant.\n\nMinor presentation points\n--\n\n\"followed by a logically sound sentence\" - might want to rephrase this, since you don't mean logical soundness in a technical sense here (I don't think).\n\nThe comment in the conclusion about being \"convinced\" the architecture generalizes well to unseen texts is irrelevant without some evidence.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}