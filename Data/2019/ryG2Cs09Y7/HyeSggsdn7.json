{"title": "FEATURE PRIORITIZATION AND REGULARIZATION IMPROVE STANDARD ACCURACY AND ADVERSARIAL ROBUSTNESS", "review": "Summary: This paper argues that improved resistance to adversarial\nattacks can be achieved by an implicit denoising method in which model\nweights learned during adversarial training are encouraged to stay\nclose to a set of reference weights using the ell_2\npenalty. Additionally, the authors claim that by introducing an\nattention model which focuses the model training on more robust\nfeatures they can further improve performance. Some experiments are\nprovided.\n\nFeedback: My main concerns with the paper are:\n\n* The experimental section is fairly thin. There are at this point a\n  large number of defense methods, of which Madry et al. is only one. In\n  light of these, the experimental section should be expanded. The\n  results should ideally be reported with error bars, which would help\n  in gauging significance of the results.\n\n* The differential impact of the two contributions is not entirely\n  clear. The results in Table 1 suggest that implicit denoising can\n  help, yet at the same time, Table 2 suggests that Black-box\n  performance is better if we just use the attention model. Overall,\n  this conflates the contributions unnecessarily and makes it hard to\n  distingish their individual impact.\n\n* The section on gradient maps is not clear. The authors argue that if\n  the gradient map aligns with the image the model depends solely on\n  the robust features. While this may be (somewhat more) intuitive in\n  the context of simple GLMs, it's not clear why it should carry over\n  to DNNs. I think it would help to make these intuitions much more\n  precise. Secondly, even if this were the case, the methodology of\n  using a neural net to classify gradient maps and from this derive a\n  robustness metric raises precisely the kinds of robustness questions\n  that the paper tries to answer. I.e.: how robust is the neural net\n  classifying the gradient images, and how meaningful are it's\n  predictions when gradient maps deviate from \"clean\" images.\n\nOverall, I feel this paper has some potentially interesting ideas, but\nneeds additional work before it is ready for publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}