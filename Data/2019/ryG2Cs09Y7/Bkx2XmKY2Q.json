{"title": "Lack of valid explanation, and insufficient experiment", "review": "This paper studies adversarial training of robust classification models. It is based on PGD training in [madry17]. It proposes two points: 1) add attention schemes, 2) add a feature regularization loss. The results on MNIST and CIFAR10 demonstrate the effectiveness. At last, it did some diagnostic study and visualization on the attention maps and gradient maps.\n\n1. Can you provide detailed explanations/intuitions why attention will help train a more robust models?\n\n2. Two related adversarial training papers are missing \"Ensemble Adversarial Training\" (ICLR2018) and \"Adversarial Logit Pairing\" (ICML2018). Also, feature (logit) regularization has been studied in ALP paper on ImageNet.\n\n3. For Table 2 on CIFAR10, I would like to see PGD20 (iterations) + 2 (step size in pixels), PGD100 + 2 and PGD200 + 2. Also, I am interested in seeing CW loss which is based on logit margin. \n\n4. I would like to see results using the \"wide\" model in [madry17] paper for ALP and LRM. I think results from large-capacity models are more convincing.\n\n5. I would like to see results on CIFAR100, which is a harder dataset, 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for justification nowadays (maybe enough one year ago). Since ImageNet is,  to some extent, computationally impossible for schools, I want to see the justification results on CIFAR100.\n\n##### Post-rebuttal\n\nI appreciate the additional results in the rebuttal. I raise the score but it is still slightly below the acceptance. The reasons are 1) incremental novelty; 2) insufficient experiments. Also, I found in table 3 that, the larger-capacity model is less robust than the smaller-capacity model against white-box iterative attacks? This is strange.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}