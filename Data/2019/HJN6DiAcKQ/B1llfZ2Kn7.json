{"title": "A new dataset for image captioning", "review": "This paper presents a new image captioning dataset. The image captions were created in the context of a \"personality\" prompt, e.g. \"happy\" or \"sad\". The dataset is used to train personality-aware image captioning models and caption retrieval models. The models are standard image captioning models or image--sentence ranking models with additional encodings of the personality trait. I am sympathetic to the argument that existing resources for image captioning make it a \"neutral\" language generation task, and this dataset may be useful for future research in image captioning. But most of the details of the new dataset are relegated to the appendices, and most of the results are devoted to the value of using pre-trained embeddings or CNNs.\n\nExperiments are conducted on the COCO dataset and the the personality captions dataset. The experiments also focus on the role of using more complex image recogniiton models. The results show that using a better visual recognition model leads to improvements in captioning and retrieval performance. This is not surprising and the same basic result has been down in many papers, going back to Donahue et al. (CVPR 2015), who showed that using VGG was better than using CaffeNet on captioning and retrieval tasks.\n\nThe human evaluation is commendable but there are insufficient details about how you carried out the experiment. Did you conduct this experiment on a crowd-sourcing platform? How reliable are the results of five human annotators? The difference is claimed to be a \"statstically significant result\", so please present the test statistic in the paper.\n\nWhat exactly does it mean that you \"learn an embedding for each train and concatenation it with each input of the encoder\"? Is the personality trait input to the encoder at every timestep? Is it also used to initialise the hidden state of the encoder? Please provide more details.\n\nThere are many references to the language generation model as a \"generative\" model. I was confused by this term, which I expected to be used to describe an actual generative model of p(x, y), whereas your captioning models estimate p(y|x).\n\nThe last sentence of Section 2 is missing a citation \"... large-scale pretraining (?) ...\"", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}