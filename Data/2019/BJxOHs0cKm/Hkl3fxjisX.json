{"title": "An extension of Neyshabur et al. PAC-Bayes bounds.", "review": "The authors prove a PAC-Bayes bound on a perturbed deterministic classifier in terms of the Lipschitz constant of the Hessian. They claim their bound suggests how insensitive the classifier is to perturbations in certain directions. \n\nThe authors also \u201cextract\u201d from the bound a complexity measure for a particular classifier, that depends on the local properties of the empirical risk surface: the diagonal entries of the Hessian, the smoothness parameter of the Hessian, and the radius of the ball being considered.  The authors call this \u201cmetric\u201d \u201cPAC-Bayes Generalization metric\u201d, or pacGen.\n\nOverall, this seems like a trivial extension of Neyshabur et al. PAC-Bayes bounds. \n\nThe experiments demonstrating that pacGen more or less tracks the generalization error of networks trained on MNIST dataset is not really surprising. Many quantities track the generalization error (see some of Bartlett\u2019s, Srebro\u2019s, Arora\u2019s work). In fact, these other quantities track it more accurately. Based on Figure 2, it seems that pacGen only roughly follows the right \u201corder\u201d of networks generalizing better than others. If pacGen is somehow superior to other quantities, why not to evaluate the actual bound? Or why not to show that it at least tracks the generalization error better than other quantities?\n\nThe introduction is not only poorly written, but many of the statements are questionable. Par 2: What complexity are you talking about? What exactly is being contradicted by the empirical evidence that over-parametrized models generalize? \n\nRegarding the comment in the introduction: \u201c Dinh et al later points out that most of the Hessian-based sharpness measures are problematic and cannot be applied directly to explain generalization.\u201d, and regarding the whole Section 5, where the authors argue that their bound would not grow much due to reparametrization:\nIf one obtains a bound that depends on the \u201cflatness\u201d of the minima, the bound might still be useful for the networks obtained by SGD (or other algorithms used in practice). The fact that Dinh et al. paper demonstrates that one can artificially reparametrize and change the landscape of a specific classifier does not contradict any generalization bounds that rely on SGD finding flat minima. Dinh et al. did not show that SGD finds classifiers in a sharp(er) minima that generalize (better).\n\nIn the experiment section, the authors compare train and test errors of perturbed (where the perturbation is based on the Hessian) and unperturbed classifiers. However, they don't compare their results to other type of perturbations, e.g. dropout. It\u2019s been shown in previous work that certain perturbations improve generalization and test error.\n\nThere are numerous typos throughout the paper.\n\n\n****************\n\n[UPDATE]\n\nI would like to thank the authors for implementing the changes and adding a plot comparing their algorithm with dropout. While the quality of the paper has improved, I think that the connection between the perturbation level and the Hessian is quite obvious. While it is a contribution to make this connection rigorous, I believe that it is not enough for a publication. Therefore, I recommend a rejection and I hope that the authors will either extend their theoretical or empirical analysis before resubmitting to other venues.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}