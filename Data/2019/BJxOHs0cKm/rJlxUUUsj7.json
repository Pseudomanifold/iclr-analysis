{"title": "The paper does some extensive calculations but is weak on qualitative insights and empirical evaluation.", "review": "This paper gives various PAC-Bayesian generalization guarantees and some\nempirical results on parameter perturbation in training using an algorithm\nmotivated by the theory.\n\nThe fundamental issue addressed in this paper is whether parameter\nperturbation during training improves generalization and, if so, what\ntheoretical basis exists for this phenomenon.  For continuously\nparameterized models, PAC-Bayesian bounds are fundamentally based on\nparameter perturbation (non-singular posteriors).  So PAC-Bayesian\ntheory is naturally tied to parameter perturbation issues.  A more\nrefined question is whether the size of the perturbation should be\ndone on a per-parameter bases and whether per-parameter noise levels\nshould be adaptive --- should the appropriate noise level for each\nparameter be adjusted on the basis of statistics in the training data.\nAdam and RMS-prop both adapt per-parameter learning rate eta_i to be\nproportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running\nestimate of the expectation over a draw of a training point of the\nsquare of the gradient of the loss with respect to parameter i.  At\nthe end of the day, this paper, based on PAC-Bayesian analysis,\nproposes that a very similar adaptation be made to per-parameter noise\nduring training but where E g_i^2 is replaced by the RMS value \\sqrt{E\ng_i^2}.  It seems that all theoretical analyses require the square\nroot --- the units need to work.  A fundamental theoretical question,\nperhaps unrelated to this paper, is why in learning rate adaptation the\nsquare root hurts the performance.\n\nThis paper can be evaluated on both theoretical and empirical grounds.\nAt a theoretical level I have several complaints.  First, the\ntheoretical analysis seem fairly mechanical and without theoretical\ninnovation. Second, the analysis obscures the prior being used (the\nlearning bias). The paper first states an assumption that each\nparameter is a-priori taken to be uniform over |w_i| <= \\tau_i and the\nKL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where\nsigma_i is the width of a uniform posterior over a smaller interval.\nBut later they say that they approximate tau_i by |w_i| + kappa_i with\nkappa_i = \\gamma |w_i| + epsilon.  I believe this works out to be\nessentially a log-uniform prior on |w_i| (over some finite range of\nlog |w_i|).  This seems quite reasonable but should be made explicit.\n\nThe paper ignores the possibility that the prior should be centered at\nthe random initialization of the parameters.  This was found to be\nessential in Dziugaite and Roy and completely changes the dependence\nof k_i on w_i.\n\nAnother complaint is that the Hoefding bound is very loose in cases\nwhere the emperical loss is small compared to its upper bound.  The\nanalysis can be more intuitively related to practice by avoiding the\nrescaling of the loss into the interval [0,1] and writing expressions\nin terms of a maximum bound on the loss L_max.  When hat{L} << L_max\n(almost always the case in practice) the relative Chernoff bound is\nmuch tighter and significantly alters the analysis.  See McAllester's\nPAC-Bayesian tutorial.\n\nThe theoretical discussion on re-parameterization misses an important\npoint, in my opinoin, relative to the need to impose a learning bias\n(the no-free-lunch theorem).  All L_2 generalization bounds can be\ninterpreted in terms of a Gaussian prior on the parameters.  In all\nsuch cases the prior (the learning bias) is not invariant to\nre-parameterization.  All L_2 generalization bounds are subject to the\nsame re-parameterization criticism.  A prior tied to a particular\nparameterization is standard practice in machine learning for in all\nL_2 generalization bounds, including SVMs.  I do think that a\nlog-uniform prior (rather than a Gaussian prior) is superior and\ngreatly reduces sensitivity to re-parameterization as noted by the\nauthors (extremely indirectly).\n\nI did not find the empirical results to very useful.  The value of\nparameter perturbation in training remains an open question. Although\nit is rarely done in practice today, it is an important fundamental\nquestion. A much more thorough investigation is needed before any\nconclusions can be drawn with confidence. Experimentation with\nperturbation methods would seem more informative than theory given the\ncurrent state of the art in relating theory to practice.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}