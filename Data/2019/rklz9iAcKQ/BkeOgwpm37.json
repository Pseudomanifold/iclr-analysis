{"title": "Alternative information-theoretic objective for unsupervised graph representation learning", "review": "This paper adapts the Deep Informax (DIM; Hjelm et al. 2018) method, which was used on\nimage data, into the graph domain. The architecture of the neural network and\nthe learning cost function are given by figure 1 and eq.(1), respectively.\n\nThe idea is to maximize the mutual information between a local representation\n(of a \"patch\" defined by graph adjacency) and a global representation (of the entire graph),\nso those different local patches are encouraged to carry some shared\nglobal information.\n\nThis is in contrast to most unsupervised graph encoders, where the objective is\nto fit the random walk similarities (node adjacency on the graph).\n\nIn an unsupervised learning scenario, where the graph structure and node features\nare given, the authors achieved state-of-the-art performance on transductive and\ninductive node classification tasks, in some cases even better than supervised baselines. \n\nThe paper is well written. I recommend acceptance and have the following concerns.\n\nMain comment 1\n\nThe title suggests that there are some information theory contents. \nHowever, section 3 does not include much information theory.\nRather, the author(s) directly give eq.(1) with pointers to references and informal discussions.\nThis is not so helpful. It is not straightforward for\nthe reader to relate eq.(1) with the definition of mutual information.\nIdeally, before eq.(1) there should be one or two equations (with text)\nto introduce the Jesen-Shannon MI estimation and information theoretic bounds etc.\n\nOverall, due to this, the contribution is mainly on adapting the DIM method info the graph domain. Although the experimental results are good, there is not much theoretical insight or \"recreative\" introduction of the DIM method from the authors' perspectives. This is the main reason for that it is not a strong accept.\n\nMain comment 2\n\nA motivation of the proposition is to \"not rely on random walks\", or graph node adjacency.\nNotice that random walks can be intuitively regarded as higher order node adjacency.\nHowever, the encoder, which is based on GCN, does rely on the adjacency matrix,\nas the convolution is done in local neighborhoods (that can also be defined based on\nrandom-walk similarities). The authors are therefore suggested to make it\nclear in related places that, it is the cost function which is not based\non node adjacency, although the neural network structure does rely on it.\n\nAs a related question, in the inductive experiments, in the mini-batch of 256 nodes\nrandomly selected, or selected by a local patch of the graph which is connected or nearby?\nIf it is the latter case, the cost function does rely on random-walk similarities,\nas the summary vector will be a local patch average.\n\nQuestions:\n\n-The summary vector is the average of all node features. On large graphs, the\naverage may carry less information as compared to small graphs. It can be\nobserved that on Pubmed and Reddit, the performance improvement is not as\nhigh as the other small graphs. Could you comment on this?\n\n-In the baseline \"DeepWalk+features\", are the two different types of features directly concatenated?\n\n-Is it straightforward to apply DGI to link prediction tasks?\n\n-It that a concern that the random corruption function will cause a high variance of the gradient?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}