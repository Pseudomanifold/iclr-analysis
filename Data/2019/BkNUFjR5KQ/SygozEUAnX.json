{"title": "This paper proposes a neural network architectures with locally dense and globally sparse connections. Using dense units a population-based evolutionary algorithm is used to find the sparse connections between modules.", "review": "The problem is of increasing practical interest and importance. \n\nThe ablation study on the contribution and effects of each constituent  part is a strong part of the experiment section and the paper. \n\nOne major concern is about the novelty of the work. There are many similar works under the umbrella of Neural Architecture search who are trying to connect different building blocks (modules) to build larger CNNs. One example that explicitly makes sparse connections between them is [1]. Other examples of very similar works are [2,3,4].\n\nThe presentation of the paper can be improved a lot. In the current setup it\u2019s very similar to a collection of ideas and tricks and techniques combined together. \n\nThere are some typos and errors in the writing. A thorough grammatical  proofreading is necessary. \n\nIn conclusion there is a claim about tackling overfitting. It\u2019s not well supported or discussed in the experiments. \n\n[1] Shazeer, Noam, et al. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\" arXiv preprint arXiv:1701.06538 (2017).\n[2] Xie, Lingxi, and Alan L. Yuille. \"Genetic CNN.\" ICCV. 2017.\n[3] Real, Esteban, et al. \"Large-scale evolution of image classifiers.\" arXiv preprint arXiv:1703.01041 (2017).\n[4] Liu, Hanxiao, et al. \"Hierarchical representations for efficient architecture search.\" arXiv preprint arXiv:1711.00436 (2017).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}