{"title": "Interesting paper", "review": "This paper studies the effect of batch normalization via a physics style mean-field theory. The theory yields a prediction of maximal learning rate for fully-connected and convolutional networks, and experimentally the max learning rate agrees very well with the theoretical prediction.\n\nThis is a well-written paper with a clean, novel result: when we fix the BatchNorm parameter \\gamma, a smaller \\gamma stabilizes the training better (allowing a greater range of learning rates). Though in practice the BatchNorm parameters are also trained, this result may suggest using a smaller initialization. \n\nA couple of things I was wondering:\n\n-- As a baseline, how would the max learning rate behave without BatchNorm? Would the theories again match the experimental result there?\n\n-- Is the presence of momentum important? If I set the momentum to be zero, it does not change the theory about the Fisher information and only affects the dependence of \\eta on the Fisher information. In this case would the theory still match the experiments?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}