{"title": "Scheme to generalize Q values across policies and tasks, by combining universal value function approximation and successor features+generalized policy improvement", "review": "The goal here is multi-task learning and generalization, assuming that the expected one-step reward for any member of the task family can be written as $\\phi(s,a,s')^T w$. The authors propose universal successor features (USF) $\\psi$s, such that the action-value functions Q can be written as $Q(s,a,w,z)=\\psi(s,a,z)^T w$, generalizing over mutiple tasks each denoted by $w$, and multiple policies each denoted by $z$. Here, $z$ represents the optimal policy induced by a reward specified by $z$ (from the same set as $w$). Using USFs $\\psi$-s, the Q values can be interpolated across policies and tasks. Due to the disentangling of reward and policy generalizations, the training sets for $w$ and $z$ can be independently sampled. The authors further generalize a temporal difference error in these USFs $\\psi$s, using the TD error to learn to approximate the $\\psi$s by a network (USF Approximator i.e USFA). They then test the generalization capabilities of these USFAs on families of a simple task and a DeepMind Lab based task.\n\nI find this paper a good fit for ICLR as the paper significantly advances learning representations for Q values that generalize across policies and tasks.\n\nSome issues to consider:\n1. Given a policy, I would think that the reward function that induces this policy is not unique. This non-uniqueness probably doesn't matter for the USF development, since the policies are restricted to those induced by z-s (from the same set as w-s), but the authors should clarify this point.\n\n2. I suppose there are no convergence guarantees on the $\\psi$-learning?\n\n3. I do believe that this work goes reasonably beyond the Ma et al 2018 paper, and the authors do clarify their advance especially in incorporating generalized policy improvement. However, the authors way of writing makes it appear as if their work only differs in some details. I recommend to remove this unexplanatory sentence:\n\"Although this work is superficially similar to ours, it differs a lot in the details.\"\n\nMinor:\npage 3: last but one line: \"more clear\" --> \"clearer\"\npage 3: \"In contrast\" --> \"By contrast\" -- but this is not a hard rule\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}