{"title": "Emergent Language is Easier than Natural Language", "review": "The primary contribution of this work is a dataset for action following through dialogue.  The authors collect a comparatively small dataset in terms of language but one which contains real images and dialogue. \n\nThere are a number of aspects of the proposed approach which I found hard to follow/justify.  First off, I was unclear on the details of the collected data (e.g. average action sequence length, dialogue length, lexical types/tokens, etc).  There's a claim of 62 acts which sums both dialogue and actions with averages of 8 and 9 dialogue acts for tourist/guide implying 45 move actions?  on a 4x4 grid?  Is it safe therefore to assume that the example dialogue is therefore atypical? It's very hard to figure out based on the number of steps across the different tables what the model should be aiming for.  Also, in 2.3 does the claim that they \"successfully complete the task\" mean in the 76.74% of cases where they succeed or did they succeed in 100% of cases and then a new human eval was run afterwards which performed worse?\n\nThe primary modeling result appears to be the success of emergent language and the bold claim that humans are bad at localizing.  This doesn't feel intuitively true from the example dialogue, but the NLG system samples to appear to be quite bad which makes me worried that it's not so much that humans are bad localizers but that the model's NLU/NLG system is quite weak and maybe there's a problem with the data-collection procedure.  Additional justification and analysis would be appreciated.\n\nAs I understand the paper right now:\n1. Humans talking to one another do very well on the task and achieve success very quickly. \n2. Emergent language can do better at the task though their approach is very sub-optimal (requiring 2-3x the number of steps).\n3. The currently proposed NLU/NLG mechanisms are very weak and cannot produce or correctly interpret actual language.\n\nThere are many moving pieces in this paper (e.g. extracting text from images vs detections), there doesn't seem to be any pretraining of the decoder, etc which makes it very hard for me to understand what's going wrong.  The results in this paper, don't convince me that emergent language is better than natural language or that agents are better communicators than humans, but that the data-collection methodology was faulty leading to lots of failures. \n\nI haven't touched on the MASC aspect and how this compares to existing work on interpretable spatial relations and questions as to why various architectural choices were made though the paper would obviously benefit from that discussion as well.\n\nI found this paper very confusing to read.  It relies heavily on 11 pages of appendices (where it puts all of the related work) and still fails to clearly explain its contributions or justify its claims.  \n\nMinor: URLs intermittently anonymized page 12 vs 19", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}