{"title": "official review", "review": "The authors proposed an AutoLoss controller that can learn to take actions of updating different parameters and using different loss functions.\n\nPros\n1. Propose a unified framework for different loss objectives and parameters.\n2. An interesting idea in meta learning for learning loss objectives/schedule.\n\nCons: \n1. The formulation uses REINFORCE, which is often known with high variance. Are the results averaged across different runs? Can you show the variance? It is hard to understand the results without discussing it. The sample complexity should be also higher than traditional approaches.\n2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?\n3. Why do you set S=1 in the experiments? What\u2019s the importance of S?\n4. I think it is quite surprising the AutoLoss can resolve mode collapse in GANs. I think more analysis is needed to support this claim. \n5. The evaluation metric of multi-task MT is quite weird. Normally people report BLEU, whereas the authors use PPL. \n6. According to https://github.com/pfnet-research/chainer-gan-lib, I think the bested reported DCGAN results is not 6.16 on CIFAR-10 and people still found other tricks such as spectral-norm is needed to prevent mode-collapse. \n\nMinor: \n1. The usage of footnote 2 is incorrect.\n2. In references, some words should be capitalized properly such as gan->GAN.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}