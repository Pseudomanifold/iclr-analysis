{"title": "Relevant topic, interesting formulation, not clear what is the benefit of \"good\" representation", "review": "-- Summary --\n\nThe authors proposes a novel approach in learning a representation for HRL. They define the notion of sub-optimality of a representation (a mapping from observation space to goal space) for a goal-conditioned HRL that measure the loss in the value as a result of using a representation. Authors then state an intriguing connection between representation learning and bounding the sub-optimality which results in a gradient based algorithm. \n\n-- Clarity --\n\nThe paper is very well written and easy to follow.\n\n-- novelty --\nTo the best of my knowledge, this is the first paper that formalizes a framework for sub-optimality of goal-conditioned HRL and I think this is the main contribution of the paper, that might have lasting effect in the field. This works mainly builds on top of Nachum et al 2018 for Data Efficient HRL.\n\n-- Questions and Concerns --\n\n[1] Authors discussed the quality of the representation by comparing to some near optimal representation (x,y) in section 7. My main concern is how really \u201cgood\u201d is the representation?, being able to recover a representation like x,y location is very impressive, but  I think of a \u201cgood\u201d representation as either a mapping that can be generalized or facilitate the learning (reducing the sample complexity). For example Figure 3 shows the performance after 10M steps, I would like to see a learning curve and comparison to previous works like Nachum et al 2018 and see if this approach resulted in a better (in term of sample complexity) algorithm than HIRO. Or an experiment that show the learned representation can be generalized to other tasks.\n\n[2] What are author's insight toward low level objective function? (equation 5 for example) I think there could be more discussion about equation 5 and why this is a good objective to optimize. For example third term is an entropy of the next state given actions, which will be zero in the case of deterministic environment, so the objective is incentivizing for actions that reduce the distance, and also has more deterministic outcome (it\u2019s kind of like empowerment - klyubin 2015). I\u2019m not sure about the prior term, would be great to hear authors thoughts on that. (a connection between MI is partially answering that question but I think would be helpful to add more discussion about this)\n\n[3] Distance function : In section C1 authors mention that they used L2 distance function for agent\u2019s reward. That\u2019s a natural choice, although would be great to study the effect of distance functions. But my main concern is that author's main claim of a good representation quality is the fact that they recovered similar to XY representation, but that might simply be an artifact of the distance function, and that (x,y) can imitate reward very well. I am curious to see what the learned representation would be with a different distance function.\n\n-- \nI think the paper is strong, and interesting however i\u2019d like to hear authors thoughts on [2], and addressing [1] and [3] would make the paper much stronger.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}