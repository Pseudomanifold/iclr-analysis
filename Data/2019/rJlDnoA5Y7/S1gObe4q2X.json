{"title": "Neat idea backed by a solid technical contribution", "review": "This paper describes a technique for replacing the softmax layer in sequence-to-sequence models with one that attempts to predict a continuous word embedding, which will then be mapped into a (potentially huge) pre-trained embedding vector via nearest neighbor search. The obvious choice for building a loss around such a prediction (squared error) is shown to be inappropriate empirically, and instead a von Mises-Fisher loss is proposed. Experiments conducted on small-data, small-model, greedy-search German->English, French->English and English->French scenarios demonstrate translation quality on par with BPE, and superior performance to a number of other continuous vector losses. They also provide convincing arguments that this new objective is more efficient in terms of both time and number of learned parameters.\n\nThis is a nice innovation for sequence-to-sequence modeling. The technical contribution required to make it work is non-trivial, and the authors have demonstrated promising results on a small system. I\u2019m not sure whether this has any chance of supplanting BPE as the go-to solution for large vocabulary models, but I think it\u2019s very healthy to add this method to the discussion.\n\nOther than the aforementioned small baseline systems, this paper has few issues, so I\u2019ll take some of my usual \u2018problems with the paper\u2019 space to discuss some downsides with this method. First: the need to use pre-trained word embeddings may be a step backward. It\u2019s always a little scary to introduce more steps into the pipeline, and it\u2019s uncomfortable to hear the authors state that they may be able to improve performance by changing the word embedding objective. As we move to large training sets, having pre-trained embeddings is likely to stop being an advantage and start being a hindrance. Second: though this can drastically increase vocabulary sizes, it is still a closed vocabulary model, which is a weakness when compared to BPE (though I suppose you could do both).\n\nSmaller issues:\n\nFirst paragraph after equation (1): \u201cthe hidden state \u2026 t, h.\u201d -> \u201cthe hidden state h \u2026 t.\u201d\n\nEquation (2): it might help your readers to spell out how setting \\kappa to ||\\hat{e}|| allows you to ignore the unit-norm assumption of \\mu.\n\n\u201cthe negative log-likelihood of the vMF\u2026\u201d - missing capital\n\nUnnumbered equation immediately before \u201cRegularization of NLLvMF\u201d: C_m||\\hat{e}|| is missing round brackets around ||\\hat{e}|| to make it an argument of the C_m function.\n\nIs predicting the word vector whose target embedding has the highest value of vMF probability any more expensive than nearest neighbor search? Does it preclude the use of very fast nearest neighbor searches?\n\nIt might be a good idea to make it clear in 4.3 that you see an extension to beam search for your method to be non-trivial (and that you aren\u2019t simply leaving out beam search for comparability to the various empirical loss functions). This didn\u2019t become clear to me until the Future Work section.\n\nIn Table 5, I don\u2019t fully understand F1 in terms of word-level translation accuracy. Recall is easy to understand (does the reference word appear in the system output?) but precision is harder to conceptualize. It might help to define the metric more carefully.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}