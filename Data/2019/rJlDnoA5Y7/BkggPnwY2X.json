{"title": "I have some concerns about this paper.", "review": "\n\n[clarity]\nThis paper is basically well written. \nThe motivation is clear and reasonable.\nHowever, I have some points that I need to confirm for review (Please see the significance part).\n\n\n[originality]\nThe idea of taking advantage of von Mises-Fisher distributions is not novel in the context of DL/DNN research community.\nE.g.,\nvon Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification.\n\nHowever, as described in the paper, the incorporation of von Mises-Fisher for calculating loss function seems to be novel, to the best of my knowledge.\n\n\n[significance]\nUnfortunately, the experiments in this paper do not fully support the effectiveness of the proposed method. \nSee below for more detailed comments.\n\n\n*weak baseline (comparison)\nAs an anonymous reviewer pointed out, the author should run baseline method with beam search if the authors aim to convince readers (including reviewers) for the effectiveness of the proposed method.\nI understand that it is important to investigate the effectiveness of the proposed method in the identical settings. However, it is also important to compare the proposed method with strong baseline to reveal the relative effectiveness of the proposed method comparing with the current state-of-the-art methods. \n\n\n* open vocabulary setting\nI am confused whether the experimental setting for the proposed method is really in an open vocabulary setting or not.\nIf my understanding is correct, the vocabulary sizes used for the proposed method were 50,000 (iwslt2016) and 300,000 (wmt16), which cannot be an open vocabulary setting. \nIf this is correct, the applicability of the proposed method is potentially limited comparing with the subword-based approach.\nIs there any comment for this question?\n\n\n* convergence speed\nI think the claim of faster convergence of the proposed method in terms of iteration may be misleading. This might be true, but it is empirically proven only by single dataset and single run. The authors should show more empirical results on several datasets or provide a theoretical justification for this claim.\n\n\nOverall, basically I like the idea of the proposed method. \nI also aim to remove the large computational cost of softmax in neural encoder-decoder approach.\nIn my feeling, the proposed method should be a bit more improved for a recommendation of clear acceptance.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}