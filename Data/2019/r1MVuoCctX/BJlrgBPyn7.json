{"title": "Missing comparison - what about highway/skip connections?", "review": "The authors introduce an language modeling architecture that introduces so-called \"minor-LSTMs\" computed directly on top of the input in addition to each network layers input. This approach seems hardly novel. considering approaches like skip/highway connections, e.g. (Zilly et al., 2016, https://arxiv.org/abs/1607.03474). Even if the proposed approach varies this theme in some way, a comparison to these strongly related approaches would be due.\n\nAlso, the presented approach could also be viewed as a kind of factorization, cf. (Kuchaiev et al., 2017, https://arxiv.org/abs/1703.10722).\n\nSec. 3.3: it remains totally unclear to me, how the authors reach the recursive equations (9) and (10) and how they relate to Eq. (8) mention before.\n\nIn the results tables on p. 6, at some point, perplexity numbers are given with a second position after the decimal point. I strongly doubt that these differences in perplexity are significant, especially on these small tasks.\n\nThe authors should take more care w.r.t. the notation, which is not consistent throughout the paper (e.g. x_j vs. x_t, y_j vs. y_t) and should be introduced properly.\n\nFinally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}