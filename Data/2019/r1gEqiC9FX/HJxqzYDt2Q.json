{"title": "Review", "review": "The authors propose a new weight re-parameterization technique called Equi-normalization (ENorm) inspired by the Sinkhorn-Knopp algorithm. The authors show that the proposed method preserve functionally equivalent property in respect of the output of the functions (Linear, Conv, and Max-Pool) and show also that ENorm converges to the global optimum through the optimization. The experimental results show that ENorm performs better than baseline methods on CIFAR-10 and ImageNet datasets.\n\npros)\n(+) The authors provide a theoretical ground.\n(+) The theoretical analysis of the convergence of the proposed algorithm is well provided.\n(+) The computational overhead reduced by the proposed method compared with BN and GN looks good.\n\ncons)\n(-) There is no comparison with other weight reparameterization methods such as Weight Normalization, Normalization propagation, Instance Normalization, or Layer Normalization. \n(-) The evidence why functionally equivalence is connected to the performance or generalization ability is not clarified.  \n(-) The experimental results cannot consistently show the effectiveness of the proposed method in test accuracy. In Table 4, the proposed method outperforms BN, but In Table 2 and 3, BN is mostly better than the proposed method.\n(-)  The batch size shown in Table 2 and 3 may be intended to show the batch-independent property of the proposed method, but BN is also doing well in those tables. Therefore, Table 2 and 3 are not adequate to show the batch-independent property.\n(-) The proposed method should evaluate with deeper networks (e.g., ResNet-50, ResNet101, or DenseNet-169) to support the superiority over BN and GN.  \n(-) Adjusting c does not seem to be promising. In Table 2 and 3, ENorm-1 is better than ENorm-1.2, and also in Table 4, only the result of ENorm-1 is provided. The authors should do a parameter study with c to make all the experiments more convincing.\n\nComments) \n- The experimental settings are not consistent. The authors should provide the reason why they set those settings or should include some studies about the parameters (for example about the paramter c). \n- Section 3.7 is not clear to me.  How's the performance going on when adjusting c < 1?\n- It is better for the authors to provide the Sinkhorn-Knopp algorithm (SK algorithm), which gave them an inspiration for this work, for better readability. \n- Why eq.(4) is necessary? For iterative optimization? If so, the authors should incorporate a detailed explanation about this in the corresponding section.\n- The authors should provide a detailed description of the parameter c. It is not clear why c is necessary, and please make sure the overall derivation does not need to be modified due to the emergence of c.\n- It seems that the authors could compact the paper by highlighting key ideas. \n- Typo: Annex A (on p.5).\n\nThe paper is written well and provides a sound theoretical analysis to show the main idea, but unfortunately, the experimental results do not seem to support the effectiveness of the proposed method.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}