{"title": "Review", "review": "This paper first identifies an inequivalence between L2 regularization and the original weight decay in adaptive stochastic gradient methods, e.g., the Adam method, and then proposes two decoupled variants, SGDW and AdamW, respective. The authors also cited a recent work to provide a justification of their proposed update rules from the perspective of Bayesian filtering. To demonstrate the effectiveness of both methods, experiments on CIFAR10 and ImageNet32x32 are conducted to compare with the original methods. Results show that the proposed methods consistently lead to faster convergence. Overall the paper is well written and easy to follow, with enough details describing the experimental settings. \n\nFirst of all I appreciate the authors pointing out that weight decay is not equal to L2 regularization in general. This is evident once the original definition of weight decay is given. The main motivation comes from the argument that instead of using L2 regularization, weight decay should be used in adaptive gradient methods. The Bayesian filtering interpretation helps to justify the proposed method. But it is not clear to me why the hyperparameters w and \\alpha are decoupled in the proposed methods? For example, in Line 6 of Alg. 1, g_t is a function of w, and later in Line 8, g_t is coupled with \\alpha which naturally introduces a term w \\alpha into m_t. So both w and \\alpha are still coupled together in the proposed algorithm. If this is the case why the authors still call w and \\alpha decoupled? \n\nTo me the most interesting result is Proposition 3 where the authors show that weight decay actually corresponds to preconditioned L2 regularization. This helps to explain what's the algorithmic difference between these two methods in adaptive gradient methods, and provides an intuitive insight on why weight decay may lead to better results compared with the vanilla L2 regularization. \n\nExperiments on image recognition tasks basically confirm the authors' claims. However, as the authors have already pointed out, it is better to have more thorough experiments on other kinds of tasks, e.g., in text classification, etc. If the improvement does come from the difference between weight decay vs L2, then I would also expect the same improvement on other tasks. It would be great to see more experimental results on other tasks to have a better understanding of this problem. So far it is not clear whether the same improvement holds in general or not. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}