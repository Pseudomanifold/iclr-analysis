{"title": "Nice insights with too strong assumptions", "review": "The authors study the learning dynamics of deep neural networks, which is of fundamental importance but lacks understanding. The authors study several dynamics like activation independence, gradient starvation, which gives new insights. However, the assumption is too strong.\n\nThere are two main results in the paper:\n1) Through learning, the neurons activates of one class. \n2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape.\n\nHowever, there are two strong assumptions: 1. the two data are perfectly separable by linear classifier. 2.  H2 assumes \"at the beginning of training data points from different classes do not activate the same neurons\". This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied. It sounds to me this assumption implicitly suggests that the algorithm is already ALMOST CONVERGENT. \n\nIf this assumption cannot be weakened, I don't think the paper can be accepted.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}