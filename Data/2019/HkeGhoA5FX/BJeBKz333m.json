{"title": "excellent results, but unclear novelty and lacking explanations", "review": "The paper proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications.\n\n\n# Results\nThe strongest point of the paper is that the quantitative and qualitative image restoration results appear to be very good, although they seem almost a bit too good.\n\n\n# Novelty\nI'm not sure about the novelty of the paper, but I suspect it to be rather incremental. The paper says \"To the best of our knowledge, this is the first time to consider residual non-local attention for image restoration problems.\" Does that mean non-local attention (in a very similar way) has already been used, just not in a residual fashion? If so, that would not constitute much novelty. I have to admit that I'm not familiar with the related work on attention, but I did not understand *why* the results of the proposed method are supposed to be much better than that of previous work.\n\n\n# Clarity\nI think the paper is not self-contained enough, since it seems to implicitly assume substantial background knowledge on attention mechanisms in CNNs. \n\nFurthermore, the introduction of the paper identifies three problems with existing CNNs that I don't necessarily fully agree with. None of these supposed problems are backed up by (experimental) evidence.\n\nI don't think it is sufficient to just show superior results than previous methods. It is also important to disentangle why the results are better. However, the presented ablation experiments are not very illuminating to me.\n\nThe attempts at explaining what the novel attention blocks do and why they lead to superior results are very vague to me. Maybe they are understandable in the context of related work, but I found many statements, such as the following, devoid of meaning:\n- \"Without considering the uneven distribution of information in the corrupted images, [...]\"\n- \"However, in this paper, we mainly focus on learning non-local attention to better guide feature extraction in trunk branch.\"\n- \"We only incorporate residual non-local attention block in low-level and high-level feature space. This is mainly because a few non-local modules can well offer non-local ability to the network for image restoration.\"\n- \"The key point in mask branch is how to grasp information of larger scope, namely larger receptive field size, so that it\u2019s possible to obtain more sophisticated attention map.\"\n\n\n# Experiments\n- The experimental results are the best part of the paper. However, it would've been nice to include some qualitative results in the main paper.\n- The proposed RNAN model is trained on a big dataset (800 images with ~2 million pixels each). Are the competing methods trained on datasets of similar size? If not, this could be a major reason for improved performance of RNAN over competing methods. At least in the appendix, RNAN and FFDNet are compared more fairly since they are trained with the same/similar data.\n- The qualitative examples in the appendix mostly show close-ups/details of very structured regions (mostly stripy patterns). Please also show some other regions without self-similar structures.\n\n\n# Misc\n- Residual non-local attention learning (section 3.3) was not clear to me.\n- The word \"trunk\" is used without definition or explanation.\n- Fig. 2 caption is too short, please expand.\n\n# Update (2018-11-29)\nGiven the substantial author feedback, I'm willing to raise my score.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}