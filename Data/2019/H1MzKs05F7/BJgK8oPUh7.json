{"title": "interesting work but with limited applicability and significance demonstrated", "review": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found.\n\nThe paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis.\n\nThe theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons:\n- This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem.\n- One very important special case of the point above: the analysis probably cannot cover the  adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes.\n\n\nOn the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. \nMy concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses.\nAdversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" \nIt is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark.\nWithout comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper.\nFor example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper.\n\n[MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}