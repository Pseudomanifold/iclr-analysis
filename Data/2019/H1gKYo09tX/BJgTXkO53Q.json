{"title": "Nice model using path summaries, but some baseline comparisons are missing", "review": "This paper presents a new code-to-sequence model called code2seq that leverages the syntactic structure of programming languages to encode source code snippets, which is then decoded to natural language using a sequence decoder. The key idea of the approach is to represent a program using a set of randomly sample k paths in its abstract syntax tree. For each path, the path is encoded using a recurrent network and concatenated with the embeddings of the two leaf terminal values of the path. The path encodings are then averaged to obtain the program embedding, which is then used to initialize a sequence decoder that also attends over the path embeddings. The code2vec model is evaluated over two tasks: 1) Code summarization: predicting a method\u2019s name from its body, and 2) Code captioning: generating a natural language sentence from method\u2019s body depicting its functionality. The code2seq model significantly outperforms the other baseline methods, and the ablation study shows the importance of various design choices.\n\nThis paper presents an elegant way to represent programs using a set of paths in the AST, which are then weighted using an attention mechanism to attend over relevant path components. The code2seq model is extensively evaluated over two domains of code summarization and code captioning, and results show significant improvements.\n\nThe novelty of the code2seq model is somewhat limited compared to the model presented in code2vec (Alon et al. 2018a) paper. In code2vec, a program is encoded as a set of paths, where each path comes from a fixed vocabulary. The code2seq model instead uses an LSTM to encode individual paths, which allows it to generalize to new paths. This is a more natural choice for embedding paths, but it doesn\u2019t appear to be a big conceptual advance in the model architecture. The use of subtoken embeddings for encoding/decoding identifier names is different in code2seq, but it has been proposed earlier in other code embedding models.\n\nFor the code summarization evaluation, would it be possible to evaluate the code2seq model on the dataset used by the code2vec paper? On that dataset, the code2vec approach gets a precision score of 63.1, recall of 54.4, and F1 score of 58.4, [Table 3 on page 18] which are comparable to overall scores of the code2seq model.\n\nOne of the key findings of the paper is that syntactic structure of programs is important to encode. Similar observations have been made in other program embedding papers that use for example Tree-RNN [1] or graph neural networks (GNN) [Allamanis et al. 2018]. It would be quite valuable to compare the current results with the Tree-RNN or GNN models (without performing additional dataflow and control-flow post processing) to see how well the paths-based embeddings work in comparison to these models.\n\nThe value of k=200 seems a bit large for the examples presented in the paper. What happens when smaller values of k are used (e.g. k=10, 20?) What are the average number of paths in the java programs in the dataset?\n\n1. Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas. Learning Program Embeddings to Propagate Feedback on Student Code\nICML 2015\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}