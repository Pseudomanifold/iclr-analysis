{"title": "Nice evaluation of AST-based encoding for code summarization tasks", "review": "This paper introduces an AST-based encoding for programming code and\nshows the effectivness of the encoding in two different task of code\nsummarization:\n\n1. Extreme code summarization - predicting (generating) function name from function body (Java)\n2. Code captioning - generating a natural language sentence for a (short) snippet of code (C#)\n\nPros:\n- Simple idea of encoding syntactic structure of the program through random paths in ASTs\n- Thorough evaluation of the technique on multiple datasets and using multiple baselines\n- Better results than previously published baselines\n- Two new datasets (based on Java code present in github) that will be made available\n- The encoding is used in two different tasks which also involve two different languages\n\nCons:\n- Some of the details of the implementation/design are not clear (see some clarifying questions below)\n- More stats on the collected datasets would have been nice\n- Personally, I'm not convinced \"extreme code summarization\"\nis a great task for code understanding (see more comments below)\n\nOverall, I enjoyed reading this paper and I think the authors did a\ngreat job explaining the technique, comparing it with other baselines,\nbuilding new datasets, etc.\n\nI have several clarifying questions/points (in no particular order):\n\n* Can you provide some intuition on why random paths in the AST encode\n  the \"meaning\" of the code? And perhaps qualitatively compare it with\n  recording some other properties from the tree that preserve its\n  structure more?\n\n* When you perform the encoding of the function body, one sample in a\n  training step contains all the k (k = 200) paths and all the 2*k\n  terminals (end of Section 2)? Or one path at a time (Section 3.2)?\n  I'm guessing is the latter, but not entirely sure. Figure 3 could\n  improve to make it clear.\n\n* Can you explain how you came up with k = 200? I think providing some\n  stats on the dataset could be helpful to understand this number.\n\n* The results for the baselines - do you train across all projects?\n  (As you point out, ConvAttention trained separately, curious whether\n  it makes a difference for the 2 datasets med and large not present\n  in the original paper).\n\n* I'm not sure I understand parts of the ablation study. In particular\n  for point 1., it seems that instead of the AST, only the terminal\n  nodes are used. Do you still use 200 random pairs of terminal? Is\n  this equivalent to a (randomly shuffled) subset of the tokens in the\n  program? Also could you explain why you do the ablation study on the\n  validation set of the medium dataset? In fact, the caption of Table\n  3 says it's done on the dev set. This part was a bit confusing.\n\n* I would have liked to see more details on the datasets introduced,\n  in particular wrt metrics that are relevant for training the model\n  you describe (e.g., stats on the ASTs, stats on the number of random\n  paths in ASTs, code length in tokens, etc.)\n\n* I'm not convinced that the task of \"extreme code summarization\" is a\n  meaningful task. My main problem with it is that the performance of\n  a human on this task would not be that great. On one hand humans\n  (I'm referring to \"programming humans\" :) ) have no problem in\n  coming up with a name for a function body; however, I'm not\n  convinced they could predict the \"gold\" standard. Or, another way of\n  thinking about this, if you have 3 humans who provided names for the\n  same function, my guess it that there will be a wide degree of\n  (dis)agreement. Some of the examples provided in the supplementary\n  material can serve as confirmation bias to my thought :): Fig 7. I\n  claim \"choose random prime\" and \"generate prime number\" are\n  semantically close, however, the precision and recall for this\n  example are both low. All this being said, I understand that it's a\n  task for which data can be generated fairly quickly to feed the\n  (beast) NN and that helps pushing the needle in understanding code\n  semantics.\n\n* It would be nice to see \"exact match\" as one of the metrics (it is\n  probably low, judging by F1 scores, but good to be reported).\n\n* Most likely the following paper could be cited in the related work:\nNeural Code Comprehension: A Learnable Representation of Code Semantics\nhttps://arxiv.org/abs/1806.07336\nhttps://nips.cc/Conferences/2018/Schedule?showEvent=11359\n\nPage 5 first phrase at the top, perhaps zi is a typo and it is\nsupposed to be z1?\n\n----\n\nUpdate: after all the discussion, I'm lowering my score a bit while still hoping the paper will get published. I'm satisfied with the results and the improvement of the paper. I still find it a bit surprising that the pairs of literals/leaves in the tree are a good approximation for the program itself (as shown in one of the ablation study).\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}