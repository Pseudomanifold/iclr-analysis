{"title": "a bit too specific in the application, limited results and comparisons of the method with SOTA", "review": "The paper proposes to learn a policy and action value over continuous next movement from professional soccer game. The policy is modeled as a gmm and the value of the mean of each component is learned jointly. \n\nThe experiment section seems a bit light. The dataset seems quite small, would be good to get learning curves on training / test sets separately to see of the model is overfitting. Also a breakdown of results with bigger / deeper convnets rather than just number of mixture components would be interesting. There is not much comparison with other ML / RL types methods. \n\np3: why not ' instead of + to denote next state ?\n\nf+i = fi + mi : why no f^{i+1}\n\nNevertheless, the movement of\nthe ball is a part of the environment and is modeled as random movement. Therefore, the movement\ndistribution is governed by a stochastic policy. \n\nEven if the ball movement was deterministic, the movement could still be stochastic.\n\nEq (2) right hand side, inconsistency between p(m | s, m) with p(s+ | s, m) defined above ?\n\nEq 6: I guess a softmax over a grid centered around player position would also work ? Possibly convolutional all the way? It wouldnt have much more parameters than the proposed arch?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}