{"title": "If we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising", "review": "\n- Summary\nThis paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem. Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network. The proposed method is evaluated on CIFAR-10 and ImageNet dataset.\n\n- Pros\n  - The proposed method shows competitive or better performance than existing neural architecture search methods.\n  - The experiments are conducted thoroughly in the CIFAR-10 and ImageNet. The selection of the datasets is appropriate. Also, the selection of the methods to be compared is appropriate.\n  - The effect of each proposed technique is appropriately evaluated.\n\n- Cons\n  - The search space of the proposed method, such as the number of operations in the convolution block, is limited.\n  - The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.\n  - The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.\n\nOverall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}