{"title": "Official Review", "review": "Summary:\nThis paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.\n\nThe main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph. All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a). The DAG\u2019s edges can be pruned via a sparsity regularization term. The optimization objective of DSO-NAS is thus:\n\nAccuracy + L2-regularization(W) + L1-sparsity(\\lambda),\n\nwhere W is the shared weights and \\lambda specifies which edges in the DAG are used.\n\nThere are 3 phases of optimization:\n1. All edges are activated and the shared weights W are trained using normal SGD. Note that this step does not involve \\lambda.\n2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018).\n3. The best architecture is selected and retrained from scratch.\n\nThis procedure works for all architectures and objectives. However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts.\n\nTheir experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet).\n\nStrengths:\n1. Regularization by sparsity is a neat idea.\n\n2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet. Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive.\n\n3. Incorporating architecture costs into the search objective is nice. However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper.\n\nWeaknesses:\n1. Some experimental details are missing. I\u2019m going to list them here:\n- Was the auxiliary tower used during the training of the shared weights W?\n\n- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?\n\n- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?\n\n- In Section 3.3, it is written that \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d. This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.\n\n2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison.\n\n3. The paper has some grammatical errors. I obviously missed many, but here are the one I found:\n\n- Section 3.3: \u201cDifferent from pruning, which the search space is usually quite limited\u201d. \u201cwhich\u201d should be \u201cwhose\u201d?\n\n- Section 4.4.1: \u201cDSO-NAS can also search architecture [...]\u201d  -> \u201cDSO-NAS can also search for architectures [...]\u201d\n\nReferences.\n[1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf\n\n[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}