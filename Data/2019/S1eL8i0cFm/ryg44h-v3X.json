{"title": "Confusing writing and limited novelty", "review": "The authors propose a training procedure to avoid models relying on \"subsidiary information\" while being optimized for a certain task, through a cosine similarity based adversarial process. The authors propose to use this measure to force orthogonality between the features extracted and the weights associated with a subsidiary task.\n\nWhile the idea of removing information not related to the main task of a learning system in order to better generalize is an interesting direction, the novelty of this paper is limited, and the current version contains several flaws. \n\nFirst of all, the paper is very confusingly written. A proper problem formulation, with a more formal definition of what \"subsidiary information\" is, is necessary to walk the reader through the paper. For instance, the introduction is extremely scarce of details (paradoxically, the abstract has more), and there is a logic gap between the description of Generative Adversarial Networks (GANs) and the introduction of the cosine similarity method. The authors write that it \"differs from existing GANs in that it trains the discriminative model by using the cosine similarity-based adversarial process\"; this is a confusing statement, as the method proposed by the authors is not a generative model. At most, this is what makes the proposed method differ from Ganin et al. (ICML2015).\n\nA proper description of the training procedure is missing. How subsidiary information is eliminated is clarified by Figure 2, but it is not clear how and when E and M are trained to perform the primary task. Does this happen only before the iterative procedure? Observing Figure 2, it seems that E and M are never trained to perform the primary task of interest while running the iterative procedure. This seems wrong, because in the described in Figure 2.b the weights of the encoder E are updated and thus also the feature space on which M was trained is changed. Also, what is the stop condition of the iterative procedure? How can one determine if the subsidiary information has been eliminated or not from the codes? Summarizing, the overall training procedure needs to be better detailed. Perhaps with the aid of an Algorithm box to detail the procedure step-by-step? \n\nThis statement of related works confused this reviewer: \"in particular, our proposed CAN framework is designed to work without target samples\". Do the authors mean that in the speaker recognition experiment the target is not used? Because it certainly is in the unsupervised domain adaptation experiments, by definition of the problem formulation. If the target is not used in the speaker recognition experiment, a suggestion to the authors is to read something from the literature about \"domain generalization\".\n\nConcerning unsupervised domain adaptation, the related work section is not complete, the authors only cite Ganin et al. (ICML2015) and Tzeng et al. (CVPR2017), but the literature is not limited to these two works. The authors should include all the relevant works and provide the connections between the proposed method and the related method to let the reader fully understand the contributions. This also makes Table 2 incomplete, as the authors only compare their method against ADDA. It should include at least more recent, effective approaches, which perform substantially better on the faced tasks.\n\nFurthermore, since the proposed approach in unsupervised domain adaptation is very related to Ganin et al. (ICML2015), where the domain information is eliminated from the features through adversarial training, the two methods should be compared in a fairer fashion. In particular, they should be compared using the same network architectures and possibly with the same settings (Leaky ReLU, L2 normalization). This experiment would make the reader better understand and appreciate the pros (and, eventually, cons) of using cosine similarity instead of vanilla adversarial training. At present, the effectiveness of using cosine similarity is not clear.\n\nDue to the aforementioned issues with the current version, this reviewer does not recommend this work for acceptance to ICLR 2019.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}