{"title": "A borderline paper. ", "review": "The authors proposed a deep framework that incorporating adversarial training process for eliminating the subsidiary information from the input data. In particular, to remove the subsidiary context, a main task neural network M and a subsidiary task neural networks are trained in an alternative way. With that, the authors present intuitive results on some real-world tasks, such as domain adaptations, and reducing channel information. In general, the paper is well-organized and clearly represented. Despite this, there lacks some insightful analysis of the proposed approach, and the empirical studies should be enriched. In particular:\n(1) the authors claimed that \"the performance of S degrades, and the performance of M increases, consequently. \", which is arbitrary and not well motivated. As the subsidiary information is removed from E, why the performance of S will degrade? \n(2) for the proposed objective function, more discussion should be provided regarding the intuition. The loss function is minimized over the cosine similarity between the E and W_i^{sub} for each class i. It is unclear whether useful information is also eliminated from the embedding E, especially some subsidiary is either relevant or correlated with the main task space.\n(3) why L2 normalization can increase the efficiency of the proposed framework?\n(4) I would expect the authors could provide more empirical studies over more benchmark datasets and comparing with more baselines. It will largely convince the readers with such experimental results. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}