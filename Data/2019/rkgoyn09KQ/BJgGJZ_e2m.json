{"title": "Extends existing DocNADE model by replacing the feedforward network with an LSTM", "review": "The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information.\n\nThe authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization.\n\nMy main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model.\n\nIt would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it).\n\nMore specific comments:\n--\nThe abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature.\n\nUsing existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.)\n\nI'm confused by one aspect of the DocNADE model: \"the topic assigned ...equally depends all the other words appearing in the same document\". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? \n\nRelated to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't.\n\nRelation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about \"distributed compositional priors\" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these \"priors\" are compositional, but that's a separate question.)\n\nEquation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM?\n\nIt would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM?\n\nSimilar question: how can the DocNADE model provide a *global* context if the model is generative?\n\nPerplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact?\n\nI don't understand why lambda has to be zero \"to compute the exact log-likelihood\".\n\nThe first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that.\n\nMinor presentational comments:\n--\nSome of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings?\n\nStrictly speaking, I'm not sure that 'bear' in the example is a proper noun.\n\n\"orderless sets of words\": bags, not sets, since the counts matter, no?\n\nThe tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}