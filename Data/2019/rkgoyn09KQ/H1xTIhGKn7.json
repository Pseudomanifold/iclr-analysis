{"title": "related work not great, but experiements extensive otherwise", "review": "DocNADE has great performance so this is a welcome bit of\nresearch extending it.\n\nThere has been a huge amount of activity in combining topic models with\n(1) embeddings and (2) neural networks such as LSTMs and RNNs.\nI will say I have great sympathy for the poor author trying to do\nfair comparisons against start-of-the-art because the standards are\nmoving quickly.\n\nIn this case, some neural network papers I have seen are TopicRNNs by\nDieng, Wang Gao and Paisley, and LLA by Zaheer, Ahmed and Smola.  The\nlatter is still a bag-of-words model and but places the LSTM over the\nsequence of topic proportions.  The Gauss-LDA and glove-DMM work is\nfairly dated (in our fast-paced ML world) and their performance is\nknown to be poor, as some papers in 2017 show.  Now I know historically LDA has been fairly poor\nwith IR tasks, but I would expect the recent supervised LDA methods,\nsome also have word embeddings, to do better as well.\nSo the discussion of related work and comparative experiments\nare poor.\n\nIf you want to illustrated good improvememts gained using embeddings,\nit helps to try different proportions, say 20/40/60/80% of a data\nset and plot.  Usually, you should see embeddings aid performance\ndramatically for smaller fractions of data sets.  Hence, your results\nseem strange.\n\nNote the data sets are all fairly small, which makes me wonder about\nthe computation time.  Could you give some computational performance\nstats for a data set?\n\nIn section 2.2 top of page 5, why is it \"pseudo\" log likelihood?\nIsn't that formula exact?\n\nThe paper has a relatively small part devoted to the model, and\nvirtually nothing on the algorithm, although this is probably covered\nin earlier DocNADE papers.   I'm assuming the model is\ntrained by SGD on the log likelihood with all the parameters\nshoved in there in one go.  Is that right?  Would be nice to mention\nwhatever it is.\n\nThe use of four different kinds of evaluations (classification, IR,\nperplexity, etc.) is good.  Note that the improvement over the earlier\nDocNADE is quite small but clearly significant, and improvement of adding\nembeddings seems even smaller, though seems better for short texts.\nI wonder if the method for including embeddings is much good!\nNot fully convinced.\n\nAFTER RESPONSE:   Wow guys, what a great revision.  Thanks so much.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}