{"title": "Method is not novel but results seem to be solid", "review": "\nCons: \nThe proposed method is not novel. For example, Lauly et al., 2017 have proposed a similar way of combining LM and DocNADE. This paper does not provide some motivations or theories behind such artificial combination (i.e., just linearly combine their hidden state) to explain why it works better than other alternatives (e.g., what about adding some linear layers before combining h_i^{DN} and h_i^{LM}).\n\nPros: \nHowever, the results seem to be solid and significantly better than the previous state-of-the-art methods. I think some recent neural topic models such as [1,2,3] are still missing even though there are already many tables in the paper (I am not an expert on neural topic modeling or embedding for IR tasks, so there might be others missing state of the arts which I am not aware of). In addition, why does Table 5 only compares perplexity between 3 methods and Table 6 only compares coherence between 4 or 5 methods, while there are 9 or 12 methods are compared in IR task (Table 3 and 4). What's the difficulty of comparing the coherence and perplexity of all different topic models (including [1,2,3])?\nI will vote for acceptance if the mentioned baselines are also compared or there are good reasons why they cannot be compared.\n\n\nWriting and presentation:\nThe quality of writing should be improved. Here are several examples.\n1. In the abstract, the following sentence needs to be rewritten and the rule of capitalization should be consistent. \"(2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging.\"\n2. I do not understand what's the purpose of the right figure in Figure 1. I think the paper does not do any matching like that.\n3. In the 3rd paragraph of the introduction, \"topmost\" -> top most \n4. The paper should have a related work section. In addition to the related work discussion scattered in the introduction, authors should discuss the difference between this work and Lauly et al., 2017. Authors should also include some related work such as [1,2,3].\n5. Just below (1), \"where,\" -> , where\n6. In the last sentence of the paragraph after (1), you mentioned \"v_{<i} are orderless\", so what's the ordering used in experiments? Random ordering?\n7. I guess \"a\" in algorithm 1 means sum_{k<i}(W_{:,v_k}), but I cannot find the explicit explanation about the purpose of \"a\".\n8. For ctx-DocNADEe, is W+E the embedding of words at input layer in LM?\n9. In the 3rd paragraph of section 2.2, you said: \"each row vector W_{j,:} is a distribution over vocabulary of size K\". Could W has negative values during optimization?  If yes, why a distribution representing a topic could have negative value. If no, you should explicitly mention this non-negativity constraint.\n10. Why are some values in Table 12 and 13 missing?\n\n[1] Cao, Z., Li, S., Liu, Y., Li, W., & Ji, H. (2015, January). A Novel Neural Topic Model and Its Supervised Extension. In AAAI (pp. 2210-2216).\n[2] Srivastava, A., & Sutton, C. (2017). Autoencoding variational inference for topic models. ICLR\n[3] Card, D., Tan, C., & Smith, N. A. (2017). A Neural Framework for Generalized Topic Models. arXiv preprint arXiv:1705.09296.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}