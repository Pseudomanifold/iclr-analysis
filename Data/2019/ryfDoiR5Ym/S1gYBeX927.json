{"title": "Interesting paper, confusing at times, needs more work.", "review": "This paper starts by discussing the usage of watermarking for Deep-NN's. Then changes topic and focuses on using DNN's for image watermarking. This is perhaps the most confusing point of this paper, since it's foundational to the paper structure.\n\nIf my understanding is correct, and let me be clear I am not 100% sure it is, but if this this paper is about \"using a DNN's to embed watermarking in images\", the authors should state it clearly at the beginning, including in the abstract and in the introduction, and resubmit. The discussion about using adversarial examples to watermark DNN's (so that the networks themselves are not stolen) is a departure from the core message, and a very confusing one indeed.\n\nBased on this current understanding, my review follows.\n\nThis paper proposes a encoder-decoder method, based on CNN's to embed a watermark into images (and audio).\n\nTo be clear, this is watermarking by detection, which means the decoder will output 1 if an image is watermarked, 0 otherwise. Effectively this watermarking is adding 1-bit of information to the content. I say this because usually (classic) watermarking methods can add codewords of more than one bit to content, thus allowing to embed the copyright owner private key(s) for future retrieval and proof of ownership; more on the implications of this later, at the end of the paper.\n\nThe network architecture(s) proposed are asymmetrical, in the sense that the encoder is high capacity and decoder is small. This makes sense, given that one want to perform decoding quickly, but adding the watermark can tolerate more computation/delay. The encoder is based on attention/convolutions and residual connections.\n\nThe decoder is based on a small DCGAN discriminator. The authors then state \"is very hard to identity the decoder after being implanted into a neural network model because it is tiny and uses only very standard neural operators.\". This again is confusing - under my understanding that this method is about image watermarking, why would we add this decoder into another network, covertly? Can the authors please clarify this?\n\nThe authors then proceed by discussing about GAN and GAN related loss functions in section 3.3, but then in section 4.3 they show a loss function that in their own words is not GAN related, because there is no \"Adversarial\" game. \nConfusing again: why to talk about GAN when there is no \"A\" - this is clearly NOT an adversarial game, there is an encoder and a recognizer which looks at a image and tells if it's watermarked or not. A better way to write this loss function would be a simple softmax cross-entropy of the output of the recognizer (D) to the image class of 1=watermarked ~ 0=not-watermarked. When I look at eq.3, I am not 100% sure it achieves that.\n\nThe authors correctly point out that for the watermark to not be detectable, it has to be small in some sense. To achieve that the authors  penalize the detection loss (eq.3) with a VGG hinge-loss on the watermarked vs non-watermarked image. The authors simply state that this works better than pixel-wise features.  \nThis makes sense, but only to some extent; it would have been better to show comparisons of watermarks between L1/L2 and VGG losses, because VGG loss will tend to prefer visually appealing distortions which might still be rather large distortions in term of PSNR. In some applications this is OK, for in some other applications this is not OK. You can imagine some content owners now wanting their images distorted more than a certain margin in a \"euclidean\" sense, in which case VGG loss might not work.\n\nThe paper then provides results for image and also speech recognition. Results show distortions levels as well changes in recognition accuracy due to the added watermarking noise, as well as watermarking detection rate. The results are good, with low distortion, negligible loss in accuracy, and almost perfect detection rate. \n\nThe experimental results are clearly the strong point of this paper. However, and this is a big however, the authors do not provide public source code to reproduce the results. I strongly encourage authors to submit publicly available code and data.\n\nFinally, to conclude, I would like to elaborate on the validity of the embed 1-bit / detect approach to watermarking.\nThis is very important, since this method does not add one code-word but only 1-bit, then the question is how can we really evaluate this system?\n\nPicture the following scenario: Let's assume I am owner-A ~ I train my network, I embed my one 1-bit into my images. Let's say that the esteemed member of our program committee is owner-B. S/He does the same, trains her network embeds the watermark into her images. We both test our detection result, based on the protocol proposed on this paper -  we both separately feed our images and our watermarked images into our detectors, and get 100% accuracy! We are very happy, watermarks look unnoticeable to the human eye and are detected by our networks!\n\nNow the BIG QUESTION IS ~ we both own 1-bit of information only right? Then what would happen if I feed the watermarked images of the esteemed-member of our program committee - owner-B into my network? Will my detection network say it's watermaked or not? Because if it says it's watermarked, now I could claim I own the content of owner-B ... same issue the other way around, owner-A watermarked data into owner-B network. What would happen? I am not saying this issue will arise, but given we only embed 1-bit of information, who owns that 1-bit? This should at least be tested. \n\nBased on all these comments I feel this is an interesting paper, which requires more work, more experiments, a more clear focus and more clarifications.\n\nThank you!", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}