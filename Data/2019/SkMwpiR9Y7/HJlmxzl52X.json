{"title": "Nice empirical motivations but weak proposed solution", "review": "Summary:\nThis paper proposes first to measure distances, in a L2 space, between functions computed by neural networks. It then compares those distances with the parameter l2 distances of those networks, and empirically shows that the l2 parameter distance is a poor proxy for distances in the function space. Following those observations, the authors propose to use such constraint to combat catastrophic forgetting, and show some results on the permuted MNIST task. Finally, they propose the Hillber-constrained gradient descent (HCGD), a gradient descent algorithm that constraint movement in the function space, and evaluate it on a CNN (CIFAR10) and an LSTM (permuted MNIST).\n\nClarity:\nThe paper is well motivated, clearly written and easy to follow.\n\nNovelty:\nThe idea of trying to move in the function space rather than in the parameter space is definitely not new (see the whole literature about Natural Gradient for instance). However, the proposed HCGD seems quite new, but unfortunately it doesn\u2019t seem to perform well.\n\nPros and Cons:\n+ The paper is well motivated, not only through the text but also with empirical evidence (section 2).\n+ The paper focuses on an important research direction in deep learning.\n+ This paper proposes a novel algorithm that penalizes movement in the function space.\n- However, it is not clear if the proposed algorithm actually penalizes the distance in function space, since it is performing a crude approximation of the distance measure (using one step of gradient).\n- Better way of penalizing movement in the function space already exists (at least for probability distributions: Natural Gradient)\n\nDetailed Comments:\n1. Batch Normalization and Weight Decay:\nI have mixed feelings about your experiments in section 2. Both Batch Normalization (BN) and Weight Decay (WD) have a regularization effect on the weights.  I am wondering if the change in ratio L2/l2 during the course of training is simply caused by the regularization terms getting stronger and stronger (compared to the cross-entropy loss). Also, BN makes the function computed by the network independent of the scale (of each row) of the weight matrices. I do think that running again those experiments without BN and WD would make the argument that \u201cthe parameter space is a proxy for function space\u201d more robust. \n2. About HCGD:\nThe origins of the HCGD algorithm is extremely similar to the origins of Natural Gradient (NG) (just switch the L2 norm with the KL). The main difference resides in how the proximal formulation (equation 2) is approximated. For NG, one approximate the KL using a 2nd order Taylor expansion and then the proximal formulation is explicitly solved for Delta theta, where HCGD takes only a simple gradient step. It is thus not clear how well this step is  indeed a good approximation of the distance in function space. For CNNs and LSTMs, K-FAC [1-2], which is a Natural Gradient approximation, has been shown to outperform ADAM, so the proposed approximation might not be good enough, as HCGD doesn't beat ADAM in the experimental setup. One experiment that would be nice to have is to do one update of the parameter in a neural network (using HCGD) and then measure how much you actually moved in the function space. \n[1] Roger Grosse, James Martens, A Kronecker-factored Approximate Fisher Matrix for Convolution Layers, ICML 2016\n[2] James Martens, Jimmy Ba, Matt Johnson,Kronecker-factored Curvature Approximations for Recurrent Neural Networks, ICLR 2018\n\nMinor Comments:\nSection 2.3: \u201cone would require require\u201d -> \u201cone would require\u201d\nFigure 3: \u201cthat a set batch size\u201d -> \u201cthat a fixed batch size\u201d\nSection 3.1.1: \u201cpermuted different on\u201d -> \u201cpermuted differently on\u201d\nSection 3.2.1: \u201cthat minimizes equation 6\u201d -> \u201cthat minimizes equation 5\u201d\n\nConclusion:\nThe paper proposes nice empirical evidence than parameter distance is not a good proxy for function distance. However, it is not clear if the proposed algorithm actually fixes this problem.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}