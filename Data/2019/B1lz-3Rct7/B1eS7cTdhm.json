{"title": "Solid work on understanding of weight decay regularization", "review": "This paper identifies and investigates three mechanisms of weight decay regularization. The authors consider weight decay for DNN architectures with/without BN and different types of optimization algorithms (SGD, Adam, and two versions of KFAC). The paper unravels insights on weight decay regularization effects, which cannot be explained only by traditional L2 regularization approach. This understanding is of high importance for the further development of regulations techniques for deep learning.\n\nStrengths:\n+ The authors draw connections between identified mechanisms and effects observed in prior work.\n+ The authors provide both clear theoretical analysis and adequate experimental evidence supporting identified regularization mechanisms.\n+ The paper is organized and written clearly.\n\nI cannot point out any flaws in the paper. The only recommendation I would give is to discuss in more detail possible implications of the observed results for new methods of regularization in deep learning and potential directions for future work. It would emphasize the significance of the obtained results.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}