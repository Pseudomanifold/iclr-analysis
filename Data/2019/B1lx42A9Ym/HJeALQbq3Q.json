{"title": "intersting model and claims, but incomprehensible paper", "review": "The paper claims to propose a novel generative probabilistic neural network model such that its encoder (classifying an image) can be approximated by a convolutional neural network with ReLU activations and MaxPooling layers. Besides the standard parameters of the units (weights and biases), the model has two additional latent variables per unit, which decide whether and where to put the template (represented by the weights of the neuron) in the subsequent layer, when generating an image from the class. Furthermore, the authors claim to derive new learning criteria for semi-supervised learning of the model including a novel regulariser and claim to prove its consistency. \n\nUnfortunately, the paper is written in a way that is completely incomprehensible (for me). The accumulating ambiguities, together with its sheer length (44 pages with all supplementary appendices!), make it impossible for me to verify the model and the proofs of the claimed theorems. This begins already with definition of the model. The authors consider the latent variables as dependent and model them by a joint distribution. Its definition remains obscure, let alone the question how to marginalise over these variables when making inference. Without a thorough understanding of the model definition, it becomes impossible (for me) to follow the presentation of the learning approaches and the proofs for the theorem claims.\n\nIn my view, the presented material exceeds the limits of a single conference paper. A clear and concise definition of the proposed model accompanied by a concise derivation of the basic inference and learning algorithm would already make a highly interesting paper.\n\nConsidering the present state of the paper, I can't, unfortunately, recommend to accept it for ICLR.\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}