{"title": "The paper is not self-contained. Otherwise it provides an interesting probabilistic interpretation of CNNs which allows to design semi-supervised learning algorithm leading to promising empirical results.", "review": "pros:\n- Interesting probabilistic interpretation of the CNNs improving work of [Patel 2016].\n- State-of-the-art results following from the proposed probabilistic model. \n\ncons:\n- The regular 10 pages of the paper are not self-contained. \n\nThe paper is written in overly condensed way. I found it impossible to clearly understand major claims of the paper without reading the accompanied 34 pages long appendix. Many concepts/notations used in the paper are introduced in the appendix. My assessment is done solely based on reading the 10 regular pages. \n\n- The probabilistic model NMR (equ (1) and (2)) defines distribution of inputs given latent variables and the outputs, $p(x|z,y)$, as well as it defines a distribution $p(z|y)$. Hence, in principle, one could maximize $p(x)=\\sum_{i} E_{z} p(x_i|z,y)p(z|y)p(y)$ when learning from unsupervised data. Instead, the authors propose to learn by MINIMIZING the expectation (not clear w.r.t which variables) of $\\log p(x,z|y)$ (equ (7)). Although it leads to empirically nice results, I do not see a clear motivation for such objective function. \n\n- The motivation for using the MIN-MAX entropy as a loss function (sec 3) is also not clear. Why it should be better than the standard cross-entropy in the statistical sense?\n\n- The proposed probabilistic model NMR differs form the previous work of [Patel 2016] by introducing the prior (1) on the latent variable. Unfortunately, pros and cons of this modifications are not fully discussed. E.g. how using dependent latent variables impact complexity of the inference.  \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}