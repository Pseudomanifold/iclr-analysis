{"title": "Interesting Approach with Nice Results", "review": "The paper presents a framework, called ChoiceNet, for learning when the \nsupervision outputs (e.g., labels) are corrupted by noise. The method relies on \nestimating the correlation between the training data distribution and a \ntarget distribution, where training data distribution is assumed to be a mixture \nof that target distribution and other unknown distributions. The paper also \npresents some compelling results on synthetic and real datasets, for both \nregression and classification problems.\n\nThe proposed idea builds on top of previously published work on Mixture Density \nNetworks (MDNs) and Mixup (Zhang et al, 2017). The main difference is the MDN \nare modified to construct the Mixture of Correlated Density Network (MCDN) \nblock, that forms the main component of ChoiceNets.\n\nI like the overall direction and idea of modelling correlation between the \ntarget distribution and the data distribution to deal with noisy labels. The \nresults are also compelling and I thus lean towards accepting this paper. My \ndecision on \"marginal accept\" is based primarily on my unfamiliarity with this \nspecific area and that some parts of the paper are not very easy or intuitive \nto read through.\n\n== Related Work ==\n\nI like the related work discussion, but would emphasize more the connection to \nMDNs and to Mixup. Only one sentence is mentioned about Mixup but reading \nthrough the abstract and the introduction that is the first paper that came to \nmy mind and thus I believe that it may deserve a bit more discussion.\n\nAlso, there are a couple more papers that felt relevant to this work but are \nnot mentioned:\n  - Estimating Accuracy from Unlabeled Data: A Bayesian Approach, Platanios et al., ICML 2016.\n    I believe this is related in how noisy labels are modeled (i.e., section 3 \n    in the reviewed paper) and in the idea of correlation/consistency as a means \n    to detect errors. There are couple more papers in this line of work that \n    may be relevant.\n  - ADIOS: Architectures Deep In Output Space, Al-Shedivat et al., ICML 2016.\n    I believe this is related in learning some structure in the output space, \n    even though not directly dealing with noisy labels.\n\n== Method ==\n\nI believe the methods section could have been written in a more \nclear/easy-to-follow way, but this may also be due to my unfamiliarity with this \narea. Figure 1 is hard to parse and does not really offer much more than section \n3.2 currently does. If the figure is improved with some more text/labels on \nboxes rather than plain equations, it may go a long way in making the methods \nsection easier to follow.\n\nI would also point out MCDN as the key contribution of this paper as ChoiceNet \nis just any base network with an MCDN block stacked on top of this. Thus, I \nbelieve this should be emphasized more to make your key contribution clear.\n\n== Experiments ==\n\nThe experiments are nicely presented and are quite thorough. A couple minor \ncomments I have are:\n\n  - It would be nice to run regression experiments for bigger real-world \n    datasets, as the ones used seem to be quite small.\n  - I am a bit confused at the fact that in table 3 you compare your method to \n    mixup and in table 4 you also show results when using both your method and \n    mixup combined. Up until that point I thought that mixup was posed as an \n    alternative method, but here it seems it's quite orthogonal and can be used \n    together, which I think makes sense, but would be good to clarify. Also, \n    given that you show combined results in table 4, why not also perform \n    exactly the same analysis for table 3 and also show numbers for CN + Mixup?\n\nIt would also be nice to use the same naming scheme for both tables. I would \nuse: ConvNet, ConvNet + CN, ConvNet + CN + Mixup, and the same with WRN for \ntable 4. This would make the tables easier to read because currently the first \nthing that comes to mind is what may be different between the two setups given \nthat they are presented side-by-side but use different naming conventions.\n\nOne question that comes to mind is that you make certain assumptions on the \nkinds of noise your model can capture, so are there any cases where you have \ngood intuition as to why your model may fail? It would be good to present a \nshort discussion on this to help readers understand whether they can benefit by \nusing your model or not.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}