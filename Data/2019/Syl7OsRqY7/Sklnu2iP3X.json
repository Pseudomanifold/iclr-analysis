{"title": "I am open to increasing my rating score as long as the authors could address my concerns and confusions below", "review": "This paper focuses on multi-choice QA and proposes a coarse-to-fine scoring framework. Where the coarse-grained answer scoring model computes the scores with the attention over the whole passages, and the fine-grained one only uses local contexts for each answer option (candidate).\n\nThe proposed approach was evaluated on the only dataset of WikiHop, and achieved large improvement over the other methods on the leaderboard. However, I found the paper lack of motivation about the designs of the coarse and fine scoring models. For example, why using self-attention after GRU and co-attention in the two answer scoring models?\n\nAnother concern I have is about the novelty. Besides the complicated model designs, the coarse and fine scoring models are both following some common ideas in previous work. And each model could achieve on-par results compared to previous baselines. This makes me feel that the whole approach looks more like model combination of two not-so-novel (and not very well-motivated) models.\n\nThirdly, the only evaluation on WikiHop brings more problems to the above two points. Since the motivation of the architecture design is not very clear, I am not sure whether the architectures could generalize to other benchmarks. Similar concern for the model combination approach.\n\nMoreover, the proposed approach is a general architecture for multiple-choice datasets requiring multiple evidence. To verify its generalizability, I suggest the authors add further experiments on one dataset from the following ones: either multi-choice QA datasets like ARC and RACE/RACE-open, or other open-domain QA datasets like TriviaQA, by treating the re-ranking of answer predictions as multi-choice QA problems (like the approach in Evidence Aggregation for Open-Domain QA from ICLR2018).\n\nA minor question: why the CFC w/o encoder could still work so well? At least the fine-grained scoring model should heavily rely on encoders. Otherwise, according to Eq (17), the fine-grained model cannot use any contextual information.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}