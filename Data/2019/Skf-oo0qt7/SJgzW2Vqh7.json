{"title": "Marginal improvement over existing bounds; Incomplete comparison with existing works", "review": "This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points:\n\n1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.\n\n2. Vacuous bounds in the regime \\beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \\beta >1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. \n\n3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] \n\n4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the \"model complexity\" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. \n\n\n[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86.1 (1998): 63-79.\n[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" Advances in Neural Information Processing Systems. 1996.\n[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in Neural Information Processing Systems. 2017.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}