{"title": "limited technical novelty", "review": "\nThe authors provide new generalization bounds for recurrent neural networks.\nTheir main result is a new bound for vanilla RNNs, but they also have\nbounds for gated RNNs.\n\nThey claim that their vanilla bound improves on an earlier\nbound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.\nThe main result of the submission is incomparable in strength with the earlier result,\nbecause this submission assumes that the activation functions in the hidden\nlayers are bounded, where the earlier paper did not.  Part of the difference in the results\n(roughly speaking, the \"min\" in the bound) can be traced to this difference in the assumptions. \n (This paper uses this assumption in the second-to-last line of the proof of Lemma 6.)\n\nI think that the root cause of the remaining difference is that this paper,\nat its core, adapts the more traditional analysis, used in Haussler's\n1992 InfComp paper.  New analyses, like from the Bartlett, et al\nNIPS'17 paper, strove for a weak dependence in the number of parameters,\nbut this proof technique appears to lead to a worse dependence on the\ndepth.  I think that, if you unwind the network, to view the function\nfrom the first t positions of the input to output number t as a\ndepth t network, and apply Haussler's bound, you will get a qualitatively\nsimilar result (in particular with bounds that scale polynomially with\nd and t).  I think that Haussler's proof technique can be adapted to\ntake advantage of the weight sharing between layers in the unrolled\nnetwork.  \n\nIt is somewhat interesting to note that the traditional bounds have\na better dependence on depth, with correspondingly better dependence\non the length of the output sequence of the RNN.\n\nI also do not see that substantial new insight is gained through the\nanalysis that incorporates gating.\n\nI do not see much technical novelty in this paper.\n\n\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}