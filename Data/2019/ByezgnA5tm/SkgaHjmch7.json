{"title": "Review", "review": "This work aims to use formal languages to add a reward shaping signal in the form of a penalty on the system when constraints are violated. There is also an interesting notion of using an embedding based on the action history to aid the agent in avoiding violations. However, I do not believe this paper did a good enough job in situating this work in the context of prior work \u2014 in particular (Camacho 2017). There is a significant related work section that does an ok job of describing many other works, but to my knowledge (Camacho 2017) is the most similar to this one (minus the embedding), yet is not mentioned here. It is difficult to find all related work of course, so I would encourage revision with detailed description of the novelty of this work in comparison with that one. I would also encourage an more thoughtful examination of the theoretical ramifications of the reward shaping signal with respect to the optimal policy as (Camacho 2017) do and as is modeled in the (Ng 1999) paper. As of this revision, however, I'm not sure I would recommend it for publication. Additionally, I suggest that the authors describe the reward shaping mechanism a bit more formally, it was unclear whether it fits into Ng's potential function methodology at first pass.\n\nComments:\n\n+ It would be nice to explain to the reader in intuitive terms what \u201cno-1D-dithering\u201d means near this text. I understand that later on this is explained, but for clarity it would be good to have a short explanation during the first mentioning of this term as well.\n+ It would be good to clarify in Figure 1 what . * (lr)^2 is since in the main text near the figure is is just (lr)^2 and the .* is only explained several pages ahead\n+ An interesting connection that might be made is that Ng et al.\u2019s reward shaping mechanism, if the\u00a0shaping function is based on a state-dependent potential then the optimal policy under the new MDP is still optimal for the old MDP. It would be interesting to see how well this holds under this holds under this schema. In fact, this seems like analysis that several other works have done for a very similar problem (see below).\n+ I have concerns about the novelty of this method. It seems rather similar to \n\nCamacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Decision-making with non-markovian rewards: From LTL to automata-based reward shaping.\" In\u00a0Proceedings of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), pp. 279-283. 2017.\nCamacho, Alberto, Oscar Chen, Scott Sanner, and Sheila A. McIlraith. \"Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping.\" In Proceedings of the Tenth International Symposium on Combinatorial Search (SoCS), pp. 159-160. 2017.\n\nHowever, that work proposes a similar framework in a much more formal way. In fact, in that work also a DFA is used as a reward shaping signal -- from what I can tell for the same purpose through a similar mechanism. It is possible, however, that I missed something which contrasts the two works.\n\nAnother work that can be referenced:\n\nDe Giacomo, Giuseppe, Luca Iocchi, Marco Favorito, and Fabio Patrizi. \"Reinforcement Learning for LTLf/LDLf Goals.\"\u00a0arXiv preprint arXiv:1807.06333\u00a0(2018).\n\nI think it is particularly important to situate this work within the context of those others. \n\n+ General the structure of the paper was a bit all over the place, crucial details were spread throughout and it took me a couple of passes to put things together. For example, it wasn't quite clear what the reward shaping mechanism was until I saw the -1000 and then had to go back to figure out that basically -1000 is added to the reward if the constraint is violated. I would suggest putting relevant details all in one place. For example, \"Our reward shaping function F(x) was  { -1000, constraint violation, 0 otherwise}\". ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}