{"title": "Idea is highly incremental. No theoretical guarantee that the proposed method is effective for domain shift problems", "review": "In this paper, the author proposed to a so-called collaborative low-rank reconstructive layer, which can be plugged in an existing deep learning architecture for domain adaptation or generalization.\n\nThe proposed idea is highly incremental in terms of technical novelty. The idea is indeed to simply concatenate the features learned from different domains, and then perform SVD to learn low-dimensional features. Such an idea is widely used in multi-domain learning or multi-task learning with a variant of dimensionality reduction techniques or deep learning architectures. Therefore, no new insights can be found from the proposed method to the transfer learning community.\n\nThough the authors claimed that learning features with globally the same distributions may not help to improve the model generalization abilities of each domain, learning low-dimensional features using SVD on the concatenated features does not have a guarantee to learn effective features for domain adaptation or generalization either. At least, theoretically, it is not reasonable why the proposed solution could work.\n\nThough the experimental results show that the proposed solution performs better than some baseline methods, the improvement could be caused by fine-tuning parameters.\n\nOne advantage of this paper is that it is easy to follow.\n\nIn summary, the quality of this paper is far below the standard of ICLR.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}