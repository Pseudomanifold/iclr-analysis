{"title": "A clean module that can be used directly for UDA, or added to different UDA frameworks as a drop-in component. Idea effective but not new, claims and motivations make sense but not well justified", "review": "This paper proposes a simple and clean approach that can be used for UDA, or used as a drop-in module by other UDA frameworks to improve the performance. The authors consider performing SVD on the combined set of source+target deep embeddings to provide an alternative low-rank view of the source data, formulating it as a differentiable reconstruction layer (CLRR). With the reconstructed view, the authors proposed to use it directly for UDA with task loss, or combine with existing UDA frameworks.\n\nMethod:\n- Overall, the method is technically correct. The idea is straightforward, and should be easy to reproduce. The motivation to perform domain adaptation on low-rank representations intuitively makes sense.\n\nClarity:\n- The paper is well-written with good clarity.\n\nResults:\n- The end-system results achieved by this paper are state-of-the-art in several experiments. However, the absolute improvement from the proposed method solely as a UDA module does not seem significant compared with many previous methods. On the other hand, the proposed method seems to complement other UDA framework well, and brings consistent performance gain (although not significant too).\n\nConcerns:\n- Contradict to what the authors claim, using low-rank or similar subspace representations for domain adaptation is not new. The authors failed to cite some related literature that share similar ideas. See the following works for example:\n1. Robust Visual Domain Adaptation with Low-Rank Reconstruction, CVPR12\n2. Domain Generalization and Adaptation using Low Rank Exemplar SVMs, T-PAMI18\nThere are also multiple works using autoencoder-like reconstructive modules to implicitly perform disentangling/cross-dataset UDA for face recognition/person re-id.\n\n- While the proposed work intuitively makes sense, the claims/motivations that performing domain alignment on low-rank, denoised representations somehow gives certain advantage over that on high-dimensional features is not well justified. My understanding is that instead of the advantage from the low-rank representation itself, it is possible that the gain actually comes from the ensemble of an additional low-rank view, rather than low-rank denoising itself. In a broader view, the proposed module may be more like a fancy end-to-end learnable data augmentation layer.\n\n- From this perspective, a more convincing experiment would be removing the branch of the original deep representations, and performing an \"apples-to-apples\" comparison by using only the low-rank representations to compare with ones that only use the original deep representations.\n\n- I'm also interested to see how much performance gain does an end-to-end SVD layer give over a non-end-to-end one. What if the layer is simply replaced by online SVD?\n\n- As mentioned, the absolute gain from the proposed method is not significant compared with existing methods, which somehow reduced the contribution of this work.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}