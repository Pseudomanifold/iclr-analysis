{"title": "A valuable contribution; yet, not well polished", "review": "The paper attempts to examine the reasons behind the strong generalisation performance of DNNs trained via SGD. The authors propose an analysis which offers a fresh view to this problem. This view has been articulated very well, and is based on sound mathematical arguments. \n\nOn the other hand, since there is no formal theorem to support the introduced assumptions, the authors have attempted to provide empirical evidence through experiments with standard DNN architectures and benchmark datasets. However, this is where the weakness of this paper lies: The provided empirical evidence, while nicely executed, is not enough to convince the critical reader. We need experiments with more diverse datasets and experimental setups. \n\nAlthough I accept the claim of the authors concerning the lack of space, they could also trim the Introduction so as to free up some space, as well as provide an indefinite number of extra supporting evidence in the form of Supplementary Material/Appendices. \n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}