{"title": "Although the experiments seem interesting,  theoretical results are not clearly presented.", "review": "This manuscript studies the implicit regularization of neural networks from the perspective of random matrix theory. The authors provide both empirical and theoretical results that aim to show that the empirical spectral density of weights of DNN captures the implicit regularization phenomenon. However, the results are far from rigorous theory and it is not clear how recent results in MP theory yields the statements made in the paper. \n\n\nDetailed comments:\n\n1. The empirical studies seem interesting. It seems that two kinds of results are shown. The first one is that ESD fits perfectly for small models, and the second one is that deep models fit heavy-tailed random matrices class. It would be interesting to see more details about how these models are trained, as training greatly affects the value of the weights.\n\n2. Theoretical results are not clearly stated. In section 2, the authors introduce the basics of MP theory. However, it is not clear how to derive the theory in this paper based on the MP theory. It seems that the main theory is the \"5+1 phases of training\". The definition of these 6 phases is not even explicitly given in this paper. Moreover, the theory of all these phases seems to depend on equation (3) , but there are no lemmas or propositions that gives a rigourious theoretical guarantee.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}