{"title": "Good paper", "review": "Summary:\nThis paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images. The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:\n-\tIncreasing batch size (8x) and model size (2x)\n-\tSplitting noise z in multiple chunks, and injecting it in multiple layers of the generator\n-\tSampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled. This seems to be used only at test-time and is used to control variety-fidelity tradeoff. The generator is encouraged to be smooth using an orthogonal regularization term.\nIn addition, the paper proposes practical recipes for characterizing collapse in GANs. In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse. In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs. Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.\n\nStrengths:\n-\tProposed techniques are intuitive and very well motivated\n-\tOne of the big pluses of this work is that authors try to \"quantify\" each proposed technique with training speed and/or performance improvement. This is really a good practice.\n-\tDetailed analysis for detecting collapse and improving stability in large-scale GAN\n-\tProbably no need to mention that, but results are quite impressive\n\nWeaknesses:\n-\tComputational budget required is massive. The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.\n\nComments/Questions:\n-\tCan you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance? \n-\tIt is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs. Providing such analysis would be also helpful for the community.\n-\tHow do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?\n\nOverall recommendation:\nThe paper is well written, ideas are well motivated/justified and results are very compelling. This is a good paper and I higly recommend acceptance.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}