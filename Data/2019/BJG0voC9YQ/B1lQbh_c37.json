{"title": "Interesting approach to relevant problem; nice integration of causal reasoning with RL; experiment setup avoids dealing with some practical challenges", "review": "Summary:\nProposes Counterfactual Guided Policy Search (CF-GPS), which uses counterfactual inference from sampled trajectories to improve an approximate simulator that is used for policy evaluation. Counterfactual inference is formalized with structural causal models of the POMDP. The method is evaluated in partially-observed Sokoban problems. The dynamics model is assumed known, and a learned model maps observation histories to a conditional distribution on the starting state. CF-GPS outperforms model-based policy search and a \"GPS-like\" algorithm in these domains. GPS in MDPs is shown to be a particular case of CF-GPS, and a connection is also suggested between stochastic value gradient and CF-GPS.\n\nReview:\nThe work is an interesting approach to a relevant problem. Related literature is covered well, and the paper is well-written in an approachable, conversational style. \n\nThe approach is technically sound and generally presented clearly, with a few missing details. It is mainly a combination of existing tools, but the combination seems to be novel. \n\nThe experiments show that the method is effective for these Sokoban problems. A weakness is that the setting is very \"clean\" in several ways. The dynamics and rewards are assumed known and the problem itself is deterministic, so the only thing being inferred in hindsight is the initial state. This could be done without all of the machinery of CF-GPS. I realize that the CF-GPS approach is domain-agnostic, but it would be useful to see it applied in a more general setting to get an idea of the practical difficulties. The issue of inaccurate dynamics models seems especially relevant, and is not addressed by the Sokoban experiment. It's also notable that the agent cannot affect any of the random outcomes in this problem, which I would think would make counterfactual reasoning more difficult.\n\nComments / Questions:\n* Please expand on what \"auto-regressive uniformization\" is and how it ensures that every POMDP can be expressed as an SCM\n* What is the prior p(U) for the experiments? \n* \"lotion-scale\" -> \"location-scale\"\n\nPros:\n* An interesting and well-motivated approach to an important problem\n* Interesting connections to GPS in MDPs\n\nCons:\n* Experimental domain does not \"exercise\" the approach fully; the counterfactual inference task is limited in scope and the dynamics and rewards are deterministic and assumed known\n* Work may not be easily reproducible due to the large number of pieces and incomplete specification of (hyper-)parameter settings ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}