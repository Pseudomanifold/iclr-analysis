{"title": "Simple idea, the presentation of the method and experiment results can be improved", "review": "Summary:\nThis paper proposed to extend TARNET (Shalit et al. 2017), a representation learning approach for counterfactual inference, in the following ways.\n\nFirst, to extend TARNET to multiple treatment setting, k head networks (instead of 2) were constructed following the shared MLP layers, where each head network modeled the outcome of one treatment. This extension seemed quite straightforward.\n\nSecond, during training, for every sample in a minibatch, find its nearest neighbors from all other treatments and add them to the minibatch. The distance was measured by the propensity score, which was defined the probability of a sample being assigned to a treatment group and could be learned by a classification model (such as support vector machine used in this work). Therefore, 1) the augmented minibatch would contain the same number of samples for each treatment group; 2) different treatment group were balanced.\n\nThird, a model selection strategy was proposed by estimating the PEHE using nearest neighbor search.\n\nComments:\nThis paper is well motivated. The key challenges in counterfactual inference is how to adjust for the bias in treatment assignment and the associated discrepancies in the distribution of different treatment groups. \n\nThe main idea of this paper, i.e., augmenting the minibatch through propensity score matching for each sample, is well explained in Section 3. However, it could be better if the introduction of model architecture (in Appendix F) was presented in the method section.\n\nDid the author need to train (k choose 2) SVMs to compute the propensity scores for samples from k treatment groups?\n\nWhen comparing different approaches, as were shown in Table 3, 4 and Figure 3,4, did the author run any statistical test, such as t-test, to confirm the difference between those distributions were significant? The standard deviations of those errors seemed quite large so the difference could be non-significant.\n\nCould the author provide more explanations on why the proposed approach, i.e., minibatch augmentation using propensity score matching, can outperform the TARNET? In TARNET, each sample it only used to update the head network corresponding to the sample's treatment assignment, why would balancing samples in the minibatch can improve the estimation of treatment effect?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}