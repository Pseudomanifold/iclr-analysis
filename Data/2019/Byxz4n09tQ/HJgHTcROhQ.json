{"title": "Official Review", "review": "Summary: \nThe paper proposes an approach for improving standard techniques for model compression, i.e. compressing a big model (teacher) in a smaller and more computationally efficient one (student), using data generated by a conditional GAN (cGAN). The paper suggests that the standard practice of training the student to imitate the behavior of the teacher *on the same training data* that the teacher was trained on is problematic and can lead to overfitting. Instead, the paper proposes learning a conditional GAN, which can potentially generate large amounts of realistic synthetic data, and use this data (in addition to original training data) for model compression.\nExperimental results show that this idea seems to improve the performance of convnet student models on CIFAR-10 classification and random forest student models on tabular data from UCI and Kaggle.\nAnother contribution of the paper is to propose an evaluation metric for generative model, called the compression score. This score evaluates the quality of generated data by using it in model compression: \u201cgood\u201d synthetic data results in a smaller gap in performance between student and teacher models.\n\nStrengths:\n-\tThe paper sheds a light on an interesting aspect in model compression. The idea of teaching a student model to imitate behavior of the teacher model on *new* data is interesting. In fact, it emphasizes the fact that we are mostly interested in imitating the teacher model\u2019s capability of generalizing to new examples rather than overfitting to training examples.\n-\tExperiments show that for several settings (model class, architecture and datasets), using synthetic data by a cGAN can be useful in reducing the gap between student and teacher models. \n-\tThe paper is clearly written and easy to follow.\n\nWeaknesses:\n-\tThe claim that reusing the same training data used for training the teacher model in model compression can lead to overfitting of student model is not very obvious and needs more experimental evidence in my opinion. One way to test this is to use some unseen real data (e.g. validation or a held-out part of training data) for model compression, and showing that it can indeed help in improving student performance.\n-\tThe claim that cGAN can generate \u201cinfinite\u201d amount of realistic data is too strong. In light of some well-known problems of GANs such as mode collapse [2] and low-support learned distributions [1], this assumption seems unrealistic. In fact, it is not too obvious how synthetic data by a generative model learned on *same training data as the teacher* can provide any additional information to real data.\n-\tWhile the idea of the proposed evaluation metric seems interesting, I believe it is not very practical, because: \n1.\tIt is computationally intensive (requires training a model from scratch on fake data)\n2.\tIt relies on performance of the compression mechanism, which might also have some idiosyncrasies that prefer some features in synthetic data which do not necessarily correspond to quality of generated data.\n\nQuestions/Suggestions:\n-\tIn addition to using held-out real data for model compression as suggested above, a useful baseline could be using standard data-augmentation techniques in model compression.\n-\tWhat would happen if a student model is very small and cannot possibly overfit training data? Would using synthetic data be still useful there?\n-\tI am actually confused about a claim made when presenting compression score in Section 5. The paper claims that the best compression score is 1 (training student model on real data), while the paper shows that in fact, good synthetic data should produce *better* accuracy than using real data. I would appreciate if authors can clarify this point.\n\nOverall recommendation: \nWhile the paper presents an interesting problem in model compression, I\u2019m leaning towards rejecting the paper because of the weaknesses mentioned above. That being said, I am happy to reconsider my decision if there is any misunderstanding on my part.\n\nReferences:\n[1] Arora, Sanjeev, and Yi Zhang. \"Do GANs actually learn the distribution? an empirical study.\" arXiv preprint arXiv:1706.08224 (2017).\n[2] Goodfellow, Ian. \"NIPS 2016 tutorial: Generative adversarial networks.\" arXiv preprint arXiv:1701.00160 (2016).\n\n\n-----\n\nUpdated score and posted a comment to author response.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}