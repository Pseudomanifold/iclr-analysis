{"title": "interesting idea, some important experiments missing", "review": "I like this paper. What the authors have done is of high quality. It is well written and clear. However, quite a lot of experiments are necessary to make this paper publishable in my opinion.\n\nStrenghts:\n- The idea to use a GAN for model compression is something that many must have considered. It is good to see that someone has actually tried it and it works well.\n- I think the compression score is definitely an interesting idea on how to compare GANs that can be of practical use in the future.\n- The experimental results, which are currently in the paper, largely support what the authors are saying.\n\nWeaknesses:\n- The authors don't compare how good this technique is in comparison to simple data augmentation. My suspicion is that the difference will be small. I realise, however, that the advantage of this method over data augmentation is that it is harder to do it for tabular data, for which the proposed method works well. Having said that, models for tabular data are usually quite simple in comparison to convnets, so compressing them would have less impact.\n- The experiments on image data are done with CIFAR-10, which as of 2018 is kind of a toy data set. Moreover, I think the authors should try to push both the baselines and their technique much harder with hyperparameter tuning to understand what is the real benefit of what they are proposing. I suspect there is a lot of slack there. For comparison, Urban et al. [1] trained a two-layer fully connected network to 74% accuracy on CIFAR-10 using model compression.\n\n[1] Urban et al. Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)? 2016.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}