{"title": "Interesting connections to study of social dilemma and role of peer evaluation; experiments not enough to make the scalability claim  ", "review": "The paper introduces a DQN based, hierarchical, peer-evaluation scheme for reward design that induces cooperation in semi-cooperative multi-agent RL systems. The key feature of this approach is its scalability since only local \u201ccommunication\u201d is required -- the number of agents is impertinent; no states and actions are shared between the agents. Moreover this \u201ccommunication\u201d is bound to be low dimensional since only scalar values are shared and has interesting connections to sociology. Interesting metaphor of \u201cfeel\u201d about a transition.\n\nRegarding sgn(Z_a) in Eq2, often DQN based approaches clip their rewards to be between say -1 and 1. The paper says this helps reduce magnitude, but is it just an optimization artifact, or it\u2019s necessary for the reward shaping to work, is slightly unclear. \n\nI agree with the paper\u2019s claim that it\u2019s important for an agent to learn from it\u2019s local observation than to depend on joint actions. However, the sentence \u201cThis is because similar partially-observed transitions involving different subsets of agents will require different samples when we assume that agents share some state or action information.\u201d is unclear to me. Is the paper trying to just say that it\u2019s more efficient because what we care about is the value of the transition and different joint actions might have the same transition value because the same change in state occured. However, it seems that paper is making an implicit assumption about how rewards look like. If the rewards are a function of both states and actions, r(s,a) ignoring actions might lead to incorrect approximations.\n\nIn Sec 3.2, under scalability and flexibility, I agree with the paper that neural networks are weird and increasing the number of parameters doesn\u2019t necessarily make the task more complex. However the last sentence ignores parameter sharing approaches as in [1], whose input size doesn\u2019t necessarily increase as the number of agents grows. I understand that the authors want to claim that the introduced approach works in non homogeneous settings as well.\n\nI get the point being made, but Table 1 is unclear to me. In my understanding of the notations, Q_a should refer to Action Q-table. But the top row seems to be showing the perceived reward matrix. How does it relate to Mission Q-table and Action Q-table is not obviously clear.\n\nGiven all the setup and focus on flexibility and scalability, as I reach the experiment section, I am expecting some bigger experiments compared to a lot of recent MARL papers which often don\u2019t have more two agents. From that perspective the experiments are a bit disappointing. Even if the focus is on pedagogy and therefore pursuit-evasion domain, not only are the maps quite small, the number of agents is not that large (maximum being 5). So it\u2019s hard to confirm whether the scalability claim necessarily make sense here. I would also prefer to see some discussion/intuitions for why the random peer evaluation works as well as it did in Fig 4(a). It doesn\u2019t seem like the problem is that of \\beta being too small. But then how is random evaluation able to do so much better than zero evaluation?\n\nOverall it\u2019s definitely an interesting paper. However it needs more experiments to confirm some of its claims about scalability and flexibility.\n\nMinor points\nI think the section on application to actor critic is unnecessary and without experiments, hard to say it would actually work that well, given there\u2019s a policy to be learned and the value function being learned is more about variance reduction than actual actions.\nIn Supplementary, Table 2: map size says 8x7. Which one is correct?\n\n[1]: https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}