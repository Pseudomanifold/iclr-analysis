{"title": "Interesting idea, more experimental results needed", "review": "** Summary: **\n\nThe authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions.\n\n** Quality: **\nThe paper is mathematically detailed and correct.\n\n** Clarity: **\nThe paper is sufficiently easy to follow and explains all the necessary background.\n\n** Originality & Significance: **\nThe paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. \n\nMy two main points where I think the paper could improve are:\n- More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution?\n- A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}