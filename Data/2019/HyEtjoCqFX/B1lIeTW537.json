{"title": "Simple approach that appears to work well", "review": "This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective.\n\nA more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution.\n\nThey also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games.\n\nThis work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information?\n\nEmpirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result.\n\nThe fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion.\n\nOverall it appears this work provides:\n- An algorithm for Soft Q learning with a learned independent prior\n- Moderate evidence for gains compared with a flat prior on Atari.\n- A connection with this approach and regularization by constraining the mutual information between state and action distributions.\n\nIt could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}