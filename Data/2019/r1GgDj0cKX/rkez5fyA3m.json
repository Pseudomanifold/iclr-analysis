{"title": "Needs more formalism regarding the regularization path", "review": "\n\n==Major comments==\n\nYou need to better explain how the regularization path is obtained for your method. It is not clear to me at all why the iterates from lines 5-8 in Alg 2 provide a valid regularization path. \n\nI am very confused by section 3.4. Is the pruning strategy introduced in this section specific to LBI? In other words, is there some property of LBI where the regularization path can be obtained by sorting the parameters by weight? It seems like it's not specific to LBI, since you use this pruning strategy for other models in your experiments. Is this the right baseline? Surely there are other sparsification strategies.\n\n\nHow/why did you select 5e-4 for lambda? You should have tuned for performance on a validation set. Also, you should have tuned separately for each of the baselines. There is no reason that they should all use the same lambda value.\n\nCan you say anything about the suboptimality of the support sets obtained by your regularization paths vs. if you had trained things independently with different regularization penalties?\n\nI am very concerned by this statement: \n\"However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers.\"\nThis is important. Do you have idea of why it is true? This instability seems like an obstacle for large-scale deployment.\n\nAre your baselines state of the art? Is there anything discussed in the related work section that you should also be comparing against?\n\n\n==Minor comments==\nThe difference between Alg 2 and 3 is mechanical and should be obvious to readers. I'd remove it, as the notation is complex and it doesn't add to the exposition quality. Instead, you should provide an algorithm box that explains how you postprocess W to obtain a sparse network.\n\nYour citation format is incorrect. You should either have something along the lines of \"foo, which does X, was introduced in author_name et al. (2010)\" or \"foo does X (author_name, 2010).\" Perhaps you're using \\citet instead of \\citep in natbib.\n\nAlgorithm box 1 is not necessary. It is a very standard concept in machine learning. \n\nOn the left hand of (5), shouldn't Prox be subscripted by L instead of P?\n\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}