{"title": "Good empirical results, but only one baseline and poor writing.", "review": "The present paper proposes a fast approximation to the softmax computation when the number of classes is very large. This is typically a bottleneck in deep learning architectures. The approximation is a sparse two-layer mixture of experts.\n\nThe paper lacks rigor and the writing is of low quality, both in its clarity and its grammar. See a list of typos below.\n\nAn example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation. Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.\n\nAlgorithm 1 does not include mitosis, which may have an effect on the resulting approximation.\n\nHow are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?\n\nThe results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?\n\nThe column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases. Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.\n\nAll in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.\n\nA brief list of typos:\n\n\"Sparse Mixture of Sparse of Sparse Experts\"\n\"if we only search right answer\"\n\"it might also like appear\"\n\"which is to design to choose the right\"\nsparsly\n\"will only consists partial\"\n\"with \u03b3 is a lasso threshold\"\n\"an arbitrarily distance function\"\n\"each 10 sub classes are belonged to one\"\n\"is also needed to tune to achieve\"", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}