{"title": "A super interesting paper discussing the impact of data distribution on adversarial robustness of trained neural networks", "review": "This paper provides several theoretical and practical insights on the impact of data distribution to adversarial robustness of trained networks. The paper reads well and provides analysis on two datasets MNIST and CIFAR10. I particularly like the result demonstrating that a lossless transformation on the data distribution could significantly impact the robustness of an adversarial trained models. The idea of using smoothness and saturation to bridge the gap between the MNIST and CIFAR10 datasets was also very interesting. One thing that is not clear from the paper is how one could use the findings from this paper and put it into practice. In other words, it would help if the authors could provide some insights on how to improve a model robustness w.r.t the changes in the data distribution. The authors did an attempt toward this in section 5, but that seems to only cover three factors that do not cause the difference in robustness.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}