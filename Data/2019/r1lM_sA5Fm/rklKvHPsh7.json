{"title": "Studied the problem of question generation. However the paper is hard to follow and the proposed model lacks novelty.", "review": "The paper studies question generation, which is an important problem in many real applications. The authors propose to use better caching model and more evalution methods to deal with the problem. However, the paper is poorly written and hard to follow, and the proposed model lacks of novelty. The main reasons are as below:\n\n1) In model section, the task definition is not clear. It is expected to see what's the question generation task studied in this paper. An example or a model overview will definitly help.\n\n2) The encoder and decoder are not novel, it is expected to cite and compare with the existing similar encoder architecture, such as the encoder proposed in bidaf \"Seo, Minjoon, et al. \"Bidirectional attention flow for machine comprehension.\" arXiv preprint arXiv:1611.01603 (2016).\"  The math symbols are aligned, for example, h_a or h^a is used to represent the encoding. Besides, adding the binary feature in the embedding is not necessary, the LSTM model could learn such sequential correlation. The decoder description is not clear as well and expected to compare with existing work (e.g., bidaf) to show the difference.\n\n3) The proposed copy mecahnism is not clear. A formal definition of s_t, v_t and y_(t-1) should be given before defining the p_t. A more serious question, what is the fuse operation used to define p_t? concat, elementwise_plus or others?\n\n4) In the training, how to deal the ground truth that are not in the vocab? The authors stated \"using a modified heuristic described below\", but no follow-ups in the paper.\n\n5) The paper is not well written and organized. Small typos: in introduction, 'and and answer span', 'and output and output sequences'. In model, 'Glorot initialization', 'Bahdanau attention', it is not the common way to cite others' work. In encoder, the defintion of the state for decoder could be reorganized to the decoder.\n\nI have read the authors' detailed rebuttal. Thanks.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}