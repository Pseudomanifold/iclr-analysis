{"title": "Doubts about the two main contributions", "review": "The overall goal of the paper is to make end-to-end dialogue systems more empathetic, so that they can respond more appropriately and in ways that acknowledge how the users are feeling. The authors make two contributions towards that goal: (1) they introduce a crowdsourced dataset (EmpatheticDialogue) annotated with fine-grained emotion labels. (2) They show improvements on dialogue generation (in terms of empathy, but also relevance and fluency) using a multi-task objective, ensemble of encoders, and a more ad-hoc technique that consists of prepending inferred emotion labels to the input.\n\nIn terms of technical novelty, the work is relatively incremental: (A) The use of multi-task objectives in sequence models [1] is relatively common nowadays (there is little mathematical details in the paper, so it\u2019s hard to see how the approach of the paper really differs from extensive related work.). (B) Prepending predictions: prepending class labels to the input is also relatively common (e.g., in multilingual NMT to select a language). [2] presents a similar approach for polite response generation, where they prepend a label using a politeness classifier.\n\nI also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together):\n\n(1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation. The problem with prompting workers for specific emotions is that this assumes they are good actors and this is likely to produce exchanges that are rather clich\u00e9 and overdone (e.g., Table 1: the label \u201cafraid\u201d yields a situation that is rather spooky and unlikely in the real world, and the conversations themselves are rather clich\u00e9 and incorporate little details that would make them sound real).  The authors justify this dataset by pointing out that existing real-world datasets underrepresent rare emotions (e.g., afraid), but that\u2019s just a reflection of how these emotions are distributed in the real world. Better subsampling strategies would enable a better balance in the distribution without having to give up on real-world data (filtering using emojis, hashtags, etc.).  As the paper shows quantitative gains using this dataset, it is probably ok to use but, qualitatively, this dataset is probably not for everyone working on emotion in NLP. \n\n(2) Improvement in empathetic dialogue generation: The paper shows improvements across the board compared to a Transformer baseline, but the question the authors do not satisfactorily address is whether their explicit (and I would say sometimes ad-hoc) treatment of empathy (e.g., using emotion classifier, etc.) is crucially needed to get better empathetic dialogues, since the authors did not control for training data size and model capacity. Indeed, the authors exploited different amounts of data (out of-domain, or both in- and out-of-domain), different model capacities (going from baseline Transformer to model ensembles), and sometimes richer input (e.g., pre-trained emotion classifier). The results might only be showing that more data or more model capacity helps, which would of course not be surprising at all. The fact that generated outputs improve in all aspects (not only empathy, but in attributes completely unrelated to empathy such as fluency and relevance) suggests that the improvement is due to more data or capacity (e.g., perhaps yielding better encoder).  More statistics in the table in terms of number of parameters and amount of in- and out-of-domain data used for each experiment would help draw a clearer picture.\n\nAbout the use of Reddit: this might not be the best background dataset, as it\u2019s mostly strangers talking to other strangers, presumably causing the baseline to be weak on empathy. Twitter or other social-network type datasets (letting you follow people rather topics) *might* be better suited as it comparatively involves more exchanges between people who actually know each other and who are thus more likely to behave empathetically.\n\nOverall, the paper doesn\u2019t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but I think there are problems with both.\n\nTypos:\n\nIntroduction: \u201cfro\u201d\nReferences: Elizaa \n\n[1] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser  \nMulti-task Sequence to Sequence Learning\nhttps://arxiv.org/abs/1511.06114\n\n[2] Tong Niu and Mohit Bansal\nPolite Dialogue Generation Without Parallel Data\nhttps://arxiv.org/pdf/1805.03162.pdf", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}