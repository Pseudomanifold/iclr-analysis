{"title": "straightforward idea, but this approach may not be applicable in general applications", "review": "This paper aims at solving the problem of estimating sparse rewards in a high-dimensional input setting. The authors provide a simplified version by learning the states from demonstrations. This idea is simple and straightforward, but the evaluation is not convincing. \n\nI am wondering if this approach still works in more general applications, e.g., when state distributions vary dramatically or visual perturbations arise in the evaluation phase.  \n\nIn addition, it is weird to use adversary scheme to estimate rewards. Namely, the agent is trying to maximize the rewards, but the discriminator is improved so as to reduce rewards. \n\nIn section 3, the authors mention an early termination of the episode, this is quite strange in real applications, because even the discriminator score is low the robot still needs to accomplish the task.\n\nFinally, robots are subject to certain physical constraints, this issue can not be addressed by merely learning demonstrated states.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}