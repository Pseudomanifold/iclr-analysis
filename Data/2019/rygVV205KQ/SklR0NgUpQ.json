{"title": "Sample complexity experiments are interesting, but the ideas presented seems to overlap ideas from existing work.", "review": "---\nUpdate: I think the experiments are interesting and worthy of publication, but the exposition could be significantly improved. For example:\n\n- Not sure if Figure 1 is needed given the context.\n- Ablation study over the proposed method without sparse reward and hyperarameter \\alpha\n- Move section 7.3 into the main text and maybe cut some in the introduction\n- More detailed comparison with closely related work (second to last paragraph in related work section), and maybe reduce exposition on behavior cloning.\n\nI like the work, but I would keep the score as is.\n---\n\n\nThe paper proposes to use a \"minimal adversary\" in generative adversarial imitation learning under high-dimensional visual spaces. While the experiments are interesting, and some parts of the method has not been proposed (using CPC features / random projection features etc.), I fear that some of the contributions presented in the paper have appeared in recent literature, such as InfoGAIL (Li et al.).\n\n- Use of image features to facilitate training: InfoGAIL used pretrained ResNet features to deal with high-dimensional inputs, only training a small neural network at the end.\n- Tracking and warm restarts: InfoGAIL does not seem to require tracking a single expert trajectory, since it only classifies (s, a) pairs and is agnostic to the sequence.\n- Reward augmentation: also used in InfoGAIL, although they did not use sparse rewards for augmentation.\n\nAnother contribution claimed by this paper is that we could do GAIL without action information. Since we can shape the rewards for most of our environments that do not depend on actions, it is unsurprising that this could work when D only takes in state information. However, it is interesting that behavior cloning pretraining is not required in the high-dimensional cases; I am interested to see a comparison between with or w/o behavior cloning in terms of sample complexity. \n\nOne setting that could potentially be useful is where the expert and policy learner do not operate within the same environment dynamics (so actions could not be same) but we would still want to imitate the behavior visually (same state space). \n\nThe paper could also benefit from clearer descriptions, such as pointers to which part of the paper discusses \"special initialization, tracking, or warm starting\", etc., from the introduction.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}