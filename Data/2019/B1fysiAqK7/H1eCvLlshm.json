{"title": "The investigated problem is interesting, but methods used to handle binary networks are not impressive", "review": "To reduce the deep neural networks' reliance on memory and high power consumption, the paper proposed a kind of probabilistic neural networks with both binary hidden node and binary weights. The paper presents a probabilistic way to binarize the activation and weight values. Also, it proposed a random version of batch-normalization and max-pooling.\n\nThe binarization of hidden node is achieved through stochastic sampling according to the sign of the stochastic pre-activation values. The weight binarization is analogously done by sampling from a binary distribution. There is no too much new here, and is a standard way to obtain binary values probabilistically. \n\nThe paper said in the introduction that the binary model will be trained with the re-parameterization trick, through either propagating the distributions or the samples from concrete distribution. But I am still not very clear how this training process is done, especially for the training of weight parameters.\n\nOverall, the problem investigated in this paper is very interesting and is of practical importance, the experimental results are preliminary but encouraging. But all the techniques used in this paper to binarize neural networks are standard, and no too much new here.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}