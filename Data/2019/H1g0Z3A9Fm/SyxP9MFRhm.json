{"title": "an interesting and novel GNN, but somehow unclear in experiments. ", "review": "This paper introduces a novel graph conv neural network, dubbed LGNN, that extends the conventional GNN using the line graph of edge adjacencies and a non-backtracking operator. It has a form of learning directed edge features for message-passing. An energy landscape analysis of the LGNN is also provided under linear assumptions. The performance of LGNN is evaluated on the problem of community detection, comparing with some baseline methods. \n\nI appreciate the LGNN formulation as a reasonable and nice extension of GNN. The formulation is clearly written and properly discussed with message passing algorithms and other GNNs. Its potential hierarchical construction is also interesting, and maybe useful for large-scale graphs. In the course of reading this paper, however, I don\u2019t find any clear reason why this paper restricts itself to community detection, rather than general node-classification problems for broader audience. It would have been more interesting if it covers other classification datasets in their experiments. \n\nMost of the weak points of this paper lie in the experimental section. \n1. The experimental sections do not have proper ablation studies, e.g., as follows.   \nAs commented in Sec 6.3, GAT may underperform due to the absence of the degree matrix and this needs to be confirmed by running GAT with the degree term. And, as commented in footnote 4, the authors used spatial batch normalization to improve the performance of LGNN. But, it\u2019s not clear how much it obtains for each experiment and, more importantly, whether they use the same spatial batch norm in other baselines. To make sure the actual gain of LGNN, this needs be done with some ablation studies. \n2. The performance gain is not so significant compared to other simpler baselines, so the net contribution of  the line-graph extension is unclear considering the above. \n3. The experimental section considers only a few number of classes (2-5) so that it\u2019s does not show how it scales with a large number of classes. In this sense, other benchmark datasets with more classes (e.g., PPI datasets used in GAT paper) would be better. \n\nI hope to get answers to these. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}