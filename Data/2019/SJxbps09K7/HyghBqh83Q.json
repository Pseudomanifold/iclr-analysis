{"title": "This paper provides interesting insight into the usefulness of GANs to perform alignment of embedding spaces across languages. The paper is well motivated and introduces the problem well. While existing experiments are sufficient, there is scope to add more experiments.", "review": "This paper is clear and well written, except for the occasional typos, eg. missing a bracket in the very first equation.\nThe problem of learning transformations to align vector spaces across languages is extremely interesting and relevant. While the paper does a thorough job in explaining the metrics used and in detailing the procedure followed, I would have liked to see the following the details in the paper,\n1)Number of unique word tokens in Bengali, Cebuano etc. While clicking on the reference link to MUSE provides these details, including the same in the paper, would make the writing wholesome.\n2)While a thorough theoretical analysis is provided to explain the quality of the transformation/rotation matrix, there are no empirical results provided on any cross-lingual classification tasks. While not too important to the paper, such an analysis would provide the reader with an idea of how much can the proposed techniques be used in end-to-end train system for cross lingual tasks.\n3)The authors comment on the quality of embeddings for languages such as Bengali, when studying the problem of alignment. Do the authors have any opinion if use of meta-embeddings i.e a weighted combination of say GloVe, word2vec and FastText can over come some of this limitations?Since each of these methods exploits a different feature of the training corpus, and there is rich literature showing the effectiveness of meta-embeddings, would this be something worth pursuing?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}