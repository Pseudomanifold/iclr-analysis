{"title": "Interesting analysis, but not much new", "review": "This paper presents a series of empirical observations when aligning word embeddings in different languages with the unsupervised method of Conneau et al. (2018). While I am the first to defend that research should not be only about fighting for SOTA and I would love to see more analysis papers like this, I think that this one in particular does not have enough novelty/substance to get accepted at such a competitive venue. The paper is well written and I enjoyed reading it, but I was left with the feeling that I did not learn much from it.\n\nGiven that the paper is well structured into 9 different observations, I will next comment them one by one:\n\n- Observation 1: Impossible cases are not solely the results of linguistic differences, but also of corpus characteristics. This was already shown in Sogaard et al. (2018) and, in my opinion, their paper does a better work at systematically studying this effect in their ablation study.\n\n- Observation 2: Impossible cases can also be the result of the inductive biases of the underlying word embedding algorithms. Same as above: this was already shown (and better supported in my opinion) in Sogaard et al. (2018).\n\n- Observation 3: GAN-based UBDI becomes more unstable and performance deteriorates with unit length normalization. This is a somewhat minor detail, but I find it interesting, as it challenges a common practice in supervised bilingual dictionary induction.\n\n- Observation 4: GAN-based UBDI becomes more unstable and performance deteriorates with PCA pruning. I also find this one interesting, although it is essentially a negative result.\n\n- Observation 5: GAN-based UBDI is largely unaffected by noise injection. I do not see why this could be relevant, as one should never find this type of synthetic noise in realistic settings.\n\n- Observation 6: In the hard cases, GAN-based UBDI gets stuck in local optima. I might be missing something here, but this seems pretty obvious. Why would the method fail to converge to the correct solution otherwise (assuming that the unsupervised objective function is appropriate)?\n\n- Observation 7: Over-parametrization does not consistently help in the hard cases. This is another negative result: not bad to know, but does not have much value on its own.\n\n- Observation 8: Changing the batch size or the learning rate to hurt the discriminator also does not help. Just another negative result. The authors themselves state that \"it seems the MUSE default hyperparameters are close to optimal\". Again, not bad to know, but this only confirms that Conneau et al. (2018) did a good job.\n\n- Observation 9: In the hard cases, model selection with cosine similarity can stabilize GAN-based UBDI. Your model selection criterion is the one proposed by Conneau et al. (2018) themselves, so I do not see where the novelty is here.\n\nOther contributions:\n\n- As cited in the paper, the procrustes fit was proposed by Kementchedjhieva et al. (2018) for this exact same problem, so there is no original contribution here either (other than applying it to a different dataset).\n\n- The paper classifies different language pairs as \"easy\", \"hard\" and \"impossible\". I think that the distinction between \"easy\" and \"hard\" is well supported, as it is putting Bengali and Cebuano into a third (even harder) group. However, I do not see enough evidence to name this third group \"impossible\". Such a strong statement should be rigorously supported, and I do not find this to be the case. At most, you could speculate that some language pairs might be impossible, but we cannot be 100% sure that they are with the provided evidence.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}