{"title": "Is this really a type of convolutional network?", "review": "The paper develops a new 'convolution' operation. \nI think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.\n\np2-3, Section 3.1 - I found the equations impossible to read. What are the subscripts over?\nIn (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??\nIs the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?\n\nEquation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?\n\nExperimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost. It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.\n\nIt would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}