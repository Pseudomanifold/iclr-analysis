{"title": "Review of Adaptive Convolutional Neural Networks.", "review": "The paper introduces adaptive kernels (that adapts its weights as a function of image content) to the framework of CNN. The benefit of adaptive kernels is the reduction of memory usage (at training and at the inference time) as well as training speedups (up to 2x). The kernels are evaluated on two datasets MNIST and CIFAR10\n\nI like the idea of building models that are memory efficient at training and at evaluation time. However, the evaluation of the proposed adaptive kernels is rather limited. In order to improve the paper, the authors could take into consideration the following points:\n\n1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?\n2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?\n3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance. How big is the generalization gap for the tested models when adaptive kernel is used?\n4. How sensitive are the results to the number of adaptive kernels in the layers.\n5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?\n6. On CIFAR10 the results seem to be worse that other methods. However, it is important to note that the Adaptive Kernels CNN has way less parameters. It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.\n7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.\n8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016). It might be beneficial to include comparison to this approach in the experimental section. Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.\n9. The ideas presented in the paper seems related to general concept of hypernetworks, where one network learns (or helps to learn) paramenters of the other network. It would be nice to position the ideas from the paper w.r.t. this line of research too.\n10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).\n\nI like the drawings, however, the font on the drawings is too small - making it hard to read.\n\nSome typos:\n1. the difficult to train the network\n2. table 2: Dynamic -> Adaptive?\n\nOverall, the paper presents interesting ideas with some degree of originality. I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}