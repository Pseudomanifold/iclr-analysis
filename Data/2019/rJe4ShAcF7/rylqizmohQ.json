{"title": "An application of transformer to music generation", "review": "In this paper the authors propose an algorithm to reduce the memory\nrequirements for calculating relative position vectors in a\nself-attention (transformer) network, based on the work of [Vaswani et\nal., 2017; Shaw et al. 2018]. The authors applied their model to a music\ngeneration task, and evaluated it on two datasets (J.S. Bach Chorales\nand Piano-e-Competition). Their model obtained improvements over the\nstate-of-the-art in the Piano-e-Competition set in terms of\nlog-likelihoods. Additionally, they performed human evaluation on the\nPiano-e-Competition set showing preference of the participants for their\nmethod over the state-of-the-art.\n\nThe application of the transformer network seems suitable for the task,\nand the authors fairly justify their motivations and choices. They show\nimprovements over the-state-of-the-art for one data-set and explained\ntheir results. They also show an interesting application of\nsequence-to-sequence models for generating complete pieces of music\nbased on a given melody.\n\nMy main concern is the novelty of the paper. The authors use the model\nproposed by [Shaw et al. 2018] with an additional modification to manage\nvery long sequences proposed by [Liu et al., 2018; Parmar et al., 2018],\n(chunking the input sequences in non-overlaping blocks and calculating\nattention only on the current and the previous blocks). Their main\ncontribution is to reduce the memory requirement for matrix operations\nfor calculating the relative position vectors of the self-attention\nfunction, which was sub-optimal in [Shaw et al. 2018]. The memory\nreduction is from O(L^2D+L^2) to O(LD+L^2). I would qualify this as an\noptimization in the implementation of the existing method rather than a\nnew approach.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}