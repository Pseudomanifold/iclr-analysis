{"title": "An increment of GraphSAGE with restricted applications", "review": "This paper modifies the GraphSAGE on unsupervised inductive node embedding.\nThe authors propose to use the bi-attention architecture to sample\ninteresting nodes (instead of the uniform sampler in GraphSAGE), and to use a\nglobal embedding bias matrix in the local aggregating functions. The method\nshowed improvements over GraphSAGE and other baselines on unsupervised\ngraph embeddings.\n\nThe proposition makes sense and the performance improvements are expected.\n\nA major comment, however, is that that the proposed method is useful in very\nrestricted settings, and it is not clear how to generalize to\nother applications which GraphSAGE can be applied on.\nThe overall technical contribution is incremental and\nmay not have enough novelty to be published in ICLR.\n\nThe technical representation is very poor, unorganized and not self-contained.\nThe paper cannot pass the threshold merely based on the way it is presented.\n\nIn the algorithms, please give the output besides the input. After the\nalgorithms, please remark on the computational and memory cost.\n\nIn algorithm 1, what is this function BIATT()? \nAfter algorithm 1, please describe this function as well as SAGE(). \n\nIn the beginning of section 3, please describe the meaning of the\nglobal bias matrix. In algorithm 1, if B is zero-initialized, why\ndoes one need it as input?\n\nSome of the equations are poor formatted (e.g. reduce_sum in page 5).\nPlease try to use rigorous mathematical formulations instead of \"pseudo equations\".\nFor example, re-write \"One_hot(i)\". In section 3.2, explain A_{gg}, etc.\nuse $\\langle \\rangle$ instead of $\\alpha$.\n\nThere are many typos.", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}