{"title": "Clear writing, strong results, sensible algorithm, good paper", "review": "This paper presents an approach to exploration in RL via random network distillation.\nThe agent generates a random neural network, and adds an \"intrinsic reward\" based on the regression error of this random function.\nThe main evidence for its efficacy comes from evaluation on Atari games, particularly Montezuma's revenge, where it attains state of the art results.\n\nThere are several things to like about this paper:\n\n- The writing is clear and well thought out. \n- The actual algorithm is sensible, simple, intuitive and clearly effective.\n- The results are significant: this is really a \"step change\" compared to previous Montezuma results.\n- This work takes the well-known \"exploration bonus\" approach, combines it with some of the observations of (Osband et al) and simplifies the treatment... so in some ways it's quite standard... but there are several new insights:\n  + Focus on normalization schemes for \"randomized prior function\"\n  + Bootstrapping \"intrinsic reward\" over episode boundaries\n  + Incorporating large-scale policy-based algorithms\n\nTo help improve the paper, I will highlight some potential issues:\n\n- For a paper on exploration, it does not make sense to present results in terms of \"parameter updates\". This should instead be presented in terms of actor/environment steps. This is something that happens consistently across the paper. If you want to show that \"many actors makes it better\" then you can divide this by #actors... so that the curves still functionally look the same. This is an easy thing to change... but I think it's important to do this!\n- Like other \"count-based\" methods, this exploration bonus is not linked to the task. As such, you have to get \"lucky\" that you do the right kind of generalization from the \"random network\". I think that you should mention this issue, potentially in your section 2. That is not to say that this is therefore a bad method, but particularly with reference to (Osband et al 2018) this approach does not address their observation from Section 2.4 of that paper... you don't necessarily get the \"right\" type of generalization from this random network (that has nothing to do with the task). You could then point out that, empirically, using a random convnet seems to do just fine in Atari! ;D\n- The whole section about \"pure exploration\" is somewhat interesting, but you shouldn't assess that performance in terms of \"reward\"... because that is just a peculiarity of these games... we could easily imagine a game where \"pure exploration\" gives a huge negative reward... but that wouldn't mean that it was bad at pure exploration! Therefore, how can you justify the quality of pure exploration by reference to the \"best return\".\n- Although the paper is definitely good, and I've already outlined several truly novel additions from this paper, on another level the actual intellectual contribution of this paper is perhaps not *as* large as it may seem from the Abstract or associated OpenAI publicity/blog posts https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/\n  + This paper is about adding an \"exploration bonus\" to RL rewards (this goes back at least to Kearns+Singh 2002)\n  + The form of this bonus comes from prediction error on a random function\n  + I have some concerns on the process of \"anonymous\" reviews in this \"blog+tweet\" setting\n\nOverall, I like the paper a lot, I think it must be accepted and also it's right at the top of ICLR best papers!\nThe writing is good, the results are good, the algorithm is good and I think it will have impact.\nThe main missing piece is a clear discussion of any of the algorithms potential weaknesses - is this the final solution to exploration? What do you think about the issues of generalization? How would this perform in a linear system? What if the basis functions are not aligned?\nIt's not that the algorithm needs to address all of these things to be a good algorithm, but the paper should try to do a better job about highlighting any potential missing pieces - particularly when the results are so impressive.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}