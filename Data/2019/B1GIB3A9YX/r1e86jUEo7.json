{"title": "Interesting idea, but rather weak paper. Can be improved a lot with additional writing effort", "review": "In this paper, the authors propose an exploration strategy based on the explicit storage and recall of trajectories leading to novel states. A pool of such trajectories is managed over time, and a method is proposed so that the agent can learn how to follow a path corresponding to these trajectories so as to explore novel states. The idea is demonstrated in a set of room experiments, and quickly shown efficient in Montezuma's Revenge and PrivateEye Atari games.\n\nOverall, the idea has some merits, but the empirical study is weak and the paper suffers from unsufficient writing effort (or more probably time).\n\nWhat I like most in the paper is the split of exploration methods into 3 categories: adding some \"intrinsic reward\" bonuses to novel states (curiosity-driven exploration) , trying to reach various goals (goal-driven exploration) and using memory to reach again novel states (memory-driven exploration). Actually, this split may be debated. For instance, some frameworks based on goals have been labelled curiosity-driven, e.g. \"Curiosity-Driven Exploration of Learned Disentangled Goal Spaces\" (Laversanne-Finot, P\u00e9r\u00e9 and Oudeyer, CoRL 2018), but anyways I find it useful. That said, this aspect of the introduction is reiterated in the \"Related Work\" section in a quite redundant way, whereas both parts could have been better integrated. Furthermore, the related work section is hardly a draft, I'll come to that later.\n\nThe presentation of the method in Section 2 is rather clear and convincing. My only concern is about the assumption that the agent is always starting in the same state. This assumption may not hold in many settings, and the approach appears to be quite dependent on it. A discussion of how it could be extended to a less restricting assumption would be welcome.\n\nThe experimental section is weaker. A few concerns:\n- I could not find much about the number of seeds, trials, the way to depict some variance, the statistical significance of the differences between results presented in Figure 1. The same is true about Figs. 2, 3 and 5.\n- In Fig.2, the claim that the author's method learns better models is hardly supported by the left-hand-side plot, and significance is not assessed.\n- I'm puzzled about the very low performance of baselines in the plots of Fig. 3. Could the author explain why these performances are null.\n- The Atari games section helps figuring out that the framework is not too specific of the rooms environment, but the lack of analysis does not help making sure that this is just the explicit recall mechanism that is responsible for superior performance and why.\n\n\nAnother point about this section is that poor writing does not help understanding some points.\n- to me, the first sentence of Section 3.2.2 does not make sense at all.\n- in the caption of Fig. 4, \"The second row is the heatmaps for states that the number of times being selected as a target state.\": I don't get what it means, thus I don't understand what that row shows.\n- Fig.5 comes with no caption\n\nAbout the related work:\n- The comparison to other methods using memory needs to be expanded. In particular, I would put HER-like mechanisms here rather than in 4.1, as \"explicit recall\" shares some importan ideas with \"experience replay\"\n- Section 4.4 (HRL) is not useful as is.\n\nFinally, in the conclusion, the claim that the method can be combined with \"many sota exploration methods\" is not supported, as the authors have only tried two and did not analyse the results in much details.\n\n\ntypos:\n\n- p4:\nwe can easily counting\n(include borders) => including\nis provide => provided\n\nare less less-visited states: quite inelegant\n\n- p7:\nIn Montezuma's Revenge, Comparing => comparing\nWhere they encourage => remove \"Where\"\n\n- p8:\nrecallcan => recall can\nthe problem of reach goals => reaching\nit succesfully reproduce => reproduces\n\nThe last paragraph of Section 4.2 needs a careful rewriting, as long sentences with parenthese in the middle appear to be some draft version.\n\ncontrol(Pritzel => Missing space\nOur method use memory => uses\nAlthough ..., but => remove but\n\nThe path function can be seen as a form of skills => skill?\nBesides, the \"can be seen\" needs to be further explained...\n\nAppendix\n\nFinally, we provided => provide\n\nis around (math formula) => cannot you be more specific?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}