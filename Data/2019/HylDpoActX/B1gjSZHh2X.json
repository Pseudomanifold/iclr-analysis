{"title": "A good work in CNN model compression", "review": "This paper proposes to use n-ary representations for convolutional neural network model quantization. A novel strategy of nested-means clustering is developed to update weights. Batch normalization is also considered in the activation quantization. Experiments on both weight quantization and activation quantization are conducted and show effectiveness.\n\nStrengths:\n1.\tThe idea of nested-means clustering is interesting, which somehow shows its effectiveness.\n2.\tState-of-the-art experimental results.\n3.\tThe representation is excellent, and it is easy to follow.\n\nConcerns:\n1.\tThough the experiment study seems solid, an ablation study is still missing. For example, how important is the nested-means clustering technique? What is the effect if replacing it with the original one or with other clustering methods? What will happen if expanding the interval in the quantization of activation? All these kinds of questions are hard to answer without an ablation study.\n2.\tIt is not clear how the weight and activation quantization are addressed together.\n3.\tIf counting the first and last layers, what is the size of the model (the number of parameters)?\n4.\tSimilarly, what are the FLOPs in different settings of experiments? This seems missing.\n5.\tWhen discussing the related work about model compression, there are important references missing. I just list two references in the latest vision and learning literature:\n[Ref1] X. Lin et al. Towards accurate binary convolutional neural network. NIPS 2017\n[Ref2] Z. Liu et al. Bi-Real Net: Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm. ECCV 2018.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}