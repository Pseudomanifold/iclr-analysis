{"title": "Interesting method but needs more experiments to support", "review": "This paper propose a method that aims to solve the following 3 problems: sensitivity to unclipped reward, robustness to the value of the discount factor, and the exploration problem. \n\nPros\nThis paper propose a transformed Bellman operator, and the author proved its convergence under some deterministic MDP conditions. The proposed transformed Bellman operator is interesting since that is analogous to some variance reduction techniques in the policy gradient literature. In the value based method literatures, those techniques have not been well studied.\n\nCons\nI think the main issue of this paper is the experiments can not fully support the advantage claim of the proposed method. \n\t1. With the author's hyper-parameters, the proposed method (Ape-X DQfD) has worse performance than the baseline Ape-X DQN, with the original hyper parameter of Ape-X DQN (Table 1). The author has a version of the baseline with the same hyper parameter as the proposed method, but the modified one is worse than the original baseline, which is not satisfactory. I think in general we should try to keep the original hyper parameter especially the original performance is better. \n\t2. With the same hyper parameters, the performance of the proposed method Ape-X DQfD is better than Ape-X DQN*(with reward clipping, gamma=0.999) with human starts but worse with no-op starts (Table 2). On the whole, their performance I would say, is similar. That makes reader questions about the utility of not use reward clipping, since without reward clipping, we did optimize the true objective, but the final performance is sometimes better and sometimes worse. I am afraid that undo the reward clipping is making the problem unnecessarily harder. \n\t3. The transformed Bellman operator transforms the Q function by a contraction. It's interesting to see what kind of effect of some ad-hoc transformations on the reward will behave. Given the particular function form the author have used, it's especially interesting to see how this transformation: r' = sgn( R) sqrt(abs(r )) will affects the performance. \n\t4. The authors ablates the method on 6 games out of the 42. However, it's mostly qualitative, rather than quantitative. I think it would be more convincing if the leave-one-out experiment could be carried out on all 42 games. \n\t5. The author combines Ape-X DQN with a modified version of DQfD, as mentioned in Section 3.4. For a fair comparison, I think there should be a corresponding modified version of DQfD as a baseline. \n\nI think the author proposed an interesting approach, however, the experiment section, especially the ablation section could be improved. It's hard to tell how much the transformed Bellman operator and the temporal consistency loss contributes on an average case, based on the current results. If the author could provide more information, I'm willing to change my review. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}