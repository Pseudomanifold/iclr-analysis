{"title": "Interesting insights about alphaGo Zero and a nice case-study.", "review": "This paper analyzes the AlphaGo Zero algorithm by showing that the optimal policy corresponds to a Nash equilibrium. The authors then show that the equilibrium corresponds to a KL-minimization. Finally, the show on a classical scheduling task.\n\nOn the positive side, the paper is well written and structured. The results presented are very interesting, specially showing that stochastic approximation of a KL-divergence minimization. The case-study is also interesting, although does not improve current state-of-the-art. On the negative side, I think the relevance and novelty of the results should be explained better.\n\nFor example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium. The MDP formalization is rather straightforward. Also, MCTS has been used extensively to find Nash equilibria in both perfect and imperfect games, e.g., \"Online monte carlo counterfactual regret minimization for search in imperfect information games\". Maybe the authors can elaborate more on the significance/relevance of this contribution.\n\nBesides, the power of AlphaGo Zero resides in the combination of the MCTS together with the compact representation learning of the value functions. The presented analysis seems to neglect the error term corresponding to the value function.\n\nThere are other minor details:\n\n- Eq(2). notation: \\forall s is missing\n- Theorem 2 should be Theorem 1\n- \"there are constraints per which state can transition\"\n- \"P1 is agent\" -> \"P1 is the agent\"\n- \"Pinker\" -> \"Pinsker\"\n- C_R in Eq(5) is not introduced.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}