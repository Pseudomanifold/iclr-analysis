{"title": "Review", "review": "Summary of paper: Builds off work from Trinh et al. (2018) that proposed a semi-supervising learning model that combined the main task with an auxiliary task of predicting tokens in the input sequence. This paper proposes improving this two-task setup by splitting the hidden representations into two subsets, one for both tasks and one for only the main task. They compare against the baseline of Trinh et al. (2018), and find small improvements in performance.\n\nAt it's core, this paper is simply trying to improve multi-task learning. Despite the heavy focus on the work of Trinh et al. (2018),  fundamentally, what the paper is trying to do is: given multiple tasks, figure out how to effectively model the tasks such that information is shared but the tasks don't harm each other. In other words, the authors mark this paper as a semi-supervised learning paper, but it is more of standard multi-task learning setup.\n\nHowever, multi-task learning (MTL) is never once mentioned in the paper. This is surprising given that there has been a wealth of literature devoted to tackling exactly this type of problem. For example, the usage of different representations for different tasks is a standard trick in MTL (multiple heads). The model is also reminiscent of Progressive Networks [1], with one representation feeding to other tasks. Unfortunately, the authors do not cite any relevant MTL work, which is a big omission. There are also no other MTL baselines in the experiments section.\n\nEven when ignoring the previous issue, the gains from the proposed method are consistently small, and explainable by noise. Furthermore, in the one experiment that there seems be a nontrivial difference (CIFAR), Table 4 shows that the model is extremely sensitive to the sharing percentage (going from 50% to 30% drops accuracy by 2.1%).\n\nTo conclude, the authors miss critical discussion and comparison of prior works in MTL, and the experimental results are unconvincing.\n\n[1] Progressive Neural Networks (https://arxiv.org/abs/1606.04671)", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}