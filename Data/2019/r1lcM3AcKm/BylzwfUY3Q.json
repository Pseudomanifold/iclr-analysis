{"title": "This work needs further analysis on a proper dataset.", "review": "This paper introduces a recurrent neural network architecture that minimizes both supervised and unsupervised losses for sequence learning. \n\nThe intuition behind of this work makes sense that the mixing gradients problem would degrade the performance of RNNs on sequence learning tasks. However, the experimental results are not convincing although quite big improvements on CIFAR10 are observed. Particularly, I am not entirely convinced by the authors' argument that adding auxiliary loss is helpful for addressing long-term dependencies. What I can see from Table 2 is that the performance of the proposed method in terms of accuracy increases as the ratio of shared parameters increases. In my opinion, these results can be also interpreted as a regularization effect. \nDatasets like MNIST and CIFAR can be used to show that the proposed idea is working as a proof of concept, but it might be difficult to draw a generalized conclusion from the results in the paper like the use of unsupervised loss during training helps.\nIn my opinion, neural machine translation datasets might be an option if the authors would like to claim the use of an unsupervised loss is helpful to capture better long-range dependencies.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}