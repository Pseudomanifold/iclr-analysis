{"title": "Interesting topics are introduced but some corrections and clarifications are necessary", "review": "The authors introduce the problem of learning embeddings that consider both text information and graph structures, as well as the embedding of a sequence of nodes with embeddings.\n\nHowever, the proposed algorithm, SENSE-S, is incremental in the sense of aggregating two simple structures. In the evaluation, it is compared only with the heuristic combination of node2vec and paragraph2vec, not with any existing work about the graph embeddings that incorporate node features even though they are mentioned in the related work.\n\nFurthermore, the objective of node sequence embedding is not clear. What do we want to represent from the embedding of node sequences? It looks like we have to keep the node embeddings anyway, and then what is the problem of just storing node ordering instead of having representation? Or can we aggregate node embeddings in some way with storing the order of nodes? These kinds of questions can be raised, mainly because of uncertain objectives. The description of preserving both ordering and node properties is too vague.\n\nAlso, SENSE does not seem to have any connection with SENSE-S. Why is SENSE-S special to SENSE? Are they independent?\n\nFinally, the authors claim that SENSE is necessary to overcome the space issue that needs q*d dimension. However, from Figure 5, it seems that the proposed algorithm actually needs O(q*d) dimensions to represent the sequence correctly. It is somewhat related to the question about i.i.d. assumption in Theorem 2, where embedding does not guarantee the orthogonality across the dimensions. \n\n* Details\n- In the introduction, \"first\" is repeated in the last paragraph of Page 1.\n- N_G(v) and N_T(\\phi) are said to be independent, but it should be the assumption since they are not the fact.\n- Eq. (2) is not aligned with Eq (3) or (4). Either one needs to be fixed or the derivation needs to be described.\n- How SVM is used needs to be described. Usage of embedding might be different depending on the usage of RBF kernel or linear kernel.\n- Using the smaller number of random walks for Citation Network because it is a larger dataset needs some explanation.\n- The calculation on the improvement percentage is completely misleading. If the accuracy is improved from 95% to 96%, it is about 1% improvement, not 20% improvement based on the error rate calculation.\n- How the training/validation/test sets are split needs more description. Is it split by nodes or edges?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}