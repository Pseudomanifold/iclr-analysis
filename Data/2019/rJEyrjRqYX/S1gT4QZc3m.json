{"title": "Unconvincing ConvLSTM variant, unsatisfactory justification of the approach, unclear model description. Contribution seems marginal.", "review": "The authors propose a variant to convLSTMs with convolutional peephole units (as opposed to the original Hadamard product) and tied gates. The description of the model is confusing, the authors don't offer a strong justification for the proposed approach, some of the technical choices seem flawed.\n\nIt is also obviously false that removing an LSTM gate does not incur in a reduction of trainable parameters: \"There were other attempts to design smaller recurrent gated models [...] based on either removing one of the gates or the activation functions [...] these models had no significant reduction in trainable parameters\".\n\nThe model description is difficult to follow, but from what I can gather the model depiction is flawed. In particular, the proposed model bases on the idea to use tied gates to reduce the number of parameters. f^{(t)}, i.e., the value that governs the input, forger and output gates, is derived via a sigmoid. When the sigmoid is zero, the network will hence set all the gates to zero, ignoring the input, forgetting the memory and suppressing the output, which is hardly a behaviour one could ever want. Unfortunately, almost half of the sigmoid spectrum will lead to such behavior. This seems to be a poor modeling decision.\n\nThe nodes in Figure 1 seem to suggest that two consecutive non-linearities are applied in a row, if W_fc is clamped to zero, as reported by the authors, there is no reason to specify it, the corresponding part of the equation can be removed and the narrative simplified. In sec3, the meaning of \u201cnet input image\u201d and \u201cnetwork gate image value\u201d is unclear. Also the description of the equation is hard to follow, e.g., the square bracket notation is eventually explained only after 8 lines of text.\n\nAt the beginning of Sec4 err_1 and err_2 are defined as the difference between the predicted frame and the target frame, and vice versa. This error is then fed to the \u201crgcLSTM input arranger unit and to the next higher layer\u201d. By my understanding the error of one layer is fed in the next as an input. I wonder if such error is provided also at inference time, giving a clear guidance to the network to produce the correct output exploiting privileged information. This could also explain why the training process was completed in only one epoch.\n\n- Typos:\n*Intro: More important \u2192 more importantly\n* page5: ReL -> ReLu", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}